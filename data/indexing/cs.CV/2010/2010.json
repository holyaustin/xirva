[{"id": "2010.00029", "submitter": "Hong-Ye Hu", "authors": "Hong-Ye Hu, Dian Wu, Yi-Zhuang You, Bruno Olshausen, Yubei Chen", "title": "RG-Flow: A hierarchical and explainable flow model based on\n  renormalization group and sparse prior", "comments": "9 pages, 7 figures, with appendix and the newly added multi-scale\n  experiments", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cond-mat.dis-nn cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Flow-based generative models have become an important class of unsupervised\nlearning approaches. In this work, we incorporate the key idea of\nrenormalization group (RG) and sparse prior distribution to design a\nhierarchical flow-based generative model, called RG-Flow, which can separate\ninformation at different scales of images with disentangled representations at\neach scale. We demonstrate our method mainly on the CelebA dataset and show\nthat the disentangled representations at different scales enable semantic\nmanipulation and style mixing of the images. To visualize the latent\nrepresentations, we introduce receptive fields for flow-based models and find\nthat the receptive fields learned by RG-Flow are similar to those in\nconvolutional neural networks. In addition, we replace the widely adopted\nGaussian prior distribution by a sparse prior distribution to further enhance\nthe disentanglement of representations. From a theoretical perspective, the\nproposed method has $O(\\log L)$ complexity for image inpainting compared to\nprevious generative models with $O(L^2)$ complexity.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2020 18:04:04 GMT"}, {"version": "v2", "created": "Fri, 2 Oct 2020 18:27:26 GMT"}, {"version": "v3", "created": "Wed, 7 Oct 2020 00:59:10 GMT"}, {"version": "v4", "created": "Fri, 18 Dec 2020 20:27:11 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Hu", "Hong-Ye", ""], ["Wu", "Dian", ""], ["You", "Yi-Zhuang", ""], ["Olshausen", "Bruno", ""], ["Chen", "Yubei", ""]]}, {"id": "2010.00042", "submitter": "Kerem Can Tezcan", "authors": "Kerem C. Tezcan, Christian F. Baumgartner, Ender Konukoglu", "title": "Sampling possible reconstructions of undersampled acquisitions in MR\n  imaging", "comments": "Main article and appendix together. GIFs can be found on\n  https://polybox.ethz.ch/index.php/s/3DPrRoYQnyzANAF", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Undersampling the k-space during MR acquisitions saves time, however results\nin an ill-posed inversion problem, leading to an infinite set of images as\npossible solutions. Traditionally, this is tackled as a reconstruction problem\nby searching for a single \"best\" image out of this solution set according to\nsome chosen regularization or prior. This approach, however, misses the\npossibility of other solutions and hence ignores the uncertainty in the\ninversion process. In this paper, we propose a method that instead returns\nmultiple images which are possible under the acquisition model and the chosen\nprior. To this end, we introduce a low dimensional latent space and model the\nposterior distribution of the latent vectors given the acquisition data in\nk-space, from which we can sample in the latent space and obtain the\ncorresponding images. We use a variational autoencoder for the latent model and\nthe Metropolis adjusted Langevin algorithm for the sampling. This approach\nallows us to obtain multiple possible images and capture the uncertainty in the\ninversion process under the used prior. We evaluate our method on images from\nthe Human Connectome Project dataset as well as in-house measured multi-coil\nimages and compare to two different methods. The results indicate that the\nproposed method is capable of producing images that match the ground truth in\nregions where acquired k-space data is informative and construct different\npossible reconstructions, which show realistic structural variations, in\nregions where acquired k-space data is not informative.\n  Keywords: Magnetic Resonance image reconstruction, uncertainty estimation,\ninverse problems, sampling, MCMC, deep learning, unsupervised learning.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2020 18:20:06 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["Tezcan", "Kerem C.", ""], ["Baumgartner", "Christian F.", ""], ["Konukoglu", "Ender", ""]]}, {"id": "2010.00052", "submitter": "Irene Ballester", "authors": "Irene Ballester, Alejandro Fontan, Javier Civera, Klaus H. Strobl,\n  Rudolph Triebel", "title": "DOT: Dynamic Object Tracking for Visual SLAM", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present DOT (Dynamic Object Tracking), a front-end that\nadded to existing SLAM systems can significantly improve their robustness and\naccuracy in highly dynamic environments. DOT combines instance segmentation and\nmulti-view geometry to generate masks for dynamic objects in order to allow\nSLAM systems based on rigid scene models to avoid such image areas in their\noptimizations.\n  To determine which objects are actually moving, DOT segments first instances\nof potentially dynamic objects and then, with the estimated camera motion,\ntracks such objects by minimizing the photometric reprojection error. This\nshort-term tracking improves the accuracy of the segmentation with respect to\nother approaches. In the end, only actually dynamic masks are generated. We\nhave evaluated DOT with ORB-SLAM 2 in three public datasets. Our results show\nthat our approach improves significantly the accuracy and robustness of\nORB-SLAM 2, especially in highly dynamic scenes.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2020 18:36:28 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["Ballester", "Irene", ""], ["Fontan", "Alejandro", ""], ["Civera", "Javier", ""], ["Strobl", "Klaus H.", ""], ["Triebel", "Rudolph", ""]]}, {"id": "2010.00058", "submitter": "Juan-Ting Lin", "authors": "Juan-Ting Lin, Dengxin Dai, and Luc Van Gool", "title": "Depth Estimation from Monocular Images and Sparse Radar Data", "comments": "9 pages, 6 figures, Accepted to 2020 IEEE International Conference on\n  Intelligent Robots and Systems (IROS 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we explore the possibility of achieving a more accurate depth\nestimation by fusing monocular images and Radar points using a deep neural\nnetwork. We give a comprehensive study of the fusion between RGB images and\nRadar measurements from different aspects and proposed a working solution based\non the observations. We find that the noise existing in Radar measurements is\none of the main key reasons that prevents one from applying the existing fusion\nmethods developed for LiDAR data and images to the new fusion problem between\nRadar data and images. The experiments are conducted on the nuScenes dataset,\nwhich is one of the first datasets which features Camera, Radar, and LiDAR\nrecordings in diverse scenes and weather conditions. Extensive experiments\ndemonstrate that our method outperforms existing fusion methods. We also\nprovide detailed ablation studies to show the effectiveness of each component\nin our method.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2020 19:01:33 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["Lin", "Juan-Ting", ""], ["Dai", "Dengxin", ""], ["Van Gool", "Luc", ""]]}, {"id": "2010.00067", "submitter": "Ioannis Papakis", "authors": "Ioannis Papakis, Abhijit Sarkar, Anuj Karpatne", "title": "GCNNMatch: Graph Convolutional Neural Networks for Multi-Object Tracking\n  via Sinkhorn Normalization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel method for online Multi-Object Tracking (MOT)\nusing Graph Convolutional Neural Network (GCNN) based feature extraction and\nend-to-end feature matching for object association. The Graph based approach\nincorporates both appearance and geometry of objects at past frames as well as\nthe current frame into the task of feature learning. This new paradigm enables\nthe network to leverage the \"context\" information of the geometry of objects\nand allows us to model the interactions among the features of multiple objects.\nAnother central innovation of our proposed framework is the use of the Sinkhorn\nalgorithm for end-to-end learning of the associations among objects during\nmodel training. The network is trained to predict object associations by taking\ninto account constraints specific to the MOT task. Experimental results\ndemonstrate the efficacy of the proposed approach in achieving top performance\non the MOT '15, '16, '17 and '20 Challenges among state-of-the-art online\napproaches. The code is available at https://github.com/IPapakis/GCNNMatch.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2020 19:18:44 GMT"}, {"version": "v2", "created": "Wed, 7 Oct 2020 00:40:43 GMT"}, {"version": "v3", "created": "Mon, 16 Nov 2020 17:35:16 GMT"}, {"version": "v4", "created": "Fri, 16 Apr 2021 16:36:09 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Papakis", "Ioannis", ""], ["Sarkar", "Abhijit", ""], ["Karpatne", "Anuj", ""]]}, {"id": "2010.00084", "submitter": "Florian Mirus", "authors": "Florian Mirus, Terrence C. Stewart, Jorg Conradt", "title": "The Importance of Balanced Data Sets: Analyzing a Vehicle Trajectory\n  Prediction Model based on Neural Networks and Distributed Representations", "comments": null, "journal-ref": null, "doi": "10.1109/IJCNN48605.2020.9206627", "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting future behavior of other traffic participants is an essential task\nthat needs to be solved by automated vehicles and human drivers alike to\nachieve safe and situationaware driving. Modern approaches to vehicles\ntrajectory prediction typically rely on data-driven models like neural\nnetworks, in particular LSTMs (Long Short-Term Memorys), achieving promising\nresults. However, the question of optimal composition of the underlying\ntraining data has received less attention. In this paper, we expand on previous\nwork on vehicle trajectory prediction based on neural network models employing\ndistributed representations to encode automotive scenes in a semantic vector\nsubstrate. We analyze the influence of variations in the training data on the\nperformance of our prediction models. Thereby, we show that the models\nemploying our semantic vector representation outperform the numerical model\nwhen trained on an adequate data set and thereby, that the composition of\ntraining data in vehicle trajectory prediction is crucial for successful\ntraining. We conduct our analysis on challenging real-world driving data.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2020 20:00:11 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["Mirus", "Florian", ""], ["Stewart", "Terrence C.", ""], ["Conradt", "Jorg", ""]]}, {"id": "2010.00114", "submitter": "Yu Guo", "authors": "Yu Guo, Cameron Smith, Milo\\v{s} Ha\\v{s}an, Kalyan Sunkavalli and\n  Shuang Zhao", "title": "MaterialGAN: Reflectance Capture using a Generative SVBRDF Model", "comments": "13 pages, 16 figures. Siggraph Asia 2020", "journal-ref": null, "doi": "10.1145/3414685.3417779", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of reconstructing spatially-varying BRDFs from a small\nset of image measurements. This is a fundamentally under-constrained problem,\nand previous work has relied on using various regularization priors or on\ncapturing many images to produce plausible results. In this work, we present\nMaterialGAN, a deep generative convolutional network based on StyleGAN2,\ntrained to synthesize realistic SVBRDF parameter maps. We show that MaterialGAN\ncan be used as a powerful material prior in an inverse rendering framework: we\noptimize in its latent representation to generate material maps that match the\nappearance of the captured images when rendered. We demonstrate this framework\non the task of reconstructing SVBRDFs from images captured under flash\nillumination using a hand-held mobile phone. Our method succeeds in producing\nplausible material maps that accurately reproduce the target images, and\noutperforms previous state-of-the-art material capture methods in evaluations\non both synthetic and real data. Furthermore, our GAN-based latent space allows\nfor high-level semantic material editing operations such as generating material\nvariations and material morphing.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2020 21:33:00 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Guo", "Yu", ""], ["Smith", "Cameron", ""], ["Ha\u0161an", "Milo\u0161", ""], ["Sunkavalli", "Kalyan", ""], ["Zhao", "Shuang", ""]]}, {"id": "2010.00127", "submitter": "Constantin Seibold", "authors": "Constantin Seibold, Jens Kleesiek, Heinz-Peter Schlemmer and Rainer\n  Stiefelhagen", "title": "Self-Guided Multiple Instance Learning for Weakly Supervised Disease\n  Classification and Localization in Chest Radiographs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The lack of fine-grained annotations hinders the deployment of automated\ndiagnosis systems, which require human-interpretable justification for their\ndecision process. In this paper, we address the problem of weakly supervised\nidentification and localization of abnormalities in chest radiographs. To that\nend, we introduce a novel loss function for training convolutional neural\nnetworks increasing the \\emph{localization confidence} and assisting the\noverall \\emph{disease identification}. The loss leverages both image- and\npatch-level predictions to generate auxiliary supervision. Rather than forming\nstrictly binary from the predictions as done in previous loss formulations, we\ncreate targets in a more customized manner, which allows the loss to account\nfor possible misclassification. We show that the supervision provided within\nthe proposed learning scheme leads to better performance and more precise\npredictions on prevalent datasets for multiple-instance learning as well as on\nthe NIH~ChestX-Ray14 benchmark for disease recognition than previously used\nlosses.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2020 22:19:40 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["Seibold", "Constantin", ""], ["Kleesiek", "Jens", ""], ["Schlemmer", "Heinz-Peter", ""], ["Stiefelhagen", "Rainer", ""]]}, {"id": "2010.00148", "submitter": "Tanweer Rashid", "authors": "Tanweer Rashid, Ahmed Abdulkadir, Ilya M. Nasrallah, Jeffrey B. Ware,\n  Hangfan Liu, Pascal Spincemaille, J. Rafael Romero, R. Nick Bryan, Susan R.\n  Heckbert and Mohamad Habes", "title": "DEEPMIR: A DEEP neural network for differential detection of cerebral\n  Microbleeds and IRon deposits in MRI", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lobar cerebral microbleeds (CMBs) and localized non-hemorrhage iron deposits\nin the basal ganglia have been associated with brain aging, vascular disease\nand neurodegenerative disorders. Particularly, CMBs are small lesions and\nrequire multiple neuroimaging modalities for accurate detection. Quantitative\nsusceptibility mapping (QSM) derived from in vivo magnetic resonance imaging\n(MRI) is necessary to differentiate between iron content and mineralization. We\nset out to develop a deep learning-based segmentation method suitable for\nsegmenting both CMBs and iron deposits. We included a convenience sample of 24\nparticipants from the MESA cohort and used T2-weighted images, susceptibility\nweighted imaging (SWI), and QSM to segment the two types of lesions. We\ndeveloped a protocol for simultaneous manual annotation of CMBs and\nnon-hemorrhage iron deposits in the basal ganglia. This manual annotation was\nthen used to train a deep convolution neural network (CNN). Specifically, we\nadapted the U-Net model with a higher number of resolution layers to be able to\ndetect small lesions such as CMBs from standard resolution MRI. We tested\ndifferent combinations of the three modalities to determine the most\ninformative data sources for the detection tasks. In the detection of CMBs\nusing single class and multiclass models, we achieved an average sensitivity\nand precision of between 0.84-0.88 and 0.40-0.59, respectively. The same\nframework detected non-hemorrhage iron deposits with an average sensitivity and\nprecision of about 0.75-0.81 and 0.62-0.75, respectively. Our results showed\nthat deep learning could automate the detection of small vessel disease lesions\nand including multimodal MR data (particularly QSM) can improve the detection\nof CMB and non-hemorrhage iron deposits with sensitivity and precision that is\ncompatible with use in large-scale research studies.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2020 23:50:14 GMT"}, {"version": "v2", "created": "Tue, 29 Dec 2020 21:08:01 GMT"}, {"version": "v3", "created": "Mon, 7 Jun 2021 05:40:41 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Rashid", "Tanweer", ""], ["Abdulkadir", "Ahmed", ""], ["Nasrallah", "Ilya M.", ""], ["Ware", "Jeffrey B.", ""], ["Liu", "Hangfan", ""], ["Spincemaille", "Pascal", ""], ["Romero", "J. Rafael", ""], ["Bryan", "R. Nick", ""], ["Heckbert", "Susan R.", ""], ["Habes", "Mohamad", ""]]}, {"id": "2010.00154", "submitter": "Xuan Xu", "authors": "Xuan Xu, Xin Xiong, Jinge Wang, Xin Li", "title": "Deformable Kernel Convolutional Network for Video Extreme\n  Super-Resolution", "comments": "To appear in ECCVW 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video super-resolution, which attempts to reconstruct high-resolution video\nframes from their corresponding low-resolution versions, has received\nincreasingly more attention in recent years. Most existing approaches opt to\nuse deformable convolution to temporally align neighboring frames and apply\ntraditional spatial attention mechanism (convolution based) to enhance\nreconstructed features. However, such spatial-only strategies cannot fully\nutilize temporal dependency among video frames. In this paper, we propose a\nnovel deep learning based VSR algorithm, named Deformable Kernel Spatial\nAttention Network (DKSAN). Thanks to newly designed Deformable Kernel\nConvolution Alignment (DKC_Align) and Deformable Kernel Spatial Attention\n(DKSA) modules, DKSAN can better exploit both spatial and temporal redundancies\nto facilitate the information propagation across different layers. We have\ntested DKSAN on AIM2020 Video Extreme Super-Resolution Challenge to\nsuper-resolve videos with a scale factor as large as 16. Experimental results\ndemonstrate that our proposed DKSAN can achieve both better subjective and\nobjective performance compared with the existing state-of-the-art EDVR on\nVid3oC and IntVID datasets.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 00:22:42 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["Xu", "Xuan", ""], ["Xiong", "Xin", ""], ["Wang", "Jinge", ""], ["Li", "Xin", ""]]}, {"id": "2010.00170", "submitter": "Samiul Alam", "authors": "Samiul Alam, Tahsin Reasat, Asif Shahriyar Sushmit, Sadi Mohammad\n  Siddiquee, Fuad Rahman, Mahady Hasan, Ahmed Imtiaz Humayun", "title": "A Large Multi-Target Dataset of Common Bengali Handwritten Graphemes", "comments": "15 pages, 12 figures, 6 Tables, Submitted to CVPR-21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Latin has historically led the state-of-the-art in handwritten optical\ncharacter recognition (OCR) research. Adapting existing systems from Latin to\nalpha-syllabary languages is particularly challenging due to a sharp contrast\nbetween their orthographies. The segmentation of graphical constituents\ncorresponding to characters becomes significantly hard due to a cursive writing\nsystem and frequent use of diacritics in the alpha-syllabary family of\nlanguages. We propose a labeling scheme based on graphemes (linguistic segments\nof word formation) that makes segmentation in-side alpha-syllabary words linear\nand present the first dataset of Bengali handwritten graphemes that are\ncommonly used in an everyday context. The dataset contains 411k curated samples\nof 1295 unique commonly used Bengali graphemes. Additionally, the test set\ncontains 900 uncommon Bengali graphemes for out of dictionary performance\nevaluation. The dataset is open-sourced as a part of a public Handwritten\nGrapheme Classification Challenge on Kaggle to benchmark vision algorithms for\nmulti-target grapheme classification. The unique graphemes present in this\ndataset are selected based on commonality in the Google Bengali ASR corpus.\nFrom competition proceedings, we see that deep-learning methods can generalize\nto a large span of out of dictionary graphemes which are absent during\ntraining. Dataset and starter codes at www.kaggle.com/c/bengaliai-cv19.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 01:51:45 GMT"}, {"version": "v2", "created": "Thu, 29 Oct 2020 23:18:35 GMT"}, {"version": "v3", "created": "Wed, 13 Jan 2021 17:19:52 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Alam", "Samiul", ""], ["Reasat", "Tahsin", ""], ["Sushmit", "Asif Shahriyar", ""], ["Siddiquee", "Sadi Mohammad", ""], ["Rahman", "Fuad", ""], ["Hasan", "Mahady", ""], ["Humayun", "Ahmed Imtiaz", ""]]}, {"id": "2010.00220", "submitter": "Khaled Kelany", "authors": "Khaled A. Helal Kelany, Nikitas Dimopoulos, Clemens P. J. Adolphs,\n  Bardia Barabadi, and Amirali Baniasadi", "title": "Quantum Annealing Approaches to the Phase-Unwrapping Problem in\n  Synthetic-Aperture Radar Imaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The focus of this work is to explore the use of quantum annealing solvers for\nthe problem of phase unwrapping of synthetic aperture radar (SAR) images.\nAlthough solutions to this problem exist based on network programming, these\ntechniques do not scale well to larger-sized images. Our approach involves\nformulating the problem as a quadratic unconstrained binary optimization (QUBO)\nproblem, which can be solved using a quantum annealer. Given that present\nembodiments of quantum annealers remain limited in the number of qubits they\npossess, we decompose the problem into a set of subproblems that can be solved\nindividually. These individual solutions are close to optimal up to an integer\nconstant, with one constant per sub-image. In a second phase, these integer\nconstants are determined as a solution to yet another QUBO problem. We test our\napproach with a variety of software-based QUBO solvers and on a variety of\nimages, both synthetic and real. Additionally, we experiment using D-Wave\nSystems's quantum annealer, the D-Wave 2000Q. The software-based solvers obtain\nhigh-quality solutions comparable to state-of-the-art phase-unwrapping solvers.\nWe are currently working on optimally mapping the problem onto the restricted\ntopology of the quantum annealer to improve the quality of the solution.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 07:04:02 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["Kelany", "Khaled A. Helal", ""], ["Dimopoulos", "Nikitas", ""], ["Adolphs", "Clemens P. J.", ""], ["Barabadi", "Bardia", ""], ["Baniasadi", "Amirali", ""]]}, {"id": "2010.00231", "submitter": "Tycho van der Ouderaa", "authors": "Tycho F.A. van der Ouderaa, Ivana I\\v{s}gum, Wouter B. Veldhuis and\n  Bob D. de Vos", "title": "Deep Group-wise Variational Diffeomorphic Image Registration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Deep neural networks are increasingly used for pair-wise image registration.\nWe propose to extend current learning-based image registration to allow\nsimultaneous registration of multiple images. To achieve this, we build upon\nthe pair-wise variational and diffeomorphic VoxelMorph approach and present a\ngeneral mathematical framework that enables both registration of multiple\nimages to their geodesic average and registration in which any of the available\nimages can be used as a fixed image. In addition, we provide a likelihood based\non normalized mutual information, a well-known image similarity metric in\nregistration, between multiple images, and a prior that allows for explicit\ncontrol over the viscous fluid energy to effectively regularize deformations.\nWe trained and evaluated our approach using intra-patient registration of\nbreast MRI and Thoracic 4DCT exams acquired over multiple time points.\nComparison with Elastix and VoxelMorph demonstrates competitive quantitative\nperformance of the proposed method in terms of image similarity and reference\nlandmark distances at significantly faster registration.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 07:37:28 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["van der Ouderaa", "Tycho F. A.", ""], ["I\u0161gum", "Ivana", ""], ["Veldhuis", "Wouter B.", ""], ["de Vos", "Bob D.", ""]]}, {"id": "2010.00243", "submitter": "Xiaoman Qi", "authors": "Xiaoman Qi, PanPan Zhu, Yuebin Wang, Liqiang Zhang, Junhuan Peng,\n  Mengfan Wu, Jialong Chen, Xudong Zhao, Ning Zang, P.Takis Mathiopoulos", "title": "MLRSNet: A Multi-label High Spatial Resolution Remote Sensing Dataset\n  for Semantic Scene Understanding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To better understand scene images in the field of remote sensing, multi-label\nannotation of scene images is necessary. Moreover, to enhance the performance\nof deep learning models for dealing with semantic scene understanding tasks, it\nis vital to train them on large-scale annotated data. However, most existing\ndatasets are annotated by a single label, which cannot describe the complex\nremote sensing images well because scene images might have multiple land cover\nclasses. Few multi-label high spatial resolution remote sensing datasets have\nbeen developed to train deep learning models for multi-label based tasks, such\nas scene classification and image retrieval. To address this issue, in this\npaper, we construct a multi-label high spatial resolution remote sensing\ndataset named MLRSNet for semantic scene understanding with deep learning from\nthe overhead perspective. It is composed of high-resolution optical satellite\nor aerial images. MLRSNet contains a total of 109,161 samples within 46 scene\ncategories, and each image has at least one of 60 predefined labels. We have\ndesigned visual recognition tasks, including multi-label based image\nclassification and image retrieval, in which a wide variety of deep learning\napproaches are evaluated with MLRSNet. The experimental results demonstrate\nthat MLRSNet is a significant benchmark for future research, and it complements\nthe current widely used datasets such as ImageNet, which fills gaps in\nmulti-label image research. Furthermore, we will continue to expand the\nMLRSNet. MLRSNet and all related materials have been made publicly available at\nhttps://data.mendeley.com/datasets/7j9bv9vwsx/2 and\nhttps://github.com/cugbrs/MLRSNet.git.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 08:03:47 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["Qi", "Xiaoman", ""], ["Zhu", "PanPan", ""], ["Wang", "Yuebin", ""], ["Zhang", "Liqiang", ""], ["Peng", "Junhuan", ""], ["Wu", "Mengfan", ""], ["Chen", "Jialong", ""], ["Zhao", "Xudong", ""], ["Zang", "Ning", ""], ["Mathiopoulos", "P. Takis", ""]]}, {"id": "2010.00246", "submitter": "Zheng Gu", "authors": "Zheng Gu, Chuanqi Dong, Jing Huo, Wenbin Li, Yang Gao", "title": "CariMe: Unpaired Caricature Generation with Multiple Exaggerations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Caricature generation aims to translate real photos into caricatures with\nartistic styles and shape exaggerations while maintaining the identity of the\nsubject. Different from the generic image-to-image translation, drawing a\ncaricature automatically is a more challenging task due to the existence of\nvarious spacial deformations. Previous caricature generation methods are\nobsessed with predicting definite image warping from a given photo while\nignoring the intrinsic representation and distribution for exaggerations in\ncaricatures. This limits their ability on diverse exaggeration generation. In\nthis paper, we generalize the caricature generation problem from instance-level\nwarping prediction to distribution-level deformation modeling. Based on this\nassumption, we present the first exploration for unpaired CARIcature generation\nwith Multiple Exaggerations (CariMe). Technically, we propose a\nMulti-exaggeration Warper network to learn the distribution-level mapping from\nphoto to facial exaggerations. This makes it possible to generate diverse and\nreasonable exaggerations from randomly sampled warp codes given one input\nphoto. To better represent the facial exaggeration and produce fine-grained\nwarping, a deformation-field-based warping method is also proposed, which helps\nus to capture more detailed exaggerations than other point-based warping\nmethods. Experiments and two perceptual studies prove the superiority of our\nmethod comparing with other state-of-the-art methods, showing the improvement\nof our work on caricature generation.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 08:14:32 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["Gu", "Zheng", ""], ["Dong", "Chuanqi", ""], ["Huo", "Jing", ""], ["Li", "Wenbin", ""], ["Gao", "Yang", ""]]}, {"id": "2010.00263", "submitter": "Miriam Bellver Bueno", "authors": "Miriam Bellver, Carles Ventura, Carina Silberer, Ioannis Kazakos,\n  Jordi Torres and Xavier Giro-i-Nieto", "title": "RefVOS: A Closer Look at Referring Expressions for Video Object\n  Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of video object segmentation with referring expressions\n(language-guided VOS) is to, given a linguistic phrase and a video, generate\nbinary masks for the object to which the phrase refers. Our work argues that\nexisting benchmarks used for this task are mainly composed of trivial cases, in\nwhich referents can be identified with simple phrases. Our analysis relies on a\nnew categorization of the phrases in the DAVIS-2017 and Actor-Action datasets\ninto trivial and non-trivial REs, with the non-trivial REs annotated with seven\nRE semantic categories. We leverage this data to analyze the results of RefVOS,\na novel neural network that obtains competitive results for the task of\nlanguage-guided image segmentation and state of the art results for\nlanguage-guided VOS. Our study indicates that the major challenges for the task\nare related to understanding motion and static actions.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 09:10:53 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["Bellver", "Miriam", ""], ["Ventura", "Carles", ""], ["Silberer", "Carina", ""], ["Kazakos", "Ioannis", ""], ["Torres", "Jordi", ""], ["Giro-i-Nieto", "Xavier", ""]]}, {"id": "2010.00288", "submitter": "Junya Saito", "authors": "Junya Saito, Ryosuke Kawamura, Akiyoshi Uchida, Sachihiro Youoku,\n  Yuushi Toyoda, Takahisa Yamamoto, Xiaoyu Mi and Kentaro Murase", "title": "Action Units Recognition by Pairwise Deep Architecture", "comments": "We changed expression of text and data", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a new automatic Action Units (AUs) recognition\nmethod used in a competition, Affective Behavior Analysis in-the-wild (ABAW).\nOur method tackles a problem of AUs label inconsistency among subjects by using\npairwise deep architecture. While the baseline score is 0.31, our method\nachieved 0.67 in validation dataset of the competition.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 10:34:41 GMT"}, {"version": "v2", "created": "Fri, 2 Oct 2020 15:57:38 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Saito", "Junya", ""], ["Kawamura", "Ryosuke", ""], ["Uchida", "Akiyoshi", ""], ["Youoku", "Sachihiro", ""], ["Toyoda", "Yuushi", ""], ["Yamamoto", "Takahisa", ""], ["Mi", "Xiaoyu", ""], ["Murase", "Kentaro", ""]]}, {"id": "2010.00291", "submitter": "Adrian Galdran", "authors": "Adrian Galdran, Jos\\'e Dolz, Hadi Chakor, Herv\\'e Lombaert, Ismail Ben\n  Ayed", "title": "Cost-Sensitive Regularization for Diabetic Retinopathy Grading from Eye\n  Fundus Images", "comments": "This paper has been accepted for publication at MICCAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Assessing the degree of disease severity in biomedical images is a task\nsimilar to standard classification but constrained by an underlying structure\nin the label space. Such a structure reflects the monotonic relationship\nbetween different disease grades. In this paper, we propose a straightforward\napproach to enforce this constraint for the task of predicting Diabetic\nRetinopathy (DR) severity from eye fundus images based on the well-known notion\nof Cost-Sensitive classification. We expand standard classification losses with\nan extra term that acts as a regularizer, imposing greater penalties on\npredicted grades when they are farther away from the true grade associated to a\nparticular image. Furthermore, we show how to adapt our method to the modelling\nof label noise in each of the sub-problems associated to DR grading, an\napproach we refer to as Atomic Sub-Task modeling. This yields models that can\nimplicitly take into account the inherent noise present in DR grade\nannotations. Our experimental analysis on several public datasets reveals that,\nwhen a standard Convolutional Neural Network is trained using this simple\nstrategy, improvements of 3-5\\% of quadratic-weighted kappa scores can be\nachieved at a negligible computational cost. Code to reproduce our results is\nreleased at https://github.com/agaldran/cost_sensitive_loss_classification.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 10:42:06 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["Galdran", "Adrian", ""], ["Dolz", "Jos\u00e9", ""], ["Chakor", "Hadi", ""], ["Lombaert", "Herv\u00e9", ""], ["Ayed", "Ismail Ben", ""]]}, {"id": "2010.00292", "submitter": "Zeyu Feng", "authors": "Zeyu Feng, Chang Xu and Dacheng Tao", "title": "Open-Set Hypothesis Transfer with Semantic Consistency", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised open-set domain adaptation (UODA) is a realistic problem where\nunlabeled target data contain unknown classes. Prior methods rely on the\ncoexistence of both source and target domain data to perform domain alignment,\nwhich greatly limits their applications when source domain data are restricted\ndue to privacy concerns. This paper addresses the challenging hypothesis\ntransfer setting for UODA, where data from source domain are no longer\navailable during adaptation on target domain. We introduce a method that\nfocuses on the semantic consistency under transformation of target data, which\nis rarely appreciated by previous domain adaptation methods. Specifically, our\nmodel first discovers confident predictions and performs classification with\npseudo-labels. Then we enforce the model to output consistent and definite\npredictions on semantically similar inputs. As a result, unlabeled data can be\nclassified into discriminative classes coincided with either source classes or\nunknown classes. Experimental results show that our model outperforms\nstate-of-the-art methods on UODA benchmarks.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 10:44:31 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["Feng", "Zeyu", ""], ["Xu", "Chang", ""], ["Tao", "Dacheng", ""]]}, {"id": "2010.00295", "submitter": "Gene Stoltz Mr", "authors": "Gene Stoltz and Andr\\'e Leon Nel", "title": "Improving spatial domain based image formation through compressed\n  sensing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we improve image reconstruction in a single-pixel scanning\nsystem by selecting an detector optimal field of view. Image reconstruction is\nbased on compressed sensing and image quality is compared to interpolated\nstaring arrays. The image quality comparisons use a \"dead leaves\" data set,\nBayesian estimation and the Peak-Signal-to-Noise Ratio (PSNR) measure.\n  Compressed sensing is explored as an interpolation algorithm and shows with\nhigh probability an improved performance compared to Lanczos interpolation.\nFurthermore, multi-level sampling in a single-pixel scanning system is\nsimulated by dynamically altering the detector field of view. It was shown that\nmulti-level sampling improves the distribution of the Peak-Signal-to-Noise\nRatio.\n  We further explore the expected sampling level distributions and PSNR\ndistributions for multi-level sampling. The PSNR distribution indicates that\nthere is a small set of levels which will improve image quality over\ninterpolated staring arrays. We further conclude that multi-level sampling will\noutperform single-level uniform random sampling on average.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 10:56:20 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["Stoltz", "Gene", ""], ["Nel", "Andr\u00e9 Leon", ""]]}, {"id": "2010.00321", "submitter": "Yi Fang", "authors": "Lingjing Wang, Xiang Li, Yi Fang", "title": "Deep-3DAligner: Unsupervised 3D Point Set Registration Network With\n  Optimizable Latent Vector", "comments": "arXiv admin note: substantial text overlap with arXiv:2006.06200", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point cloud registration is the process of aligning a pair of point sets via\nsearching for a geometric transformation. Unlike classical optimization-based\nmethods, recent learning-based methods leverage the power of deep learning for\nregistering a pair of point sets. In this paper, we propose to develop a novel\nmodel that organically integrates the optimization to learning, aiming to\naddress the technical challenges in 3D registration. More specifically, in\naddition to the deep transformation decoding network, our framework introduce\nan optimizable deep \\underline{S}patial \\underline{C}orrelation\n\\underline{R}epresentation (SCR) feature. The SCR feature and weights of the\ntransformation decoder network are jointly updated towards the minimization of\nan unsupervised alignment loss. We further propose an adaptive Chamfer loss for\naligning partial shapes. To verify the performance of our proposed method, we\nconducted extensive experiments on the ModelNet40 dataset. The results\ndemonstrate that our method achieves significantly better performance than the\nprevious state-of-the-art approaches in the full/partial point set registration\ntask.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 22:44:38 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["Wang", "Lingjing", ""], ["Li", "Xiang", ""], ["Fang", "Yi", ""]]}, {"id": "2010.00332", "submitter": "Maxim Neumann", "authors": "Maxim Neumann, Andr\\'e Susano Pinto, Xiaohua Zhai, and Neil Houlsby", "title": "Training general representations for remote sensing using in-domain\n  knowledge", "comments": "Accepted at the IEEE International Geoscience and Remote Sensing\n  Symposium (IGARSS) 2020. arXiv admin note: substantial text overlap with\n  arXiv:1911.06721", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatically finding good and general remote sensing representations allows\nto perform transfer learning on a wide range of applications - improving the\naccuracy and reducing the required number of training samples. This paper\ninvestigates development of generic remote sensing representations, and\nexplores which characteristics are important for a dataset to be a good source\nfor representation learning. For this analysis, five diverse remote sensing\ndatasets are selected and used for both, disjoint upstream representation\nlearning and downstream model training and evaluation. A common evaluation\nprotocol is used to establish baselines for these datasets that achieve\nstate-of-the-art performance. As the results indicate, especially with a low\nnumber of available training samples a significant performance enhancement can\nbe observed when including additionally in-domain data in comparison to\ntraining models from scratch or fine-tuning only on ImageNet (up to 11% and\n40%, respectively, at 100 training samples). All datasets and pretrained\nrepresentation models are published online.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2020 15:00:07 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["Neumann", "Maxim", ""], ["Pinto", "Andr\u00e9 Susano", ""], ["Zhai", "Xiaohua", ""], ["Houlsby", "Neil", ""]]}, {"id": "2010.00347", "submitter": "Luca Ferranti", "authors": "Luca Ferranti, Xiaotian Li, Jani Boutellier, Juho Kannala", "title": "Can You Trust Your Pose? Confidence Estimation in Visual Localization", "comments": "To appear in ICPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Camera pose estimation in large-scale environments is still an open question\nand, despite recent promising results, it may still fail in some situations.\nThe research so far has focused on improving subcomponents of estimation\npipelines, to achieve more accurate poses. However, there is no guarantee for\nthe result to be correct, even though the correctness of pose estimation is\ncritically important in several visual localization applications,such as in\nautonomous navigation. In this paper we bring to attention a novel research\nquestion, pose confidence estimation,where we aim at quantifying how reliable\nthe visually estimated pose is. We develop a novel confidence measure to fulfil\nthis task and show that it can be flexibly applied to different datasets,indoor\nor outdoor, and for various visual localization pipelines.We also show that the\nproposed techniques can be used to accomplish a secondary goal: improving the\naccuracy of existing pose estimation pipelines. Finally, the proposed approach\nis computationally light-weight and adds only a negligible increase to the\ncomputational effort of pose estimation.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 12:25:48 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["Ferranti", "Luca", ""], ["Li", "Xiaotian", ""], ["Boutellier", "Jani", ""], ["Kannala", "Juho", ""]]}, {"id": "2010.00352", "submitter": "Joseph K J", "authors": "K J Joseph and Vineeth N Balasubramanian", "title": "Meta-Consolidation for Continual Learning", "comments": "Accepted to NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The ability to continuously learn and adapt itself to new tasks, without\nlosing grasp of already acquired knowledge is a hallmark of biological learning\nsystems, which current deep learning systems fall short of. In this work, we\npresent a novel methodology for continual learning called MERLIN:\nMeta-Consolidation for Continual Learning.\n  We assume that weights of a neural network $\\boldsymbol \\psi$, for solving\ntask $\\boldsymbol t$, come from a meta-distribution $p(\\boldsymbol{\\psi|t})$.\nThis meta-distribution is learned and consolidated incrementally. We operate in\nthe challenging online continual learning setting, where a data point is seen\nby the model only once.\n  Our experiments with continual learning benchmarks of MNIST, CIFAR-10,\nCIFAR-100 and Mini-ImageNet datasets show consistent improvement over five\nbaselines, including a recent state-of-the-art, corroborating the promise of\nMERLIN.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 12:34:35 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["Joseph", "K J", ""], ["Balasubramanian", "Vineeth N", ""]]}, {"id": "2010.00361", "submitter": "Zipeng Xu", "authors": "Zipeng Xu, Fangxiang Feng, Xiaojie Wang, Yushu Yang, Huixing Jiang,\n  Zhongyuan Ouyang", "title": "Answer-Driven Visual State Estimator for Goal-Oriented Visual Dialogue", "comments": "Accepted at ACM International Conference on Multimedia (ACM MM 2020)", "journal-ref": null, "doi": "10.1145/3394171.3413668", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A goal-oriented visual dialogue involves multi-turn interactions between two\nagents, Questioner and Oracle. During which, the answer given by Oracle is of\ngreat significance, as it provides golden response to what Questioner concerns.\nBased on the answer, Questioner updates its belief on target visual content and\nfurther raises another question. Notably, different answers drive into\ndifferent visual beliefs and future questions. However, existing methods always\nindiscriminately encode answers after much longer questions, resulting in a\nweak utilization of answers. In this paper, we propose an Answer-Driven Visual\nState Estimator (ADVSE) to impose the effects of different answers on visual\nstates. First, we propose an Answer-Driven Focusing Attention (ADFA) to capture\nthe answer-driven effect on visual attention by sharpening question-related\nattention and adjusting it by answer-based logical operation at each turn. Then\nbased on the focusing attention, we get the visual state estimation by\nConditional Visual Information Fusion (CVIF), where overall information and\ndifference information are fused conditioning on the question-answer state. We\nevaluate the proposed ADVSE to both question generator and guesser tasks on the\nlarge-scale GuessWhat?! dataset and achieve the state-of-the-art performances\non both tasks. The qualitative results indicate that the ADVSE boosts the agent\nto generate highly efficient questions and obtains reliable visual attentions\nduring the reasonable question generation and guess processes.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 12:46:38 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["Xu", "Zipeng", ""], ["Feng", "Fangxiang", ""], ["Wang", "Xiaojie", ""], ["Yang", "Yushu", ""], ["Jiang", "Huixing", ""], ["Ouyang", "Zhongyuan", ""]]}, {"id": "2010.00378", "submitter": "Angelica I. Aviles-Rivero", "authors": "Angelica I Aviles-Rivero, Philip Sellars, Carola-Bibiane Sch\\\"onlieb,\n  Nicolas Papadakis", "title": "GraphXCOVID: Explainable Deep Graph Diffusion Pseudo-Labelling for\n  Identifying COVID-19 on Chest X-rays", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Can one learn to diagnose COVID-19 under extreme minimal supervision? Since\nthe outbreak of the novel COVID-19 there has been a rush for developing\nArtificial Intelligence techniques for expert-level disease identification on\nChest X-ray data. In particular, the use of deep supervised learning has become\nthe go-to paradigm. However, the performance of such models is heavily\ndependent on the availability of a large and representative labelled dataset.\nThe creation of which is a heavily expensive and time consuming task, and\nespecially imposes a great challenge for a novel disease. Semi-supervised\nlearning has shown the ability to match the incredible performance of\nsupervised models whilst requiring a small fraction of the labelled examples.\nThis makes the semi-supervised paradigm an attractive option for identifying\nCOVID-19. In this work, we introduce a graph based deep semi-supervised\nframework for classifying COVID-19 from chest X-rays. Our framework introduces\nan optimisation model for graph diffusion that reinforces the natural relation\namong the tiny labelled set and the vast unlabelled data. We then connect the\ndiffusion prediction output as pseudo-labels that are used in an iterative\nscheme in a deep net. We demonstrate, through our experiments, that our model\nis able to outperform the current leading supervised model with a tiny fraction\nof the labelled examples. Finally, we provide attention maps to accommodate the\nradiologist's mental model, better fitting their perceptual and cognitive\nabilities. These visualisation aims to assist the radiologist in judging\nwhether the diagnostic is correct or not, and in consequence to accelerate the\ndecision.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2020 15:38:24 GMT"}, {"version": "v2", "created": "Sun, 4 Jul 2021 18:30:43 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Aviles-Rivero", "Angelica I", ""], ["Sellars", "Philip", ""], ["Sch\u00f6nlieb", "Carola-Bibiane", ""], ["Papadakis", "Nicolas", ""]]}, {"id": "2010.00400", "submitter": "Ruben Tolosana", "authors": "Javier Hernandez-Ortega, Ruben Tolosana, Julian Fierrez and Aythami\n  Morales", "title": "DeepFakesON-Phys: DeepFakes Detection based on Heart Rate Estimation", "comments": null, "journal-ref": "Proc. 35th AAAI Conference on Artificial Intelligence Workshops,\n  2021", "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This work introduces a novel DeepFake detection framework based on\nphysiological measurement. In particular, we consider information related to\nthe heart rate using remote photoplethysmography (rPPG). rPPG methods analyze\nvideo sequences looking for subtle color changes in the human skin, revealing\nthe presence of human blood under the tissues. In this work we investigate to\nwhat extent rPPG is useful for the detection of DeepFake videos.\n  The proposed fake detector named DeepFakesON-Phys uses a Convolutional\nAttention Network (CAN), which extracts spatial and temporal information from\nvideo frames, analyzing and combining both sources to better detect fake\nvideos. This detection approach has been experimentally evaluated using the\nlatest public databases in the field: Celeb-DF and DFDC. The results achieved,\nabove 98% AUC (Area Under the Curve) on both databases, outperform the state of\nthe art and prove the success of fake detectors based on physiological\nmeasurement to detect the latest DeepFake videos.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 13:37:58 GMT"}, {"version": "v2", "created": "Tue, 6 Oct 2020 09:47:24 GMT"}, {"version": "v3", "created": "Mon, 14 Dec 2020 14:34:23 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Hernandez-Ortega", "Javier", ""], ["Tolosana", "Ruben", ""], ["Fierrez", "Julian", ""], ["Morales", "Aythami", ""]]}, {"id": "2010.00450", "submitter": "Mojtaba Bemana", "authors": "Mojtaba Bemana, Karol Myszkowski, Hans-Peter Seidel, Tobias Ritschel", "title": "X-Fields: Implicit Neural View-, Light- and Time-Image Interpolation", "comments": "15 pages, 19 figures, accepted at SIGGRAPH Asia 2020, project\n  webpage: https://xfields.mpi-inf.mpg.de/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We suggest to represent an X-Field -a set of 2D images taken across different\nview, time or illumination conditions, i.e., video, light field, reflectance\nfields or combinations thereof-by learning a neural network (NN) to map their\nview, time or light coordinates to 2D images. Executing this NN at new\ncoordinates results in joint view, time or light interpolation. The key idea to\nmake this workable is a NN that already knows the \"basic tricks\" of graphics\n(lighting, 3D projection, occlusion) in a hard-coded and differentiable form.\nThe NN represents the input to that rendering as an implicit map, that for any\nview, time, or light coordinate and for any pixel can quantify how it will move\nif view, time or light coordinates change (Jacobian of pixel position with\nrespect to view, time, illumination, etc.). Our X-Field representation is\ntrained for one scene within minutes, leading to a compact set of trainable\nparameters and hence real-time navigation in view, time and illumination.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 14:46:00 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["Bemana", "Mojtaba", ""], ["Myszkowski", "Karol", ""], ["Seidel", "Hans-Peter", ""], ["Ritschel", "Tobias", ""]]}, {"id": "2010.00456", "submitter": "Jiale Cao", "authors": "Jiale Cao, Yanwei Pang, Jin Xie, Fahad Shahbaz Khan, Ling Shao", "title": "From Handcrafted to Deep Features for Pedestrian Detection: A Survey", "comments": "IEEE TPAMI, Projects: https://github.com/JialeCao001/PedSurvey", "journal-ref": "IEEE Transactions on Pattern Analysis and Machine Intelligence,\n  2021", "doi": "10.1109/TPAMI.2021.3076733", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pedestrian detection is an important but challenging problem in computer\nvision, especially in human-centric tasks. Over the past decade, significant\nimprovement has been witnessed with the help of handcrafted features and deep\nfeatures. Here we present a comprehensive survey on recent advances in\npedestrian detection. First, we provide a detailed review of single-spectral\npedestrian detection that includes handcrafted features based methods and deep\nfeatures based approaches. For handcrafted features based methods, we present\nan extensive review of approaches and find that handcrafted features with large\nfreedom degrees in shape and space have better performance. In the case of deep\nfeatures based approaches, we split them into pure CNN based methods and those\nemploying both handcrafted and CNN based features. We give the statistical\nanalysis and tendency of these methods, where feature enhanced, part-aware, and\npost-processing methods have attracted main attention. In addition to\nsingle-spectral pedestrian detection, we also review multi-spectral pedestrian\ndetection, which provides more robust features for illumination variance.\nFurthermore, we introduce some related datasets and evaluation metrics, and\ncompare some representative methods. We conclude this survey by emphasizing\nopen problems that need to be addressed and highlighting various future\ndirections. Researchers can track an up-to-date list at\nhttps://github.com/JialeCao001/PedSurvey.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 14:51:10 GMT"}, {"version": "v2", "created": "Wed, 12 May 2021 03:59:38 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Cao", "Jiale", ""], ["Pang", "Yanwei", ""], ["Xie", "Jin", ""], ["Khan", "Fahad Shahbaz", ""], ["Shao", "Ling", ""]]}, {"id": "2010.00467", "submitter": "Tianyu Pang", "authors": "Tianyu Pang, Xiao Yang, Yinpeng Dong, Hang Su, Jun Zhu", "title": "Bag of Tricks for Adversarial Training", "comments": "ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial training (AT) is one of the most effective strategies for\npromoting model robustness. However, recent benchmarks show that most of the\nproposed improvements on AT are less effective than simply early stopping the\ntraining procedure. This counter-intuitive fact motivates us to investigate the\nimplementation details of tens of AT methods. Surprisingly, we find that the\nbasic settings (e.g., weight decay, training schedule, etc.) used in these\nmethods are highly inconsistent. In this work, we provide comprehensive\nevaluations on CIFAR-10, focusing on the effects of mostly overlooked training\ntricks and hyperparameters for adversarially trained models. Our empirical\nobservations suggest that adversarial robustness is much more sensitive to some\nbasic training settings than we thought. For example, a slightly different\nvalue of weight decay can reduce the model robust accuracy by more than 7%,\nwhich is probable to override the potential promotion induced by the proposed\nmethods. We conclude a baseline training setting and re-implement previous\ndefenses to achieve new state-of-the-art results. These facts also appeal to\nmore concerns on the overlooked confounders when benchmarking defenses.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 15:03:51 GMT"}, {"version": "v2", "created": "Wed, 25 Nov 2020 16:36:29 GMT"}, {"version": "v3", "created": "Wed, 31 Mar 2021 09:34:21 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Pang", "Tianyu", ""], ["Yang", "Xiao", ""], ["Dong", "Yinpeng", ""], ["Su", "Hang", ""], ["Zhu", "Jun", ""]]}, {"id": "2010.00482", "submitter": "Arash Mahyari", "authors": "Arash Mahyari, Peter Pirolli", "title": "Physical Exercise Recommendation and Success Prediction Using\n  Interconnected Recurrent Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.IR cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unhealthy behaviors, e.g., physical inactivity and unhealthful food choice,\nare the primary healthcare cost drivers in developed countries. Pervasive\ncomputational, sensing, and communication technology provided by smartphones\nand smartwatches have made it possible to support individuals in their everyday\nlives to develop healthier lifestyles. In this paper, we propose an exercise\nrecommendation system that also predicts individual success rates. The system,\nconsisting of two inter-connected recurrent neural networks (RNNs), uses the\nhistory of workouts to recommend the next workout activity for each individual.\nThe system then predicts the probability of successful completion of the\npredicted activity by the individual. The prediction accuracy of this\ninterconnected-RNN model is assessed on previously published data from a\nfour-week mobile health experiment and is shown to improve upon previous\npredictions from a computational cognitive model.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 15:22:59 GMT"}, {"version": "v2", "created": "Wed, 27 Jan 2021 20:04:20 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Mahyari", "Arash", ""], ["Pirolli", "Peter", ""]]}, {"id": "2010.00494", "submitter": "Abbas Cheddad", "authors": "Charitha Dissanayake Lekamlage, Fabia Afzal, Erik Westerberg, Abbas\n  Cheddad", "title": "Mini-DDSM: Mammography-based Automatic Age Estimation", "comments": "C.D. Lekamlage, F. Afzal, E. Westerberg and A. Cheddad, \"Mini-DDSM:\n  Mammography-based Automatic Age Estimation,\" in the 3rd International\n  Conference on Digital Medicine and Image Processing (DMIP 2020), ACM, Kyoto,\n  Japan, November 06-09, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Age estimation has attracted attention for its various medical applications.\nThere are many studies on human age estimation from biomedical images. However,\nthere is no research done on mammograms for age estimation, as far as we know.\nThe purpose of this study is to devise an AI-based model for estimating age\nfrom mammogram images. Due to lack of public mammography data sets that have\nthe age attribute, we resort to using a web crawler to download thumbnail\nmammographic images and their age fields from the public data set; the Digital\nDatabase for Screening Mammography. The original images in this data set\nunfortunately can only be retrieved by a software which is broken.\nSubsequently, we extracted deep learning features from the collected data set,\nby which we built a model using Random Forests regressor to estimate the age\nautomatically. The performance assessment was measured using the mean absolute\nerror values. The average error value out of 10 tests on random selection of\nsamples was around 8 years. In this paper, we show the merits of this approach\nto fill up missing age values. We ran logistic and linear regression models on\nanother independent data set to further validate the advantage of our proposed\nwork. This paper also introduces the free-access Mini-DDSM data set.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 15:35:11 GMT"}, {"version": "v2", "created": "Thu, 19 Nov 2020 12:45:23 GMT"}, {"version": "v3", "created": "Tue, 24 Nov 2020 10:13:41 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Lekamlage", "Charitha Dissanayake", ""], ["Afzal", "Fabia", ""], ["Westerberg", "Erik", ""], ["Cheddad", "Abbas", ""]]}, {"id": "2010.00500", "submitter": "Justyna P. Zwolak", "authors": "Justyna P. Zwolak, Sandesh S. Kalantre, Thomas McJunkin, Brian J.\n  Weber, Jacob M. Taylor", "title": "Ray-based classification framework for high-dimensional data", "comments": null, "journal-ref": "Proceedings of the Machine Learning and the Physical Sciences\n  Workshop at NeurIPS 2020, Vancouver, Canada", "doi": null, "report-no": null, "categories": "cs.LG cond-mat.mes-hall cs.CV quant-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While classification of arbitrary structures in high dimensions may require\ncomplete quantitative information, for simple geometrical structures,\nlow-dimensional qualitative information about the boundaries defining the\nstructures can suffice. Rather than using dense, multi-dimensional data, we\npropose a deep neural network (DNN) classification framework that utilizes a\nminimal collection of one-dimensional representations, called \\emph{rays}, to\nconstruct the \"fingerprint\" of the structure(s) based on substantially reduced\ninformation. We empirically study this framework using a synthetic dataset of\ndouble and triple quantum dot devices and apply it to the classification\nproblem of identifying the device state. We show that the performance of the\nray-based classifier is already on par with traditional 2D images for low\ndimensional systems, while significantly cutting down the data acquisition\ncost.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 15:46:29 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Zwolak", "Justyna P.", ""], ["Kalantre", "Sandesh S.", ""], ["McJunkin", "Thomas", ""], ["Weber", "Brian J.", ""], ["Taylor", "Jacob M.", ""]]}, {"id": "2010.00505", "submitter": "Yingnan Ju", "authors": "Yingnan Ju, Yue Chen", "title": "An Ultra Lightweight CNN for Low Resource Circuit Component Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present an ultra lightweight system that can effectively\nrecognize different circuit components in an image with very limited training\ndata. Along with the system, we also release the data set we created for the\ntask. A two-stage approach is employed by our system. Selective search was\napplied to find the location of each circuit component. Based on its result, we\ncrop the original image into smaller pieces. The pieces are then fed to the\nConvolutional Neural Network (CNN) for classification to identify each circuit\ncomponent. It is of engineering significance and works well in circuit\ncomponent recognition in a low resource setting. The accuracy of our system\nreaches 93.4\\%, outperforming the support vector machine (SVM) baseline\n(75.00%) and the existing state-of-the-art RetinaNet solutions (92.80%).\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 15:54:08 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["Ju", "Yingnan", ""], ["Chen", "Yue", ""]]}, {"id": "2010.00511", "submitter": "Ardhendu Shekhar Tripathi", "authors": "Ardhendu Shekhar Tripathi, Martin Danelljan, Luc Van Gool, Radu\n  Timofte", "title": "Few-Shot Classification By Few-Iteration Meta-Learning", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning in a low-data regime from only a few labeled examples is an\nimportant, but challenging problem. Recent advancements within meta-learning\nhave demonstrated encouraging performance, in particular, for the task of\nfew-shot classification. We propose a novel optimization-based meta-learning\napproach for few-shot classification. It consists of an embedding network,\nproviding a general representation of the image, and a base learner module. The\nlatter learns a linear classifier during the inference through an unrolled\noptimization procedure. We design an inner learning objective composed of (i) a\nrobust classification loss on the support set and (ii) an entropy loss,\nallowing transductive learning from unlabeled query samples. By employing an\nefficient initialization module and a Steepest Descent based optimization\nalgorithm, our base learner predicts a powerful classifier within only a few\niterations. Further, our strategy enables important aspects of the base learner\nobjective to be learned during meta-training. To the best of our knowledge,\nthis work is the first to integrate both induction and transduction into the\nbase learner in an optimization-based meta-learning framework. We perform a\ncomprehensive experimental analysis, demonstrating the effectiveness of our\napproach on four few-shot classification datasets.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 15:59:31 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["Tripathi", "Ardhendu Shekhar", ""], ["Danelljan", "Martin", ""], ["Van Gool", "Luc", ""], ["Timofte", "Radu", ""]]}, {"id": "2010.00514", "submitter": "Shaofei Huang", "authors": "Shaofei Huang, Tianrui Hui, Si Liu, Guanbin Li, Yunchao Wei, Jizhong\n  Han, Luoqi Liu, Bo Li", "title": "Referring Image Segmentation via Cross-Modal Progressive Comprehension", "comments": "Accepted by CVPR 2020. Code is available at\n  https://github.com/spyflying/CMPC-Refseg", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Referring image segmentation aims at segmenting the foreground masks of the\nentities that can well match the description given in the natural language\nexpression. Previous approaches tackle this problem using implicit feature\ninteraction and fusion between visual and linguistic modalities, but usually\nfail to explore informative words of the expression to well align features from\nthe two modalities for accurately identifying the referred entity. In this\npaper, we propose a Cross-Modal Progressive Comprehension (CMPC) module and a\nText-Guided Feature Exchange (TGFE) module to effectively address the\nchallenging task. Concretely, the CMPC module first employs entity and\nattribute words to perceive all the related entities that might be considered\nby the expression. Then, the relational words are adopted to highlight the\ncorrect entity as well as suppress other irrelevant ones by multimodal graph\nreasoning. In addition to the CMPC module, we further leverage a simple yet\neffective TGFE module to integrate the reasoned multimodal features from\ndifferent levels with the guidance of textual information. In this way,\nfeatures from multi-levels could communicate with each other and be refined\nbased on the textual context. We conduct extensive experiments on four popular\nreferring segmentation benchmarks and achieve new state-of-the-art\nperformances.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 16:02:30 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["Huang", "Shaofei", ""], ["Hui", "Tianrui", ""], ["Liu", "Si", ""], ["Li", "Guanbin", ""], ["Wei", "Yunchao", ""], ["Han", "Jizhong", ""], ["Liu", "Luoqi", ""], ["Li", "Bo", ""]]}, {"id": "2010.00515", "submitter": "Shaofei Huang", "authors": "Tianrui Hui, Si Liu, Shaofei Huang, Guanbin Li, Sansi Yu, Faxi Zhang,\n  Jizhong Han", "title": "Linguistic Structure Guided Context Modeling for Referring Image\n  Segmentation", "comments": "Accepted by ECCV 2020. Code is available at\n  https://github.com/spyflying/LSCM-Refseg", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Referring image segmentation aims to predict the foreground mask of the\nobject referred by a natural language sentence. Multimodal context of the\nsentence is crucial to distinguish the referent from the background. Existing\nmethods either insufficiently or redundantly model the multimodal context. To\ntackle this problem, we propose a \"gather-propagate-distribute\" scheme to model\nmultimodal context by cross-modal interaction and implement this scheme as a\nnovel Linguistic Structure guided Context Modeling (LSCM) module. Our LSCM\nmodule builds a Dependency Parsing Tree suppressed Word Graph (DPT-WG) which\nguides all the words to include valid multimodal context of the sentence while\nexcluding disturbing ones through three steps over the multimodal feature,\ni.e., gathering, constrained propagation and distributing. Extensive\nexperiments on four benchmarks demonstrate that our method outperforms all the\nprevious state-of-the-arts.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 16:03:51 GMT"}, {"version": "v2", "created": "Fri, 2 Oct 2020 03:19:48 GMT"}, {"version": "v3", "created": "Mon, 5 Oct 2020 08:49:43 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Hui", "Tianrui", ""], ["Liu", "Si", ""], ["Huang", "Shaofei", ""], ["Li", "Guanbin", ""], ["Yu", "Sansi", ""], ["Zhang", "Faxi", ""], ["Han", "Jizhong", ""]]}, {"id": "2010.00516", "submitter": "Meenakshi Khosla", "authors": "Meenakshi Khosla, Gia H. Ngo, Keith Jamison, Amy Kuceyeski and Mert R.\n  Sabuncu", "title": "Neural encoding with visual attention", "comments": "NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual perception is critically influenced by the focus of attention. Due to\nlimited resources, it is well known that neural representations are biased in\nfavor of attended locations. Using concurrent eye-tracking and functional\nMagnetic Resonance Imaging (fMRI) recordings from a large cohort of human\nsubjects watching movies, we first demonstrate that leveraging gaze\ninformation, in the form of attentional masking, can significantly improve\nbrain response prediction accuracy in a neural encoding model. Next, we propose\na novel approach to neural encoding by including a trainable soft-attention\nmodule. Using our new approach, we demonstrate that it is possible to learn\nvisual attention policies by end-to-end learning merely on fMRI response data,\nand without relying on any eye-tracking. Interestingly, we find that attention\nlocations estimated by the model on independent data agree well with the\ncorresponding eye fixation patterns, despite no explicit supervision to do so.\nTogether, these findings suggest that attention modules can be instrumental in\nneural encoding models of visual stimuli.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 16:04:21 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["Khosla", "Meenakshi", ""], ["Ngo", "Gia H.", ""], ["Jamison", "Keith", ""], ["Kuceyeski", "Amy", ""], ["Sabuncu", "Mert R.", ""]]}, {"id": "2010.00521", "submitter": "Litu Rout", "authors": "Litu Rout", "title": "Why Adversarial Interaction Creates Non-Homogeneous Patterns: A\n  Pseudo-Reaction-Diffusion Model for Turing Instability", "comments": "35th AAAI Conference on Artificial Intelligence", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Long after Turing's seminal Reaction-Diffusion (RD) model, the elegance of\nhis fundamental equations alleviated much of the skepticism surrounding pattern\nformation. Though Turing model is a simplification and an idealization, it is\none of the best-known theoretical models to explain patterns as a reminiscent\nof those observed in nature. Over the years, concerted efforts have been made\nto align theoretical models to explain patterns in real systems. The apparent\ndifficulty in identifying the specific dynamics of the RD system makes the\nproblem particularly challenging. Interestingly, we observe Turing-like\npatterns in a system of neurons with adversarial interaction. In this study, we\nestablish the involvement of Turing instability to create such patterns. By\ntheoretical and empirical studies, we present a pseudo-reaction-diffusion model\nto explain the mechanism that may underlie these phenomena. While supervised\nlearning attains homogeneous equilibrium, this paper suggests that the\nintroduction of an adversary helps break this homogeneity to create\nnon-homogeneous patterns at equilibrium. Further, we prove that randomly\ninitialized gradient descent with over-parameterization can converge\nexponentially fast to an $\\epsilon$-stationary point even under adversarial\ninteraction. In addition, different from sole supervision, we show that the\nsolutions obtained under adversarial interaction are not limited to a tiny\nsubspace around initialization.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 16:09:22 GMT"}, {"version": "v2", "created": "Tue, 8 Dec 2020 10:29:39 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Rout", "Litu", ""]]}, {"id": "2010.00522", "submitter": "Litu Rout", "authors": "Litu Rout", "title": "Understanding the Role of Adversarial Regularization in Supervised\n  Learning", "comments": "Under Review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite numerous attempts sought to provide empirical evidence of adversarial\nregularization outperforming sole supervision, the theoretical understanding of\nsuch phenomena remains elusive. In this study, we aim to resolve whether\nadversarial regularization indeed performs better than sole supervision at a\nfundamental level. To bring this insight into fruition, we study vanishing\ngradient issue, asymptotic iteration complexity, gradient flow and provable\nconvergence in the context of sole supervision and adversarial regularization.\nThe key ingredient is a theoretical justification supported by empirical\nevidence of adversarial acceleration in gradient descent. In addition,\nmotivated by a recently introduced unit-wise capacity based generalization\nbound, we analyze the generalization error in adversarial framework. Guided by\nour observation, we cast doubts on the ability of this measure to explain\ngeneralization. We therefore leave as open questions to explore new measures\nthat can explain generalization behavior in adversarial learning. Furthermore,\nwe observe an intriguing phenomenon in the neural embedded vector space while\ncontrasting adversarial learning with sole supervision.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 16:10:05 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["Rout", "Litu", ""]]}, {"id": "2010.00536", "submitter": "Xing Liang", "authors": "Xing Liang, Anastassia Angelopoulou, Epaminondas Kapetanios, Bencie\n  Woll, Reda Al-batat, Tyron Woolfe", "title": "A Multi-modal Machine Learning Approach and Toolkit to Automate\n  Recognition of Early Stages of Dementia among British Sign Language Users", "comments": null, "journal-ref": "ECCV 2020 Workshops. Lecture Notes in Computer Science, Vol 12536.\n  Springer, Cham", "doi": "10.1007/978-3-030-66096-3_20", "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ageing population trend is correlated with an increased prevalence of\nacquired cognitive impairments such as dementia. Although there is no cure for\ndementia, a timely diagnosis helps in obtaining necessary support and\nappropriate medication. Researchers are working urgently to develop effective\ntechnological tools that can help doctors undertake early identification of\ncognitive disorder. In particular, screening for dementia in ageing Deaf\nsigners of British Sign Language (BSL) poses additional challenges as the\ndiagnostic process is bound up with conditions such as quality and availability\nof interpreters, as well as appropriate questionnaires and cognitive tests. On\nthe other hand, deep learning based approaches for image and video analysis and\nunderstanding are promising, particularly the adoption of Convolutional Neural\nNetwork (CNN), which require large amounts of training data. In this paper,\nhowever, we demonstrate novelty in the following way: a) a multi-modal machine\nlearning based automatic recognition toolkit for early stages of dementia among\nBSL users in that features from several parts of the body contributing to the\nsign envelope, e.g., hand-arm movements and facial expressions, are combined,\nb) universality in that it is possible to apply our technique to users of any\nsign language, since it is language independent, c) given the trade-off between\ncomplexity and accuracy of machine learning (ML) prediction models as well as\nthe limited amount of training and testing data being available, we show that\nour approach is not over-fitted and has the potential to scale up.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 16:35:48 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Liang", "Xing", ""], ["Angelopoulou", "Anastassia", ""], ["Kapetanios", "Epaminondas", ""], ["Woll", "Bencie", ""], ["Al-batat", "Reda", ""], ["Woolfe", "Tyron", ""]]}, {"id": "2010.00560", "submitter": "Zhengfei Kuang", "authors": "Jiaman Li, Zhengfei Kuang, Yajie Zhao, Mingming He, Karl Bladin and\n  Hao Li", "title": "Dynamic Facial Asset and Rig Generation from a Single Scan", "comments": "18 pages, 25 figures, ACM SIGGRAPH Asia 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The creation of high-fidelity computer-generated (CG) characters used in film\nand gaming requires intensive manual labor and a comprehensive set of facial\nassets to be captured with complex hardware, resulting in high cost and long\nproduction cycles. In order to simplify and accelerate this digitization\nprocess, we propose a framework for the automatic generation of high-quality\ndynamic facial assets, including rigs which can be readily deployed for artists\nto polish. Our framework takes a single scan as input to generate a set of\npersonalized blendshapes, dynamic and physically-based textures, as well as\nsecondary facial components (e.g., teeth and eyeballs). Built upon a facial\ndatabase consisting of pore-level details, with over $4,000$ scans of varying\nexpressions and identities, we adopt a self-supervised neural network to learn\npersonalized blendshapes from a set of template expressions. We also model the\njoint distribution between identities and expressions, enabling the inference\nof the full set of personalized blendshapes with dynamic appearances from a\nsingle neutral input scan. Our generated personalized face rig assets are\nseamlessly compatible with cutting-edge industry pipelines for facial animation\nand rendering. We demonstrate that our framework is robust and effective by\ninferring on a wide range of novel subjects, and illustrate compelling\nrendering results while animating faces with generated customized\nphysically-based dynamic textures.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 17:25:25 GMT"}, {"version": "v2", "created": "Mon, 5 Oct 2020 20:12:53 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Li", "Jiaman", ""], ["Kuang", "Zhengfei", ""], ["Zhao", "Yajie", ""], ["He", "Mingming", ""], ["Bladin", "Karl", ""], ["Li", "Hao", ""]]}, {"id": "2010.00562", "submitter": "Jose Manuel Gomez-Perez", "authors": "Jose Manuel Gomez-Perez, Raul Ortega", "title": "ISAAQ -- Mastering Textbook Questions with Pre-trained Transformers and\n  Bottom-Up and Top-Down Attention", "comments": "Accepted for publication as a long paper in EMNLP2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Textbook Question Answering is a complex task in the intersection of Machine\nComprehension and Visual Question Answering that requires reasoning with\nmultimodal information from text and diagrams. For the first time, this paper\ntaps on the potential of transformer language models and bottom-up and top-down\nattention to tackle the language and visual understanding challenges this task\nentails. Rather than training a language-visual transformer from scratch we\nrely on pre-trained transformers, fine-tuning and ensembling. We add bottom-up\nand top-down attention to identify regions of interest corresponding to diagram\nconstituents and their relationships, improving the selection of relevant\nvisual information for each question and answer options. Our system ISAAQ\nreports unprecedented success in all TQA question types, with accuracies of\n81.36%, 71.11% and 55.12% on true/false, text-only and diagram multiple choice\nquestions. ISAAQ also demonstrates its broad applicability, obtaining\nstate-of-the-art results in other demanding datasets.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 17:28:47 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["Gomez-Perez", "Jose Manuel", ""], ["Ortega", "Raul", ""]]}, {"id": "2010.00573", "submitter": "Hanjiang Hu", "authors": "Hanjiang Hu, Zhijian Qiao, Ming Cheng, Zhe Liu and Hesheng Wang", "title": "DASGIL: Domain Adaptation for Semantic and Geometric-aware Image-based\n  Localization", "comments": "Submitted to TIP", "journal-ref": null, "doi": "10.1109/TIP.2020.3043875", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Long-Term visual localization under changing environments is a challenging\nproblem in autonomous driving and mobile robotics due to season, illumination\nvariance, etc. Image retrieval for localization is an efficient and effective\nsolution to the problem. In this paper, we propose a novel multi-task\narchitecture to fuse the geometric and semantic information into the\nmulti-scale latent embedding representation for visual place recognition. To\nuse the high-quality ground truths without any human effort, the effective\nmulti-scale feature discriminator is proposed for adversarial training to\nachieve the domain adaptation from synthetic virtual KITTI dataset to\nreal-world KITTI dataset. The proposed approach is validated on the Extended\nCMU-Seasons dataset and Oxford RobotCar dataset through a series of crucial\ncomparison experiments, where our performance outperforms state-of-the-art\nbaselines for retrieval-based localization and large-scale place recognition\nunder the challenging environment.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 17:44:25 GMT"}, {"version": "v2", "created": "Wed, 25 Nov 2020 15:47:17 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Hu", "Hanjiang", ""], ["Qiao", "Zhijian", ""], ["Cheng", "Ming", ""], ["Liu", "Zhe", ""], ["Wang", "Hesheng", ""]]}, {"id": "2010.00583", "submitter": "Abdullah Sarhan", "authors": "Abdullah Sarhan, Ali Al-Khaz\\'Aly, Adam Gorner, Andrew Swift, Jon\n  Rokne, Reda Alhajj, and Andrew Crichton", "title": "Utilizing Transfer Learning and a Customized Loss Function for Optic\n  Disc Segmentation from Retinal Images", "comments": "Accepted by ACCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate segmentation of the optic disc from a retinal image is vital to\nextracting retinal features that may be highly correlated with retinal\nconditions such as glaucoma. In this paper, we propose a deep-learning based\napproach capable of segmenting the optic disc given a high-precision retinal\nfundus image. Our approach utilizes a UNET-based model with a VGG16 encoder\ntrained on the ImageNet dataset. This study can be distinguished from other\nstudies in the customization made for the VGG16 model, the diversity of the\ndatasets adopted, the duration of disc segmentation, the loss function\nutilized, and the number of parameters required to train our model. Our\napproach was tested on seven publicly available datasets augmented by a dataset\nfrom a private clinic that was annotated by two Doctors of Optometry through a\nweb portal built for this purpose. We achieved an accuracy of 99.78\\% and a\nDice coefficient of 94.73\\% for a disc segmentation from a retinal image in\n0.03 seconds. The results obtained from comprehensive experiments demonstrate\nthe robustness of our approach to disc segmentation of retinal images obtained\nfrom different sources.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 17:55:15 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["Sarhan", "Abdullah", ""], ["Al-Khaz\u00c1ly", "Ali", ""], ["Gorner", "Adam", ""], ["Swift", "Andrew", ""], ["Rokne", "Jon", ""], ["Alhajj", "Reda", ""], ["Crichton", "Andrew", ""]]}, {"id": "2010.00635", "submitter": "Wenlong Wu", "authors": "Wenlong Wu, James M. Keller, Jeffrey Dale, James C. Bezdek", "title": "StreamSoNG: A Soft Streaming Classification Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Examining most streaming clustering algorithms leads to the understanding\nthat they are actually incremental classification models. They model existing\nand newly discovered structures via summary information that we call\nfootprints. Incoming data is normally assigned a crisp label (into one of the\nstructures) and that structure's footprint is incrementally updated. There is\nno reason that these assignments need to be crisp. In this paper, we propose a\nnew streaming classification algorithm that uses Neural Gas prototypes as\nfootprints and produces a possibilistic label vector (of typicalities) for each\nincoming vector. These typicalities are generated by a modified possibilistic\nk-nearest neighbor algorithm. The approach is tested on synthetic and real\nimage datasets. We compare our approach to three other streaming classifiers\nbased on the Adaptive Random Forest, Very Fast Decision Rules, and the\nDenStream algorithm with excellent results.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 18:22:04 GMT"}, {"version": "v2", "created": "Tue, 13 Jul 2021 16:52:18 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Wu", "Wenlong", ""], ["Keller", "James M.", ""], ["Dale", "Jeffrey", ""], ["Bezdek", "James C.", ""]]}, {"id": "2010.00638", "submitter": "Insaf Ashrapov", "authors": "Insaf Ashrapov", "title": "Tabular GANs for uneven distribution", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  GANs are well known for success in the realistic image generation. However,\nthey can be applied in tabular data generation as well. We will review and\nexamine some recent papers about tabular GANs in action. We will generate data\nto make train distribution bring closer to the test. Then compare model\nperformance trained on the initial train dataset, with trained on the train\nwith GAN generated data, also we train the model by sampling train by\nadversarial training. We show that using GAN might be an option in case of\nuneven data distribution between train and test data.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 18:39:32 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Ashrapov", "Insaf", ""]]}, {"id": "2010.00641", "submitter": "Corey Toler-Franklin", "authors": "Qingchao Zhang, Coy D. Heldermon, Corey Toler-Franklin", "title": "Multiscale Detection of Cancerous Tissue in High Resolution Slide Scans", "comments": "14 pages, 7 figures, 2 tables", "journal-ref": "Advances in Visual Computing. ISVC 2020. Lecture Notes in Computer\n  Science", "doi": "10.1007/978-3-030-64559-5_11", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an algorithm for multi-scale tumor (chimeric cell) detection in\nhigh resolution slide scans. The broad range of tumor sizes in our dataset pose\na challenge for current Convolutional Neural Networks (CNN) which often fail\nwhen image features are very small (8 pixels). Our approach modifies the\neffective receptive field at different layers in a CNN so that objects with a\nbroad range of varying scales can be detected in a single forward pass. We\ndefine rules for computing adaptive prior anchor boxes which we show are\nsolvable under the equal proportion interval principle. Two mechanisms in our\nCNN architecture alleviate the effects of non-discriminative features prevalent\nin our data - a foveal detection algorithm that incorporates a cascade\nresidual-inception module and a deconvolution module with additional context\ninformation. When integrated into a Single Shot MultiBox Detector (SSD), these\nadditions permit more accurate detection of small-scale objects. The results\npermit efficient real-time analysis of medical images in pathology and related\nbiomedical research fields.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 18:56:46 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Zhang", "Qingchao", ""], ["Heldermon", "Coy D.", ""], ["Toler-Franklin", "Corey", ""]]}, {"id": "2010.00654", "submitter": "Zhisheng Xiao", "authors": "Zhisheng Xiao, Karsten Kreis, Jan Kautz, Arash Vahdat", "title": "VAEBM: A Symbiosis between Variational Autoencoders and Energy-based\n  Models", "comments": "ICLR 2021 (spotlight)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Energy-based models (EBMs) have recently been successful in representing\ncomplex distributions of small images. However, sampling from them requires\nexpensive Markov chain Monte Carlo (MCMC) iterations that mix slowly in high\ndimensional pixel space. Unlike EBMs, variational autoencoders (VAEs) generate\nsamples quickly and are equipped with a latent space that enables fast\ntraversal of the data manifold. However, VAEs tend to assign high probability\ndensity to regions in data space outside the actual data distribution and often\nfail at generating sharp images. In this paper, we propose VAEBM, a symbiotic\ncomposition of a VAE and an EBM that offers the best of both worlds. VAEBM\ncaptures the overall mode structure of the data distribution using a\nstate-of-the-art VAE and it relies on its EBM component to explicitly exclude\nnon-data-like regions from the model and refine the image samples. Moreover,\nthe VAE component in VAEBM allows us to speed up MCMC updates by\nreparameterizing them in the VAE's latent space. Our experimental results show\nthat VAEBM outperforms state-of-the-art VAEs and EBMs in generative quality on\nseveral benchmark image datasets by a large margin. It can generate\nhigh-quality images as large as 256$\\times$256 pixels with short MCMC chains.\nWe also demonstrate that VAEBM provides complete mode coverage and performs\nwell in out-of-distribution detection.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 19:28:28 GMT"}, {"version": "v2", "created": "Tue, 9 Feb 2021 16:41:47 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Xiao", "Zhisheng", ""], ["Kreis", "Karsten", ""], ["Kautz", "Jan", ""], ["Vahdat", "Arash", ""]]}, {"id": "2010.00672", "submitter": "Sam Sattarzadeh", "authors": "Sam Sattarzadeh, Mahesh Sudhakar, Anthony Lem, Shervin Mehryar, K. N.\n  Plataniotis, Jongseong Jang, Hyunwoo Kim, Yeonjeong Jeong, Sangmin Lee,\n  Kyunghoon Bae", "title": "Explaining Convolutional Neural Networks through Attribution-Based Input\n  Sampling and Block-Wise Feature Aggregation", "comments": "9 pages, 9 figures, Accepted at the Thirty-Fifth AAAI Conference on\n  Artificial Intelligence (AAAI-21)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As an emerging field in Machine Learning, Explainable AI (XAI) has been\noffering remarkable performance in interpreting the decisions made by\nConvolutional Neural Networks (CNNs). To achieve visual explanations for CNNs,\nmethods based on class activation mapping and randomized input sampling have\ngained great popularity. However, the attribution methods based on these\ntechniques provide lower resolution and blurry explanation maps that limit\ntheir explanation power. To circumvent this issue, visualization based on\nvarious layers is sought. In this work, we collect visualization maps from\nmultiple layers of the model based on an attribution-based input sampling\ntechnique and aggregate them to reach a fine-grained and complete explanation.\nWe also propose a layer selection strategy that applies to the whole family of\nCNN-based models, based on which our extraction framework is applied to\nvisualize the last layers of each convolutional block of the model. Moreover,\nwe perform an empirical analysis of the efficacy of derived lower-level\ninformation to enhance the represented attributions. Comprehensive experiments\nconducted on shallow and deep models trained on natural and industrial\ndatasets, using both ground-truth and model-truth based evaluation metrics\nvalidate our proposed algorithm by meeting or outperforming the\nstate-of-the-art methods in terms of explanation ability and visual quality,\ndemonstrating that our method shows stability regardless of the size of objects\nor instances to be explained.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 20:27:30 GMT"}, {"version": "v2", "created": "Thu, 24 Dec 2020 21:33:05 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Sattarzadeh", "Sam", ""], ["Sudhakar", "Mahesh", ""], ["Lem", "Anthony", ""], ["Mehryar", "Shervin", ""], ["Plataniotis", "K. N.", ""], ["Jang", "Jongseong", ""], ["Kim", "Hyunwoo", ""], ["Jeong", "Yeonjeong", ""], ["Lee", "Sangmin", ""], ["Bae", "Kyunghoon", ""]]}, {"id": "2010.00679", "submitter": "Li Jing", "authors": "Li Jing, Jure Zbontar, Yann LeCun", "title": "Implicit Rank-Minimizing Autoencoder", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important component of autoencoders is the method by which the information\ncapacity of the latent representation is minimized or limited. In this work,\nthe rank of the covariance matrix of the codes is implicitly minimized by\nrelying on the fact that gradient descent learning in multi-layer linear\nnetworks leads to minimum-rank solutions. By inserting a number of extra linear\nlayers between the encoder and the decoder, the system spontaneously learns\nrepresentations with a low effective dimension. The model, dubbed Implicit\nRank-Minimizing Autoencoder (IRMAE), is simple, deterministic, and learns\ncompact latent spaces. We demonstrate the validity of the method on several\nimage generation and representation learning tasks.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 20:48:52 GMT"}, {"version": "v2", "created": "Wed, 14 Oct 2020 15:36:27 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Jing", "Li", ""], ["Zbontar", "Jure", ""], ["LeCun", "Yann", ""]]}, {"id": "2010.00694", "submitter": "Razvan Caramalau", "authors": "Razvan Caramalau, Binod Bhattarai, Tae-Kyun Kim", "title": "Active Learning for Bayesian 3D Hand Pose Estimation", "comments": "Accepted at WACV 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a Bayesian approximation to a deep learning architecture for 3D\nhand pose estimation. Through this framework, we explore and analyse the two\ntypes of uncertainties that are influenced either by data or by the learning\ncapability. Furthermore, we draw comparisons against the standard estimator\nover three popular benchmarks. The first contribution lies in outperforming the\nbaseline while in the second part we address the active learning application.\nWe also show that with a newly proposed acquisition function, our Bayesian 3D\nhand pose estimator obtains lowest errors with the least amount of data. The\nunderlying code is publicly available at\nhttps://github.com/razvancaramalau/al_bhpe.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 21:36:26 GMT"}, {"version": "v2", "created": "Sun, 21 Feb 2021 04:56:35 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Caramalau", "Razvan", ""], ["Bhattarai", "Binod", ""], ["Kim", "Tae-Kyun", ""]]}, {"id": "2010.00702", "submitter": "Simon Niklaus", "authors": "Simon Niklaus and Xuaner Cecilia Zhang and Jonathan T. Barron and Neal\n  Wadhwa and Rahul Garg and Feng Liu and Tianfan Xue", "title": "Learned Dual-View Reflection Removal", "comments": "http://sniklaus.com/dualref", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional reflection removal algorithms either use a single image as input,\nwhich suffers from intrinsic ambiguities, or use multiple images from a moving\ncamera, which is inconvenient for users. We instead propose a learning-based\ndereflection algorithm that uses stereo images as input. This is an effective\ntrade-off between the two extremes: the parallax between two views provides\ncues to remove reflections, and two views are easy to capture due to the\nadoption of stereo cameras in smartphones. Our model consists of a\nlearning-based reflection-invariant flow model for dual-view registration, and\na learned synthesis model for combining aligned image pairs. Because no dataset\nfor dual-view reflection removal exists, we render a synthetic dataset of\ndual-views with and without reflections for use in training. Our evaluation on\nan additional real-world dataset of stereo pairs shows that our algorithm\noutperforms existing single-image and multi-image dereflection approaches.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 21:59:58 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Niklaus", "Simon", ""], ["Zhang", "Xuaner Cecilia", ""], ["Barron", "Jonathan T.", ""], ["Wadhwa", "Neal", ""], ["Garg", "Rahul", ""], ["Liu", "Feng", ""], ["Xue", "Tianfan", ""]]}, {"id": "2010.00704", "submitter": "Lijun Zhu", "authors": "Arthur J. Redfern and Lijun Zhu and Molly K. Newquist", "title": "BCNN: A Binary CNN with All Matrix Ops Quantized to 1 Bit Precision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a CNN where all CNN style 2D convolution operations that\nlower to matrix matrix multiplication are fully binary. The network is derived\nfrom a common building block structure that is consistent with a constructive\nproof outline showing that binary neural networks are universal function\napproximators. 71.24% top 1 accuracy on the 2012 ImageNet validation set was\nachieved with a 2 step training procedure and implementation strategies\noptimized for binary operands are provided.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 22:04:13 GMT"}, {"version": "v2", "created": "Tue, 6 Oct 2020 19:01:46 GMT"}, {"version": "v3", "created": "Sat, 31 Oct 2020 01:32:03 GMT"}, {"version": "v4", "created": "Fri, 5 Mar 2021 14:46:10 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Redfern", "Arthur J.", ""], ["Zhu", "Lijun", ""], ["Newquist", "Molly K.", ""]]}, {"id": "2010.00716", "submitter": "Bruno Ferrarini", "authors": "Bruno Ferrarini, Michael Milford, Klaus D. McDonald-Maier and Shoaib\n  Ehsan", "title": "Binary Neural Networks for Memory-Efficient and Effective Visual Place\n  Recognition in Changing Environments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual place recognition (VPR) is a robot's ability to determine whether a\nplace was visited before using visual data. While conventional hand-crafted\nmethods for VPR fail under extreme environmental appearance changes, those\nbased on convolutional neural networks (CNNs) achieve state-of-the-art\nperformance but result in model sizes that demand a large amount of memory.\nHence, CNN-based approaches are unsuitable for memory-constrained platforms,\nsuch as small robots and drones. In this paper, we take a multi-step approach\nof decreasing the precision of model parameters, combining it with network\ndepth reduction and fewer neurons in the classifier stage to propose a new\nclass of highly compact models that drastically reduce the memory requirements\nwhile maintaining state-of-the-art VPR performance, and can be tuned to various\nplatforms and application scenarios. To the best of our knowledge, this is the\nfirst attempt to propose binary neural networks for solving the visual place\nrecognition problem effectively under changing conditions and with\nsignificantly reduced memory requirements. Our best-performing binary neural\nnetwork with a minimum number of layers, dubbed FloppyNet, achieves comparable\nVPR performance when considered against its full precision and deeper\ncounterparts while consuming 99% less memory.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 22:59:34 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Ferrarini", "Bruno", ""], ["Milford", "Michael", ""], ["McDonald-Maier", "Klaus D.", ""], ["Ehsan", "Shoaib", ""]]}, {"id": "2010.00717", "submitter": "Yanyu Zhang", "authors": "Yanyu Zhang", "title": "Deep Reinforcement Learning with Mixed Convolutional Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent research has shown that map raw pixels from a single front-facing\ncamera directly to steering commands are surprisingly powerful. This paper\npresents a convolutional neural network (CNN) to playing the CarRacing-v0 using\nimitation learning in OpenAI Gym. The dataset is generated by playing the game\nmanually in Gym and used a data augmentation method to expand the dataset to 4\ntimes larger than before. Also, we read the true speed, four ABS sensors,\nsteering wheel position, and gyroscope for each image and designed a mixed\nmodel by combining the sensor input and image input. After training, this model\ncan automatically detect the boundaries of road features and drive the robot\nlike a human. By comparing with AlexNet and VGG16 using the average reward in\nCarRacing-v0, our model wins the maximum overall system performance.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 23:02:59 GMT"}, {"version": "v2", "created": "Tue, 6 Oct 2020 04:35:44 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Zhang", "Yanyu", ""]]}, {"id": "2010.00721", "submitter": "Spiridon Kasapis", "authors": "Spiridon Kasapis, Geng Zhang, Jonathon Smereka and Nickolas\n  Vlahopoulos", "title": "Using ROC and Unlabeled Data for Increasing Low-Shot Transfer Learning\n  Classification Accuracy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the most important characteristics of human visual intelligence is the\nability to identify unknown objects. The capability to distinguish between a\nsubstance which a human mind has no previous experience of and a familiar\nobject, is innate to every human. In everyday life, within seconds of seeing an\n\"unknown\" object, we are able to categorize it as such without any substantial\neffort. Convolutional Neural Networks, regardless of how they are trained (i.e.\nin a conventional manner or through transfer learning) can recognize only the\nclasses that they are trained for. When using them for classification, any\ncandidate image will be placed in one of the available classes. We propose a\nlow-shot classifier which can serve as the top layer to any existing CNN that\nthe feature extractor was already trained. Using a limited amount of labeled\ndata for the type of images which need to be specifically classified along with\nunlabeled data for all other images, a unique target matrix and a Receiver\nOperator Curve (ROC) criterion, we are able to increase identification accuracy\nby up to 30% for the images that do not belong to any specific classes, while\nretaining the ability to identify images that belong to the specific classes of\ninterest.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 23:11:07 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Kasapis", "Spiridon", ""], ["Zhang", "Geng", ""], ["Smereka", "Jonathon", ""], ["Vlahopoulos", "Nickolas", ""]]}, {"id": "2010.00731", "submitter": "Meet Pragnesh Shah", "authors": "Meet Shah, Zhiling Huang, Ankit Laddha, Matthew Langford, Blake\n  Barber, Sidney Zhang, Carlos Vallespi-Gonzalez, Raquel Urtasun", "title": "LiRaNet: End-to-End Trajectory Prediction using Spatio-Temporal Radar\n  Fusion", "comments": "Accepted to Conference on Robot Learning (CoRL) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we present LiRaNet, a novel end-to-end trajectory prediction\nmethod which utilizes radar sensor information along with widely used lidar and\nhigh definition (HD) maps. Automotive radar provides rich, complementary\ninformation, allowing for longer range vehicle detection as well as\ninstantaneous radial velocity measurements. However, there are factors that\nmake the fusion of lidar and radar information challenging, such as the\nrelatively low angular resolution of radar measurements, their sparsity and the\nlack of exact time synchronization with lidar. To overcome these challenges, we\npropose an efficient spatio-temporal radar feature extraction scheme which\nachieves state-of-the-art performance on multiple large-scale datasets.Further,\nby incorporating radar information, we show a 52% reduction in prediction error\nfor objects with high acceleration and a 16% reduction in prediction error for\nobjects at longer range.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 00:13:00 GMT"}, {"version": "v2", "created": "Thu, 15 Oct 2020 16:53:14 GMT"}, {"version": "v3", "created": "Thu, 12 Nov 2020 22:29:07 GMT"}], "update_date": "2020-11-16", "authors_parsed": [["Shah", "Meet", ""], ["Huang", "Zhiling", ""], ["Laddha", "Ankit", ""], ["Langford", "Matthew", ""], ["Barber", "Blake", ""], ["Zhang", "Sidney", ""], ["Vallespi-Gonzalez", "Carlos", ""], ["Urtasun", "Raquel", ""]]}, {"id": "2010.00735", "submitter": "Wentao Zhu", "authors": "Yufang Huang, Wentao Zhu, Deyi Xiong, Yiye Zhang, Changjian Hu, Feiyu\n  Xu", "title": "Cycle-Consistent Adversarial Autoencoders for Unsupervised Text Style\n  Transfer", "comments": "COLING 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Unsupervised text style transfer is full of challenges due to the lack of\nparallel data and difficulties in content preservation. In this paper, we\npropose a novel neural approach to unsupervised text style transfer, which we\nrefer to as Cycle-consistent Adversarial autoEncoders (CAE) trained from\nnon-parallel data. CAE consists of three essential components: (1) LSTM\nautoencoders that encode a text in one style into its latent representation and\ndecode an encoded representation into its original text or a transferred\nrepresentation into a style-transferred text, (2) adversarial style transfer\nnetworks that use an adversarially trained generator to transform a latent\nrepresentation in one style into a representation in another style, and (3) a\ncycle-consistent constraint that enhances the capacity of the adversarial style\ntransfer networks in content preservation. The entire CAE with these three\ncomponents can be trained end-to-end. Extensive experiments and in-depth\nanalyses on two widely-used public datasets consistently validate the\neffectiveness of proposed CAE in both style transfer and content preservation\nagainst several strong baselines in terms of four automatic evaluation metrics\nand human evaluation.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 00:43:39 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Huang", "Yufang", ""], ["Zhu", "Wentao", ""], ["Xiong", "Deyi", ""], ["Zhang", "Yiye", ""], ["Hu", "Changjian", ""], ["Xu", "Feiyu", ""]]}, {"id": "2010.00741", "submitter": "M Usman Maqbool Bhutta", "authors": "M Usman Maqbool Bhutta, Shoaib Aslam, Peng Yun, Jianhao Jiao and Ming\n  Liu", "title": "Smart-Inspect: Micro Scale Localization and Classification of Smartphone\n  Glass Defects for Industrial Automation", "comments": "IEEE/RSJ International Conference on Intelligent Robots and Systems\n  (IROS 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The presence of any type of defect on the glass screen of smart devices has a\ngreat impact on their quality. We present a robust semi-supervised learning\nframework for intelligent micro-scaled localization and classification of\ndefects on a 16K pixel image of smartphone glass. Our model features the\nefficient recognition and labeling of three types of defects: scratches, light\nleakage due to cracks, and pits. Our method also differentiates between the\ndefects and light reflections due to dust particles and sensor regions, which\nare classified as non-defect areas. We use a partially labeled dataset to\nachieve high robustness and excellent classification of defect and non-defect\nareas as compared to principal components analysis (PCA), multi-resolution and\ninformation-fusion-based algorithms. In addition, we incorporated two\nclassifiers at different stages of our inspection framework for labeling and\nrefining the unlabeled defects. We successfully enhanced the inspection\ndepth-limit up to 5 microns. The experimental results show that our method\noutperforms manual inspection in testing the quality of glass screen samples by\nidentifying defects on samples that have been marked as good by human\ninspection.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 01:15:00 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Bhutta", "M Usman Maqbool", ""], ["Aslam", "Shoaib", ""], ["Yun", "Peng", ""], ["Jiao", "Jianhao", ""], ["Liu", "Ming", ""]]}, {"id": "2010.00743", "submitter": "Mustafa Hajij", "authors": "Mustafa Hajij, Kyle Istvan, Ghada Zamzmi", "title": "Cell Complex Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CG cs.CV math.AT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cell complexes are topological spaces constructed from simple blocks called\ncells. They generalize graphs, simplicial complexes, and polyhedral complexes\nthat form important domains for practical applications. They also provide a\ncombinatorial formalism that allows the inclusion of complicated relationships\nof restrictive structures such as graphs and meshes. In this paper, we propose\n\\textbf{Cell Complexes Neural Networks (CXNs)}, a general, combinatorial and\nunifying construction for performing neural network-type computations on cell\ncomplexes. We introduce an inter-cellular message passing scheme on cell\ncomplexes that takes the topology of the underlying space into account and\ngeneralizes message passing scheme to graphs. Finally, we introduce a unified\ncell complex encoder-decoder framework that enables learning representation of\ncells for a given complex inside the Euclidean spaces. In particular, we show\nhow our cell complex autoencoder construction can give, in the special case\n\\textbf{cell2vec}, a generalization for node2vec.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 01:38:12 GMT"}, {"version": "v2", "created": "Mon, 12 Oct 2020 06:35:37 GMT"}, {"version": "v3", "created": "Sat, 27 Feb 2021 17:11:18 GMT"}, {"version": "v4", "created": "Tue, 2 Mar 2021 03:50:54 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Hajij", "Mustafa", ""], ["Istvan", "Kyle", ""], ["Zamzmi", "Ghada", ""]]}, {"id": "2010.00747", "submitter": "Yuhao Zhang", "authors": "Yuhao Zhang, Hang Jiang, Yasuhide Miura, Christopher D. Manning,\n  Curtis P. Langlotz", "title": "Contrastive Learning of Medical Visual Representations from Paired\n  Images and Text", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning visual representations of medical images is core to medical image\nunderstanding but its progress has been held back by the small size of\nhand-labeled datasets. Existing work commonly relies on transferring weights\nfrom ImageNet pretraining, which is suboptimal due to drastically different\nimage characteristics, or rule-based label extraction from the textual report\ndata paired with medical images, which is inaccurate and hard to generalize. We\npropose an alternative unsupervised strategy to learn medical visual\nrepresentations directly from the naturally occurring pairing of images and\ntextual data. Our method of pretraining medical image encoders with the paired\ntext data via a bidirectional contrastive objective between the two modalities\nis domain-agnostic, and requires no additional expert input. We test our method\nby transferring our pretrained weights to 4 medical image classification tasks\nand 2 zero-shot retrieval tasks, and show that our method leads to image\nrepresentations that considerably outperform strong baselines in most settings.\nNotably, in all 4 classification tasks, our method requires only 10% as much\nlabeled training data as an ImageNet initialized counterpart to achieve better\nor comparable performance, demonstrating superior data efficiency.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 02:10:18 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Zhang", "Yuhao", ""], ["Jiang", "Hang", ""], ["Miura", "Yasuhide", ""], ["Manning", "Christopher D.", ""], ["Langlotz", "Curtis P.", ""]]}, {"id": "2010.00757", "submitter": "Zhe Jiang", "authors": "Zhe Jiang, Marcus Stephen Kirby, Wenchong He, Arpan Man Sainju", "title": "Deep Learning for Earth Image Segmentation based on Imperfect Polyline\n  Labels with Annotation Errors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, deep learning techniques (e.g., U-Net, DeepLab) have\nachieved tremendous success in image segmentation. The performance of these\nmodels heavily relies on high-quality ground truth segment labels.\nUnfortunately, in many real-world problems, ground truth segment labels often\nhave geometric annotation errors due to manual annotation mistakes, GPS errors,\nor visually interpreting background imagery at a coarse resolution. Such\nlocation errors will significantly impact the training performance of existing\ndeep learning algorithms. Existing research on label errors either models\nground truth errors in label semantics (assuming label locations to be correct)\nor models label location errors with simple square patch shifting. These\nmethods cannot fully incorporate the geometric properties of label location\nerrors. To fill the gap, this paper proposes a generic learning framework based\non the EM algorithm to update deep learning model parameters and infer hidden\ntrue label locations simultaneously. Evaluations on a real-world hydrological\ndataset in the streamline refinement application show that the proposed\nframework outperforms baseline methods in classification accuracy (reducing the\nnumber of false positives by 67% and reducing the number of false negatives by\n55%).\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 02:54:06 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Jiang", "Zhe", ""], ["Kirby", "Marcus Stephen", ""], ["He", "Wenchong", ""], ["Sainju", "Arpan Man", ""]]}, {"id": "2010.00763", "submitter": "Weili Nie", "authors": "Weili Nie, Zhiding Yu, Lei Mao, Ankit B. Patel, Yuke Zhu, Animashree\n  Anandkumar", "title": "Bongard-LOGO: A New Benchmark for Human-Level Concept Learning and\n  Reasoning", "comments": "22 pages, NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Humans have an inherent ability to learn novel concepts from only a few\nsamples and generalize these concepts to different situations. Even though\ntoday's machine learning models excel with a plethora of training data on\nstandard recognition tasks, a considerable gap exists between machine-level\npattern recognition and human-level concept learning. To narrow this gap, the\nBongard problems (BPs) were introduced as an inspirational challenge for visual\ncognition in intelligent systems. Despite new advances in representation\nlearning and learning to learn, BPs remain a daunting challenge for modern AI.\nInspired by the original one hundred BPs, we propose a new benchmark\nBongard-LOGO for human-level concept learning and reasoning. We develop a\nprogram-guided generation technique to produce a large set of\nhuman-interpretable visual cognition problems in action-oriented LOGO language.\nOur benchmark captures three core properties of human cognition: 1)\ncontext-dependent perception, in which the same object may have disparate\ninterpretations given different contexts; 2) analogy-making perception, in\nwhich some meaningful concepts are traded off for other meaningful concepts;\nand 3) perception with a few samples but infinite vocabulary. In experiments,\nwe show that the state-of-the-art deep learning methods perform substantially\nworse than human subjects, implying that they fail to capture core human\ncognition properties. Finally, we discuss research directions towards a general\narchitecture for visual reasoning to tackle this benchmark.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 03:19:46 GMT"}, {"version": "v2", "created": "Sun, 1 Nov 2020 19:28:12 GMT"}, {"version": "v3", "created": "Sat, 21 Nov 2020 22:24:02 GMT"}, {"version": "v4", "created": "Mon, 4 Jan 2021 21:50:06 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Nie", "Weili", ""], ["Yu", "Zhiding", ""], ["Mao", "Lei", ""], ["Patel", "Ankit B.", ""], ["Zhu", "Yuke", ""], ["Anandkumar", "Animashree", ""]]}, {"id": "2010.00793", "submitter": "Runmin Cong", "authors": "Chongyi Li, Runmin Cong, Chunle Guo, Hua Li, Chunjie Zhang, Feng\n  Zheng, and Yao Zhao", "title": "A Parallel Down-Up Fusion Network for Salient Object Detection in\n  Optical Remote Sensing Images", "comments": "Neurocomputing, 415, pp. 411-420, 2020", "journal-ref": null, "doi": "10.1016/j.neucom.2020.05.108", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The diverse spatial resolutions, various object types, scales and\norientations, and cluttered backgrounds in optical remote sensing images (RSIs)\nchallenge the current salient object detection (SOD) approaches. It is commonly\nunsatisfactory to directly employ the SOD approaches designed for nature scene\nimages (NSIs) to RSIs. In this paper, we propose a novel Parallel Down-up\nFusion network (PDF-Net) for SOD in optical RSIs, which takes full advantage of\nthe in-path low- and high-level features and cross-path multi-resolution\nfeatures to distinguish diversely scaled salient objects and suppress the\ncluttered backgrounds. To be specific, keeping a key observation that the\nsalient objects still are salient no matter the resolutions of images are in\nmind, the PDF-Net takes successive down-sampling to form five parallel paths\nand perceive scaled salient objects that are commonly existed in optical RSIs.\nMeanwhile, we adopt the dense connections to take advantage of both low- and\nhigh-level information in the same path and build up the relations of cross\npaths, which explicitly yield strong feature representations. At last, we fuse\nthe multiple-resolution features in parallel paths to combine the benefits of\nthe features with different resolutions, i.e., the high-resolution feature\nconsisting of complete structure and clear details while the low-resolution\nfeatures highlighting the scaled salient objects. Extensive experiments on the\nORSSD dataset demonstrate that the proposed network is superior to the\nstate-of-the-art approaches both qualitatively and quantitatively.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 05:27:57 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Li", "Chongyi", ""], ["Cong", "Runmin", ""], ["Guo", "Chunle", ""], ["Li", "Hua", ""], ["Zhang", "Chunjie", ""], ["Zheng", "Feng", ""], ["Zhao", "Yao", ""]]}, {"id": "2010.00795", "submitter": "Zheng Li", "authors": "Zheng Li, Ying Huang, Defang Chen, Tianren Luo, Ning Cai, Zhigeng Pan", "title": "Online Knowledge Distillation via Multi-branch Diversity Enhancement", "comments": "ACCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge distillation is an effective method to transfer the knowledge from\nthe cumbersome teacher model to the lightweight student model. Online knowledge\ndistillation uses the ensembled prediction results of multiple student models\nas soft targets to train each student model. However, the homogenization\nproblem will lead to difficulty in further improving model performance. In this\nwork, we propose a new distillation method to enhance the diversity among\nmultiple student models. We introduce Feature Fusion Module (FFM), which\nimproves the performance of the attention mechanism in the network by\nintegrating rich semantic information contained in the last block of multiple\nstudent models. Furthermore, we use the Classifier Diversification(CD) loss\nfunction to strengthen the differences between the student models and deliver a\nbetter ensemble result. Extensive experiments proved that our method\nsignificantly enhances the diversity among student models and brings better\ndistillation performance. We evaluate our method on three image classification\ndatasets: CIFAR-10/100 and CINIC-10. The results show that our method achieves\nstate-of-the-art performance on these datasets.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 05:52:12 GMT"}, {"version": "v2", "created": "Thu, 12 Nov 2020 07:06:53 GMT"}, {"version": "v3", "created": "Fri, 13 Nov 2020 14:18:39 GMT"}], "update_date": "2020-11-16", "authors_parsed": [["Li", "Zheng", ""], ["Huang", "Ying", ""], ["Chen", "Defang", ""], ["Luo", "Tianren", ""], ["Cai", "Ning", ""], ["Pan", "Zhigeng", ""]]}, {"id": "2010.00801", "submitter": "MaungMaung AprilPyone", "authors": "MaungMaung AprilPyone, Hitoshi Kiya", "title": "Block-wise Image Transformation with Secret Key for Adversarially Robust\n  Defense", "comments": "Under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel defensive transformation that enables us to\nmaintain a high classification accuracy under the use of both clean images and\nadversarial examples for adversarially robust defense. The proposed\ntransformation is a block-wise preprocessing technique with a secret key to\ninput images. We developed three algorithms to realize the proposed\ntransformation: Pixel Shuffling, Bit Flipping, and FFX Encryption. Experiments\nwere carried out on the CIFAR-10 and ImageNet datasets by using both black-box\nand white-box attacks with various metrics including adaptive ones. The results\nshow that the proposed defense achieves high accuracy close to that of using\nclean images even under adaptive attacks for the first time. In the best-case\nscenario, a model trained by using images transformed by FFX Encryption (block\nsize of 4) yielded an accuracy of 92.30% on clean images and 91.48% under PGD\nattack with a noise distance of 8/255, which is close to the non-robust\naccuracy (95.45%) for the CIFAR-10 dataset, and it yielded an accuracy of\n72.18% on clean images and 71.43% under the same attack, which is also close to\nthe standard accuracy (73.70%) for the ImageNet dataset. Overall, all three\nproposed algorithms are demonstrated to outperform state-of-the-art defenses\nincluding adversarial training whether or not a model is under attack.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 06:07:12 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["AprilPyone", "MaungMaung", ""], ["Kiya", "Hitoshi", ""]]}, {"id": "2010.00802", "submitter": "Thomas Kurbiel", "authors": "Thomas Kurbiel, Akash Sachdeva, Kun Zhao and Markus Buehren", "title": "PrognoseNet: A Generative Probabilistic Framework for Multimodal\n  Position Prediction given Context Information", "comments": "8 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to predict multiple possible future positions of the ego-vehicle\ngiven the surrounding context while also estimating their probabilities is key\nto safe autonomous driving. Most of the current state-of-the-art Deep Learning\napproaches are trained on trajectory data to achieve this task. However\ntrajectory data captured by sensor systems is highly imbalanced, since by far\nmost of the trajectories follow straight lines with an approximately constant\nvelocity. This poses a huge challenge for the task of predicting future\npositions, which is inherently a regression problem. Current state-of-the-art\napproaches alleviate this problem only by major preprocessing of the training\ndata, e.g. resampling, clustering into anchors etc. In this paper we propose an\napproach which reformulates the prediction problem as a classification task,\nallowing for powerful tools, e.g. focal loss, to combat the imbalance. To this\nend we design a generative probabilistic model consisting of a deep neural\nnetwork with a Mixture of Gaussian head. A smart choice of the latent variable\nallows for the reformulation of the log-likelihood function as a combination of\na classification problem and a much simplified regression problem. The output\nof our model is an estimate of the probability density function of future\npositions, hence allowing for prediction of multiple possible positions while\nalso estimating their probabilities. The proposed approach can easily\nincorporate context information and does not require any preprocessing of the\ndata.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 06:13:41 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Kurbiel", "Thomas", ""], ["Sachdeva", "Akash", ""], ["Zhao", "Kun", ""], ["Buehren", "Markus", ""]]}, {"id": "2010.00806", "submitter": "Phat Thai", "authors": "Phat Thai, Sameer Alam, Nimrod Lilith, Phu N. Tran, Binh Nguyen Thanh", "title": "Deep4Air: A Novel Deep Learning Framework for Airport Airside\n  Surveillance", "comments": null, "journal-ref": null, "doi": "10.1109/ICMEW53276.2021.9456005", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An airport runway and taxiway (airside) area is a highly dynamic and complex\nenvironment featuring interactions between different types of vehicles (speed\nand dimension), under varying visibility and traffic conditions. Airport ground\nmovements are deemed safety-critical activities, and safe-separation procedures\nmust be maintained by Air Traffic Controllers (ATCs). Large airports with\ncomplicated runway-taxiway systems use advanced ground surveillance systems.\nHowever, these systems have inherent limitations and a lack of real-time\nanalytics. In this paper, we propose a novel computer-vision based framework,\nnamely \"Deep4Air\", which can not only augment the ground surveillance systems\nvia the automated visual monitoring of runways and taxiways for aircraft\nlocation, but also provide real-time speed and distance analytics for aircraft\non runways and taxiways. The proposed framework includes an adaptive deep\nneural network for efficiently detecting and tracking aircraft. The\nexperimental results show an average precision of detection and tracking of up\nto 99.8% on simulated data with validations on surveillance videos from the\ndigital tower at George Bush Intercontinental Airport. The results also\ndemonstrate that \"Deep4Air\" can locate aircraft positions relative to the\nairport runway and taxiway infrastructure with high accuracy. Furthermore,\naircraft speed and separation distance are monitored in real-time, providing\nenhanced safety management.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 06:33:21 GMT"}, {"version": "v2", "created": "Wed, 21 Jul 2021 14:58:10 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Thai", "Phat", ""], ["Alam", "Sameer", ""], ["Lilith", "Nimrod", ""], ["Tran", "Phu N.", ""], ["Thanh", "Binh Nguyen", ""]]}, {"id": "2010.00820", "submitter": "Ignacio Sarasua", "authors": "Benjamin Gutierrez Becker, Ignacio Sarasua, Christian Wachinger", "title": "Discriminative and Generative Models for Anatomical Shape Analysison\n  Point Clouds with Deep Neural Networks", "comments": "Accepted for publication in the Medical Image Analysis journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce deep neural networks for the analysis of anatomical shapes that\nlearn a low-dimensional shape representation from the given task, instead of\nrelying on hand-engineered representations. Our framework is modular and\nconsists of several computing blocks that perform fundamental shape processing\ntasks. The networks operate on unordered point clouds and provide invariance to\nsimilarity transformations, avoiding the need to identify point correspondences\nbetween shapes. Based on the framework, we assemble a discriminative model for\ndisease classification and age regression, as well as a generative model for\nthe accruate reconstruction of shapes. In particular, we propose a conditional\ngenerative model, where the condition vector provides a mechanism to control\nthe generative process. instance, it enables to assess shape variations\nspecific to a particular diagnosis, when passing it as side information. Next\nto working on single shapes, we introduce an extension for the joint analysis\nof multiple anatomical structures, where the simultaneous modeling of multiple\nstructures can lead to a more compact encoding and a better understanding of\ndisorders. We demonstrate the advantages of our framework in comprehensive\nexperiments on real and synthetic data. The key insights are that (i) learning\na shape representation specific to the given task yields higher performance\nthan alternative shape descriptors, (ii) multi-structure analysis is both more\nefficient and more accurate than single-structure analysis, and (iii) point\nclouds generated by our model capture morphological differences associated to\nAlzheimers disease, to the point that they can be used to train a\ndiscriminative model for disease classification. Our framework naturally scales\nto the analysis of large datasets, giving it the potential to learn\ncharacteristic variations in large populations.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 07:37:40 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Becker", "Benjamin Gutierrez", ""], ["Sarasua", "Ignacio", ""], ["Wachinger", "Christian", ""]]}, {"id": "2010.00821", "submitter": "Wolfgang Fuhl", "authors": "Wolfgang Fuhl, Yao Rong, Thomas Motz, Michael Scheidt, Andreas Hartel,\n  Andreas Koch, Enkelejda Kasneci", "title": "Explainable Online Validation of Machine Learning Models for Practical\n  Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a reformulation of the regression and classification, which aims\nto validate the result of a machine learning algorithm. Our reformulation\nsimplifies the original problem and validates the result of the machine\nlearning algorithm using the training data. Since the validation of machine\nlearning algorithms must always be explainable, we perform our experiments with\nthe kNN algorithm as well as with an algorithm based on conditional\nprobabilities, which is proposed in this work. For the evaluation of our\napproach, three publicly available data sets were used and three classification\nand two regression problems were evaluated. The presented algorithm based on\nconditional probabilities is also online capable and requires only a fraction\nof memory compared to the kNN algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 07:38:31 GMT"}, {"version": "v2", "created": "Fri, 30 Oct 2020 14:20:29 GMT"}, {"version": "v3", "created": "Sun, 17 Jan 2021 12:26:28 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Fuhl", "Wolfgang", ""], ["Rong", "Yao", ""], ["Motz", "Thomas", ""], ["Scheidt", "Michael", ""], ["Hartel", "Andreas", ""], ["Koch", "Andreas", ""], ["Kasneci", "Enkelejda", ""]]}, {"id": "2010.00824", "submitter": "Lirui Wang", "authors": "Lirui Wang, Yu Xiang, Wei Yang, Arsalan Mousavian, Dieter Fox", "title": "Goal-Auxiliary Actor-Critic for 6D Robotic Grasping with Point Clouds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  6D robotic grasping beyond top-down bin-picking scenarios is a challenging\ntask. Previous solutions based on 6D grasp synthesis with robot motion planning\nusually operate in an open-loop setting, which are sensitive to grasp synthesis\nerrors. In this work, we propose a new method for learning closed-loop control\npolicies for 6D grasping. Our policy takes a segmented point cloud of an object\nfrom an egocentric camera as input, and outputs continuous 6D control actions\nof the robot gripper for grasping the object. We combine imitation learning and\nreinforcement learning and introduce a goal-auxiliary actor-critic algorithm\nfor policy learning. We demonstrate that our learned policy can be integrated\ninto a tabletop 6D grasping system and a human-robot handover system to improve\nthe grasping performance of unseen objects. Our videos and code can be found at\nhttps://sites.google.com/view/gaddpg .\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 07:42:00 GMT"}, {"version": "v2", "created": "Tue, 12 Jan 2021 19:19:25 GMT"}, {"version": "v3", "created": "Tue, 2 Mar 2021 01:47:06 GMT"}, {"version": "v4", "created": "Thu, 1 Jul 2021 00:59:01 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Wang", "Lirui", ""], ["Xiang", "Yu", ""], ["Yang", "Wei", ""], ["Mousavian", "Arsalan", ""], ["Fox", "Dieter", ""]]}, {"id": "2010.00829", "submitter": "Volker Krueger", "authors": "Bjarne Grossmann, Francesco Rovida and Volker Krueger", "title": "Continuous close-range 3D object pose estimation", "comments": null, "journal-ref": "2019 IEEE/RSJ International Conference on Intelligent Robots and\n  Systems (IROS)", "doi": "10.1109/IROS40897.2019.8967580", "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the context of future manufacturing lines, removing fixtures will be a\nfundamental step to increase the flexibility of autonomous systems in assembly\nand logistic operations. Vision-based 3D pose estimation is a necessity to\naccurately handle objects that might not be placed at fixed positions during\nthe robot task execution. Industrial tasks bring multiple challenges for the\nrobust pose estimation of objects such as difficult object properties, tight\ncycle times and constraints on camera views. In particular, when interacting\nwith objects, we have to work with close-range partial views of objects that\npose a new challenge for typical view-based pose estimation methods. In this\npaper, we present a 3D pose estimation method based on a gradient-ascend\nparticle filter that integrates new observations on-the-fly to improve the pose\nestimate. Thereby, we can apply this method online during task execution to\nsave valuable cycle time. In contrast to other view-based pose estimation\nmethods, we model potential views in full 6- dimensional space that allows us\nto cope with close-range partial objects views. We demonstrate the approach on\na real assembly task, in which the algorithm usually converges to the correct\npose within 10-15 iterations with an average accuracy of less than 8mm.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 07:48:17 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Grossmann", "Bjarne", ""], ["Rovida", "Francesco", ""], ["Krueger", "Volker", ""]]}, {"id": "2010.00839", "submitter": "Douglas Meneghetti", "authors": "Leonardo Anjoletto Ferreira, Douglas De Rizzo Meneghetti, Paulo\n  Eduardo Santos", "title": "CAPTION: Correction by Analyses, POS-Tagging and Interpretation of\n  Objects using only Nouns", "comments": "Published at the First Annual International Workshop on\n  Interpretability: Methodologies and algorithms (IMA 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, Deep Learning (DL) methods have shown an excellent performance in\nimage captioning and visual question answering. However, despite their\nperformance, DL methods do not learn the semantics of the words that are being\nused to describe a scene, making it difficult to spot incorrect words used in\ncaptions or to interchange words that have similar meanings. This work proposes\na combination of DL methods for object detection and natural language\nprocessing to validate image's captions. We test our method in the FOIL-COCO\ndata set, since it provides correct and incorrect captions for various images\nusing only objects represented in the MS-COCO image data set. Results show that\nour method has a good overall performance, in some cases similar to the human\nperformance.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 08:06:42 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Ferreira", "Leonardo Anjoletto", ""], ["Meneghetti", "Douglas De Rizzo", ""], ["Santos", "Paulo Eduardo", ""]]}, {"id": "2010.00853", "submitter": "Guillaume Noyel", "authors": "Guillaume Noyel (CMM), Jesus Angulo (CMM), Dominique Jeulin (CMM)", "title": "Morphological segmentation of hyperspectral images", "comments": null, "journal-ref": "Image Analysis and Stereology, International Society for\n  Stereology, 2007, 26 (3), pp.101-109", "doi": "10.5566/ias.v26.p101-109", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The present paper develops a general methodology for the morphological\nsegmentation of hyperspectral images, i.e., with an important number of\nchannels. This approach, based on watershed, is composed of a spectral\nclassification to obtain the markers and a vectorial gradient which gives the\nspatial information. Several alternative gradients are adapted to the different\nhyperspectral functions. Data reduction is performed either by Factor Analysis\nor by model fitting. Image segmentation is done on different spaces: factor\nspace, parameters space, etc. On all these spaces the spatial/spectral\nsegmentation approach is applied, leading to relevant results on the image.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 08:32:52 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Noyel", "Guillaume", "", "CMM"], ["Angulo", "Jesus", "", "CMM"], ["Jeulin", "Dominique", "", "CMM"]]}, {"id": "2010.00866", "submitter": "Wolfgang Fuhl", "authors": "Wolfgang Fuhl, Enkelejda Kasneci", "title": "Weight and Gradient Centralization in Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Batch normalization is currently the most widely used variant of internal\nnormalization for deep neural networks. Additional work has shown that the\nnormalization of weights and additional conditioning as well as the\nnormalization of gradients further improve the generalization. In this work, we\ncombine several of these methods and thereby increase the generalization of the\nnetworks. The advantage of the newer methods compared to the batch\nnormalization is not only increased generalization, but also that these methods\nonly have to be applied during training and, therefore, do not influence the\nrunning time during use. Link to CUDA code\nhttps://atreus.informatik.uni-tuebingen.de/seafile/d/8e2ab8c3fdd444e1a135/\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 08:50:04 GMT"}, {"version": "v2", "created": "Fri, 30 Oct 2020 14:13:39 GMT"}, {"version": "v3", "created": "Sun, 17 Jan 2021 12:05:14 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Fuhl", "Wolfgang", ""], ["Kasneci", "Enkelejda", ""]]}, {"id": "2010.00873", "submitter": "Wolfgang Fuhl", "authors": "Wolfgang Fuhl, Enkelejda Kasneci", "title": "Rotated Ring, Radial and Depth Wise Separable Radial Convolutions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simple image rotations significantly reduce the accuracy of deep neural\nnetworks. Moreover, training with all possible rotations increases the data\nset, which also increases the training duration. In this work, we address\ntrainable rotation invariant convolutions as well as the construction of nets,\nsince fully connected layers can only be rotation invariant with a\none-dimensional input. On the one hand, we show that our approach is\nrotationally invariant for different models and on different public data sets.\nWe also discuss the influence of purely rotational invariant features on\naccuracy. The rotationally adaptive convolution models presented in this work\nare more computationally intensive than normal convolution models. Therefore,\nwe also present a depth wise separable approach with radial convolution. Link\nto CUDA code\nhttps://atreus.informatik.uni-tuebingen.de/seafile/d/8e2ab8c3fdd444e1a135/\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 09:01:51 GMT"}, {"version": "v2", "created": "Fri, 30 Oct 2020 14:15:03 GMT"}, {"version": "v3", "created": "Sun, 17 Jan 2021 12:08:07 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Fuhl", "Wolfgang", ""], ["Kasneci", "Enkelejda", ""]]}, {"id": "2010.00882", "submitter": "Haifeng Li", "authors": "Chao Tao, Ji Qi, Weipeng Lu, Hao Wang and Haifeng Li", "title": "Remote Sensing Image Scene Classification with Self-Supervised Paradigm\n  under Limited Labeled Samples", "comments": "6 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the development of deep learning, supervised learning methods perform\nwell in remote sensing images (RSIs) scene classification. However, supervised\nlearning requires a huge number of annotated data for training. When labeled\nsamples are not sufficient, the most common solution is to fine-tune the\npre-training models using a large natural image dataset (e.g. ImageNet).\nHowever, this learning paradigm is not a panacea, especially when the target\nremote sensing images (e.g. multispectral and hyperspectral data) have\ndifferent imaging mechanisms from RGB natural images. To solve this problem, we\nintroduce new self-supervised learning (SSL) mechanism to obtain the\nhigh-performance pre-training model for RSIs scene classification from large\nunlabeled data. Experiments on three commonly used RSIs scene classification\ndatasets demonstrated that this new learning paradigm outperforms the\ntraditional dominant ImageNet pre-trained model. Moreover, we analyze the\nimpacts of several factors in SSL on RSIs scene classification tasks, including\nthe choice of self-supervised signals, the domain difference between the source\nand target dataset, and the amount of pre-training data. The insights distilled\nfrom our studies can help to foster the development of SSL in the remote\nsensing community. Since SSL could learn from unlabeled massive RSIs which are\nextremely easy to obtain, it will be a potentially promising way to alleviate\ndependence on labeled samples and thus efficiently solve many problems, such as\nglobal mapping.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 09:27:19 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Tao", "Chao", ""], ["Qi", "Ji", ""], ["Lu", "Weipeng", ""], ["Wang", "Hao", ""], ["Li", "Haifeng", ""]]}, {"id": "2010.00890", "submitter": "Javad Amirian", "authors": "Javad Amirian, Bingqing Zhang, Francisco Valente Castro, Juan Jose\n  Baldelomar, Jean-Bernard Hayet and Julien Pettre", "title": "OpenTraj: Assessing Prediction Complexity in Human Trajectories Datasets", "comments": "ACCV2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human Trajectory Prediction (HTP) has gained much momentum in the last years\nand many solutions have been proposed to solve it. Proper benchmarking being a\nkey issue for comparing methods, this paper addresses the question of\nevaluating how complex is a given dataset with respect to the prediction\nproblem. For assessing a dataset complexity, we define a series of indicators\naround three concepts: Trajectory predictability; Trajectory regularity;\nContext complexity. We compare the most common datasets used in HTP in the\nlight of these indicators and discuss what this may imply on benchmarking of\nHTP algorithms. Our source code is released on Github.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 09:37:18 GMT"}, {"version": "v2", "created": "Mon, 2 Nov 2020 21:24:33 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Amirian", "Javad", ""], ["Zhang", "Bingqing", ""], ["Castro", "Francisco Valente", ""], ["Baldelomar", "Juan Jose", ""], ["Hayet", "Jean-Bernard", ""], ["Pettre", "Julien", ""]]}, {"id": "2010.00893", "submitter": "Hujie Pan", "authors": "Hujie Pan, Xuesong Li, Min Xu", "title": "Weight Encode Reconstruction Network for Computed Tomography in a\n  Semi-Case-Wise and Learning-Based Way", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classic algebraic reconstruction technology (ART) for computed tomography\nrequires pre-determined weights of the voxels for projecting pixel values.\nHowever, such weight cannot be accurately obtained due to the limitation of the\nphysical understanding and computation resources. In this study, we propose a\nsemi-case-wise learning-based method named Weight Encode Reconstruction Network\n(WERNet) to tackle the issues mentioned above. The model is trained in a\nself-supervised manner without the label of a voxel set. It contains two\nbranches, including the voxel weight encoder and the voxel attention part.\nUsing gradient normalization, we are able to co-train the encoder and voxel set\nnumerically stably. With WERNet, the reconstructed result was obtained with a\ncosine similarity greater than 0.999 with the ground truth. Moreover, the model\nshows the extraordinary capability of denoising comparing to the classic ART\nmethod. In the generalization test of the model, the encoder is transferable\nfrom a voxel set with complex structure to the unseen cases without the\ndeduction of the accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 09:46:35 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Pan", "Hujie", ""], ["Li", "Xuesong", ""], ["Xu", "Min", ""]]}, {"id": "2010.00907", "submitter": "Ilyas Sirazitdinov", "authors": "Ilyas Sirazitdinov, Heinrich Schulz, Axel Saalbach, Steffen Renisch\n  and Dmitry V. Dylov", "title": "Tubular Shape Aware Data Generation for Semantic Segmentation in Medical\n  Imaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chest X-ray is one of the most widespread examinations of the human body. In\ninterventional radiology, its use is frequently associated with the need to\nvisualize various tube-like objects, such as puncture needles, guiding sheaths,\nwires, and catheters. Detection and precise localization of these tube-like\nobjects in the X-ray images is, therefore, of utmost value, catalyzing the\ndevelopment of accurate target-specific segmentation algorithms. Similar to the\nother medical imaging tasks, the manual pixel-wise annotation of the tubes is a\nresource-consuming process. In this work, we aim to alleviate the lack of the\nannotated images by using artificial data. Specifically, we present an approach\nfor synthetic data generation of the tube-shaped objects, with a generative\nadversarial network being regularized with a prior-shape constraint. Our method\neliminates the need for paired image--mask data and requires only a\nweakly-labeled dataset (10--20 images) to reach the accuracy of the\nfully-supervised models. We report the applicability of the approach for the\ntask of segmenting tubes and catheters in the X-ray images, whereas the results\nshould also hold for the other imaging modalities.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 10:28:25 GMT"}, {"version": "v2", "created": "Mon, 7 Dec 2020 15:11:33 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Sirazitdinov", "Ilyas", ""], ["Schulz", "Heinrich", ""], ["Saalbach", "Axel", ""], ["Renisch", "Steffen", ""], ["Dylov", "Dmitry V.", ""]]}, {"id": "2010.00909", "submitter": "Nobukatsu Kajiura", "authors": "Nobukatsu Kajiura, Satoshi Kosugi, Xueting Wang, Toshihiko Yamasaki", "title": "Self-Play Reinforcement Learning for Fast Image Retargeting", "comments": "Accepted to ACM Multimedia 2020", "journal-ref": null, "doi": "10.1145/3394171.3413857", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, we address image retargeting, which is a task that adjusts\ninput images to arbitrary sizes. In one of the best-performing methods called\nMULTIOP, multiple retargeting operators were combined and retargeted images at\neach stage were generated to find the optimal sequence of operators that\nminimized the distance between original and retargeted images. The limitation\nof this method is in its tremendous processing time, which severely prohibits\nits practical use. Therefore, the purpose of this study is to find the optimal\ncombination of operators within a reasonable processing time; we propose a\nmethod of predicting the optimal operator for each step using a reinforcement\nlearning agent. The technical contributions of this study are as follows.\nFirstly, we propose a reward based on self-play, which will be insensitive to\nthe large variance in the content-dependent distance measured in MULTIOP.\nSecondly, we propose to dynamically change the loss weight for each action to\nprevent the algorithm from falling into a local optimum and from choosing only\nthe most frequently used operator in its training. Our experiments showed that\nwe achieved multi-operator image retargeting with less processing time by three\norders of magnitude and the same quality as the original multi-operator-based\nmethod, which was the best-performing algorithm in retargeting tasks.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 10:31:27 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Kajiura", "Nobukatsu", ""], ["Kosugi", "Satoshi", ""], ["Wang", "Xueting", ""], ["Yamasaki", "Toshihiko", ""]]}, {"id": "2010.00923", "submitter": "Chaoqun Xia", "authors": "Chaoqun Xia, Xiaorun Li, Liaoying Zhao, Shuhan Chen", "title": "Multiple Infrared Small Targets Detection based on Hierarchical Maximal\n  Entropy Random Walk", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The technique of detecting multiple dim and small targets with low\nsignal-to-clutter ratios (SCR) is very important for infrared search and\ntracking systems. In this paper, we establish a detection method derived from\nmaximal entropy random walk (MERW) to robustly detect multiple small targets.\nInitially, we introduce the primal MERW and analyze the feasibility of applying\nit to small target detection. However, the original weight matrix of the MERW\nis sensitive to interferences. Therefore, a specific weight matrix is designed\nfor the MERW in principle of enhancing characteristics of small targets and\nsuppressing strong clutters. Moreover, the primal MERW has a critical\nlimitation of strong bias to the most salient small target. To achieve multiple\nsmall targets detection, we develop a hierarchical version of the MERW method.\nBased on the hierarchical MERW (HMERW), we propose a small target detection\nmethod as follows. First, filtering technique is used to smooth the infrared\nimage. Second, an output map is obtained by importing the filtered image into\nthe HMERW. Then, a coefficient map is constructed to fuse the stationary\ndirtribution map of the HMERW. Finally, an adaptive threshold is used to\nsegment multiple small targets from the fusion map. Extensive experiments on\npractical data sets demonstrate that the proposed method is superior to the\nstate-of-the-art methods in terms of target enhancement, background suppression\nand multiple small targets detection.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 11:11:34 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Xia", "Chaoqun", ""], ["Li", "Xiaorun", ""], ["Zhao", "Liaoying", ""], ["Chen", "Shuhan", ""]]}, {"id": "2010.00925", "submitter": "Zohaib Salahuddin", "authors": "Zohaib Salahuddin, Matthias Lenga and Hannes Nickisch", "title": "Multi-Resolution 3D Convolutional Neural Networks for Automatic Coronary\n  Centerline Extraction in Cardiac CT Angiography Scans", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a deep learning-based automatic coronary artery tree centerline\ntracker (AuCoTrack) extending the vessel tracker by Wolterink\n(arXiv:1810.03143). A dual pathway Convolutional Neural Network (CNN) operating\non multi-scale 3D inputs predicts the direction of the coronary arteries as\nwell as the presence of a bifurcation. A similar multi-scale dual pathway 3D\nCNN is trained to identify coronary artery endpoints for terminating the\ntracking process. Two or more continuation directions are derived based on the\nbifurcation detection. The iterative tracker detects the entire left and right\ncoronary artery trees based on only two ostium landmarks derived from a\nmodel-based segmentation of the heart.\n  The 3D CNNs were trained on a proprietary dataset consisting of 43 CCTA\nscans. An average sensitivity of 87.1% and clinically relevant overlap of 89.1%\nwas obtained relative to a refined manual segmentation. In addition, the MICCAI\n2008 Coronary Artery Tracking Challenge (CAT08) training and test datasets were\nused to benchmark the algorithm and to assess its generalization. An average\noverlap of 93.6% and a clinically relevant overlap of 96.4% were obtained. The\nproposed method achieved better overlap scores than the current\nstate-of-the-art automatic centerline extraction techniques on the CAT08\ndataset with a vessel detection rate of 95%.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 11:20:25 GMT"}, {"version": "v2", "created": "Mon, 7 Dec 2020 06:09:17 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Salahuddin", "Zohaib", ""], ["Lenga", "Matthias", ""], ["Nickisch", "Hannes", ""]]}, {"id": "2010.00928", "submitter": "Panagiotis Agrafiotis", "authors": "Dimitrios Skarlatos and Panagiotis Agrafiotis", "title": "Image-based underwater 3D reconstruction for Cultural Heritage: from\n  image collection to 3D. Critical steps and considerations", "comments": "Pre-submission version of the manuscript", "journal-ref": null, "doi": "10.1007/978-3-030-37191-3_8", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Underwater Cultural Heritage (CH) sites are widely spread; from ruins in\ncoastlines up to shipwrecks in deep. The documentation and preservation of this\nheritage is an obligation of the mankind, dictated also by the international\ntreaties like the Convention on the Protection of the Underwater Cultural\nHer-itage which fosters the use of \"non-destructive techniques and survey\nmeth-ods in preference over the recovery of objects\". However, submerged CH\nlacks in protection and monitoring in regards to the land CH and nowadays\nrecording and documenting, for digital preservation as well as dissemination\nthrough VR to wide public, is of most importance. At the same time, it is most\ndifficult to document it, due to inherent restrictions posed by the\nenviron-ment. In order to create high detailed textured 3D models, optical\nsensors and photogrammetric techniques seems to be the best solution. This\nchapter dis-cusses critical aspects of all phases of image based underwater 3D\nreconstruc-tion process, from data acquisition and data preparation using\ncolour restora-tion and colour enhancement algorithms to Structure from Motion\n(SfM) and Multi-View Stereo (MVS) techniques to produce an accurate, precise\nand complete 3D model for a number of applications.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 11:32:33 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Skarlatos", "Dimitrios", ""], ["Agrafiotis", "Panagiotis", ""]]}, {"id": "2010.00929", "submitter": "Boris Joukovsky", "authors": "Huynh Van Luong, Boris Joukovsky, Yonina C. Eldar, Nikos Deligiannis", "title": "A Deep-Unfolded Reference-Based RPCA Network For Video\n  Foreground-Background Separation", "comments": "5 pages, accepted for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep unfolded neural networks are designed by unrolling the iterations of\noptimization algorithms. They can be shown to achieve faster convergence and\nhigher accuracy than their optimization counterparts. This paper proposes a new\ndeep-unfolding-based network design for the problem of Robust Principal\nComponent Analysis (RPCA) with application to video foreground-background\nseparation. Unlike existing designs, our approach focuses on modeling the\ntemporal correlation between the sparse representations of consecutive video\nframes. To this end, we perform the unfolding of an iterative algorithm for\nsolving reweighted $\\ell_1$-$\\ell_1$ minimization; this unfolding leads to a\ndifferent proximal operator (a.k.a. different activation function) adaptively\nlearned per neuron. Experimentation using the moving MNIST dataset shows that\nthe proposed network outperforms a recently proposed state-of-the-art RPCA\nnetwork in the task of video foreground-background separation.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 11:40:09 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Van Luong", "Huynh", ""], ["Joukovsky", "Boris", ""], ["Eldar", "Yonina C.", ""], ["Deligiannis", "Nikos", ""]]}, {"id": "2010.00947", "submitter": "Shengyu Zhang", "authors": "Shengyu Zhang, Donghui Wang, Zhou Zhao, Siliang Tang, Di Xie, Fei Wu", "title": "MGD-GAN: Text-to-Pedestrian generation through Multi-Grained\n  Discrimination", "comments": "8 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate the problem of text-to-pedestrian synthesis,\nwhich has many potential applications in art, design, and video surveillance.\nExisting methods for text-to-bird/flower synthesis are still far from solving\nthis fine-grained image generation problem, due to the complex structure and\nheterogeneous appearance that the pedestrians naturally take on. To this end,\nwe propose the Multi-Grained Discrimination enhanced Generative Adversarial\nNetwork, that capitalizes a human-part-based Discriminator (HPD) and a\nself-cross-attended (SCA) global Discriminator in order to capture the\ncoherence of the complex body structure. A fined-grained word-level attention\nmechanism is employed in the HPD module to enforce diversified appearance and\nvivid details. In addition, two pedestrian generation metrics, named Pose Score\nand Pose Variance, are devised to evaluate the generation quality and\ndiversity, respectively. We conduct extensive experiments and ablation studies\non the caption-annotated pedestrian dataset, CUHK Person Description Dataset.\nThe substantial improvement over the various metrics demonstrates the efficacy\nof MGD-GAN on the text-to-pedestrian synthesis scenario.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 12:24:48 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Zhang", "Shengyu", ""], ["Wang", "Donghui", ""], ["Zhao", "Zhou", ""], ["Tang", "Siliang", ""], ["Xie", "Di", ""], ["Wu", "Fei", ""]]}, {"id": "2010.00969", "submitter": "Yuchao Gu", "authors": "Yu-Chao Gu, Li-Juan Wang, Yun Liu, Yi Yang, Yu-Huan Wu, Shao-Ping Lu,\n  Ming-Ming Cheng", "title": "DOTS: Decoupling Operation and Topology in Differentiable Architecture\n  Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Differentiable Architecture Search (DARTS) has attracted extensive attention\ndue to its efficiency in searching for cell structures. DARTS mainly focuses on\nthe operation search and derives the cell topology from the operation weights.\nHowever, the operation weights can not indicate the importance of cell topology\nand result in poor topology rating correctness. To tackle this, we propose to\nDecouple the Operation and Topology Search (DOTS), which decouples the topology\nrepresentation from operation weights and makes an explicit topology search.\nDOTS is achieved by introducing a topology search space that contains\ncombinations of candidate edges. The proposed search space directly reflects\nthe search objective and can be easily extended to support a flexible number of\nedges in the searched cell. Existing gradient-based NAS methods can be\nincorporated into DOTS for further improvement by the topology search.\nConsidering that some operations (e.g., Skip-Connection) can affect the\ntopology, we propose a group operation search scheme to preserve\ntopology-related operations for a better topology search. The experiments on\nCIFAR10/100 and ImageNet demonstrate that DOTS is an effective solution for\ndifferentiable NAS.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 13:00:18 GMT"}, {"version": "v2", "created": "Tue, 17 Nov 2020 08:55:25 GMT"}, {"version": "v3", "created": "Thu, 8 Apr 2021 07:36:23 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Gu", "Yu-Chao", ""], ["Wang", "Li-Juan", ""], ["Liu", "Yun", ""], ["Yang", "Yi", ""], ["Wu", "Yu-Huan", ""], ["Lu", "Shao-Ping", ""], ["Cheng", "Ming-Ming", ""]]}, {"id": "2010.00973", "submitter": "Rao Fu", "authors": "Rao Fu, Jie Yang, Jiawei Sun, Fang-Lue Zhang, Yu-Kun Lai and Lin Gao", "title": "RISA-Net: Rotation-Invariant Structure-Aware Network for Fine-Grained 3D\n  Shape Retrieval", "comments": "The code and dataset are available at:\n  https://github.com/IGLICT/RisaNET", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fine-grained 3D shape retrieval aims to retrieve 3D shapes similar to a query\nshape in a repository with models belonging to the same class, which requires\nshape descriptors to be capable of representing detailed geometric information\nto discriminate shapes with globally similar structures. Moreover, 3D objects\ncan be placed with arbitrary position and orientation in real-world\napplications, which further requires shape descriptors to be robust to rigid\ntransformations. The shape descriptions used in existing 3D shape retrieval\nsystems fail to meet the above two criteria. In this paper, we introduce a\nnovel deep architecture, RISA-Net, which learns rotation invariant 3D shape\ndescriptors that are capable of encoding fine-grained geometric information and\nstructural information, and thus achieve accurate results on the task of\nfine-grained 3D object retrieval. RISA-Net extracts a set of compact and\ndetailed geometric features part-wisely and discriminatively estimates the\ncontribution of each semantic part to shape representation. Furthermore, our\nmethod is able to learn the importance of geometric and structural information\nof all the parts when generating the final compact latent feature of a 3D shape\nfor fine-grained retrieval. We also build and publish a new 3D shape dataset\nwith sub-class labels for validating the performance of fine-grained 3D shape\nretrieval methods. Qualitative and quantitative experiments show that our\nRISA-Net outperforms state-of-the-art methods on the fine-grained object\nretrieval task, demonstrating its capability in geometric detail extraction.\nThe code and dataset are available at: https://github.com/IGLICT/RisaNET.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 13:06:12 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Fu", "Rao", ""], ["Yang", "Jie", ""], ["Sun", "Jiawei", ""], ["Zhang", "Fang-Lue", ""], ["Lai", "Yu-Kun", ""], ["Gao", "Lin", ""]]}, {"id": "2010.00975", "submitter": "Zhizhe Liu", "authors": "Zhizhe Liu, Xingxing Zhang, Zhenfeng Zhu, Shuai Zheng, Yao Zhao and\n  Jian Cheng", "title": "Taking Modality-free Human Identification as Zero-shot Learning", "comments": "12 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human identification is an important topic in event detection, person\ntracking, and public security. There have been numerous methods proposed for\nhuman identification, such as face identification, person re-identification,\nand gait identification. Typically, existing methods predominantly classify a\nqueried image to a specific identity in an image gallery set (I2I). This is\nseriously limited for the scenario where only a textual description of the\nquery or an attribute gallery set is available in a wide range of video\nsurveillance applications (A2I or I2A). However, very few efforts have been\ndevoted towards modality-free identification, i.e., identifying a query in a\ngallery set in a scalable way. In this work, we take an initial attempt, and\nformulate such a novel Modality-Free Human Identification (named MFHI) task as\na generic zero-shot learning model in a scalable way. Meanwhile, it is capable\nof bridging the visual and semantic modalities by learning a discriminative\nprototype of each identity. In addition, the semantics-guided spatial attention\nis enforced on visual modality to obtain representations with both high global\ncategory-level and local attribute-level discrimination. Finally, we design and\nconduct an extensive group of experiments on two common challenging\nidentification tasks, including face identification and person\nre-identification, demonstrating that our method outperforms a wide variety of\nstate-of-the-art methods on modality-free human identification.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 13:08:27 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Liu", "Zhizhe", ""], ["Zhang", "Xingxing", ""], ["Zhu", "Zhenfeng", ""], ["Zheng", "Shuai", ""], ["Zhao", "Yao", ""], ["Cheng", "Jian", ""]]}, {"id": "2010.00977", "submitter": "David W. Romero", "authors": "David W. Romero, Jean-Baptiste Cordonnier", "title": "Group Equivariant Stand-Alone Self-Attention For Vision", "comments": "Proceedings of the 9th International Conference on Learning\n  Representations (ICLR), 2021", "journal-ref": "Proceedings of the International Conference on Learning\n  Representations, 2021", "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a general self-attention formulation to impose group equivariance\nto arbitrary symmetry groups. This is achieved by defining positional encodings\nthat are invariant to the action of the group considered. Since the group acts\non the positional encoding directly, group equivariant self-attention networks\n(GSA-Nets) are steerable by nature. Our experiments on vision benchmarks\ndemonstrate consistent improvements of GSA-Nets over non-equivariant\nself-attention networks.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 13:16:00 GMT"}, {"version": "v2", "created": "Thu, 18 Mar 2021 19:19:38 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Romero", "David W.", ""], ["Cordonnier", "Jean-Baptiste", ""]]}, {"id": "2010.00984", "submitter": "Felice Antonio Merra", "authors": "Vito Walter Anelli, Tommaso Di Noia, Daniele Malitesta, Felice Antonio\n  Merra", "title": "An Empirical Study of DNNs Robustification Inefficacy in Protecting\n  Visual Recommenders", "comments": "9 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual-based recommender systems (VRSs) enhance recommendation performance by\nintegrating users' feedback with the visual features of product images\nextracted from a deep neural network (DNN). Recently, human-imperceptible\nimages perturbations, defined \\textit{adversarial attacks}, have been\ndemonstrated to alter the VRSs recommendation performance, e.g., pushing/nuking\ncategory of products. However, since adversarial training techniques have\nproven to successfully robustify DNNs in preserving classification accuracy, to\nthe best of our knowledge, two important questions have not been investigated\nyet: 1) How well can these defensive mechanisms protect the VRSs performance?\n2) What are the reasons behind ineffective/effective defenses? To answer these\nquestions, we define a set of defense and attack settings, as well as\nrecommender models, to empirically investigate the efficacy of defensive\nmechanisms. The results indicate alarming risks in protecting a VRS through the\nDNN robustification. Our experiments shed light on the importance of visual\nfeatures in very effective attack scenarios. Given the financial impact of VRSs\non many companies, we believe this work might rise the need to investigate how\nto successfully protect visual-based recommenders. Source code and data are\navailable at\nhttps://anonymous.4open.science/r/868f87ca-c8a4-41ba-9af9-20c41de33029/.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 13:29:41 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Anelli", "Vito Walter", ""], ["Di Noia", "Tommaso", ""], ["Malitesta", "Daniele", ""], ["Merra", "Felice Antonio", ""]]}, {"id": "2010.00988", "submitter": "Boris Oreshkin N", "authors": "Boris N. Oreshkin and Tal Arbel", "title": "Uncertainty driven probabilistic voxel selection for image registration", "comments": null, "journal-ref": "in IEEE Transactions on Medical Imaging, vol. 32, no. 10, pp.\n  1777-1790, Oct. 2013", "doi": "10.1109/TMI.2013.2264467", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel probabilistic voxel selection strategy for\nmedical image registration in time-sensitive contexts, where the goal is\naggressive voxel sampling (e.g. using less than 1% of the total number) while\nmaintaining registration accuracy and low failure rate. We develop a Bayesian\nframework whereby, first, a voxel sampling probability field (VSPF) is built\nbased on the uncertainty on the transformation parameters. We then describe a\npractical, multi-scale registration algorithm, where, at each optimization\niteration, different voxel subsets are sampled based on the VSPF. The approach\nmaximizes accuracy without committing to a particular fixed subset of voxels.\nThe probabilistic sampling scheme developed is shown to manage the tradeoff\nbetween the robustness of traditional random voxel selection (by permitting\nmore exploration) and the accuracy of fixed voxel selection (by permitting a\ngreater proportion of informative voxels).\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 13:33:54 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Oreshkin", "Boris N.", ""], ["Arbel", "Tal", ""]]}, {"id": "2010.00991", "submitter": "Raphael Ortiz", "authors": "Raphael Ortiz, Gustavo de Medeiros, Antoine H.F.M. Peters, Prisca\n  Liberali, Markus Rempfler", "title": "RDCNet: Instance segmentation with a minimalist recurrent residual\n  network", "comments": "Accepted at MICCAI-MLMI 2020 workshop", "journal-ref": null, "doi": "10.1007/978-3-030-59861-7", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Instance segmentation is a key step for quantitative microscopy. While\nseveral machine learning based methods have been proposed for this problem,\nmost of them rely on computationally complex models that are trained on\nsurrogate tasks. Building on recent developments towards end-to-end trainable\ninstance segmentation, we propose a minimalist recurrent network called\nrecurrent dilated convolutional network (RDCNet), consisting of a shared\nstacked dilated convolution (sSDC) layer that iteratively refines its output\nand thereby generates interpretable intermediate predictions. It is\nlight-weight and has few critical hyperparameters, which can be related to\nphysical aspects such as object size or density.We perform a sensitivity\nanalysis of its main parameters and we demonstrate its versatility on 3 tasks\nwith different imaging modalities: nuclear segmentation of H&E slides, of 3D\nanisotropic stacks from light-sheet fluorescence microscopy and leaf\nsegmentation of top-view images of plants. It achieves state-of-the-art on 2 of\nthe 3 datasets.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 13:36:45 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Ortiz", "Raphael", ""], ["de Medeiros", "Gustavo", ""], ["Peters", "Antoine H. F. M.", ""], ["Liberali", "Prisca", ""], ["Rempfler", "Markus", ""]]}, {"id": "2010.00995", "submitter": "Ylva Ferstl", "authors": "Ylva Ferstl, Michael Neff, Rachel McDonnell", "title": "Understanding the Predictability of Gesture Parameters from Speech and\n  their Perceptual Importance", "comments": "To be published in the Proceedings of the 20th ACM International\n  Conference on Intelligent Virtual Agents (IVA 20)", "journal-ref": null, "doi": "10.1145/3383652.3423882", "report-no": null, "categories": "cs.HC cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gesture behavior is a natural part of human conversation. Much work has\nfocused on removing the need for tedious hand-animation to create embodied\nconversational agents by designing speech-driven gesture generators. However,\nthese generators often work in a black-box manner, assuming a general\nrelationship between input speech and output motion. As their success remains\nlimited, we investigate in more detail how speech may relate to different\naspects of gesture motion. We determine a number of parameters characterizing\ngesture, such as speed and gesture size, and explore their relationship to the\nspeech signal in a two-fold manner. First, we train multiple recurrent networks\nto predict the gesture parameters from speech to understand how well gesture\nattributes can be modeled from speech alone. We find that gesture parameters\ncan be partially predicted from speech, and some parameters, such as path\nlength, being predicted more accurately than others, like velocity. Second, we\ndesign a perceptual study to assess the importance of each gesture parameter\nfor producing motion that people perceive as appropriate for the speech.\nResults show that a degradation in any parameter was viewed negatively, but\nsome changes, such as hand shape, are more impactful than others. A video\nsummarization can be found at https://youtu.be/aw6-_5kmLjY.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 13:43:33 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Ferstl", "Ylva", ""], ["Neff", "Michael", ""], ["McDonnell", "Rachel", ""]]}, {"id": "2010.01005", "submitter": "Haoshu Fang", "authors": "Hao-Shu Fang, Yichen Xie, Dian Shao, Cewu Lu", "title": "DIRV: Dense Interaction Region Voting for End-to-End Human-Object\n  Interaction Detection", "comments": "Paper is accepted. Code available at:\n  https://github.com/MVIG-SJTU/DIRV", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years, human-object interaction (HOI) detection has achieved\nimpressive advances. However, conventional two-stage methods are usually slow\nin inference. On the other hand, existing one-stage methods mainly focus on the\nunion regions of interactions, which introduce unnecessary visual information\nas disturbances to HOI detection. To tackle the problems above, we propose a\nnovel one-stage HOI detection approach DIRV in this paper, based on a new\nconcept called interaction region for the HOI problem. Unlike previous methods,\nour approach concentrates on the densely sampled interaction regions across\ndifferent scales for each human-object pair, so as to capture the subtle visual\nfeatures that is most essential to the interaction. Moreover, in order to\ncompensate for the detection flaws of a single interaction region, we introduce\na novel voting strategy that makes full use of those overlapped interaction\nregions in place of conventional Non-Maximal Suppression (NMS). Extensive\nexperiments on two popular benchmarks: V-COCO and HICO-DET show that our\napproach outperforms existing state-of-the-arts by a large margin with the\nhighest inference speed and lightest network architecture. We achieved 56.1 mAP\non V-COCO without addtional input. Our code is publicly available at:\nhttps://github.com/MVIG-SJTU/DIRV\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 13:57:58 GMT"}, {"version": "v2", "created": "Tue, 19 Jan 2021 16:48:22 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Fang", "Hao-Shu", ""], ["Xie", "Yichen", ""], ["Shao", "Dian", ""], ["Lu", "Cewu", ""]]}, {"id": "2010.01007", "submitter": "Haoshu Fang", "authors": "Yichen Xie, Hao-Shu Fang, Dian Shao, Yong-Lu Li, Cewu Lu", "title": "DecAug: Augmenting HOI Detection via Decomposition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human-object interaction (HOI) detection requires a large amount of annotated\ndata. Current algorithms suffer from insufficient training samples and category\nimbalance within datasets. To increase data efficiency, in this paper, we\npropose an efficient and effective data augmentation method called DecAug for\nHOI detection. Based on our proposed object state similarity metric, object\npatterns across different HOIs are shared to augment local object appearance\nfeatures without changing their state. Further, we shift spatial correlation\nbetween humans and objects to other feasible configurations with the aid of a\npose-guided Gaussian Mixture Model while preserving their interactions.\nExperiments show that our method brings up to 3.3 mAP and 1.6 mAP improvements\non V-COCO and HICODET dataset for two advanced models. Specifically,\ninteractions with fewer samples enjoy more notable improvement. Our method can\nbe easily integrated into various HOI detection models with negligible extra\ncomputational consumption. Our code will be made publicly available.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 13:59:05 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Xie", "Yichen", ""], ["Fang", "Hao-Shu", ""], ["Shao", "Dian", ""], ["Li", "Yong-Lu", ""], ["Lu", "Cewu", ""]]}, {"id": "2010.01028", "submitter": "Yannis Kalantidis", "authors": "Yannis Kalantidis, Mert Bulent Sariyildiz, Noe Pion, Philippe\n  Weinzaepfel, Diane Larlus", "title": "Hard Negative Mixing for Contrastive Learning", "comments": "Accepted at NeurIPS 2020. Project page with pretrained models:\n  https://europe.naverlabs.com/mochi", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contrastive learning has become a key component of self-supervised learning\napproaches for computer vision. By learning to embed two augmented versions of\nthe same image close to each other and to push the embeddings of different\nimages apart, one can train highly transferable visual representations. As\nrevealed by recent studies, heavy data augmentation and large sets of negatives\nare both crucial in learning such representations. At the same time, data\nmixing strategies either at the image or the feature level improve both\nsupervised and semi-supervised learning by synthesizing novel examples, forcing\nnetworks to learn more robust features. In this paper, we argue that an\nimportant aspect of contrastive learning, i.e., the effect of hard negatives,\nhas so far been neglected. To get more meaningful negative samples, current top\ncontrastive self-supervised learning approaches either substantially increase\nthe batch sizes, or keep very large memory banks; increasing the memory size,\nhowever, leads to diminishing returns in terms of performance. We therefore\nstart by delving deeper into a top-performing framework and show evidence that\nharder negatives are needed to facilitate better and faster learning. Based on\nthese observations, and motivated by the success of data mixing, we propose\nhard negative mixing strategies at the feature level, that can be computed\non-the-fly with a minimal computational overhead. We exhaustively ablate our\napproach on linear classification, object detection and instance segmentation\nand show that employing our hard negative mixing procedure improves the quality\nof visual representations learned by a state-of-the-art self-supervised\nlearning method.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 14:34:58 GMT"}, {"version": "v2", "created": "Fri, 4 Dec 2020 11:46:48 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Kalantidis", "Yannis", ""], ["Sariyildiz", "Mert Bulent", ""], ["Pion", "Noe", ""], ["Weinzaepfel", "Philippe", ""], ["Larlus", "Diane", ""]]}, {"id": "2010.01037", "submitter": "Sanjukta Krishnagopal", "authors": "Sanjukta Krishnagopal and Jacob Bedrossian", "title": "Encoded Prior Sliced Wasserstein AutoEncoder for learning latent\n  manifold representations", "comments": "8 pages, 4 figures in the main text, Submitted to The International\n  Conference on Learning Representations (ICLR)2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While variational autoencoders have been successful generative models for a\nvariety of tasks, the use of conventional Gaussian or Gaussian mixture priors\nare limited in their ability to capture topological or geometric properties of\ndata in the latent representation. In this work, we introduce an Encoded Prior\nSliced Wasserstein AutoEncoder (EPSWAE) wherein an additional prior-encoder\nnetwork learns an unconstrained prior to match the encoded data manifold. The\nautoencoder and prior-encoder networks are iteratively trained using the Sliced\nWasserstein Distance (SWD), which efficiently measures the distance between two\n$\\textit{arbitrary}$ sampleable distributions without being constrained to a\nspecific form as in the KL divergence, and without requiring expensive\nadversarial training. Additionally, we enhance the conventional SWD by\nintroducing a nonlinear shearing, i.e., averaging over random\n$\\textit{nonlinear}$ transformations, to better capture differences between two\ndistributions. The prior is further encouraged to encode the data manifold by\nuse of a structural consistency term that encourages isometry between feature\nspace and latent space. Lastly, interpolation along $\\textit{geodesics}$ on the\nlatent space representation of the data manifold generates samples that lie on\nthe manifold and hence is advantageous compared with standard Euclidean\ninterpolation. To this end, we introduce a graph-based algorithm for\nidentifying network-geodesics in latent space from samples of the prior that\nmaximize the density of samples along the path while minimizing total energy.\nWe apply our framework to 3D-spiral, MNIST, and CelebA datasets, and show that\nits latent representations and interpolations are comparable to the state of\nthe art on equivalent architectures.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 14:58:54 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Krishnagopal", "Sanjukta", ""], ["Bedrossian", "Jacob", ""]]}, {"id": "2010.01040", "submitter": "Samuel Coward", "authors": "Samuel Coward, Erik Visse-Martindale, Chithrupa Ramesh", "title": "Attention-Based Clustering: Learning a Kernel from Context", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In machine learning, no data point stands alone. We believe that context is\nan underappreciated concept in many machine learning methods. We propose\nAttention-Based Clustering (ABC), a neural architecture based on the attention\nmechanism, which is designed to learn latent representations that adapt to\ncontext within an input set, and which is inherently agnostic to input sizes\nand number of clusters. By learning a similarity kernel, our method directly\ncombines with any out-of-the-box kernel-based clustering approach. We present\ncompetitive results for clustering Omniglot characters and include analytical\nevidence of the effectiveness of an attention-based approach for clustering.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 15:06:06 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Coward", "Samuel", ""], ["Visse-Martindale", "Erik", ""], ["Ramesh", "Chithrupa", ""]]}, {"id": "2010.01041", "submitter": "David Niblick", "authors": "David Niblick, Avinash Kak", "title": "Homography Estimation with Convolutional Neural Networks Under\n  Conditions of Variance", "comments": "9 pages, 16 figures, submitted to 2020 Computer Vision and Pattern\n  Recognition Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Planar homography estimation is foundational to many computer vision\nproblems, such as Simultaneous Localization and Mapping (SLAM) and Augmented\nReality (AR). However, conditions of high variance confound even the\nstate-of-the-art algorithms. In this report, we analyze the performance of two\nrecently published methods using Convolutional Neural Networks (CNNs) that are\nmeant to replace the more traditional feature-matching based approaches to the\nestimation of homography. Our evaluation of the CNN based methods focuses\nparticularly on measuring the performance under conditions of significant\nnoise, illumination shift, and occlusion. We also measure the benefits of\ntraining CNNs to varying degrees of noise. Additionally, we compare the effect\nof using color images instead of grayscale images for inputs to CNNs. Finally,\nwe compare the results against baseline feature-matching based homography\nestimation methods using SIFT, SURF, and ORB. We find that CNNs can be trained\nto be more robust against noise, but at a small cost to accuracy in the\nnoiseless case. Additionally, CNNs perform significantly better in conditions\nof extreme variance than their feature-matching based counterparts. With regard\nto color inputs, we conclude that with no change in the CNN architecture to\ntake advantage of the additional information in the color planes, the\ndifference in performance using color inputs or grayscale inputs is negligible.\nAbout the CNNs trained with noise-corrupted inputs, we show that training a CNN\nto a specific magnitude of noise leads to a \"Goldilocks Zone\" with regard to\nthe noise levels where that CNN performs best.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 15:11:25 GMT"}, {"version": "v2", "created": "Thu, 22 Oct 2020 16:05:37 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Niblick", "David", ""], ["Kak", "Avinash", ""]]}, {"id": "2010.01073", "submitter": "Zhao Hengyuan", "authors": "Hengyuan Zhao, Xiangtao Kong, Jingwen He, Yu Qiao and Chao Dong", "title": "Efficient Image Super-Resolution Using Pixel Attention", "comments": "17 pages, 5 figures, conference, accpeted by ECCVW (AIM2020 ESR\n  Challenge)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work aims at designing a lightweight convolutional neural network for\nimage super resolution (SR). With simplicity bare in mind, we construct a\npretty concise and effective network with a newly proposed pixel attention\nscheme. Pixel attention (PA) is similar as channel attention and spatial\nattention in formulation. The difference is that PA produces 3D attention maps\ninstead of a 1D attention vector or a 2D map. This attention scheme introduces\nfewer additional parameters but generates better SR results. On the basis of\nPA, we propose two building blocks for the main branch and the reconstruction\nbranch, respectively. The first one - SC-PA block has the same structure as the\nSelf-Calibrated convolution but with our PA layer. This block is much more\nefficient than conventional residual/dense blocks, for its twobranch\narchitecture and attention scheme. While the second one - UPA block combines\nthe nearest-neighbor upsampling, convolution and PA layers. It improves the\nfinal reconstruction quality with little parameter cost. Our final model- PAN\ncould achieve similar performance as the lightweight networks - SRResNet and\nCARN, but with only 272K parameters (17.92% of SRResNet and 17.09% of CARN).\nThe effectiveness of each proposed component is also validated by ablation\nstudy. The code is available at https://github.com/zhaohengyuan1/PAN.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 16:04:33 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Zhao", "Hengyuan", ""], ["Kong", "Xiangtao", ""], ["He", "Jingwen", ""], ["Qiao", "Yu", ""], ["Dong", "Chao", ""]]}, {"id": "2010.01086", "submitter": "Alina Marcu M.Sc", "authors": "Marius Leordeanu, Mihai Pirvu, Dragos Costea, Alina Marcu, Emil\n  Slusanschi and Rahul Sukthankar", "title": "Semi-Supervised Learning for Multi-Task Scene Understanding by Neural\n  Graph Consensus", "comments": "Accepted at the 35th AAAI Conference on Artificial Intelligence (AAAI\n  2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the challenging problem of semi-supervised learning in the context\nof multiple visual interpretations of the world by finding consensus in a graph\nof neural networks. Each graph node is a scene interpretation layer, while each\nedge is a deep net that transforms one layer at one node into another from a\ndifferent node. During the supervised phase edge networks are trained\nindependently. During the next unsupervised stage edge nets are trained on the\npseudo-ground truth provided by consensus among multiple paths that reach the\nnets' start and end nodes. These paths act as ensemble teachers for any given\nedge and strong consensus is used for high-confidence supervisory signal. The\nunsupervised learning process is repeated over several generations, in which\neach edge becomes a \"student\" and also part of different ensemble \"teachers\"\nfor training other students. By optimizing such consensus between different\npaths, the graph reaches consistency and robustness over multiple\ninterpretations and generations, in the face of unknown labels. We give\ntheoretical justifications of the proposed idea and validate it on a large\ndataset. We show how prediction of different representations such as depth,\nsemantic segmentation, surface normals and pose from RGB input could be\neffectively learned through self-supervised consensus in our graph. We also\ncompare to state-of-the-art methods for multi-task and semi-supervised learning\nand show superior performance.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 16:30:49 GMT"}, {"version": "v2", "created": "Thu, 3 Dec 2020 15:31:41 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Leordeanu", "Marius", ""], ["Pirvu", "Mihai", ""], ["Costea", "Dragos", ""], ["Marcu", "Alina", ""], ["Slusanschi", "Emil", ""], ["Sukthankar", "Rahul", ""]]}, {"id": "2010.01089", "submitter": "Hanchen Wang", "authors": "Hanchen Wang, Qi Liu, Xiangyu Yue, Joan Lasenby, Matthew J. Kusner", "title": "Unsupervised Point Cloud Pre-Training via View-Point Occlusion,\n  Completion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We describe a simple pre-training approach for point clouds. It works in\nthree steps: 1. Mask all points occluded in a camera view; 2. Learn an\nencoder-decoder model to reconstruct the occluded points; 3. Use the encoder\nweights as initialisation for downstream point cloud tasks. We find that even\nwhen we construct a single pre-training dataset (from ModelNet40), this\npre-training method improves accuracy across different datasets and encoders,\non a wide range of downstream tasks. Specifically, we show that our method\noutperforms previous pre-training methods in object classification, and both\npart-based and semantic segmentation tasks. We study the pre-trained features\nand find that they lead to wide downstream minima, have high transformation\ninvariance, and have activations that are highly correlated with part labels.\nCode and data are available at: https://github.com/hansen7/OcCo\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 16:43:14 GMT"}, {"version": "v2", "created": "Tue, 13 Apr 2021 01:20:53 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Wang", "Hanchen", ""], ["Liu", "Qi", ""], ["Yue", "Xiangyu", ""], ["Lasenby", "Joan", ""], ["Kusner", "Matthew J.", ""]]}, {"id": "2010.01091", "submitter": "Franziska Lippoldt", "authors": "Franziska Lippoldt", "title": "Efficient Colon Cancer Grading with Graph Neural Networks", "comments": "10 pages, draft version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dealing with the application of grading colorectal cancer images, this work\nproposes a 3 step pipeline for prediction of cancer levels from a\nhistopathology image. The overall model performs better compared to other state\nof the art methods on the colorectal cancer grading data set and shows\nexcellent performance for the extended colorectal cancer grading set. The\nperformance improvements can be attributed to two main factors: The feature\nselection and graph augmentation method described here are spatially aware, but\noverall pixel position independent. Further, the graph size in terms of nodes\nbecomes stable with respect to the model's prediction and accuracy for\nsufficiently large models. The graph neural network itself consists of three\nconvolutional blocks and linear layers, which is a rather simple design\ncompared to other networks for this application.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 16:43:33 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Lippoldt", "Franziska", ""]]}, {"id": "2010.01097", "submitter": "Kun Yuan", "authors": "Kun Yuan, Quanquan Li, Dapeng Chen, Aojun Zhou and Junjie Yan", "title": "Dynamic Graph: Learning Instance-aware Connectivity for Neural Networks", "comments": "13 pages, 3 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One practice of employing deep neural networks is to apply the same\narchitecture to all the input instances. However, a fixed architecture may not\nbe representative enough for data with high diversity. To promote the model\ncapacity, existing approaches usually employ larger convolutional kernels or\ndeeper network structure, which may increase the computational cost. In this\npaper, we address this issue by raising the Dynamic Graph Network (DG-Net). The\nnetwork learns the instance-aware connectivity, which creates different forward\npaths for different instances. Specifically, the network is initialized as a\ncomplete directed acyclic graph, where the nodes represent convolutional blocks\nand the edges represent the connection paths. We generate edge weights by a\nlearnable module \\textit{router} and select the edges whose weights are larger\nthan a threshold, to adjust the connectivity of the neural network structure.\nInstead of using the same path of the network, DG-Net aggregates features\ndynamically in each node, which allows the network to have more representation\nability. To facilitate the training, we represent the network connectivity of\neach sample in an adjacency matrix. The matrix is updated to aggregate features\nin the forward pass, cached in the memory, and used for gradient computing in\nthe backward pass. We verify the effectiveness of our method with several\nstatic architectures, including MobileNetV2, ResNet, ResNeXt, and RegNet.\nExtensive experiments are performed on ImageNet classification and COCO object\ndetection, which shows the effectiveness and generalization ability of our\napproach.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 16:50:26 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Yuan", "Kun", ""], ["Li", "Quanquan", ""], ["Chen", "Dapeng", ""], ["Zhou", "Aojun", ""], ["Yan", "Junjie", ""]]}, {"id": "2010.01110", "submitter": "Evangelos Ntavelis", "authors": "Evangelos Ntavelis, Andr\\'es Romero, Siavash Bigdeli, Radu Timofte", "title": "AIM 2020 Challenge on Image Extreme Inpainting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper reviews the AIM 2020 challenge on extreme image inpainting. This\nreport focuses on proposed solutions and results for two different tracks on\nextreme image inpainting: classical image inpainting and semantically guided\nimage inpainting. The goal of track 1 is to inpaint considerably large part of\nthe image using no supervision but the context. Similarly, the goal of track 2\nis to inpaint the image by having access to the entire semantic segmentation\nmap of the image to inpaint. The challenge had 88 and 74 participants,\nrespectively. 11 and 6 teams competed in the final phase of the challenge,\nrespectively. This report gauges current solutions and set a benchmark for\nfuture extreme image inpainting methods.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 17:11:17 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Ntavelis", "Evangelos", ""], ["Romero", "Andr\u00e9s", ""], ["Bigdeli", "Siavash", ""], ["Timofte", "Radu", ""]]}, {"id": "2010.01114", "submitter": "Patrick Dendorfer", "authors": "Patrick Dendorfer and Aljo\\v{s}a O\\v{s}ep and Laura Leal-Taix\\'e", "title": "Goal-GAN: Multimodal Trajectory Prediction Based on Goal Position\n  Estimation", "comments": "Oral presentation at ACCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present Goal-GAN, an interpretable and end-to-end trainable\nmodel for human trajectory prediction. Inspired by human navigation, we model\nthe task of trajectory prediction as an intuitive two-stage process: (i) goal\nestimation, which predicts the most likely target positions of the agent,\nfollowed by a (ii) routing module which estimates a set of plausible\ntrajectories that route towards the estimated goal. We leverage information\nabout the past trajectory and visual context of the scene to estimate a\nmulti-modal probability distribution over the possible goal positions, which is\nused to sample a potential goal during the inference. The routing is governed\nby a recurrent neural network that reacts to physical constraints in the nearby\nsurroundings and generates feasible paths that route towards the sampled goal.\nOur extensive experimental evaluation shows that our method establishes a new\nstate-of-the-art on several benchmarks while being able to generate a realistic\nand diverse set of trajectories that conform to physical constraints.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 17:17:45 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Dendorfer", "Patrick", ""], ["O\u0161ep", "Aljo\u0161a", ""], ["Leal-Taix\u00e9", "Laura", ""]]}, {"id": "2010.01131", "submitter": "Soham Shah", "authors": "Soham Shah, Siddhi Vinayak Pandey, Archit Sorathiya, Raj Sheth, Alok\n  Kumar Singh, Jignesh Thaker", "title": "Embedded Systems and Computer Vision Techniques utilized in Spray\n  Painting Robots: A Review", "comments": "8 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The advent of the era of machines has limited human interaction and this has\nincreased their presence in the last decade. The requirement to increase the\neffectiveness, durability and reliability in the robots has also risen quite\ndrastically too. Present paper covers the various embedded system and computer\nvision methodologies, techniques and innovations used in the field of spray\npainting robots. There have been many advancements in the sphere of painting\nrobots utilized for high rise buildings, wall painting, road marking paintings,\netc. Review focuses on image processing, computational and computer vision\ntechniques that can be applied in the product to increase efficiency of the\nperformance drastically. Image analysis, filtering, enhancement, object\ndetection, edge detection methods, path and localization methods and fine\ntuning of parameters are being discussed in depth to use while developing such\nproducts. Dynamic system design is being deliberated by using which results in\nreduction of human interaction, environment sustainability and better quality\nof work in detail. Embedded systems involving the micro-controllers,\nprocessors, communicating devices, sensors and actuators, soft-ware to use\nthem; is being explained for end-to-end development and enhancement of accuracy\nand precision in Spray Painting Robots.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 17:59:03 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Shah", "Soham", ""], ["Pandey", "Siddhi Vinayak", ""], ["Sorathiya", "Archit", ""], ["Sheth", "Raj", ""], ["Singh", "Alok Kumar", ""], ["Thaker", "Jignesh", ""]]}, {"id": "2010.01148", "submitter": "Chih-Ting Liu", "authors": "Chih-Ting Liu, Yu-Jhe Li, Shao-Yi Chien, Yu-Chiang Frank Wang", "title": "Semantics-Guided Clustering with Deep Progressive Learning for\n  Semi-Supervised Person Re-identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification (re-ID) requires one to match images of the same\nperson across camera views. As a more challenging task, semi-supervised re-ID\ntackles the problem that only a number of identities in training data are fully\nlabeled, while the remaining are unlabeled. Assuming that such labeled and\nunlabeled training data share disjoint identity labels, we propose a novel\nframework of Semantics-Guided Clustering with Deep Progressive Learning\n(SGC-DPL) to jointly exploit the above data. By advancing the proposed\nSemantics-Guided Affinity Propagation (SG-AP), we are able to assign\npseudo-labels to selected unlabeled data in a progressive fashion, under the\nsemantics guidance from the labeled ones. As a result, our approach is able to\naugment the labeled training data in the semi-supervised setting. Our\nexperiments on two large-scale person re-ID benchmarks demonstrate the\nsuperiority of our SGC-DPL over state-of-the-art methods across different\ndegrees of supervision. In extension, the generalization ability of our SGC-DPL\nis also verified in other tasks like vehicle re-ID or image retrieval with the\nsemi-supervised setting.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 18:02:35 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Liu", "Chih-Ting", ""], ["Li", "Yu-Jhe", ""], ["Chien", "Shao-Yi", ""], ["Wang", "Yu-Chiang Frank", ""]]}, {"id": "2010.01158", "submitter": "Zhenyu Wu", "authors": "Zhenyu Wu, Duc Hoang, Shih-Yao Lin, Yusheng Xie, Liangjian Chen,\n  Yen-Yu Lin, Zhangyang Wang, Wei Fan", "title": "MM-Hand: 3D-Aware Multi-Modal Guided Hand Generative Network for 3D Hand\n  Pose Synthesis", "comments": "Accepted by ACM Multimedia 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating the 3D hand pose from a monocular RGB image is important but\nchallenging. A solution is training on large-scale RGB hand images with\naccurate 3D hand keypoint annotations. However, it is too expensive in\npractice. Instead, we have developed a learning-based approach to synthesize\nrealistic, diverse, and 3D pose-preserving hand images under the guidance of 3D\npose information. We propose a 3D-aware multi-modal guided hand generative\nnetwork (MM-Hand), together with a novel geometry-based curriculum learning\nstrategy. Our extensive experimental results demonstrate that the 3D-annotated\nimages generated by MM-Hand qualitatively and quantitatively outperform\nexisting options. Moreover, the augmented data can consistently improve the\nquantitative performance of the state-of-the-art 3D hand pose estimators on two\nbenchmark datasets. The code will be available at\nhttps://github.com/ScottHoang/mm-hand.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 18:27:34 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Wu", "Zhenyu", ""], ["Hoang", "Duc", ""], ["Lin", "Shih-Yao", ""], ["Xie", "Yusheng", ""], ["Chen", "Liangjian", ""], ["Lin", "Yen-Yu", ""], ["Wang", "Zhangyang", ""], ["Fan", "Wei", ""]]}, {"id": "2010.01163", "submitter": "Renat Sergazinov", "authors": "Renat Sergazinov, Miroslav Kramar", "title": "Machine learning approach to force reconstruction in photoelastic\n  materials", "comments": "20 pages, 6 figures, 2 tables; changed formatting of the tables;\n  reduced picture resolutions", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV physics.app-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Photoelastic techniques have a long tradition in both qualitative and\nquantitative analysis of the stresses in granular materials. Over the last two\ndecades, computational methods for reconstructing forces between particles from\ntheir photoelastic response have been developed by many different experimental\nteams. Unfortunately, all of these methods are computationally expensive. This\nlimits their use for processing extensive data sets that capture the time\nevolution of granular ensembles consisting of a large number of particles. In\nthis paper, we present a novel approach to this problem which leverages the\npower of convolutional neural networks to recognize complex spatial patterns.\nThe main drawback of using neural networks is that training them usually\nrequires a large labeled data set which is hard to obtain experimentally. We\nshow that this problem can be successfully circumvented by pretraining the\nnetworks on a large synthetic data set and then fine-tuning them on much\nsmaller experimental data sets. Due to our current lack of experimental data,\nwe demonstrate the potential of our method by changing the size of the\nconsidered particles which alters the exhibited photoelastic patterns more than\ntypical experimental errors.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 18:50:20 GMT"}, {"version": "v2", "created": "Sat, 17 Oct 2020 21:54:18 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Sergazinov", "Renat", ""], ["Kramar", "Miroslav", ""]]}, {"id": "2010.01173", "submitter": "Sumeet Menon", "authors": "Sumeet Menon, David Chapman, Phuong Nguyen, Yelena Yesha, Michael\n  Morris, Babak Saboury", "title": "Deep Expectation-Maximization for Semi-Supervised Lung Cancer Screening", "comments": "This paper has been accepted at the ACM SIGKDD Workshop DCCL 2019.\n  https://sites.google.com/view/kdd-workshop-2019/accepted-papers\n  https://drive.google.com/file/d/0B8FX-5qN3tbjM3c4SVZDYWxjbGhCekhjUV9PUC11b3dOSXRR/view", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a semi-supervised algorithm for lung cancer screening in which a\n3D Convolutional Neural Network (CNN) is trained using the\nExpectation-Maximization (EM) meta-algorithm. Semi-supervised learning allows a\nsmaller labelled data-set to be combined with an unlabeled data-set in order to\nprovide a larger and more diverse training sample. EM allows the algorithm to\nsimultaneously calculate a maximum likelihood estimate of the CNN training\ncoefficients along with the labels for the unlabeled training set which are\ndefined as a latent variable space. We evaluate the model performance of the\nSemi-Supervised EM algorithm for CNNs through cross-domain training of the\nKaggle Data Science Bowl 2017 (Kaggle17) data-set with the National Lung\nScreening Trial (NLST) data-set. Our results show that the Semi-Supervised EM\nalgorithm greatly improves the classification accuracy of the cross-domain lung\ncancer screening, although results are lower than a fully supervised approach\nwith the advantage of additional labelled data from the unsupervised sample. As\nsuch, we demonstrate that Semi-Supervised EM is a valuable technique to improve\nthe accuracy of lung cancer screening models using 3D CNNs.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 19:17:07 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Menon", "Sumeet", ""], ["Chapman", "David", ""], ["Nguyen", "Phuong", ""], ["Yesha", "Yelena", ""], ["Morris", "Michael", ""], ["Saboury", "Babak", ""]]}, {"id": "2010.01177", "submitter": "Dmitry V. Dylov", "authors": "Viktor Shipitsin, Iaroslav Bespalov, Dmitry V. Dylov", "title": "Global Adaptive Filtering Layer for Computer Vision", "comments": "28 pages, 25 figures (main article and supplementary material). V.S.\n  and I.B contributed equally, D.V.D is Corresponding author", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We devise a universal adaptive neural layer to \"learn\" optimal frequency\nfilter for each image together with the weights of the base neural network that\nperforms some computer vision task. The proposed approach takes the source\nimage in the spatial domain, automatically selects the best frequencies from\nthe frequency domain, and transmits the inverse-transform image to the main\nneural network. Remarkably, such a simple add-on layer dramatically improves\nthe performance of the main network regardless of its design. We observe that\nthe light networks gain a noticeable boost in the performance metrics; whereas,\nthe training of the heavy ones converges faster when our adaptive layer is\nallowed to \"learn\" alongside the main architecture. We validate the idea in\nfour classical computer vision tasks: classification, segmentation, denoising,\nand erasing, considering popular natural and medical data benchmarks.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 19:43:49 GMT"}, {"version": "v2", "created": "Fri, 4 Dec 2020 09:45:11 GMT"}, {"version": "v3", "created": "Mon, 29 Mar 2021 20:42:24 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Shipitsin", "Viktor", ""], ["Bespalov", "Iaroslav", ""], ["Dylov", "Dmitry V.", ""]]}, {"id": "2010.01191", "submitter": "Vincent Cartillier", "authors": "Vincent Cartillier, Zhile Ren, Neha Jain, Stefan Lee, Irfan Essa,\n  Dhruv Batra", "title": "Semantic MapNet: Building Allocentric Semantic Maps and Representations\n  from Egocentric Views", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the task of semantic mapping - specifically, an embodied agent (a\nrobot or an egocentric AI assistant) is given a tour of a new environment and\nasked to build an allocentric top-down semantic map (\"what is where?\") from\negocentric observations of an RGB-D camera with known pose (via localization\nsensors). Towards this goal, we present SemanticMapNet (SMNet), which consists\nof: (1) an Egocentric Visual Encoder that encodes each egocentric RGB-D frame,\n(2) a Feature Projector that projects egocentric features to appropriate\nlocations on a floor-plan, (3) a Spatial Memory Tensor of size floor-plan\nlength x width x feature-dims that learns to accumulate projected egocentric\nfeatures, and (4) a Map Decoder that uses the memory tensor to produce semantic\ntop-down maps. SMNet combines the strengths of (known) projective camera\ngeometry and neural representation learning. On the task of semantic mapping in\nthe Matterport3D dataset, SMNet significantly outperforms competitive baselines\nby 4.01-16.81% (absolute) on mean-IoU and 3.81-19.69% (absolute) on Boundary-F1\nmetrics. Moreover, we show how to use the neural episodic memories and\nspatio-semantic allocentric representations build by SMNet for subsequent tasks\nin the same space - navigating to objects seen during the tour(\"Find chair\") or\nanswering questions about the space (\"How many chairs did you see in the\nhouse?\"). Project page: https://vincentcartillier.github.io/smnet.html.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 20:44:46 GMT"}, {"version": "v2", "created": "Thu, 4 Mar 2021 05:10:53 GMT"}, {"version": "v3", "created": "Thu, 11 Mar 2021 00:26:51 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Cartillier", "Vincent", ""], ["Ren", "Zhile", ""], ["Jain", "Neha", ""], ["Lee", "Stefan", ""], ["Essa", "Irfan", ""], ["Batra", "Dhruv", ""]]}, {"id": "2010.01202", "submitter": "John Sigman PhD.", "authors": "John B. Sigman, Gregory P. Spell, Kevin J Liang, and Lawrence Carin", "title": "Background Adaptive Faster R-CNN for Semi-Supervised Convolutional\n  Object Detection of Threats in X-Ray Images", "comments": null, "journal-ref": "Proc. SPIE 11404, Anomaly Detection and Imaging with X-Rays (ADIX)\n  V, 1140404 (26 May 2020)", "doi": "10.1117/12.2558542", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently, progress has been made in the supervised training of Convolutional\nObject Detectors (e.g. Faster R-CNN) for threat recognition in carry-on luggage\nusing X-ray images. This is part of the Transportation Security\nAdministration's (TSA's) mission to protect air travelers in the United States.\nWhile more training data with threats may reliably improve performance for this\nclass of deep algorithm, it is expensive to stage in realistic contexts. By\ncontrast, data from the real world can be collected quickly with minimal cost.\nIn this paper, we present a semi-supervised approach for threat recognition\nwhich we call Background Adaptive Faster R-CNN. This approach is a training\nmethod for two-stage object detectors which uses Domain Adaptation methods from\nthe field of deep learning. The data sources described earlier make two\n\"domains\": a hand-collected data domain of images with threats, and a\nreal-world domain of images assumed without threats. Two domain discriminators,\none for discriminating object proposals and one for image features, are\nadversarially trained to prevent encoding domain-specific information. Without\nthis penalty a Convolutional Neural Network (CNN) can learn to identify domains\nbased on superficial characteristics, and minimize a supervised loss function\nwithout improving its ability to recognize objects. For the hand-collected\ndata, only object proposals and image features from backgrounds are used. The\nlosses for these domain-adaptive discriminators are added to the Faster R-CNN\nlosses of images from both domains. This can reduce threat detection false\nalarm rates by matching the statistics of extracted features from\nhand-collected backgrounds to real world data. Performance improvements are\ndemonstrated on two independently-collected datasets of labeled threats.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 21:05:13 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Sigman", "John B.", ""], ["Spell", "Gregory P.", ""], ["Liang", "Kevin J", ""], ["Carin", "Lawrence", ""]]}, {"id": "2010.01204", "submitter": "Kourosh Meshgi", "authors": "Kourosh Meshgi, Maryam Sadat Mirzaei, Shigeyuki Oba", "title": "Leveraging Tacit Information Embedded in CNN Layers for Visual Tracking", "comments": "ACCV 2020 Camera Ready Extended Version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Different layers in CNNs provide not only different levels of abstraction for\ndescribing the objects in the input but also encode various implicit\ninformation about them. The activation patterns of different features contain\nvaluable information about the stream of incoming images: spatial relations,\ntemporal patterns, and co-occurrence of spatial and spatiotemporal (ST)\nfeatures. The studies in visual tracking literature, so far, utilized only one\nof the CNN layers, a pre-fixed combination of them, or an ensemble of trackers\nbuilt upon individual layers. In this study, we employ an adaptive combination\nof several CNN layers in a single DCF tracker to address variations of the\ntarget appearances and propose the use of style statistics on both spatial and\ntemporal properties of the target, directly extracted from CNN layers for\nvisual tracking. Experiments demonstrate that using the additional implicit\ndata of CNNs significantly improves the performance of the tracker. Results\ndemonstrate the effectiveness of using style similarity and activation\nconsistency regularization in improving its localization and scale accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 21:16:26 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Meshgi", "Kourosh", ""], ["Mirzaei", "Maryam Sadat", ""], ["Oba", "Shigeyuki", ""]]}, {"id": "2010.01217", "submitter": "Vishal Mandal", "authors": "Vishal Mandal, Abdul Rashid Mussah, Peng Jin, Yaw Adu-Gyamfi", "title": "Artificial Intelligence Enabled Traffic Monitoring System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Manual traffic surveillance can be a daunting task as Traffic Management\nCenters operate a myriad of cameras installed over a network. Injecting some\nlevel of automation could help lighten the workload of human operators\nperforming manual surveillance and facilitate making proactive decisions which\nwould reduce the impact of incidents and recurring congestion on roadways. This\narticle presents a novel approach to automatically monitor real time traffic\nfootage using deep convolutional neural networks and a stand-alone graphical\nuser interface. The authors describe the results of research received in the\nprocess of developing models that serve as an integrated framework for an\nartificial intelligence enabled traffic monitoring system. The proposed system\ndeploys several state-of-the-art deep learning algorithms to automate different\ntraffic monitoring needs. Taking advantage of a large database of annotated\nvideo surveillance data, deep learning-based models are trained to detect\nqueues, track stationary vehicles, and tabulate vehicle counts. A pixel-level\nsegmentation approach is applied to detect traffic queues and predict severity.\nReal-time object detection algorithms coupled with different tracking systems\nare deployed to automatically detect stranded vehicles as well as perform\nvehicular counts. At each stages of development, interesting experimental\nresults are presented to demonstrate the effectiveness of the proposed system.\nOverall, the results demonstrate that the proposed framework performs\nsatisfactorily under varied conditions without being immensely impacted by\nenvironmental hazards such as blurry camera views, low illumination, rain, or\nsnow.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 22:28:02 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Mandal", "Vishal", ""], ["Mussah", "Abdul Rashid", ""], ["Jin", "Peng", ""], ["Adu-Gyamfi", "Yaw", ""]]}, {"id": "2010.01220", "submitter": "Simone Palazzo", "authors": "Giovanni Bellitto, Federica Proietto Salanitri, Simone Palazzo,\n  Francesco Rundo, Daniela Giordano, Concetto Spampinato", "title": "Hierarchical Domain-Adapted Feature Learning for Video Saliency\n  Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a 3D fully convolutional architecture for video\nsaliency prediction that employs hierarchical supervision on intermediate maps\n(referred to as conspicuity maps) generated using features extracted at\ndifferent abstraction levels. We provide the base hierarchical learning\nmechanism with two techniques for domain adaptation and domain-specific\nlearning. For the former, we encourage the model to unsupervisedly learn\nhierarchical general features using gradient reversal at multiple scales, to\nenhance generalization capabilities on datasets for which no annotations are\nprovided during training. As for domain specialization, we employ\ndomain-specific operations (namely, priors, smoothing and batch normalization)\nby specializing the learned features on individual datasets in order to\nmaximize performance. The results of our experiments show that the proposed\nmodel yields state-of-the-art accuracy on supervised saliency prediction. When\nthe base hierarchical model is empowered with domain-specific modules,\nperformance improves, outperforming state-of-the-art models on three out of\nfive metrics on the DHF1K benchmark and reaching the second-best results on the\nother two. When, instead, we test it in an unsupervised domain adaptation\nsetting, by enabling hierarchical gradient reversal layers, we obtain\nperformance comparable to supervised state-of-the-art.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 23:00:00 GMT"}, {"version": "v2", "created": "Tue, 6 Oct 2020 10:42:27 GMT"}, {"version": "v3", "created": "Wed, 5 May 2021 11:08:28 GMT"}, {"version": "v4", "created": "Thu, 6 May 2021 08:09:36 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Bellitto", "Giovanni", ""], ["Salanitri", "Federica Proietto", ""], ["Palazzo", "Simone", ""], ["Rundo", "Francesco", ""], ["Giordano", "Daniela", ""], ["Spampinato", "Concetto", ""]]}, {"id": "2010.01231", "submitter": "Arun Das", "authors": "Arun Das, Jeffrey Mock, Henry Chacon, Farzan Irani, Edward Golob,\n  Peyman Najafirad", "title": "Stuttering Speech Disfluency Prediction using Explainable Attribution\n  Vectors of Facial Muscle Movements", "comments": "Submitting to IEEE Trans. 10 pages, 7 figures. Final Manuscript", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Speech disorders such as stuttering disrupt the normal fluency of speech by\ninvoluntary repetitions, prolongations and blocking of sounds and syllables. In\naddition to these disruptions to speech fluency, most adults who stutter (AWS)\nalso experience numerous observable secondary behaviors before, during, and\nafter a stuttering moment, often involving the facial muscles. Recent studies\nhave explored automatic detection of stuttering using Artificial Intelligence\n(AI) based algorithm from respiratory rate, audio, etc. during speech\nutterance. However, most methods require controlled environments and/or\ninvasive wearable sensors, and are unable explain why a decision (fluent vs\nstuttered) was made. We hypothesize that pre-speech facial activity in AWS,\nwhich can be captured non-invasively, contains enough information to accurately\nclassify the upcoming utterance as either fluent or stuttered. Towards this\nend, this paper proposes a novel explainable AI (XAI) assisted convolutional\nneural network (CNN) classifier to predict near future stuttering by learning\ntemporal facial muscle movement patterns of AWS and explains the important\nfacial muscles and actions involved. Statistical analyses reveal significantly\nhigh prevalence of cheek muscles (p<0.005) and lip muscles (p<0.005) to predict\nstuttering and shows a behavior conducive of arousal and anticipation to speak.\nThe temporal study of these upper and lower facial muscles may facilitate early\ndetection of stuttering, promote automated assessment of stuttering and have\napplication in behavioral therapies by providing automatic non-invasive\nfeedback in realtime.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 23:45:41 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Das", "Arun", ""], ["Mock", "Jeffrey", ""], ["Chacon", "Henry", ""], ["Irani", "Farzan", ""], ["Golob", "Edward", ""], ["Najafirad", "Peyman", ""]]}, {"id": "2010.01238", "submitter": "Gustavo Olague Dr.", "authors": "Gustavo Olague and Gerardo Ibarra-Vazquez and Mariana Chan-Ley and\n  Cesar Puente and Carlos Soubervielle-Montalvo and Axel Martinez", "title": "A Deep Genetic Programming based Methodology for Art Media\n  Classification Robust to Adversarial Perturbations", "comments": "13 pages, 3 figures, International Symposium on Visual Computing 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.SC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Art Media Classification problem is a current research area that has\nattracted attention due to the complex extraction and analysis of features of\nhigh-value art pieces. The perception of the attributes can not be subjective,\nas humans sometimes follow a biased interpretation of artworks while ensuring\nautomated observation's trustworthiness. Machine Learning has outperformed many\nareas through its learning process of artificial feature extraction from images\ninstead of designing handcrafted feature detectors. However, a major concern\nrelated to its reliability has brought attention because, with small\nperturbations made intentionally in the input image (adversarial attack), its\nprediction can be completely changed. In this manner, we foresee two ways of\napproaching the situation: (1) solve the problem of adversarial attacks in\ncurrent neural networks methodologies, or (2) propose a different approach that\ncan challenge deep learning without the effects of adversarial attacks. The\nfirst one has not been solved yet, and adversarial attacks have become even\nmore complex to defend. Therefore, this work presents a Deep Genetic\nProgramming method, called Brain Programming, that competes with deep learning\nand studies the transferability of adversarial attacks using two artworks\ndatabases made by art experts. The results show that the Brain Programming\nmethod preserves its performance in comparison with AlexNet, making it robust\nto these perturbations and competing to the performance of Deep Learning.\n", "versions": [{"version": "v1", "created": "Sat, 3 Oct 2020 00:36:34 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Olague", "Gustavo", ""], ["Ibarra-Vazquez", "Gerardo", ""], ["Chan-Ley", "Mariana", ""], ["Puente", "Cesar", ""], ["Soubervielle-Montalvo", "Carlos", ""], ["Martinez", "Axel", ""]]}, {"id": "2010.01242", "submitter": "Kevin Bui", "authors": "Kevin Bui, Fredrick Park, Shuai Zhang, Yingyong Qi, Jack Xin", "title": "Improving Network Slimming with Nonconvex Regularization", "comments": "version1 published in ISVC'20; version 2: fixed typo; version3 is the\n  extended version and submitted to a journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Convolutional neural networks (CNNs) have developed to become powerful models\nfor various computer vision tasks ranging from object detection to semantic\nsegmentation. However, most of state-of-the-art CNNs can not be deployed\ndirectly on edge devices such as smartphones and drones, which need low latency\nunder limited power and memory bandwidth. One popular, straightforward approach\nto compressing CNNs is network slimming, which imposes $\\ell_1$ regularization\non the channel-associated scaling factors via the batch normalization layers\nduring training. Network slimming thereby identifies insignificant channels\nthat can be pruned for inference. In this paper, we propose replacing the\n$\\ell_1$ penalty with an alternative sparse, nonconvex penalty in order to\nyield a more compressed and/or accurate CNN architecture. We investigate\n$\\ell_p (0 < p < 1)$, transformed $\\ell_1$ (T$\\ell_1$), minimax concave penalty\n(MCP), and smoothly clipped absolute deviation (SCAD) due to their recent\nsuccesses and popularity in solving sparse optimization problems, such as\ncompressed sensing and variable selection. We demonstrate the effectiveness of\nnetwork slimming with nonconvex penalties on VGGNet, Densenet, and Resnet on\nstandard image classification datasets. Based on the numerical experiments,\nT$\\ell_1$ preserves model accuracy against channel pruning, $\\ell_{1/2, 3/4}$\nyield better compressed models with similar accuracies after retraining as\n$\\ell_1$, and MCP and SCAD provide more accurate models after retraining with\nsimilar compression as $\\ell_1$. Network slimming with T$\\ell_1$ regularization\nalso outperforms the latest Bayesian modification of network slimming in\ncompressing a CNN architecture in terms of memory storage while preserving its\nmodel accuracy after channel pruning.\n", "versions": [{"version": "v1", "created": "Sat, 3 Oct 2020 01:04:02 GMT"}, {"version": "v2", "created": "Thu, 22 Apr 2021 06:11:11 GMT"}, {"version": "v3", "created": "Thu, 24 Jun 2021 03:44:36 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Bui", "Kevin", ""], ["Park", "Fredrick", ""], ["Zhang", "Shuai", ""], ["Qi", "Yingyong", ""], ["Xin", "Jack", ""]]}, {"id": "2010.01245", "submitter": "Aniket Anand Deshmukh", "authors": "Jayanth Reddy Regatti, Aniket Anand Deshmukh, Eren Manavoglu, Urun\n  Dogan", "title": "Consensus Clustering With Unsupervised Representation Learning", "comments": "Accepted by the 2021 International Joint Conference on Neural\n  Networks (IJCNN 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in deep clustering and unsupervised representation learning\nare based on the idea that different views of an input image (generated through\ndata augmentation techniques) must either be closer in the representation\nspace, or have a similar cluster assignment. Bootstrap Your Own Latent (BYOL)\nis one such representation learning algorithm that has achieved\nstate-of-the-art results in self-supervised image classification on ImageNet\nunder the linear evaluation protocol. However, the utility of the learnt\nfeatures of BYOL to perform clustering is not explored. In this work, we study\nthe clustering ability of BYOL and observe that features learnt using BYOL may\nnot be optimal for clustering. We propose a novel consensus clustering based\nloss function, and train BYOL with the proposed loss in an end-to-end way that\nimproves the clustering ability and outperforms similar clustering based\nmethods on some popular computer vision datasets.\n", "versions": [{"version": "v1", "created": "Sat, 3 Oct 2020 01:16:46 GMT"}, {"version": "v2", "created": "Thu, 8 Jul 2021 17:20:52 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Regatti", "Jayanth Reddy", ""], ["Deshmukh", "Aniket Anand", ""], ["Manavoglu", "Eren", ""], ["Dogan", "Urun", ""]]}, {"id": "2010.01246", "submitter": "Yifan Xing", "authors": "Yifan Xing, Yuanjun Xiong, Wei Xia", "title": "3D-Aided Data Augmentation for Robust Face Understanding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data augmentation has been highly effective in narrowing the data gap and\nreducing the cost for human annotation, especially for tasks where ground truth\nlabels are difficult and expensive to acquire. In face recognition, large pose\nand illumination variation of face images has been a key factor for performance\ndegradation. However, human annotation for the various face understanding tasks\nincluding face landmark localization, face attributes classification and face\nrecognition under these challenging scenarios are highly costly to acquire.\nTherefore, it would be desirable to perform data augmentation for these cases.\nBut simple 2D data augmentation techniques on the image domain are not able to\nsatisfy the requirement of these challenging cases. As such, 3D face modeling,\nin particular, single image 3D face modeling, stands a feasible solution for\nthese challenging conditions beyond 2D based data augmentation. To this end, we\npropose a method that produces realistic 3D augmented images from multiple\nviewpoints with different illumination conditions through 3D face modeling,\neach associated with geometrically accurate face landmarks, attributes and\nidentity information. Experiments demonstrate that the proposed 3D data\naugmentation method significantly improves the performance and robustness of\nvarious face understanding tasks while achieving state-of-arts on multiple\nbenchmarks.\n", "versions": [{"version": "v1", "created": "Sat, 3 Oct 2020 01:18:07 GMT"}, {"version": "v2", "created": "Tue, 6 Oct 2020 02:55:36 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Xing", "Yifan", ""], ["Xiong", "Yuanjun", ""], ["Xia", "Wei", ""]]}, {"id": "2010.01250", "submitter": "Zhichao Huang", "authors": "Zhichao Huang, Yaowei Huang, Tong Zhang", "title": "CorrAttack: Black-box Adversarial Attack with Structured Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new method for score-based adversarial attack, where the\nattacker queries the loss-oracle of the target model. Our method employs a\nparameterized search space with a structure that captures the relationship of\nthe gradient of the loss function. We show that searching over the structured\nspace can be approximated by a time-varying contextual bandits problem, where\nthe attacker takes feature of the associated arm to make modifications of the\ninput, and receives an immediate reward as the reduction of the loss function.\nThe time-varying contextual bandits problem can then be solved by a Bayesian\noptimization procedure, which can take advantage of the features of the\nstructured action space. The experiments on ImageNet and the Google Cloud\nVision API demonstrate that the proposed method achieves the state of the art\nsuccess rates and query efficiencies for both undefended and defended models.\n", "versions": [{"version": "v1", "created": "Sat, 3 Oct 2020 01:44:16 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Huang", "Zhichao", ""], ["Huang", "Yaowei", ""], ["Zhang", "Tong", ""]]}, {"id": "2010.01251", "submitter": "Jingfei Chang", "authors": "Jingfei Chang and Yang Lu and Ping Xue and Xing Wei and Zhen Wei", "title": "UCP: Uniform Channel Pruning for Deep Convolutional Neural Networks\n  Compression and Acceleration", "comments": "21 pages,7 figures and 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To apply deep CNNs to mobile terminals and portable devices, many scholars\nhave recently worked on the compressing and accelerating deep convolutional\nneural networks. Based on this, we propose a novel uniform channel pruning\n(UCP) method to prune deep CNN, and the modified squeeze-and-excitation blocks\n(MSEB) is used to measure the importance of the channels in the convolutional\nlayers. The unimportant channels, including convolutional kernels related to\nthem, are pruned directly, which greatly reduces the storage cost and the\nnumber of calculations. There are two types of residual blocks in ResNet. For\nResNet with bottlenecks, we use the pruning method with traditional CNN to trim\nthe 3x3 convolutional layer in the middle of the blocks. For ResNet with basic\nresidual blocks, we propose an approach to consistently prune all residual\nblocks in the same stage to ensure that the compact network structure is\ndimensionally correct. Considering that the network loses considerable\ninformation after pruning and that the larger the pruning amplitude is, the\nmore information that will be lost, we do not choose fine-tuning but retrain\nfrom scratch to restore the accuracy of the network after pruning. Finally, we\nverified our method on CIFAR-10, CIFAR-100 and ILSVRC-2012 for image\nclassification. The results indicate that the performance of the compact\nnetwork after retraining from scratch, when the pruning rate is small, is\nbetter than the original network. Even when the pruning amplitude is large, the\naccuracy can be maintained or decreased slightly. On the CIFAR-100, when\nreducing the parameters and FLOPs up to 82% and 62% respectively, the accuracy\nof VGG-19 even improved by 0.54% after retraining.\n", "versions": [{"version": "v1", "created": "Sat, 3 Oct 2020 01:51:06 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Chang", "Jingfei", ""], ["Lu", "Yang", ""], ["Xue", "Ping", ""], ["Wei", "Xing", ""], ["Wei", "Zhen", ""]]}, {"id": "2010.01267", "submitter": "Yi Xu", "authors": "Yi Xu, Asaf Noy, Ming Lin, Qi Qian, Hao Li, Rong Jin", "title": "WeMix: How to Better Utilize Data Augmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data augmentation is a widely used training trick in deep learning to improve\nthe network generalization ability. Despite many encouraging results, several\nrecent studies did point out limitations of the conventional data augmentation\nscheme in certain scenarios, calling for a better theoretical understanding of\ndata augmentation. In this work, we develop a comprehensive analysis that\nreveals pros and cons of data augmentation. The main limitation of data\naugmentation arises from the data bias, i.e. the augmented data distribution\ncan be quite different from the original one. This data bias leads to a\nsuboptimal performance of existing data augmentation methods. To this end, we\ndevelop two novel algorithms, termed \"AugDrop\" and \"MixLoss\", to correct the\ndata bias in the data augmentation. Our theoretical analysis shows that both\nalgorithms are guaranteed to improve the effect of data augmentation through\nthe bias correction, which is further validated by our empirical studies.\nFinally, we propose a generic algorithm \"WeMix\" by combining AugDrop and\nMixLoss, whose effectiveness is observed from extensive empirical evaluations.\n", "versions": [{"version": "v1", "created": "Sat, 3 Oct 2020 03:12:18 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Xu", "Yi", ""], ["Noy", "Asaf", ""], ["Lin", "Ming", ""], ["Qian", "Qi", ""], ["Li", "Hao", ""], ["Jin", "Rong", ""]]}, {"id": "2010.01282", "submitter": "Chao Tan", "authors": "Chao Tan", "title": "TCLNet: Learning to Locate Typhoon Center Using Deep Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of typhoon center location plays an important role in typhoon\nintensity analysis and typhoon path prediction. Conventional typhoon center\nlocation algorithms mostly rely on digital image processing and mathematical\nmorphology operation, which achieve limited performance. In this paper, we\nproposed an efficient fully convolutional end-to-end deep neural network named\nTCLNet to automatically locate the typhoon center position. We design the\nnetwork structure carefully so that our TCLNet can achieve remarkable\nperformance base on its lightweight architecture. In addition, we also present\na brand new large-scale typhoon center location dataset (TCLD) so that the\nTCLNet can be trained in a supervised manner. Furthermore, we propose to use a\nnovel TCL+ piecewise loss function to further improve the performance of\nTCLNet. Extensive experimental results and comparison demonstrate the\nperformance of our model, and our TCLNet achieve a 14.4% increase in accuracy\non the basis of a 92.7% reduction in parameters compared with SOTA deep\nlearning based typhoon center location methods.\n", "versions": [{"version": "v1", "created": "Sat, 3 Oct 2020 05:34:00 GMT"}, {"version": "v2", "created": "Sun, 30 May 2021 07:19:32 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Tan", "Chao", ""]]}, {"id": "2010.01283", "submitter": "Chao Tan", "authors": "Chao Tan", "title": "Generating the Cloud Motion Winds Field from Satellite Cloud Imagery\n  Using Deep Learning Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud motion winds (CMW) are routinely derived by tracking features in\nsequential geostationary satellite infrared cloud imagery. In this paper, we\nexplore the cloud motion winds algorithm based on data-driven deep learning\napproach, and different from conventional hand-craft feature tracking and\ncorrelation matching algorithms, we use deep learning model to automatically\nlearn the motion feature representations and directly output the field of cloud\nmotion winds. In addition, we propose a novel large-scale cloud motion winds\ndataset (CMWD) for training deep learning models. We also try to use a single\ncloud imagery to predict the cloud motion winds field in a fixed region, which\nis impossible to achieve using traditional algorithms. The experimental results\ndemonstrate that our algorithm can predict the cloud motion winds field\nefficiently, and even with a single cloud imagery as input.\n", "versions": [{"version": "v1", "created": "Sat, 3 Oct 2020 05:40:36 GMT"}, {"version": "v2", "created": "Sun, 30 May 2021 07:21:57 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Tan", "Chao", ""]]}, {"id": "2010.01291", "submitter": "Chao Tan", "authors": "Chao Tan, Xin Feng", "title": "Unsupervised Shadow Removal Using Target Consistency Generative\n  Adversarial Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised shadow removal aims to learn a non-linear function to map the\noriginal image from shadow domain to non-shadow domain in the absence of paired\nshadow and non-shadow data. In this paper, we develop a simple yet efficient\ntarget-consistency generative adversarial network (TC-GAN) for the shadow\nremoval task in the unsupervised manner. Compared with the bidirectional\nmapping in cycle-consistency GAN based methods for shadow removal, TC-GAN tries\nto learn a one-sided mapping to cast shadow images into shadow-free ones. With\nthe proposed target-consistency constraint, the correlations between shadow\nimages and the output shadow-free image are strictly confined. Extensive\ncomparison experiments results show that TC-GAN outperforms the\nstate-of-the-art unsupervised shadow removal methods by 14.9% in terms of FID\nand 31.5% in terms of KID. It is rather remarkable that TC-GAN achieves\ncomparable performance with supervised shadow removal methods.\n", "versions": [{"version": "v1", "created": "Sat, 3 Oct 2020 06:55:26 GMT"}, {"version": "v2", "created": "Sun, 30 May 2021 08:07:03 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Tan", "Chao", ""], ["Feng", "Xin", ""]]}, {"id": "2010.01301", "submitter": "Hafiq Anas", "authors": "Hafiq Anas, Bacha Rehman, Wee Hong Ong", "title": "Deep Convolutional Neural Network Based Facial Expression Recognition in\n  the Wild", "comments": "3 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper describes the proposed methodology, data used and the results of\nour participation in the ChallengeTrack 2 (Expr Challenge Track) of the\nAffective Behavior Analysis in-the-wild (ABAW) Competition 2020. In this\ncompetition, we have used a proposed deep convolutional neural network (CNN)\nmodel to perform automatic facial expression recognition (AFER) on the given\ndataset. Our proposed model has achieved an accuracy of 50.77% and an F1 score\nof 29.16% on the validation set.\n", "versions": [{"version": "v1", "created": "Sat, 3 Oct 2020 08:17:00 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Anas", "Hafiq", ""], ["Rehman", "Bacha", ""], ["Ong", "Wee Hong", ""]]}, {"id": "2010.01305", "submitter": "Kun Zhao", "authors": "Kun Zhao, Yongkun Liu, Siyuan Hao, Shaoxing Lu, Hongbin Liu, Lijian\n  Zhou", "title": "Bounding Boxes Are All We Need: Street View Image Classification via\n  Context Encoding of Detected Buildings", "comments": "Figure 1 has been added, and the order of the rest of the figures\n  continues. Figure 6 (Figure 5 of the previous version) and Figure 7 (Figure 6\n  of the previous version) have been modified. Figure 7, Figure 15, and Figure\n  16 of the previous versionhave have been removed. The structure of Section 4\n  has been adjusted", "journal-ref": "IEEE Transactions on Geoscience and Remote Sensing 2021 (Early\n  Access)", "doi": "10.1109/TGRS.2021.3064316", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Street view images classification aiming at urban land use analysis is\ndifficult because the class labels (e.g., commercial area), are concepts with\nhigher abstract level compared to the ones of general visual tasks (e.g.,\npersons and cars). Therefore, classification models using only visual features\noften fail to achieve satisfactory performance. In this paper, a novel approach\nbased on a \"Detector-Encoder-Classifier\" framework is proposed. Instead of\nusing visual features of the whole image directly as common image-level models\nbased on convolutional neural networks (CNNs) do, the proposed framework\nfirstly obtains the bounding boxes of buildings in street view images from a\ndetector. Their contextual information such as the co-occurrence patterns of\nbuilding classes and their layout are then encoded into metadata by the\nproposed algorithm \"CODING\" (Context encOding of Detected buildINGs). Finally,\nthese bounding box metadata are classified by a recurrent neural network (RNN).\nIn addition, we made a dual-labeled dataset named \"BEAUTY\" (Building dEtection\nAnd Urban funcTional-zone portraYing) of 19,070 street view images and 38,857\nbuildings based on the existing BIC GSV [1]. The dataset can be used not only\nfor street view image classification, but also for multi-class building\ndetection. Experiments on \"BEAUTY\" show that the proposed approach achieves a\n12.65% performance improvement on macro-precision and 12% on macro-recall over\nimage-level CNN based models. Our code and dataset are available at\nhttps://github.com/kyle-one/Context-Encoding-of-Detected-Buildings/\n", "versions": [{"version": "v1", "created": "Sat, 3 Oct 2020 08:49:51 GMT"}, {"version": "v2", "created": "Mon, 12 Oct 2020 05:52:29 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Zhao", "Kun", ""], ["Liu", "Yongkun", ""], ["Hao", "Siyuan", ""], ["Lu", "Shaoxing", ""], ["Liu", "Hongbin", ""], ["Zhou", "Lijian", ""]]}, {"id": "2010.01315", "submitter": "Fan Zhang Dr", "authors": "Fan Zhang, David Hall, Tao Xu, Stephen Boyle and David Bull", "title": "A simulation environment for drone cinematography", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we present a workflow for the simulation of drone operations\nexploiting realistic background environments constructed within Unreal Engine 4\n(UE4). Methods for environmental image capture, 3D reconstruction\n(photogrammetry) and the creation of foreground assets are presented along with\na flexible and user-friendly simulation interface. Given the geographical\nlocation of the selected area and the camera parameters employed, the scanning\nstrategy and its associated flight parameters are first determined for image\ncapture. Source imagery can be extracted from virtual globe software or\nobtained through aerial photography of the scene (e.g. using drones). The\nlatter case is clearly more time consuming but can provide enhanced detail,\nparticularly where coverage of virtual globe software is limited. The captured\nimages are then used to generate 3D background environment models employing\nphotogrammetry software. The reconstructed 3D models are then imported into the\nsimulation interface as background environment assets together with appropriate\nforeground object models as a basis for shot planning and rehearsal. The tool\nsupports both free-flight and parameterisable standard shot types along with\nprogrammable scenarios associated with foreground assets and event dynamics. It\nalso supports the exporting of flight plans. Camera shots can also be designed\nto provide suitable coverage of any landmarks which need to appear in-shot.\nThis simulation tool will contribute to enhanced productivity, improved safety\n(awareness and mitigations for crowds and buildings), improved confidence of\noperators and directors and ultimately enhanced quality of viewer experience.\n", "versions": [{"version": "v1", "created": "Sat, 3 Oct 2020 09:57:56 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Zhang", "Fan", ""], ["Hall", "David", ""], ["Xu", "Tao", ""], ["Boyle", "Stephen", ""], ["Bull", "David", ""]]}, {"id": "2010.01318", "submitter": "Yilin Xiong", "authors": "Yilin Xiong, Zijian Zhou, Yuhao Dou and Zhizhong Su", "title": "Gaussian Vector: An Efficient Solution for Facial Landmark Detection", "comments": "19 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Significant progress has been made in facial landmark detection with the\ndevelopment of Convolutional Neural Networks. The widely-used algorithms can be\nclassified into coordinate regression methods and heatmap based methods.\nHowever, the former loses spatial information, resulting in poor performance\nwhile the latter suffers from large output size or high post-processing\ncomplexity. This paper proposes a new solution, Gaussian Vector, to preserve\nthe spatial information as well as reduce the output size and simplify the\npost-processing. Our method provides novel vector supervision and introduces\nBand Pooling Module to convert heatmap into a pair of vectors for each\nlandmark. This is a plug-and-play component which is simple and effective.\nMoreover, Beyond Box Strategy is proposed to handle the landmarks out of the\nface bounding box. We evaluate our method on 300W, COFW, WFLW and JD-landmark.\nThat the results significantly surpass previous works demonstrates the\neffectiveness of our approach.\n", "versions": [{"version": "v1", "created": "Sat, 3 Oct 2020 10:15:41 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Xiong", "Yilin", ""], ["Zhou", "Zijian", ""], ["Dou", "Yuhao", ""], ["Su", "Zhizhong", ""]]}, {"id": "2010.01342", "submitter": "Ayse Serbetci Turan", "authors": "Ayse Serbetci and Yusuf Sinan Akgul", "title": "End-to-End Training of CNN Ensembles for Person Re-Identification", "comments": null, "journal-ref": "Pattern Recognition, 104, 107319 (2020)", "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an end-to-end ensemble method for person re-identification (ReID)\nto address the problem of overfitting in discriminative models. These models\nare known to converge easily, but they are biased to the training data in\ngeneral and may produce a high model variance, which is known as overfitting.\nThe ReID task is more prone to this problem due to the large discrepancy\nbetween training and test distributions. To address this problem, our proposed\nensemble learning framework produces several diverse and accurate base learners\nin a single DenseNet. Since most of the costly dense blocks are shared, our\nmethod is computationally efficient, which makes it favorable compared to the\nconventional ensemble models. Experiments on several benchmark datasets\ndemonstrate that our method achieves state-of-the-art results. Noticeable\nperformance improvements, especially on relatively small datasets, indicate\nthat the proposed method deals with the overfitting problem effectively.\n", "versions": [{"version": "v1", "created": "Sat, 3 Oct 2020 12:40:13 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Serbetci", "Ayse", ""], ["Akgul", "Yusuf Sinan", ""]]}, {"id": "2010.01343", "submitter": "Ayush Srivastava", "authors": "Ayush Srivastava, Oshin Dutta, Prathosh AP, Sumeet Agarwal, Jigyasa\n  Gupta", "title": "A Variational Information Bottleneck Based Method to Compress Sequential\n  Networks for Human Action Recognition", "comments": "Accepted at IEEE WACV 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last few years, compression of deep neural networks has become an\nimportant strand of machine learning and computer vision research. Deep models\nrequire sizeable computational complexity and storage, when used for instance\nfor Human Action Recognition (HAR) from videos, making them unsuitable to be\ndeployed on edge devices. In this paper, we address this issue and propose a\nmethod to effectively compress Recurrent Neural Networks (RNNs) such as Gated\nRecurrent Units (GRUs) and Long-Short-Term-Memory Units (LSTMs) that are used\nfor HAR. We use a Variational Information Bottleneck (VIB) theory-based pruning\napproach to limit the information flow through the sequential cells of RNNs to\na small subset. Further, we combine our pruning method with a specific\ngroup-lasso regularization technique that significantly improves compression.\nThe proposed techniques reduce model parameters and memory footprint from\nlatent representations, with little or no reduction in the validation accuracy\nwhile increasing the inference speed several-fold. We perform experiments on\nthe three widely used Action Recognition datasets, viz. UCF11, HMDB51, and\nUCF101, to validate our approach. It is shown that our method achieves over 70\ntimes greater compression than the nearest competitor with comparable accuracy\nfor the task of action recognition on UCF11.\n", "versions": [{"version": "v1", "created": "Sat, 3 Oct 2020 12:41:51 GMT"}, {"version": "v2", "created": "Mon, 9 Nov 2020 14:36:53 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Srivastava", "Ayush", ""], ["Dutta", "Oshin", ""], ["AP", "Prathosh", ""], ["Agarwal", "Sumeet", ""], ["Gupta", "Jigyasa", ""]]}, {"id": "2010.01357", "submitter": "Jiafei Duan", "authors": "Jiafei Duan, Samson Yu, Hui Li Tan, Cheston Tan", "title": "Actionet: An Interactive End-To-End Platform For Task-Based Data\n  Collection And Augmentation In 3D Environment", "comments": "https://github.com/SamsonYuBaiJian/actionet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of task planning for artificial agents remains largely unsolved.\nWhile there has been increasing interest in data-driven approaches for the\nstudy of task planning for artificial agents, a significant remaining\nbottleneck is the dearth of large-scale comprehensive task-based datasets. In\nthis paper, we present ActioNet, an interactive end-to-end platform for data\ncollection and augmentation of task-based dataset in 3D environment. Using\nActioNet, we collected a large-scale comprehensive task-based dataset,\ncomprising over 3000 hierarchical task structures and videos. Using the\nhierarchical task structures, the videos are further augmented across 50\ndifferent scenes to give over 150,000 video. To our knowledge, ActioNet is the\nfirst interactive end-to-end platform for such task-based dataset generation\nand the accompanying dataset is the largest task-based dataset of such\ncomprehensive nature. The ActioNet platform and dataset will be made available\nto facilitate research in hierarchical task planning.\n", "versions": [{"version": "v1", "created": "Sat, 3 Oct 2020 13:37:01 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Duan", "Jiafei", ""], ["Yu", "Samson", ""], ["Tan", "Hui Li", ""], ["Tan", "Cheston", ""]]}, {"id": "2010.01362", "submitter": "Daphna Keidar", "authors": "Elisha Goldstein, Daphna Keidar, Daniel Yaron, Yair Shachar, Ayelet\n  Blass, Leonid Charbinsky, Israel Aharony, Liza Lifshitz, Dimitri Lumelsky,\n  Ziv Neeman, Matti Mizrachi, Majd Hajouj, Nethanel Eizenbach, Eyal Sela,\n  Chedva S Weiss, Philip Levin, Ofer Benjaminov, Gil N Bachar, Shlomit Tamir,\n  Yael Rapson, Dror Suhami, Amiel A Dror, Naama R Bogot, Ahuva Grubstein, Nogah\n  Shabshin, Yishai M Elyada, Yonina C Eldar", "title": "COVID-19 Classification of X-ray Images Using Deep Neural Networks", "comments": "Elisha Goldstein, Daphna Keidar, and Daniel Yaron have made an equal\n  contribution and are equal first authors, listed alphabetically", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the midst of the coronavirus disease 2019 (COVID-19) outbreak, chest X-ray\n(CXR) imaging is playing an important role in the diagnosis and monitoring of\npatients with COVID-19. Machine learning solutions have been shown to be useful\nfor X-ray analysis and classification in a range of medical contexts. The\npurpose of this study is to create and evaluate a machine learning model for\ndiagnosis of COVID-19, and to provide a tool for searching for similar patients\naccording to their X-ray scans. In this retrospective study, a classifier was\nbuilt using a pre-trained deep learning model (ReNet50) and enhanced by data\naugmentation and lung segmentation to detect COVID-19 in frontal CXR images\ncollected between January 2018 and July 2020 in four hospitals in Israel. A\nnearest-neighbors algorithm was implemented based on the network results that\nidentifies the images most similar to a given image. The model was evaluated\nusing accuracy, sensitivity, area under the curve (AUC) of receiver operating\ncharacteristic (ROC) curve and of the precision-recall (P-R) curve. The dataset\nsourced for this study includes 2362 CXRs, balanced for positive and negative\nCOVID-19, from 1384 patients (63 +/- 18 years, 552 men). Our model achieved\n89.7% (314/350) accuracy and 87.1% (156/179) sensitivity in classification of\nCOVID-19 on a test dataset comprising 15% (350 of 2326) of the original data,\nwith AUC of ROC 0.95 and AUC of the P-R curve 0.94. For each image we retrieve\nimages with the most similar DNN-based image embeddings; these can be used to\ncompare with previous cases.\n", "versions": [{"version": "v1", "created": "Sat, 3 Oct 2020 13:57:08 GMT"}, {"version": "v2", "created": "Wed, 7 Oct 2020 08:28:41 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Goldstein", "Elisha", ""], ["Keidar", "Daphna", ""], ["Yaron", "Daniel", ""], ["Shachar", "Yair", ""], ["Blass", "Ayelet", ""], ["Charbinsky", "Leonid", ""], ["Aharony", "Israel", ""], ["Lifshitz", "Liza", ""], ["Lumelsky", "Dimitri", ""], ["Neeman", "Ziv", ""], ["Mizrachi", "Matti", ""], ["Hajouj", "Majd", ""], ["Eizenbach", "Nethanel", ""], ["Sela", "Eyal", ""], ["Weiss", "Chedva S", ""], ["Levin", "Philip", ""], ["Benjaminov", "Ofer", ""], ["Bachar", "Gil N", ""], ["Tamir", "Shlomit", ""], ["Rapson", "Yael", ""], ["Suhami", "Dror", ""], ["Dror", "Amiel A", ""], ["Bogot", "Naama R", ""], ["Grubstein", "Ahuva", ""], ["Shabshin", "Nogah", ""], ["Elyada", "Yishai M", ""], ["Eldar", "Yonina C", ""]]}, {"id": "2010.01401", "submitter": "Sadaf Gulshad", "authors": "Sadaf Gulshad, Jan Hendrik Metzen, Arnold Smeulders", "title": "Adversarial and Natural Perturbations for General Robustness", "comments": "Currently under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we aim to explore the general robustness of neural network\nclassifiers by utilizing adversarial as well as natural perturbations.\nDifferent from previous works which mainly focus on studying the robustness of\nneural networks against adversarial perturbations, we also evaluate their\nrobustness on natural perturbations before and after robustification. After\nstandardizing the comparison between adversarial and natural perturbations, we\ndemonstrate that although adversarial training improves the performance of the\nnetworks against adversarial perturbations, it leads to drop in the performance\nfor naturally perturbed samples besides clean samples. In contrast, natural\nperturbations like elastic deformations, occlusions and wave does not only\nimprove the performance against natural perturbations, but also lead to\nimprovement in the performance for the adversarial perturbations. Additionally\nthey do not drop the accuracy on the clean images.\n", "versions": [{"version": "v1", "created": "Sat, 3 Oct 2020 17:53:18 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Gulshad", "Sadaf", ""], ["Metzen", "Jan Hendrik", ""], ["Smeulders", "Arnold", ""]]}, {"id": "2010.01402", "submitter": "Madhu Vankadari", "authors": "Madhu Vankadari, Sourav Garg, Anima Majumder, Swagat Kumar, and\n  Ardhendu Behera", "title": "Unsupervised Monocular Depth Estimation for Night-time Images using\n  Adversarial Domain Feature Adaptation", "comments": "ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we look into the problem of estimating per-pixel depth maps\nfrom unconstrained RGB monocular night-time images which is a difficult task\nthat has not been addressed adequately in the literature. The state-of-the-art\nday-time depth estimation methods fail miserably when tested with night-time\nimages due to a large domain shift between them. The usual photo metric losses\nused for training these networks may not work for night-time images due to the\nabsence of uniform lighting which is commonly present in day-time images,\nmaking it a difficult problem to solve. We propose to solve this problem by\nposing it as a domain adaptation problem where a network trained with day-time\nimages is adapted to work for night-time images. Specifically, an encoder is\ntrained to generate features from night-time images that are indistinguishable\nfrom those obtained from day-time images by using a PatchGAN-based adversarial\ndiscriminative learning method. Unlike the existing methods that directly adapt\ndepth prediction (network output), we propose to adapt feature maps obtained\nfrom the encoder network so that a pre-trained day-time depth decoder can be\ndirectly used for predicting depth from these adapted features. Hence, the\nresulting method is termed as \"Adversarial Domain Feature Adaptation (ADFA)\"\nand its efficacy is demonstrated through experimentation on the challenging\nOxford night driving dataset. Also, The modular encoder-decoder architecture\nfor the proposed ADFA method allows us to use the encoder module as a feature\nextractor which can be used in many other applications. One such application is\ndemonstrated where the features obtained from our adapted encoder network are\nshown to outperform other state-of-the-art methods in a visual place\nrecognition problem, thereby, further establishing the usefulness and\neffectiveness of the proposed approach.\n", "versions": [{"version": "v1", "created": "Sat, 3 Oct 2020 17:55:16 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Vankadari", "Madhu", ""], ["Garg", "Sourav", ""], ["Majumder", "Anima", ""], ["Kumar", "Swagat", ""], ["Behera", "Ardhendu", ""]]}, {"id": "2010.01421", "submitter": "Udit Singh Parihar", "authors": "Satyajit Tourani, Dhagash Desai, Udit Singh Parihar, Sourav Garg, Ravi\n  Kiran Sarvadevabhatla, Michael Milford, K. Madhava Krishna", "title": "Early Bird: Loop Closures from Opposing Viewpoints for\n  Perceptually-Aliased Indoor Environments", "comments": "Accepted to VISAPP 2021. Video Link: https://youtu.be/q6cKYW0kX4s", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Significant advances have been made recently in Visual Place Recognition\n(VPR), feature correspondence, and localization due to the proliferation of\ndeep-learning-based methods. However, existing approaches tend to address,\npartially or fully, only one of two key challenges: viewpoint change and\nperceptual aliasing. In this paper, we present novel research that\nsimultaneously addresses both challenges by combining deep-learned features\nwith geometric transformations based on reasonable domain assumptions about\nnavigation on a ground-plane, whilst also removing the requirement for\nspecialized hardware setup (e.g. lighting, downwards facing cameras). In\nparticular, our integration of VPR with SLAM by leveraging the robustness of\ndeep-learned features and our homography-based extreme viewpoint invariance\nsignificantly boosts the performance of VPR, feature correspondence, and pose\ngraph submodules of the SLAM pipeline. For the first time, we demonstrate a\nlocalization system capable of state-of-the-art performance despite perceptual\naliasing and extreme 180-degree-rotated viewpoint change in a range of\nreal-world and simulated experiments. Our system is able to achieve early loop\nclosures that prevent significant drifts in SLAM trajectories. We also compare\nextensively several deep architectures for VPR and descriptor matching. We also\nshow that superior place recognition and descriptor matching across opposite\nviews results in a similar performance gain in back-end pose graph\noptimization.\n", "versions": [{"version": "v1", "created": "Sat, 3 Oct 2020 20:18:55 GMT"}, {"version": "v2", "created": "Tue, 15 Dec 2020 21:29:46 GMT"}, {"version": "v3", "created": "Sun, 20 Dec 2020 13:41:24 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Tourani", "Satyajit", ""], ["Desai", "Dhagash", ""], ["Parihar", "Udit Singh", ""], ["Garg", "Sourav", ""], ["Sarvadevabhatla", "Ravi Kiran", ""], ["Milford", "Michael", ""], ["Krishna", "K. Madhava", ""]]}, {"id": "2010.01424", "submitter": "Pengchuan Zhang", "authors": "Yi Wei, Zhe Gan, Wenbo Li, Siwei Lyu, Ming-Ching Chang, Lei Zhang,\n  Jianfeng Gao, Pengchuan Zhang", "title": "MagGAN: High-Resolution Face Attribute Editing with Mask-Guided\n  Generative Adversarial Network", "comments": "published at ACCV2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Mask-guided Generative Adversarial Network (MagGAN) for\nhigh-resolution face attribute editing, in which semantic facial masks from a\npre-trained face parser are used to guide the fine-grained image editing\nprocess. With the introduction of a mask-guided reconstruction loss, MagGAN\nlearns to only edit the facial parts that are relevant to the desired attribute\nchanges, while preserving the attribute-irrelevant regions (e.g., hat, scarf\nfor modification `To Bald'). Further, a novel mask-guided conditioning strategy\nis introduced to incorporate the influence region of each attribute change into\nthe generator. In addition, a multi-level patch-wise discriminator structure is\nproposed to scale our model for high-resolution ($1024 \\times 1024$) face\nediting. Experiments on the CelebA benchmark show that the proposed method\nsignificantly outperforms prior state-of-the-art approaches in terms of both\nimage quality and editing performance.\n", "versions": [{"version": "v1", "created": "Sat, 3 Oct 2020 20:56:16 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Wei", "Yi", ""], ["Gan", "Zhe", ""], ["Li", "Wenbo", ""], ["Lyu", "Siwei", ""], ["Chang", "Ming-Ching", ""], ["Zhang", "Lei", ""], ["Gao", "Jianfeng", ""], ["Zhang", "Pengchuan", ""]]}, {"id": "2010.01432", "submitter": "Ruisheng Su", "authors": "Ruisheng Su, Sandra A.P. Cornelissen, Matthijs van der Sluijs, Adriaan\n  C.G.M. van Es, Wim H. van Zwam, Diederik W.J. Dippel, Geert Lycklama, Pieter\n  Jan van Doormaal, Wiro J. Niessen, Aad van der Lugt, and Theo van Walsum", "title": "autoTICI: Automatic Brain Tissue Reperfusion Scoring on 2D DSA Images of\n  Acute Ischemic Stroke Patients", "comments": "Accepted for publication in IEEE Transactions on Medical Imaging\n  (IEEE TMI)", "journal-ref": null, "doi": "10.1109/TMI.2021.3077113", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Thrombolysis in Cerebral Infarction (TICI) score is an important metric\nfor reperfusion therapy assessment in acute ischemic stroke. It is commonly\nused as a technical outcome measure after endovascular treatment (EVT).\nExisting TICI scores are defined in coarse ordinal grades based on visual\ninspection, leading to inter- and intra-observer variation. In this work, we\npresent autoTICI, an automatic and quantitative TICI scoring method. First,\neach digital subtraction angiography (DSA) acquisition is separated into four\nphases (non-contrast, arterial, parenchymal and venous phase) using a\nmulti-path convolutional neural network (CNN), which exploits spatio-temporal\nfeatures. The network also incorporates sequence level label dependencies in\nthe form of a state-transition matrix. Next, a minimum intensity map (MINIP) is\ncomputed using the motion corrected arterial and parenchymal frames. On the\nMINIP image, vessel, perfusion and background pixels are segmented. Finally, we\nquantify the autoTICI score as the ratio of reperfused pixels after EVT. On a\nroutinely acquired multi-center dataset, the proposed autoTICI shows good\ncorrelation with the extended TICI (eTICI) reference with an average area under\nthe curve (AUC) score of 0.81. The AUC score is 0.90 with respect to the\ndichotomized eTICI. In terms of clinical outcome prediction, we demonstrate\nthat autoTICI is overall comparable to eTICI.\n", "versions": [{"version": "v1", "created": "Sat, 3 Oct 2020 22:06:55 GMT"}, {"version": "v2", "created": "Tue, 6 Oct 2020 06:54:42 GMT"}, {"version": "v3", "created": "Fri, 7 May 2021 10:51:07 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Su", "Ruisheng", ""], ["Cornelissen", "Sandra A. P.", ""], ["van der Sluijs", "Matthijs", ""], ["van Es", "Adriaan C. G. M.", ""], ["van Zwam", "Wim H.", ""], ["Dippel", "Diederik W. J.", ""], ["Lycklama", "Geert", ""], ["van Doormaal", "Pieter Jan", ""], ["Niessen", "Wiro J.", ""], ["van der Lugt", "Aad", ""], ["van Walsum", "Theo", ""]]}, {"id": "2010.01446", "submitter": "Yu Sun", "authors": "Yu Sun, Jiaming Liu, Yiran Sun, Brendt Wohlberg, Ulugbek S. Kamilov", "title": "Async-RED: A Provably Convergent Asynchronous Block Parallel Stochastic\n  Method using Deep Denoising Priors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regularization by denoising (RED) is a recently developed framework for\nsolving inverse problems by integrating advanced denoisers as image priors.\nRecent work has shown its state-of-the-art performance when combined with\npre-trained deep denoisers. However, current RED algorithms are inadequate for\nparallel processing on multicore systems. We address this issue by proposing a\nnew asynchronous RED (ASYNC-RED) algorithm that enables asynchronous parallel\nprocessing of data, making it significantly faster than its serial counterparts\nfor large-scale inverse problems. The computational complexity of ASYNC-RED is\nfurther reduced by using a random subset of measurements at every iteration. We\npresent complete theoretical analysis of the algorithm by establishing its\nconvergence under explicit assumptions on the data-fidelity and the denoiser.\nWe validate ASYNC-RED on image recovery using pre-trained deep denoisers as\npriors.\n", "versions": [{"version": "v1", "created": "Sat, 3 Oct 2020 23:55:36 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Sun", "Yu", ""], ["Liu", "Jiaming", ""], ["Sun", "Yiran", ""], ["Wohlberg", "Brendt", ""], ["Kamilov", "Ulugbek S.", ""]]}, {"id": "2010.01453", "submitter": "Xiaohao Cai", "authors": "Wai-Tsun Yeung, Xiaohao Cai, Zizhen Liang, Byung-Ho Kang", "title": "3D Orientation Field Transform", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The two-dimensional (2D) orientation field transform has been proved to be\neffective at enhancing 2D contours and curves in images by means of top-down\nprocessing. It, however, has no counterpart in three-dimensional (3D) images\ndue to the extremely complicated orientation in 3D compared to 2D. Practically\nand theoretically, the demand and interest in 3D can only be increasing. In\nthis work, we modularise the concept and generalise it to 3D curves. Different\nmodular combinations are found to enhance curves to different extents and with\ndifferent sensitivity to the packing of the 3D curves. In principle, the\nproposed 3D orientation field transform can naturally tackle any dimensions. As\na special case, it is also ideal for 2D images, owning simpler methodology\ncompared to the previous 2D orientation field transform. The proposed method is\ndemonstrated with several transmission electron microscopy tomograms ranging\nfrom 2D curve enhancement to, the more important and interesting, 3D ones.\n", "versions": [{"version": "v1", "created": "Sun, 4 Oct 2020 00:29:46 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Yeung", "Wai-Tsun", ""], ["Cai", "Xiaohao", ""], ["Liang", "Zizhen", ""], ["Kang", "Byung-Ho", ""]]}, {"id": "2010.01464", "submitter": "Sandipan Banerjee", "authors": "Sandipan Banerjee, Ajjen Joshi, Prashant Mahajan, Sneha Bhattacharya,\n  Survi Kyal and Taniya Mishra", "title": "LEGAN: Disentangled Manipulation of Directional Lighting and Facial\n  Expressions by Leveraging Human Perceptual Judgements", "comments": "AMFG Workshop at CVPR 2021 (CVPRW 2021). Get our synthetic face\n  images & their perceptual labels here:\n  https://github.com/Affectiva/LEGAN_Perceptual_Dataset", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Building facial analysis systems that generalize to extreme variations in\nlighting and facial expressions is a challenging problem that can potentially\nbe alleviated using natural-looking synthetic data. Towards that, we propose\nLEGAN, a novel synthesis framework that leverages perceptual quality judgments\nfor jointly manipulating lighting and expressions in face images, without\nrequiring paired training data. LEGAN disentangles the lighting and expression\nsubspaces and performs transformations in the feature space before upscaling to\nthe desired output image. The fidelity of the synthetic image is further\nrefined by integrating a perceptual quality estimation model, trained with face\nimages rendered using multiple synthesis methods and their crowd-sourced\nnaturalness ratings, into the LEGAN framework as an auxiliary discriminator.\nUsing objective metrics like FID and LPIPS, LEGAN is shown to generate higher\nquality face images when compared with popular GAN models like StarGAN and\nStarGAN-v2 for lighting and expression synthesis. We also conduct a perceptual\nstudy using images synthesized by LEGAN and other GAN models and show the\ncorrelation between our quality estimation and visual fidelity. Finally, we\ndemonstrate the effectiveness of LEGAN as training data augmenter for\nexpression recognition and face verification tasks.\n", "versions": [{"version": "v1", "created": "Sun, 4 Oct 2020 01:56:54 GMT"}, {"version": "v2", "created": "Sat, 5 Dec 2020 01:01:31 GMT"}, {"version": "v3", "created": "Fri, 18 Jun 2021 22:00:59 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Banerjee", "Sandipan", ""], ["Joshi", "Ajjen", ""], ["Mahajan", "Prashant", ""], ["Bhattacharya", "Sneha", ""], ["Kyal", "Survi", ""], ["Mishra", "Taniya", ""]]}, {"id": "2010.01465", "submitter": "Hongming Li", "authors": "Hongming Li, Yong Fan", "title": "MDReg-Net: Multi-resolution diffeomorphic image registration using fully\n  convolutional networks with deep self-supervision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a diffeomorphic image registration algorithm to learn spatial\ntransformations between pairs of images to be registered using fully\nconvolutional networks (FCNs) under a self-supervised learning setting. The\nnetwork is trained to estimate diffeomorphic spatial transformations between\npairs of images by maximizing an image-wise similarity metric between fixed and\nwarped moving images, similar to conventional image registration algorithms. It\nis implemented in a multi-resolution image registration framework to optimize\nand learn spatial transformations at different image resolutions jointly and\nincrementally with deep self-supervision in order to better handle large\ndeformation between images. A spatial Gaussian smoothing kernel is integrated\nwith the FCNs to yield sufficiently smooth deformation fields to achieve\ndiffeomorphic image registration. Particularly, spatial transformations learned\nat coarser resolutions are utilized to warp the moving image, which is\nsubsequently used for learning incremental transformations at finer\nresolutions. This procedure proceeds recursively to the full image resolution\nand the accumulated transformations serve as the final transformation to warp\nthe moving image at the finest resolution. Experimental results for registering\nhigh resolution 3D structural brain magnetic resonance (MR) images have\ndemonstrated that image registration networks trained by our method obtain\nrobust, diffeomorphic image registration results within seconds with improved\naccuracy compared with state-of-the-art image registration algorithms.\n", "versions": [{"version": "v1", "created": "Sun, 4 Oct 2020 02:00:37 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Li", "Hongming", ""], ["Fan", "Yong", ""]]}, {"id": "2010.01477", "submitter": "Zhigang Jia", "authors": "Zhi-Gang Jia, Zi-Jin Qiu, Mei-Xiang Zhao", "title": "Generalized Two-Dimensional Quaternion Principal Component Analysis with\n  Weighting for Color Image Recognition", "comments": "15 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A generalized two-dimensional quaternion principal component analysis\n(G2DQPCA) approach with weighting is presented for color image analysis. As a\ngeneral framework of 2DQPCA, G2DQPCA is flexible to adapt different constraints\nor requirements by imposing $L_{p}$ norms both on the constraint function and\nthe objective function. The gradient operator of quaternion vector functions is\nredefined by the structure-preserving gradient operator of real vector\nfunction. Under the framework of minorization-maximization (MM), an iterative\nalgorithm is developed to obtain the optimal closed-form solution of G2DQPCA.\nThe projection vectors generated by the deflating scheme are required to be\northogonal to each other. A weighting matrix is defined to magnify the effect\nof main features. The weighted projection bases remain the accuracy of face\nrecognition unchanged or moving in a tight range as the number of features\nincreases. The numerical results based on the real face databases validate that\nthe newly proposed method performs better than the state-of-the-art algorithms.\n", "versions": [{"version": "v1", "created": "Sun, 4 Oct 2020 03:37:23 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Jia", "Zhi-Gang", ""], ["Qiu", "Zi-Jin", ""], ["Zhao", "Mei-Xiang", ""]]}, {"id": "2010.01485", "submitter": "Anusua Trivedi", "authors": "Anusua Trivedi, Sreya Muppalla, Shreyaan Pathak, Azadeh Mobasher,\n  Pawel Janowski, Rahul Dodhia, Juan M. Lavista Ferres", "title": "Improving Lesion Detection by exploring bias on Skin Lesion dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  All datasets contain some biases, often unintentional, due to how they were\nacquired and annotated. These biases distort machine-learning models'\nperformance, creating spurious correlations that the models can unfairly\nexploit, or, contrarily destroying clear correlations that the models could\nlearn. With the popularity of deep learning models, automated skin lesion\nanalysis is starting to play an essential role in the early detection of\nMelanoma. The ISIC Archive is one of the most used skin lesion sources to\nbenchmark deep learning-based tools. Bissoto et al. experimented with different\nbounding-box based masks and showed that deep learning models could classify\nskin lesion images without clinically meaningful information in the input data.\nTheir findings seem confounding since the ablated regions (random rectangular\nboxes) are not significant. The shape of the lesion is a crucial factor in the\nclinical characterization of a skin lesion. In that context, we performed a set\nof experiments that generate shape-preserving masks instead of rectangular\nbounding-box based masks. A deep learning model trained on these\nshape-preserving masked images does not outperform models trained on images\nwithout clinically meaningful information. That strongly suggests spurious\ncorrelations guiding the models. We propose use of general adversarial network\n(GAN) to mitigate the underlying bias.\n", "versions": [{"version": "v1", "created": "Sun, 4 Oct 2020 05:04:58 GMT"}], "update_date": "2020-11-22", "authors_parsed": [["Trivedi", "Anusua", ""], ["Muppalla", "Sreya", ""], ["Pathak", "Shreyaan", ""], ["Mobasher", "Azadeh", ""], ["Janowski", "Pawel", ""], ["Dodhia", "Rahul", ""], ["Ferres", "Juan M. Lavista", ""]]}, {"id": "2010.01499", "submitter": "Silvia Liberata Ullo", "authors": "Silvia Liberata Ullo, Amrita Mohan, Alessandro Sebastianelli, Shaik\n  Ejaz Ahamed, Basant Kumar, Ramji Dwivedi, G. R. Sinha", "title": "A New Mask R-CNN Based Method for Improved Landslide Detection", "comments": "9 pages, 8 figures, 6 tables, submitted to JSTARS special issue on\n  Cultural Heritage", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  This paper presents a novel method of landslide detection by exploiting the\nMask R-CNN capability of identifying an object layout by using a pixel-based\nsegmentation, along with transfer learning used to train the proposed model. A\ndata set of 160 elements is created containing landslide and non-landslide\nimages. The proposed method consists of three steps: (i) augmenting training\nimage samples to increase the volume of the training data, (ii) fine tuning\nwith limited image samples, and (iii) performance evaluation of the algorithm\nin terms of precision, recall and F1 measure, on the considered landslide\nimages, by adopting ResNet-50 and 101 as backbone models. The experimental\nresults are quite encouraging as the proposed method achieves Precision equals\nto 1.00, Recall 0.93 and F1 measure 0.97, when ResNet-101 is used as backbone\nmodel, and with a low number of landslide photographs used as training samples.\nThe proposed algorithm can be potentially useful for land use planners and\npolicy makers of hilly areas where intermittent slope deformations necessitate\nlandslide detection as prerequisite before planning.\n", "versions": [{"version": "v1", "created": "Sun, 4 Oct 2020 07:46:37 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Ullo", "Silvia Liberata", ""], ["Mohan", "Amrita", ""], ["Sebastianelli", "Alessandro", ""], ["Ahamed", "Shaik Ejaz", ""], ["Kumar", "Basant", ""], ["Dwivedi", "Ramji", ""], ["Sinha", "G. R.", ""]]}, {"id": "2010.01506", "submitter": "Yingpeng Deng", "authors": "Yingpeng Deng and Lina J. Karam", "title": "A Study for Universal Adversarial Attacks on Texture Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given the outstanding progress that convolutional neural networks (CNNs) have\nmade on natural image classification and object recognition problems, it is\nshown that deep learning methods can achieve very good recognition performance\non many texture datasets. However, while CNNs for natural image\nclassification/object recognition tasks have been revealed to be highly\nvulnerable to various types of adversarial attack methods, the robustness of\ndeep learning methods for texture recognition is yet to be examined. In our\npaper, we show that there exist small image-agnostic/univesal perturbations\nthat can fool the deep learning models with more than 80\\% of testing fooling\nrates on all tested texture datasets. The computed perturbations using various\nattack methods on the tested datasets are generally quasi-imperceptible,\ncontaining structured patterns with low, middle and high frequency components.\n", "versions": [{"version": "v1", "created": "Sun, 4 Oct 2020 08:11:11 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Deng", "Yingpeng", ""], ["Karam", "Lina J.", ""]]}, {"id": "2010.01528", "submitter": "Sayna Ebrahimi", "authors": "Sayna Ebrahimi, Suzanne Petryk, Akash Gokul, William Gan, Joseph E.\n  Gonzalez, Marcus Rohrbach, Trevor Darrell", "title": "Remembering for the Right Reasons: Explanations Reduce Catastrophic\n  Forgetting", "comments": "Accepted at ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of continual learning (CL) is to learn a sequence of tasks without\nsuffering from the phenomenon of catastrophic forgetting. Previous work has\nshown that leveraging memory in the form of a replay buffer can reduce\nperformance degradation on prior tasks. We hypothesize that forgetting can be\nfurther reduced when the model is encouraged to remember the \\textit{evidence}\nfor previously made decisions. As a first step towards exploring this\nhypothesis, we propose a simple novel training paradigm, called Remembering for\nthe Right Reasons (RRR), that additionally stores visual model explanations for\neach example in the buffer and ensures the model has \"the right reasons\" for\nits predictions by encouraging its explanations to remain consistent with those\nused to make decisions at training time. Without this constraint, there is a\ndrift in explanations and increase in forgetting as conventional continual\nlearning algorithms learn new tasks. We demonstrate how RRR can be easily added\nto any memory or regularization-based approach and results in reduced\nforgetting, and more importantly, improved model explanations. We have\nevaluated our approach in the standard and few-shot settings and observed a\nconsistent improvement across various CL approaches using different\narchitectures and techniques to generate model explanations and demonstrated\nour approach showing a promising connection between explainability and\ncontinual learning. Our code is available at\n\\url{https://github.com/SaynaEbrahimi/Remembering-for-the-Right-Reasons}.\n", "versions": [{"version": "v1", "created": "Sun, 4 Oct 2020 10:05:27 GMT"}, {"version": "v2", "created": "Mon, 3 May 2021 03:26:30 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Ebrahimi", "Sayna", ""], ["Petryk", "Suzanne", ""], ["Gokul", "Akash", ""], ["Gan", "William", ""], ["Gonzalez", "Joseph E.", ""], ["Rohrbach", "Marcus", ""], ["Darrell", "Trevor", ""]]}, {"id": "2010.01532", "submitter": "Kang Li", "authors": "Kang Li, Lequan Yu, Shujun Wang and Pheng-Ann Heng", "title": "Towards Cross-modality Medical Image Segmentation with Online Mutual\n  Knowledge Distillation", "comments": "Accepted by AAAI 2020", "journal-ref": "AAAI 2020", "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The success of deep convolutional neural networks is partially attributed to\nthe massive amount of annotated training data. However, in practice, medical\ndata annotations are usually expensive and time-consuming to be obtained.\nConsidering multi-modality data with the same anatomic structures are widely\navailable in clinic routine, in this paper, we aim to exploit the prior\nknowledge (e.g., shape priors) learned from one modality (aka., assistant\nmodality) to improve the segmentation performance on another modality (aka.,\ntarget modality) to make up annotation scarcity. To alleviate the learning\ndifficulties caused by modality-specific appearance discrepancy, we first\npresent an Image Alignment Module (IAM) to narrow the appearance gap between\nassistant and target modality data.We then propose a novel Mutual Knowledge\nDistillation (MKD) scheme to thoroughly exploit the modality-shared knowledge\nto facilitate the target-modality segmentation. To be specific, we formulate\nour framework as an integration of two individual segmentors. Each segmentor\nnot only explicitly extracts one modality knowledge from corresponding\nannotations, but also implicitly explores another modality knowledge from its\ncounterpart in mutual-guided manner. The ensemble of two segmentors would\nfurther integrate the knowledge from both modalities and generate reliable\nsegmentation results on target modality. Experimental results on the public\nmulti-class cardiac segmentation data, i.e., MMWHS 2017, show that our method\nachieves large improvements on CT segmentation by utilizing additional MRI data\nand outperforms other state-of-the-art multi-modality learning methods.\n", "versions": [{"version": "v1", "created": "Sun, 4 Oct 2020 10:25:13 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Li", "Kang", ""], ["Yu", "Lequan", ""], ["Wang", "Shujun", ""], ["Heng", "Pheng-Ann", ""]]}, {"id": "2010.01549", "submitter": "Faria Huq", "authors": "Faria Huq, Nafees Ahmed, Anindya Iqbal", "title": "Static and Animated 3D Scene Generation from Free-form Text Descriptions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating coherent and useful image/video scenes from a free-form textual\ndescription is technically a very difficult problem to handle. Textual\ndescription of the same scene can vary greatly from person to person, or\nsometimes even for the same person from time to time. As the choice of words\nand syntax vary while preparing a textual description, it is challenging for\nthe system to reliably produce a consistently desirable output from different\nforms of language input. The prior works of scene generation have been mostly\nconfined to rigorous sentence structures of text input which restrict the\nfreedom of users to write description. In our work, we study a new pipeline\nthat aims to generate static as well as animated 3D scenes from different types\nof free-form textual scene description without any major restriction. In\nparticular, to keep our study practical and tractable, we focus on a small\nsubspace of all possible 3D scenes, containing various combinations of cube,\ncylinder and sphere. We design a two-stage pipeline. In the first stage, we\nencode the free-form text using an encoder-decoder neural architecture. In the\nsecond stage, we generate a 3D scene based on the generated encoding. Our\nneural architecture exploits state-of-the-art language model as encoder to\nleverage rich contextual encoding and a new multi-head decoder to predict\nmultiple features of an object in the scene simultaneously. For our\nexperiments, we generate a large synthetic data-set which contains 13,00,000\nand 14,00,000 samples of unique static and animated scene descriptions,\nrespectively. We achieve 98.427% accuracy on test data set in detecting the 3D\nobjects features successfully. Our work shows a proof of concept of one\napproach towards solving the problem, and we believe with enough training data,\nthe same pipeline can be expanded to handle even broader set of 3D scene\ngeneration problems.\n", "versions": [{"version": "v1", "created": "Sun, 4 Oct 2020 11:31:21 GMT"}, {"version": "v2", "created": "Sat, 28 Nov 2020 19:28:30 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Huq", "Faria", ""], ["Ahmed", "Nafees", ""], ["Iqbal", "Anindya", ""]]}, {"id": "2010.01557", "submitter": "Pablo Barros", "authors": "Pablo Barros, Alessandra Sciutti", "title": "The FaceChannelS: Strike of the Sequences for the AffWild 2 Challenge", "comments": "Submission that describes the models submitted to the AffWild2\n  challenge", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Predicting affective information from human faces became a popular task for\nmost of the machine learning community in the past years. The development of\nimmense and dense deep neural networks was backed by the availability of\nnumerous labeled datasets. These models, most of the time, present\nstate-of-the-art results in such benchmarks, but are very difficult to adapt to\nother scenarios. In this paper, we present one more chapter of benchmarking\ndifferent versions of the FaceChannel neural network: we demonstrate how our\nlittle model can predict affective information from the facial expression on\nthe novel AffWild2 dataset.\n", "versions": [{"version": "v1", "created": "Sun, 4 Oct 2020 12:00:48 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Barros", "Pablo", ""], ["Sciutti", "Alessandra", ""]]}, {"id": "2010.01567", "submitter": "Michael Lyons", "authors": "Michael J. Lyons", "title": "Facial gesture interfaces for expression and communication", "comments": "6 pages, 8 figures", "journal-ref": "2004 IEEE International Conference on Systems, Man and Cybernetics", "doi": "10.1109/ICSMC.2004.1398365", "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Considerable effort has been devoted to the automatic extraction of\ninformation about action of the face from image sequences. Within the context\nof human-computer interaction (HCI) we may distinguish systems that allow\nexpression from those which aim at recognition. Most of the work in facial\naction processing has been directed at automatically recognizing affect from\nfacial actions. By contrast, facial gesture interfaces, which respond to\ndeliberate facial actions, have received comparatively little attention. This\npaper reviews several projects on vision-based interfaces that rely on facial\naction for intentional HCI. Applications to several domains are introduced,\nincluding text entry, artistic and musical expression and assistive technology\nfor motor-impaired users.\n", "versions": [{"version": "v1", "created": "Sun, 4 Oct 2020 12:51:48 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Lyons", "Michael J.", ""]]}, {"id": "2010.01588", "submitter": "Lima Agnel Tony", "authors": "Lima Agnel Tony, Shuvrangshu Jana, Varun V P, Vidyadhara B V,\n  Mohitvishnu S Gadde, Abhishek Kashyap, Rahul Ravichandran, Debasish Ghose", "title": "Collaborative Tracking and Capture of Aerial Object using UAVs", "comments": null, "journal-ref": "MBZIRC Symposium 2020, ADNEC, Abu Dhabi", "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work details the problem of aerial target capture using multiple UAVs.\nThis problem is motivated from the challenge 1 of Mohammed Bin Zayed\nInternational Robotic Challenge 2020. The UAVs utilise visual feedback to\nautonomously detect target, approach it and capture without disturbing the\nvehicle which carries the target. Multi-UAV collaboration improves the\nefficiency of the system and increases the chance of capturing the ball\nrobustly in short span of time. In this paper, the proposed architecture is\nvalidated through simulation in ROS-Gazebo environment and is further\nimplemented on hardware.\n", "versions": [{"version": "v1", "created": "Sun, 4 Oct 2020 14:23:03 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Tony", "Lima Agnel", ""], ["Jana", "Shuvrangshu", ""], ["P", "Varun V", ""], ["B", "Vidyadhara", "V"], ["Gadde", "Mohitvishnu S", ""], ["Kashyap", "Abhishek", ""], ["Ravichandran", "Rahul", ""], ["Ghose", "Debasish", ""]]}, {"id": "2010.01592", "submitter": "Ali Khodabakhsh", "authors": "Ali Khodabakhsh, Zahid Akhtar", "title": "Unknown Presentation Attack Detection against Rational Attackers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.GT cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the impressive progress in the field of presentation attack detection\nand multimedia forensics over the last decade, these systems are still\nvulnerable to attacks in real-life settings. Some of the challenges for\nexisting solutions are the detection of unknown attacks, the ability to perform\nin adversarial settings, few-shot learning, and explainability. In this study,\nthese limitations are approached by reliance on a game-theoretic view for\nmodeling the interactions between the attacker and the detector. Consequently,\na new optimization criterion is proposed and a set of requirements are defined\nfor improving the performance of these systems in real-life settings.\nFurthermore, a novel detection technique is proposed using generator-based\nfeature sets that are not biased towards any specific attack species. To\nfurther optimize the performance on known attacks, a new loss function coined\ncategorical margin maximization loss (C-marmax) is proposed which gradually\nimproves the performance against the most powerful attack. The proposed\napproach provides a more balanced performance across known and unknown attacks\nand achieves state-of-the-art performance in known and unknown attack detection\ncases against rational attackers. Lastly, the few-shot learning potential of\nthe proposed approach is studied as well as its ability to provide pixel-level\nexplainability.\n", "versions": [{"version": "v1", "created": "Sun, 4 Oct 2020 14:37:10 GMT"}, {"version": "v2", "created": "Fri, 2 Jul 2021 22:37:17 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Khodabakhsh", "Ali", ""], ["Akhtar", "Zahid", ""]]}, {"id": "2010.01617", "submitter": "Ezequiel de la Rosa", "authors": "Ezequiel de la Rosa, Diana M. Sima, Bjoern Menze, Jan S. Kirschke,\n  David Robben", "title": "AIFNet: Automatic Vascular Function Estimation for Perfusion Analysis\n  Using Deep Learning", "comments": "Preprint submitted to Elsevier", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Perfusion imaging is crucial in acute ischemic stroke for quantifying the\nsalvageable penumbra and irreversibly damaged core lesions. As such, it helps\nclinicians to decide on the optimal reperfusion treatment. In perfusion CT\nimaging, deconvolution methods are used to obtain clinically interpretable\nperfusion parameters that allow identifying brain tissue abnormalities.\nDeconvolution methods require the selection of two reference vascular functions\nas inputs to the model: the arterial input function (AIF) and the venous output\nfunction, with the AIF as the most critical model input. When manually\nperformed, the vascular function selection is time demanding, suffers from poor\nreproducibility and is subject to the professionals' experience. This leads to\npotentially unreliable quantification of the penumbra and core lesions and,\nhence, might harm the treatment decision process. In this work we automatize\nthe perfusion analysis with AIFNet, a fully automatic and end-to-end trainable\ndeep learning approach for estimating the vascular functions. Unlike previous\nmethods using clustering or segmentation techniques to select vascular voxels,\nAIFNet is directly optimized at the vascular function estimation, which allows\nto better recognise the time-curve profiles. Validation on the public ISLES18\nstroke database shows that AIFNet reaches inter-rater performance for the\nvascular function estimation and, subsequently, for the parameter maps and core\nlesion quantification obtained through deconvolution. We conclude that AIFNet\nhas potential for clinical transfer and could be incorporated in perfusion\ndeconvolution software.\n", "versions": [{"version": "v1", "created": "Sun, 4 Oct 2020 16:14:45 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["de la Rosa", "Ezequiel", ""], ["Sima", "Diana M.", ""], ["Menze", "Bjoern", ""], ["Kirschke", "Jan S.", ""], ["Robben", "David", ""]]}, {"id": "2010.01626", "submitter": "Ashish Kubade", "authors": "Ashish Kubade, Diptiben Patel, Avinash Sharma, K. S. Rajan", "title": "AFN: Attentional Feedback Network based 3D Terrain Super-Resolution", "comments": "Accepted as oral at ACCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Terrain, representing features of an earth surface, plays a crucial role in\nmany applications such as simulations, route planning, analysis of surface\ndynamics, computer graphics-based games, entertainment, films, to name a few.\nWith recent advancements in digital technology, these applications demand the\npresence of high-resolution details in the terrain. In this paper, we propose a\nnovel fully convolutional neural network-based super-resolution architecture to\nincrease the resolution of low-resolution Digital Elevation Model (LRDEM) with\nthe help of information extracted from the corresponding aerial image as a\ncomplementary modality. We perform the super-resolution of LRDEM using an\nattention-based feedback mechanism named 'Attentional Feedback Network' (AFN),\nwhich selectively fuses the information from LRDEM and aerial image to enhance\nand infuse the high-frequency features and to produce the terrain\nrealistically. We compare the proposed architecture with existing\nstate-of-the-art DEM super-resolution methods and show that the proposed\narchitecture outperforms enhancing the resolution of input LRDEM accurately and\nin a realistic manner.\n", "versions": [{"version": "v1", "created": "Sun, 4 Oct 2020 16:51:39 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Kubade", "Ashish", ""], ["Patel", "Diptiben", ""], ["Sharma", "Avinash", ""], ["Rajan", "K. S.", ""]]}, {"id": "2010.01650", "submitter": "Philipp Singer", "authors": "Christof Henkel and Philipp Singer", "title": "Supporting large-scale image recognition with out-of-domain samples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article presents an efficient end-to-end method to perform\ninstance-level recognition employed to the task of labeling and ranking\nlandmark images. In a first step, we embed images in a high dimensional feature\nspace using convolutional neural networks trained with an additive angular\nmargin loss and classify images using visual similarity. We then efficiently\nre-rank predictions and filter noise utilizing similarity to out-of-domain\nimages. Using this approach we achieved the 1st place in the 2020 edition of\nthe Google Landmark Recognition challenge.\n", "versions": [{"version": "v1", "created": "Sun, 4 Oct 2020 18:44:01 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Henkel", "Christof", ""], ["Singer", "Philipp", ""]]}, {"id": "2010.01663", "submitter": "Valanarasu Jeya Maria Jose", "authors": "Jeya Maria Jose Valanarasu, Vishwanath A. Sindagi, Ilker\n  Hacihaliloglu, Vishal M. Patel", "title": "KiU-Net: Overcomplete Convolutional Architectures for Biomedical Image\n  and Volumetric Segmentation", "comments": "Journal Extension of KiU-Net (MICCAI-2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most methods for medical image segmentation use U-Net or its variants as they\nhave been successful in most of the applications. After a detailed analysis of\nthese \"traditional\" encoder-decoder based approaches, we observed that they\nperform poorly in detecting smaller structures and are unable to segment\nboundary regions precisely. This issue can be attributed to the increase in\nreceptive field size as we go deeper into the encoder. The extra focus on\nlearning high level features causes the U-Net based approaches to learn less\ninformation about low-level features which are crucial for detecting small\nstructures. To overcome this issue, we propose using an overcomplete\nconvolutional architecture where we project our input image into a higher\ndimension such that we constrain the receptive field from increasing in the\ndeep layers of the network. We design a new architecture for image\nsegmentation- KiU-Net which has two branches: (1) an overcomplete convolutional\nnetwork Kite-Net which learns to capture fine details and accurate edges of the\ninput, and (2) U-Net which learns high level features. Furthermore, we also\npropose KiU-Net 3D which is a 3D convolutional architecture for volumetric\nsegmentation. We perform a detailed study of KiU-Net by performing experiments\non five different datasets covering various image modalities like ultrasound\n(US), magnetic resonance imaging (MRI), computed tomography (CT), microscopic\nand fundus images. The proposed method achieves a better performance as\ncompared to all the recent methods with an additional benefit of fewer\nparameters and faster convergence. Additionally, we also demonstrate that the\nextensions of KiU-Net based on residual blocks and dense blocks result in\nfurther performance improvements. The implementation of KiU-Net can be found\nhere: https://github.com/jeya-maria-jose/KiU-Net-pytorch\n", "versions": [{"version": "v1", "created": "Sun, 4 Oct 2020 19:23:33 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Valanarasu", "Jeya Maria Jose", ""], ["Sindagi", "Vishwanath A.", ""], ["Hacihaliloglu", "Ilker", ""], ["Patel", "Vishal M.", ""]]}, {"id": "2010.01664", "submitter": "Usman Sajid", "authors": "Usman Sajid, Wenchi Ma, Guanghui Wang", "title": "Multi-Resolution Fusion and Multi-scale Input Priors Based Crowd\n  Counting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowd counting in still images is a challenging problem in practice due to\nhuge crowd-density variations, large perspective changes, severe occlusion, and\nvariable lighting conditions. The state-of-the-art patch rescaling module (PRM)\nbased approaches prove to be very effective in improving the crowd counting\nperformance. However, the PRM module requires an additional and compromising\ncrowd-density classification process. To address these issues and challenges,\nthe paper proposes a new multi-resolution fusion based end-to-end crowd\ncounting network. It employs three deep-layers based columns/branches, each\ncatering the respective crowd-density scale. These columns regularly fuse\n(share) the information with each other. The network is divided into three\nphases with each phase containing one or more columns. Three input priors are\nintroduced to serve as an efficient and effective alternative to the PRM\nmodule, without requiring any additional classification operations. Along with\nthe final crowd count regression head, the network also contains three\nauxiliary crowd estimation regression heads, which are strategically placed at\neach phase end to boost the overall performance. Comprehensive experiments on\nthree benchmark datasets demonstrate that the proposed approach outperforms all\nthe state-of-the-art models under the RMSE evaluation metric. The proposed\napproach also has better generalization capability with the best results during\nthe cross-dataset experiments.\n", "versions": [{"version": "v1", "created": "Sun, 4 Oct 2020 19:30:13 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Sajid", "Usman", ""], ["Ma", "Wenchi", ""], ["Wang", "Guanghui", ""]]}, {"id": "2010.01669", "submitter": "Samuel Budd", "authors": "Samuel Budd, Prachi Patkee, Ana Baburamani, Mary Rutherford, Emma C.\n  Robinson, Bernhard Kainz", "title": "Surface Agnostic Metrics for Cortical Volume Segmentation and Regression", "comments": "Best Paper Honourable Mentions @ MLCN 2020 (MICCAI 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The cerebral cortex performs higher-order brain functions and is thus\nimplicated in a range of cognitive disorders. Current analysis of cortical\nvariation is typically performed by fitting surface mesh models to inner and\nouter cortical boundaries and investigating metrics such as surface area and\ncortical curvature or thickness. These, however, take a long time to run, and\nare sensitive to motion and image and surface resolution, which can prohibit\ntheir use in clinical settings. In this paper, we instead propose a machine\nlearning solution, training a novel architecture to predict cortical thickness\nand curvature metrics from T2 MRI images, while additionally returning metrics\nof prediction uncertainty. Our proposed model is tested on a clinical cohort\n(Down Syndrome) for which surface-based modelling often fails. Results suggest\nthat deep convolutional neural networks are a viable option to predict cortical\nmetrics across a range of brain development stages and pathologies.\n", "versions": [{"version": "v1", "created": "Sun, 4 Oct 2020 19:46:04 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Budd", "Samuel", ""], ["Patkee", "Prachi", ""], ["Baburamani", "Ana", ""], ["Rutherford", "Mary", ""], ["Robinson", "Emma C.", ""], ["Kainz", "Bernhard", ""]]}, {"id": "2010.01679", "submitter": "Mallikarjun Byrasandra Ramalinga Reddy", "authors": "Mallikarjun B R and Ayush Tewari and Hans-Peter Seidel and Mohamed\n  Elgharib and Christian Theobalt", "title": "Learning Complete 3D Morphable Face Models from Images and Videos", "comments": "Project Page - https://gvv.mpi-inf.mpg.de/projects/LeMoMo", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most 3D face reconstruction methods rely on 3D morphable models, which\ndisentangle the space of facial deformations into identity geometry,\nexpressions and skin reflectance. These models are typically learned from a\nlimited number of 3D scans and thus do not generalize well across different\nidentities and expressions. We present the first approach to learn complete 3D\nmodels of face identity geometry, albedo and expression just from images and\nvideos. The virtually endless collection of such data, in combination with our\nself-supervised learning-based approach allows for learning face models that\ngeneralize beyond the span of existing approaches. Our network design and loss\nfunctions ensure a disentangled parameterization of not only identity and\nalbedo, but also, for the first time, an expression basis. Our method also\nallows for in-the-wild monocular reconstruction at test time. We show that our\nlearned models better generalize and lead to higher quality image-based\nreconstructions than existing approaches.\n", "versions": [{"version": "v1", "created": "Sun, 4 Oct 2020 20:51:23 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["R", "Mallikarjun B", ""], ["Tewari", "Ayush", ""], ["Seidel", "Hans-Peter", ""], ["Elgharib", "Mohamed", ""], ["Theobalt", "Christian", ""]]}, {"id": "2010.01681", "submitter": "Matthew Guzdial", "authors": "Adrian Gonzalez, Matthew Guzdial and Felix Ramos", "title": "Generating Gameplay-Relevant Art Assets with Transfer Learning", "comments": "7 pages, 8 figures", "journal-ref": "Proceedings of the 2020 Experimental AI in Games Workshop", "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In game development, designing compelling visual assets that convey\ngameplay-relevant features requires time and experience. Recent image\ngeneration methods that create high-quality content could reduce development\ncosts, but these approaches do not consider game mechanics. We propose a\nConvolutional Variational Autoencoder (CVAE) system to modify and generate new\ngame visuals based on their gameplay relevance. We test this approach with\nPok\\'emon sprites and Pok\\'emon type information, since types are one of the\ngame's core mechanics and they directly impact the game's visuals. Our\nexperimental results indicate that adopting a transfer learning approach can\nhelp to improve visual quality and stability over unseen data.\n", "versions": [{"version": "v1", "created": "Sun, 4 Oct 2020 20:58:40 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Gonzalez", "Adrian", ""], ["Guzdial", "Matthew", ""], ["Ramos", "Felix", ""]]}, {"id": "2010.01695", "submitter": "Marius Schubert", "authors": "Marius Schubert, Karsten Kahl, Matthias Rottmann", "title": "MetaDetect: Uncertainty Quantification and Prediction Quality Estimates\n  for Object Detection", "comments": "11 pages, 5 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In object detection with deep neural networks, the box-wise objectness score\ntends to be overconfident, sometimes even indicating high confidence in\npresence of inaccurate predictions. Hence, the reliability of the prediction\nand therefore reliable uncertainties are of highest interest. In this work, we\npresent a post processing method that for any given neural network provides\npredictive uncertainty estimates and quality estimates. These estimates are\nlearned by a post processing model that receives as input a hand-crafted set of\ntransparent metrics in form of a structured dataset. Therefrom, we learn two\ntasks for predicted bounding boxes. We discriminate between true positives\n($\\mathit{IoU}\\geq0.5$) and false positives ($\\mathit{IoU} < 0.5$) which we\nterm meta classification, and we predict $\\mathit{IoU}$ values directly which\nwe term meta regression. The probabilities of the meta classification model aim\nat learning the probabilities of success and failure and therefore provide a\nmodelled predictive uncertainty estimate. On the other hand, meta regression\ngives rise to a quality estimate. In numerical experiments, we use the publicly\navailable YOLOv3 network and the Faster-RCNN network and evaluate meta\nclassification and regression performance on the Kitti, Pascal VOC and COCO\ndatasets. We demonstrate that our metrics are indeed well correlated with the\n$\\mathit{IoU}$. For meta classification we obtain classification accuracies of\nup to 98.92% and AUROCs of up to 99.93%. For meta regression we obtain an $R^2$\nvalue of up to 91.78%. These results yield significant improvements compared to\nother network's objectness score and other baseline approaches. Therefore, we\nobtain more reliable uncertainty and quality estimates which is particularly\ninteresting in the absence of ground truth.\n", "versions": [{"version": "v1", "created": "Sun, 4 Oct 2020 21:49:23 GMT"}, {"version": "v2", "created": "Tue, 6 Oct 2020 15:38:53 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Schubert", "Marius", ""], ["Kahl", "Karsten", ""], ["Rottmann", "Matthias", ""]]}, {"id": "2010.01725", "submitter": "Moshiur R Farazi", "authors": "Moshiur Farazi, Salman Khan and Nick Barnes", "title": "Attention Guided Semantic Relationship Parsing for Visual Question\n  Answering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans explain inter-object relationships with semantic labels that\ndemonstrate a high-level understanding required to perform complex\nVision-Language tasks such as Visual Question Answering (VQA). However,\nexisting VQA models represent relationships as a combination of object-level\nvisual features which constrain a model to express interactions between objects\nin a single domain, while the model is trying to solve a multi-modal task. In\nthis paper, we propose a general purpose semantic relationship parser which\ngenerates a semantic feature vector for each subject-predicate-object triplet\nin an image, and a Mutual and Self Attention (MSA) mechanism that learns to\nidentify relationship triplets that are important to answer the given question.\nTo motivate the significance of semantic relationships, we show an oracle\nsetting with ground-truth relationship triplets, where our model achieves a\n~25% accuracy gain over the closest state-of-the-art model on the challenging\nGQA dataset. Further, with our semantic parser, we show that our model\noutperforms other comparable approaches on VQA and GQA datasets.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 00:23:49 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Farazi", "Moshiur", ""], ["Khan", "Salman", ""], ["Barnes", "Nick", ""]]}, {"id": "2010.01729", "submitter": "Youngeun Kim", "authors": "Youngeun Kim, Priyadarshini Panda", "title": "Revisiting Batch Normalization for Training Low-latency Deep Spiking\n  Neural Networks from Scratch", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spiking Neural Networks (SNNs) have recently emerged as an alternative to\ndeep learning owing to sparse, asynchronous and binary event (or spike) driven\nprocessing, that can yield huge energy efficiency benefits on neuromorphic\nhardware. However, training high-accuracy and low-latency SNNs from scratch\nsuffers from non-differentiable nature of a spiking neuron. To address this\ntraining issue in SNNs, we revisit batch normalization and propose a temporal\nBatch Normalization Through Time (BNTT) technique. Most prior SNN works till\nnow have disregarded batch normalization deeming it ineffective for training\ntemporal SNNs. Different from previous works, our proposed BNTT decouples the\nparameters in a BNTT layer along the time axis to capture the temporal dynamics\nof spikes. The temporally evolving learnable parameters in BNTT allow a neuron\nto control its spike rate through different time-steps, enabling low-latency\nand low-energy training from scratch. We conduct experiments on CIFAR-10,\nCIFAR-100, Tiny-ImageNet and event-driven DVS-CIFAR10 datasets. BNTT allows us\nto train deep SNN architectures from scratch, for the first time, on complex\ndatasets with just few 25-30 time-steps. We also propose an early exit\nalgorithm using the distribution of parameters in BNTT to reduce the latency at\ninference, that further improves the energy-efficiency.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 00:49:30 GMT"}, {"version": "v2", "created": "Tue, 27 Oct 2020 21:42:42 GMT"}, {"version": "v3", "created": "Tue, 24 Nov 2020 17:14:42 GMT"}, {"version": "v4", "created": "Mon, 30 Nov 2020 00:36:59 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Kim", "Youngeun", ""], ["Panda", "Priyadarshini", ""]]}, {"id": "2010.01732", "submitter": "Ian Manchester", "authors": "Max Revay, Ruigang Wang, Ian R. Manchester", "title": "Lipschitz Bounded Equilibrium Networks", "comments": "Conference submission, 19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.SY eess.SY math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces new parameterizations of equilibrium neural networks,\ni.e. networks defined by implicit equations. This model class includes standard\nmultilayer and residual networks as special cases. The new parameterization\nadmits a Lipschitz bound during training via unconstrained optimization: no\nprojections or barrier functions are required. Lipschitz bounds are a common\nproxy for robustness and appear in many generalization bounds. Furthermore,\ncompared to previous works we show well-posedness (existence of solutions)\nunder less restrictive conditions on the network weights and more natural\nassumptions on the activation functions: that they are monotone and slope\nrestricted. These results are proved by establishing novel connections with\nconvex optimization, operator splitting on non-Euclidean spaces, and\ncontracting neural ODEs. In image classification experiments we show that the\nLipschitz bounds are very accurate and improve robustness to adversarial\nattacks.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 01:00:40 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Revay", "Max", ""], ["Wang", "Ruigang", ""], ["Manchester", "Ian R.", ""]]}, {"id": "2010.01757", "submitter": "Fathi  Mahdi Elsiddig Haroun", "authors": "Fathi Mahdi Elsiddig Haroun, Siti Noratiqah Mohamad Deros, Norashidah\n  Md Din", "title": "A Review of Vegetation Encroachment Detection in Power Transmission\n  Lines using Optical Sensing Satellite Imagery", "comments": "7 pages, 7 figures , 2 Tables", "journal-ref": null, "doi": "10.30534/ijatcse/2020/8691.42020", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Vegetation encroachment in power transmission lines can cause outages, which\nmay result in severe impact on economic of power utilities companies as well as\nthe consumer. Vegetation detection and monitoring along the power line corridor\nright-of-way (ROW) are implemented to protect power transmission lines from\nvegetation penetration. There were various methods used to monitor the\nvegetation penetration, however, most of them were too expensive and time\nconsuming. Satellite images can play a major role in vegetation monitoring,\nbecause it can cover high spatial area with relatively low cost. In this paper,\nthe current techniques used to detect the vegetation encroachment using\nsatellite images are reviewed and categorized into four sectors; Vegetation\nIndex based method, object-based detection method, stereo matching based and\nother current techniques. However, the current methods depend usually on\nsetting manually serval threshold values and parameters which make the\ndetection process very static. Machine Learning (ML) and deep learning (DL)\nalgorithms can provide a very high accuracy with flexibility in the detection\nprocess. Hence, in addition to review the current technique of vegetation\npenetration monitoring in power transmission, the potential of using Machine\nLearning based algorithms are also included.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 03:24:31 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Haroun", "Fathi Mahdi Elsiddig", ""], ["Deros", "Siti Noratiqah Mohamad", ""], ["Din", "Norashidah Md", ""]]}, {"id": "2010.01761", "submitter": "Yufan Zhou", "authors": "Yufan Zhou, Changyou Chen, Jinhui Xu", "title": "Learning Manifold Implicitly via Explicit Heat-Kernel Learning", "comments": "Accepted by NeurIPS 2020. Some typos have been corrected", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Manifold learning is a fundamental problem in machine learning with numerous\napplications. Most of the existing methods directly learn the low-dimensional\nembedding of the data in some high-dimensional space, and usually lack the\nflexibility of being directly applicable to down-stream applications. In this\npaper, we propose the concept of implicit manifold learning, where manifold\ninformation is implicitly obtained by learning the associated heat kernel. A\nheat kernel is the solution of the corresponding heat equation, which describes\nhow \"heat\" transfers on the manifold, thus containing ample geometric\ninformation of the manifold. We provide both practical algorithm and\ntheoretical analysis of our framework. The learned heat kernel can be applied\nto various kernel-based machine learning models, including deep generative\nmodels (DGM) for data generation and Stein Variational Gradient Descent for\nBayesian inference. Extensive experiments show that our framework can achieve\nstate-of-the-art results compared to existing methods for the two tasks.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 03:39:58 GMT"}, {"version": "v2", "created": "Thu, 10 Dec 2020 00:36:25 GMT"}, {"version": "v3", "created": "Mon, 15 Mar 2021 00:12:39 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Zhou", "Yufan", ""], ["Chen", "Changyou", ""], ["Xu", "Jinhui", ""]]}, {"id": "2010.01762", "submitter": "Zejiang Shen", "authors": "Zejiang Shen, Jian Zhao, Melissa Dell, Yaoliang Yu, Weining Li", "title": "OLALA: Object-Level Active Learning for Efficient Document Layout\n  Annotation", "comments": "12 pages, 7 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Document images often have intricate layout structures, with numerous content\nregions (e.g. texts, figures, tables) densely arranged on each page. This makes\nthe manual annotation of layout datasets expensive and inefficient. These\ncharacteristics also challenge existing active learning methods, as image-level\nscoring and selection suffer from the overexposure of common objects.Inspired\nby recent progresses in semi-supervised learning and self-training, we propose\nan Object-Level Active Learning framework for efficient document layout\nAnnotation, OLALA. In this framework, only regions with the most ambiguous\nobject predictions within an image are selected for annotators to label,\noptimizing the use of the annotation budget. For unselected predictions, the\nsemi-automatic correction algorithm is proposed to identify certain errors\nbased on prior knowledge of layout structures and rectifies them with minor\nsupervision. Additionally, we carefully design a perturbation-based object\nscoring function for document images. It governs the object selection process\nvia evaluating prediction ambiguities, and considers both the positions and\ncategories of predicted layout objects. Extensive experiments show that OLALA\ncan significantly boost model performance and improve annotation efficiency,\ngiven the same labeling budget. Code for this paper can be accessed via\nhttps://github.com/lolipopshock/detectron2_al.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 03:48:07 GMT"}, {"version": "v2", "created": "Wed, 14 Oct 2020 04:41:03 GMT"}, {"version": "v3", "created": "Mon, 29 Mar 2021 19:32:25 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Shen", "Zejiang", ""], ["Zhao", "Jian", ""], ["Dell", "Melissa", ""], ["Yu", "Yaoliang", ""], ["Li", "Weining", ""]]}, {"id": "2010.01773", "submitter": "Xin Liu", "authors": "Xin Liu, Ziheng Jiang, Josh Fromm, Xuhai Xu, Shwetak Patel, Daniel\n  McDuff", "title": "MetaPhys: Few-Shot Adaptation for Non-Contact Physiological Measurement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are large individual differences in physiological processes, making\ndesigning personalized health sensing algorithms challenging. Existing machine\nlearning systems struggle to generalize well to unseen subjects or contexts and\ncan often contain problematic biases. Video-based physiological measurement is\nnot an exception. Therefore, learning personalized or customized models from a\nsmall number of unlabeled samples is very attractive as it would allow fast\ncalibrations to improve generalization and help correct biases. In this paper,\nwe present a novel meta-learning approach called MetaPhys for personalized\nvideo-based cardiac measurement for contactless pulse and heart rate\nmonitoring. Our method uses only 18-seconds of video for customization and\nworks effectively in both supervised and unsupervised manners. We evaluate our\nproposed approach on two benchmark datasets and demonstrate superior\nperformance in cross-dataset evaluation with substantial reductions (42% to\n44%) in errors compared with state-of-the-art approaches. We have also\ndemonstrated our proposed method significantly helps reduce the bias in skin\ntype.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 04:41:03 GMT"}, {"version": "v2", "created": "Sun, 28 Feb 2021 18:39:40 GMT"}, {"version": "v3", "created": "Sat, 6 Mar 2021 04:37:54 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Liu", "Xin", ""], ["Jiang", "Ziheng", ""], ["Fromm", "Josh", ""], ["Xu", "Xuhai", ""], ["Patel", "Shwetak", ""], ["McDuff", "Daniel", ""]]}, {"id": "2010.01775", "submitter": "Shilin Zhu", "authors": "Shilin Zhu, Zexiang Xu, Tiancheng Sun, Alexandr Kuznetsov, Mark Meyer,\n  Henrik Wann Jensen, Hao Su, Ravi Ramamoorthi", "title": "Photon-Driven Neural Path Guiding", "comments": "Keywords: computer graphics, rendering, path tracing, path guiding,\n  machine learning, neural networks, denoising, reconstruction", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although Monte Carlo path tracing is a simple and effective algorithm to\nsynthesize photo-realistic images, it is often very slow to converge to\nnoise-free results when involving complex global illumination. One of the most\nsuccessful variance-reduction techniques is path guiding, which can learn\nbetter distributions for importance sampling to reduce pixel noise. However,\nprevious methods require a large number of path samples to achieve reliable\npath guiding. We present a novel neural path guiding approach that can\nreconstruct high-quality sampling distributions for path guiding from a sparse\nset of samples, using an offline trained neural network. We leverage photons\ntraced from light sources as the input for sampling density reconstruction,\nwhich is highly effective for challenging scenes with strong global\nillumination. To fully make use of our deep neural network, we partition the\nscene space into an adaptive hierarchical grid, in which we apply our network\nto reconstruct high-quality sampling distributions for any local region in the\nscene. This allows for highly efficient path guiding for any path bounce at any\nlocation in path tracing. We demonstrate that our photon-driven neural path\nguiding method can generalize well on diverse challenging testing scenes that\nare not seen in training. Our approach achieves significantly better rendering\nresults of testing scenes than previous state-of-the-art path guiding methods.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 04:54:01 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Zhu", "Shilin", ""], ["Xu", "Zexiang", ""], ["Sun", "Tiancheng", ""], ["Kuznetsov", "Alexandr", ""], ["Meyer", "Mark", ""], ["Jensen", "Henrik Wann", ""], ["Su", "Hao", ""], ["Ramamoorthi", "Ravi", ""]]}, {"id": "2010.01792", "submitter": "Sheikh Shams Azam", "authors": "Sheikh Shams Azam, Taejin Kim, Seyyedali Hosseinalipour, Christopher\n  Brinton, Carlee Joe-Wong, Saurabh Bagchi", "title": "Towards Generalized and Distributed Privacy-Preserving Representation\n  Learning", "comments": "18 pages, 22 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.MA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Privacy-preserving representation learning (PPRL) aims to learn a data\nencoding that obfuscates sensitive information and retains target information.\nWe develop the Exclusion-Inclusion Generative Adversarial Network (EIGAN),\nwhich generalizes existing adversarial PPRL approaches to account for multiple,\npotentially overlapping ally and adversary objectives in a dataset. We further\nextend EIGAN to the case where the data is distributed and cannot be centrally\naggregated for training due to privacy constraints. In doing so, we introduce\nD-EIGAN, the first distributed PPRL method, which decentralizes EIGAN training\nbased on federated learning with fractional parameter sharing. We theoretically\nanalyze the convergence of EIGAN and behavior of adversaries under the optimal\nEIGAN and D-EIGAN encoders, considering the impact of dependencies among target\nand sensitive objectives on the encoder performance. Our experiments\ndemonstrate the advantages of EIGAN encodings in terms of accuracy, robustness,\nand scalability; EIGAN outperforms the previous state-of-the-art in centralized\nPPRL by a significant margin (47%). The experiments further reveal that\nD-EIGAN's performance is consistent with that of EIGAN under different node\ndata distributions and is resilient to communication constraints.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 05:43:47 GMT"}, {"version": "v2", "created": "Sat, 17 Oct 2020 22:30:21 GMT"}, {"version": "v3", "created": "Sun, 7 Feb 2021 01:59:04 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Azam", "Sheikh Shams", ""], ["Kim", "Taejin", ""], ["Hosseinalipour", "Seyyedali", ""], ["Brinton", "Christopher", ""], ["Joe-Wong", "Carlee", ""], ["Bagchi", "Saurabh", ""]]}, {"id": "2010.01795", "submitter": "Isha Garg", "authors": "Isha Garg, Sayeed Shafayet Chowdhury and Kaushik Roy", "title": "DCT-SNN: Using DCT to Distribute Spatial Information over Time for\n  Learning Low-Latency Spiking Neural Networks", "comments": "The first two authors contributed equally to this paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spiking Neural Networks (SNNs) offer a promising alternative to traditional\ndeep learning frameworks, since they provide higher computational efficiency\ndue to event-driven information processing. SNNs distribute the analog values\nof pixel intensities into binary spikes over time. However, the most widely\nused input coding schemes, such as Poisson based rate-coding, do not leverage\nthe additional temporal learning capability of SNNs effectively. Moreover,\nthese SNNs suffer from high inference latency which is a major bottleneck to\ntheir deployment. To overcome this, we propose a scalable time-based encoding\nscheme that utilizes the Discrete Cosine Transform (DCT) to reduce the number\nof timesteps required for inference. DCT decomposes an image into a weighted\nsum of sinusoidal basis images. At each time step, the Hadamard product of the\nDCT coefficients and a single frequency base, taken in order, is given to an\naccumulator that generates spikes upon crossing a threshold. We use the\nproposed scheme to learn DCT-SNN, a low-latency deep SNN with\nleaky-integrate-and-fire neurons, trained using surrogate gradient descent\nbased backpropagation. We achieve top-1 accuracy of 89.94%, 68.3% and 52.43% on\nCIFAR-10, CIFAR-100 and TinyImageNet, respectively using VGG architectures.\nNotably, DCT-SNN performs inference with 2-14X reduced latency compared to\nother state-of-the-art SNNs, while achieving comparable accuracy to their\nstandard deep learning counterparts. The dimension of the transform allows us\nto control the number of timesteps required for inference. Additionally, we can\ntrade-off accuracy with latency in a principled manner by dropping the highest\nfrequency components during inference.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 05:55:34 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Garg", "Isha", ""], ["Chowdhury", "Sayeed Shafayet", ""], ["Roy", "Kaushik", ""]]}, {"id": "2010.01809", "submitter": "Xudong Wang", "authors": "Xudong Wang, Long Lian, Zhongqi Miao, Ziwei Liu, Stella X. Yu", "title": "Long-tailed Recognition by Routing Diverse Distribution-Aware Experts", "comments": "Accepted at ICLR 2021 (Spotlight); Update the bias-variance\n  decomposition section", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural data are often long-tail distributed over semantic classes. Existing\nrecognition methods tackle this imbalanced classification by placing more\nemphasis on the tail data, through class re-balancing/re-weighting or\nensembling over different data groups, resulting in increased tail accuracies\nbut reduced head accuracies.\n  We take a dynamic view of the training data and provide a principled model\nbias and variance analysis as the training data fluctuates: Existing long-tail\nclassifiers invariably increase the model variance and the head-tail model bias\ngap remains large, due to more and larger confusion with hard negatives for the\ntail.\n  We propose a new long-tailed classifier called RoutIng Diverse Experts\n(RIDE). It reduces the model variance with multiple experts, reduces the model\nbias with a distribution-aware diversity loss, reduces the computational cost\nwith a dynamic expert routing module. RIDE outperforms the state-of-the-art by\n5% to 7% on CIFAR100-LT, ImageNet-LT and iNaturalist 2018 benchmarks. It is\nalso a universal framework that is applicable to various backbone networks,\nlong-tailed algorithms, and training mechanisms for consistent performance\ngains. Our code is available at:\nhttps://github.com/frank-xwang/RIDE-LongTailRecognition.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 06:53:44 GMT"}, {"version": "v2", "created": "Wed, 7 Apr 2021 06:37:20 GMT"}, {"version": "v3", "created": "Sat, 15 May 2021 23:05:59 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Wang", "Xudong", ""], ["Lian", "Long", ""], ["Miao", "Zhongqi", ""], ["Liu", "Ziwei", ""], ["Yu", "Stella X.", ""]]}, {"id": "2010.01810", "submitter": "Kyunghun Kim", "authors": "Kyunghun Kim, Yeohun Yun, Keon-Woo Kang, Kyeongbo Kong, Siyeong Lee,\n  Suk-Ju Kang", "title": "Painting Outside as Inside: Edge Guided Image Outpainting via\n  Bidirectional Rearrangement with Progressive Step Learning", "comments": "Paper accepted in WACV 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image outpainting is a very intriguing problem as the outside of a given\nimage can be continuously filled by considering as the context of the image.\nThis task has two main challenges. The first is to maintain the spatial\nconsistency in contents of generated regions and the original input. The second\nis to generate a high-quality large image with a small amount of adjacent\ninformation. Conventional image outpainting methods generate inconsistent,\nblurry, and repeated pixels. To alleviate the difficulty of an outpainting\nproblem, we propose a novel image outpainting method using bidirectional\nboundary region rearrangement. We rearrange the image to benefit from the image\ninpainting task by reflecting more directional information. The bidirectional\nboundary region rearrangement enables the generation of the missing region\nusing bidirectional information similar to that of the image inpainting task,\nthereby generating the higher quality than the conventional methods using\nunidirectional information. Moreover, we use the edge map generator that\nconsiders images as original input with structural information and hallucinates\nthe edges of unknown regions to generate the image. Our proposed method is\ncompared with other state-of-the-art outpainting and inpainting methods both\nqualitatively and quantitatively. We further compared and evaluated them using\nBRISQUE, one of the No-Reference image quality assessment (IQA) metrics, to\nevaluate the naturalness of the output. The experimental results demonstrate\nthat our method outperforms other methods and generates new images with\n360{\\deg}panoramic characteristics.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 06:53:55 GMT"}, {"version": "v2", "created": "Mon, 9 Nov 2020 05:18:19 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Kim", "Kyunghun", ""], ["Yun", "Yeohun", ""], ["Kang", "Keon-Woo", ""], ["Kong", "Kyeongbo", ""], ["Lee", "Siyeong", ""], ["Kang", "Suk-Ju", ""]]}, {"id": "2010.01823", "submitter": "Duy Vo Nguyen Le", "authors": "Vo Nguyen Le Duy, Shogo Iwazaki, Ichiro Takeuchi", "title": "Quantifying Statistical Significance of Neural Network\n  Representation-Driven Hypotheses by Selective Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the past few years, various approaches have been developed to explain and\ninterpret deep neural network (DNN) representations, but it has been pointed\nout that these representations are sometimes unstable and not reproducible. In\nthis paper, we interpret these representations as hypotheses driven by DNN\n(called DNN-driven hypotheses) and propose a method to quantify the reliability\nof these hypotheses in statistical hypothesis testing framework. To this end,\nwe introduce Selective Inference (SI) framework, which has received much\nattention in the past few years as a new statistical inference framework for\ndata-driven hypotheses. The basic idea of SI is to make conditional inferences\non the selected hypotheses under the condition that they are selected. In order\nto use SI framework for DNN representations, we develop a new SI algorithm\nbased on homotopy method which enables us to derive the exact (non-asymptotic)\nconditional sampling distribution of the DNN-driven hypotheses. We conduct\nexperiments on both synthetic and real-world datasets, through which we offer\nevidence that our proposed method can successfully control the false positive\nrate, has decent performance in terms of computational efficiency, and provides\ngood results in practical applications.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 07:16:40 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Duy", "Vo Nguyen Le", ""], ["Iwazaki", "Shogo", ""], ["Takeuchi", "Ichiro", ""]]}, {"id": "2010.01824", "submitter": "Saptarshi Sinha", "authors": "Saptarshi Sinha, Hiroki Ohashi and Katsuyuki Nakamura", "title": "Class-Wise Difficulty-Balanced Loss for Solving Class-Imbalance", "comments": "Accepted for ACCV 2020 oral presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Class-imbalance is one of the major challenges in real world datasets, where\na few classes (called majority classes) constitute much more data samples than\nthe rest (called minority classes). Learning deep neural networks using such\ndatasets leads to performances that are typically biased towards the majority\nclasses. Most of the prior works try to solve class-imbalance by assigning more\nweights to the minority classes in various manners (e.g., data re-sampling,\ncost-sensitive learning). However, we argue that the number of available\ntraining data may not be always a good clue to determine the weighting strategy\nbecause some of the minority classes might be sufficiently represented even by\na small number of training data. Overweighting samples of such classes can lead\nto drop in the model's overall performance. We claim that the 'difficulty' of a\nclass as perceived by the model is more important to determine the weighting.\nIn this light, we propose a novel loss function named Class-wise\nDifficulty-Balanced loss, or CDB loss, which dynamically distributes weights to\neach sample according to the difficulty of the class that the sample belongs\nto. Note that the assigned weights dynamically change as the 'difficulty' for\nthe model may change with the learning progress. Extensive experiments are\nconducted on both image (artificially induced class-imbalanced MNIST,\nlong-tailed CIFAR and ImageNet-LT) and video (EGTEA) datasets. The results show\nthat CDB loss consistently outperforms the recently proposed loss functions on\nclass-imbalanced datasets irrespective of the data type (i.e., video or image).\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 07:19:19 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Sinha", "Saptarshi", ""], ["Ohashi", "Hiroki", ""], ["Nakamura", "Katsuyuki", ""]]}, {"id": "2010.01841", "submitter": "Seyesaeid Mirkamali", "authors": "Seyedsaeid Mirkamali, P. Nagabhushan", "title": "Depth-wise layering of 3d images using dense depth maps: a threshold\n  based approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image segmentation has long been a basic problem in computer vision.\nDepth-wise Layering is a kind of segmentation that slices an image in a\ndepth-wise sequence unlike the conventional image segmentation problems dealing\nwith surface-wise decomposition. The proposed Depth-wise Layering technique\nuses a single depth image of a static scene to slice it into multiple layers.\nThe technique employs a thresholding approach to segment rows of the dense\ndepth map into smaller partitions called Line-Segments in this paper. Then, it\nuses the line-segment labelling method to identify number of objects and layers\nof the scene independently. The final stage is to link objects of the scene to\ntheir respective object-layers. We evaluate the efficiency of the proposed\ntechnique by applying that on many images along with their dense depth maps.\nThe experiments have shown promising results of layering.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 07:55:18 GMT"}], "update_date": "2020-11-22", "authors_parsed": [["Mirkamali", "Seyedsaeid", ""], ["Nagabhushan", "P.", ""]]}, {"id": "2010.01863", "submitter": "Fang Aiqing", "authors": "Aiqing Fang, Xinbo Zhao, Jiaqi Yang, Beibei Qin, Yanning Zhang", "title": "AE-Netv2: Optimization of Image Fusion Efficiency and Network\n  Architecture", "comments": "Some mistakes have been fixed", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing image fusion methods pay few research attention to image fusion\nefficiency and network architecture. However, the efficiency and accuracy of\nimage fusion has an important impact in practical applications. To solve this\nproblem, we propose an \\textit{efficient autonomous evolution image fusion\nmethod, dubed by AE-Netv2}. Different from other image fusion methods based on\ndeep learning, AE-Netv2 is inspired by human brain cognitive mechanism.\nFirstly, we discuss the influence of different network architecture on image\nfusion quality and fusion efficiency, which provides a reference for the design\nof image fusion architecture. Secondly, we explore the influence of pooling\nlayer on image fusion task and propose an image fusion method with pooling\nlayer. Finally, we explore the commonness and characteristics of different\nimage fusion tasks, which provides a research basis for further research on the\ncontinuous learning characteristics of human brain in the field of image\nfusion. Comprehensive experiments demonstrate the superiority of AE-Netv2\ncompared with state-of-the-art methods in different fusion tasks at a real time\nspeed of 100+ FPS on GTX 2070. Among all tested methods based on deep learning,\nAE-Netv2 has the faster speed, the smaller model size and the better\nrobustness.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 08:58:10 GMT"}, {"version": "v2", "created": "Tue, 6 Oct 2020 07:58:49 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Fang", "Aiqing", ""], ["Zhao", "Xinbo", ""], ["Yang", "Jiaqi", ""], ["Qin", "Beibei", ""], ["Zhang", "Yanning", ""]]}, {"id": "2010.01872", "submitter": "Chee-Kheng Chng Ck", "authors": "Chee-Kheng Chng, Alvaro Parra, Tat-Jun Chin, Yasir Latif", "title": "Monocular Rotational Odometry with Incremental Rotation Averaging and\n  Loop Closure", "comments": "Accepted to DICTA 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating absolute camera orientations is essential for attitude estimation\ntasks. An established approach is to first carry out visual odometry (VO) or\nvisual SLAM (V-SLAM), and retrieve the camera orientations (3 DOF) from the\ncamera poses (6 DOF) estimated by VO or V-SLAM. One drawback of this approach,\nbesides the redundancy in estimating full 6 DOF camera poses, is the dependency\non estimating a map (3D scene points) jointly with the 6 DOF poses due to the\nbasic constraint on structure-and-motion. To simplify the task of absolute\norientation estimation, we formulate the monocular rotational odometry problem\nand devise a fast algorithm to accurately estimate camera orientations with\n2D-2D feature matches alone. Underpinning our system is a new incremental\nrotation averaging method for fast and constant time iterative updating.\nFurthermore, our system maintains a view-graph that 1) allows solving loop\nclosure to remove camera orientation drift, and 2) can be used to warm start a\nV-SLAM system. We conduct extensive quantitative experiments on real-world\ndatasets to demonstrate the accuracy of our incremental camera orientation\nsolver. Finally, we showcase the benefit of our algorithm to V-SLAM: 1) solving\nthe known rotation problem to estimate the trajectory of the camera and the\nsurrounding map, and 2)enabling V-SLAM systems to track pure rotational\nmotions.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 09:19:06 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Chng", "Chee-Kheng", ""], ["Parra", "Alvaro", ""], ["Chin", "Tat-Jun", ""], ["Latif", "Yasir", ""]]}, {"id": "2010.01884", "submitter": "Pascal Colling", "authors": "Pascal Colling, Lutz Roese-Koerner, Hanno Gottschalk, Matthias\n  Rottmann", "title": "MetaBox+: A new Region Based Active Learning Method for Semantic\n  Segmentation using Priority Maps", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel region based active learning method for semantic image\nsegmentation, called MetaBox+. For acquisition, we train a meta regression\nmodel to estimate the segment-wise Intersection over Union (IoU) of each\npredicted segment of unlabeled images. This can be understood as an estimation\nof segment-wise prediction quality. Queried regions are supposed to minimize to\ncompeting targets, i.e., low predicted IoU values / segmentation quality and\nlow estimated annotation costs. For estimating the latter we propose a simple\nbut practical method for annotation cost estimation. We compare our method to\nentropy based methods, where we consider the entropy as uncertainty of the\nprediction. The comparison and analysis of the results provide insights into\nannotation costs as well as robustness and variance of the methods. Numerical\nexperiments conducted with two different networks on the Cityscapes dataset\nclearly demonstrate a reduction of annotation effort compared to random\nacquisition. Noteworthily, we achieve 95%of the mean Intersection over Union\n(mIoU), using MetaBox+ compared to when training with the full dataset, with\nonly 10.47% / 32.01% annotation effort for the two networks, respectively.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 09:36:47 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Colling", "Pascal", ""], ["Roese-Koerner", "Lutz", ""], ["Gottschalk", "Hanno", ""], ["Rottmann", "Matthias", ""]]}, {"id": "2010.01892", "submitter": "Jan Klopp", "authors": "Po-Hsiang Yu, Sih-Sian Wu, Jan P. Klopp, Liang-Gee Chen, Shao-Yi Chien", "title": "Joint Pruning & Quantization for Extremely Sparse Neural Networks", "comments": "13 page, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate pruning and quantization for deep neural networks. Our goal is\nto achieve extremely high sparsity for quantized networks to enable\nimplementation on low cost and low power accelerator hardware. In a practical\nscenario, there are particularly many applications for dense prediction tasks,\nhence we choose stereo depth estimation as target.\n  We propose a two stage pruning and quantization pipeline and introduce a\nTaylor Score alongside a new fine-tuning mode to achieve extreme sparsity\nwithout sacrificing performance.\n  Our evaluation does not only show that pruning and quantization should be\ninvestigated jointly, but also shows that almost 99% of memory demand can be\ncut while hardware costs can be reduced up to 99.9%. In addition, to compare\nwith other works, we demonstrate that our pruning stage alone beats the\nstate-of-the-art when applied to ResNet on CIFAR10 and ImageNet.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 10:04:29 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Yu", "Po-Hsiang", ""], ["Wu", "Sih-Sian", ""], ["Klopp", "Jan P.", ""], ["Chen", "Liang-Gee", ""], ["Chien", "Shao-Yi", ""]]}, {"id": "2010.01895", "submitter": "Hamid Sarmadi", "authors": "Hamid Sarmadi, Rafael Mu\\~noz-Salinas, M. \\'Alvaro Berb\\'is, Antonio\n  Luna, R. Medina-Carnicer", "title": "Joint Scene and Object Tracking for Cost-Effective Augmented Reality\n  Assisted Patient Positioning in Radiation Therapy", "comments": "16 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  BACKGROUND AND OBJECTIVE: The research done in the field of Augmented Reality\n(AR) for patient positioning in radiation therapy is scarce. We propose an\nefficient and cost-effective algorithm for tracking the scene and the patient\nto interactively assist the patient's positioning process by providing visual\nfeedback to the operator. Up to our knowledge, this is the first framework that\ncan be employed for mobile interactive AR to guide patient positioning.\nMETHODS: We propose a point cloud processing method that combined with a\nfiducial marker-mapper algorithm and the generalized ICP algorithm tracks the\npatient and the camera precisely and efficiently only using the CPU unit. The\nalignment between the 3D reference model and body marker map is calculated\nemploying an efficient body reconstruction algorithm. RESULTS: Our quantitative\nevaluation shows that the proposed method achieves a translational and\nrotational error of 4.17 mm/0.82 deg at 9 fps. Furthermore, the qualitative\nresults demonstrate the usefulness of our algorithm in patient positioning on\ndifferent human subjects. CONCLUSION: Since our algorithm achieves a relatively\nhigh frame rate and accuracy employing a regular laptop (without the usage of a\ndedicated GPU), it is a very cost-effective AR-based patient positioning\nmethod. It also opens the way for other researchers by introducing a framework\nthat could be improved upon for better mobile interactive AR patient\npositioning solutions in the future.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 10:20:46 GMT"}, {"version": "v2", "created": "Mon, 11 Jan 2021 07:40:18 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Sarmadi", "Hamid", ""], ["Mu\u00f1oz-Salinas", "Rafael", ""], ["Berb\u00eds", "M. \u00c1lvaro", ""], ["Luna", "Antonio", ""], ["Medina-Carnicer", "R.", ""]]}, {"id": "2010.01910", "submitter": "Alina Marcu M.Sc", "authors": "Alina Marcu, Vlad Licaret, Dragos Costea and Marius Leordeanu", "title": "Semantics through Time: Semi-supervised Segmentation of Aerial Videos\n  with Iterative Label Propagation", "comments": "Accepted as oral presentation at Asian Conference on Computer Vision\n  (ACCV), 2020. arXiv admin note: text overlap with arXiv:1910.10026", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic segmentation is a crucial task for robot navigation and safety.\nHowever, current supervised methods require a large amount of pixelwise\nannotations to yield accurate results. Labeling is a tedious and time consuming\nprocess that has hampered progress in low altitude UAV applications. This paper\nmakes an important step towards automatic annotation by introducing SegProp, a\nnovel iterative flow-based method, with a direct connection to spectral\nclustering in space and time, to propagate the semantic labels to frames that\nlack human annotations. The labels are further used in semi-supervised learning\nscenarios. Motivated by the lack of a large video aerial dataset, we also\nintroduce Ruralscapes, a new dataset with high resolution (4K) images and\nmanually-annotated dense labels every 50 frames - the largest of its kind, to\nthe best of our knowledge. Our novel SegProp automatically annotates the\nremaining unlabeled 98% of frames with an accuracy exceeding 90% (F-measure),\nsignificantly outperforming other state-of-the-art label propagation methods.\nMoreover, when integrating other methods as modules inside SegProp's iterative\nlabel propagation loop, we achieve a significant boost over the baseline\nlabels. Finally, we test SegProp in a full semi-supervised setting: we train\nseveral state-of-the-art deep neural networks on the\nSegProp-automatically-labeled training frames and test them on completely novel\nvideos. We convincingly demonstrate, every time, a significant improvement over\nthe supervised scenario.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 15:15:50 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Marcu", "Alina", ""], ["Licaret", "Vlad", ""], ["Costea", "Dragos", ""], ["Leordeanu", "Marius", ""]]}, {"id": "2010.01912", "submitter": "Amnon Drory", "authors": "Amnon Drory, Tal Shomer, Shai Avidan and Raja Giryes", "title": "Best Buddies Registration for Point Clouds", "comments": "Accepted to ACCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose new, and robust, loss functions for the point cloud registration\nproblem. Our loss functions are inspired by the Best Buddies Similarity (BBS)\nmeasure that counts the number of mutual nearest neighbors between two point\nsets. This measure has been shown to be robust to outliers and missing data in\nthe case of template matching for images. We present several algorithms,\ncollectively named Best Buddy Registration (BBR), where each algorithm consists\nof optimizing one of these loss functions with Adam gradient descent. The loss\nfunctions differ in several ways, including the distance function used\n(point-to-point vs. point-to-plane), and how the BBS measure is combined with\nthe actual distances between pairs of points. Experiments on various data sets,\nboth synthetic and real, demonstrate the effectiveness of the BBR algorithms,\nshowing that they are quite robust to noise, outliers, and distractors, and\ncope well with extremely sparse point clouds. One variant, BBR-F, achieves\nstate-of-the-art accuracy in the registration of automotive lidar scans taken\nup to several seconds apart, from the KITTI and Apollo-Southbay datasets.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 10:49:18 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Drory", "Amnon", ""], ["Shomer", "Tal", ""], ["Avidan", "Shai", ""], ["Giryes", "Raja", ""]]}, {"id": "2010.01919", "submitter": "Jiawei Liu", "authors": "Jiawei Liu, Qiang Wang, Huijie Fan, Shuai Wang, Wentao Li, Yandong\n  Tang, Danbo Wang, Mingyi Zhou, Li Chen", "title": "Automatic Label Correction for the Accurate Edge Detection of\n  Overlapping Cervical Cells", "comments": "code and dataset:\n  https://github.com/nachifur/automatic-label-correction-CCEDD", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate labeling is essential for supervised deep learning methods. In this\npaper, to accurately segment images of multiple overlapping cervical cells with\ndeep learning models, we propose an automatic label correction algorithm to\nimprove the edge positioning accuracy of overlapping cervical cells in manual\nlabeling. Our algorithm is designed based on gradient guidance, and can\nautomatically correct edge positions for overlapping cervical cells and\ndifferences among manual labeling with different annotators. Using the proposed\nalgorithm, we constructed an open cervical cell edge detection dataset (CCEDD)\nwith high labeling accuracy. The experiments on the dataset for training show\nthat our automatic label correction algorithm can improve the accuracy of\nmanual labels and further improve the positioning accuracy of overlapping cells\nwith deep learning models. We have released the dataset and code at\nhttps://github.com/nachifur/automatic-label-correction-CCEDD.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 11:01:45 GMT"}, {"version": "v2", "created": "Tue, 13 Jul 2021 12:38:04 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Liu", "Jiawei", ""], ["Wang", "Qiang", ""], ["Fan", "Huijie", ""], ["Wang", "Shuai", ""], ["Li", "Wentao", ""], ["Tang", "Yandong", ""], ["Wang", "Danbo", ""], ["Zhou", "Mingyi", ""], ["Chen", "Li", ""]]}, {"id": "2010.01926", "submitter": "Thomas Varsavsky", "authors": "Thomas Varsavsky, Mauricio Orbes-Arteaga, Carole H. Sudre, Mark S.\n  Graham, Parashkev Nachev, M. Jorge Cardoso", "title": "Test-time Unsupervised Domain Adaptation", "comments": "Accepted at MICCAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks trained on publicly available medical imaging\ndatasets (source domain) rarely generalise to different scanners or acquisition\nprotocols (target domain). This motivates the active field of domain\nadaptation. While some approaches to the problem require labeled data from the\ntarget domain, others adopt an unsupervised approach to domain adaptation\n(UDA). Evaluating UDA methods consists of measuring the model's ability to\ngeneralise to unseen data in the target domain. In this work, we argue that\nthis is not as useful as adapting to the test set directly. We therefore\npropose an evaluation framework where we perform test-time UDA on each subject\nseparately. We show that models adapted to a specific target subject from the\ntarget domain outperform a domain adaptation method which has seen more data of\nthe target domain but not this specific target subject. This result supports\nthe thesis that unsupervised domain adaptation should be used at test-time,\neven if only using a single target-domain subject\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 11:30:36 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Varsavsky", "Thomas", ""], ["Orbes-Arteaga", "Mauricio", ""], ["Sudre", "Carole H.", ""], ["Graham", "Mark S.", ""], ["Nachev", "Parashkev", ""], ["Cardoso", "M. Jorge", ""]]}, {"id": "2010.01929", "submitter": "Benjin Zhu", "authors": "Benjin Zhu, Junqiang Huang, Zeming Li, Xiangyu Zhang, Jian Sun", "title": "EqCo: Equivalent Rules for Self-supervised Contrastive Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a method, named EqCo (Equivalent Rules for\nContrastive Learning), to make self-supervised learning irrelevant to the\nnumber of negative samples in InfoNCE-based contrastive learning frameworks.\nInspired by the InfoMax principle, we point that the margin term in contrastive\nloss needs to be adaptively scaled according to the number of negative pairs in\norder to keep steady mutual information bound and gradient magnitude. EqCo\nbridges the performance gap among a wide range of negative sample sizes, so\nthat we can use only a few negative pairs (e.g. 16 per query) to perform\nself-supervised contrastive training on large-scale vision datasets like\nImageNet, while with almost no accuracy drop. This is quite a contrast to the\nwidely used large batch training or memory bank mechanism in current practices.\nEquipped with EqCo, our simplified MoCo (SiMo) achieves comparable accuracy\nwith MoCo v2 on ImageNet (linear evaluation protocol) while only involves 4\nnegative pairs per query instead of 65536, suggesting that large quantities of\nnegative samples might not be a critical factor in InfoNCE loss.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 11:39:04 GMT"}, {"version": "v2", "created": "Mon, 30 Nov 2020 11:20:23 GMT"}, {"version": "v3", "created": "Mon, 15 Mar 2021 04:53:58 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Zhu", "Benjin", ""], ["Huang", "Junqiang", ""], ["Li", "Zeming", ""], ["Zhang", "Xiangyu", ""], ["Sun", "Jian", ""]]}, {"id": "2010.01942", "submitter": "Bao Nguyen", "authors": "Bao Nguyen, Adam Feldman, Sarath Bethapudi, Andrew Jennings, Chris G.\n  Willcocks", "title": "Unsupervised Region-based Anomaly Detection in Brain MRI with\n  Adversarial Image Inpainting", "comments": "5 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Medical segmentation is performed to determine the bounds of regions of\ninterest (ROI) prior to surgery. By allowing the study of growth, structure,\nand behaviour of the ROI in the planning phase, critical information can be\nobtained, increasing the likelihood of a successful operation. Usually,\nsegmentations are performed manually or via machine learning methods trained on\nmanual annotations. In contrast, this paper proposes a fully automatic,\nunsupervised inpainting-based brain tumour segmentation system for T1-weighted\nMRI. First, a deep convolutional neural network (DCNN) is trained to\nreconstruct missing healthy brain regions. Then, upon application, anomalous\nregions are determined by identifying areas of highest reconstruction loss.\nFinally, superpixel segmentation is performed to segment those regions. We show\nthe proposed system is able to segment various sized and abstract tumours and\nachieves a mean and standard deviation Dice score of 0.771 and 0.176,\nrespectively.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 12:13:44 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Nguyen", "Bao", ""], ["Feldman", "Adam", ""], ["Bethapudi", "Sarath", ""], ["Jennings", "Andrew", ""], ["Willcocks", "Chris G.", ""]]}, {"id": "2010.01947", "submitter": "David Azcona", "authors": "David Azcona, Kevin McGuinness and Alan F. Smeaton", "title": "A Comparative Study of Existing and New Deep Learning Methods for\n  Detecting Knee Injuries using the MRNet Dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This work presents a comparative study of existing and new techniques to\ndetect knee injuries by leveraging Stanford's MRNet Dataset. All approaches are\nbased on deep learning and we explore the comparative performances of transfer\nlearning and a deep residual network trained from scratch. We also exploit some\ncharacteristics of Magnetic Resonance Imaging (MRI) data by, for example, using\na fixed number of slices or 2D images from each of the axial, coronal and\nsagittal planes as well as combining the three planes into one multi-plane\nnetwork. Overall we achieved a performance of 93.4% AUC on the validation data\nby using the more recent deep learning architectures and data augmentation\nstrategies. More flexible architectures are also proposed that might help with\nthe development and training of models that process MRIs. We found that\ntransfer learning and a carefully tuned data augmentation strategy were the\ncrucial factors in determining best performance.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 12:27:18 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Azcona", "David", ""], ["McGuinness", "Kevin", ""], ["Smeaton", "Alan F.", ""]]}, {"id": "2010.01968", "submitter": "Fazlolah Mohaghegh", "authors": "Fazlolah Mohaghegh, Jayathi Murthy", "title": "Machine Learning and Computer Vision Techniques to Predict Thermal\n  Properties of Particulate Composites", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.app-ph cs.CV cs.LG physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate thermal analysis of composites and porous media requires detailed\ncharacterization of local thermal properties in small scale. For some important\napplications such as lithium-ion batteries, changes in the properties during\nthe operation makes the analysis even more challenging, necessitating a rapid\ncharacterization. We propose a new method to characterize the thermal\nproperties of particulate composites based on actual micro-images. Our\ncomputer-vision-based approach constructs 3D images from stacks of 2D SEM\nimages and then extracts several representative elemental volumes (REVs) from\nthe reconstructed images at random places, which leads to having a range of\ngeometrical features for different REVs. A deep learning algorithm is designed\nbased on convolutional neural nets to take the shape of the geometry and result\nin the effective conductivity of the REV. The training of the network is\nperformed in two methods: First, based on implementing a coarser grid that uses\nthe average values of conductivities from the fine grid and the resulted\neffective conductivity from the DNS solution of the fine grid. The other method\nuses conductivity values on cross sections from each REV in different\ndirections. The results of training based on averaging show that using a\ncoarser grid in the network does not have a meaningful effect on the network\nerror; however, it decreases the training time up to three orders of magnitude.\nWe showed that one general network can make accurate predictions using\ndifferent types of electrode images, representing the difference in the\ngeometry and constituents. Moreover, training based on averaging is more\naccurate than training based on cross sections. The study of the robustness of\nimplementing a machine learning technique in predicting the thermal percolation\nshows the prediction error is almost half of the error from predictions based\non the volume fraction.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2020 06:21:40 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Mohaghegh", "Fazlolah", ""], ["Murthy", "Jayathi", ""]]}, {"id": "2010.01982", "submitter": "Omar Alirr Dr.", "authors": "Omar Ibrahim Alirr", "title": "Automatic Deep Learning System for COVID-19 Infection Quantification in\n  chest CT", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coronavirus Disease spread globally and infected millions of people quickly,\ncausing high pressure on the health-system facilities. PCR screening is the\nadopted diagnostic testing method for COVID-19 detection. However, PCR is\ncriticized due its low sensitivity ratios, also, it is time-consuming and\nmanual complicated process. CT imaging proved its ability to detect the disease\neven for asymptotic patients, which make it a trustworthy alternative for PCR.\nIn addition, the appearance of COVID-19 infections in CT slices, offers high\npotential to support in disease evolution monitoring using automated infection\nsegmentation methods. However, COVID-19 infection areas include high variations\nin term of size, shape, contrast and intensity homogeneity, which impose a big\nchallenge on segmentation process. To address these challenges, this paper\nproposed an automatic deep learning system for COVID-19 infection areas\nsegmentation. The system include different steps to enhance and improve\ninfection areas appearance in the CT slices so they can be learned efficiently\nusing the deep network. The system start prepare the region of interest by\nsegmenting the lung organ, which then undergo edge enhancing diffusion\nfiltering (EED) to improve the infection areas contrast and intensity\nhomogeneity. The proposed FCN is implemented using U-net architecture with\nmodified residual block with concatenation skip connection. The block improves\nthe learning of gradient values by forwarding the infection area features\nthrough the network. To demonstrate the generalization and effectiveness of the\nproposed system, it is trained and tested using many 2D CT slices extracted\nfrom diverse datasets from different sources. The proposed system is evaluated\nusing different measures and achieved dice overlapping score of 0.961 and 0.780\nfor lung and infection areas segmentation, respectively.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 21:05:59 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Alirr", "Omar Ibrahim", ""]]}, {"id": "2010.01999", "submitter": "Ruchika Chavhan", "authors": "Ruchika Chavhan, Biplab Banerjee, Xiao Xiang Zhu, and Subhasis\n  Chaudhuri", "title": "A Novel Actor Dual-Critic Model for Remote Sensing Image Captioning", "comments": "8 pages, 21 figures Accepted at the International Conference on\n  Pattern Recognition (ICPR) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We deal with the problem of generating textual captions from optical remote\nsensing (RS) images using the notion of deep reinforcement learning. Due to the\nhigh inter-class similarity in reference sentences describing remote sensing\ndata, jointly encoding the sentences and images encourages prediction of\ncaptions that are semantically more precise than the ground truth in many\ncases. To this end, we introduce an Actor Dual-Critic training strategy where a\nsecond critic model is deployed in the form of an encoder-decoder RNN to encode\nthe latent information corresponding to the original and generated captions.\nWhile all actor-critic methods use an actor to predict sentences for an image\nand a critic to provide rewards, our proposed encoder-decoder RNN guarantees\nhigh-level comprehension of images by sentence-to-image translation. We observe\nthat the proposed model generates sentences on the test data highly similar to\nthe ground truth and is successful in generating even better captions in many\ncritical cases. Extensive experiments on the benchmark Remote Sensing Image\nCaptioning Dataset (RSICD) and the UCM-captions dataset confirm the superiority\nof the proposed approach in comparison to the previous state-of-the-art where\nwe obtain a gain of sharp increments in both the ROUGE-L and CIDEr measures.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 13:35:02 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Chavhan", "Ruchika", ""], ["Banerjee", "Biplab", ""], ["Zhu", "Xiao Xiang", ""], ["Chaudhuri", "Subhasis", ""]]}, {"id": "2010.02036", "submitter": "Or Patashnik", "authors": "Or Patashnik, Dov Danon, Hao Zhang, Daniel Cohen-Or", "title": "BalaGAN: Image Translation Between Imbalanced Domains via Cross-Modal\n  Transfer", "comments": null, "journal-ref": "Proceedings of the IEEE/CVF Conference on Computer Vision and\n  Pattern Recognition (CVPR) Workshops 2021", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art image-to-image translation methods tend to struggle in an\nimbalanced domain setting, where one image domain lacks richness and diversity.\nWe introduce a new unsupervised translation network, BalaGAN, specifically\ndesigned to tackle the domain imbalance problem. We leverage the latent\nmodalities of the richer domain to turn the image-to-image translation problem,\nbetween two imbalanced domains, into a balanced, multi-class, and conditional\ntranslation problem, more resembling the style transfer setting. Specifically,\nwe analyze the source domain and learn a decomposition of it into a set of\nlatent modes or classes, without any supervision. This leaves us with a\nmultitude of balanced cross-domain translation tasks, between all pairs of\nclasses, including the target domain. During inference, the trained network\ntakes as input a source image, as well as a reference or style image from one\nof the modes as a condition, and produces an image which resembles the source\non the pixel-wise level, but shares the same mode as the reference. We show\nthat employing modalities within the dataset improves the quality of the\ntranslated images, and that BalaGAN outperforms strong baselines of both\nunconditioned and style-transfer-based image-to-image translation methods, in\nterms of image quality and diversity.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 14:16:41 GMT"}, {"version": "v2", "created": "Sat, 5 Jun 2021 14:24:50 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Patashnik", "Or", ""], ["Danon", "Dov", ""], ["Zhang", "Hao", ""], ["Cohen-Or", "Daniel", ""]]}, {"id": "2010.02041", "submitter": "Katar\\'ina T\\'othov\\'a", "authors": "Katar\\'ina T\\'othov\\'a, Sarah Parisot, Matthew Lee, Esther\n  Puyol-Ant\\'on, Andrew King, Marc Pollefeys, Ender Konukoglu", "title": "Probabilistic 3D surface reconstruction from sparse MRI information", "comments": "MICCAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Surface reconstruction from magnetic resonance (MR) imaging data is\nindispensable in medical image analysis and clinical research. A reliable and\neffective reconstruction tool should: be fast in prediction of accurate well\nlocalised and high resolution models, evaluate prediction uncertainty, work\nwith as little input data as possible. Current deep learning state of the art\n(SOTA) 3D reconstruction methods, however, often only produce shapes of limited\nvariability positioned in a canonical position or lack uncertainty evaluation.\nIn this paper, we present a novel probabilistic deep learning approach for\nconcurrent 3D surface reconstruction from sparse 2D MR image data and aleatoric\nuncertainty prediction. Our method is capable of reconstructing large surface\nmeshes from three quasi-orthogonal MR imaging slices from limited training sets\nwhilst modelling the location of each mesh vertex through a Gaussian\ndistribution. Prior shape information is encoded using a built-in linear\nprincipal component analysis (PCA) model. Extensive experiments on cardiac MR\ndata show that our probabilistic approach successfully assesses prediction\nuncertainty while at the same time qualitatively and quantitatively outperforms\nSOTA methods in shape prediction. Compared to SOTA, we are capable of properly\nlocalising and orientating the prediction via the use of a spatially aware\nneural network.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 14:18:52 GMT"}], "update_date": "2020-11-22", "authors_parsed": [["T\u00f3thov\u00e1", "Katar\u00edna", ""], ["Parisot", "Sarah", ""], ["Lee", "Matthew", ""], ["Puyol-Ant\u00f3n", "Esther", ""], ["King", "Andrew", ""], ["Pollefeys", "Marc", ""], ["Konukoglu", "Ender", ""]]}, {"id": "2010.02059", "submitter": "Keemin Sohn", "authors": "Byeonghyeop Yu, Johyun Shin, Gyeongjun Kim, Seungbin Roh, Keemin Sohn", "title": "Non-anchor-based vehicle detection for traffic surveillance using\n  bounding ellipses", "comments": "13 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Cameras for traffic surveillance are usually pole-mounted and produce images\nthat reflect a birds-eye view. Vehicles in such images, in general, assume an\nellipse form. A bounding box for the vehicles usually includes a large empty\nspace when the vehicle orientation is not parallel to the edges of the box. To\ncircumvent this problem, the present study applied bounding ellipses to a\nnon-anchor-based, single-shot detection model (CenterNet). Since this model\ndoes not depend on anchor boxes, non-max suppression (NMS) that requires\ncomputing the intersection over union (IOU) between predicted bounding boxes is\nunnecessary for inference. The SpotNet that extends the CenterNet model by\nadding a segmentation head was also tested with bounding ellipses. Two other\nanchor-based, single-shot detection models (YOLO4 and SSD) were chosen as\nreferences for comparison. The model performance was compared based on a local\ndataset that was doubly annotated with bounding boxes and ellipses. As a\nresult, the performance of the two models with bounding ellipses exceeded that\nof the reference models with bounding boxes. When the backbone of the ellipse\nmodels was pretrained on an open dataset (UA-DETRAC), the performance was\nfurther enhanced. The data augmentation schemes developed for YOLO4 also\nimproved the performance of the proposed models. As a result, the best mAP\nscore of a CenterNet with bounding ellipses exceeds 0.9.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 14:48:18 GMT"}, {"version": "v2", "created": "Fri, 5 Mar 2021 04:05:41 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Yu", "Byeonghyeop", ""], ["Shin", "Johyun", ""], ["Kim", "Gyeongjun", ""], ["Roh", "Seungbin", ""], ["Sohn", "Keemin", ""]]}, {"id": "2010.02086", "submitter": "Kailas Vodrahalli", "authors": "Kailas Vodrahalli, Roxana Daneshjou, Roberto A Novoa, Albert Chiou,\n  Justin M Ko, and James Zou", "title": "TrueImage: A Machine Learning Algorithm to Improve the Quality of\n  Telehealth Photos", "comments": "12 pages, 5 figures, Preprint of an article published in Pacific\n  Symposium on Biocomputing \\c{opyright} 2020 World Scientific Publishing Co.,\n  Singapore, http://psb.stanford.edu/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CY cs.LG eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Telehealth is an increasingly critical component of the health care\necosystem, especially due to the COVID-19 pandemic. Rapid adoption of\ntelehealth has exposed limitations in the existing infrastructure. In this\npaper, we study and highlight photo quality as a major challenge in the\ntelehealth workflow. We focus on teledermatology, where photo quality is\nparticularly important; the framework proposed here can be generalized to other\nhealth domains. For telemedicine, dermatologists request that patients submit\nimages of their lesions for assessment. However, these images are often of\ninsufficient quality to make a clinical diagnosis since patients do not have\nexperience taking clinical photos. A clinician has to manually triage poor\nquality images and request new images to be submitted, leading to wasted time\nfor both the clinician and the patient. We propose an automated image\nassessment machine learning pipeline, TrueImage, to detect poor quality\ndermatology photos and to guide patients in taking better photos. Our\nexperiments indicate that TrueImage can reject 50% of the sub-par quality\nimages, while retaining 80% of good quality images patients send in, despite\nheterogeneity and limitations in the training data. These promising results\nsuggest that our solution is feasible and can improve the quality of\nteledermatology care.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 17:47:57 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Vodrahalli", "Kailas", ""], ["Daneshjou", "Roxana", ""], ["Novoa", "Roberto A", ""], ["Chiou", "Albert", ""], ["Ko", "Justin M", ""], ["Zou", "James", ""]]}, {"id": "2010.02125", "submitter": "Yura Perugachi-Diaz", "authors": "Yura Perugachi-Diaz, Jakub M. Tomczak, Sandjai Bhulai", "title": "Invertible DenseNets", "comments": "Accepted at 3rd Symposium on Advances in Approximate Bayesian\n  Inference (AABI)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Invertible Dense Networks (i-DenseNets), a more parameter\nefficient alternative to Residual Flows. The method relies on an analysis of\nthe Lipschitz continuity of the concatenation in DenseNets, where we enforce\nthe invertibility of the network by satisfying the Lipschitz constraint.\nAdditionally, we extend this method by proposing a learnable concatenation,\nwhich not only improves the model performance but also indicates the importance\nof the concatenated representation. We demonstrate the performance of\ni-DenseNets and Residual Flows on toy, MNIST, and CIFAR10 data. Both\ni-DenseNets outperform Residual Flows evaluated in negative log-likelihood, on\nall considered datasets under an equal parameter budget.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 16:11:39 GMT"}, {"version": "v2", "created": "Fri, 23 Oct 2020 17:53:32 GMT"}, {"version": "v3", "created": "Fri, 8 Jan 2021 18:33:17 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Perugachi-Diaz", "Yura", ""], ["Tomczak", "Jakub M.", ""], ["Bhulai", "Sandjai", ""]]}, {"id": "2010.02153", "submitter": "Georgios Evangelidis", "authors": "Branislav Micusik, Georgios Evangelidis", "title": "Ego-Motion Alignment from Face Detections for Collaborative Augmented\n  Reality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sharing virtual content among multiple smart glasses wearers is an essential\nfeature of a seamless Collaborative Augmented Reality experience. To enable the\nsharing, local coordinate systems of the underlying 6D ego-pose trackers,\nrunning independently on each set of glasses, have to be spatially and\ntemporally aligned with respect to each other. In this paper, we propose a\nnovel lightweight solution for this problem, which is referred as ego-motion\nalignment. We show that detecting each other's face or glasses together with\ntracker ego-poses sufficiently conditions the problem to spatially relate local\ncoordinate systems. Importantly, the detected glasses can serve as reliable\nanchors to bring sufficient accuracy for the targeted practical use. The\nproposed idea allows us to abandon the traditional visual localization step\nwith fiducial markers or scene points as anchors. A novel closed form minimal\nsolver which solves a Quadratic Eigenvalue Problem is derived and its\nrefinement with Gaussian Belief Propagation is introduced. Experiments validate\nthe presented approach and show its high practical potential.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 16:57:48 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Micusik", "Branislav", ""], ["Evangelidis", "Georgios", ""]]}, {"id": "2010.02154", "submitter": "Shahabedin Nabavi", "authors": "Shahabedin Nabavi (1), Azar Ejmalian (2), Mohsen Ebrahimi Moghaddam\n  (1), Ahmad Ali Abin (1), Alejandro F. Frangi (3), Mohammad Mohammadi (4 and\n  5), Hamidreza Saligheh Rad (6) ((1) Faculty of Computer Science and\n  Engineering, Shahid Beheshti University, Tehran, Iran. (2) Anesthesiology\n  Research Center, Shahid Beheshti University of Medical Sciences, Tehran,\n  Iran. (3) Centre for Computational Imaging and Simulation Technologies in\n  Biomedicine (CISTIB), School of Computing, University of Leeds, Leeds, UK.\n  (4) Department of Medical Physics, Royal Adelaide Hospital, Adelaide, South\n  Australia, Australia. (5) School of Physical Sciences, The University of\n  Adelaide, Adelaide, South Australia, Australia. (6) Quantitative MR Imaging\n  and Spectroscopy Group (QMISG), Tehran University of Medical Sciences,\n  Tehran, Iran.)", "title": "Medical Imaging and Computational Image Analysis in COVID-19 Diagnosis:\n  A Review", "comments": "29 pages, 4 tables", "journal-ref": "Computers in Biology and Medicine, 2021, 104605,", "doi": "10.1016/j.compbiomed.2021.104605", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coronavirus disease (COVID-19) is an infectious disease caused by a newly\ndiscovered coronavirus. The disease presents with symptoms such as shortness of\nbreath, fever, dry cough, and chronic fatigue, amongst others. Sometimes the\nsymptoms of the disease increase so much they lead to the death of the\npatients. The disease may be asymptomatic in some patients in the early stages,\nwhich can lead to increased transmission of the disease to others. Many studies\nhave tried to use medical imaging for early diagnosis of COVID-19. This study\nattempts to review papers on automatic methods for medical image analysis and\ndiagnosis of COVID-19. For this purpose, PubMed, Google Scholar, arXiv and\nmedRxiv were searched to find related studies by the end of April 2020, and the\nessential points of the collected studies were summarised. The contribution of\nthis study is four-fold: 1) to use as a tutorial of the field for both\nclinicians and technologists, 2) to comprehensively review the characteristics\nof COVID-19 as presented in medical images, 3) to examine automated artificial\nintelligence-based approaches for COVID-19 diagnosis based on the accuracy and\nthe method used, 4) to express the research limitations in this field and the\nmethods used to overcome them. COVID-19 reveals signs in medical images can be\nused for early diagnosis of the disease even in asymptomatic patients. Using\nautomated machine learning-based methods can diagnose the disease with high\naccuracy from medical images and reduce time, cost and error of diagnostic\nprocedure. It is recommended to collect bulk imaging data from patients in the\nshortest possible time to improve the performance of COVID-19 automated\ndiagnostic methods.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 06:38:06 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Nabavi", "Shahabedin", "", "4 and\n  5"], ["Ejmalian", "Azar", "", "4 and\n  5"], ["Moghaddam", "Mohsen Ebrahimi", "", "4 and\n  5"], ["Abin", "Ahmad Ali", "", "4 and\n  5"], ["Frangi", "Alejandro F.", "", "4 and\n  5"], ["Mohammadi", "Mohammad", "", "4 and\n  5"], ["Rad", "Hamidreza Saligheh", ""]]}, {"id": "2010.02178", "submitter": "Bilal Alsallakh", "authors": "Bilal Alsallakh and Narine Kokhlikyan and Vivek Miglani and Jun Yuan\n  and Orion Reblitz-Richardson", "title": "Mind the Pad -- CNNs can Develop Blind Spots", "comments": "Appendix E available at\n  https://drive.google.com/file/d/1bIvRQJIBwJbKTfpg0hNaFX2ThuuDO8PU/view?usp=sharing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show how feature maps in convolutional networks are susceptible to spatial\nbias. Due to a combination of architectural choices, the activation at certain\nlocations is systematically elevated or weakened. The major source of this bias\nis the padding mechanism. Depending on several aspects of convolution\narithmetic, this mechanism can apply the padding unevenly, leading to\nasymmetries in the learned weights. We demonstrate how such bias can be\ndetrimental to certain tasks such as small object detection: the activation is\nsuppressed if the stimulus lies in the impacted area, leading to blind spots\nand misdetection. We propose solutions to mitigate spatial bias and demonstrate\nhow they can improve model accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 17:24:48 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Alsallakh", "Bilal", ""], ["Kokhlikyan", "Narine", ""], ["Miglani", "Vivek", ""], ["Yuan", "Jun", ""], ["Reblitz-Richardson", "Orion", ""]]}, {"id": "2010.02217", "submitter": "Chen Wei", "authors": "Chen Wei, Huiyu Wang, Wei Shen, Alan Yuille", "title": "CO2: Consistent Contrast for Unsupervised Visual Representation Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contrastive learning has been adopted as a core method for unsupervised\nvisual representation learning. Without human annotation, the common practice\nis to perform an instance discrimination task: Given a query image crop, this\ntask labels crops from the same image as positives, and crops from other\nrandomly sampled images as negatives. An important limitation of this label\nassignment strategy is that it can not reflect the heterogeneous similarity\nbetween the query crop and each crop from other images, taking them as equally\nnegative, while some of them may even belong to the same semantic class as the\nquery. To address this issue, inspired by consistency regularization in\nsemi-supervised learning on unlabeled data, we propose Consistent Contrast\n(CO2), which introduces a consistency regularization term into the current\ncontrastive learning framework. Regarding the similarity of the query crop to\neach crop from other images as \"unlabeled\", the consistency term takes the\ncorresponding similarity of a positive crop as a pseudo label, and encourages\nconsistency between these two similarities. Empirically, CO2 improves Momentum\nContrast (MoCo) by 2.9% top-1 accuracy on ImageNet linear protocol, 3.8% and\n1.1% top-5 accuracy on 1% and 10% labeled semi-supervised settings. It also\ntransfers to image classification, object detection, and semantic segmentation\non PASCAL VOC. This shows that CO2 learns better visual representations for\nthese downstream tasks.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 18:00:01 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Wei", "Chen", ""], ["Wang", "Huiyu", ""], ["Shen", "Wei", ""], ["Yuille", "Alan", ""]]}, {"id": "2010.02270", "submitter": "Hyeongmin Lee", "authors": "Hyeongmin Lee, Taeoh Kim, Hanbin Son, Sangwook Baek, Minsu Cheon,\n  Sangyoun Lee", "title": "Smoother Network Tuning and Interpolation for Continuous-level Image\n  Processing", "comments": "arXiv admin note: substantial text overlap with arXiv:2003.05145", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Convolutional Neural Network (CNN) based image processing, most studies\npropose networks that are optimized to single-level (or single-objective);\nthus, they underperform on other levels and must be retrained for delivery of\noptimal performance. Using multiple models to cover multiple levels involves\nvery high computational costs. To solve these problems, recent approaches train\nnetworks on two different levels and propose their own interpolation methods to\nenable arbitrary intermediate levels. However, many of them fail to generalize\nor have certain side effects in practical usage. In this paper, we define these\nframeworks as network tuning and interpolation and propose a novel module for\ncontinuous-level learning, called Filter Transition Network (FTN). This module\nis a structurally smoother module than existing ones. Therefore, the frameworks\nwith FTN generalize well across various tasks and networks and cause fewer\nundesirable side effects. For stable learning of FTN, we additionally propose a\nmethod to initialize non-linear neural network layers with identity mappings.\nExtensive results for various image processing tasks indicate that the\nperformance of FTN is comparable in multiple continuous levels, and is\nsignificantly smoother and lighter than that of other frameworks.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 18:29:52 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Lee", "Hyeongmin", ""], ["Kim", "Taeoh", ""], ["Son", "Hanbin", ""], ["Baek", "Sangwook", ""], ["Cheon", "Minsu", ""], ["Lee", "Sangyoun", ""]]}, {"id": "2010.02281", "submitter": "Aysen Degerli", "authors": "Aysen Degerli, Morteza Zabihi, Serkan Kiranyaz, Tahir Hamid, Rashid\n  Mazhar, Ridha Hamila, and Moncef Gabbouj", "title": "Early Detection of Myocardial Infarction in Low-Quality Echocardiography", "comments": null, "journal-ref": "IEEE Access (2021)", "doi": "10.1109/ACCESS.2021.3059595", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Myocardial infarction (MI), or commonly known as heart attack, is a\nlife-threatening health problem worldwide from which 32.4 million people suffer\neach year. Early diagnosis and treatment of MI are crucial to prevent further\nheart tissue damages or death. The earliest and most reliable sign of ischemia\nis regional wall motion abnormality (RWMA) of the affected part of the\nventricular muscle. Echocardiography can easily, inexpensively, and\nnon-invasively exhibit the RWMA. In this article, we introduce a three-phase\napproach for early MI detection in low-quality echocardiography: 1)\nsegmentation of the entire left ventricle (LV) wall using a state-of-the-art\ndeep learning model, 2) analysis of the segmented LV wall by feature\nengineering, and 3) early MI detection. The main contributions of this study\nare highly accurate segmentation of the LV wall from low-quality\nechocardiography, pseudo labeling approach for ground-truth formation of the\nunannotated LV wall, and the first public echocardiographic dataset (HMC-QU)*\nfor MI detection. Furthermore, the outputs of the proposed approach can\nsignificantly help cardiologists for a better assessment of the LV wall\ncharacteristics. The proposed approach has achieved 95.72% sensitivity and\n99.58% specificity for the LV wall segmentation, and 85.97% sensitivity, 74.03%\nspecificity, and 86.85% precision for MI detection on the HMC-QU dataset. *The\nbenchmark HMC-QU dataset is publicly shared at the repository\nhttps://www.kaggle.com/aysendegerli/hmcqu-dataset\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 18:47:04 GMT"}, {"version": "v2", "created": "Mon, 15 Mar 2021 10:01:30 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Degerli", "Aysen", ""], ["Zabihi", "Morteza", ""], ["Kiranyaz", "Serkan", ""], ["Hamid", "Tahir", ""], ["Mazhar", "Rashid", ""], ["Hamila", "Ridha", ""], ["Gabbouj", "Moncef", ""]]}, {"id": "2010.02315", "submitter": "Andr\\'es Felipe Romero Vergara", "authors": "Andr\\'es Romero, Luc Van Gool, Radu Timofte", "title": "SMILE: Semantically-guided Multi-attribute Image and Layout Editing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attribute image manipulation has been a very active topic since the\nintroduction of Generative Adversarial Networks (GANs). Exploring the\ndisentangled attribute space within a transformation is a very challenging task\ndue to the multiple and mutually-inclusive nature of the facial images, where\ndifferent labels (eyeglasses, hats, hair, identity, etc.) can co-exist at the\nsame time. Several works address this issue either by exploiting the modality\nof each domain/attribute using a conditional random vector noise, or extracting\nthe modality from an exemplary image. However, existing methods cannot handle\nboth random and reference transformations for multiple attributes, which limits\nthe generality of the solutions. In this paper, we successfully exploit a\nmultimodal representation that handles all attributes, be it guided by random\nnoise or exemplar images, while only using the underlying domain information of\nthe target domain. We present extensive qualitative and quantitative results\nfor facial datasets and several different attributes that show the superiority\nof our method. Additionally, our method is capable of adding, removing or\nchanging either fine-grained or coarse attributes by using an image as a\nreference or by exploring the style distribution space, and it can be easily\nextended to head-swapping and face-reenactment applications without being\ntrained on videos.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 20:15:21 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Romero", "Andr\u00e9s", ""], ["Van Gool", "Luc", ""], ["Timofte", "Radu", ""]]}, {"id": "2010.02319", "submitter": "Jaya Sreevalsan-Nair", "authors": "Jaya Sreevalsan-Nair and Komal Dadhich and Siri Chandana Daggubati", "title": "Tensor Fields for Data Extraction from Chart Images: Bar Charts and\n  Scatter Plots", "comments": "17 pages, 7 figures, 1 table, peer-reviewed and accepted for\n  publication in \"Topological Methods in Visualization: Theory, Software and\n  Applications,\" Ingrid Hotz, Talha Bin Masood, Filip Sadlo, and Julien Tierny\n  (Eds.). Springer-Verlag", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.NA math.NA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Charts are an essential part of both graphicacy (graphical literacy), and\nstatistical literacy. As chart understanding has become increasingly relevant\nin data science, automating chart analysis by processing raster images of the\ncharts has become a significant problem. Automated chart reading involves data\nextraction and contextual understanding of the data from chart images. In this\npaper, we perform the first step of determining the computational model of\nchart images for data extraction for selected chart types, namely, bar charts,\nand scatter plots. We demonstrate the use of positive semidefinite second-order\ntensor fields as an effective model. We identify an appropriate tensor field as\nthe model and propose a methodology for the use of its degenerate point\nextraction for data extraction from chart images. Our results show that tensor\nvoting is effective for data extraction from bar charts and scatter plots, and\nhistograms, as a special case of bar charts.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 20:19:40 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Sreevalsan-Nair", "Jaya", ""], ["Dadhich", "Komal", ""], ["Daggubati", "Siri Chandana", ""]]}, {"id": "2010.02323", "submitter": "David McNeely-White", "authors": "David McNeely-White, Benjamin Sattelberg, Nathaniel Blanchard, Ross\n  Beveridge", "title": "Exploring the Interchangeability of CNN Embedding Spaces", "comments": "Preprint, 11 pages, 2 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  CNN feature spaces can be linearly mapped and consequently are often\ninterchangeable. This equivalence holds across variations in architectures,\ntraining datasets, and network tasks. Specifically, we mapped between 10\nimage-classification CNNs and between 4 facial-recognition CNNs. When image\nembeddings generated by one CNN are transformed into embeddings corresponding\nto the feature space of a second CNN trained on the same task, their respective\nimage classification or face verification performance is largely preserved. For\nCNNs trained to the same classes and sharing a common backend-logit (soft-max)\narchitecture, a linear-mapping may always be calculated directly from the\nbackend layer weights. However, the case of a closed-set analysis with perfect\nknowledge of classifiers is limiting. Therefore, empirical methods of\nestimating mappings are presented for both the closed-set image classification\ntask and the open-set task of face recognition. The results presented expose\nthe essentially interchangeable nature of CNNs embeddings for two important and\ncommon recognition tasks. The implications are far-reaching, suggesting an\nunderlying commonality between representations learned by networks designed and\ntrained for a common task. One practical implication is that face embeddings\nfrom some commonly used CNNs can be compared using these mappings.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 20:32:40 GMT"}, {"version": "v2", "created": "Mon, 2 Nov 2020 19:57:46 GMT"}, {"version": "v3", "created": "Mon, 7 Dec 2020 22:32:54 GMT"}, {"version": "v4", "created": "Fri, 12 Feb 2021 01:59:35 GMT"}], "update_date": "2021-02-15", "authors_parsed": [["McNeely-White", "David", ""], ["Sattelberg", "Benjamin", ""], ["Blanchard", "Nathaniel", ""], ["Beveridge", "Ross", ""]]}, {"id": "2010.02330", "submitter": "Jing Shi", "authors": "Jing Shi, Ning Xu, Trung Bui, Franck Dernoncourt, Zheng Wen, Chenliang\n  Xu", "title": "A Benchmark and Baseline for Language-Driven Image Editing", "comments": "Accepted by ACCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Language-driven image editing can significantly save the laborious image\nediting work and be friendly to the photography novice. However, most similar\nwork can only deal with a specific image domain or can only do global\nretouching. To solve this new task, we first present a new language-driven\nimage editing dataset that supports both local and global editing with editing\noperation and mask annotations. Besides, we also propose a baseline method that\nfully utilizes the annotation to solve this problem. Our new method treats each\nediting operation as a sub-module and can automatically predict operation\nparameters. Not only performing well on challenging user data, but such an\napproach is also highly interpretable. We believe our work, including both the\nbenchmark and the baseline, will advance the image editing area towards a more\ngeneral and free-form level.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 20:51:16 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Shi", "Jing", ""], ["Xu", "Ning", ""], ["Bui", "Trung", ""], ["Dernoncourt", "Franck", ""], ["Wen", "Zheng", ""], ["Xu", "Chenliang", ""]]}, {"id": "2010.02343", "submitter": "Behzad Ghazanfari", "authors": "Behzad Ghazanfari, Fatemeh Afghah", "title": "Multi-level Feature Learning on Embedding Layer of Convolutional\n  Autoencoders and Deep Inverse Feature Learning for Image Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces Multi-Level feature learning alongside the Embedding\nlayer of Convolutional Autoencoder (CAE-MLE) as a novel approach in deep\nclustering. We use agglomerative clustering as the multi-level feature learning\nthat provides a hierarchical structure on the latent feature space. It is shown\nthat applying multi-level feature learning considerably improves the basic deep\nconvolutional embedding clustering (DCEC). CAE-MLE considers the clustering\nloss of agglomerative clustering simultaneously alongside the learning latent\nfeature of CAE. In the following of the previous works in inverse feature\nlearning, we show that the representation of learning of error as a general\nstrategy can be applied on different deep clustering approaches and it leads to\npromising results. We develop deep inverse feature learning (deep IFL) on\nCAE-MLE as a novel approach that leads to the state-of-the-art results among\nthe same category methods. The experimental results show that the CAE-MLE\nimproves the results of the basic method, DCEC, around 7% -14% on two\nwell-known datasets of MNIST and USPS. Also, it is shown that the proposed deep\nIFL improves the primary results about 9%-17%. Therefore, both proposed\napproaches of CAE-MLE and deep IFL based on CAE-MLE can lead to notable\nperformance improvement in comparison to the majority of existing techniques.\nThe proposed approaches while are based on a basic convolutional autoencoder\nlead to outstanding results even in comparison to variational autoencoders or\ngenerative adversarial networks.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 21:24:10 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Ghazanfari", "Behzad", ""], ["Afghah", "Fatemeh", ""]]}, {"id": "2010.02345", "submitter": "Connor Daly", "authors": "Connor Daly, Yuzuko Nakamura, Tobias Ritschel", "title": "Deep Generative Modelling of Human Reach-and-Place Action", "comments": "Accompanying video found here: https://youtu.be/o1SICcMZiiU 9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The motion of picking up and placing an object in 3D space is full of subtle\ndetail. Typically these motions are formed from the same constraints,\noptimizing for swiftness, energy efficiency, as well as physiological limits.\nYet, even for identical goals, the motion realized is always subject to natural\nvariation. To capture these aspects computationally, we suggest a deep\ngenerative model for human reach-and-place action, conditioned on a start and\nend position.We have captured a dataset of 600 such human 3D actions, to sample\nthe 2x3-D space of 3D source and targets. While temporal variation is often\nmodeled with complex learning machinery like recurrent neural networks or\nnetworks with memory or attention, we here demonstrate a much simpler approach\nthat is convolutional in time and makes use of(periodic) temporal encoding.\nProvided a latent code and conditioned on start and end position, the model\ngenerates a complete 3D character motion in linear time as a sequence of\nconvolutions. Our evaluation includes several ablations, analysis of generative\ndiversity and applications.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 21:36:20 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Daly", "Connor", ""], ["Nakamura", "Yuzuko", ""], ["Ritschel", "Tobias", ""]]}, {"id": "2010.02350", "submitter": "Neha Mukund Kalibhat", "authors": "Neha Mukund Kalibhat, Yogesh Balaji, Soheil Feizi", "title": "Winning Lottery Tickets in Deep Generative Models", "comments": "Published at AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The lottery ticket hypothesis suggests that sparse, sub-networks of a given\nneural network, if initialized properly, can be trained to reach comparable or\neven better performance to that of the original network. Prior works in lottery\ntickets have primarily focused on the supervised learning setup, with several\npapers proposing effective ways of finding \"winning tickets\" in classification\nproblems. In this paper, we confirm the existence of winning tickets in deep\ngenerative models such as GANs and VAEs. We show that the popular iterative\nmagnitude pruning approach (with late rewinding) can be used with generative\nlosses to find the winning tickets. This approach effectively yields tickets\nwith sparsity up to 99% for AutoEncoders, 93% for VAEs and 89% for GANs on\nCIFAR and Celeb-A datasets. We also demonstrate the transferability of winning\ntickets across different generative models (GANs and VAEs) sharing the same\narchitecture, suggesting that winning tickets have inductive biases that could\nhelp train a wide range of deep generative models. Furthermore, we show the\npractical benefits of lottery tickets in generative models by detecting tickets\nat very early stages in training called \"early-bird tickets\". Through\nearly-bird tickets, we can achieve up to 88% reduction in floating-point\noperations (FLOPs) and 54% reduction in training time, making it possible to\ntrain large-scale generative models over tight resource constraints. These\nresults out-perform existing early pruning methods like SNIP (Lee, Ajanthan,\nand Torr 2019) and GraSP (Wang, Zhang, and Grosse 2020). Our findings shed\nlight towards existence of proper network initializations that could improve\nconvergence and stability of generative models.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 21:45:39 GMT"}, {"version": "v2", "created": "Fri, 29 Jan 2021 18:44:21 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Kalibhat", "Neha Mukund", ""], ["Balaji", "Yogesh", ""], ["Feizi", "Soheil", ""]]}, {"id": "2010.02358", "submitter": "Aymen Shabou", "authors": "Mohamed Kerroumi, Othmane Sayem and Aymen Shabou", "title": "VisualWordGrid: Information Extraction From Scanned Documents Using A\n  Multimodal Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel approach for scanned document representation to perform\nfield extraction. It allows the simultaneous encoding of the textual, visual\nand layout information in a 3-axis tensor used as an input to a segmentation\nmodel. We improve the recent Chargrid and Wordgrid \\cite{chargrid} models in\nseveral ways, first by taking into account the visual modality, then by\nboosting its robustness in regards to small datasets while keeping the\ninference time low. Our approach is tested on public and private document-image\ndatasets, showing higher performances compared to the recent state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 21:58:19 GMT"}, {"version": "v2", "created": "Wed, 7 Oct 2020 22:02:06 GMT"}, {"version": "v3", "created": "Sun, 11 Oct 2020 21:29:46 GMT"}, {"version": "v4", "created": "Tue, 13 Oct 2020 06:41:13 GMT"}, {"version": "v5", "created": "Sun, 4 Jul 2021 21:25:52 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Kerroumi", "Mohamed", ""], ["Sayem", "Othmane", ""], ["Shabou", "Aymen", ""]]}, {"id": "2010.02367", "submitter": "Madhumitha Sakthi", "authors": "Madhumitha Sakthi, Ahmed Tewfik", "title": "Automotive Radar Data Acquisition using Object Detection", "comments": "Submitted to EUSIPCO 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The growing urban complexity demands an efficient algorithm to acquire and\nprocess various sensor information from autonomous vehicles. In this paper, we\nintroduce an algorithm to utilize object detection results from the image to\nadaptively sample and acquire radar data using Compressed Sensing (CS). This\nnovel algorithm is motivated by the hypothesis that with a limited sampling\nbudget, allocating more sampling budget to areas with the object as opposed to\na uniform sampling ultimately improves relevant object detection performance.\nWe improve detection performance by dynamically allocating a lower sampling\nrate to objects such as buses than pedestrians leading to better reconstruction\nthan baseline across areas with objects of interest. We automate the sampling\nrate allocation using linear programming and show significant time savings\nwhile reducing the radar block size by a factor of 2. We also analyze a Binary\nPermuted Diagonal measurement matrix for radar acquisition which is\nhardware-efficient and show its performance is similar to Gaussian and Binary\nPermuted Block Diagonal matrix. Our experiments on the Oxford radar dataset\nshow an effective reconstruction of objects of interest with 10% sampling rate.\nFinally, we develop a transformer-based 2D object detection network using the\nNuScenes radar and image data.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 22:22:08 GMT"}, {"version": "v2", "created": "Mon, 1 Mar 2021 18:51:22 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Sakthi", "Madhumitha", ""], ["Tewfik", "Ahmed", ""]]}, {"id": "2010.02392", "submitter": "Karl Willis", "authors": "Karl D.D. Willis, Yewen Pu, Jieliang Luo, Hang Chu, Tao Du, Joseph G.\n  Lambourne, Armando Solar-Lezama, Wojciech Matusik", "title": "Fusion 360 Gallery: A Dataset and Environment for Programmatic CAD\n  Construction from Human Design Sequences", "comments": "Accepted to SIGGRAPH 2021; data/code available at\n  https://github.com/AutodeskAILab/Fusion360GalleryDataset", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parametric computer-aided design (CAD) is a standard paradigm used to design\nmanufactured objects, where a 3D shape is represented as a program supported by\nthe CAD software. Despite the pervasiveness of parametric CAD and a growing\ninterest from the research community, currently there does not exist a dataset\nof realistic CAD models in a concise programmatic form. In this paper we\npresent the Fusion 360 Gallery, consisting of a simple language with just the\nsketch and extrude modeling operations, and a dataset of 8,625 human design\nsequences expressed in this language. We also present an interactive\nenvironment called the Fusion 360 Gym, which exposes the sequential\nconstruction of a CAD program as a Markov decision process, making it amendable\nto machine learning approaches. As a use case for our dataset and environment,\nwe define the CAD reconstruction task of recovering a CAD program from a target\ngeometry. We report results of applying state-of-the-art methods of program\nsynthesis with neurally guided search on this task.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 23:18:21 GMT"}, {"version": "v2", "created": "Mon, 17 May 2021 03:58:04 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Willis", "Karl D. D.", ""], ["Pu", "Yewen", ""], ["Luo", "Jieliang", ""], ["Chu", "Hang", ""], ["Du", "Tao", ""], ["Lambourne", "Joseph G.", ""], ["Solar-Lezama", "Armando", ""], ["Matusik", "Wojciech", ""]]}, {"id": "2010.02406", "submitter": "Chongke Wu", "authors": "Chongke Wu, Sicong Shao, Cihan Tunc, Salim Hariri", "title": "Video Anomaly Detection Using Pre-Trained Deep Convolutional Neural Nets\n  and Context Mining", "comments": "Accepted by AICCSA 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anomaly detection is critically important for intelligent surveillance\nsystems to detect in a timely manner any malicious activities. Many video\nanomaly detection approaches using deep learning methods focus on a single\ncamera video stream with a fixed scenario. These deep learning methods use\nlarge-scale training data with large complexity. As a solution, in this paper,\nwe show how to use pre-trained convolutional neural net models to perform\nfeature extraction and context mining, and then use denoising autoencoder with\nrelatively low model complexity to provide efficient and accurate surveillance\nanomaly detection, which can be useful for the resource-constrained devices\nsuch as edge devices of the Internet of Things (IoT). Our anomaly detection\nmodel makes decisions based on the high-level features derived from the\nselected embedded computer vision models such as object classification and\nobject detection. Additionally, we derive contextual properties from the\nhigh-level features to further improve the performance of our video anomaly\ndetection method. We use two UCSD datasets to demonstrate that our approach\nwith relatively low model complexity can achieve comparable performance\ncompared to the state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 00:26:14 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Wu", "Chongke", ""], ["Shao", "Sicong", ""], ["Tunc", "Cihan", ""], ["Hariri", "Salim", ""]]}, {"id": "2010.02414", "submitter": "Jialiang Shen", "authors": "Jialiang Shen, Yucheng Wang, Jian Zhang", "title": "ASDN: A Deep Convolutional Network for Arbitrary Scale Image\n  Super-Resolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks have significantly improved the peak\nsignal-to-noise ratio of SuperResolution (SR). However, image viewer\napplications commonly allow users to zoom the images to arbitrary magnification\nscales, thus far imposing a large number of required training scales at a\ntremendous computational cost. To obtain a more computationally efficient model\nfor arbitrary scale SR, this paper employs a Laplacian pyramid method to\nreconstruct any-scale high-resolution (HR) images using the high-frequency\nimage details in a Laplacian Frequency Representation. For SR of small-scales\n(between 1 and 2), images are constructed by interpolation from a sparse set of\nprecalculated Laplacian pyramid levels. SR of larger scales is computed by\nrecursion from small scales, which significantly reduces the computational\ncost. For a full comparison, fixed- and any-scale experiments are conducted\nusing various benchmarks. At fixed scales, ASDN outperforms predefined\nupsampling methods (e.g., SRCNN, VDSR, DRRN) by about 1 dB in PSNR. At\nany-scale, ASDN generally exceeds Meta-SR on many scales.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 01:18:46 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Shen", "Jialiang", ""], ["Wang", "Yucheng", ""], ["Zhang", "Jian", ""]]}, {"id": "2010.02418", "submitter": "Ang Li", "authors": "Yogesh Balaji, Mehrdad Farajtabar, Dong Yin, Alex Mott, Ang Li", "title": "The Effectiveness of Memory Replay in Large Scale Continual Learning", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study continual learning in the large scale setting where tasks in the\ninput sequence are not limited to classification, and the outputs can be of\nhigh dimension. Among multiple state-of-the-art methods, we found vanilla\nexperience replay (ER) still very competitive in terms of both performance and\nscalability, despite its simplicity. However, a degraded performance is\nobserved for ER with small memory. A further visualization of the feature space\nreveals that the intermediate representation undergoes a distributional drift.\nWhile existing methods usually replay only the input-output pairs, we\nhypothesize that their regularization effect is inadequate for complex deep\nmodels and diverse tasks with small replay buffer size. Following this\nobservation, we propose to replay the activation of the intermediate layers in\naddition to the input-output pairs. Considering that saving raw activation maps\ncan dramatically increase memory and compute cost, we propose the Compressed\nActivation Replay technique, where compressed representations of layer\nactivation are saved to the replay buffer. We show that this approach can\nachieve superior regularization effect while adding negligible memory overhead\nto replay method. Experiments on both the large-scale Taskonomy benchmark with\na diverse set of tasks and standard common datasets (Split-CIFAR and\nSplit-miniImageNet) demonstrate the effectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 01:23:12 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Balaji", "Yogesh", ""], ["Farajtabar", "Mehrdad", ""], ["Yin", "Dong", ""], ["Mott", "Alex", ""], ["Li", "Ang", ""]]}, {"id": "2010.02430", "submitter": "Zitian Chen", "authors": "Zitian Chen, Subhransu Maji, Erik Learned-Miller", "title": "Shot in the Dark: Few-Shot Learning with No Base-Class Labels", "comments": "Learning from Limited or Imperfect Data (L2ID) Workshop @ CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Few-shot learning aims to build classifiers for new classes from a small\nnumber of labeled examples and is commonly facilitated by access to examples\nfrom a distinct set of 'base classes'. The difference in data distribution\nbetween the test set (novel classes) and the base classes used to learn an\ninductive bias often results in poor generalization on the novel classes. To\nalleviate problems caused by the distribution shift, previous research has\nexplored the use of unlabeled examples from the novel classes, in addition to\nlabeled examples of the base classes, which is known as the transductive\nsetting. In this work, we show that, surprisingly, off-the-shelf\nself-supervised learning outperforms transductive few-shot methods by 3.9% for\n5-shot accuracy on miniImageNet without using any base class labels. This\nmotivates us to examine more carefully the role of features learned through\nself-supervision in few-shot learning. Comprehensive experiments are conducted\nto compare the transferability, robustness, efficiency, and the complementarity\nof supervised and self-supervised features.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 02:05:27 GMT"}, {"version": "v2", "created": "Fri, 23 Apr 2021 01:16:11 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Chen", "Zitian", ""], ["Maji", "Subhransu", ""], ["Learned-Miller", "Erik", ""]]}, {"id": "2010.02451", "submitter": "Yansheng Li", "authors": "Yansheng Li, Song Ouyang, and Yongjun Zhang", "title": "Collaboratively boosting data-driven deep learning and knowledge-guided\n  ontological reasoning for semantic segmentation of remote sensing imagery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As one kind of architecture from the deep learning family, deep semantic\nsegmentation network (DSSN) achieves a certain degree of success on the\nsemantic segmentation task and obviously outperforms the traditional methods\nbased on hand-crafted features. As a classic data-driven technique, DSSN can be\ntrained by an end-to-end mechanism and competent for employing the low-level\nand mid-level cues (i.e., the discriminative image structure) to understand\nimages, but lacks the high-level inference ability. By contrast, human beings\nhave an excellent inference capacity and can be able to reliably interpret the\nRS imagery only when human beings master the basic RS domain knowledge. In\nliterature, ontological modeling and reasoning is an ideal way to imitate and\nemploy the domain knowledge of human beings, but is still rarely explored and\nadopted in the RS domain. To remedy the aforementioned critical limitation of\nDSSN, this paper proposes a collaboratively boosting framework (CBF) to combine\ndata-driven deep learning module and knowledge-guided ontological reasoning\nmodule in an iterative way.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 03:32:17 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Li", "Yansheng", ""], ["Ouyang", "Song", ""], ["Zhang", "Yongjun", ""]]}, {"id": "2010.02456", "submitter": "Andrew Lohn", "authors": "Andrew J. Lohn", "title": "Downscaling Attack and Defense: Turning What You See Back Into What You\n  Get", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The resizing of images, which is typically a required part of preprocessing\nfor computer vision systems, is vulnerable to attack. Images can be created\nsuch that the image is completely different at machine-vision scales than at\nother scales and the default settings for some common computer vision and\nmachine learning systems are vulnerable. We show that defenses exist and are\ntrivial to administer provided that defenders are aware of the threat. These\nattacks and defenses help to establish the role of input sanitization in\nmachine learning.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 03:41:05 GMT"}, {"version": "v2", "created": "Wed, 7 Oct 2020 18:24:29 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Lohn", "Andrew J.", ""]]}, {"id": "2010.02467", "submitter": "Jianmo Ni", "authors": "Jianmo Ni, Chun-Nan Hsu, Amilcare Gentili, Julian McAuley", "title": "Learning Visual-Semantic Embeddings for Reporting Abnormal Findings on\n  Chest X-rays", "comments": "7 pages, 2 figures, to be published in Findings of EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic medical image report generation has drawn growing attention due to\nits potential to alleviate radiologists' workload. Existing work on report\ngeneration often trains encoder-decoder networks to generate complete reports.\nHowever, such models are affected by data bias (e.g.~label imbalance) and face\ncommon issues inherent in text generation models (e.g.~repetition). In this\nwork, we focus on reporting abnormal findings on radiology images; instead of\ntraining on complete radiology reports, we propose a method to identify\nabnormal findings from the reports in addition to grouping them with\nunsupervised clustering and minimal rules. We formulate the task as cross-modal\nretrieval and propose Conditional Visual-Semantic Embeddings to align images\nand fine-grained abnormal findings in a joint embedding space. We demonstrate\nthat our method is able to retrieve abnormal findings and outperforms existing\ngeneration models on both clinical correctness and text generation metrics.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 04:18:18 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Ni", "Jianmo", ""], ["Hsu", "Chun-Nan", ""], ["Gentili", "Amilcare", ""], ["McAuley", "Julian", ""]]}, {"id": "2010.02468", "submitter": "Kohei Suenaga", "authors": "Yuhki Hatakeyama (SenseTime Japan), Hiroki Sakuma (SenseTime Japan),\n  Yoshinori Konishi (SenseTime Japan), and Kohei Suenaga (Kyoto University)", "title": "Visualizing Color-wise Saliency of Black-Box Image Classification Models", "comments": "To appear in ACCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image classification based on machine learning is being commonly used.\nHowever, a classification result given by an advanced method, including deep\nlearning, is often hard to interpret. This problem of interpretability is one\nof the major obstacles in deploying a trained model in safety-critical systems.\nSeveral techniques have been proposed to address this problem; one of which is\nRISE, which explains a classification result by a heatmap, called a saliency\nmap, which explains the significance of each pixel. We propose MC-RISE\n(Multi-Color RISE), which is an enhancement of RISE to take color information\ninto account in an explanation. Our method not only shows the saliency of each\npixel in a given image as the original RISE does, but the significance of color\ncomponents of each pixel; a saliency map with color information is useful\nespecially in the domain where the color information matters (e.g.,\ntraffic-sign recognition). We implemented MC-RISE and evaluate them using two\ndatasets (GTSRB and ImageNet) to demonstrate the effectiveness of our methods\nin comparison with existing techniques for interpreting image classification\nresults.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 04:27:18 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Hatakeyama", "Yuhki", "", "SenseTime Japan"], ["Sakuma", "Hiroki", "", "SenseTime Japan"], ["Konishi", "Yoshinori", "", "SenseTime Japan"], ["Suenaga", "Kohei", "", "Kyoto University"]]}, {"id": "2010.02475", "submitter": "Yuchen Ma", "authors": "Zeming Li, Yuchen Ma, Yukang Chen, Xiangyu Zhang, Jian Sun", "title": "Joint COCO and Mapillary Workshop at ICCV 2019: COCO Instance\n  Segmentation Challenge Track", "comments": "1st Place Technical Report in ICCV2019/ ECCV2020: MegDetV2", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this report, we present our object detection/instance segmentation system,\nMegDetV2, which works in a two-pass fashion, first to detect instances then to\nobtain segmentation. Our baseline detector is mainly built on a new designed\nRPN, called RPN++. On the COCO-2019 detection/instance-segmentation test-dev\ndataset, our system achieves 61.0/53.1 mAP, which surpassed our 2018 winning\nresults by 5.0/4.2 respectively. We achieve the best results in COCO Challenge\n2019 and 2020.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 04:49:37 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Li", "Zeming", ""], ["Ma", "Yuchen", ""], ["Chen", "Yukang", ""], ["Zhang", "Xiangyu", ""], ["Sun", "Jian", ""]]}, {"id": "2010.02484", "submitter": "Youjian Zhang", "authors": "Youjian Zhang, Chaoyue Wang, Stephen J. Maybank, Dacheng Tao", "title": "Self-supervised Exposure Trajectory Recovery for Dynamic Blur Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic scene blurring is an important yet challenging topic. Recently, deep\nlearning methods have achieved impressive performance for dynamic scene\ndeblurring. However, the motion information contained in a blurry image has yet\nto be fully explored and accurately formulated because: (i) the ground truth of\nblurry motion is difficult to obtain; (ii) the temporal ordering is destroyed\nduring the exposure; and (iii) the motion estimation is highly ill-posed. By\nrevisiting the principle of camera exposure, dynamic blur can be described by\nthe relative motions of sharp content with respect to each exposed pixel. We\ndefine exposure trajectories, which record the trajectories of relative motions\nto represent the motion information contained in a blurry image and explain the\ncauses of the dynamic blur. A new blur representation, which we call motion\noffset, is proposed to model pixel-wise displacements of the latent sharp image\nat multiple timepoints. Under mild constraints, the learned motion offsets can\nrecover dense, (non-)linear exposure trajectories, which significantly reduce\ntemporal disorder and ill-posed problems. Finally, we demonstrate that the\nestimated exposure trajectories can fit real-world dynamic blurs and further\ncontribute to motion-aware image deblurring and warping-based video extraction\nfrom a single blurry image.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 05:23:33 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Zhang", "Youjian", ""], ["Wang", "Chaoyue", ""], ["Maybank", "Stephen J.", ""], ["Tao", "Dacheng", ""]]}, {"id": "2010.02488", "submitter": "Zhiwei Xu", "authors": "Zhiwei Xu, Thalaiyasingam Ajanthan, Vibhav Vineet, Richard Hartley", "title": "RANP: Resource Aware Neuron Pruning at Initialization for 3D CNNs", "comments": "International Conference on 3D Vision (3DV), 2020 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although 3D Convolutional Neural Networks (CNNs) are essential for most\nlearning based applications involving dense 3D data, their applicability is\nlimited due to excessive memory and computational requirements. Compressing\nsuch networks by pruning therefore becomes highly desirable. However, pruning\n3D CNNs is largely unexplored possibly because of the complex nature of typical\npruning algorithms that embeds pruning into an iterative optimization paradigm.\nIn this work, we introduce a Resource Aware Neuron Pruning (RANP) algorithm\nthat prunes 3D CNNs at initialization to high sparsity levels. Specifically,\nthe core idea is to obtain an importance score for each neuron based on their\nsensitivity to the loss function. This neuron importance is then reweighted\naccording to the neuron resource consumption related to FLOPs or memory. We\ndemonstrate the effectiveness of our pruning method on 3D semantic segmentation\nwith widely used 3D-UNets on ShapeNet and BraTS'18 as well as on video\nclassification with MobileNetV2 and I3D on UCF101 dataset. In these\nexperiments, our RANP leads to roughly 50-95 reduction in FLOPs and 35-80\nreduction in memory with negligible loss in accuracy compared to the unpruned\nnetworks. This significantly reduces the computational resources required to\ntrain 3D CNNs. The pruned network obtained by our algorithm can also be easily\nscaled up and transferred to another dataset for training.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 05:34:39 GMT"}, {"version": "v2", "created": "Fri, 9 Oct 2020 03:09:59 GMT"}, {"version": "v3", "created": "Sun, 25 Oct 2020 11:52:31 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Xu", "Zhiwei", ""], ["Ajanthan", "Thalaiyasingam", ""], ["Vineet", "Vibhav", ""], ["Hartley", "Richard", ""]]}, {"id": "2010.02502", "submitter": "Jiaming Song", "authors": "Jiaming Song, Chenlin Meng, Stefano Ermon", "title": "Denoising Diffusion Implicit Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Denoising diffusion probabilistic models (DDPMs) have achieved high quality\nimage generation without adversarial training, yet they require simulating a\nMarkov chain for many steps to produce a sample. To accelerate sampling, we\npresent denoising diffusion implicit models (DDIMs), a more efficient class of\niterative implicit probabilistic models with the same training procedure as\nDDPMs. In DDPMs, the generative process is defined as the reverse of a\nMarkovian diffusion process. We construct a class of non-Markovian diffusion\nprocesses that lead to the same training objective, but whose reverse process\ncan be much faster to sample from. We empirically demonstrate that DDIMs can\nproduce high quality samples $10 \\times$ to $50 \\times$ faster in terms of\nwall-clock time compared to DDPMs, allow us to trade off computation for sample\nquality, and can perform semantically meaningful image interpolation directly\nin the latent space.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 06:15:51 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Song", "Jiaming", ""], ["Meng", "Chenlin", ""], ["Ermon", "Stefano", ""]]}, {"id": "2010.02505", "submitter": "Boris Oreshkin N", "authors": "Boris N. Oreshkin and Tal Arbel", "title": "Optimization over Random and Gradient Probabilistic Pixel Sampling for\n  Fast, Robust Multi-Resolution Image Registration", "comments": "arXiv admin note: substantial text overlap with arXiv:2010.00988", "journal-ref": "WBIR 2012. Lecture Notes in Computer Science, vol 7359. Springer,\n  Berlin, Heidelberg", "doi": "10.1007/978-3-642-31340-0_16", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an approach to fast image registration through\nprobabilistic pixel sampling. We propose a practical scheme to leverage the\nbenefits of two state-of-the-art pixel sampling approaches: gradient magnitude\nbased pixel sampling and uniformly random sampling. Our framework involves\nlearning the optimal balance between the two sampling schemes off-line during\ntraining, based on a small training dataset, using particle swarm optimization.\nWe then test the proposed sampling approach on 3D rigid registration against\ntwo state-of-the-art approaches based on the popular, publicly available,\nVanderbilt RIRE dataset. Our results indicate that the proposed sampling\napproach yields much faster, accurate and robust registration results when\ncompared against the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 13:43:50 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Oreshkin", "Boris N.", ""], ["Arbel", "Tal", ""]]}, {"id": "2010.02508", "submitter": "Ryan Campbell", "authors": "Ryan Campbell, Chris Finlay, Adam M Oberman", "title": "Adversarial Boot Camp: label free certified robustness in one epoch", "comments": "13 pages, 5 figures, 5 tables. Under review as a conference paper at\n  ICLR 2021. arXiv admin note: substantial text overlap with arXiv:2006.06061", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning models are vulnerable to adversarial attacks. One approach\nto addressing this vulnerability is certification, which focuses on models that\nare guaranteed to be robust for a given perturbation size. A drawback of recent\ncertified models is that they are stochastic: they require multiple\ncomputationally expensive model evaluations with random noise added to a given\ninput. In our work, we present a deterministic certification approach which\nresults in a certifiably robust model. This approach is based on an equivalence\nbetween training with a particular regularized loss, and the expected values of\nGaussian averages. We achieve certified models on ImageNet-1k by retraining a\nmodel with this loss for one epoch without the use of label information.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 13:47:45 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Campbell", "Ryan", ""], ["Finlay", "Chris", ""], ["Oberman", "Adam M", ""]]}, {"id": "2010.02516", "submitter": "Siddhant Ranade", "authors": "Siddhant Ranade, Xin Yu, Shantnu Kakkar, Pedro Miraldo, Srikumar\n  Ramalingam", "title": "Mapping of Sparse 3D Data using Alternating Projection", "comments": "ACCV2020 oral. This article supersedes arXiv:1906.05888", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel technique to register sparse 3D scans in the absence of\ntexture. While existing methods such as KinectFusion or Iterative Closest\nPoints (ICP) heavily rely on dense point clouds, this task is particularly\nchallenging under sparse conditions without RGB data. Sparse texture-less data\ndoes not come with high-quality boundary signal, and this prohibits the use of\ncorrespondences from corners, junctions, or boundary lines. Moreover, in the\ncase of sparse data, it is incorrect to assume that the same point will be\ncaptured in two consecutive scans. We take a different approach and first\nre-parameterize the point-cloud using a large number of line segments. In this\nre-parameterized data, there exists a large number of line intersection (and\nnot correspondence) constraints that allow us to solve the registration task.\nWe propose the use of a two-step alternating projection algorithm by\nformulating the registration as the simultaneous satisfaction of intersection\nand rigidity constraints. The proposed approach outperforms other top-scoring\nalgorithms on both Kinect and LiDAR datasets. In Kinect, we can use 100X\ndownsampled sparse data and still outperform competing methods operating on\nfull-resolution data.\n", "versions": [{"version": "v1", "created": "Sun, 4 Oct 2020 17:40:30 GMT"}, {"version": "v2", "created": "Fri, 9 Oct 2020 18:22:45 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Ranade", "Siddhant", ""], ["Yu", "Xin", ""], ["Kakkar", "Shantnu", ""], ["Miraldo", "Pedro", ""], ["Ramalingam", "Srikumar", ""]]}, {"id": "2010.02529", "submitter": "Zhenyu Wu", "authors": "Yuli Zheng, Zhenyu Wu, Ye Yuan, Tianlong Chen, Zhangyang Wang", "title": "PCAL: A Privacy-preserving Intelligent Credit Risk Modeling Framework\n  Based on Adversarial Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Credit risk modeling has permeated our everyday life. Most banks and\nfinancial companies use this technique to model their clients' trustworthiness.\nWhile machine learning is increasingly used in this field, the resulting\nlarge-scale collection of user private information has reinvigorated the\nprivacy debate, considering dozens of data breach incidents every year caused\nby unauthorized hackers, and (potentially even more) information misuse/abuse\nby authorized parties. To address those critical concerns, this paper proposes\na framework of Privacy-preserving Credit risk modeling based on Adversarial\nLearning (PCAL). PCAL aims to mask the private information inside the original\ndataset, while maintaining the important utility information for the target\nprediction task performance, by (iteratively) weighing between a privacy-risk\nloss and a utility-oriented loss. PCAL is compared against off-the-shelf\noptions in terms of both utility and privacy protection. Results indicate that\nPCAL can learn an effective, privacy-free representation from user data,\nproviding a solid foundation towards privacy-preserving machine learning for\ncredit risk analysis.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 07:04:59 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Zheng", "Yuli", ""], ["Wu", "Zhenyu", ""], ["Yuan", "Ye", ""], ["Chen", "Tianlong", ""], ["Wang", "Zhangyang", ""]]}, {"id": "2010.02546", "submitter": "Qianwei Zhou", "authors": "Qianwei Zhou, Yuhang Chen, Baoqing Li, Xiaoxin Li, Chen Zhou,\n  Jingchang Huang, Haigen Hu", "title": "Training Deep Neural Networks for Wireless Sensor Networks Using Loosely\n  and Weakly Labeled Images", "comments": null, "journal-ref": null, "doi": "10.1016/j.neucom.2020.09.040", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although deep learning has achieved remarkable successes over the past years,\nfew reports have been published about applying deep neural networks to Wireless\nSensor Networks (WSNs) for image targets recognition where data, energy,\ncomputation resources are limited. In this work, a Cost-Effective Domain\nGeneralization (CEDG) algorithm has been proposed to train an efficient network\nwith minimum labor requirements. CEDG transfers networks from a publicly\navailable source domain to an application-specific target domain through an\nautomatically allocated synthetic domain. The target domain is isolated from\nparameters tuning and used for model selection and testing only. The target\ndomain is significantly different from the source domain because it has new\ntarget categories and is consisted of low-quality images that are out of focus,\nlow in resolution, low in illumination, low in photographing angle. The trained\nnetwork has about 7M (ResNet-20 is about 41M) multiplications per prediction\nthat is small enough to allow a digital signal processor chip to do real-time\nrecognitions in our WSN. The category-level averaged error on the unseen and\nunbalanced target domain has been decreased by 41.12%.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 08:28:34 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Zhou", "Qianwei", ""], ["Chen", "Yuhang", ""], ["Li", "Baoqing", ""], ["Li", "Xiaoxin", ""], ["Zhou", "Chen", ""], ["Huang", "Jingchang", ""], ["Hu", "Haigen", ""]]}, {"id": "2010.02560", "submitter": "Dongki Jung", "authors": "Dongki Jung, Seunghan Yang, Jaehoon Choi, Changick Kim", "title": "Arbitrary Style Transfer using Graph Instance Normalization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Style transfer is the image synthesis task, which applies a style of one\nimage to another while preserving the content. In statistical methods, the\nadaptive instance normalization (AdaIN) whitens the source images and applies\nthe style of target images through normalizing the mean and variance of\nfeatures. However, computing feature statistics for each instance would neglect\nthe inherent relationship between features, so it is hard to learn global\nstyles while fitting to the individual training dataset. In this paper, we\npresent a novel learnable normalization technique for style transfer using\ngraph convolutional networks, termed Graph Instance Normalization (GrIN). This\nalgorithm makes the style transfer approach more robust by taking into account\nsimilar information shared between instances. Besides, this simple module is\nalso applicable to other tasks like image-to-image translation or domain\nadaptation.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 09:07:20 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Jung", "Dongki", ""], ["Yang", "Seunghan", ""], ["Choi", "Jaehoon", ""], ["Kim", "Changick", ""]]}, {"id": "2010.02582", "submitter": "Wei Han", "authors": "Wei Han and Hantao Huang and Tao Han", "title": "Finding the Evidence: Localization-aware Answer Prediction for Text\n  Visual Question Answering", "comments": "Accepted in COLING2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image text carries essential information to understand the scene and perform\nreasoning. Text-based visual question answering (text VQA) task focuses on\nvisual questions that require reading text in images. Existing text VQA systems\ngenerate an answer by selecting from optical character recognition (OCR) texts\nor a fixed vocabulary. Positional information of text is underused and there is\na lack of evidence for the generated answer. As such, this paper proposes a\nlocalization-aware answer prediction network (LaAP-Net) to address this\nchallenge. Our LaAP-Net not only generates the answer to the question but also\npredicts a bounding box as evidence of the generated answer. Moreover, a\ncontext-enriched OCR representation (COR) for multimodal fusion is proposed to\nfacilitate the localization task. Our proposed LaAP-Net outperforms existing\napproaches on three benchmark datasets for the text VQA task by a noticeable\nmargin.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 09:46:20 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Han", "Wei", ""], ["Huang", "Hantao", ""], ["Han", "Tao", ""]]}, {"id": "2010.02623", "submitter": "Umair Haider Dogar", "authors": "Muhammad Umair Haider, Murtaza Taj", "title": "Comprehensive Online Network Pruning via Learnable Scaling Factors", "comments": "Submitted to WACV2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  One of the major challenges in deploying deep neural network architectures is\ntheir size which has an adverse effect on their inference time and memory\nrequirements. Deep CNNs can either be pruned width-wise by removing filters\nbased on their importance or depth-wise by removing layers and blocks. Width\nwise pruning (filter pruning) is commonly performed via learnable gates or\nswitches and sparsity regularizers whereas pruning of layers has so far been\nperformed arbitrarily by manually designing a smaller network usually referred\nto as a student network. We propose a comprehensive pruning strategy that can\nperform both width-wise as well as depth-wise pruning. This is achieved by\nintroducing gates at different granularities (neuron, filter, layer, block)\nwhich are then controlled via an objective function that simultaneously\nperforms pruning at different granularity during each forward pass. Our\napproach is applicable to wide-variety of architectures without any constraints\non spatial dimensions or connection type (sequential, residual, parallel or\ninception). Our method has resulted in a compression ratio of 70% to 90%\nwithout noticeable loss in accuracy when evaluated on benchmark datasets.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 11:04:17 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Haider", "Muhammad Umair", ""], ["Taj", "Murtaza", ""]]}, {"id": "2010.02631", "submitter": "Zhengxiong Luo", "authors": "Zhengxiong Luo, Yan Huang, Shang Li, Liang Wang and Tieniu Tan", "title": "Unfolding the Alternating Optimization for Blind Super Resolution", "comments": "Conference on Neural Information Processing Systems (NeurIPS) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous methods decompose blind super resolution (SR) problem into two\nsequential steps: \\textit{i}) estimating blur kernel from given low-resolution\n(LR) image and \\textit{ii}) restoring SR image based on estimated kernel. This\ntwo-step solution involves two independently trained models, which may not be\nwell compatible with each other. Small estimation error of the first step could\ncause severe performance drop of the second one. While on the other hand, the\nfirst step can only utilize limited information from LR image, which makes it\ndifficult to predict highly accurate blur kernel. Towards these issues, instead\nof considering these two steps separately, we adopt an alternating optimization\nalgorithm, which can estimate blur kernel and restore SR image in a single\nmodel. Specifically, we design two convolutional neural modules, namely\n\\textit{Restorer} and \\textit{Estimator}. \\textit{Restorer} restores SR image\nbased on predicted kernel, and \\textit{Estimator} estimates blur kernel with\nthe help of restored SR image. We alternate these two modules repeatedly and\nunfold this process to form an end-to-end trainable network. In this way,\n\\textit{Estimator} utilizes information from both LR and SR images, which makes\nthe estimation of blur kernel easier. More importantly, \\textit{Restorer} is\ntrained with the kernel estimated by \\textit{Estimator}, instead of\nground-truth kernel, thus \\textit{Restorer} could be more tolerant to the\nestimation error of \\textit{Estimator}. Extensive experiments on synthetic\ndatasets and real-world images show that our model can largely outperform\nstate-of-the-art methods and produce more visually favorable results at much\nhigher speed. The source code is available at\nhttps://github.com/greatlog/DAN.git.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 11:27:27 GMT"}, {"version": "v2", "created": "Wed, 7 Oct 2020 07:13:48 GMT"}, {"version": "v3", "created": "Fri, 16 Oct 2020 15:37:29 GMT"}, {"version": "v4", "created": "Wed, 25 Nov 2020 11:02:44 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Luo", "Zhengxiong", ""], ["Huang", "Yan", ""], ["Li", "Shang", ""], ["Wang", "Liang", ""], ["Tan", "Tieniu", ""]]}, {"id": "2010.02634", "submitter": "Ethan Harris", "authors": "Ethan Harris, Daniela Mihai, Jonathon Hare", "title": "How Convolutional Neural Network Architecture Biases Learned Opponency\n  and Colour Tuning", "comments": "Final version; Accepted for publication in Neural Computation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work suggests that changing Convolutional Neural Network (CNN)\narchitecture by introducing a bottleneck in the second layer can yield changes\nin learned function. To understand this relationship fully requires a way of\nquantitatively comparing trained networks. The fields of electrophysiology and\npsychophysics have developed a wealth of methods for characterising visual\nsystems which permit such comparisons. Inspired by these methods, we propose an\napproach to obtaining spatial and colour tuning curves for convolutional\nneurons, which can be used to classify cells in terms of their spatial and\ncolour opponency. We perform these classifications for a range of CNNs with\ndifferent depths and bottleneck widths. Our key finding is that networks with a\nbottleneck show a strong functional organisation: almost all cells in the\nbottleneck layer become both spatially and colour opponent, cells in the layer\nfollowing the bottleneck become non-opponent. The colour tuning data can\nfurther be used to form a rich understanding of how colour is encoded by a\nnetwork. As a concrete demonstration, we show that shallower networks without a\nbottleneck learn a complex non-linear colour system, whereas deeper networks\nwith tight bottlenecks learn a simple channel opponent code in the bottleneck\nlayer. We further develop a method of obtaining a hue sensitivity curve for a\ntrained CNN which enables high level insights that complement the low level\nfindings from the colour tuning data. We go on to train a series of networks\nunder different conditions to ascertain the robustness of the discussed\nresults. Ultimately, our methods and findings coalesce with prior art,\nstrengthening our ability to interpret trained CNNs and furthering our\nunderstanding of the connection between architecture and learned\nrepresentation. Code for all experiments is available at\nhttps://github.com/ecs-vlc/opponency.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 11:33:48 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Harris", "Ethan", ""], ["Mihai", "Daniela", ""], ["Hare", "Jonathon", ""]]}, {"id": "2010.02644", "submitter": "Reuben R Shamir", "authors": "Reuben R Shamir and Zeev Bomzon", "title": "A Method for Tumor Treating Fields Fast Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tumor Treating Fields (TTFields) is an FDA approved treatment for specific\ntypes of cancer and significantly extends patients life. The intensity of the\nTTFields within the tumor was associated with the treatment outcomes: the\nlarger the intensity the longer the patients are likely to survive. Therefore,\nit was suggested to optimize TTFields transducer array location such that their\nintensity is maximized. Such optimization requires multiple computations of\nTTFields in a simulation framework. However, these computations are typically\nperformed using finite element methods or similar approaches that are time\nconsuming. Therefore, only a limited number of transducer array locations can\nbe examined in practice. To overcome this issue, we have developed a method for\nfast estimation of TTFields intensity. We have designed and implemented a\nmethod that inputs a segmentation of the patients head, a table of tissues\nelectrical properties and the location of the transducer array. The method\noutputs a spatial estimation of the TTFields intensity by incorporating a few\nrelevant parameters in a random-forest regressor. The method was evaluated on\n10 patients (20 TA layouts) in a leave-one-out framework. The computation time\nwas 1.5 minutes using the suggested method, and 180-240 minutes using the\ncommercial simulation. The average error was 0.14 V/cm (SD = 0.06 V/cm) in\ncomparison to the result of the commercial simulation. These results suggest\nthat a fast estimation of TTFields based on a few parameters is feasible. The\npresented method may facilitate treatment optimization and further extend\npatients life.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 11:45:06 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Shamir", "Reuben R", ""], ["Bomzon", "Zeev", ""]]}, {"id": "2010.02659", "submitter": "Harshal Nishar", "authors": "Harshal Nishar, Nikhil Chavanke, Nitin Singhal", "title": "Histopathological Stain Transfer using Style Transfer Network with\n  Adversarial Loss", "comments": "Accepted at MICCAI 2020 Main Track Oral Presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning models that are trained on histopathological images obtained\nfrom a single lab and/or scanner give poor inference performance on images\nobtained from another scanner/lab with a different staining protocol. In recent\nyears, there has been a good amount of research done for image stain\nnormalization to address this issue. In this work, we present a novel approach\nfor the stain normalization problem using fast neural style transfer coupled\nwith adversarial loss. We also propose a novel stain transfer generator network\nbased on High-Resolution Network (HRNet) which requires less training time and\ngives good generalization with few paired training images of reference stain\nand test stain. This approach has been tested on Whole Slide Images (WSIs)\nobtained from 8 different labs, where images from one lab were treated as a\nreference stain. A deep learning model was trained on this stain and the rest\nof the images were transferred to it using the corresponding stain transfer\ngenerator network. Experimentation suggests that this approach is able to\nsuccessfully perform stain normalization with good visual quality and provides\nbetter inference performance compared to not applying stain normalization.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 12:10:50 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Nishar", "Harshal", ""], ["Chavanke", "Nikhil", ""], ["Singhal", "Nitin", ""]]}, {"id": "2010.02680", "submitter": "Allan Pinto", "authors": "Allan Pinto, Manuel A. C\\'ordova, Luis G. L. Decker, Jose L.\n  Flores-Campana, Marcos R. Souza, Andreza A. dos Santos, Jhonatas S.\n  Concei\\c{c}\\~ao, Henrique F. Gagliardi, Diogo C. Luvizon, Ricardo da S.\n  Torres and Helio Pedrini", "title": "Parallax Motion Effect Generation Through Instance Segmentation And\n  Depth Estimation", "comments": "2020 IEEE International Conference on Image Processing (ICIP), Abu\n  Dhabi, United Arab Emirates", "journal-ref": "2020 IEEE International Conference on Image Processing (ICIP), Abu\n  Dhabi, United Arab Emirates, 2020, pp. 1621-1625", "doi": "10.1109/ICIP40778.2020.9191168", "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stereo vision is a growing topic in computer vision due to the innumerable\nopportunities and applications this technology offers for the development of\nmodern solutions, such as virtual and augmented reality applications. To\nenhance the user's experience in three-dimensional virtual environments, the\nmotion parallax estimation is a promising technique to achieve this objective.\nIn this paper, we propose an algorithm for generating parallax motion effects\nfrom a single image, taking advantage of state-of-the-art instance segmentation\nand depth estimation approaches. This work also presents a comparison against\nsuch algorithms to investigate the trade-off between efficiency and quality of\nthe parallax motion effects, taking into consideration a multi-task learning\nnetwork capable of estimating instance segmentation and depth estimation at\nonce. Experimental results and visual quality assessment indicate that the\nPyD-Net network (depth estimation) combined with Mask R-CNN or FBNet networks\n(instance segmentation) can produce parallax motion effects with good visual\nquality.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 12:56:59 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Pinto", "Allan", ""], ["C\u00f3rdova", "Manuel A.", ""], ["Decker", "Luis G. L.", ""], ["Flores-Campana", "Jose L.", ""], ["Souza", "Marcos R.", ""], ["Santos", "Andreza A. dos", ""], ["Concei\u00e7\u00e3o", "Jhonatas S.", ""], ["Gagliardi", "Henrique F.", ""], ["Luvizon", "Diogo C.", ""], ["Torres", "Ricardo da S.", ""], ["Pedrini", "Helio", ""]]}, {"id": "2010.02715", "submitter": "Mohammad Razib Mustafiz", "authors": "Razib Mustafiz, Khaled Mohsin", "title": "Assessing Automated Machine Learning service to detect COVID-19 from\n  X-Ray and CT images: A Real-time Smartphone Application case study", "comments": "21 Pages, 6 Tables", "journal-ref": "IJCSI-2020-17-6-12569", "doi": "10.20944/preprints202009.0647.v1", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The recent outbreak of SARS COV-2 gave us a unique opportunity to study for a\nnon interventional and sustainable AI solution. Lung disease remains a major\nhealthcare challenge with high morbidity and mortality worldwide. The\npredominant lung disease was lung cancer. Until recently, the world has\nwitnessed the global pandemic of COVID19, the Novel coronavirus outbreak. We\nhave experienced how viral infection of lung and heart claimed thousands of\nlives worldwide. With the unprecedented advancement of Artificial Intelligence\nin recent years, Machine learning can be used to easily detect and classify\nmedical imagery. It is much faster and most of the time more accurate than\nhuman radiologists. Once implemented, it is more cost-effective and\ntime-saving. In our study, we evaluated the efficacy of Microsoft Cognitive\nService to detect and classify COVID19 induced pneumonia from other\nViral/Bacterial pneumonia based on X-Ray and CT images. We wanted to assess the\nimplication and accuracy of the Automated ML-based Rapid Application\nDevelopment (RAD) environment in the field of Medical Image diagnosis. This\nstudy will better equip us to respond with an ML-based diagnostic Decision\nSupport System(DSS) for a Pandemic situation like COVID19. After optimization,\nthe trained network achieved 96.8% Average Precision which was implemented as a\nWeb Application for consumption. However, the same trained network did not\nperform the same like Web Application when ported to Smartphone for Real-time\ninference. Which was our main interest of study. The authors believe, there is\nscope for further study on this issue. One of the main goal of this study was\nto develop and evaluate the performance of AI-powered Smartphone-based\nReal-time Application. Facilitating primary diagnostic services in less\nequipped and understaffed rural healthcare centers of the world with unreliable\ninternet service.\n", "versions": [{"version": "v1", "created": "Sat, 3 Oct 2020 23:18:05 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Mustafiz", "Razib", ""], ["Mohsin", "Khaled", ""]]}, {"id": "2010.02725", "submitter": "Nawarathnage Lakmal Deshapriya", "authors": "N. Lakmal Deshapriya, Matthew N. Dailey, Manzul Kumar Hazarika,\n  Hiroyuki Miyazaki", "title": "Vec2Instance: Parameterization for Deep Instance Segmentation", "comments": "12 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Current advances in deep learning is leading to human-level accuracy in\ncomputer vision tasks such as object classification, localization, semantic\nsegmentation, and instance segmentation. In this paper, we describe a new deep\nconvolutional neural network architecture called Vec2Instance for instance\nsegmentation. Vec2Instance provides a framework for parametrization of\ninstances, allowing convolutional neural networks to efficiently estimate the\ncomplex shapes of instances around their centroids. We demonstrate the\nfeasibility of the proposed architecture with respect to instance segmentation\ntasks on satellite images, which have a wide range of applications. Moreover,\nwe demonstrate the usefulness of the new method for extracting building\nfoot-prints from satellite images. Total pixel-wise accuracy of our approach is\n89\\%, near the accuracy of the state-of-the-art Mask RCNN (91\\%). Vec2Instance\nis an alternative approach to complex instance segmentation pipelines, offering\nsimplicity and intuitiveness. The code developed under this study is available\nin the Vec2Instance GitHub repository, https://github.com/lakmalnd/Vec2Instance\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 13:51:02 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Deshapriya", "N. Lakmal", ""], ["Dailey", "Matthew N.", ""], ["Hazarika", "Manzul Kumar", ""], ["Miyazaki", "Hiroyuki", ""]]}, {"id": "2010.02732", "submitter": "Alexander Grimwood", "authors": "Alexander Grimwood, Helen McNair, Yipeng Hu, Ester Bonmati, Dean\n  Barratt, Emma Harris", "title": "Assisted Probe Positioning for Ultrasound Guided Radiotherapy Using\n  Image Sequence Classification", "comments": "Accepted to MICCAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Effective transperineal ultrasound image guidance in prostate external beam\nradiotherapy requires consistent alignment between probe and prostate at each\nsession during patient set-up. Probe placement and ultrasound image\ninter-pretation are manual tasks contingent upon operator skill, leading to\ninteroperator uncertainties that degrade radiotherapy precision. We demonstrate\na method for ensuring accurate probe placement through joint classification of\nimages and probe position data. Using a multi-input multi-task algorithm,\nspatial coordinate data from an optically tracked ultrasound probe is combined\nwith an image clas-sifier using a recurrent neural network to generate two sets\nof predictions in real-time. The first set identifies relevant prostate anatomy\nvisible in the field of view using the classes: outside prostate, prostate\nperiphery, prostate centre. The second set recommends a probe angular\nadjustment to achieve alignment between the probe and prostate centre with the\nclasses: move left, move right, stop. The algo-rithm was trained and tested on\n9,743 clinical images from 61 treatment sessions across 32 patients. We\nevaluated classification accuracy against class labels de-rived from three\nexperienced observers at 2/3 and 3/3 agreement thresholds. For images with\nunanimous consensus between observers, anatomical classification accuracy was\n97.2% and probe adjustment accuracy was 94.9%. The algorithm identified optimal\nprobe alignment within a mean (standard deviation) range of 3.7$^{\\circ}$\n(1.2$^{\\circ}$) from angle labels with full observer consensus, comparable to\nthe 2.8$^{\\circ}$ (2.6$^{\\circ}$) mean interobserver range. We propose such an\nalgorithm could assist ra-diotherapy practitioners with limited experience of\nultrasound image interpreta-tion by providing effective real-time feedback\nduring patient set-up.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 13:55:02 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Grimwood", "Alexander", ""], ["McNair", "Helen", ""], ["Hu", "Yipeng", ""], ["Bonmati", "Ester", ""], ["Barratt", "Dean", ""], ["Harris", "Emma", ""]]}, {"id": "2010.02745", "submitter": "Moritz Platscher", "authors": "Moritz Platscher and Jonathan Zopes and Christian Federau", "title": "Image Translation for Medical Image Generation -- Ischemic Stroke\n  Lesions", "comments": "22 pages; 8 figures; 1 table; submitted to Medical Image Analysis", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning-based automated disease detection and segmentation algorithms\npromise to accelerate and improve many clinical processes. However, such\nalgorithms require vast amounts of annotated training data, which are typically\nnot available in a medical context, e.g., due to data privacy concerns, legal\nobstructions, and non-uniform data formats. Synthetic databases of annotated\npathologies could provide the required amounts of training data. Here, we\ndemonstrate with the example of ischemic stroke that a significant improvement\nin lesion segmentation is feasible using deep learning-based data augmentation.\nTo this end, we train different image-to-image translation models to synthesize\ndiffusion-weighted magnetic resonance images (DWIs) of brain volumes with and\nwithout stroke lesions from semantic segmentation maps. In addition, we train a\ngenerative adversarial network to generate synthetic lesion masks.\nSubsequently, we combine these two components to build a large database of\nsynthetic stroke DWIs. The performance of the various generative models is\nevaluated using a U-Net which is trained to segment stroke lesions on a\nclinical test set. We compare the results to human expert inter-reader scores.\nFor the model with the best performance, we report a maximum Dice score of\n82.6\\%, which significantly outperforms the model trained on the clinical\nimages alone (74.8\\%), and also the inter-reader Dice score of two human\nreaders of 76.9\\%. Moreover, we show that for a very limited database of only\n10 or 50 clinical cases, synthetic data can be used to pre-train the\nsegmentation algorithms, which ultimately yields an improvement by a factor of\nas high as 8 compared to a setting where no synthetic data is used.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 09:12:28 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Platscher", "Moritz", ""], ["Zopes", "Jonathan", ""], ["Federau", "Christian", ""]]}, {"id": "2010.02746", "submitter": "Karim Makki", "authors": "Karim Makki and Amine Bohi and Augustin .C Ogier and Marc Emmanuel\n  Bellemare", "title": "Characterization of surface motion patterns in highly deformable soft\n  tissue organs from dynamic Magnetic Resonance Imaging", "comments": "arXiv admin note: text overlap with arXiv:2003.08332", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV math.DG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present a pipeline for characterization of bladder surface\ndynamics during deep respiratory movements from dynamic Magnetic Resonance\nImaging (MRI). Dynamic MRI may capture temporal anatomical changes in soft\ntissue organs with high-contrast but the obtained sequences usually suffer from\nlimited volume coverage which makes the high resolution reconstruction of organ\nshape trajectories a major challenge in temporal studies. For a compact shape\nrepresentation, the reconstructed temporal data with full volume coverage are\nfirst used to establish a subject-specific dynamical 4D mesh sequences using\nthe large deformation diffeomorphic metric mapping (LDDMM) framework. Then, we\nperformed a statistical characterization of organ shape changes from mechanical\nparameters such as mesh elongations and distortions. Since shape space is\ncurved, we have also used the intrinsic curvature changes as metric to quantify\nsurface evolution. However, the numerical computation of curvature is strongly\ndependant on the surface parameterization (i.e. the mesh resolution). To cope\nwith this dependency, we propose a non-parametric level set method to evaluate\nspatio-temporal surface evolution. Independent of parameterization and\nminimizing the length of the geodesic curves, it shrinks smoothly the surface\ncurves towards a sphere by minimizing a Dirichlet energy. An Eulerian PDE\napproach is used for evaluation of surface dynamics from the curve-shortening\nflow. Results demonstrate the numerical stability of the derived descriptor\nthroughout smooth continuous-time organ trajectories. Intercorrelations between\nindividuals' motion patterns from different geometric features are computed\nusing the Laplace-Beltrami Operator (LBO) eigenfunctions for spherical mapping.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 08:38:08 GMT"}, {"version": "v2", "created": "Fri, 9 Oct 2020 09:10:58 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Makki", "Karim", ""], ["Bohi", "Amine", ""], ["Ogier", "Augustin . C", ""], ["Bellemare", "Marc Emmanuel", ""]]}, {"id": "2010.02771", "submitter": "William Oswaldo Chamorro Hernandez Mr", "authors": "William Chamorro, Juan Andrade-Cetto, Joan Sol\\`a", "title": "High Speed Event Camera TRacking", "comments": "Presented in BMVC 2020, oral session # 5", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Event cameras are bioinspired sensors with reaction times in the order of\nmicroseconds. This property makes them appealing for use in highly-dynamic\ncomputer vision applications. In this work,we explore the limits of this\nsensing technology and present an ultra-fast tracking algorithm able to\nestimate six-degree-of-freedom motion with dynamics over 25.8 g, at a\nthroughput of 10 kHz,processing over a million events per second. Our method is\ncapable of tracking either camera motion or the motion of an object in front of\nit, using an error-state Kalman filter formulated in a Lie-theoretic sense. The\nmethod includes a robust mechanism for the matching of events with projected\nline segments with very fast outlier rejection. Meticulous treatment of sparse\nmatrices is applied to achieve real-time performance. Different motion models\nof varying complexity are considered for the sake of comparison and performance\nanalysis\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 14:41:06 GMT"}, {"version": "v2", "created": "Tue, 13 Oct 2020 11:15:22 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Chamorro", "William", ""], ["Andrade-Cetto", "Juan", ""], ["Sol\u00e0", "Joan", ""]]}, {"id": "2010.02778", "submitter": "Weichao Lan", "authors": "Weichao Lan, Liang Lan", "title": "Compressing Deep Convolutional Neural Networks by Stacking\n  Low-dimensional Binary Convolution Filters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Convolutional Neural Networks (CNN) have been successfully applied to\nmany real-life problems. However, the huge memory cost of deep CNN models poses\na great challenge of deploying them on memory-constrained devices (e.g., mobile\nphones). One popular way to reduce the memory cost of deep CNN model is to\ntrain binary CNN where the weights in convolution filters are either 1 or -1\nand therefore each weight can be efficiently stored using a single bit.\nHowever, the compression ratio of existing binary CNN models is upper bounded\nby around 32. To address this limitation, we propose a novel method to compress\ndeep CNN model by stacking low-dimensional binary convolution filters. Our\nproposed method approximates a standard convolution filter by selecting and\nstacking filters from a set of low-dimensional binary convolution filters. This\nset of low-dimensional binary convolution filters is shared across all filters\nfor a given convolution layer. Therefore, our method will achieve much larger\ncompression ratio than binary CNN models. In order to train our proposed model,\nwe have theoretically shown that our proposed model is equivalent to select and\nstack intermediate feature maps generated by low-dimensional binary filters.\nTherefore, our proposed model can be efficiently trained using the\nsplit-transform-merge strategy. We also provide detailed analysis of the memory\nand computation cost of our model in model inference. We compared the proposed\nmethod with other five popular model compression techniques on two benchmark\ndatasets. Our experimental results have demonstrated that our proposed method\nachieves much higher compression ratio than existing methods while maintains\ncomparable accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 14:49:22 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Lan", "Weichao", ""], ["Lan", "Liang", ""]]}, {"id": "2010.02808", "submitter": "Rob Romijnders", "authors": "Rob Romijnders, Aravindh Mahendran, Michael Tschannen, Josip Djolonga,\n  Marvin Ritter, Neil Houlsby, Mario Lucic", "title": "Representation learning from videos in-the-wild: An object-centric\n  approach", "comments": "Published at WACV 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method to learn image representations from uncurated videos. We\ncombine a supervised loss from off-the-shelf object detectors and\nself-supervised losses which naturally arise from the video-shot-frame-object\nhierarchy present in each video. We report competitive results on 19 transfer\nlearning tasks of the Visual Task Adaptation Benchmark (VTAB), and on 8\nout-of-distribution-generalization tasks, and discuss the benefits and\nshortcomings of the proposed approach. In particular, it improves over the\nbaseline on all 18/19 few-shot learning tasks and 8/8 out-of-distribution\ngeneralization tasks. Finally, we perform several ablation studies and analyze\nthe impact of the pretrained object detector on the performance across this\nsuite of tasks.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 15:17:45 GMT"}, {"version": "v2", "created": "Tue, 9 Feb 2021 17:08:17 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Romijnders", "Rob", ""], ["Mahendran", "Aravindh", ""], ["Tschannen", "Michael", ""], ["Djolonga", "Josip", ""], ["Ritter", "Marvin", ""], ["Houlsby", "Neil", ""], ["Lucic", "Mario", ""]]}, {"id": "2010.02814", "submitter": "Shehroz Khan", "authors": "Shehroz S. Khan, Faraz Khoshbakhtian, Ahmed Bilal Ashraf", "title": "Anomaly Detection Approach to Identify Early Cases in a Pandemic using\n  Chest X-rays", "comments": "9 pages, 3 tables, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The current COVID-19 pandemic is now getting contained, albeit at the cost of\nmorethan2.3million human lives. A critical phase in any pandemic is the early\ndetection of cases to develop preventive treatments and strategies. In the case\nof COVID-19,several studies have indicated that chest radiography images of the\ninfected patients show characteristic abnormalities. However, at the onset of a\ngiven pandemic, such asCOVID-19, there may not be sufficient data for the\naffected cases to train models for their robust detection. Hence, supervised\nclassification is ill-posed for this problem because the time spent in\ncollecting large amounts of data from infected persons could lead to the loss\nof human lives and delays in preventive interventions. Therefore, we formulate\nthe problem of identifying early cases in a pandemic as an anomaly detection\nproblem, in which the data for healthy patients is abundantly available,\nwhereas no training data is present for the class of interest (COVID-19 in our\ncase). To solve this problem, we present several unsupervised deep learning\napproaches, including convolutional and adversarially trained autoencoder. We\ntested two settings on a publicly available dataset (COVIDx)by training the\nmodel on chest X-rays from (i) only healthy adults, and (ii) healthy and other\nnon-COVID-19 pneumonia, and detected COVID-19 as an anomaly.\nAfterperforming3-fold cross validation, we obtain a ROC-AUC of0.765. These\nresults are very encouraging and pave the way towards research for ensuring\nemergency preparedness in future pandemics, especially the ones that could be\ndetected from chest X-rays\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 15:21:31 GMT"}, {"version": "v2", "created": "Tue, 13 Apr 2021 19:54:28 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Khan", "Shehroz S.", ""], ["Khoshbakhtian", "Faraz", ""], ["Ashraf", "Ahmed Bilal", ""]]}, {"id": "2010.02818", "submitter": "Mengran Fan", "authors": "Mengran Fan, Tapabrata Chakrabort, Eric I-Chao Chang, Yan Xu, Jens\n  Rittscher", "title": "Microscopic fine-grained instance classification through deep attention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fine-grained classification of microscopic image data with limited samples is\nan open problem in computer vision and biomedical imaging. Deep learning based\nvision systems mostly deal with high number of low-resolution images, whereas\nsubtle detail in biomedical images require higher resolution. To bridge this\ngap, we propose a simple yet effective deep network that performs two tasks\nsimultaneously in an end-to-end manner. First, it utilises a gated attention\nmodule that can focus on multiple key instances at high resolution without\nextra annotations or region proposals. Second, the global structural features\nand local instance features are fused for final image level classification. The\nresult is a robust but lightweight end-to-end trainable deep network that\nyields state-of-the-art results in two separate fine-grained multi-instance\nbiomedical image classification tasks: a benchmark breast cancer histology\ndataset and our new fungi species mycology dataset. In addition, we demonstrate\nthe interpretability of the proposed model by visualising the concordance of\nthe learned features with clinically relevant features.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 15:29:58 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Fan", "Mengran", ""], ["Chakrabort", "Tapabrata", ""], ["Chang", "Eric I-Chao", ""], ["Xu", "Yan", ""], ["Rittscher", "Jens", ""]]}, {"id": "2010.02824", "submitter": "Mandela Patrick", "authors": "Mandela Patrick, Po-Yao Huang, Yuki Asano, Florian Metze, Alexander\n  Hauptmann, Jo\\~ao Henriques, Andrea Vedaldi", "title": "Support-set bottlenecks for video-text representation learning", "comments": "Accepted as spotlight paper at the International Conference on\n  Learning Representations (ICLR) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The dominant paradigm for learning video-text representations -- noise\ncontrastive learning -- increases the similarity of the representations of\npairs of samples that are known to be related, such as text and video from the\nsame sample, and pushes away the representations of all other pairs. We posit\nthat this last behaviour is too strict, enforcing dissimilar representations\neven for samples that are semantically-related -- for example, visually similar\nvideos or ones that share the same depicted action. In this paper, we propose a\nnovel method that alleviates this by leveraging a generative model to naturally\npush these related samples together: each sample's caption must be\nreconstructed as a weighted combination of other support samples' visual\nrepresentations. This simple idea ensures that representations are not\noverly-specialized to individual samples, are reusable across the dataset, and\nresults in representations that explicitly encode semantics shared between\nsamples, unlike noise contrastive learning. Our proposed method outperforms\nothers by a large margin on MSR-VTT, VATEX and ActivityNet, and MSVD for\nvideo-to-text and text-to-video retrieval.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 15:38:54 GMT"}, {"version": "v2", "created": "Thu, 14 Jan 2021 10:34:56 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Patrick", "Mandela", ""], ["Huang", "Po-Yao", ""], ["Asano", "Yuki", ""], ["Metze", "Florian", ""], ["Hauptmann", "Alexander", ""], ["Henriques", "Jo\u00e3o", ""], ["Vedaldi", "Andrea", ""]]}, {"id": "2010.02849", "submitter": "Alexandre Rame", "authors": "Alexandre Rame, Arthur Douillard, Charles Ollion", "title": "CORE: Color Regression for Multiple Colors Fashion Garments", "comments": "3 pages,1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Among all fashion attributes, color is challenging to detect due to its\nsubjective perception. Existing classification approaches can not go beyond the\npredefined list of discrete color names. In this paper, we argue that color\ndetection is a regression problem. Thus, we propose a new architecture, based\non attention modules and in two-stages. The first stage corrects the image\nillumination while detecting the main discrete color name. The second stage\ncombines a colorname-attention (dependent of the detected color) with an\nobject-attention (dependent of the clothing category) and finally weights a\nspatial pooling over the image pixels' RGB values. We further expand our work\nfor multiple colors garments. We collect a dataset where each fashion item is\nlabeled with a continuous color palette: we empirically show the benefits of\nour approach.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 16:12:30 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Rame", "Alexandre", ""], ["Douillard", "Arthur", ""], ["Ollion", "Charles", ""]]}, {"id": "2010.02871", "submitter": "Evgeny Ponomarev", "authors": "Evgeny Ponomarev and Sergey Matveev and Ivan Oseledets", "title": "LETI: Latency Estimation Tool and Investigation of Neural Networks\n  inference on Mobile GPU", "comments": "Please, don't use this draft version because there are some mistakes\n  in data", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A lot of deep learning applications are desired to be run on mobile devices.\nBoth accuracy and inference time are meaningful for a lot of them. While the\nnumber of FLOPs is usually used as a proxy for neural network latency, it may\nbe not the best choice. In order to obtain a better approximation of latency,\nresearch community uses look-up tables of all possible layers for latency\ncalculation for the final prediction of the inference on mobile CPU. It\nrequires only a small number of experiments. Unfortunately, on mobile GPU this\nmethod is not applicable in a straight-forward way and shows low precision. In\nthis work, we consider latency approximation on mobile GPU as a data and\nhardware-specific problem. Our main goal is to construct a convenient latency\nestimation tool for investigation(LETI) of neural network inference and\nbuilding robust and accurate latency prediction models for each specific task.\nTo achieve this goal, we build open-source tools which provide a convenient way\nto conduct massive experiments on different target devices focusing on mobile\nGPU. After evaluation of the dataset, we learn the regression model on\nexperimental data and use it for future latency prediction and analysis. We\nexperimentally demonstrate the applicability of such an approach on a subset of\npopular NAS-Benchmark 101 dataset and also evaluate the most popular neural\nnetwork architectures for two mobile GPUs. As a result, we construct latency\nprediction model with good precision on the target evaluation subset. We\nconsider LETI as a useful tool for neural architecture search or massive\nlatency evaluation. The project is available at https://github.com/leti-ai\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 16:51:35 GMT"}, {"version": "v2", "created": "Tue, 27 Jul 2021 17:27:45 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Ponomarev", "Evgeny", ""], ["Matveev", "Sergey", ""], ["Oseledets", "Ivan", ""]]}, {"id": "2010.02890", "submitter": "Guy Gilboa", "authors": "Guy Gilboa", "title": "Iterative Methods for Computing Eigenvectors of Nonlinear Operators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.CV cs.NA math.AP math.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this chapter we are examining several iterative methods for solving\nnonlinear eigenvalue problems. These arise in variational image-processing,\ngraph partition and classification, nonlinear physics and more. The canonical\neigenproblem we solve is $T(u)=\\lambda u$, where $T:\\R^n\\to \\R^n$ is some\nbounded nonlinear operator. Other variations of eigenvalue problems are also\ndiscussed. We present a progression of 5 algorithms, coauthored in recent years\nby the author and colleagues. Each algorithm attempts to solve a unique problem\nor to improve the theoretical foundations. The algorithms can be understood as\nnonlinear PDE's which converge to an eigenfunction in the continuous time\ndomain. This allows a unique view and understanding of the discrete iterative\nprocess. Finally, it is shown how to evaluate numerically the results, along\nwith some examples and insights related to priors of nonlinear denoisers, both\nclassical algorithms and ones based on deep networks.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 17:16:38 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Gilboa", "Guy", ""]]}, {"id": "2010.02893", "submitter": "Dongki Jung", "authors": "Jaehoon Choi, Dongki Jung, Donghwan Lee, Changick Kim", "title": "SAFENet: Self-Supervised Monocular Depth Estimation with Semantic-Aware\n  Feature Extraction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-supervised monocular depth estimation has emerged as a promising method\nbecause it does not require groundtruth depth maps during training. As an\nalternative for the groundtruth depth map, the photometric loss enables to\nprovide self-supervision on depth prediction by matching the input image\nframes. However, the photometric loss causes various problems, resulting in\nless accurate depth values compared with supervised approaches. In this paper,\nwe propose SAFENet that is designed to leverage semantic information to\novercome the limitations of the photometric loss. Our key idea is to exploit\nsemantic-aware depth features that integrate the semantic and geometric\nknowledge. Therefore, we introduce multi-task learning schemes to incorporate\nsemantic-awareness into the representation of depth features. Experiments on\nKITTI dataset demonstrate that our methods compete or even outperform the\nstate-of-the-art methods. Furthermore, extensive experiments on different\ndatasets show its better generalization ability and robustness to various\nconditions, such as low-light or adverse weather.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 17:22:25 GMT"}, {"version": "v2", "created": "Tue, 13 Oct 2020 19:52:49 GMT"}, {"version": "v3", "created": "Tue, 29 Dec 2020 07:54:37 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Choi", "Jaehoon", ""], ["Jung", "Dongki", ""], ["Lee", "Donghwan", ""], ["Kim", "Changick", ""]]}, {"id": "2010.02917", "submitter": "Jyoti Aneja", "authors": "Jyoti Aneja, Alexander Schwing, Jan Kautz, Arash Vahdat", "title": "NCP-VAE: Variational Autoencoders with Noise Contrastive Priors", "comments": "22 pages including appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational autoencoders (VAEs) are one of the powerful likelihood-based\ngenerative models with applications in various domains. However, they struggle\nto generate high-quality images, especially when samples are obtained from the\nprior without any tempering. One explanation for VAEs' poor generative quality\nis the prior hole problem: the prior distribution fails to match the aggregate\napproximate posterior. Due to this mismatch, there exist areas in the latent\nspace with high density under the prior that do not correspond to any encoded\nimage. Samples from those areas are decoded to corrupted images. To tackle this\nissue, we propose an energy-based prior defined by the product of a base prior\ndistribution and a reweighting factor, designed to bring the base closer to the\naggregate posterior. We train the reweighting factor by noise contrastive\nestimation, and we generalize it to hierarchical VAEs with many latent variable\ngroups. Our experiments confirm that the proposed noise contrastive priors\nimprove the generative performance of state-of-the-art VAEs by a large margin\non the MNIST, CIFAR-10, CelebA 64, and CelebA HQ 256 datasets.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 17:59:02 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Aneja", "Jyoti", ""], ["Schwing", "Alexander", ""], ["Kautz", "Jan", ""], ["Vahdat", "Arash", ""]]}, {"id": "2010.02949", "submitter": "Bowen Zhang", "authors": "Bowen Zhang, Hexiang Hu, Vihan Jain, Eugene Ie, Fei Sha", "title": "Learning to Represent Image and Text with Denotation Graph", "comments": "to appear at EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning to fuse vision and language information and representing them is an\nimportant research problem with many applications. Recent progresses have\nleveraged the ideas of pre-training (from language modeling) and attention\nlayers in Transformers to learn representation from datasets containing images\naligned with linguistic expressions that describe the images. In this paper, we\npropose learning representations from a set of implied, visually grounded\nexpressions between image and text, automatically mined from those datasets. In\nparticular, we use denotation graphs to represent how specific concepts (such\nas sentences describing images) can be linked to abstract and generic concepts\n(such as short phrases) that are also visually grounded. This type of\ngeneric-to-specific relations can be discovered using linguistic analysis\ntools. We propose methods to incorporate such relations into learning\nrepresentation. We show that state-of-the-art multimodal learning models can be\nfurther improved by leveraging automatically harvested structural relations.\nThe representations lead to stronger empirical results on downstream tasks of\ncross-modal image retrieval, referring expression, and compositional\nattribute-object recognition. Both our codes and the extracted denotation\ngraphs on the Flickr30K and the COCO datasets are publically available on\nhttps://sha-lab.github.io/DG.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 18:00:58 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Zhang", "Bowen", ""], ["Hu", "Hexiang", ""], ["Jain", "Vihan", ""], ["Ie", "Eugene", ""], ["Sha", "Fei", ""]]}, {"id": "2010.02959", "submitter": "Herv\\'e Le Borgne", "authors": "Yannick Le Cacheux and Herv\\'e Le Borgne and Michel Crucianu", "title": "Using Sentences as Semantic Representations in Large Scale Zero-Shot\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zero-shot learning aims to recognize instances of unseen classes, for which\nno visual instance is available during training, by learning multimodal\nrelations between samples from seen classes and corresponding class semantic\nrepresentations. These class representations usually consist of either\nattributes, which do not scale well to large datasets, or word embeddings,\nwhich lead to poorer performance. A good trade-off could be to employ short\nsentences in natural language as class descriptions. We explore different\nsolutions to use such short descriptions in a ZSL setting and show that while\nsimple methods cannot achieve very good results with sentences alone, a\ncombination of usual word embeddings and sentences can significantly outperform\ncurrent state-of-the-art.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 18:22:21 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Cacheux", "Yannick Le", ""], ["Borgne", "Herv\u00e9 Le", ""], ["Crucianu", "Michel", ""]]}, {"id": "2010.03006", "submitter": "Tim Lebailly", "authors": "Tim Lebailly, Sena Kiciroglu, Mathieu Salzmann, Pascal Fua, Wei Wang", "title": "Motion Prediction Using Temporal Inception Module", "comments": "16 pages, 4 figures. To appear in the proceedings of the 15th Asian\n  Conference on Computer Vision, ACCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human motion prediction is a necessary component for many applications in\nrobotics and autonomous driving. Recent methods propose using\nsequence-to-sequence deep learning models to tackle this problem. However, they\ndo not focus on exploiting different temporal scales for different length\ninputs. We argue that the diverse temporal scales are important as they allow\nus to look at the past frames with different receptive fields, which can lead\nto better predictions. In this paper, we propose a Temporal Inception Module\n(TIM) to encode human motion. Making use of TIM, our framework produces input\nembeddings using convolutional layers, by using different kernel sizes for\ndifferent input lengths. The experimental results on standard motion prediction\nbenchmark datasets Human3.6M and CMU motion capture dataset show that our\napproach consistently outperforms the state of the art methods.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 20:26:01 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Lebailly", "Tim", ""], ["Kiciroglu", "Sena", ""], ["Salzmann", "Mathieu", ""], ["Fua", "Pascal", ""], ["Wang", "Wei", ""]]}, {"id": "2010.03016", "submitter": "Bowen Zhang", "authors": "Bowen Zhang, Hao Chen, Meng Wang, Yuanjun Xiong", "title": "Online Action Detection in Streaming Videos with Time Buffers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We formulate the problem of online temporal action detection in live\nstreaming videos, acknowledging one important property of live streaming videos\nthat there is normally a broadcast delay between the latest captured frame and\nthe actual frame viewed by the audience. The standard setting of the online\naction detection task requires immediate prediction after a new frame is\ncaptured. We illustrate that its lack of consideration of the delay is imposing\nunnecessary constraints on the models and thus not suitable for this problem.\nWe propose to adopt the problem setting that allows models to make use of the\nsmall `buffer time' incurred by the delay in live streaming videos. We design\nan action start and end detection framework for this online with buffer setting\nwith two major components: flattened I3D and window-based suppression.\nExperiments on three standard temporal action detection benchmarks under the\nproposed setting demonstrate the effectiveness of the proposed framework. We\nshow that by having a suitable problem setting for this problem with\nwide-applications, we can achieve much better detection accuracy than\noff-the-shelf online action detection models.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 20:43:50 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Zhang", "Bowen", ""], ["Chen", "Hao", ""], ["Wang", "Meng", ""], ["Xiong", "Yuanjun", ""]]}, {"id": "2010.03019", "submitter": "Zhuoran Shen", "authors": "Zhuoran Shen, Irwan Bello, Raviteja Vemulapalli, Xuhui Jia, Ching-Hui\n  Chen", "title": "Global Self-Attention Networks for Image Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, a series of works in computer vision have shown promising results\non various image and video understanding tasks using self-attention. However,\ndue to the quadratic computational and memory complexities of self-attention,\nthese works either apply attention only to low-resolution feature maps in later\nstages of a deep network or restrict the receptive field of attention in each\nlayer to a small local region. To overcome these limitations, this work\nintroduces a new global self-attention module, referred to as the GSA module,\nwhich is efficient enough to serve as the backbone component of a deep network.\nThis module consists of two parallel layers: a content attention layer that\nattends to pixels based only on their content and a positional attention layer\nthat attends to pixels based on their spatial locations. The output of this\nmodule is the sum of the outputs of the two layers. Based on the proposed GSA\nmodule, we introduce new standalone global attention-based deep networks that\nuse GSA modules instead of convolutions to model pixel interactions. Due to the\nglobal extent of the proposed GSA module, a GSA network has the ability to\nmodel long-range pixel interactions throughout the network. Our experimental\nresults show that GSA networks outperform the corresponding convolution-based\nnetworks significantly on the CIFAR-100 and ImageNet datasets while using less\nparameters and computations. The proposed GSA networks also outperform various\nexisting attention-based networks on the ImageNet dataset.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 20:55:34 GMT"}, {"version": "v2", "created": "Wed, 14 Oct 2020 05:46:59 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Shen", "Zhuoran", ""], ["Bello", "Irwan", ""], ["Vemulapalli", "Raviteja", ""], ["Jia", "Xuhui", ""], ["Chen", "Ching-Hui", ""]]}, {"id": "2010.03023", "submitter": "Rakshit Naidu", "authors": "Rakshit Naidu, Ankita Ghosh, Yash Maurya, Shamanth R Nayak K, Soumya\n  Snigdha Kundu", "title": "IS-CAM: Integrated Score-CAM for axiomatic-based explanations", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Convolutional Neural Networks have been known as black-box models as humans\ncannot interpret their inner functionalities. With an attempt to make CNNs more\ninterpretable and trustworthy, we propose IS-CAM (Integrated Score-CAM), where\nwe introduce the integration operation within the Score-CAM pipeline to achieve\nvisually sharper attribution maps quantitatively. Our method is evaluated on\n2000 randomly selected images from the ILSVRC 2012 Validation dataset, which\nproves the versatility of IS-CAM to account for different models and methods.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 21:03:03 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Naidu", "Rakshit", ""], ["Ghosh", "Ankita", ""], ["Maurya", "Yash", ""], ["K", "Shamanth R Nayak", ""], ["Kundu", "Soumya Snigdha", ""]]}, {"id": "2010.03024", "submitter": "Yash Satsangi", "authors": "Yash Satsangi, Shimon Whiteson, Frans A. Oliehoek, Henri Bouma", "title": "Real-Time Resource Allocation for Tracking Systems", "comments": "http://auai.org/uai2017/proceedings/papers/130.pdf", "journal-ref": "UAI 2017", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated tracking is key to many computer vision applications. However, many\ntracking systems struggle to perform in real-time due to the high computational\ncost of detecting people, especially in ultra high resolution images. We\npropose a new algorithm called \\emph{PartiMax} that greatly reduces this cost\nby applying the person detector only to the relevant parts of the image.\nPartiMax exploits information in the particle filter to select $k$ of the $n$\ncandidate \\emph{pixel boxes} in the image. We prove that PartiMax is guaranteed\nto make a near-optimal selection with error bounds that are independent of the\nproblem size. Furthermore, empirical results on a real-life dataset show that\nour system runs in real-time by processing only 10\\% of the pixel boxes in the\nimage while still retaining 80\\% of the original tracking performance achieved\nwhen processing all pixel boxes.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2020 08:29:05 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Satsangi", "Yash", ""], ["Whiteson", "Shimon", ""], ["Oliehoek", "Frans A.", ""], ["Bouma", "Henri", ""]]}, {"id": "2010.03026", "submitter": "Guilherme Nardari", "authors": "Guilherme V. Nardari, Avraham Cohen, Steven W. Chen, Xu Liu, Vaibhav\n  Arcot, Roseli A. F. Romero, and Vijay Kumar", "title": "Place Recognition in Forests with Urquhart Tessellations", "comments": "9 pages, 6 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this letter, we present a novel descriptor based on Urquhart tessellations\nderived from the position of trees in a forest. We propose a framework that\nuses these descriptors to detect previously seen observations and landmark\ncorrespondences, even with partial overlap and noise. We run loop closure\ndetection experiments in simulation and real-world data map-merging from\ndifferent flights of an Unmanned Aerial Vehicle (UAV) in a pine tree forest and\nshow that our method outperforms state-of-the-art approaches in accuracy and\nrobustness.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2020 20:19:38 GMT"}, {"version": "v2", "created": "Mon, 16 Nov 2020 14:13:38 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Nardari", "Guilherme V.", ""], ["Cohen", "Avraham", ""], ["Chen", "Steven W.", ""], ["Liu", "Xu", ""], ["Arcot", "Vaibhav", ""], ["Romero", "Roseli A. F.", ""], ["Kumar", "Vijay", ""]]}, {"id": "2010.03027", "submitter": "Xiao Yan", "authors": "Xiao Yan, Gang Kou, Feng Xiao, Dapeng Zhang, Xianghua Gan", "title": "Predicting Hourly Demand in Station-free Bike-sharing Systems with\n  Video-level Data", "comments": "12 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Temporal and spatial features are both important for predicting the demands\nin the bike-sharing systems. Many relevant experiments in the literature\nsupport this. Meanwhile, it is observed that the data structure of spatial\nfeatures with vector form is weaker in space than the videos, which have\nnatural spatial structure. Therefore, to obtain more spatial features, this\nstudy introduces city map to generate GPS demand videos while employing a novel\nalgorithm : eidetic 3D convolutional long short-term memory network named\nE3D-LSTM to process the video-level data in bike-sharing system. The\nspatio-temporal correlations and feature importance are experimented and\nvisualized to validate the significance of spatial and temporal features.\nDespite the deep learning model is powerful in non-linear fitting ability,\nstatistic model has better interpretation. This study adopts ensemble learning,\nwhich is a popular policy, to improve the performance and decrease variance. In\nthis paper, we propose a novel model stacked by deep learning and statistical\nmodels, named the fusion multi-channel eidetic 3D convolutional long short-term\nmemory network(FM-E3DCL-Net), to better process temporal and spatial features\non the dataset about 100,000 transactions within one month in Shanghai of\nMobike company. Furthermore, other factors like weather, holiday and time\nintervals are proved useful in addition to historical demand, since they\ndecrease the root mean squared error (RMSE) by 29.4%. On this basis, the\nensemble learning further decreases RMSE by 6.6%.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2020 06:51:23 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Yan", "Xiao", ""], ["Kou", "Gang", ""], ["Xiao", "Feng", ""], ["Zhang", "Dapeng", ""], ["Gan", "Xianghua", ""]]}, {"id": "2010.03028", "submitter": "Hazrat Ali", "authors": "Hazrat Ali, Johannes Umander, Robin Rohl\\'en and Christer Gr\\\"onlund", "title": "A deep learning pipeline for identification of motor units in\n  musculoskeletal ultrasound", "comments": null, "journal-ref": null, "doi": "10.1109/ACCESS.2020.3023495", "report-no": null, "categories": "cs.CV cs.LG eess.IV eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ultrasound imaging provides information from a large part of the muscle. It\nhas recently been shown that ultrafast ultrasound imaging can be used to record\nand analyze the mechanical response of individual MUs using blind source\nseparation. In this work, we present an alternative method - a deep learning\npipeline - to identify active MUs in ultrasound image sequences, including\nsegmentation of their territories and signal estimation of their mechanical\nresponses (twitch train). We train and evaluate the model using simulated data\nmimicking the complex activation pattern of tens of activated MUs with\noverlapping territories and partially synchronized activation patterns. Using a\nslow fusion approach (based on 3D CNNs), we transform the spatiotemporal image\nsequence data to 2D representations and apply a deep neural network\narchitecture for segmentation. Next, we employ a second deep neural network\narchitecture for signal estimation. The results show that the proposed pipeline\ncan effectively identify individual MUs, estimate their territories, and\nestimate their twitch train signal at low contraction forces. The framework can\nretain spatio-temporal consistencies and information of the mechanical response\nof MU activity even when the ultrasound image sequences are transformed into a\n2D representation for compatibility with more traditional computer vision and\nimage processing techniques. The proposed pipeline is potentially useful to\nidentify simultaneously active MUs in whole muscles in ultrasound image\nsequences of voluntary skeletal muscle contractions at low force levels.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2020 20:44:29 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Ali", "Hazrat", ""], ["Umander", "Johannes", ""], ["Rohl\u00e9n", "Robin", ""], ["Gr\u00f6nlund", "Christer", ""]]}, {"id": "2010.03045", "submitter": "Diganta Misra", "authors": "Diganta Misra, Trikay Nalamada, Ajay Uppili Arasanipalai, Qibin Hou", "title": "Rotate to Attend: Convolutional Triplet Attention Module", "comments": "Accepted to WACV 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Benefiting from the capability of building inter-dependencies among channels\nor spatial locations, attention mechanisms have been extensively studied and\nbroadly used in a variety of computer vision tasks recently. In this paper, we\ninvestigate light-weight but effective attention mechanisms and present triplet\nattention, a novel method for computing attention weights by capturing\ncross-dimension interaction using a three-branch structure. For an input\ntensor, triplet attention builds inter-dimensional dependencies by the rotation\noperation followed by residual transformations and encodes inter-channel and\nspatial information with negligible computational overhead. Our method is\nsimple as well as efficient and can be easily plugged into classic backbone\nnetworks as an add-on module. We demonstrate the effectiveness of our method on\nvarious challenging tasks including image classification on ImageNet-1k and\nobject detection on MSCOCO and PASCAL VOC datasets. Furthermore, we provide\nextensive in-sight into the performance of triplet attention by visually\ninspecting the GradCAM and GradCAM++ results. The empirical evaluation of our\nmethod supports our intuition on the importance of capturing dependencies\nacross dimensions when computing attention weights. Code for this paper can be\npublicly accessed at https://github.com/LandskapeAI/triplet-attention\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 21:31:00 GMT"}, {"version": "v2", "created": "Thu, 5 Nov 2020 19:08:52 GMT"}], "update_date": "2020-11-09", "authors_parsed": [["Misra", "Diganta", ""], ["Nalamada", "Trikay", ""], ["Arasanipalai", "Ajay Uppili", ""], ["Hou", "Qibin", ""]]}, {"id": "2010.03060", "submitter": "Gongbo Liang", "authors": "Gongbo Liang, Connor Greenwell, Yu Zhang, Xiaoqin Wang, Ramakanth\n  Kavuluru, Nathan Jacobs", "title": "Contrastive Cross-Modal Pre-Training: A General Strategy for Small\n  Sample Medical Imaging", "comments": "This work is under review with the IEEE Journal of Biomedical and\n  Health Informatics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key challenge in training neural networks for a given medical imaging task\nis often the difficulty of obtaining a sufficient number of manually labeled\nexamples. In contrast, textual imaging reports, which are often readily\navailable in medical records, contain rich but unstructured interpretations\nwritten by experts as part of standard clinical practice. We propose using\nthese textual reports as a form of weak supervision to improve the image\ninterpretation performance of a neural network without requiring additional\nmanually labeled examples. We use an image-text matching task to train a\nfeature extractor and then fine-tune it in a transfer learning setting for a\nsupervised task using a small labeled dataset. The end result is a neural\nnetwork that automatically interprets imagery without requiring textual reports\nduring inference. This approach can be applied to any task for which text-image\npairs are readily available. We evaluate our method on three classification\ntasks and find consistent performance improvements, reducing the need for\nlabeled data by 67%-98%.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 22:20:29 GMT"}, {"version": "v2", "created": "Mon, 19 Apr 2021 15:18:04 GMT"}, {"version": "v3", "created": "Mon, 10 May 2021 17:10:18 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Liang", "Gongbo", ""], ["Greenwell", "Connor", ""], ["Zhang", "Yu", ""], ["Wang", "Xiaoqin", ""], ["Kavuluru", "Ramakanth", ""], ["Jacobs", "Nathan", ""]]}, {"id": "2010.03071", "submitter": "Ashiq Imran", "authors": "Ashiq Imran and Vassilis Athitsos", "title": "Domain Adaptive Transfer Learning on Visual Attention Aware Data\n  Augmentation for Fine-grained Visual Categorization", "comments": "18 pages, 12 figures, 4 tables", "journal-ref": "Will be published in ISVC 2020", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fine-Grained Visual Categorization (FGVC) is a challenging topic in computer\nvision. It is a problem characterized by large intra-class differences and\nsubtle inter-class differences. In this paper, we tackle this problem in a\nweakly supervised manner, where neural network models are getting fed with\nadditional data using a data augmentation technique through a visual attention\nmechanism. We perform domain adaptive knowledge transfer via fine-tuning on our\nbase network model. We perform our experiment on six challenging and commonly\nused FGVC datasets, and we show competitive improvement on accuracies by using\nattention-aware data augmentation techniques with features derived from deep\nlearning model InceptionV3, pre-trained on large scale datasets. Our method\noutperforms competitor methods on multiple FGVC datasets and showed competitive\nresults on other datasets. Experimental studies show that transfer learning\nfrom large scale datasets can be utilized effectively with visual attention\nbased data augmentation, which can obtain state-of-the-art results on several\nFGVC datasets. We present a comprehensive analysis of our experiments. Our\nmethod achieves state-of-the-art results in multiple fine-grained\nclassification datasets including challenging CUB200-2011 bird, Flowers-102,\nand FGVC-Aircrafts datasets.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 22:47:57 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Imran", "Ashiq", ""], ["Athitsos", "Vassilis", ""]]}, {"id": "2010.03072", "submitter": "Ryutaroh Matsumoto", "authors": "Koichiro Yamanaka, Ryutaroh Matsumoto, Keita Takahashi, and Toshiaki\n  Fujii", "title": "Adversarial Patch Attacks on Monocular Depth Estimation Networks", "comments": "Publisher's Open Access PDF with the CC-BY copyright. Associated\n  video, data and programs are available at\n  https://www.fujii.nuee.nagoya-u.ac.jp/Research/MonoDepth/", "journal-ref": "IEEE Access, vol.8, pp.179094-179104, October 2020", "doi": "10.1109/ACCESS.2020.3027372", "report-no": null, "categories": "cs.CV cs.CR eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Thanks to the excellent learning capability of deep convolutional neural\nnetworks (CNN), monocular depth estimation using CNNs has achieved great\nsuccess in recent years. However, depth estimation from a monocular image alone\nis essentially an ill-posed problem, and thus, it seems that this approach\nwould have inherent vulnerabilities. To reveal this limitation, we propose a\nmethod of adversarial patch attack on monocular depth estimation. More\nspecifically, we generate artificial patterns (adversarial patches) that can\nfool the target methods into estimating an incorrect depth for the regions\nwhere the patterns are placed. Our method can be implemented in the real world\nby physically placing the printed patterns in real scenes. We also analyze the\nbehavior of monocular depth estimation under attacks by visualizing the\nactivation levels of the intermediate layers and the regions potentially\naffected by the adversarial attack.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 22:56:22 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Yamanaka", "Koichiro", ""], ["Matsumoto", "Ryutaroh", ""], ["Takahashi", "Keita", ""], ["Fujii", "Toshiaki", ""]]}, {"id": "2010.03108", "submitter": "Pan Ji", "authors": "Pengfei Fang, Pan Ji, Jieming Zhou, Lars Petersson, Mehrtash Harandi", "title": "Channel Recurrent Attention Networks for Video Pedestrian Retrieval", "comments": "To appear in ACCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Full attention, which generates an attention value per element of the input\nfeature maps, has been successfully demonstrated to be beneficial in visual\ntasks. In this work, we propose a fully attentional network, termed {\\it\nchannel recurrent attention network}, for the task of video pedestrian\nretrieval. The main attention unit, \\textit{channel recurrent attention},\nidentifies attention maps at the frame level by jointly leveraging spatial and\nchannel patterns via a recurrent neural network. This channel recurrent\nattention is designed to build a global receptive field by recurrently\nreceiving and learning the spatial vectors. Then, a \\textit{set aggregation}\ncell is employed to generate a compact video representation. Empirical\nexperimental results demonstrate the superior performance of the proposed deep\nnetwork, outperforming current state-of-the-art results across standard video\nperson retrieval benchmarks, and a thorough ablation study shows the\neffectiveness of the proposed units.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 02:01:13 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Fang", "Pengfei", ""], ["Ji", "Pan", ""], ["Zhou", "Jieming", ""], ["Petersson", "Lars", ""], ["Harandi", "Mehrtash", ""]]}, {"id": "2010.03115", "submitter": "Cao Yun", "authors": "Yun Cao, Jie Mei, Yuebin Wang, Liqiang Zhang, Junhuan Peng, Bing\n  Zhang, Lihua Li, and Yibo Zheng", "title": "SLCRF: Subspace Learning with Conditional Random Field for Hyperspectral\n  Image Classification", "comments": "13 pages, 6 figures", "journal-ref": null, "doi": "10.1109/TGRS.2020.3011429", "report-no": null, "categories": "cs.CV math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Subspace learning (SL) plays an important role in hyperspectral image (HSI)\nclassification, since it can provide an effective solution to reduce the\nredundant information in the image pixels of HSIs. Previous works about SL aim\nto improve the accuracy of HSI recognition. Using a large number of labeled\nsamples, related methods can train the parameters of the proposed solutions to\nobtain better representations of HSI pixels. However, the data instances may\nnot be sufficient enough to learn a precise model for HSI classification in\nreal applications. Moreover, it is well-known that it takes much time, labor\nand human expertise to label HSI images. To avoid the aforementioned problems,\na novel SL method that includes the probability assumption called subspace\nlearning with conditional random field (SLCRF) is developed. In SLCRF, first,\nthe 3D convolutional autoencoder (3DCAE) is introduced to remove the redundant\ninformation in HSI pixels. In addition, the relationships are also constructed\nusing the spectral-spatial information among the adjacent pixels. Then, the\nconditional random field (CRF) framework can be constructed and further\nembedded into the HSI SL procedure with the semi-supervised approach. Through\nthe linearized alternating direction method termed LADMAP, the objective\nfunction of SLCRF is optimized using a defined iterative algorithm. The\nproposed method is comprehensively evaluated using the challenging public HSI\ndatasets. We can achieve stateof-the-art performance using these HSI sets.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 02:25:32 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Cao", "Yun", ""], ["Mei", "Jie", ""], ["Wang", "Yuebin", ""], ["Zhang", "Liqiang", ""], ["Peng", "Junhuan", ""], ["Zhang", "Bing", ""], ["Li", "Lihua", ""], ["Zheng", "Yibo", ""]]}, {"id": "2010.03116", "submitter": "Cao Yun", "authors": "Yun Cao, Yuebin Wang, Junhuan Peng, Liqiang Zhang, Linlin Xu, Kai Yan,\n  and Lihua Li", "title": "DML-GANR: Deep Metric Learning With Generative Adversarial Network\n  Regularization for High Spatial Resolution Remote Sensing Image Retrieval", "comments": "17 pages", "journal-ref": null, "doi": "10.1109/TGRS.2020.2991545", "report-no": null, "categories": "cs.CV math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With a small number of labeled samples for training, it can save considerable\nmanpower and material resources, especially when the amount of high spatial\nresolution remote sensing images (HSR-RSIs) increases considerably. However,\nmany deep models face the problem of overfitting when using a small number of\nlabeled samples. This might degrade HSRRSI retrieval accuracy. Aiming at\nobtaining more accurate HSR-RSI retrieval performance with small training\nsamples, we develop a deep metric learning approach with generative adversarial\nnetwork regularization (DML-GANR) for HSR-RSI retrieval. The DML-GANR starts\nfrom a high-level feature extraction (HFE) to extract high-level features,\nwhich includes convolutional layers and fully connected (FC) layers. Each of\nthe FC layers is constructed by deep metric learning (DML) to maximize the\ninterclass variations and minimize the intraclass variations. The generative\nadversarial network (GAN) is adopted to mitigate the overfitting problem and\nvalidate the qualities of extracted high-level features. DML-GANR is optimized\nthrough a customized approach, and the optimal parameters are obtained. The\nexperimental results on the three data sets demonstrate the superior\nperformance of DML-GANR over state-of-the-art techniques in HSR-RSI retrieval.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 02:26:03 GMT"}], "update_date": "2020-12-30", "authors_parsed": [["Cao", "Yun", ""], ["Wang", "Yuebin", ""], ["Peng", "Junhuan", ""], ["Zhang", "Liqiang", ""], ["Xu", "Linlin", ""], ["Yan", "Kai", ""], ["Li", "Lihua", ""]]}, {"id": "2010.03122", "submitter": "Yan Luo", "authors": "Yukang Jiang, Jianying Pan, Yanhe Shen, Jin Zhu, Jiamin Huang, Huirui\n  Xie, Xueqin Wang, Yan Luo", "title": "A Fast and Effective Method of Macula Automatic Detection for Retina\n  Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Retina image processing is one of the crucial and popular topics of medical\nimage processing. The macula fovea is responsible for sharp central vision,\nwhich is necessary for human behaviors where visual detail is of primary\nimportance, such as reading, writing, driving, etc. This paper proposes a novel\nmethod to locate the macula through a series of morphological processing. On\nthe premise of maintaining high accuracy, our approach is simpler and faster\nthan others. Furthermore, for the hospital's real images, our method is also\nable to detect the macula robustly.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 02:41:29 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Jiang", "Yukang", ""], ["Pan", "Jianying", ""], ["Shen", "Yanhe", ""], ["Zhu", "Jin", ""], ["Huang", "Jiamin", ""], ["Xie", "Huirui", ""], ["Wang", "Xueqin", ""], ["Luo", "Yan", ""]]}, {"id": "2010.03132", "submitter": "Kanchana Ranasinghe Mr", "authors": "Sameera Ramasinghe, Kanchana Ranasinghe, Salman Khan, Nick Barnes, and\n  Stephen Gould", "title": "Conditional Generative Modeling via Learning the Latent Space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although deep learning has achieved appealing results on several machine\nlearning tasks, most of the models are deterministic at inference, limiting\ntheir application to single-modal settings. We propose a novel general-purpose\nframework for conditional generation in multimodal spaces, that uses latent\nvariables to model generalizable learning patterns while minimizing a family of\nregression cost functions. At inference, the latent variables are optimized to\nfind optimal solutions corresponding to multiple output modes. Compared to\nexisting generative solutions, in multimodal spaces, our approach demonstrates\nfaster and stable convergence, and can learn better representations for\ndownstream tasks. Importantly, it provides a simple generic model that can beat\nhighly engineered pipelines tailored using domain expertise on a variety of\ntasks, while generating diverse outputs. Our codes will be released.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 03:11:34 GMT"}, {"version": "v2", "created": "Fri, 9 Oct 2020 03:29:17 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Ramasinghe", "Sameera", ""], ["Ranasinghe", "Kanchana", ""], ["Khan", "Salman", ""], ["Barnes", "Nick", ""], ["Gould", "Stephen", ""]]}, {"id": "2010.03173", "submitter": "Mohamed Mejri", "authors": "Mejri Mohamed, Antoine Richard, Cedric Pradalier", "title": "A Study on Trees's Knots Prediction from their Bark Outer-Shape", "comments": "arXiv admin note: text overlap with arXiv:2002.04571", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  In the industry, the value of wood-logs strongly depends on their internal\nstructure and more specifically on the knots' distribution inside the trees. As\nof today, CT-scanners are the prevalent tool to acquire accurate images of the\ntrees internal structure. However, CT-scanners are expensive, and slow, making\ntheir use impractical for most industrial applications. Knowing where the knots\nare within a tree could improve the efficiency of the overall tree industry by\nreducing waste and improving the quality of wood-logs by-products. In this\npaper we evaluate different deep-learning based architectures to predict the\ninternal knots distribution of a tree from its outer-shape, something that has\nnever been done before. Three types of techniques based on Convolutional Neural\nNetworks (CNN) will be studied.\n  The architectures are tested on both real and synthetic CT-scanned trees.\nWith these experiments, we demonstrate that CNNs can be used to predict\ninternal knots distribution based on the external surface of the trees. The\ngoal being to show that these inexpensive and fast methods could be used to\nreplace the CT-scanners.\n  Additionally, we look into the performance of several off-the-shelf\nobject-detectors to detect knots inside CT-scanned images. This method is used\nto autonomously label part of our real CT-scanned trees alleviating the need to\nmanually segment the whole of the images.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 19:13:10 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Mohamed", "Mejri", ""], ["Richard", "Antoine", ""], ["Pradalier", "Cedric", ""]]}, {"id": "2010.03182", "submitter": "Siqu Long", "authors": "Soyeon Caren Han, Siqu Long, Siwen Luo, Kunze Wang, Josiah Poon", "title": "VICTR: Visual Information Captured Text Representation for Text-to-Image\n  Multimodal Tasks", "comments": "Accepted by COLING 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text-to-image multimodal tasks, generating/retrieving an image from a given\ntext description, are extremely challenging tasks since raw text descriptions\ncover quite limited information in order to fully describe visually realistic\nimages. We propose a new visual contextual text representation for\ntext-to-image multimodal tasks, VICTR, which captures rich visual semantic\ninformation of objects from the text input. First, we use the text description\nas initial input and conduct dependency parsing to extract the syntactic\nstructure and analyse the semantic aspect, including object quantities, to\nextract the scene graph. Then, we train the extracted objects, attributes, and\nrelations in the scene graph and the corresponding geometric relation\ninformation using Graph Convolutional Networks, and it generates text\nrepresentation which integrates textual and visual semantic information. The\ntext representation is aggregated with word-level and sentence-level embedding\nto generate both visual contextual word and sentence representation. For the\nevaluation, we attached VICTR to the state-of-the-art models in text-to-image\ngeneration.VICTR is easily added to existing models and improves across both\nquantitative and qualitative aspects.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 05:25:30 GMT"}, {"version": "v2", "created": "Wed, 14 Oct 2020 12:20:43 GMT"}, {"version": "v3", "created": "Sun, 25 Oct 2020 05:21:52 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Han", "Soyeon Caren", ""], ["Long", "Siqu", ""], ["Luo", "Siwen", ""], ["Wang", "Kunze", ""], ["Poon", "Josiah", ""]]}, {"id": "2010.03196", "submitter": "Ekta Samani", "authors": "Ekta U. Samani, Xingjian Yang, Ashis G. Banerjee", "title": "Visual Object Recognition in Indoor Environments Using Topologically\n  Persistent Features", "comments": "This work has been accepted for publication in the IEEE Robotics And\n  Automation Letters", "journal-ref": null, "doi": "10.1109/LRA.2021.3099460", "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object recognition in unseen indoor environments remains a challenging\nproblem for visual perception of mobile robots. In this letter, we propose the\nuse of topologically persistent features, which rely on the objects' shape\ninformation, to address this challenge. In particular, we extract two kinds of\nfeatures, namely, sparse persistence image (PI) and amplitude, by applying\npersistent homology to multi-directional height function-based filtrations of\nthe cubical complexes representing the object segmentation maps. The features\nare then used to train a fully connected network for recognition. For\nperformance evaluation, in addition to a widely used shape dataset and a\nbenchmark indoor scenes dataset, we collect a new dataset, comprising scene\nimages from two different environments, namely, a living room and a mock\nwarehouse. The scenes are captured using varying camera poses under different\nillumination conditions and include up to five different objects from a given\nset of fourteen objects. On the benchmark indoor scenes dataset, sparse PI\nfeatures show better recognition performance in unseen environments than the\nfeatures learned using the widely used ResNetV2-56 and EfficientNet-B4 models.\nFurther, they provide slightly higher recall and accuracy values than Faster\nR-CNN, an end-to-end object detection method, and its state-of-the-art variant,\nDomain Adaptive Faster R-CNN. The performance of our methods also remains\nrelatively unchanged from the training environment (living room) to the unseen\nenvironment (mock warehouse) in the new dataset. In contrast, the performance\nof the object detection methods drops substantially. We also implement the\nproposed method on a real-world robot to demonstrate its usefulness.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 06:04:17 GMT"}, {"version": "v2", "created": "Sat, 17 Oct 2020 19:11:03 GMT"}, {"version": "v3", "created": "Mon, 8 Mar 2021 19:21:48 GMT"}, {"version": "v4", "created": "Thu, 20 May 2021 17:24:47 GMT"}, {"version": "v5", "created": "Wed, 28 Jul 2021 18:05:18 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Samani", "Ekta U.", ""], ["Yang", "Xingjian", ""], ["Banerjee", "Ashis G.", ""]]}, {"id": "2010.03199", "submitter": "Vikram Singh", "authors": "Vikram Singh (1), Anurag Mittal (1) ((1) Indian Institute of\n  Technology - Madras)", "title": "WDN: A Wide and Deep Network to Divide-and-Conquer Image\n  Super-resolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Divide and conquer is an established algorithm design paradigm that has\nproven itself to solve a variety of problems efficiently. However, it is yet to\nbe fully explored in solving problems with a neural network, particularly the\nproblem of image super-resolution. In this work, we propose an approach to\ndivide the problem of image super-resolution into multiple sub-problems and\nthen solve/conquer them with the help of a neural network. Unlike a typical\ndeep neural network, we design an alternate network architecture that is much\nwider (along with being deeper) than existing networks and is specially\ndesigned to implement the divide-and-conquer design paradigm with a neural\nnetwork. Additionally, a technique to calibrate the intensities of feature map\npixels is being introduced. Extensive experimentation on five datasets reveals\nthat our approach towards the problem and the proposed architecture generate\nbetter and sharper results than current state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 06:15:11 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Singh", "Vikram", ""], ["Mittal", "Anurag", ""]]}, {"id": "2010.03201", "submitter": "Xuelin Qian", "authors": "Xuelin Qian, Huazhu Fu, Weiya Shi, Tao Chen, Yanwei Fu, Fei Shan,\n  Xiangyang Xue", "title": "M3Lung-Sys: A Deep Learning System for Multi-Class Lung Pneumonia\n  Screening from CT Imaging", "comments": "IEEE Journal of Biomedical and Health Informatics (JBHI), 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To counter the outbreak of COVID-19, the accurate diagnosis of suspected\ncases plays a crucial role in timely quarantine, medical treatment, and\npreventing the spread of the pandemic. Considering the limited training cases\nand resources (e.g, time and budget), we propose a Multi-task Multi-slice Deep\nLearning System (M3Lung-Sys) for multi-class lung pneumonia screening from CT\nimaging, which only consists of two 2D CNN networks, i.e., slice- and\npatient-level classification networks. The former aims to seek the feature\nrepresentations from abundant CT slices instead of limited CT volumes, and for\nthe overall pneumonia screening, the latter one could recover the temporal\ninformation by feature refinement and aggregation between different slices. In\naddition to distinguish COVID-19 from Healthy, H1N1, and CAP cases, our M 3\nLung-Sys also be able to locate the areas of relevant lesions, without any\npixel-level annotation. To further demonstrate the effectiveness of our model,\nwe conduct extensive experiments on a chest CT imaging dataset with a total of\n734 patients (251 healthy people, 245 COVID-19 patients, 105 H1N1 patients, and\n133 CAP patients). The quantitative results with plenty of metrics indicate the\nsuperiority of our proposed model on both slice- and patient-level\nclassification tasks. More importantly, the generated lesion location maps make\nour system interpretable and more valuable to clinicians.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 06:22:24 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Qian", "Xuelin", ""], ["Fu", "Huazhu", ""], ["Shi", "Weiya", ""], ["Chen", "Tao", ""], ["Fu", "Yanwei", ""], ["Shan", "Fei", ""], ["Xue", "Xiangyang", ""]]}, {"id": "2010.03203", "submitter": "Yan Yang", "authors": "Yan Yang, Md Zakir Hossain, Tom Gedeon, Shafin Rahman", "title": "RealSmileNet: A Deep End-To-End Network for Spontaneous and Posed Smile\n  Recognition", "comments": "Accepted by ACCV", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Smiles play a vital role in the understanding of social interactions within\ndifferent communities, and reveal the physical state of mind of people in both\nreal and deceptive ways. Several methods have been proposed to recognize\nspontaneous and posed smiles. All follow a feature-engineering based pipeline\nrequiring costly pre-processing steps such as manual annotation of face\nlandmarks, tracking, segmentation of smile phases, and hand-crafted features.\nThe resulting computation is expensive, and strongly dependent on\npre-processing steps. We investigate an end-to-end deep learning model to\naddress these problems, the first end-to-end model for spontaneous and posed\nsmile recognition. Our fully automated model is fast and learns the feature\nextraction processes by training a series of convolution and ConvLSTM layer\nfrom scratch. Our experiments on four datasets demonstrate the robustness and\ngeneralization of the proposed model by achieving state-of-the-art\nperformances.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 06:23:38 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Yang", "Yan", ""], ["Hossain", "Md Zakir", ""], ["Gedeon", "Tom", ""], ["Rahman", "Shafin", ""]]}, {"id": "2010.03213", "submitter": "Michael Lyons", "authors": "Michael J. Lyons, Michael Haehnel, Nobuji Tetsutani", "title": "Designing, Playing, and Performing with a Vision-based Mouth Interface", "comments": "Proceedings of the International Conference on New Interfaces for\n  Musical Expression, 2003", "journal-ref": null, "doi": "10.5281/zenodo.1176529", "report-no": null, "categories": "cs.HC cs.CV cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The role of the face and mouth in speech production as well asnon-verbal\ncommunication suggests the use of facial action tocontrol musical sound. Here\nwe document work on theMouthesizer, a system which uses a headworn\nminiaturecamera and computer vision algorithm to extract shapeparameters from\nthe mouth opening and output these as MIDIcontrol changes. We report our\nexperience with variousgesture-to-sound mappings and musical applications,\nanddescribe a live performance which used the Mouthesizerinterface.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 06:47:42 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Lyons", "Michael J.", ""], ["Haehnel", "Michael", ""], ["Tetsutani", "Nobuji", ""]]}, {"id": "2010.03223", "submitter": "Michael Lyons", "authors": "Mathias Funk, Kazuhiro Kuwabara, Michael J. Lyons", "title": "Sonification of Facial Actions for Musical Expression", "comments": "Proceedings of the International Conference on New Interfaces for\n  Musical Expression, 2005 (NIME-05)", "journal-ref": null, "doi": "10.5281/zenodo.1176749", "report-no": null, "categories": "cs.HC cs.CV cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The central role of the face in social interaction and non-verbal\ncommunication suggests we explore facial action as a means of musical\nexpression. This paper presents the design, implementation, and preliminary\nstudies of a novel system utilizing face detection and optic flow algorithms to\nassociate facial movements with sound synthesis in a topographically specific\nfashion. We report on our experience with various gesture-to-sound mappings and\napplications, and describe our preliminary experiments at musical performance\nusing the system.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 07:04:07 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Funk", "Mathias", ""], ["Kuwabara", "Kazuhiro", ""], ["Lyons", "Michael J.", ""]]}, {"id": "2010.03244", "submitter": "Suzanne Wetstein", "authors": "Suzanne C. Wetstein, Nikolas Stathonikos, Josien P.W. Pluim, Yujing J.\n  Heng, Natalie D. ter Hoeve, Celien P.H. Vreuls, Paul J. van Diest, Mitko Veta", "title": "Deep Learning-Based Grading of Ductal Carcinoma In Situ in Breast\n  Histopathology Images", "comments": null, "journal-ref": "Laboratory Investigation. Published February 19th, 2021", "doi": "10.1038/s41374-021-00540-6", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ductal carcinoma in situ (DCIS) is a non-invasive breast cancer that can\nprogress into invasive ductal carcinoma (IDC). Studies suggest DCIS is often\novertreated since a considerable part of DCIS lesions may never progress into\nIDC. Lower grade lesions have a lower progression speed and risk, possibly\nallowing treatment de-escalation. However, studies show significant\ninter-observer variation in DCIS grading. Automated image analysis may provide\nan objective solution to address high subjectivity of DCIS grading by\npathologists.\n  In this study, we developed a deep learning-based DCIS grading system. It was\ndeveloped using the consensus DCIS grade of three expert observers on a dataset\nof 1186 DCIS lesions from 59 patients. The inter-observer agreement, measured\nby quadratic weighted Cohen's kappa, was used to evaluate the system and\ncompare its performance to that of expert observers. We present an analysis of\nthe lesion-level and patient-level inter-observer agreement on an independent\ntest set of 1001 lesions from 50 patients.\n  The deep learning system (dl) achieved on average slightly higher\ninter-observer agreement to the observers (o1, o2 and o3)\n($\\kappa_{o1,dl}=0.81, \\kappa_{o2,dl}=0.53, \\kappa_{o3,dl}=0.40$) than the\nobservers amongst each other ($\\kappa_{o1,o2}=0.58, \\kappa_{o1,o3}=0.50,\n\\kappa_{o2,o3}=0.42$) at the lesion-level. At the patient-level, the deep\nlearning system achieved similar agreement to the observers\n($\\kappa_{o1,dl}=0.77, \\kappa_{o2,dl}=0.75, \\kappa_{o3,dl}=0.70$) as the\nobservers amongst each other ($\\kappa_{o1,o2}=0.77, \\kappa_{o1,o3}=0.75,\n\\kappa_{o2,o3}=0.72$).\n  In conclusion, we developed a deep learning-based DCIS grading system that\nachieved a performance similar to expert observers. We believe this is the\nfirst automated system that could assist pathologists by providing robust and\nreproducible second opinions on DCIS grade.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 07:56:55 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Wetstein", "Suzanne C.", ""], ["Stathonikos", "Nikolas", ""], ["Pluim", "Josien P. W.", ""], ["Heng", "Yujing J.", ""], ["ter Hoeve", "Natalie D.", ""], ["Vreuls", "Celien P. H.", ""], ["van Diest", "Paul J.", ""], ["Veta", "Mitko", ""]]}, {"id": "2010.03245", "submitter": "Jingyi Xu", "authors": "Jingyi Xu and Zhixin Shu and Dimitris Samaras", "title": "Learning Clusterable Visual Features for Zero-Shot Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In zero-shot learning (ZSL), conditional generators have been widely used to\ngenerate additional training features. These features can then be used to train\nthe classifiers for testing data. However, some testing data are considered\n\"hard\" as they lie close to the decision boundaries and are prone to\nmisclassification, leading to performance degradation for ZSL. In this paper,\nwe propose to learn clusterable features for ZSL problems. Using a Conditional\nVariational Autoencoder (CVAE) as the feature generator, we project the\noriginal features to a new feature space supervised by an auxiliary\nclassification loss. To further increase clusterability, we fine-tune the\nfeatures using Gaussian similarity loss. The clusterable visual features are\nnot only more suitable for CVAE reconstruction but are also more separable\nwhich improves classification accuracy. Moreover, we introduce Gaussian noise\nto enlarge the intra-class variance of the generated features, which helps to\nimprove the classifier's robustness. Our experiments on SUN,CUB, and AWA2\ndatasets show consistent improvement over previous state-of-the-art ZSL results\nby a large margin. In addition to its effectiveness on zero-shot\nclassification, experiments show that our method to increase feature\nclusterability benefits few-shot learning algorithms as well.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 07:58:55 GMT"}, {"version": "v2", "created": "Wed, 14 Oct 2020 17:34:34 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Xu", "Jingyi", ""], ["Shu", "Zhixin", ""], ["Samaras", "Dimitris", ""]]}, {"id": "2010.03255", "submitter": "Jingyi Xu", "authors": "Jingyi Xu and Mingzhen Huang and ShahRukh Athar and Dimitris Samaras", "title": "Variational Transfer Learning for Fine-grained Few-shot Visual\n  Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fine-grained few-shot recognition often suffers from the problem of training\ndata scarcity for novel categories.The network tends to overfit and does not\ngeneralize well to unseen classes due to insufficient training data. Many\nmethods have been proposed to synthesize additional data to support the\ntraining. In this paper, we focus one enlarging the intra-class variance of the\nunseen class to improve few-shot classification performance. We assume that the\ndistribution of intra-class variance generalizes across the base class and the\nnovel class. Thus, the intra-class variance of the base set can be transferred\nto the novel set for feature augmentation. Specifically, we first model the\ndistribution of intra-class variance on the base set via variational inference.\nThen the learned distribution is transferred to the novel set to generate\nadditional features, which are used together with the original ones to train a\nclassifier. Experimental results show a significant boost over the\nstate-of-the-art methods on the challenging fine-grained few-shot image\nclassification benchmarks.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 08:13:42 GMT"}, {"version": "v2", "created": "Mon, 12 Oct 2020 20:43:39 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Xu", "Jingyi", ""], ["Huang", "Mingzhen", ""], ["Athar", "ShahRukh", ""], ["Samaras", "Dimitris", ""]]}, {"id": "2010.03265", "submitter": "Michael Lyons", "authors": "Gamhewage C. de Silva, Tamara Smyth, Michael J. Lyons", "title": "A Novel Face-tracking Mouth Controller and its Application to\n  Interacting with Bioacoustic Models", "comments": "Proceedings of the International Conference on New Interfaces for\n  Musical Expression, 2004 (NIME-04)", "journal-ref": null, "doi": "10.5281/zenodo.1176666", "report-no": null, "categories": "cs.HC cs.CV cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We describe a simple, computationally light, real-time system for tracking\nthe lower face and extracting information about the shape of the open mouth\nfrom a video sequence. The system allows unencumbered control of audio\nsynthesis modules by the action of the mouth. We report work in progress to use\nthe mouth controller to interact with a physical model of sound production by\nthe avian syrinx.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 08:36:43 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["de Silva", "Gamhewage C.", ""], ["Smyth", "Tamara", ""], ["Lyons", "Michael J.", ""]]}, {"id": "2010.03266", "submitter": "Xingbo Liu", "authors": "Xiao Kang, Xingbo Liu, Xiushan Nie, Yilong Yin", "title": "Learning Binary Semantic Embedding for Histology Image Classification\n  and Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the development of medical imaging technology and machine learning,\ncomputer-assisted diagnosis which can provide impressive reference to\npathologists, attracts extensive research interests. The exponential growth of\nmedical images and uninterpretability of traditional classification models have\nhindered the applications of computer-assisted diagnosis. To address these\nissues, we propose a novel method for Learning Binary Semantic Embedding\n(LBSE). Based on the efficient and effective embedding, classification and\nretrieval are performed to provide interpretable computer-assisted diagnosis\nfor histology images. Furthermore, double supervision, bit uncorrelation and\nbalance constraint, asymmetric strategy and discrete optimization are\nseamlessly integrated in the proposed method for learning binary embedding.\nExperiments conducted on three benchmark datasets validate the superiority of\nLBSE under various scenarios.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 08:36:44 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Kang", "Xiao", ""], ["Liu", "Xingbo", ""], ["Nie", "Xiushan", ""], ["Yin", "Yilong", ""]]}, {"id": "2010.03268", "submitter": "Davood Zabihzadeh", "authors": "Davood Zabihzadeh, Amar Tuama, Ali Karami-Mollaee", "title": "Low-Rank Robust Online Distance/Similarity Learning based on the\n  Rescaled Hinge Loss", "comments": "An Online Distance-Similarity learning approach in noisy environment", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important challenge in metric learning is scalability to both size and\ndimension of input data. Online metric learning algorithms are proposed to\naddress this challenge. Existing methods are commonly based on (Passive\nAggressive) PA approach. Hence, they can rapidly process large volumes of data\nwith an adaptive learning rate. However, these algorithms are based on the\nHinge loss and so are not robust against outliers and label noise. Also,\nexisting online methods usually assume training triplets or pairwise\nconstraints are exist in advance. However, many datasets in real-world\napplications are in the form of input data and their associated labels. We\naddress these challenges by formulating the online Distance-Similarity learning\nproblem with the robust Rescaled hinge loss function. The proposed model is\nrather general and can be applied to any PA-based online Distance-Similarity\nalgorithm. Also, we develop an efficient robust one-pass triplet construction\nalgorithm. Finally, to provide scalability in high dimensional DML\nenvironments, the low-rank version of the proposed methods is presented that\nnot only reduces the computational cost significantly but also keeps the\npredictive performance of the learned metrics. Also, it provides a\nstraightforward extension of our methods for deep Distance-Similarity learning.\nWe conduct several experiments on datasets from various applications. The\nresults confirm that the proposed methods significantly outperform\nstate-of-the-art online DML methods in the presence of label noise and outliers\nby a large margin.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 08:38:34 GMT"}, {"version": "v2", "created": "Sat, 10 Oct 2020 09:30:36 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Zabihzadeh", "Davood", ""], ["Tuama", "Amar", ""], ["Karami-Mollaee", "Ali", ""]]}, {"id": "2010.03271", "submitter": "Xingbo Liu", "authors": "Xiao Kang, Xingbo Liu, Xiushan Nie, Xiaoming Xi, Yilong Yin", "title": "Attention Model Enhanced Network for Classification of Breast Cancer\n  Image", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Breast cancer classification remains a challenging task due to inter-class\nambiguity and intra-class variability. Existing deep learning-based methods try\nto confront this challenge by utilizing complex nonlinear projections. However,\nthese methods typically extract global features from entire images, neglecting\nthe fact that the subtle detail information can be crucial in extracting\ndiscriminative features. In this study, we propose a novel method named\nAttention Model Enhanced Network (AMEN), which is formulated in a multi-branch\nfashion with pixel-wised attention model and classification submodular.\nSpecifically, the feature learning part in AMEN can generate pixel-wised\nattention map, while the classification submodular are utilized to classify the\nsamples. To focus more on subtle detail information, the sample image is\nenhanced by the pixel-wised attention map generated from former branch.\nFurthermore, boosting strategy are adopted to fuse classification results from\ndifferent branches for better performance. Experiments conducted on three\nbenchmark datasets demonstrate the superiority of the proposed method under\nvarious scenarios.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 08:44:21 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Kang", "Xiao", ""], ["Liu", "Xingbo", ""], ["Nie", "Xiushan", ""], ["Xi", "Xiaoming", ""], ["Yin", "Yilong", ""]]}, {"id": "2010.03282", "submitter": "Ahmed Salem", "authors": "Ahmed Salem, Michael Backes, Yang Zhang", "title": "Don't Trigger Me! A Triggerless Backdoor Attack Against Deep Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Backdoor attack against deep neural networks is currently being profoundly\ninvestigated due to its severe security consequences. Current state-of-the-art\nbackdoor attacks require the adversary to modify the input, usually by adding a\ntrigger to it, for the target model to activate the backdoor. This added\ntrigger not only increases the difficulty of launching the backdoor attack in\nthe physical world, but also can be easily detected by multiple defense\nmechanisms. In this paper, we present the first triggerless backdoor attack\nagainst deep neural networks, where the adversary does not need to modify the\ninput for triggering the backdoor. Our attack is based on the dropout\ntechnique. Concretely, we associate a set of target neurons that are dropped\nout during model training with the target label. In the prediction phase, the\nmodel will output the target label when the target neurons are dropped again,\ni.e., the backdoor attack is launched. This triggerless feature of our attack\nmakes it practical in the physical world. Extensive experiments show that our\ntriggerless backdoor attack achieves a perfect attack success rate with a\nnegligible damage to the model's utility.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 09:01:39 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Salem", "Ahmed", ""], ["Backes", "Michael", ""], ["Zhang", "Yang", ""]]}, {"id": "2010.03288", "submitter": "Philipp Benz", "authors": "Philipp Benz, Chaoning Zhang, Tooba Imtiaz, In So Kweon", "title": "Double Targeted Universal Adversarial Perturbations", "comments": "Accepted at ACCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite their impressive performance, deep neural networks (DNNs) are widely\nknown to be vulnerable to adversarial attacks, which makes it challenging for\nthem to be deployed in security-sensitive applications, such as autonomous\ndriving. Image-dependent perturbations can fool a network for one specific\nimage, while universal adversarial perturbations are capable of fooling a\nnetwork for samples from all classes without selection. We introduce a double\ntargeted universal adversarial perturbations (DT-UAPs) to bridge the gap\nbetween the instance-discriminative image-dependent perturbations and the\ngeneric universal perturbations. This universal perturbation attacks one\ntargeted source class to sink class, while having a limited adversarial effect\non other non-targeted source classes, for avoiding raising suspicions.\nTargeting the source and sink class simultaneously, we term it double targeted\nattack (DTA). This provides an attacker with the freedom to perform precise\nattacks on a DNN model while raising little suspicion. We show the\neffectiveness of the proposed DTA algorithm on a wide range of datasets and\nalso demonstrate its potential as a physical attack.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 09:08:51 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Benz", "Philipp", ""], ["Zhang", "Chaoning", ""], ["Imtiaz", "Tooba", ""], ["Kweon", "In So", ""]]}, {"id": "2010.03300", "submitter": "Philipp Benz", "authors": "Chaoning Zhang, Philipp Benz, Tooba Imtiaz, In So Kweon", "title": "CD-UAP: Class Discriminative Universal Adversarial Perturbation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A single universal adversarial perturbation (UAP) can be added to all natural\nimages to change most of their predicted class labels. It is of high practical\nrelevance for an attacker to have flexible control over the targeted classes to\nbe attacked, however, the existing UAP method attacks samples from all classes.\nIn this work, we propose a new universal attack method to generate a single\nperturbation that fools a target network to misclassify only a chosen group of\nclasses, while having limited influence on the remaining classes. Since the\nproposed attack generates a universal adversarial perturbation that is\ndiscriminative to targeted and non-targeted classes, we term it class\ndiscriminative universal adversarial perturbation (CD-UAP). We propose one\nsimple yet effective algorithm framework, under which we design and compare\nvarious loss function configurations tailored for the class discriminative\nuniversal attack. The proposed approach has been evaluated with extensive\nexperiments on various benchmark datasets. Additionally, our proposed approach\nachieves state-of-the-art performance for the original task of UAP attacking\nall classes, which demonstrates the effectiveness of our approach.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 09:26:42 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Zhang", "Chaoning", ""], ["Benz", "Philipp", ""], ["Imtiaz", "Tooba", ""], ["Kweon", "In So", ""]]}, {"id": "2010.03316", "submitter": "Philipp Benz", "authors": "Philipp Benz, Chaoning Zhang, In So Kweon", "title": "Batch Normalization Increases Adversarial Vulnerability: Disentangling\n  Usefulness and Robustness of Model Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Batch normalization (BN) has been widely used in modern deep neural networks\n(DNNs) due to fast convergence. BN is observed to increase the model accuracy\nwhile at the cost of adversarial robustness. We conjecture that the increased\nadversarial vulnerability is caused by BN shifting the model to rely more on\nnon-robust features (NRFs). Our exploration finds that other normalization\ntechniques also increase adversarial vulnerability and our conjecture is also\nsupported by analyzing the model corruption robustness and feature\ntransferability. With a classifier DNN defined as a feature set $F$ we propose\na framework for disentangling $F$ robust usefulness into $F$ usefulness and $F$\nrobustness. We adopt a local linearity based metric, termed LIGS, to define and\nquantify $F$ robustness. Measuring the $F$ robustness with the LIGS provides\ndirect insight on the feature robustness shift independent of usefulness.\nMoreover, the LIGS trend during the whole training stage sheds light on the\norder of learned features, i.e. from RFs (robust features) to NRFs, or vice\nversa. Our work analyzes how BN and other factors influence the DNN from the\nfeature perspective. Prior works mainly adopt accuracy to evaluate their\ninfluence regarding $F$ usefulness, while we believe evaluating $F$ robustness\nis equally important, for which our work fills the gap.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 10:24:33 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Benz", "Philipp", ""], ["Zhang", "Chaoning", ""], ["Kweon", "In So", ""]]}, {"id": "2010.03318", "submitter": "Jaeyoo Park", "authors": "Seohyun Kim, Jaeyoo Park, Bohyung Han", "title": "Rotation-Invariant Local-to-Global Representation Learning for 3D Point\n  Cloud", "comments": "15 pages, Accepted by NeurIPS 2020 (camera-ready)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a local-to-global representation learning algorithm for 3D point\ncloud data, which is appropriate to handle various geometric transformations,\nespecially rotation, without explicit data augmentation with respect to the\ntransformations. Our model takes advantage of multi-level abstraction based on\ngraph convolutional neural networks, which constructs a descriptor hierarchy to\nencode rotation-invariant shape information of an input object in a bottom-up\nmanner. The descriptors in each level are obtained from a neural network based\non a graph via stochastic sampling of 3D points, which is effective in making\nthe learned representations robust to the variations of input data. The\nproposed algorithm presents the state-of-the-art performance on the\nrotation-augmented 3D object recognition and segmentation benchmarks, and we\nfurther analyze its characteristics through comprehensive ablative experiments.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 10:30:20 GMT"}, {"version": "v2", "created": "Thu, 8 Oct 2020 05:38:29 GMT"}, {"version": "v3", "created": "Fri, 23 Oct 2020 05:36:01 GMT"}, {"version": "v4", "created": "Wed, 31 Mar 2021 04:39:26 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Kim", "Seohyun", ""], ["Park", "Jaeyoo", ""], ["Han", "Bohyung", ""]]}, {"id": "2010.03320", "submitter": "Kamil Kowol", "authors": "Kamil Kowol, Matthias Rottmann, Stefan Bracke, Hanno Gottschalk", "title": "YOdar: Uncertainty-based Sensor Fusion for Vehicle Detection with Camera\n  and Radar Sensors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present an uncertainty-based method for sensor fusion with\ncamera and radar data. The outputs of two neural networks, one processing\ncamera and the other one radar data, are combined in an uncertainty aware\nmanner. To this end, we gather the outputs and corresponding meta information\nfor both networks. For each predicted object, the gathered information is\npost-processed by a gradient boosting method to produce a joint prediction of\nboth networks. In our experiments we combine the YOLOv3 object detection\nnetwork with a customized $1D$ radar segmentation network and evaluate our\nmethod on the nuScenes dataset. In particular we focus on night scenes, where\nthe capability of object detection networks based on camera data is potentially\nhandicapped. Our experiments show, that this approach of uncertainty aware\nfusion, which is also of very modular nature, significantly gains performance\ncompared to single sensor baselines and is in range of specifically tailored\ndeep learning based fusion approaches.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 10:40:02 GMT"}, {"version": "v2", "created": "Mon, 23 Nov 2020 18:47:59 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Kowol", "Kamil", ""], ["Rottmann", "Matthias", ""], ["Bracke", "Stefan", ""], ["Gottschalk", "Hanno", ""]]}, {"id": "2010.03325", "submitter": "Fangbo Qin", "authors": "Fangbo Qin, Jie Qin, Siyu Huang, De Xu", "title": "Contour Primitive of Interest Extraction Network Based on One-Shot\n  Learning for Object-Agnostic Vision Measurement", "comments": "Accepted by IEEE ICRA 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image contour based vision measurement is widely applied in robot\nmanipulation and industrial automation. It is appealing to realize\nobject-agnostic vision system, which can be conveniently reused for various\ntypes of objects. We propose the contour primitive of interest extraction\nnetwork (CPieNet) based on the one-shot learning framework. First, CPieNet is\nfeatured by that its contour primitive of interest (CPI) output, a designated\nregular contour part lying on a specified object, provides the essential\ngeometric information for vision measurement. Second, CPieNet has the one-shot\nlearning ability, utilizing a support sample to assist the perception of the\nnovel object. To realize lower-cost training, we generate support-query sample\npairs from unpaired online public images, which cover a wide range of object\ncategories. To obtain single-pixel wide contour for precise measurement, the\nGabor-filters based non-maximum suppression is designed to thin the raw\ncontour. For the novel CPI extraction task, we built the Object Contour\nPrimitives dataset using online public images, and the Robotic Object Contour\nMeasurement dataset using a camera mounted on a robot. The effectiveness of the\nproposed methods is validated by a series of experiments.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 11:00:30 GMT"}, {"version": "v2", "created": "Thu, 25 Mar 2021 02:23:39 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Qin", "Fangbo", ""], ["Qin", "Jie", ""], ["Huang", "Siyu", ""], ["Xu", "De", ""]]}, {"id": "2010.03331", "submitter": "Roberto Arroyo", "authors": "Roberto Arroyo, David Jim\\'enez-Cabello and Javier\n  Mart\\'inez-Cebri\\'an", "title": "Multi-label classification of promotions in digital leaflets using\n  textual and visual information", "comments": "Conference on Computational Linguistics (COLING). Workshop on Natural\n  Language Processing in E-Commerce (EcomNLP 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Product descriptions in e-commerce platforms contain detailed and valuable\ninformation about retailers assortment. In particular, coding promotions within\ndigital leaflets are of great interest in e-commerce as they capture the\nattention of consumers by showing regular promotions for different products.\nHowever, this information is embedded into images, making it difficult to\nextract and process for downstream tasks. In this paper, we present an\nend-to-end approach that classifies promotions within digital leaflets into\ntheir corresponding product categories using both visual and textual\ninformation. Our approach can be divided into three key components: 1) region\ndetection, 2) text recognition and 3) text classification. In many cases, a\nsingle promotion refers to multiple product categories, so we introduce a\nmulti-label objective in the classification head. We demonstrate the\neffectiveness of our approach for two separated tasks: 1) image-based detection\nof the descriptions for each individual promotion and 2) multi-label\nclassification of the product categories using the text from the product\ndescriptions. We train and evaluate our models using a private dataset composed\nof images from digital leaflets obtained by Nielsen. Results show that we\nconsistently outperform the proposed baseline by a large margin in all the\nexperiments.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 11:05:12 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Arroyo", "Roberto", ""], ["Jim\u00e9nez-Cabello", "David", ""], ["Mart\u00ednez-Cebri\u00e1n", "Javier", ""]]}, {"id": "2010.03341", "submitter": "Moi Hoon Yap", "authors": "Moi Hoon Yap and Ryo Hachiuma and Azadeh Alavi and Raphael Brungel and\n  Bill Cassidy and Manu Goyal and Hongtao Zhu and Johannes Ruckert and Moshe\n  Olshansky and Xiao Huang and Hideo Saito and Saeed Hassanpour and Christoph\n  M. Friedrich and David Ascher and Anping Song and Hiroki Kajita and David\n  Gillespie and Neil D. Reeves and Joseph Pappachan and Claire O'Shea and Eibe\n  Frank", "title": "Deep Learning in Diabetic Foot Ulcers Detection: A Comprehensive\n  Evaluation", "comments": "19 pages, 18 figures, 10 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been a substantial amount of research involving computer methods\nand technology for the detection and recognition of diabetic foot ulcers\n(DFUs), but there is a lack of systematic comparisons of state-of-the-art deep\nlearning object detection frameworks applied to this problem. DFUC2020 provided\nparticipants with a comprehensive dataset consisting of 2,000 images for\ntraining and 2,000 images for testing. This paper summarises the results of\nDFUC2020 by comparing the deep learning-based algorithms proposed by the\nwinning teams: Faster R-CNN, three variants of Faster R-CNN and an ensemble\nmethod; YOLOv3; YOLOv5; EfficientDet; and a new Cascade Attention Network. For\neach deep learning method, we provide a detailed description of model\narchitecture, parameter settings for training and additional stages including\npre-processing, data augmentation and post-processing. We provide a\ncomprehensive evaluation for each method. All the methods required a data\naugmentation stage to increase the number of images available for training and\na post-processing stage to remove false positives. The best performance was\nobtained from Deformable Convolution, a variant of Faster R-CNN, with a mean\naverage precision (mAP) of 0.6940 and an F1-Score of 0.7434. Finally, we\ndemonstrate that the ensemble method based on different deep learning methods\ncan enhanced the F1-Score but not the mAP.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 11:31:27 GMT"}, {"version": "v2", "created": "Thu, 15 Oct 2020 07:29:10 GMT"}, {"version": "v3", "created": "Mon, 24 May 2021 12:46:50 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Yap", "Moi Hoon", ""], ["Hachiuma", "Ryo", ""], ["Alavi", "Azadeh", ""], ["Brungel", "Raphael", ""], ["Cassidy", "Bill", ""], ["Goyal", "Manu", ""], ["Zhu", "Hongtao", ""], ["Ruckert", "Johannes", ""], ["Olshansky", "Moshe", ""], ["Huang", "Xiao", ""], ["Saito", "Hideo", ""], ["Hassanpour", "Saeed", ""], ["Friedrich", "Christoph M.", ""], ["Ascher", "David", ""], ["Song", "Anping", ""], ["Kajita", "Hiroki", ""], ["Gillespie", "David", ""], ["Reeves", "Neil D.", ""], ["Pappachan", "Joseph", ""], ["O'Shea", "Claire", ""], ["Frank", "Eibe", ""]]}, {"id": "2010.03367", "submitter": "Shadi AlZu'bi", "authors": "Shadi Al-Zu'bi", "title": "Secure 3D medical Imaging", "comments": "24 Pages, 4 Tables, 6 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image segmentation has proved its importance and plays an important role in\nvarious domains such as health systems and satellite-oriented military\napplications. In this context, accuracy, image quality, and execution time deem\nto be the major issues to always consider. Although many techniques have been\napplied, and their experimental results have shown appealing achievements for\n2D images in real-time environments, however, there is a lack of works about 3D\nimage segmentation despite its importance in improving segmentation accuracy.\nSpecifically, HMM was used in this domain. However, it suffers from the time\ncomplexity, which was updated using different accelerators. As it is important\nto have efficient 3D image segmentation, we propose in this paper a novel\nsystem for partitioning the 3D segmentation process across several distributed\nmachines. The concepts behind distributed multi-media network segmentation were\nemployed to accelerate the segmentation computational time of training Hidden\nMarkov Model (HMMs). Furthermore, a secure transmission has been considered in\nthis distributed environment and various bidirectional multimedia security\nalgorithms have been applied. The contribution of this work lies in providing\nan efficient and secure algorithm for 3D image segmentation. Through a number\nof extensive experiments, it was proved that our proposed system is of\ncomparable efficiency to the state of art methods in terms of segmentation\naccuracy, security and execution time.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 07:21:11 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Al-Zu'bi", "Shadi", ""]]}, {"id": "2010.03370", "submitter": "Haosu Zhou", "authors": "Haosu Zhou, Qingfeng Xu, Nan Li", "title": "A study on using image based machine learning methods to develop the\n  surrogate models of stamp forming simulations", "comments": "16 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the design optimization of metal forming, it is increasingly significant\nto use surrogate models to analyse the finite element analysis (FEA)\nsimulations. However, traditional surrogate models using scalar based machine\nlearning methods (SBMLMs) fall in short of accuracy and generalizability. This\nis because SBMLMs fail to harness the location information of the simulations.\nTo overcome these shortcomings, image based machine learning methods (IBMLMs)\nare leveraged in this paper. The underlying theory of location information,\nwhich supports the advantages of IBMLM, is qualitatively interpreted. Based on\nthis theory, a Res-SE-U-Net IBMLM surrogate model is developed and compared\nwith a multi-layer perceptron (MLP) as a referencing SBMLM surrogate model. It\nis demonstrated that the IBMLM model is advantageous over the MLP SBMLM model\nin accuracy, generalizability, robustness, and informativeness. This paper\npresents a promising methodology of leveraging IBMLMs in surrogate models to\nmake maximum use of info from FEA results. Future prospective studies that\ninspired by this paper are also discussed.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2020 22:46:56 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Zhou", "Haosu", ""], ["Xu", "Qingfeng", ""], ["Li", "Nan", ""]]}, {"id": "2010.03378", "submitter": "Manish Joshi", "authors": "Aparna Bhale, Manish Joshi", "title": "Descriptive analysis of computational methods for automating mammograms\n  with practical applications", "comments": "33 pages and 2 Figures. A review paper of the research work related\n  to mamography", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mammography is a vital screening technique for early revealing and\nidentification of breast cancer in order to assist to decrease mortality rate.\nPractical applications of mammograms are not limited to breast cancer\nrevealing, identification ,but include task based lens design, image\ncompression, image classification, content based image retrieval and a host of\nothers. Mammography computational analysis methods are a useful tool for\nspecialists to reveal hidden features and extract significant information in\nmammograms. Digital mammograms are mammography images available along with the\nconventional screen-film mammography to make automation of mammograms easier.\nIn this paper, we descriptively discuss computational advancement in digital\nmammograms to serve as a compass for research and practice in the domain of\ncomputational mammography and related fields. The discussion focuses on\nresearch aiming at a variety of applications and automations of mammograms. It\ncovers different perspectives on image pre-processing, feature extraction,\napplication of mammograms, screen-film mammogram, digital mammogram and\ndevelopment of benchmark corpora for experimenting with digital mammograms.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 05:40:26 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Bhale", "Aparna", ""], ["Joshi", "Manish", ""]]}, {"id": "2010.03396", "submitter": "Hristina Uzunova", "authors": "Hristina Uzunova, Jan Ehrhardt, Heinz Handels", "title": "Memory-efficient GAN-based Domain Translation of High Resolution 3D\n  Medical Images", "comments": "Accepted for Computerized Medical Imaging and Graphics", "journal-ref": null, "doi": "10.1016/j.compmedimag.2020.101801", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative adversarial networks (GANs) are currently rarely applied on 3D\nmedical images of large size, due to their immense computational demand. The\npresent work proposes a multi-scale patch-based GAN approach for establishing\nunpaired domain translation by generating 3D medical image volumes of high\nresolution in a memory-efficient way. The key idea to enable memory-efficient\nimage generation is to first generate a low-resolution version of the image\nfollowed by the generation of patches of constant sizes but successively\ngrowing resolutions. To avoid patch artifacts and incorporate global\ninformation, the patch generation is conditioned on patches from previous\nresolution scales. Those multi-scale GANs are trained to generate realistically\nlooking images from image sketches in order to perform an unpaired domain\ntranslation. This allows to preserve the topology of the test data and generate\nthe appearance of the training domain data. The evaluation of the domain\ntranslation scenarios is performed on brain MRIs of size 155x240x240 and thorax\nCTs of size up to 512x512x512. Compared to common patch-based approaches, the\nmulti-resolution scheme enables better image quality and prevents patch\nartifacts. Also, it ensures constant GPU memory demand independent from the\nimage size, allowing for the generation of arbitrarily large images.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 08:43:27 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Uzunova", "Hristina", ""], ["Ehrhardt", "Jan", ""], ["Handels", "Heinz", ""]]}, {"id": "2010.03403", "submitter": "Jiwei Wei", "authors": "Jiwei Wei, Xing Xu, Yang Yang, Yanli Ji, Zheng Wang, Heng Tao Shen", "title": "Universal Weighting Metric Learning for Cross-Modal Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-modal matching has been a highlighted research topic in both vision and\nlanguage areas. Learning appropriate mining strategy to sample and weight\ninformative pairs is crucial for the cross-modal matching performance. However,\nmost existing metric learning methods are developed for unimodal matching,\nwhich is unsuitable for cross-modal matching on multimodal data with\nheterogeneous features. To address this problem, we propose a simple and\ninterpretable universal weighting framework for cross-modal matching, which\nprovides a tool to analyze the interpretability of various loss functions.\nFurthermore, we introduce a new polynomial loss under the universal weighting\nframework, which defines a weight function for the positive and negative\ninformative pairs respectively. Experimental results on two image-text matching\nbenchmarks and two video-text matching benchmarks validate the efficacy of the\nproposed method.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 13:16:45 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Wei", "Jiwei", ""], ["Xu", "Xing", ""], ["Yang", "Yang", ""], ["Ji", "Yanli", ""], ["Wang", "Zheng", ""], ["Shen", "Heng Tao", ""]]}, {"id": "2010.03420", "submitter": "Charles Saah", "authors": "Ferdinand Kartriku, Dr. Robert Sowah and Charles Saah", "title": "Deep Neural Network: An Efficient and Optimized Machine Learning\n  Paradigm for Reducing Genome Sequencing Error", "comments": "for associated mpeg file, see https://ijettjournal.org/", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.GN cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Genomic data I used in many fields but, it has become known that most of the\nplatforms used in the sequencing process produce significant errors. This means\nthat the analysis and inferences generated from these data may have some errors\nthat need to be corrected. On the two main types of genome errors -\nsubstitution and indels - our work is focused on correcting indels. A deep\nlearning approach was used to correct the errors in sequencing the chosen\ndataset\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 08:16:35 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Kartriku", "Ferdinand", ""], ["Sowah", "Dr. Robert", ""], ["Saah", "Charles", ""]]}, {"id": "2010.03449", "submitter": "Thai Son Nguyen", "authors": "Thai-Son Nguyen, Sebastian Stueker, Alex Waibel", "title": "Super-Human Performance in Online Low-latency Recognition of\n  Conversational Speech", "comments": "To appear in Interspeech 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Achieving super-human performance in recognizing human speech has been a goal\nfor several decades, as researchers have worked on increasingly challenging\ntasks. In the 1990's it was discovered, that conversational speech between two\nhumans turns out to be considerably more difficult than read speech as\nhesitations, disfluencies, false starts and sloppy articulation complicate\nacoustic processing and require robust handling of acoustic, lexical and\nlanguage context, jointly. Early attempts with statistical models could only\nreach error rates over 50% and far from human performance (WER of around 5.5%).\nNeural hybrid models and recent attention-based encoder-decoder models have\nconsiderably improved performance as such contexts can now be learned in an\nintegral fashion. However, processing such contexts requires an entire\nutterance presentation and thus introduces unwanted delays before a recognition\nresult can be output. In this paper, we address performance as well as latency.\nWe present results for a system that can achieve super-human performance (at a\nWER of 5.0%, over the Switchboard conversational benchmark) at a word based\nlatency of only 1 second behind a speaker's speech. The system uses multiple\nattention-based encoder-decoder networks integrated within a novel low latency\nincremental inference approach.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 14:41:32 GMT"}, {"version": "v2", "created": "Thu, 22 Oct 2020 15:10:57 GMT"}, {"version": "v3", "created": "Wed, 10 Feb 2021 20:16:58 GMT"}, {"version": "v4", "created": "Tue, 8 Jun 2021 14:47:11 GMT"}, {"version": "v5", "created": "Mon, 26 Jul 2021 20:56:49 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Nguyen", "Thai-Son", ""], ["Stueker", "Sebastian", ""], ["Waibel", "Alex", ""]]}, {"id": "2010.03459", "submitter": "Benoit Gaujac", "authors": "Benoit Gaujac and Ilya Feige and David Barber", "title": "Learning disentangled representations with the Wasserstein Autoencoder", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Disentangled representation learning has undoubtedly benefited from objective\nfunction surgery. However, a delicate balancing act of tuning is still required\nin order to trade off reconstruction fidelity versus disentanglement. Building\non previous successes of penalizing the total correlation in the latent\nvariables, we propose TCWAE (Total Correlation Wasserstein Autoencoder).\nWorking in the WAE paradigm naturally enables the separation of the\ntotal-correlation term, thus providing disentanglement control over the learned\nrepresentation, while offering more flexibility in the choice of reconstruction\ncost. We propose two variants using different KL estimators and perform\nextensive quantitative comparisons on data sets with known generative factors,\nshowing competitive results relative to state-of-the-art techniques. We further\nstudy the trade off between disentanglement and reconstruction on\nmore-difficult data sets with unknown generative factors, where the flexibility\nof the WAE paradigm in the reconstruction term improves reconstructions.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 14:52:06 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Gaujac", "Benoit", ""], ["Feige", "Ilya", ""], ["Barber", "David", ""]]}, {"id": "2010.03467", "submitter": "Benoit Gaujac", "authors": "Benoit Gaujac and Ilya Feige and David Barber", "title": "Learning Deep-Latent Hierarchies by Stacking Wasserstein Autoencoders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic models with hierarchical-latent-variable structures provide\nstate-of-the-art results amongst non-autoregressive, unsupervised density-based\nmodels. However, the most common approach to training such models based on\nVariational Autoencoders (VAEs) often fails to leverage deep-latent\nhierarchies; successful approaches require complex inference and optimisation\nschemes. Optimal Transport is an alternative, non-likelihood-based framework\nfor training generative models with appealing theoretical properties, in\nprinciple allowing easier training convergence between distributions. In this\nwork we propose a novel approach to training models with deep-latent\nhierarchies based on Optimal Transport, without the need for highly bespoke\nmodels and inference networks. We show that our method enables the generative\nmodel to fully leverage its deep-latent hierarchy, avoiding the well known\n\"latent variable collapse\" issue of VAEs; therefore, providing qualitatively\nbetter sample generations as well as more interpretable latent representation\nthan the original Wasserstein Autoencoder with Maximum Mean Discrepancy\ndivergence.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 15:04:20 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Gaujac", "Benoit", ""], ["Feige", "Ilya", ""], ["Barber", "David", ""]]}, {"id": "2010.03468", "submitter": "Yue Yang", "authors": "Yue Yang, Pengtao Xie", "title": "Discriminative Cross-Modal Data Augmentation for Medical Imaging\n  Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While deep learning methods have shown great success in medical image\nanalysis, they require a number of medical images to train. Due to data privacy\nconcerns and unavailability of medical annotators, it is oftentimes very\ndifficult to obtain a lot of labeled medical images for model training. In this\npaper, we study cross-modality data augmentation to mitigate the data\ndeficiency issue in the medical imaging domain. We propose a discriminative\nunpaired image-to-image translation model which translates images in source\nmodality into images in target modality where the translation task is conducted\njointly with the downstream prediction task and the translation is guided by\nthe prediction. Experiments on two applications demonstrate the effectiveness\nof our method.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 15:07:00 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Yang", "Yue", ""], ["Xie", "Pengtao", ""]]}, {"id": "2010.03497", "submitter": "Daniel Deniz", "authors": "Daniel Deniz, Francisco Barranco, Juan Isern and Eduardo Ros", "title": "Reconfigurable Cyber-Physical System for Lifestyle Video-Monitoring via\n  Deep Learning", "comments": null, "journal-ref": "2020 25th IEEE International Conference on Emerging Technologies\n  and Factory Automation (ETFA), Vol. 1. IEEE, 1705-1712", "doi": "10.1109/ETFA46521.2020.9211910", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Indoor monitoring of people at their homes has become a popular application\nin Smart Health. With the advances in Machine Learning and hardware for\nembedded devices, new distributed approaches for Cyber-Physical Systems (CPSs)\nare enabled. Also, changing environments and need for cost reduction motivate\nnovel reconfigurable CPS architectures. In this work, we propose an indoor\nmonitoring reconfigurable CPS that uses embedded local nodes (Nvidia Jetson\nTX2). We embed Deep Learning architectures to address Human Action Recognition.\nLocal processing at these nodes let us tackle some common issues: reduction of\ndata bandwidth usage and preservation of privacy (no raw images are\ntransmitted). Also real-time processing is facilitated since optimized nodes\ncompute only its local video feed. Regarding the reconfiguration, a remote\nplatform monitors CPS qualities and a Quality and Resource Management (QRM)\ntool sends commands to the CPS core to trigger its reconfiguration. Our\nproposal is an energy-aware system that triggers reconfiguration based on\nenergy consumption for battery-powered nodes. Reconfiguration reduces up to 22%\nthe local nodes energy consumption extending the device operating time,\npreserving similar accuracy with respect to the alternative with no\nreconfiguration.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 16:05:09 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Deniz", "Daniel", ""], ["Barranco", "Francisco", ""], ["Isern", "Juan", ""], ["Ros", "Eduardo", ""]]}, {"id": "2010.03506", "submitter": "Lukas Koestler", "authors": "L. Koestler and N. Yang and R. Wang and D. Cremers", "title": "Learning Monocular 3D Vehicle Detection without 3D Bounding Box Labels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The training of deep-learning-based 3D object detectors requires large\ndatasets with 3D bounding box labels for supervision that have to be generated\nby hand-labeling. We propose a network architecture and training procedure for\nlearning monocular 3D object detection without 3D bounding box labels. By\nrepresenting the objects as triangular meshes and employing differentiable\nshape rendering, we define loss functions based on depth maps, segmentation\nmasks, and ego- and object-motion, which are generated by pre-trained,\noff-the-shelf networks. We evaluate the proposed algorithm on the real-world\nKITTI dataset and achieve promising performance in comparison to\nstate-of-the-art methods requiring 3D bounding box labels for training and\nsuperior performance to conventional baseline methods.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 16:24:46 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Koestler", "L.", ""], ["Yang", "N.", ""], ["Wang", "R.", ""], ["Cremers", "D.", ""]]}, {"id": "2010.03523", "submitter": "Divya Kothandaraman", "authors": "Divya Kothandaraman, Rohan Chandra, Dinesh Manocha", "title": "BoMuDANet: Unsupervised Adaptation for Visual Scene Understanding in\n  Unstructured Driving Environments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an unsupervised adaptation approach for visual scene understanding\nin unstructured traffic environments. Our method is designed for unstructured\nreal-world scenarios with dense and heterogeneous traffic consisting of cars,\ntrucks, two-and three-wheelers, and pedestrians. We describe a new semantic\nsegmentation technique based on unsupervised domain adaptation (DA), that can\nidentify the class or category of each region in RGB images or videos. We also\npresent a novel self-training algorithm (Alt-Inc) for multi-source DA that\nimproves the accuracy. Our overall approach is a deep learning-based technique\nand consists of an unsupervised neural network that achieves 87.18% accuracy on\nthe challenging India Driving Dataset. Our method works well on roads that may\nnot be well-marked or may include dirt, unidentifiable debris, potholes, etc. A\nkey aspect of our approach is that it can also identify objects that are\nencountered by the model for the fist time during the testing phase. We compare\nour method against the state-of-the-art methods and show an improvement of\n5.17% - 42.9%. Furthermore, we also conduct user studies that qualitatively\nvalidate the improvements in visual scene understanding of unstructured driving\nenvironments.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2020 08:25:44 GMT"}, {"version": "v2", "created": "Tue, 13 Oct 2020 06:19:25 GMT"}, {"version": "v3", "created": "Sun, 23 May 2021 15:27:04 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Kothandaraman", "Divya", ""], ["Chandra", "Rohan", ""], ["Manocha", "Dinesh", ""]]}, {"id": "2010.03533", "submitter": "Utku Evci", "authors": "Utku Evci, Yani A. Ioannou, Cem Keskin, Yann Dauphin", "title": "Gradient Flow in Sparse Neural Networks and How Lottery Tickets Win", "comments": "sparse training, sparsity, pruning, lottery ticket hypothesis,\n  lottery tickets, sparse initialization, initialization, deep learning,\n  gradient flow", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse Neural Networks (NNs) can match the generalization of dense NNs using\na fraction of the compute/storage for inference, and also have the potential to\nenable efficient training. However, naively training unstructured sparse NNs\nfrom random initialization results in significantly worse generalization, with\nthe notable exception of Lottery Tickets (LTs) and Dynamic Sparse Training\n(DST). In this work, we attempt to answer: (1) why training unstructured sparse\nnetworks from random initialization performs poorly and; (2) what makes LTs and\nDST the exceptions? We show that sparse NNs have poor gradient flow at\ninitialization and propose a modified initialization for unstructured\nconnectivity. Furthermore, we find that DST methods significantly improve\ngradient flow during training over traditional sparse training methods.\nFinally, we show that LTs do not improve gradient flow, rather their success\nlies in re-learning the pruning solution they are derived from - however, this\ncomes at the cost of learning novel solutions.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 17:26:08 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Evci", "Utku", ""], ["Ioannou", "Yani A.", ""], ["Keskin", "Cem", ""], ["Dauphin", "Yann", ""]]}, {"id": "2010.03549", "submitter": "Amirsina Torfi", "authors": "Amirsina Torfi, Mohammadreza Beyki, Edward A. Fox", "title": "On the Evaluation of Generative Adversarial Networks By Discriminative\n  Models", "comments": "Accepted to be published in ICPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Networks (GANs) can accurately model complex\nmulti-dimensional data and generate realistic samples. However, due to their\nimplicit estimation of data distributions, their evaluation is a challenging\ntask. The majority of research efforts associated with tackling this issue were\nvalidated by qualitative visual evaluation. Such approaches do not generalize\nwell beyond the image domain. Since many of those evaluation metrics are\nproposed and bound to the vision domain, they are difficult to apply to other\ndomains. Quantitative measures are necessary to better guide the training and\ncomparison of different GANs models. In this work, we leverage Siamese neural\nnetworks to propose a domain-agnostic evaluation metric: (1) with a qualitative\nevaluation that is consistent with human evaluation, (2) that is robust\nrelative to common GAN issues such as mode dropping and invention, and (3) does\nnot require any pretrained classifier. The empirical results in this paper\ndemonstrate the superiority of this method compared to the popular Inception\nScore and are competitive with the FID score.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 17:50:39 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Torfi", "Amirsina", ""], ["Beyki", "Mohammadreza", ""], ["Fox", "Edward A.", ""]]}, {"id": "2010.03558", "submitter": "Adrian Bulat", "authors": "Adrian Bulat and Brais Martinez and Georgios Tzimiropoulos", "title": "High-Capacity Expert Binary Networks", "comments": "Accepted at ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network binarization is a promising hardware-aware direction for creating\nefficient deep models. Despite its memory and computational advantages,\nreducing the accuracy gap between binary models and their real-valued\ncounterparts remains an unsolved challenging research problem. To this end, we\nmake the following 3 contributions: (a) To increase model capacity, we propose\nExpert Binary Convolution, which, for the first time, tailors conditional\ncomputing to binary networks by learning to select one data-specific expert\nbinary filter at a time conditioned on input features. (b) To increase\nrepresentation capacity, we propose to address the inherent information\nbottleneck in binary networks by introducing an efficient width expansion\nmechanism which keeps the binary operations within the same budget. (c) To\nimprove network design, we propose a principled binary network growth mechanism\nthat unveils a set of network topologies of favorable properties. Overall, our\nmethod improves upon prior work, with no increase in computational cost, by\n$\\sim6 \\%$, reaching a groundbreaking $\\sim 71\\%$ on ImageNet classification.\nCode will be made available\n$\\href{https://www.adrianbulat.com/binary-networks}{here}$.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 17:58:10 GMT"}, {"version": "v2", "created": "Tue, 30 Mar 2021 18:16:16 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Bulat", "Adrian", ""], ["Martinez", "Brais", ""], ["Tzimiropoulos", "Georgios", ""]]}, {"id": "2010.03592", "submitter": "Jonathan Barron", "authors": "Jonathan T. Barron, Jitendra Malik", "title": "Shape, Illumination, and Reflectance from Shading", "comments": null, "journal-ref": "TPAMI 2015", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A fundamental problem in computer vision is that of inferring the intrinsic,\n3D structure of the world from flat, 2D images of that world. Traditional\nmethods for recovering scene properties such as shape, reflectance, or\nillumination rely on multiple observations of the same scene to overconstrain\nthe problem. Recovering these same properties from a single image seems almost\nimpossible in comparison -- there are an infinite number of shapes, paint, and\nlights that exactly reproduce a single image. However, certain explanations are\nmore likely than others: surfaces tend to be smooth, paint tends to be uniform,\nand illumination tends to be natural. We therefore pose this problem as one of\nstatistical inference, and define an optimization problem that searches for the\n*most likely* explanation of a single image. Our technique can be viewed as a\nsuperset of several classic computer vision problems (shape-from-shading,\nintrinsic images, color constancy, illumination estimation, etc) and\noutperforms all previous solutions to those constituent problems.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 18:14:41 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Barron", "Jonathan T.", ""], ["Malik", "Jitendra", ""]]}, {"id": "2010.03624", "submitter": "Joshua Engelsma", "authors": "Joshua J. Engelsma, Debayan Deb, Kai Cao, Anjoo Bhatnagar, Prem S.\n  Sudhish and Anil K. Jain", "title": "Infant-ID: Fingerprints for Global Good", "comments": "16 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many of the least developed and developing countries, a multitude of\ninfants continue to suffer and die from vaccine-preventable diseases and\nmalnutrition. Lamentably, the lack of official identification documentation\nmakes it exceedingly difficult to track which infants have been vaccinated and\nwhich infants have received nutritional supplements. Answering these questions\ncould prevent this infant suffering and premature death around the world. To\nthat end, we propose Infant-Prints, an end-to-end, low-cost, infant fingerprint\nrecognition system. Infant-Prints is comprised of our (i) custom built,\ncompact, low-cost (85 USD), high-resolution (1,900 ppi), ergonomic fingerprint\nreader, and (ii) high-resolution infant fingerprint matcher. To evaluate the\nefficacy of Infant-Prints, we collected a longitudinal infant fingerprint\ndatabase captured in 4 different sessions over a 12-month time span (December\n2018 to January 2020), from 315 infants at the Saran Ashram Hospital, a\ncharitable hospital in Dayalbagh, Agra, India. Our experimental results\ndemonstrate, for the first time, that Infant-Prints can deliver accurate and\nreliable recognition (over time) of infants enrolled between the ages of 2-3\nmonths, in time for effective delivery of vaccinations, healthcare, and\nnutritional supplements (TAR=95.2% @ FAR = 1.0% for infants aged 8-16 weeks at\nenrollment and authenticated 3 months later).\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 19:51:52 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Engelsma", "Joshua J.", ""], ["Deb", "Debayan", ""], ["Cao", "Kai", ""], ["Bhatnagar", "Anjoo", ""], ["Sudhish", "Prem S.", ""], ["Jain", "Anil K.", ""]]}, {"id": "2010.03630", "submitter": "Philipp Benz", "authors": "Philipp Benz, Chaoning Zhang, Adil Karjauv, In So Kweon", "title": "Revisiting Batch Normalization for Improving Corruption Robustness", "comments": "Accepted at WACV 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance of DNNs trained on clean images has been shown to decrease\nwhen the test images have common corruptions. In this work, we interpret\ncorruption robustness as a domain shift and propose to rectify batch\nnormalization (BN) statistics for improving model robustness. This is motivated\nby perceiving the shift from the clean domain to the corruption domain as a\nstyle shift that is represented by the BN statistics. We find that simply\nestimating and adapting the BN statistics on a few (32 for instance)\nrepresentation samples, without retraining the model, improves the corruption\nrobustness by a large margin on several benchmark datasets with a wide range of\nmodel architectures. For example, on ImageNet-C, statistics adaptation improves\nthe top1 accuracy of ResNet50 from 39.2% to 48.7%. Moreover, we find that this\ntechnique can further improve state-of-the-art robust models from 58.1% to\n63.3%.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 19:56:47 GMT"}, {"version": "v2", "created": "Mon, 9 Nov 2020 14:34:56 GMT"}, {"version": "v3", "created": "Thu, 31 Dec 2020 03:59:07 GMT"}, {"version": "v4", "created": "Thu, 28 Jan 2021 08:35:52 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Benz", "Philipp", ""], ["Zhang", "Chaoning", ""], ["Karjauv", "Adil", ""], ["Kweon", "In So", ""]]}, {"id": "2010.03639", "submitter": "Fabian Balsiger", "authors": "Alain Jungo, Olivier Scheidegger, Mauricio Reyes, Fabian Balsiger", "title": "pymia: A Python package for data handling and evaluation in deep\n  learning-based medical image analysis", "comments": "first and last author contributed equally", "journal-ref": "Computer Methods and Programs in Biomedicine (2021), 198, 105796", "doi": "10.1016/j.cmpb.2020.105796", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background and Objective: Deep learning enables tremendous progress in\nmedical image analysis. One driving force of this progress are open-source\nframeworks like TensorFlow and PyTorch. However, these frameworks rarely\naddress issues specific to the domain of medical image analysis, such as 3-D\ndata handling and distance metrics for evaluation. pymia, an open-source Python\npackage, tries to address these issues by providing flexible data handling and\nevaluation independent of the deep learning framework.\n  Methods: The pymia package provides data handling and evaluation\nfunctionalities. The data handling allows flexible medical image handling in\nevery commonly used format (e.g., 2-D, 2.5-D, and 3-D; full- or patch-wise).\nEven data beyond images like demographics or clinical reports can easily be\nintegrated into deep learning pipelines. The evaluation allows stand-alone\nresult calculation and reporting, as well as performance monitoring during\ntraining using a vast amount of domain-specific metrics for segmentation,\nreconstruction, and regression.\n  Results: The pymia package is highly flexible, allows for fast prototyping,\nand reduces the burden of implementing data handling routines and evaluation\nmethods. While data handling and evaluation are independent of the deep\nlearning framework used, they can easily be integrated into TensorFlow and\nPyTorch pipelines. The developed package was successfully used in a variety of\nresearch projects for segmentation, reconstruction, and regression.\n  Conclusions: The pymia package fills the gap of current deep learning\nframeworks regarding data handling and evaluation in medical image analysis. It\nis available at https://github.com/rundherum/pymia and can directly be\ninstalled from the Python Package Index using pip install pymia.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 20:25:52 GMT"}, {"version": "v2", "created": "Wed, 28 Apr 2021 13:25:08 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Jungo", "Alain", ""], ["Scheidegger", "Olivier", ""], ["Reyes", "Mauricio", ""], ["Balsiger", "Fabian", ""]]}, {"id": "2010.03644", "submitter": "Wanrong Zhu", "authors": "Wanrong Zhu, Xin Eric Wang, Pradyumna Narayana, Kazoo Sone, Sugato\n  Basu, William Yang Wang", "title": "Towards Understanding Sample Variance in Visually Grounded Language\n  Generation: Evaluations and Observations", "comments": "EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A major challenge in visually grounded language generation is to build robust\nbenchmark datasets and models that can generalize well in real-world settings.\nTo do this, it is critical to ensure that our evaluation protocols are correct,\nand benchmarks are reliable. In this work, we set forth to design a set of\nexperiments to understand an important but often ignored problem in visually\ngrounded language generation: given that humans have different utilities and\nvisual attention, how will the sample variance in multi-reference datasets\naffect the models' performance? Empirically, we study several multi-reference\ndatasets and corresponding vision-and-language tasks. We show that it is of\nparamount importance to report variance in experiments; that human-generated\nreferences could vary drastically in different datasets/tasks, revealing the\nnature of each task; that metric-wise, CIDEr has shown systematically larger\nvariances than others. Our evaluations on reference-per-instance shed light on\nthe design of reliable datasets in the future.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 20:45:14 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Zhu", "Wanrong", ""], ["Wang", "Xin Eric", ""], ["Narayana", "Pradyumna", ""], ["Sone", "Kazoo", ""], ["Basu", "Sugato", ""], ["Wang", "William Yang", ""]]}, {"id": "2010.03697", "submitter": "Benjamin Haeffele", "authors": "Benjamin D. Haeffele, Chong You, Ren\\'e Vidal", "title": "A Critique of Self-Expressive Deep Subspace Clustering", "comments": "Published as a conference paper at the International Conference on\n  Learning Representations (ICLR) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Subspace clustering is an unsupervised clustering technique designed to\ncluster data that is supported on a union of linear subspaces, with each\nsubspace defining a cluster with dimension lower than the ambient space. Many\nexisting formulations for this problem are based on exploiting the\nself-expressive property of linear subspaces, where any point within a subspace\ncan be represented as linear combination of other points within the subspace.\nTo extend this approach to data supported on a union of non-linear manifolds,\nnumerous studies have proposed learning an embedding of the original data using\na neural network which is regularized by a self-expressive loss function on the\ndata in the embedded space to encourage a union of linear subspaces prior on\nthe data in the embedded space. Here we show that there are a number of\npotential flaws with this approach which have not been adequately addressed in\nprior work. In particular, we show the model formulation is often ill-posed in\nthat it can lead to a degenerate embedding of the data, which need not\ncorrespond to a union of subspaces at all and is poorly suited for clustering.\nWe validate our theoretical results experimentally and also repeat prior\nexperiments reported in the literature, where we conclude that a significant\nportion of the previously claimed performance benefits can be attributed to an\nad-hoc post processing step rather than the deep subspace clustering model.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 00:14:59 GMT"}, {"version": "v2", "created": "Fri, 19 Mar 2021 20:33:37 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Haeffele", "Benjamin D.", ""], ["You", "Chong", ""], ["Vidal", "Ren\u00e9", ""]]}, {"id": "2010.03712", "submitter": "Yuchen Wang", "authors": "Yuchen Wang, Mingze Xu, John Paden, Lora Koenig, Geoffrey Fox, David\n  Crandall", "title": "Deep Tiered Image Segmentation For Detecting Internal Ice Layers in\n  Radar Imagery", "comments": "ICME version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the structure of Earth's polar ice sheets is important for\nmodeling how global warming will impact polar ice and, in turn, the Earth's\nclimate. Ground-penetrating radar is able to collect observations of the\ninternal structure of snow and ice, but the process of manually labeling these\nobservations is slow and laborious. Recent work has developed automatic\ntechniques for finding the boundaries between the ice and the bedrock, but\nfinding internal layers - the subtle boundaries that indicate where one year's\nice accumulation ended and the next began - is much more challenging because\nthe number of layers varies and the boundaries often merge and split. In this\npaper, we propose a novel deep neural network for solving a general class of\ntiered segmentation problems. We then apply it to detecting internal layers in\npolar ice, evaluating on a large-scale dataset of polar ice radar data with\nhuman-labeled annotations as ground truth.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 01:13:03 GMT"}, {"version": "v2", "created": "Thu, 25 Mar 2021 18:37:09 GMT"}, {"version": "v3", "created": "Tue, 6 Apr 2021 04:31:32 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Wang", "Yuchen", ""], ["Xu", "Mingze", ""], ["Paden", "John", ""], ["Koenig", "Lora", ""], ["Fox", "Geoffrey", ""], ["Crandall", "David", ""]]}, {"id": "2010.03735", "submitter": "Sharif Abuadbba Dr", "authors": "Bedeuro Kim, Alsharif Abuadbba, Yansong Gao, Yifeng Zheng, Muhammad\n  Ejaz Ahmed, Hyoungshick Kim, Surya Nepal", "title": "Decamouflage: A Framework to Detect Image-Scaling Attacks on\n  Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As an essential processing step in computer vision applications, image\nresizing or scaling, more specifically downsampling, has to be applied before\nfeeding a normally large image into a convolutional neural network (CNN) model\nbecause CNN models typically take small fixed-size images as inputs. However,\nimage scaling functions could be adversarially abused to perform a newly\nrevealed attack called image-scaling attack, which can affect a wide range of\ncomputer vision applications building upon image-scaling functions.\n  This work presents an image-scaling attack detection framework, termed as\nDecamouflage. Decamouflage consists of three independent detection methods: (1)\nrescaling, (2) filtering/pooling, and (3) steganalysis. While each of these\nthree methods is efficient standalone, they can work in an ensemble manner not\nonly to improve the detection accuracy but also to harden potential adaptive\nattacks. Decamouflage has a pre-determined detection threshold that is generic.\nMore precisely, as we have validated, the threshold determined from one dataset\nis also applicable to other different datasets. Extensive experiments show that\nDecamouflage achieves detection accuracy of 99.9\\% and 99.8\\% in the white-box\n(with the knowledge of attack algorithms) and the black-box (without the\nknowledge of attack algorithms) settings, respectively. To corroborate the\nefficiency of Decamouflage, we have also measured its run-time overhead on a\npersonal PC with an i5 CPU and found that Decamouflage can detect image-scaling\nattacks in milliseconds. Overall, Decamouflage can accurately detect image\nscaling attacks in both white-box and black-box settings with acceptable\nrun-time overhead.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 02:30:55 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Kim", "Bedeuro", ""], ["Abuadbba", "Alsharif", ""], ["Gao", "Yansong", ""], ["Zheng", "Yifeng", ""], ["Ahmed", "Muhammad Ejaz", ""], ["Kim", "Hyoungshick", ""], ["Nepal", "Surya", ""]]}, {"id": "2010.03739", "submitter": "Amir Bar", "authors": "David Chettrit, Tomer Meir, Hila Lebel, Mila Orlovsky, Ronen Gordon,\n  Ayelet Akselrod-Ballin, Amir Bar", "title": "3D Convolutional Sequence to Sequence Model for Vertebral Compression\n  Fractures Identification in CT", "comments": "Accepted to MICCAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An osteoporosis-related fracture occurs every three seconds worldwide,\naffecting one in three women and one in five men aged over 50. The early\ndetection of at-risk patients facilitates effective and well-evidenced\npreventative interventions, reducing the incidence of major osteoporotic\nfractures. In this study, we present an automatic system for identification of\nvertebral compression fractures on Computed Tomography images, which are often\nan undiagnosed precursor to major osteoporosis-related fractures. The system\nintegrates a compact 3D representation of the spine, utilizing a Convolutional\nNeural Network (CNN) for spinal cord detection and a novel end-to-end sequence\nto sequence 3D architecture. We evaluate several model variants that exploit\ndifferent representation and classification approaches and present a framework\ncombining an ensemble of models that achieves state of the art results,\nvalidated on a large data set, with a patient-level fracture identification of\n0.955 Area Under the Curve (AUC). The system proposed has the potential to\nsupport osteoporosis clinical management, improve treatment pathways, and to\nchange the course of one of the most burdensome diseases of our generation.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 02:39:40 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Chettrit", "David", ""], ["Meir", "Tomer", ""], ["Lebel", "Hila", ""], ["Orlovsky", "Mila", ""], ["Gordon", "Ronen", ""], ["Akselrod-Ballin", "Ayelet", ""], ["Bar", "Amir", ""]]}, {"id": "2010.03740", "submitter": "Zixun Huang", "authors": "Zixun Huang, Li-Wen Wang, Frank H. F. Leung, Sunetra Banerjee, De\n  Yang, Timothy Lee, Juan Lyu, Sai Ho Ling, Yong-Ping Zheng", "title": "Bone Feature Segmentation in Ultrasound Spine Image with Robustness to\n  Speckle and Regular Occlusion Noise", "comments": "SMC2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D ultrasound imaging shows great promise for scoliosis diagnosis thanks to\nits low-costing, radiation-free and real-time characteristics. The key to\naccessing scoliosis by ultrasound imaging is to accurately segment the bone\narea and measure the scoliosis degree based on the symmetry of the bone\nfeatures. The ultrasound images tend to contain many speckles and regular\nocclusion noise which is difficult, tedious and time-consuming for experts to\nfind out the bony feature. In this paper, we propose a robust bone feature\nsegmentation method based on the U-net structure for ultrasound spine Volume\nProjection Imaging (VPI) images. The proposed segmentation method introduces a\ntotal variance loss to reduce the sensitivity of the model to small-scale and\nregular occlusion noise. The proposed approach improves 2.3% of Dice score and\n1% of AUC score as compared with the u-net model and shows high robustness to\nspeckle and regular occlusion noise.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 02:44:39 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Huang", "Zixun", ""], ["Wang", "Li-Wen", ""], ["Leung", "Frank H. F.", ""], ["Banerjee", "Sunetra", ""], ["Yang", "De", ""], ["Lee", "Timothy", ""], ["Lyu", "Juan", ""], ["Ling", "Sai Ho", ""], ["Zheng", "Yong-Ping", ""]]}, {"id": "2010.03743", "submitter": "Fuxiao Liu", "authors": "Fuxiao Liu and Yinghan Wang and Tianlu Wang and Vicente Ordonez", "title": "VisualNews : Benchmark and Challenges in Entity-aware Image Captioning", "comments": "9 pages, 5 figures, in submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose VisualNews-Captioner, an entity-aware model for the\ntask of news image captioning. We also introduce VisualNews, a large-scale\nbenchmark consisting of more than one million news images along with associated\nnews articles, image captions, author information, and other metadata. Unlike\nthe standard image captioning task, news images depict situations where people,\nlocations, and events are of paramount importance. Our proposed method is able\nto effectively combine visual and textual features to generate captions with\nricher information such as events and entities. More specifically, we propose\nan Entity-Aware module along with an Entity-Guide attention layer to encourage\nmore accurate predictions for named entities. Our method achieves\nstate-of-the-art results on both the GoodNews and VisualNews datasets while\nhaving significantly fewer parameters than competing methods. Our larger and\nmore diverse VisualNews dataset further highlights the remaining challenges in\ncaptioning news images.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 03:07:00 GMT"}, {"version": "v2", "created": "Tue, 13 Oct 2020 17:41:41 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Liu", "Fuxiao", ""], ["Wang", "Yinghan", ""], ["Wang", "Tianlu", ""], ["Ordonez", "Vicente", ""]]}, {"id": "2010.03758", "submitter": "Daniel Mas Montserrat", "authors": "Daniel Mas Montserrat, J\\'anos Horv\\'ath, S. K. Yarlagadda, Fengqing\n  Zhu, Edward J. Delp", "title": "Generative Autoregressive Ensembles for Satellite Imagery Manipulation\n  Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Satellite imagery is becoming increasingly accessible due to the growing\nnumber of orbiting commercial satellites. Many applications make use of such\nimages: agricultural management, meteorological prediction, damage assessment\nfrom natural disasters, or cartography are some of the examples. Unfortunately,\nthese images can be easily tampered and modified with image manipulation tools\ndamaging downstream applications. Because the nature of the manipulation\napplied to the image is typically unknown, unsupervised methods that don't\nrequire prior knowledge of the tampering techniques used are preferred. In this\npaper, we use ensembles of generative autoregressive models to model the\ndistribution of the pixels of the image in order to detect potential\nmanipulations. We evaluate the performance of the presented approach obtaining\naccurate localization results compared to previously presented approaches.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 04:41:30 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Montserrat", "Daniel Mas", ""], ["Horv\u00e1th", "J\u00e1nos", ""], ["Yarlagadda", "S. K.", ""], ["Zhu", "Fengqing", ""], ["Delp", "Edward J.", ""]]}, {"id": "2010.03768", "submitter": "Mohit Shridhar", "authors": "Mohit Shridhar, Xingdi Yuan, Marc-Alexandre C\\^ot\\'e, Yonatan Bisk,\n  Adam Trischler, Matthew Hausknecht", "title": "ALFWorld: Aligning Text and Embodied Environments for Interactive\n  Learning", "comments": "ICLR 2021; Data, code, and videos are available at alfworld.github.io", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a simple request like Put a washed apple in the kitchen fridge, humans\ncan reason in purely abstract terms by imagining action sequences and scoring\ntheir likelihood of success, prototypicality, and efficiency, all without\nmoving a muscle. Once we see the kitchen in question, we can update our\nabstract plans to fit the scene. Embodied agents require the same abilities,\nbut existing work does not yet provide the infrastructure necessary for both\nreasoning abstractly and executing concretely. We address this limitation by\nintroducing ALFWorld, a simulator that enables agents to learn abstract, text\nbased policies in TextWorld (C\\^ot\\'e et al., 2018) and then execute goals from\nthe ALFRED benchmark (Shridhar et al., 2020) in a rich visual environment.\nALFWorld enables the creation of a new BUTLER agent whose abstract knowledge,\nlearned in TextWorld, corresponds directly to concrete, visually grounded\nactions. In turn, as we demonstrate empirically, this fosters better agent\ngeneralization than training only in the visually grounded environment.\nBUTLER's simple, modular design factors the problem to allow researchers to\nfocus on models for improving every piece of the pipeline (language\nunderstanding, planning, navigation, and visual scene understanding).\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 05:13:36 GMT"}, {"version": "v2", "created": "Sun, 14 Mar 2021 22:44:38 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Shridhar", "Mohit", ""], ["Yuan", "Xingdi", ""], ["C\u00f4t\u00e9", "Marc-Alexandre", ""], ["Bisk", "Yonatan", ""], ["Trischler", "Adam", ""], ["Hausknecht", "Matthew", ""]]}, {"id": "2010.03771", "submitter": "Ha Le", "authors": "Ha Le and Ioannis A. Kakadiaris", "title": "DBLFace: Domain-Based Labels for NIR-VIS Heterogeneous Face Recognition", "comments": "accepted to IJCB20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning-based domain-invariant feature learning methods are advancing\nin near-infrared and visible (NIR-VIS) heterogeneous face recognition. However,\nthese methods are prone to overfitting due to the large intra-class variation\nand the lack of NIR images for training. In this paper, we introduce\nDomain-Based Label Face (DBLFace), a learning approach based on the assumption\nthat a subject is not represented by a single label but by a set of labels.\nEach label represents images of a specific domain. In particular, a set of two\nlabels per subject, one for the NIR images and one for the VIS images, are used\nfor training a NIR-VIS face recognition model. The classification of images\ninto different domains reduces the intra-class variation and lessens the\nnegative impact of data imbalance in training. To train a network with sets of\nlabels, we introduce a domain-based angular margin loss and a maximum angular\nloss to maintain the inter-class discrepancy and to enforce the close\nrelationship of labels in a set. Quantitative experiments confirm that DBLFace\nsignificantly improves the rank-1 identification rate by 6.7% on the EDGE20\ndataset and achieves state-of-the-art performance on the CASIA NIR-VIS 2.0\ndataset.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 05:22:47 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Le", "Ha", ""], ["Kakadiaris", "Ioannis A.", ""]]}, {"id": "2010.03791", "submitter": "Shervin Minaee", "authors": "Amirali Abdolrashidi, Mehdi Minaei, Elham Azimi, Shervin Minaee", "title": "Age and Gender Prediction From Face Images Using Attentional\n  Convolutional Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic prediction of age and gender from face images has drawn a lot of\nattention recently, due it is wide applications in various facial analysis\nproblems. However, due to the large intra-class variation of face images (such\nas variation in lighting, pose, scale, occlusion), the existing models are\nstill behind the desired accuracy level, which is necessary for the use of\nthese models in real-world applications. In this work, we propose a deep\nlearning framework, based on the ensemble of attentional and residual\nconvolutional networks, to predict gender and age group of facial images with\nhigh accuracy rate. Using attention mechanism enables our model to focus on the\nimportant and informative parts of the face, which can help it to make a more\naccurate prediction. We train our model in a multi-task learning fashion, and\naugment the feature embedding of the age classifier, with the predicted gender,\nand show that doing so can further increase the accuracy of age prediction. Our\nmodel is trained on a popular face age and gender dataset, and achieved\npromising results. Through visualization of the attention maps of the train\nmodel, we show that our model has learned to become sensitive to the right\nregions of the face.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 06:33:55 GMT"}, {"version": "v2", "created": "Mon, 7 Dec 2020 20:06:33 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Abdolrashidi", "Amirali", ""], ["Minaei", "Mehdi", ""], ["Azimi", "Elham", ""], ["Minaee", "Shervin", ""]]}, {"id": "2010.03815", "submitter": "Cenk Bircanoglu", "authors": "Cenk Bircanoglu", "title": "A Comparative Study on Effects of Original and Pseudo Labels for Weakly\n  Supervised Learning for Car Localization Problem", "comments": "8 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, the effects of different class labels created as a result of\nmultiple conceptual meanings on localization using Weakly Supervised Learning\npresented on Car Dataset. In addition, the generated labels are included in the\ncomparison, and the solution turned into Unsupervised Learning. This paper\ninvestigates multiple setups for car localization in the images with other\napproaches rather than Supervised Learning. To predict localization labels,\nClass Activation Mapping (CAM) is implemented and from the results, the\nbounding boxes are extracted by using morphological edge detection. Besides the\noriginal class labels, generated class labels also employed to train CAM on\nwhich turn to a solution to Unsupervised Learning example. In the experiments,\nwe first analyze the effects of class labels in Weakly Supervised localization\non the Compcars dataset. We then show that the proposed Unsupervised approach\noutperforms the Weakly Supervised method in this particular dataset by\napproximately %6.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 07:41:40 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Bircanoglu", "Cenk", ""]]}, {"id": "2010.03844", "submitter": "Min Yang", "authors": "Cong Xu, Dan Li and Min Yang", "title": "Improve Adversarial Robustness via Weight Penalization on Classification\n  Layer", "comments": "22 pages, 10 figures, 43 references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well-known that deep neural networks are vulnerable to adversarial\nattacks. Recent studies show that well-designed classification parts can lead\nto better robustness. However, there is still much space for improvement along\nthis line. In this paper, we first prove that, from a geometric point of view,\nthe robustness of a neural network is equivalent to some angular margin\ncondition of the classifier weights. We then explain why ReLU type function is\nnot a good choice for activation under this framework. These findings reveal\nthe limitations of the existing approaches and lead us to develop a novel\nlight-weight-penalized defensive method, which is simple and has a good\nscalability. Empirical results on multiple benchmark datasets demonstrate that\nour method can effectively improve the robustness of the network without\nrequiring too much additional computation, while maintaining a high\nclassification precision for clean data.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 08:57:57 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Xu", "Cong", ""], ["Li", "Dan", ""], ["Yang", "Min", ""]]}, {"id": "2010.03855", "submitter": "Dong-Jin Kim", "authors": "Dong-Jin Kim, Tae-Hyun Oh, Jinsoo Choi, In So Kweon", "title": "Dense Relational Image Captioning via Multi-task Triple-Stream Networks", "comments": "Journal extension of our CVPR 2019 paper ( arXiv:1903.05942 ). Source\n  code : https://github.com/Dong-JinKim/DenseRelationalCaptioning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce dense relational captioning, a novel image captioning task which\naims to generate multiple captions with respect to relational information\nbetween objects in a visual scene. Relational captioning provides explicit\ndescriptions of each relationship between object combinations. This framework\nis advantageous in both diversity and amount of information, leading to a\ncomprehensive image understanding based on relationships, e.g., relational\nproposal generation. For relational understanding between objects, the\npart-of-speech (POS, i.e., subject-object-predicate categories) can be a\nvaluable prior information to guide the causal sequence of words in a caption.\nWe enforce our framework to not only learn to generate captions but also\npredict the POS of each word. To this end, we propose the multi-task\ntriple-stream network (MTTSNet) which consists of three recurrent units\nresponsible for each POS which is trained by jointly predicting the correct\ncaptions and POS for each word. In addition, we found that the performance of\nMTTSNet can be improved by modulating the object embeddings with an explicit\nrelational module. We demonstrate that our proposed model can generate more\ndiverse and richer captions, via extensive experimental analysis on large scale\ndatasets and several metrics. We additionally extend analysis to an ablation\nstudy, applications on holistic image captioning, scene graph generation, and\nretrieval tasks.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 09:17:55 GMT"}, {"version": "v2", "created": "Mon, 12 Oct 2020 18:14:14 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Kim", "Dong-Jin", ""], ["Oh", "Tae-Hyun", ""], ["Choi", "Jinsoo", ""], ["Kweon", "In So", ""]]}, {"id": "2010.03872", "submitter": "Taimur Hassan", "authors": "Hina Raja and Taimur Hassan and Muhammad Usman Akram and Naoufel\n  Werghi", "title": "Clinically Verified Hybrid Deep Learning System for Retinal Ganglion\n  Cells Aware Grading of Glaucomatous Progression", "comments": "Accepted in IEEE Transactions on Biomedical Engineering, Source Code:\n  https://github.com/taimurhassan/rag-net-v2", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Objective: Glaucoma is the second leading cause of blindness worldwide.\nGlaucomatous progression can be easily monitored by analyzing the degeneration\nof retinal ganglion cells (RGCs). Many researchers have screened glaucoma by\nmeasuring cup-to-disc ratios from fundus and optical coherence tomography\nscans. However, this paper presents a novel strategy that pays attention to the\nRGC atrophy for screening glaucomatous pathologies and grading their severity.\nMethods: The proposed framework encompasses a hybrid convolutional network that\nextracts the retinal nerve fiber layer, ganglion cell with the inner plexiform\nlayer and ganglion cell complex regions, allowing thus a quantitative screening\nof glaucomatous subjects. Furthermore, the severity of glaucoma in screened\ncases is objectively graded by analyzing the thickness of these regions.\nResults: The proposed framework is rigorously tested on publicly available\nArmed Forces Institute of Ophthalmology (AFIO) dataset, where it achieved the\nF1 score of 0.9577 for diagnosing glaucoma, a mean dice coefficient score of\n0.8697 for extracting the RGC regions and an accuracy of 0.9117 for grading\nglaucomatous progression. Furthermore, the performance of the proposed\nframework is clinically verified with the markings of four expert\nophthalmologists, achieving a statistically significant Pearson correlation\ncoefficient of 0.9236. Conclusion: An automated assessment of RGC degeneration\nyields better glaucomatous screening and grading as compared to the\nstate-of-the-art solutions. Significance: An RGC-aware system not only screens\nglaucoma but can also grade its severity and here we present an end-to-end\nsolution that is thoroughly evaluated on a standardized dataset and is\nclinically validated for analyzing glaucomatous pathologies.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 10:01:48 GMT"}], "update_date": "2020-11-22", "authors_parsed": [["Raja", "Hina", ""], ["Hassan", "Taimur", ""], ["Akram", "Muhammad Usman", ""], ["Werghi", "Naoufel", ""]]}, {"id": "2010.03897", "submitter": "Conghao Wang", "authors": "Beihao xia, Conghao Wong, Heng Li, Shiming Chen, Qinmu Peng, Xinge You", "title": "BGM: Building a Dynamic Guidance Map without Visual Images for\n  Trajectory Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual images usually contain the informative context of the environment,\nthereby helping to predict agents' behaviors. However, they hardly impose the\ndynamic effects on agents' actual behaviors due to the respectively fixed\nsemantics. To solve this problem, we propose a deterministic model named BGM to\nconstruct a guidance map to represent the dynamic semantics, which circumvents\nto use visual images for each agent to reflect the difference of activities in\ndifferent periods. We first record all agents' activities in the scene within a\nperiod close to the current to construct a guidance map and then feed it to a\nContext CNN to obtain their context features. We adopt a Historical Trajectory\nEncoder to extract the trajectory features and then combine them with the\ncontext feature as the input of the social energy based trajectory decoder,\nthus obtaining the prediction that meets the social rules. Experiments\ndemonstrate that BGM achieves state-of-the-art prediction accuracy on the two\nwidely used ETH and UCY datasets and handles more complex scenarios.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 10:48:47 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["xia", "Beihao", ""], ["Wong", "Conghao", ""], ["Li", "Heng", ""], ["Chen", "Shiming", ""], ["Peng", "Qinmu", ""], ["You", "Xinge", ""]]}, {"id": "2010.03902", "submitter": "Mahesh Pal Dr.", "authors": "Mahesh Pal, Akshay, B. Charan Teja", "title": "IRX-1D: A Simple Deep Learning Architecture for Remote Sensing\n  Classifications", "comments": "22 Page, 6 tables, 9 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We proposes a simple deep learning architecture combining elements of\nInception, ResNet and Xception networks. Four new datasets were used for\nclassification with both small and large training samples. Results in terms of\nclassification accuracy suggests improved performance by proposed architecture\nin comparison to Bayesian optimised 2D-CNN with small training samples.\nComparison of results using small training sample with Indiana Pines\nhyperspectral dataset suggests comparable or better performance by proposed\narchitecture than nine reported works using different deep learning\narchitectures. In spite of achieving high classification accuracy with limited\ntraining samples, comparison of classified image suggests different land cover\nclasses are assigned to same area when compared with the classified image\nprovided by the model trained using large training samples with all datasets.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 11:07:02 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Pal", "Mahesh", ""], ["Akshay", "", ""], ["Teja", "B. Charan", ""]]}, {"id": "2010.03965", "submitter": "Yajun An", "authors": "Yajun An, Zachary Golden, Tarka Wilcox, Renzhi Cao", "title": "High Definition image classification in Geoscience using Machine\n  Learning", "comments": "8 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High Definition (HD) digital photos taken with drones are widely used in the\nstudy of Geoscience. However, blurry images are often taken in collected data,\nand it takes a lot of time and effort to distinguish clear images from blurry\nones. In this work, we apply Machine learning techniques, such as Support\nVector Machine (SVM) and Neural Network (NN) to classify HD images in\nGeoscience as clear and blurry, and therefore automate data cleaning in\nGeoscience. We compare the results of classification based on features\nabstracted from several mathematical models. Some of the implementation of our\nmachine learning tool is freely available at:\nhttps://github.com/zachgolden/geoai.\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2020 17:30:03 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["An", "Yajun", ""], ["Golden", "Zachary", ""], ["Wilcox", "Tarka", ""], ["Cao", "Renzhi", ""]]}, {"id": "2010.03972", "submitter": "Hao Sun", "authors": "Hao Sun, Nick Pears and Hang Dai", "title": "A Human Ear Reconstruction Autoencoder", "comments": "Submitted to VISAPP", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ear, as an important part of the human head, has received much less\nattention compared to the human face in the area of computer vision. Inspired\nby previous work on monocular 3D face reconstruction using an autoencoder\nstructure to achieve self-supervised learning, we aim to utilise such a\nframework to tackle the 3D ear reconstruction task, where more subtle and\ndifficult curves and features are present on the 2D ear input images. Our Human\nEar Reconstruction Autoencoder (HERA) system predicts 3D ear poses and shape\nparameters for 3D ear meshes, without any supervision to these parameters. To\nmake our approach cover the variance for in-the-wild images, even grayscale\nimages, we propose an in-the-wild ear colour model. The constructed end-to-end\nself-supervised model is then evaluated both with 2D landmark localisation\nperformance and the appearance of the reconstructed 3D ears.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 12:52:23 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Sun", "Hao", ""], ["Pears", "Nick", ""], ["Dai", "Hang", ""]]}, {"id": "2010.03975", "submitter": "Bradley Segal Segal", "authors": "Bradley Segal, David M. Rubin, Grace Rubin, Adam Pantanowitz", "title": "Evaluating the Clinical Realism of Synthetic Chest X-Rays Generated\n  Using Progressively Growing GANs", "comments": "18 pages, 8 figures, 2 tables", "journal-ref": null, "doi": "10.1007/s42979-021-00720-7", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chest x-rays are a vital tool in the workup of many patients. Similar to most\nmedical imaging modalities, they are profoundly multi-modal and are capable of\nvisualising a variety of combinations of conditions. There is an ever pressing\nneed for greater quantities of labelled data to develop new diagnostic tools,\nhowever this is in direct opposition to concerns regarding patient\nconfidentiality which constrains access through permission requests and ethics\napprovals. Previous work has sought to address these concerns by creating\nclass-specific GANs that synthesise images to augment training data. These\napproaches cannot be scaled as they introduce computational trade offs between\nmodel size and class number which places fixed limits on the quality that such\ngenerates can achieve. We address this concern by introducing latent class\noptimisation which enables efficient, multi-modal sampling from a GAN and with\nwhich we synthesise a large archive of labelled generates. We apply a PGGAN to\nthe task of unsupervised x-ray synthesis and have radiologists evaluate the\nclinical realism of the resultant samples. We provide an in depth review of the\nproperties of varying pathologies seen on generates as well as an overview of\nthe extent of disease diversity captured by the model. We validate the\napplication of the Fr\\'echet Inception Distance (FID) to measure the quality of\nx-ray generates and find that they are similar to other high resolution tasks.\nWe quantify x-ray clinical realism by asking radiologists to distinguish\nbetween real and fake scans and find that generates are more likely to be\nclassed as real than by chance, but there is still progress required to achieve\ntrue realism. We confirm these findings by evaluating synthetic classification\nmodel performance on real scans. We conclude by discussing the limitations of\nPGGAN generates and how to achieve controllable, realistic generates.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 11:47:22 GMT"}, {"version": "v2", "created": "Wed, 10 Mar 2021 21:13:38 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Segal", "Bradley", ""], ["Rubin", "David M.", ""], ["Rubin", "Grace", ""], ["Pantanowitz", "Adam", ""]]}, {"id": "2010.03978", "submitter": "Abolfazl Farahani", "authors": "Abolfazl Farahani, Sahar Voghoei, Khaled Rasheed, Hamid R. Arabnia", "title": "A Brief Review of Domain Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classical machine learning assumes that the training and test sets come from\nthe same distributions. Therefore, a model learned from the labeled training\ndata is expected to perform well on the test data. However, This assumption may\nnot always hold in real-world applications where the training and the test data\nfall from different distributions, due to many factors, e.g., collecting the\ntraining and test sets from different sources, or having an out-dated training\nset due to the change of data over time. In this case, there would be a\ndiscrepancy across domain distributions, and naively applying the trained model\non the new dataset may cause degradation in the performance. Domain adaptation\nis a sub-field within machine learning that aims to cope with these types of\nproblems by aligning the disparity between domains such that the trained model\ncan be generalized into the domain of interest. This paper focuses on\nunsupervised domain adaptation, where the labels are only available in the\nsource domain. It addresses the categorization of domain adaptation from\ndifferent viewpoints. Besides, It presents some successful shallow and deep\ndomain adaptation approaches that aim to deal with domain adaptation problems.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 07:05:32 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Farahani", "Abolfazl", ""], ["Voghoei", "Sahar", ""], ["Rasheed", "Khaled", ""], ["Arabnia", "Hamid R.", ""]]}, {"id": "2010.03988", "submitter": "Adrian Shajkofci", "authors": "Adrian Shajkofci, Michael Liebling", "title": "Free annotated data for deep learning in microscopy? A hitchhiker's\n  guide", "comments": "Accepted in Photoniques 104", "journal-ref": "Photoniques 104 2020", "doi": "10.1051/photon/202010430", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In microscopy, the time burden and cost of acquiring and annotating large\ndatasets that many deep learning models take as a prerequisite, often appears\nto make these methods impractical. Can this requirement for annotated data be\nrelaxed? Is it possible to borrow the knowledge gathered from datasets in other\napplication fields and leverage it for microscopy? Here, we aim to provide an\noverview of methods that have recently emerged to successfully train\nlearning-based methods in bio-microscopy.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 14:00:46 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Shajkofci", "Adrian", ""], ["Liebling", "Michael", ""]]}, {"id": "2010.03990", "submitter": "Aman Kamboj", "authors": "Aman Kamboj, Rajneesh Rani, Aditya Nigam, Ranjeet Ranjan Jha", "title": "UESegNet: Context Aware Unconstrained ROI Segmentation Networks for Ear\n  Biometric", "comments": null, "journal-ref": null, "doi": "10.1007/s10044-020-00914-4", "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biometric-based personal authentication systems have seen a strong demand\nmainly due to the increasing concern in various privacy and security\napplications. Although the use of each biometric trait is problem dependent,\nthe human ear has been found to have enough discriminating characteristics to\nallow its use as a strong biometric measure. To locate an ear in a 2D side face\nimage is a challenging task, numerous existing approaches have achieved\nsignificant performance, but the majority of studies are based on the\nconstrained environment. However, ear biometrics possess a great level of\ndifficulties in the unconstrained environment, where pose, scale, occlusion,\nilluminations, background clutter etc. varies to a great extent. To address the\nproblem of ear localization in the wild, we have proposed two high-performance\nregion of interest (ROI) segmentation models UESegNet-1 and UESegNet-2, which\nare fundamentally based on deep convolutional neural networks and primarily\nuses contextual information to localize ear in the unconstrained environment.\nAdditionally, we have applied state-of-the-art deep learning models viz; FRCNN\n(Faster Region Proposal Network) and SSD (Single Shot MultiBox Detecor) for ear\nlocalization task. To test the model's generalization, they are evaluated on\nsix different benchmark datasets viz; IITD, IITK, USTB-DB3, UND-E, UND-J2 and\nUBEAR, all of which contain challenging images. The performance of the models\nis compared on the basis of object detection performance measure parameters\nsuch as IOU (Intersection Over Union), Accuracy, Precision, Recall, and\nF1-Score. It has been observed that the proposed models UESegNet-1 and\nUESegNet-2 outperformed the FRCNN and SSD at higher values of IOUs i.e. an\naccuracy of 100\\% is achieved at IOU 0.5 on majority of the databases.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 14:05:15 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Kamboj", "Aman", ""], ["Rani", "Rajneesh", ""], ["Nigam", "Aditya", ""], ["Jha", "Ranjeet Ranjan", ""]]}, {"id": "2010.03997", "submitter": "Juli\\'an Del Gobbo", "authors": "Juli\\'an Del Gobbo, Rosana Matuk Herrera", "title": "Unconstrained Text Detection in Manga", "comments": "Thesis, University of Buenos Aires. arXiv admin note: text overlap\n  with arXiv:2009.04042", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The detection and recognition of unconstrained text is an open problem in\nresearch. Text in comic books has unusual styles that raise many challenges for\ntext detection. This work aims to identify text characters at a pixel level in\na comic genre with highly sophisticated text styles: Japanese manga. To\novercome the lack of a manga dataset with individual character level\nannotations, we create our own. Most of the literature in text detection use\nbounding box metrics, which are unsuitable for pixel-level evaluation. Thus, we\nimplemented special metrics to evaluate performance. Using these resources, we\ndesigned and evaluated a deep network model, outperforming current methods for\ntext detection in manga in most metrics.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 13:28:13 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Del Gobbo", "Juli\u00e1n", ""], ["Herrera", "Rosana Matuk", ""]]}, {"id": "2010.04002", "submitter": "G\\\"ul Varol", "authors": "Liliane Momeni, G\\\"ul Varol, Samuel Albanie, Triantafyllos Afouras,\n  Andrew Zisserman", "title": "Watch, read and lookup: learning to spot signs from multiple supervisors", "comments": "Appears in: Asian Conference on Computer Vision 2020 (ACCV 2020) -\n  Oral presentation. 29 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The focus of this work is sign spotting - given a video of an isolated sign,\nour task is to identify whether and where it has been signed in a continuous,\nco-articulated sign language video. To achieve this sign spotting task, we\ntrain a model using multiple types of available supervision by: (1) watching\nexisting sparsely labelled footage; (2) reading associated subtitles (readily\navailable translations of the signed content) which provide additional\nweak-supervision; (3) looking up words (for which no co-articulated labelled\nexamples are available) in visual sign language dictionaries to enable novel\nsign spotting. These three tasks are integrated into a unified learning\nframework using the principles of Noise Contrastive Estimation and Multiple\nInstance Learning. We validate the effectiveness of our approach on low-shot\nsign spotting benchmarks. In addition, we contribute a machine-readable British\nSign Language (BSL) dictionary dataset of isolated signs, BSLDict, to\nfacilitate study of this task. The dataset, models and code are available at\nour project page.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 14:12:56 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Momeni", "Liliane", ""], ["Varol", "G\u00fcl", ""], ["Albanie", "Samuel", ""], ["Afouras", "Triantafyllos", ""], ["Zisserman", "Andrew", ""]]}, {"id": "2010.04007", "submitter": "Jon Haitz Legarreta", "authors": "Jon Haitz Legarreta, Laurent Petit, Fran\\c{c}ois Rheault, Guillaume\n  Theaud, Carl Lemaire, Maxime Descoteaux and Pierre-Marc Jodoin", "title": "Tractography filtering using autoencoders", "comments": "Preprint. Paper under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.AI cs.CV cs.LG q-bio.NC q-bio.QM", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Current brain white matter fiber tracking techniques show a number of\nproblems, including: generating large proportions of streamlines that do not\naccurately describe the underlying anatomy; extracting streamlines that are not\nsupported by the underlying diffusion signal; and under-representing some fiber\npopulations, among others. In this paper, we describe a novel unsupervised\nlearning method to filter streamlines from diffusion MRI tractography, and\nhence, to obtain more reliable tractograms.\n  We show that a convolutional neural network autoencoder provides a\nstraightforward and elegant way to learn a robust representation of brain\nstreamlines, which can be used to filter undesired samples with a nearest\nneighbor algorithm. Our method, dubbed FINTA (Filtering in Tractography using\nAutoencoders) comes with several key advantages: training does not need labeled\ndata, as it uses raw tractograms, it is fast and easily reproducible, it does\nnot rely on the input diffusion MRI data, and thus, does not suffer from domain\nadaptation issues. We demonstrate the ability of FINTA to discriminate between\n\"plausible\" and \"implausible\" streamlines as well as to recover individual\nstreamline group instances from a raw tractogram, from both synthetic and real\nhuman brain diffusion MRI tractography data, including partial tractograms.\nResults reveal that FINTA has a superior filtering performance compared to\nstate-of-the-art methods.\n  Together, this work brings forward a new deep learning framework in\ntractography based on autoencoders, and shows how it can be applied for\nfiltering purposes. It sets the foundations for opening up new prospects\ntowards more accurate and robust tractometry and connectivity diffusion MRI\nanalyses, which may ultimately lead to improve the imaging of the white matter\nanatomy.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 16:45:55 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Legarreta", "Jon Haitz", ""], ["Petit", "Laurent", ""], ["Rheault", "Fran\u00e7ois", ""], ["Theaud", "Guillaume", ""], ["Lemaire", "Carl", ""], ["Descoteaux", "Maxime", ""], ["Jodoin", "Pierre-Marc", ""]]}, {"id": "2010.04009", "submitter": "Aashish Sharma Mr", "authors": "Aashish Sharma, Robby T. Tan, and Loong-Fah Cheong", "title": "Estimation of Camera Response Function using Prediction Consistency and\n  Gradual Refinement with an Extension to Deep Learning", "comments": "11 Pages, 12 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most existing methods for CRF estimation from a single image fail to handle\ngeneral real images. For instance, EdgeCRF based on colour patches extracted\nfrom edges works effectively only when the presence of noise is insignificant,\nwhich is not the case for many real images; and, CRFNet, a recent method based\non fully supervised deep learning works only for the CRFs that are in the\ntraining data, and hence fail to deal with other possible CRFs beyond the\ntraining data. To address these problems, we introduce a non-deep-learning\nmethod using prediction consistency and gradual refinement. First, we rely more\non the patches of the input image that provide more consistent predictions. If\nthe predictions from a patch are more consistent, it means that the patch is\nlikely to be less affected by noise or any inferior colour combinations, and\nhence, it can be more reliable for CRF estimation. Second, we employ a gradual\nrefinement scheme in which we start from a simple CRF model to generate a\nresult which is more robust to noise but less accurate, and then we gradually\nincrease the model's complexity to improve the result. This is because a simple\nmodel, while being less accurate, overfits less to noise than a complex model\ndoes. Our experiments show that our method outperforms the existing\nsingle-image methods for daytime and nighttime real images. We further propose\na more efficient deep learning extension that performs test-time training\n(based on unsupervised losses) on the test input image. This provides our\nmethod better generalization performance than CRFNet making it more practically\napplicable for CRF estimation for general real images.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 14:19:48 GMT"}, {"version": "v2", "created": "Tue, 23 Feb 2021 03:05:56 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Sharma", "Aashish", ""], ["Tan", "Robby T.", ""], ["Cheong", "Loong-Fah", ""]]}, {"id": "2010.04011", "submitter": "Adrian Shajkofci", "authors": "Adrian Shajkofci, Michael Liebling", "title": "Spatially-Variant CNN-based Point Spread Function Estimation for Blind\n  Deconvolution and Depth Estimation in Optical Microscopy", "comments": "Preprint", "journal-ref": "IEEE Transactions on Image Processing, vol. 29, pp. 5848-5861,\n  2020", "doi": "10.1109/TIP.2020.2986880", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optical microscopy is an essential tool in biology and medicine. Imaging\nthin, yet non-flat objects in a single shot (without relying on more\nsophisticated sectioning setups) remains challenging as the shallow depth of\nfield that comes with high-resolution microscopes leads to unsharp image\nregions and makes depth localization and quantitative image interpretation\ndifficult.\n  Here, we present a method that improves the resolution of light microscopy\nimages of such objects by locally estimating image distortion while jointly\nestimating object distance to the focal plane. Specifically, we estimate the\nparameters of a spatially-variant Point-Spread function (PSF) model using a\nConvolutional Neural Network (CNN), which does not require instrument- or\nobject-specific calibration. Our method recovers PSF parameters from the image\nitself with up to a squared Pearson correlation coefficient of 0.99 in ideal\nconditions, while remaining robust to object rotation, illumination variations,\nor photon noise. When the recovered PSFs are used with a spatially-variant and\nregularized Richardson-Lucy deconvolution algorithm, we observed up to 2.1 dB\nbetter signal-to-noise ratio compared to other blind deconvolution techniques.\nFollowing microscope-specific calibration, we further demonstrate that the\nrecovered PSF model parameters permit estimating surface depth with a precision\nof 2 micrometers and over an extended range when using engineered PSFs. Our\nmethod opens up multiple possibilities for enhancing images of non-flat objects\nwith minimal need for a priori knowledge about the optical setup.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 14:20:16 GMT"}, {"version": "v2", "created": "Tue, 13 Oct 2020 09:39:50 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Shajkofci", "Adrian", ""], ["Liebling", "Michael", ""]]}, {"id": "2010.04022", "submitter": "Zanobya Khan", "authors": "Zanobya N. Khan", "title": "Frequency and Spatial domain based Saliency for Pigmented Skin Lesion\n  Segmentation", "comments": "9 pages, 9 figures and 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Skin lesion segmentation can be rather a challenging task owing to the\npresence of artifacts, low contrast between lesion and boundary, color\nvariegation, fuzzy skin lesion borders and heterogeneous background in\ndermoscopy images. In this paper, we propose a simple yet effective\nsaliency-based approach derived in the frequency and spatial domain to detect\npigmented skin lesion. Two color models are utilized for the construction of\nthese maps. We suggest a different metric for each color model to design map in\nthe spatial domain via color features. The map in the frequency domain is\ngenerated from aggregated images. We adopt a separate fusion scheme to combine\nsalient features in their respective domains. Finally, two-phase saliency\nintegration scheme is devised to combine these maps using pixelwise\nmultiplication. Performance of the proposed method is assessed on PH2 and ISIC\n2016 datasets. The outcome of the experiments suggests that the proposed scheme\ngenerate better segmentation result as compared to state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 14:38:42 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Khan", "Zanobya N.", ""]]}, {"id": "2010.04030", "submitter": "Cathrin Elich", "authors": "Cathrin Elich, Martin R. Oswald, Marc Pollefeys, Joerg Stueckler", "title": "Semi-Supervised Learning of Multi-Object 3D Scene Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Representing scenes at the granularity of objects is a prerequisite for scene\nunderstanding and decision making. We propose a novel approach for learning\nmulti-object 3D scene representations from images. A recurrent encoder\nregresses a latent representation of 3D shapes, poses and texture of each\nobject from an input RGB image. The 3D shapes are represented continuously in\nfunction-space as signed distance functions (SDF) which we efficiently\npre-train from example shapes in a supervised way. By differentiable rendering\nwe then train our model to decompose scenes self-supervised from RGB-D images.\nOur approach learns to decompose images into the constituent objects of the\nscene and to infer their shape, pose and texture from a single view. We\nevaluate the accuracy of our model in inferring the 3D scene layout and\ndemonstrate its generative capabilities.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 14:49:23 GMT"}, {"version": "v2", "created": "Tue, 26 Jan 2021 10:30:02 GMT"}, {"version": "v3", "created": "Tue, 20 Apr 2021 09:14:09 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Elich", "Cathrin", ""], ["Oswald", "Martin R.", ""], ["Pollefeys", "Marc", ""], ["Stueckler", "Joerg", ""]]}, {"id": "2010.04038", "submitter": "Lazaro Janier Gonzalez-Soler Soler", "authors": "Lazaro J. Gonzalez-Soler and Jose Patino and Marta Gomez-Barrero and\n  Massimiliano Todisco and Christoph Busch and Nicholas Evans", "title": "Texture-based Presentation Attack Detection for Automatic Speaker\n  Verification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CV cs.LG eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biometric systems are nowadays employed across a broad range of applications.\nThey provide high security and efficiency and, in many cases, are user\nfriendly. Despite these and other advantages, biometric systems in general and\nAutomatic speaker verification (ASV) systems in particular can be vulnerable to\nattack presentations. The most recent ASVSpoof 2019 competition showed that\nmost forms of attacks can be detected reliably with ensemble classifier-based\npresentation attack detection (PAD) approaches. These, though, depend\nfundamentally upon the complementarity of systems in the ensemble. With the\nmotivation to increase the generalisability of PAD solutions, this paper\nreports our exploration of texture descriptors applied to the analysis of\nspeech spectrogram images. In particular, we propose a common fisher vector\nfeature space based on a generative model. Experimental results show the\nsoundness of our approach: at most, 16 in 100 bona fide presentations are\nrejected whereas only one in 100 attack presentations are accepted.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 15:03:29 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Gonzalez-Soler", "Lazaro J.", ""], ["Patino", "Jose", ""], ["Gomez-Barrero", "Marta", ""], ["Todisco", "Massimiliano", ""], ["Busch", "Christoph", ""], ["Evans", "Nicholas", ""]]}, {"id": "2010.04049", "submitter": "Jiancheng Yang", "authors": "Jiancheng Yang, Mingze Gao, Kaiming Kuang, Bingbing Ni, Yunlang She,\n  Dong Xie, Chang Chen", "title": "Hierarchical Classification of Pulmonary Lesions: A Large-Scale\n  Radio-Pathomics Study", "comments": "MICCAI 2020 (Early Accepted)", "journal-ref": null, "doi": "10.1007/978-3-030-59725-2_48", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Diagnosis of pulmonary lesions from computed tomography (CT) is important but\nchallenging for clinical decision making in lung cancer related diseases. Deep\nlearning has achieved great success in computer aided diagnosis (CADx) area for\nlung cancer, whereas it suffers from label ambiguity due to the difficulty in\nthe radiological diagnosis. Considering that invasive pathological analysis\nserves as the clinical golden standard of lung cancer diagnosis, in this study,\nwe solve the label ambiguity issue via a large-scale radio-pathomics dataset\ncontaining 5,134 radiological CT images with pathologically confirmed labels,\nincluding cancers (e.g., invasive/non-invasive adenocarcinoma, squamous\ncarcinoma) and non-cancer diseases (e.g., tuberculosis, hamartoma). This\nretrospective dataset, named Pulmonary-RadPath, enables development and\nvalidation of accurate deep learning systems to predict invasive pathological\nlabels with a non-invasive procedure, i.e., radiological CT scans. A\nthree-level hierarchical classification system for pulmonary lesions is\ndeveloped, which covers most diseases in cancer-related diagnosis. We explore\nseveral techniques for hierarchical classification on this dataset, and propose\na Leaky Dense Hierarchy approach with proven effectiveness in experiments. Our\nstudy significantly outperforms prior arts in terms of data scales (6x larger),\ndisease comprehensiveness and hierarchies. The promising results suggest the\npotentials to facilitate precision medicine.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 15:14:34 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Yang", "Jiancheng", ""], ["Gao", "Mingze", ""], ["Kuang", "Kaiming", ""], ["Ni", "Bingbing", ""], ["She", "Yunlang", ""], ["Xie", "Dong", ""], ["Chen", "Chang", ""]]}, {"id": "2010.04055", "submitter": "Quanshi Zhang", "authors": "Xin Wang, Jie Ren, Shuyun Lin, Xiangming Zhu, Yisen Wang, Quanshi\n  Zhang", "title": "A Unified Approach to Interpreting and Boosting Adversarial\n  Transferability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we use the interaction inside adversarial perturbations to\nexplain and boost the adversarial transferability. We discover and prove the\nnegative correlation between the adversarial transferability and the\ninteraction inside adversarial perturbations. The negative correlation is\nfurther verified through different DNNs with various inputs. Moreover, this\nnegative correlation can be regarded as a unified perspective to understand\ncurrent transferability-boosting methods. To this end, we prove that some\nclassic methods of enhancing the transferability essentially decease\ninteractions inside adversarial perturbations. Based on this, we propose to\ndirectly penalize interactions during the attacking process, which\nsignificantly improves the adversarial transferability.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 15:19:22 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Wang", "Xin", ""], ["Ren", "Jie", ""], ["Lin", "Shuyun", ""], ["Zhu", "Xiangming", ""], ["Wang", "Yisen", ""], ["Zhang", "Quanshi", ""]]}, {"id": "2010.04065", "submitter": "Yehuda Dar", "authors": "Veronica Corona, Yehuda Dar, Guy Williams, Carola-Bibiane Sch\\\"onlieb", "title": "Regularized Compression of MRI Data: Modular Optimization of Joint\n  Reconstruction and Coding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Magnetic Resonance Imaging (MRI) processing chain starts with a critical\nacquisition stage that provides raw data for reconstruction of images for\nmedical diagnosis. This flow usually includes a near-lossless data compression\nstage that enables digital storage and/or transmission in binary formats. In\nthis work we propose a framework for joint optimization of the MRI\nreconstruction and lossy compression, producing compressed representations of\nmedical images that achieve improved trade-offs between quality and bit-rate.\nMoreover, we demonstrate that lossy compression can even improve the\nreconstruction quality compared to settings based on lossless compression. Our\nmethod has a modular optimization structure, implemented using the alternating\ndirection method of multipliers (ADMM) technique and the state-of-the-art image\ncompression technique (BPG) as a black-box module iteratively applied. This\nestablishes a medical data compression approach compatible with a lossy\ncompression standard of choice. A main novelty of the proposed algorithm is in\nthe total-variation regularization added to the modular compression process,\nleading to decompressed images of higher quality without any additional\nprocessing at/after the decompression stage. Our experiments show that our\nregularization-based approach for joint MRI reconstruction and compression\noften achieves significant PSNR gains between 4 to 9 dB at high bit-rates\ncompared to non-regularized solutions of the joint task. Compared to\nregularization-based solutions, our optimization method provides PSNR gains\nbetween 0.5 to 1 dB at high bit-rates, which is the range of interest for\nmedical image compression.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 15:32:52 GMT"}, {"version": "v2", "created": "Mon, 9 Nov 2020 15:01:34 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Corona", "Veronica", ""], ["Dar", "Yehuda", ""], ["Williams", "Guy", ""], ["Sch\u00f6nlieb", "Carola-Bibiane", ""]]}, {"id": "2010.04072", "submitter": "Giulia Orr\\`u", "authors": "Giulia Orr\\`u, Marco Micheletto, Julian Fierrez, Gian Luca Marcialis", "title": "Are Adaptive Face Recognition Systems still Necessary? Experiments on\n  the APE Dataset", "comments": "Preprint version of a paper accepted at IPAS 2020 (Fourth IEEE\n  International Conference on Image Processing, Applications and Systems)", "journal-ref": "2020 IEEE 4th International Conference on Image Processing,\n  Applications and Systems (IPAS), Genova, Italy, 2020, pp. 77-82", "doi": "10.1109/IPAS50080.2020.9334946", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last five years, deep learning methods, in particular CNN, have\nattracted considerable attention in the field of face-based recognition,\nachieving impressive results. Despite this progress, it is not yet clear\nprecisely to what extent deep features are able to follow all the intra-class\nvariations that the face can present over time. In this paper we investigate\nthe performance the performance improvement of face recognition systems by\nadopting self updating strategies of the face templates. For that purpose, we\nevaluate the performance of a well-known deep-learning face representation,\nnamely, FaceNet, on a dataset that we generated explicitly conceived to embed\nintra-class variations of users on a large time span of captures: the\nAPhotoEveryday (APE) dataset. Moreover, we compare these deep features with\nhandcrafted features extracted using the BSIF algorithm. In both cases, we\nevaluate various template update strategies, in order to detect the most useful\nfor such kind of features. Experimental results show the effectiveness of\n\"optimized\" self-update methods with respect to systems without update or\nrandom selection of templates.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 15:45:55 GMT"}, {"version": "v2", "created": "Sat, 17 Oct 2020 14:36:11 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Orr\u00f9", "Giulia", ""], ["Micheletto", "Marco", ""], ["Fierrez", "Julian", ""], ["Marcialis", "Gian Luca", ""]]}, {"id": "2010.04075", "submitter": "Giorgia Pitteri", "authors": "Giorgia Pitteri, Aur\\'elie Bugeau, Slobodan Ilic, Vincent Lepetit", "title": "3D Object Detection and Pose Estimation of Unseen Objects in Color\n  Images with Local Surface Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach for detecting and estimating the 3D poses of objects\nin images that requires only an untextured CAD model and no training phase for\nnew objects. Our approach combines Deep Learning and 3D geometry: It relies on\nan embedding of local 3D geometry to match the CAD models to the input images.\nFor points at the surface of objects, this embedding can be computed directly\nfrom the CAD model; for image locations, we learn to predict it from the image\nitself. This establishes correspondences between 3D points on the CAD model and\n2D locations of the input images. However, many of these correspondences are\nambiguous as many points may have similar local geometries. We show that we can\nuse Mask-RCNN in a class-agnostic way to detect the new objects without\nretraining and thus drastically limit the number of possible correspondences.\nWe can then robustly estimate a 3D pose from these discriminative\ncorrespondences using a RANSAC- like algorithm. We demonstrate the performance\nof this approach on the T-LESS dataset, by using a small number of objects to\nlearn the embedding and testing it on the other objects. Our experiments show\nthat our method is on par or better than previous methods.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 15:57:06 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Pitteri", "Giorgia", ""], ["Bugeau", "Aur\u00e9lie", ""], ["Ilic", "Slobodan", ""], ["Lepetit", "Vincent", ""]]}, {"id": "2010.04143", "submitter": "Louis-Philippe Asselin", "authors": "Louis-Philippe Asselin, Denis Laurendeau, Jean-Fran\\c{c}ois Lalonde", "title": "Deep SVBRDF Estimation on Real Materials", "comments": "Accepted submission to 3DV 2020. Project page\n  https://lvsn.github.io/real-svbrdf", "journal-ref": null, "doi": "10.1109/3DV50981.2020.00126", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work has demonstrated that deep learning approaches can successfully\nbe used to recover accurate estimates of the spatially-varying BRDF (SVBRDF) of\na surface from as little as a single image. Closer inspection reveals, however,\nthat most approaches in the literature are trained purely on synthetic data,\nwhich, while diverse and realistic, is often not representative of the richness\nof the real world. In this paper, we show that training such networks\nexclusively on synthetic data is insufficient to achieve adequate results when\ntested on real data. Our analysis leverages a new dataset of real materials\nobtained with a novel portable multi-light capture apparatus. Through an\nextensive series of experiments and with the use of a novel deep learning\narchitecture, we explore two strategies for improving results on real data:\nfinetuning, and a per-material optimization procedure. We show that adapting\nnetwork weights to real data is of critical importance, resulting in an\napproach which significantly outperforms previous methods for SVBRDF estimation\non real materials. Dataset and code are available at\nhttps://lvsn.github.io/real-svbrdf\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 17:41:26 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Asselin", "Louis-Philippe", ""], ["Laurendeau", "Denis", ""], ["Lalonde", "Jean-Fran\u00e7ois", ""]]}, {"id": "2010.04145", "submitter": "Zuheng Ming", "authors": "Zuheng Ming, Muriel Visani, Muhammad Muzzamil Luqman, Jean-Christophe\n  Burie", "title": "A Survey On Anti-Spoofing Methods For Face Recognition with RGB Cameras\n  of Generic Consumer Devices", "comments": "one-column, 53 pages,43 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The widespread deployment of face recognition-based biometric systems has\nmade face Presentation Attack Detection (face anti-spoofing) an increasingly\ncritical issue. This survey thoroughly investigates the face Presentation\nAttack Detection (PAD) methods, that only require RGB cameras of generic\nconsumer devices, over the past two decades. We present an attack\nscenario-oriented typology of the existing face PAD methods and we provide a\nreview of over 50 of the most recent face PAD methods and their related issues.\nWe adopt a comprehensive presentation of the methods that have most influenced\nface PAD following the proposed typology, and in chronological order. By doing\nso, we depict the main challenges, evolutions and current trends in the field\nof face PAD, and provide insights on its future research. From an experimental\npoint of view, this survey paper provides a summarized overview of the\navailable public databases and extensive comparative experimental results of\ndifferent PAD methods.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 17:44:30 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Ming", "Zuheng", ""], ["Visani", "Muriel", ""], ["Luqman", "Muhammad Muzzamil", ""], ["Burie", "Jean-Christophe", ""]]}, {"id": "2010.04159", "submitter": "Jifeng Dai", "authors": "Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, Jifeng Dai", "title": "Deformable DETR: Deformable Transformers for End-to-End Object Detection", "comments": "ICLR 2021 Oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  DETR has been recently proposed to eliminate the need for many hand-designed\ncomponents in object detection while demonstrating good performance. However,\nit suffers from slow convergence and limited feature spatial resolution, due to\nthe limitation of Transformer attention modules in processing image feature\nmaps. To mitigate these issues, we proposed Deformable DETR, whose attention\nmodules only attend to a small set of key sampling points around a reference.\nDeformable DETR can achieve better performance than DETR (especially on small\nobjects) with 10 times less training epochs. Extensive experiments on the COCO\nbenchmark demonstrate the effectiveness of our approach. Code is released at\nhttps://github.com/fundamentalvision/Deformable-DETR.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 17:59:21 GMT"}, {"version": "v2", "created": "Mon, 30 Nov 2020 13:30:43 GMT"}, {"version": "v3", "created": "Wed, 17 Mar 2021 08:22:51 GMT"}, {"version": "v4", "created": "Thu, 18 Mar 2021 03:14:26 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Zhu", "Xizhou", ""], ["Su", "Weijie", ""], ["Lu", "Lewei", ""], ["Li", "Bin", ""], ["Wang", "Xiaogang", ""], ["Dai", "Jifeng", ""]]}, {"id": "2010.04203", "submitter": "Marcus Valtonen \\\"Ornhag", "authors": "Marcus Valtonen \\\"Ornhag and Patrik Persson and M{\\aa}rten Wadenb\\\"ack\n  and Kalle {\\AA}str\\\"om and Anders Heyden", "title": "Efficient Real-Time Radial Distortion Correction for UAVs", "comments": null, "journal-ref": "Proceedings of the IEEE/CVF Winter Conference on Applications of\n  Computer Vision (WACV), 2021, pp. 1751-1760", "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a novel algorithm for onboard radial distortion\ncorrection for unmanned aerial vehicles (UAVs) equipped with an inertial\nmeasurement unit (IMU), that runs in real-time. This approach makes calibration\nprocedures redundant, thus allowing for exchange of optics extemporaneously. By\nutilizing the IMU data, the cameras can be aligned with the gravity direction.\nThis allows us to work with fewer degrees of freedom, and opens up for further\nintrinsic calibration. We propose a fast and robust minimal solver for\nsimultaneously estimating the focal length, radial distortion profile and\nmotion parameters from homographies. The proposed solver is tested on both\nsynthetic and real data, and perform better or on par with state-of-the-art\nmethods relying on pre-calibration procedures.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 18:34:56 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["\u00d6rnhag", "Marcus Valtonen", ""], ["Persson", "Patrik", ""], ["Wadenb\u00e4ck", "M\u00e5rten", ""], ["\u00c5str\u00f6m", "Kalle", ""], ["Heyden", "Anders", ""]]}, {"id": "2010.04225", "submitter": "Ryan Omidi", "authors": "Ryan Omidi, Ali Moghimi, Alireza Pourreza, Mohamed El-Hadedy, Anas\n  Salah Eddin", "title": "Ensemble Hyperspectral Band Selection for Detecting Nitrogen Status in\n  Grape Leaves", "comments": "8 PAGES, ACCEPTED IN 19TH IEEE INTERNATIONAL CONFERENCE ON MACHINE\n  LEARNING AND APPLICATIONS (ICMLA 2020), MODIFIED ONE AUTHOR NAME", "journal-ref": null, "doi": "10.1109/ICMLA51294.2020.00054", "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The large data size and dimensionality of hyperspectral data demands complex\nprocessing and data analysis. Multispectral data do not suffer the same\nlimitations, but are normally restricted to blue, green, red, red edge, and\nnear infrared bands. This study aimed to identify the optimal set of spectral\nbands for nitrogen detection in grape leaves using ensemble feature selection\non hyperspectral data from over 3,000 leaves from 150 Flame Seedless table\ngrapevines. Six machine learning base rankers were included in the ensemble:\nrandom forest, LASSO, SelectKBest, ReliefF, SVM-RFE, and chaotic crow search\nalgorithm (CCSA). The pipeline identified less than 0.45% of the bands as most\ninformative about grape nitrogen status. The selected violet, yellow-orange,\nand shortwave infrared bands lie outside of the typical blue, green, red, red\nedge, and near infrared bands of commercial multispectral cameras, so the\npotential improvement in remote sensing of nitrogen in grapevines brought forth\nby a customized multispectral sensor centered at the selected bands is\npromising and worth further investigation. The proposed pipeline may also be\nused for application-specific multispectral sensor design in domains other than\nagriculture.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 19:09:10 GMT"}, {"version": "v2", "created": "Mon, 12 Oct 2020 01:17:28 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Omidi", "Ryan", ""], ["Moghimi", "Ali", ""], ["Pourreza", "Alireza", ""], ["El-Hadedy", "Mohamed", ""], ["Eddin", "Anas Salah", ""]]}, {"id": "2010.04257", "submitter": "Akbar Siami Namin", "authors": "Varsha Nair, Moitrayee Chatterjee, Neda Tavakoli, Akbar Siami Namin,\n  Craig Snoeyink", "title": "Fast Fourier Transformation for Optimizing Convolutional Neural Networks\n  in Object Recognition", "comments": "Pre-print of a paper to appear in the proceedings of the IEEE\n  International Conference on Machine Learning Applications (ICMLA 2020), 10\n  pages, 9 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes to use Fast Fourier Transformation-based U-Net (a refined\nfully convolutional networks) and perform image convolution in neural networks.\nLeveraging the Fast Fourier Transformation, it reduces the image convolution\ncosts involved in the Convolutional Neural Networks (CNNs) and thus reduces the\noverall computational costs. The proposed model identifies the object\ninformation from the images. We apply the Fast Fourier transform algorithm on\nan image data set to obtain more accessible information about the image data,\nbefore segmenting them through the U-Net architecture. More specifically, we\nimplement the FFT-based convolutional neural network to improve the training\ntime of the network. The proposed approach was applied to publicly available\nBroad Bioimage Benchmark Collection (BBBC) dataset. Our model demonstrated\nimprovement in training time during convolution from $600-700$ ms/step to\n$400-500$ ms/step. We evaluated the accuracy of our model using Intersection\nover Union (IoU) metric showing significant improvements.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 21:07:55 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Nair", "Varsha", ""], ["Chatterjee", "Moitrayee", ""], ["Tavakoli", "Neda", ""], ["Namin", "Akbar Siami", ""], ["Snoeyink", "Craig", ""]]}, {"id": "2010.04278", "submitter": "Ivan Sipiran", "authors": "Alexis Mendoza, Alexander Apaza, Ivan Sipiran, Cristian Lopez", "title": "Refinement of Predicted Missing Parts Enhance Point Cloud Completion", "comments": "11 pages, 6 figures, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point cloud completion is the task of predicting complete geometry from\npartial observations using a point set representation for a 3D shape. Previous\napproaches propose neural networks to directly estimate the whole point cloud\nthrough encoder-decoder models fed by the incomplete point set. By predicting\nthe complete model, the current methods compute redundant information because\nthe output also contains the known incomplete input geometry. This paper\nproposes an end-to-end neural network architecture that focuses on computing\nthe missing geometry and merging the known input and the predicted point cloud.\nOur method is composed of two neural networks: the missing part prediction\nnetwork and the merging-refinement network. The first module focuses on\nextracting information from the incomplete input to infer the missing geometry.\nThe second module merges both point clouds and improves the distribution of the\npoints. Our experiments on ShapeNet dataset show that our method outperforms\nthe state-of-the-art methods in point cloud completion. The code of our methods\nand experiments is available in\n\\url{https://github.com/ivansipiran/Refinement-Point-Cloud-Completion}.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 22:01:23 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Mendoza", "Alexis", ""], ["Apaza", "Alexander", ""], ["Sipiran", "Ivan", ""], ["Lopez", "Cristian", ""]]}, {"id": "2010.04283", "submitter": "Laurent Chauvin", "authors": "L. Chauvin, M. Ben Lazreg, J.B. Carluer, W. Wells, M. Toews", "title": "Large Scale Indexing of Generic Medical Image Data using Unbiased\n  Shallow Keypoints and Deep CNN Features", "comments": "11 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We propose a unified appearance model accounting for traditional shallow\n(i.e. 3D SIFT keypoints) and deep (i.e. CNN output layers) image feature\nrepresentations, encoding respectively specific, localized neuroanatomical\npatterns and rich global information into a single indexing and classification\nframework. A novel Bayesian model combines shallow and deep features based on\nan assumption of conditional independence and validated by experiments indexing\nspecific family members and general group categories in 3D MRI neuroimage data\nof 1010 subjects from the Human Connectome Project, including twins and\nnon-twin siblings. A novel domain adaptation strategy is presented,\ntransforming deep CNN vectors elements into binary class-informative\ndescriptors. A GPU-based implementation of all processing is provided.\nState-of-the-art performance is achieved in large-scale neuroimage indexing,\nboth in terms of computational complexity, accuracy in identifying family\nmembers and sex classification.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 22:15:52 GMT"}, {"version": "v2", "created": "Tue, 20 Oct 2020 19:31:49 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Chauvin", "L.", ""], ["Lazreg", "M. Ben", ""], ["Carluer", "J. B.", ""], ["Wells", "W.", ""], ["Toews", "M.", ""]]}, {"id": "2010.04308", "submitter": "Wei-Hung Weng", "authors": "Wei-Hung Weng, Jonathan Deaton, Vivek Natarajan, Gamaleldin F.\n  Elsayed, Yuan Liu", "title": "Addressing the Real-world Class Imbalance Problem in Dermatology", "comments": "Machine Learning for Health Workshop at NeurIPS 2020; 14 pages + 4\n  pages appendix, 8 figures, 6 appendix tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Class imbalance is a common problem in medical diagnosis, causing a standard\nclassifier to be biased towards the common classes and perform poorly on the\nrare classes. This is especially true for dermatology, a specialty with\nthousands of skin conditions but many of which have low prevalence in the real\nworld. Motivated by recent advances, we explore few-shot learning methods as\nwell as conventional class imbalance techniques for the skin condition\nrecognition problem and propose an evaluation setup to fairly assess the\nreal-world utility of such approaches. We find the performance of few-show\nlearning methods does not reach that of conventional class imbalance\ntechniques, but combining the two approaches using a novel ensemble improves\nmodel performance, especially for rare classes. We conclude that ensembling can\nbe useful to address the class imbalance problem, yet progress can further be\naccelerated by real-world evaluation setups for benchmarking new methods.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 00:24:55 GMT"}, {"version": "v2", "created": "Sat, 14 Nov 2020 03:45:48 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Weng", "Wei-Hung", ""], ["Deaton", "Jonathan", ""], ["Natarajan", "Vivek", ""], ["Elsayed", "Gamaleldin F.", ""], ["Liu", "Yuan", ""]]}, {"id": "2010.04324", "submitter": "Xin Feng", "authors": "Xin Feng, Wenjie Pei, Zihui Jia, Fanglin Chen, David Zhang, and\n  Guangming Lu", "title": "Deep-Masking Generative Network: A Unified Framework for Background\n  Restoration from Superimposed Images", "comments": "16 pages, accepted for publication in IEEE Transactions on Image\n  Processing (TIP)", "journal-ref": null, "doi": "10.1109/TIP.2021.3076589", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Restoring the clean background from the superimposed images containing a\nnoisy layer is the common crux of a classical category of tasks on image\nrestoration such as image reflection removal, image deraining and image\ndehazing. These tasks are typically formulated and tackled individually due to\nthe diverse and complicated appearance patterns of noise layers within the\nimage. In this work we present the Deep-Masking Generative Network (DMGN),\nwhich is a unified framework for background restoration from the superimposed\nimages and is able to cope with different types of noise. Our proposed DMGN\nfollows a coarse-to-fine generative process: a coarse background image and a\nnoise image are first generated in parallel, then the noise image is further\nleveraged to refine the background image to achieve a higher-quality background\nimage. In particular, we design the novel Residual Deep-Masking Cell as the\ncore operating unit for our DMGN to enhance the effective information and\nsuppress the negative information during image generation via learning a gating\nmask to control the information flow. By iteratively employing this Residual\nDeep-Masking Cell, our proposed DMGN is able to generate both high-quality\nbackground image and noisy image progressively. Furthermore, we propose a\ntwo-pronged strategy to effectively leverage the generated noise image as\ncontrasting cues to facilitate the refinement of the background image.\nExtensive experiments across three typical tasks for image background\nrestoration, including image reflection removal, image rain steak removal and\nimage dehazing, show that our DMGN consistently outperforms state-of-the-art\nmethods specifically designed for each single task.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 01:47:52 GMT"}, {"version": "v2", "created": "Mon, 12 Apr 2021 09:47:26 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Feng", "Xin", ""], ["Pei", "Wenjie", ""], ["Jia", "Zihui", ""], ["Chen", "Fanglin", ""], ["Zhang", "David", ""], ["Lu", "Guangming", ""]]}, {"id": "2010.04331", "submitter": "Xinghao Yang", "authors": "Xinghao Yang, Weifeng Liu, Shengli Zhang, Wei Liu, Dacheng Tao", "title": "Targeted Attention Attack on Deep Learning Models in Road Sign\n  Recognition", "comments": "11 pages, 4 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real world traffic sign recognition is an important step towards building\nautonomous vehicles, most of which highly dependent on Deep Neural Networks\n(DNNs). Recent studies demonstrated that DNNs are surprisingly susceptible to\nadversarial examples. Many attack methods have been proposed to understand and\ngenerate adversarial examples, such as gradient based attack, score based\nattack, decision based attack, and transfer based attacks. However, most of\nthese algorithms are ineffective in real-world road sign attack, because (1)\niteratively learning perturbations for each frame is not realistic for a fast\nmoving car and (2) most optimization algorithms traverse all pixels equally\nwithout considering their diverse contribution. To alleviate these problems,\nthis paper proposes the targeted attention attack (TAA) method for real world\nroad sign attack. Specifically, we have made the following contributions: (1)\nwe leverage the soft attention map to highlight those important pixels and skip\nthose zero-contributed areas - this also helps to generate natural\nperturbations, (2) we design an efficient universal attack that optimizes a\nsingle perturbation/noise based on a set of training images under the guidance\nof the pre-trained attention map, (3) we design a simple objective function\nthat can be easily optimized, (4) we evaluate the effectiveness of TAA on real\nworld data sets. Experimental results validate that the TAA method improves the\nattack successful rate (nearly 10%) and reduces the perturbation loss (about a\nquarter) compared with the popular RP2 method. Additionally, our TAA also\nprovides good properties, e.g., transferability and generalization capability.\nWe provide code and data to ensure the reproducibility:\nhttps://github.com/AdvAttack/RoadSignAttack.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 02:31:34 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Yang", "Xinghao", ""], ["Liu", "Weifeng", ""], ["Zhang", "Shengli", ""], ["Liu", "Wei", ""], ["Tao", "Dacheng", ""]]}, {"id": "2010.04339", "submitter": "Aditya Ganapathi", "authors": "Aditya Ganapathi, Priya Sundaresan, Brijen Thananjeyan, Ashwin\n  Balakrishna, Daniel Seita, Ryan Hoque, Joseph E. Gonzalez, Ken Goldberg", "title": "MMGSD: Multi-Modal Gaussian Shape Descriptors for Correspondence\n  Matching in 1D and 2D Deformable Objects", "comments": "IROS 2020 Workshop on Managing Deformation: A Step Towards Higher\n  Robot Autonomy", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore learning pixelwise correspondences between images of deformable\nobjects in different configurations. Traditional correspondence matching\napproaches such as SIFT, SURF, and ORB can fail to provide sufficient\ncontextual information for fine-grained manipulation. We propose Multi-Modal\nGaussian Shape Descriptor (MMGSD), a new visual representation of deformable\nobjects which extends ideas from dense object descriptors to predict all\nsymmetric correspondences between different object configurations. MMGSD is\nlearned in a self-supervised manner from synthetic data and produces\ncorrespondence heatmaps with measurable uncertainty. In simulation, experiments\nsuggest that MMGSD can achieve an RMSE of 32.4 and 31.3 for square cloth and\nbraided synthetic nylon rope respectively. The results demonstrate an average\nof 47.7% improvement over a provided baseline based on contrastive learning,\nsymmetric pixel-wise contrastive loss (SPCL), as opposed to MMGSD which\nenforces distributional continuity.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 03:11:13 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Ganapathi", "Aditya", ""], ["Sundaresan", "Priya", ""], ["Thananjeyan", "Brijen", ""], ["Balakrishna", "Ashwin", ""], ["Seita", "Daniel", ""], ["Hoque", "Ryan", ""], ["Gonzalez", "Joseph E.", ""], ["Goldberg", "Ken", ""]]}, {"id": "2010.04354", "submitter": "Mingzhu Shen", "authors": "Mingzhu Shen, Feng Liang, Chuming Li, Chen Lin, Ming Sun, Junjie Yan,\n  Wanli Ouyang", "title": "Once Quantized for All: Progressively Searching for Quantized Efficient\n  Models", "comments": "The first two authors contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic search of Quantized Neural Networks has attracted a lot of\nattention. However, the existing quantization aware Neural Architecture Search\n(NAS) approaches inherit a two-stage search-retrain schema, which is not only\ntime-consuming but also adversely affected by the unreliable ranking of\narchitectures during the search. To avoid the undesirable effect of the\nsearch-retrain schema, we present Once Quantized for All (OQA), a novel\nframework that searches for quantized efficient models and deploys their\nquantized weights at the same time without additional post-process. While\nsupporting a huge architecture search space, our OQA can produce a series of\nultra-low bit-width(e.g. 4/3/2 bit) quantized efficient models. A progressive\nbit inheritance procedure is introduced to support ultra-low bit-width. Our\ndiscovered model family, OQANets, achieves a new state-of-the-art (SOTA) on\nquantized efficient models compared with various quantization methods and\nbit-widths. In particular, OQA2bit-L achieves 64.0% ImageNet Top-1 accuracy,\noutperforming its 2-bit counterpart EfficientNet-B0@QKD by a large margin of\n14% using 30% less computation budget. Code is available at\nhttps://github.com/LaVieEnRoseSMZ/OQA.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 03:52:16 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Shen", "Mingzhu", ""], ["Liang", "Feng", ""], ["Li", "Chuming", ""], ["Lin", "Chen", ""], ["Sun", "Ming", ""], ["Yan", "Junjie", ""], ["Ouyang", "Wanli", ""]]}, {"id": "2010.04363", "submitter": "Zhiwei Xu", "authors": "Zhiwei Xu, Thalaiyasingam Ajanthan, Richard Hartley", "title": "Refining Semantic Segmentation with Superpixel by Transparent\n  Initialization and Sparse Encoder", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although deep learning greatly improves the performance of semantic\nsegmentation, its success mainly lies in object central areas without accurate\nedges. As superpixels are a popular and effective auxiliary to preserve object\nedges, in this paper, we jointly learn semantic segmentation with trainable\nsuperpixels. We achieve it with fully-connected layers with Transparent\nInitialization (TI) and efficient logit consistency using a sparse encoder. The\nproposed TI preserves the effects of learned parameters of pretrained networks.\nThis avoids a significant increase of the loss of pretrained networks, which\notherwise may be caused by inappropriate parameter initialization of the\nadditional layers. Meanwhile, consistent pixel labels in each superpixel are\nguaranteed by logit consistency. The sparse encoder with sparse matrix\noperations substantially reduces both the memory requirement and the\ncomputational complexity. We demonstrated the superiority of TI over other\nparameter initialization methods and tested its numerical stability. The\neffectiveness of our proposal was validated on PASCAL VOC 2012, ADE20K, and\nPASCAL Context showing enhanced semantic segmentation edges. With quantitative\nevaluations on segmentation edges using performance ratio and F-measure, our\nmethod outperforms the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 04:20:54 GMT"}, {"version": "v2", "created": "Sat, 7 Nov 2020 23:36:29 GMT"}, {"version": "v3", "created": "Tue, 24 Nov 2020 10:14:58 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Xu", "Zhiwei", ""], ["Ajanthan", "Thalaiyasingam", ""], ["Hartley", "Richard", ""]]}, {"id": "2010.04365", "submitter": "Zhou Fang", "authors": "Zhou Fang, Tianren Yang, Ying Jin", "title": "DeepStreet: A deep learning powered urban street network generation\n  module", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In countries experiencing unprecedented waves of urbanization, there is a\nneed for rapid and high quality urban street design. Our study presents a novel\ndeep learning powered approach, DeepStreet (DS), for automatic street network\ngeneration that can be applied to the urban street design with local\ncharacteristics. DS is driven by a Convolutional Neural Network (CNN) that\nenables the interpolation of streets based on the areas of immediate vicinity.\nSpecifically, the CNN is firstly trained to detect, recognize and capture the\nlocal features as well as the patterns of the existing street network sourced\nfrom the OpenStreetMap. With the trained CNN, DS is able to predict street\nnetworks' future expansion patterns within the predefined region conditioned on\nits surrounding street networks. To test the performance of DS, we apply it to\nan area in and around the Eixample area in the City of Barcelona, a well known\nexample in the fields of urban and transport planning with iconic grid like\nstreet networks in the centre and irregular road alignments farther afield. The\nresults show that DS can (1) detect and self cluster different types of complex\nstreet patterns in Barcelona; (2) predict both gridiron and irregular street\nand road networks. DS proves to have a great potential as a novel tool for\ndesigners to efficiently design the urban street network that well maintains\nthe consistency across the existing and newly generated urban street network.\nFurthermore, the generated networks can serve as a benchmark to guide the local\nplan-making especially in rapidly developing cities.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 04:27:41 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Fang", "Zhou", ""], ["Yang", "Tianren", ""], ["Jin", "Ying", ""]]}, {"id": "2010.04367", "submitter": "Jianing Qian", "authors": "Jianing Qian, Junyu Nan, Siddharth Ancha, Brian Okorn, David Held", "title": "Robust Instance Tracking via Uncertainty Flow", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current state-of-the-art trackers often fail due to distractorsand large\nobject appearance changes. In this work, we explore the use ofdense optical\nflow to improve tracking robustness. Our main insight is that, because flow\nestimation can also have errors, we need to incorporate an estimate of flow\nuncertainty for robust tracking. We present a novel tracking framework which\ncombines appearance and flow uncertainty information to track objects in\nchallenging scenarios. We experimentally verify that our framework improves\ntracking robustness, leading to new state-of-the-art results. Further, our\nexperimental ablations shows the importance of flow uncertainty for robust\ntracking.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 04:39:19 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Qian", "Jianing", ""], ["Nan", "Junyu", ""], ["Ancha", "Siddharth", ""], ["Okorn", "Brian", ""], ["Held", "David", ""]]}, {"id": "2010.04368", "submitter": "Mohammad Sadegh Aliakbarian", "authors": "Sadegh Aliakbarian", "title": "Deep Sequence Learning for Video Anticipation: From Discrete and\n  Deterministic to Continuous and Stochastic", "comments": "The draft of my PhD thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video anticipation is the task of predicting one/multiple future\nrepresentation(s) given limited, partial observation. This is a challenging\ntask due to the fact that given limited observation, the future representation\ncan be highly ambiguous. Based on the nature of the task, video anticipation\ncan be considered from two viewpoints: the level of details and the level of\ndeterminism in the predicted future. In this research, we start from\nanticipating a coarse representation of a deterministic future and then move\ntowards predicting continuous and fine-grained future representations of a\nstochastic process. The example of the former is video action anticipation in\nwhich we are interested in predicting one action label given a partially\nobserved video and the example of the latter is forecasting multiple diverse\ncontinuations of human motion given partially observed one. In particular, in\nthis thesis, we make several contributions to the literature of video\nanticipation...\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 04:40:58 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Aliakbarian", "Sadegh", ""]]}, {"id": "2010.04382", "submitter": "Perry Deng", "authors": "Perry Deng, Cooper Linsky, Matthew Wright", "title": "Weaponizing Unicodes with Deep Learning -- Identifying Homoglyphs with\n  Weakly Labeled Data", "comments": "Updated DOI", "journal-ref": null, "doi": "10.1109/ISI49825.2020.9280538", "report-no": null, "categories": "cs.CR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visually similar characters, or homoglyphs, can be used to perform social\nengineering attacks or to evade spam and plagiarism detectors. It is thus\nimportant to understand the capabilities of an attacker to identify homoglyphs\n-- particularly ones that have not been previously spotted -- and leverage them\nin attacks. We investigate a deep-learning model using embedding learning,\ntransfer learning, and augmentation to determine the visual similarity of\ncharacters and thereby identify potential homoglyphs. Our approach uniquely\ntakes advantage of weak labels that arise from the fact that most characters\nare not homoglyphs. Our model drastically outperforms the Normalized\nCompression Distance approach on pairwise homoglyph identification, for which\nwe achieve an average precision of 0.97. We also present the first attempt at\nclustering homoglyphs into sets of equivalence classes, which is more efficient\nthan pairwise information for security practitioners to quickly lookup\nhomoglyphs or to normalize confusable string encodings. To measure clustering\nperformance, we propose a metric (mBIOU) building on the classic\nIntersection-Over-Union (IOU) metric. Our clustering method achieves 0.592\nmBIOU, compared to 0.430 for the naive baseline. We also use our model to\npredict over 8,000 previously unknown homoglyphs, and find good early\nindications that many of these may be true positives. Source code and list of\npredicted homoglyphs are uploaded to Github:\nhttps://github.com/PerryXDeng/weaponizing_unicode\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 06:03:18 GMT"}, {"version": "v2", "created": "Mon, 12 Oct 2020 16:45:21 GMT"}, {"version": "v3", "created": "Tue, 13 Oct 2020 03:13:22 GMT"}, {"version": "v4", "created": "Tue, 22 Dec 2020 18:11:46 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Deng", "Perry", ""], ["Linsky", "Cooper", ""], ["Wright", "Matthew", ""]]}, {"id": "2010.04384", "submitter": "Pengpeng Liu", "authors": "Pengpeng Liu, Xintong Han, Michael Lyu, Irwin King, Jia Xu", "title": "Learning 3D Face Reconstruction with a Pose Guidance Network", "comments": "ACCV 2020 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a self-supervised learning approach to learning monocular 3D face\nreconstruction with a pose guidance network (PGN). First, we unveil the\nbottleneck of pose estimation in prior parametric 3D face learning methods, and\npropose to utilize 3D face landmarks for estimating pose parameters. With our\nspecially designed PGN, our model can learn from both faces with fully labeled\n3D landmarks and unlimited unlabeled in-the-wild face images. Our network is\nfurther augmented with a self-supervised learning scheme, which exploits face\ngeometry information embedded in multiple frames of the same person, to\nalleviate the ill-posed nature of regressing 3D face geometry from a single\nimage. These three insights yield a single approach that combines the\ncomplementary strengths of parametric model learning and data-driven learning\ntechniques. We conduct a rigorous evaluation on the challenging AFLW2000-3D,\nFlorence and FaceWarehouse datasets, and show that our method outperforms the\nstate-of-the-art for all metrics.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 06:11:17 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Liu", "Pengpeng", ""], ["Han", "Xintong", ""], ["Lyu", "Michael", ""], ["King", "Irwin", ""], ["Xu", "Jia", ""]]}, {"id": "2010.04402", "submitter": "Seung-Won Park", "authors": "Seung-won Park", "title": "Generating Novel Glyph without Human Data by Learning to Communicate", "comments": "To appear at 4th Workshop on Machine Learning for Creativity and\n  Design at NeurIPS 2020; 6 pages with 4 figures and 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present Neural Glyph, a system that generates novel glyph\nwithout any training data. The generator and the classifier are trained to\ncommunicate via visual symbols as a medium, which enforces the generator to\ncome up with a set of distinctive symbols. Our method results in glyphs that\nresemble the human-made glyphs, which may imply that the visual appearances of\nexisting glyphs can be attributed to constraints of communication via writing.\nImportant tricks that enable this framework are described and the code is made\navailable.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 07:18:36 GMT"}, {"version": "v2", "created": "Sun, 22 Nov 2020 12:57:16 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Park", "Seung-won", ""]]}, {"id": "2010.04413", "submitter": "Yao Li", "authors": "Yao Li, Xianggang Yu, Xiaoguang Han, Nianjuan Jiang, Kui Jia, Jiangbo\n  Lu", "title": "A deep learning based interactive sketching system for fashion images\n  design", "comments": "6 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose an interactive system to design diverse high-quality\ngarment images from fashion sketches and the texture information. The major\nchallenge behind this system is to generate high-quality and detailed texture\naccording to the user-provided texture information. Prior works mainly use the\ntexture patch representation and try to map a small texture patch to a whole\ngarment image, hence unable to generate high-quality details. In contrast,\ninspired by intrinsic image decomposition, we decompose this task into texture\nsynthesis and shading enhancement. In particular, we propose a novel bi-colored\nedge texture representation to synthesize textured garment images and a shading\nenhancer to render shading based on the grayscale edges. The bi-colored edge\nrepresentation provides simple but effective texture cues and color\nconstraints, so that the details can be better reconstructed. Moreover, with\nthe rendered shading, the synthesized garment image becomes more vivid.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 07:50:56 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Li", "Yao", ""], ["Yu", "Xianggang", ""], ["Han", "Xiaoguang", ""], ["Jiang", "Nianjuan", ""], ["Jia", "Kui", ""], ["Lu", "Jiangbo", ""]]}, {"id": "2010.04421", "submitter": "Yanfeng Lu", "authors": "Jia-Yi Chang, Yan-Feng Lu, Ya-Jun Liu, Bo Zhou, Hong Qiao", "title": "Long-distance tiny face detection based on enhanced YOLOv3 for unmanned\n  system", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Remote tiny face detection applied in unmanned system is a challeng-ing work.\nThe detector cannot obtain sufficient context semantic information due to the\nrelatively long distance. The received poor fine-grained features make the face\ndetection less accurate and robust. To solve the problem of long-distance\ndetection of tiny faces, we propose an enhanced network model (YOLOv3-C) based\non the YOLOv3 algorithm for unmanned platform. In this model, we bring in\nmulti-scale features from feature pyramid networks and make the features\nfu-sion to adjust prediction feature map of the output, which improves the\nsensitivity of the entire algorithm for tiny target faces. The enhanced model\nimproves the accuracy of tiny face detection in the cases of long-distance and\nhigh-density crowds. The experimental evaluation results demonstrated the\nsuperior perfor-mance of the proposed YOLOv3-C in comparison with other\nrelevant detectors in remote tiny face detection. It is worth mentioning that\nour proposed method achieves comparable performance with the state of the art\nYOLOv4[1] in the tiny face detection tasks.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 08:12:58 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Chang", "Jia-Yi", ""], ["Lu", "Yan-Feng", ""], ["Liu", "Ya-Jun", ""], ["Zhou", "Bo", ""], ["Qiao", "Hong", ""]]}, {"id": "2010.04425", "submitter": "Sebastian Van Der Voort", "authors": "Sebastian R. van der Voort, Fatih Incekara, Maarten M.J. Wijnenga,\n  Georgios Kapsas, Renske Gahrmann, Joost W. Schouten, Rishi Nandoe Tewarie,\n  Geert J. Lycklama, Philip C. De Witt Hamer, Roelant S. Eijgelaar, Pim J.\n  French, Hendrikus J. Dubbink, Arnaud J.P.E. Vincent, Wiro J. Niessen, Martin\n  J. van den Bent, Marion Smits, Stefan Klein", "title": "WHO 2016 subtyping and automated segmentation of glioma using multi-task\n  deep learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate characterization of glioma is crucial for clinical decision making.\nA delineation of the tumor is also desirable in the initial decision stages but\nis a time-consuming task. Leveraging the latest GPU capabilities, we developed\na single multi-task convolutional neural network that uses the full 3D,\nstructural, pre-operative MRI scans to can predict the IDH mutation status, the\n1p/19q co-deletion status, and the grade of a tumor, while simultaneously\nsegmenting the tumor. We trained our method using the largest, most diverse\npatient cohort to date containing 1508 glioma patients from 16 institutes. We\ntested our method on an independent dataset of 240 patients from 13 different\ninstitutes, and achieved an IDH-AUC of 0.90, 1p/19q-AUC of 0.85, grade-AUC of\n0.81, and a mean whole tumor DICE score of 0.84. Thus, our method\nnon-invasively predicts multiple, clinically relevant parameters and\ngeneralizes well to the broader clinical population.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 08:18:53 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["van der Voort", "Sebastian R.", ""], ["Incekara", "Fatih", ""], ["Wijnenga", "Maarten M. J.", ""], ["Kapsas", "Georgios", ""], ["Gahrmann", "Renske", ""], ["Schouten", "Joost W.", ""], ["Tewarie", "Rishi Nandoe", ""], ["Lycklama", "Geert J.", ""], ["Hamer", "Philip C. De Witt", ""], ["Eijgelaar", "Roelant S.", ""], ["French", "Pim J.", ""], ["Dubbink", "Hendrikus J.", ""], ["Vincent", "Arnaud J. P. E.", ""], ["Niessen", "Wiro J.", ""], ["Bent", "Martin J. van den", ""], ["Smits", "Marion", ""], ["Klein", "Stefan", ""]]}, {"id": "2010.04427", "submitter": "Keondo Park", "authors": "Keondo Park, Wonyoung Jang, Woochul Lee, Kisung Nam, Kihong Seong,\n  Kyuwook Chai, Wen-Syan Li", "title": "Real-time Mask Detection on Google Edge TPU", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  After the COVID-19 outbreak, it has become important to automatically detect\nwhether people are wearing masks in order to reduce risk of front-line workers.\nIn addition, processing user data locally is a great way to address both\nprivacy and network bandwidth issues. In this paper, we present a\nlight-weighted model for detecting whether people in a particular area wear\nmasks, which can also be deployed on Coral Dev Board, a commercially available\ndevelopment board containing Google Edge TPU. Our approach combines the object\ndetecting network based on MobileNetV2 plus SSD and the quantization scheme for\ninteger-only hardware. As a result, the lighter model in the Edge TPU has a\nsignificantly lower latency which is more appropriate for real-time execution\nwhile maintaining accuracy comparable to a floating point device.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 08:21:34 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Park", "Keondo", ""], ["Jang", "Wonyoung", ""], ["Lee", "Woochul", ""], ["Nam", "Kisung", ""], ["Seong", "Kihong", ""], ["Chai", "Kyuwook", ""], ["Li", "Wen-Syan", ""]]}, {"id": "2010.04428", "submitter": "Yicheng Wu", "authors": "Yicheng Wu, Chengwei Pan, Shuqi Wang, Ming Zhang, Yong Xia, Yizhou Yu", "title": "Rethinking the Extraction and Interaction of Multi-Scale Features for\n  Vessel Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Analyzing the morphological attributes of blood vessels plays a critical role\nin the computer-aided diagnosis of many cardiovascular and ophthalmologic\ndiseases. Although being extensively studied, segmentation of blood vessels,\nparticularly thin vessels and capillaries, remains challenging mainly due to\nthe lack of an effective interaction between local and global features. In this\npaper, we propose a novel deep learning model called PC-Net to segment retinal\nvessels and major arteries in 2D fundus image and 3D computed tomography\nangiography (CTA) scans, respectively. In PC-Net, the pyramid\nsqueeze-and-excitation (PSE) module introduces spatial information to each\nconvolutional block, boosting its ability to extract more effective multi-scale\nfeatures, and the coarse-to-fine (CF) module replaces the conventional decoder\nto enhance the details of thin vessels and process hard-to-classify pixels\nagain. We evaluated our PC-Net on the Digital Retinal Images for Vessel\nExtraction (DRIVE) database and an in-house 3D major artery (3MA) database\nagainst several recent methods. Our results not only demonstrate the\neffectiveness of the proposed PSE module and CF module, but also suggest that\nour proposed PC-Net sets new state of the art in the segmentation of retinal\nvessels (AUC: 98.31%) and major arteries (AUC: 98.35%) on both databases,\nrespectively.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 08:22:54 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Wu", "Yicheng", ""], ["Pan", "Chengwei", ""], ["Wang", "Shuqi", ""], ["Zhang", "Ming", ""], ["Xia", "Yong", ""], ["Yu", "Yizhou", ""]]}, {"id": "2010.04456", "submitter": "Vincent Le-Guen", "authors": "Vincent Le Guen, Yuan Yin, J\\'er\\'emie Dona, Ibrahim Ayed, Emmanuel de\n  B\\'ezenac, Nicolas Thome, Patrick Gallinari", "title": "Augmenting Physical Models with Deep Networks for Complex Dynamics\n  Forecasting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Forecasting complex dynamical phenomena in settings where only partial\nknowledge of their dynamics is available is a prevalent problem across various\nscientific fields. While purely data-driven approaches are arguably\ninsufficient in this context, standard physical modeling based approaches tend\nto be over-simplistic, inducing non-negligible errors. In this work, we\nintroduce the APHYNITY framework, a principled approach for augmenting\nincomplete physical dynamics described by differential equations with deep\ndata-driven models. It consists in decomposing the dynamics into two\ncomponents: a physical component accounting for the dynamics for which we have\nsome prior knowledge, and a data-driven component accounting for errors of the\nphysical model. The learning problem is carefully formulated such that the\nphysical model explains as much of the data as possible, while the data-driven\ncomponent only describes information that cannot be captured by the physical\nmodel, no more, no less. This not only provides the existence and uniqueness\nfor this decomposition, but also ensures interpretability and benefits\ngeneralization. Experiments made on three important use cases, each\nrepresentative of a different family of phenomena, i.e. reaction-diffusion\nequations, wave equations and the non-linear damped pendulum, show that\nAPHYNITY can efficiently leverage approximate physical models to accurately\nforecast the evolution of the system and correctly identify relevant physical\nparameters.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 09:31:03 GMT"}, {"version": "v2", "created": "Wed, 10 Feb 2021 08:49:33 GMT"}, {"version": "v3", "created": "Thu, 1 Apr 2021 18:18:27 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Guen", "Vincent Le", ""], ["Yin", "Yuan", ""], ["Dona", "J\u00e9r\u00e9mie", ""], ["Ayed", "Ibrahim", ""], ["de B\u00e9zenac", "Emmanuel", ""], ["Thome", "Nicolas", ""], ["Gallinari", "Patrick", ""]]}, {"id": "2010.04470", "submitter": "Sunil Gundapu", "authors": "Sunil Gundapu, Radhika Mamidi", "title": "gundapusunil at SemEval-2020 Task 8: Multimodal Memotion Analysis", "comments": "8 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent technological advancements in the Internet and Social media usage have\nresulted in the evolution of faster and efficient platforms of communication.\nThese platforms include visual, textual and speech mediums and have brought a\nunique social phenomenon called Internet memes. Internet memes are in the form\nof images with witty, catchy, or sarcastic text descriptions. In this paper, we\npresent a multi-modal sentiment analysis system using deep neural networks\ncombining Computer Vision and Natural Language Processing. Our aim is different\nthan the normal sentiment analysis goal of predicting whether a text expresses\npositive or negative sentiment; instead, we aim to classify the Internet meme\nas a positive, negative, or neutral, identify the type of humor expressed and\nquantify the extent to which a particular effect is being expressed. Our system\nhas been developed using CNN and LSTM and outperformed the baseline score.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 09:53:14 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Gundapu", "Sunil", ""], ["Mamidi", "Radhika", ""]]}, {"id": "2010.04483", "submitter": "Gangming Zhao", "authors": "Gangming Zhao and Chaowei Fang and Guanbin Li and Licheng Jiao and\n  Yizhou Yu", "title": "Contralaterally Enhanced Networks for Thoracic Disease Detection", "comments": "submitted to TMI", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying and locating diseases in chest X-rays are very challenging, due\nto the low visual contrast between normal and abnormal regions, and distortions\ncaused by other overlapping tissues. An interesting phenomenon is that there\nexist many similar structures in the left and right parts of the chest, such as\nribs, lung fields and bronchial tubes. This kind of similarities can be used to\nidentify diseases in chest X-rays, according to the experience of\nbroad-certificated radiologists. Aimed at improving the performance of existing\ndetection methods, we propose a deep end-to-end module to exploit the\ncontralateral context information for enhancing feature representations of\ndisease proposals. First of all, under the guidance of the spine line, the\nspatial transformer network is employed to extract local contralateral patches,\nwhich can provide valuable context information for disease proposals. Then, we\nbuild up a specific module, based on both additive and subtractive operations,\nto fuse the features of the disease proposal and the contralateral patch. Our\nmethod can be integrated into both fully and weakly supervised disease\ndetection frameworks. It achieves 33.17 AP50 on a carefully annotated private\nchest X-ray dataset which contains 31,000 images. Experiments on the NIH chest\nX-ray dataset indicate that our method achieves state-of-the-art performance in\nweakly-supervised disease localization.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 10:15:26 GMT"}], "update_date": "2020-11-22", "authors_parsed": [["Zhao", "Gangming", ""], ["Fang", "Chaowei", ""], ["Li", "Guanbin", ""], ["Jiao", "Licheng", ""], ["Yu", "Yizhou", ""]]}, {"id": "2010.04495", "submitter": "Seyed Iman Mirzadeh", "authors": "Seyed Iman Mirzadeh, Mehrdad Farajtabar, Dilan Gorur, Razvan Pascanu,\n  Hassan Ghasemzadeh", "title": "Linear Mode Connectivity in Multitask and Continual Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continual (sequential) training and multitask (simultaneous) training are\noften attempting to solve the same overall objective: to find a solution that\nperforms well on all considered tasks. The main difference is in the training\nregimes, where continual learning can only have access to one task at a time,\nwhich for neural networks typically leads to catastrophic forgetting. That is,\nthe solution found for a subsequent task does not perform well on the previous\nones anymore. However, the relationship between the different minima that the\ntwo training regimes arrive at is not well understood. What sets them apart? Is\nthere a local structure that could explain the difference in performance\nachieved by the two different schemes? Motivated by recent work showing that\ndifferent minima of the same task are typically connected by very simple curves\nof low error, we investigate whether multitask and continual solutions are\nsimilarly connected. We empirically find that indeed such connectivity can be\nreliably achieved and, more interestingly, it can be done by a linear path,\nconditioned on having the same initialization for both. We thoroughly analyze\nthis observation and discuss its significance for the continual learning\nprocess. Furthermore, we exploit this finding to propose an effective algorithm\nthat constrains the sequentially learned minima to behave as the multitask\nsolution. We show that our method outperforms several state of the art\ncontinual learning algorithms on various vision benchmarks.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 10:53:25 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Mirzadeh", "Seyed Iman", ""], ["Farajtabar", "Mehrdad", ""], ["Gorur", "Dilan", ""], ["Pascanu", "Razvan", ""], ["Ghasemzadeh", "Hassan", ""]]}, {"id": "2010.04502", "submitter": "Ye Zheng", "authors": "Ye Zheng, Ruoran Huang, Chuanqi Han, Xi Huang, Li Cui", "title": "Background Learnable Cascade for Zero-Shot Object Detection", "comments": "18 pages, 5figures", "journal-ref": null, "doi": "10.1007/978-3-030-69535-4_7", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zero-shot detection (ZSD) is crucial to large-scale object detection with the\naim of simultaneously localizing and recognizing unseen objects. There remain\nseveral challenges for ZSD, including reducing the ambiguity between background\nand unseen objects as well as improving the alignment between visual and\nsemantic concept. In this work, we propose a novel framework named Background\nLearnable Cascade (BLC) to improve ZSD performance. The major contributions for\nBLC are as follows: (i) we propose a multi-stage cascade structure named\nCascade Semantic R-CNN to progressively refine the alignment between visual and\nsemantic of ZSD; (ii) we develop the semantic information flow structure and\ndirectly add it between each stage in Cascade Semantic RCNN to further improve\nthe semantic feature learning; (iii) we propose the background learnable region\nproposal network (BLRPN) to learn an appropriate word vector for background\nclass and use this learned vector in Cascade Semantic R CNN, this design makes\n\\Background Learnable\" and reduces the confusion between background and unseen\nclasses. Our extensive experiments show BLC obtains significantly performance\nimprovements for MS-COCO over state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 11:20:02 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Zheng", "Ye", ""], ["Huang", "Ruoran", ""], ["Han", "Chuanqi", ""], ["Huang", "Xi", ""], ["Cui", "Li", ""]]}, {"id": "2010.04511", "submitter": "Biel Moy\\`a", "authors": "Nata\\v{s}a Petrovi\\'c, Gabriel Moy\\`a-Alcover, Antoni Jaume-i-Cap\\'o,\n  Manuel Gonz\\'alez-Hidalgo", "title": "Sickle-cell disease diagnosis support selecting the most appropriate\n  machinelearning method: Towards a general and interpretable approach for\n  cellmorphology analysis from microscopy images", "comments": "35 pages, 10 tables", "journal-ref": "Computers in Biology and Medicine, 2020, pending publication", "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this work we propose an approach to select the classification method and\nfeatures, based on the state-of-the-art, with best performance for diagnostic\nsupport through peripheral blood smear images of red blood cells. In our case\nwe used samples of patients with sickle-cell disease which can be generalized\nfor other study cases. To trust the behavior of the proposed system, we also\nanalyzed the interpretability.\n  We pre-processed and segmented microscopic images, to ensure high feature\nquality. We applied the methods used in the literature to extract the features\nfrom blood cells and the machine learning methods to classify their morphology.\nNext, we searched for their best parameters from the resulting data in the\nfeature extraction phase. Then, we found the best parameters for every\nclassifier using Randomized and Grid search.\n  For the sake of scientific progress, we published parameters for each\nclassifier, the implemented code library, the confusion matrices with the raw\ndata, and we used the public erythrocytesIDB dataset for validation. We also\ndefined how to select the most important features for classification to\ndecrease the complexity and the training time, and for interpretability purpose\nin opaque models. Finally, comparing the best performing classification methods\nwith the state-of-the-art, we obtained better results even with interpretable\nmodel classifiers.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 11:46:38 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Petrovi\u0107", "Nata\u0161a", ""], ["Moy\u00e0-Alcover", "Gabriel", ""], ["Jaume-i-Cap\u00f3", "Antoni", ""], ["Gonz\u00e1lez-Hidalgo", "Manuel", ""]]}, {"id": "2010.04513", "submitter": "Weihao Xia", "authors": "Weihao Xia, Yujiu Yang, Jing-Hao Xue, Wensen Feng", "title": "Controllable Continuous Gaze Redirection", "comments": "accepted by ACM International Conference on Multimedia (ACM MM), 2020", "journal-ref": null, "doi": "10.1145/3394171.3413868", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present interpGaze, a novel framework for controllable gaze\nredirection that achieves both precise redirection and continuous\ninterpolation. Given two gaze images with different attributes, our goal is to\nredirect the eye gaze of one person into any gaze direction depicted in the\nreference image or to generate continuous intermediate results. To accomplish\nthis, we design a model including three cooperative components: an encoder, a\ncontroller and a decoder. The encoder maps images into a well-disentangled and\nhierarchically-organized latent space. The controller adjusts the magnitudes of\nlatent vectors to the desired strength of corresponding attributes by altering\na control vector. The decoder converts the desired representations from the\nattribute space to the image space. To facilitate covering the full space of\ngaze directions, we introduce a high-quality gaze image dataset with a large\nrange of directions, which also benefits researchers in related areas.\nExtensive experimental validation and comparisons to several baseline methods\nshow that the proposed interpGaze outperforms state-of-the-art methods in terms\nof image quality and redirection precision.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 11:50:06 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Xia", "Weihao", ""], ["Yang", "Yujiu", ""], ["Xue", "Jing-Hao", ""], ["Feng", "Wensen", ""]]}, {"id": "2010.04516", "submitter": "Mahdi Ghorbani", "authors": "Mahdi Ghorbani, Fahimeh Fooladgar, Shohreh Kasaei", "title": "Be Your Own Best Competitor! Multi-Branched Adversarial Knowledge\n  Transfer", "comments": "11 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural network architectures have attained remarkable improvements in\nscene understanding tasks. Utilizing an efficient model is one of the most\nimportant constraints for limited-resource devices. Recently, several\ncompression methods have been proposed to diminish the heavy computational\nburden and memory consumption. Among them, the pruning and quantizing methods\nexhibit a critical drop in performances by compressing the model parameters.\nWhile the knowledge distillation methods improve the performance of compact\nmodels by focusing on training lightweight networks with the supervision of\ncumbersome networks. In the proposed method, the knowledge distillation has\nbeen performed within the network by constructing multiple branches over the\nprimary stream of the model, known as the self-distillation method. Therefore,\nthe ensemble of sub-neural network models has been proposed to transfer the\nknowledge among themselves with the knowledge distillation policies as well as\nan adversarial learning strategy. Hence, The proposed ensemble of sub-models is\ntrained against a discriminator model adversarially. Besides, their knowledge\nis transferred within the ensemble by four different loss functions. The\nproposed method has been devoted to both lightweight image classification and\nencoder-decoder architectures to boost the performance of small and compact\nmodels without incurring extra computational overhead at the inference process.\nExtensive experimental results on the main challenging datasets show that the\nproposed network outperforms the primary model in terms of accuracy at the same\nnumber of parameters and computational cost. The obtained results show that the\nproposed model has achieved significant improvement over earlier ideas of\nself-distillation methods. The effectiveness of the proposed models has also\nbeen illustrated in the encoder-decoder model.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 11:57:45 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Ghorbani", "Mahdi", ""], ["Fooladgar", "Fahimeh", ""], ["Kasaei", "Shohreh", ""]]}, {"id": "2010.04517", "submitter": "Rohith Pudari", "authors": "Rohith Pudari, Sunil Bhutada, Sai Pavan Mudavath", "title": "Real Time Face Recognition Using Convoluted Neural Networks", "comments": null, "journal-ref": "International Journal of Sciences and Technology (2019) volume - 7", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face Recognition is one of the process of identifying people using their\nface, it has various applications like authentication systems, surveillance\nsystems and law enforcement. Convolutional Neural Networks are proved to be\nbest for facial recognition. Detecting faces using core-ml api and processing\nthe extracted face through a coreML model, which is trained to recognize\nspecific persons. The creation of dataset is done by converting face videos of\nthe persons to be recognized into Hundreds of images of person, which is\nfurther used for training and validation of the model to provide accurate\nreal-time results.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 12:04:49 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Pudari", "Rohith", ""], ["Bhutada", "Sunil", ""], ["Mudavath", "Sai Pavan", ""]]}, {"id": "2010.04525", "submitter": "Zhizheng Zhang", "authors": "Zhizheng Zhang, Cuiling Lan, Wenjun Zeng, Zhibo Chen, Shih-Fu Chang", "title": "Uncertainty-Aware Few-Shot Image Classification", "comments": "Accepted by IJCAI2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Few-shot image classification learns to recognize new categories from limited\nlabelled data. Metric learning based approaches have been widely investigated,\nwhere a query sample is classified by finding the nearest prototype from the\nsupport set based on their feature similarities. A neural network has different\nuncertainties on its calculated similarities of different pairs. Understanding\nand modeling the uncertainty on the similarity could promote the exploitation\nof limited samples in few-shot optimization. In this work, we propose\nUncertainty-Aware Few-Shot framework for image classification by modeling\nuncertainty of the similarities of query-support pairs and performing\nuncertainty-aware optimization. Particularly, we exploit such uncertainty by\nconverting observed similarities to probabilistic representations and\nincorporate them to the loss for more effective optimization. In order to\njointly consider the similarities between a query and the prototypes in a\nsupport set, a graph-based model is utilized to estimate the uncertainty of the\npairs. Extensive experiments show our proposed method brings significant\nimprovements on top of a strong baseline and achieves the state-of-the-art\nperformance.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 12:26:27 GMT"}, {"version": "v2", "created": "Thu, 3 Jun 2021 13:48:34 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Zhang", "Zhizheng", ""], ["Lan", "Cuiling", ""], ["Zeng", "Wenjun", ""], ["Chen", "Zhibo", ""], ["Chang", "Shih-Fu", ""]]}, {"id": "2010.04536", "submitter": "Zhou Fang", "authors": "Zhou Fang, Ying Jin, Tianren Yang", "title": "Incorporating planning intelligence into deep learning: A planning\n  support tool for street network design", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep learning applications in shaping ad hoc planning proposals are limited\nby the difficulty in integrating professional knowledge about cities with\nartificial intelligence. We propose a novel, complementary use of deep neural\nnetworks and planning guidance to automate street network generation that can\nbe context-aware, example-based and user-guided. The model tests suggest that\nthe incorporation of planning knowledge (e.g., road junctions and neighborhood\ntypes) in the model training leads to a more realistic prediction of street\nconfigurations. Furthermore, the new tool provides both professional and lay\nusers an opportunity to systematically and intuitively explore benchmark\nproposals for comparisons and further evaluations.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 12:57:05 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Fang", "Zhou", ""], ["Jin", "Ying", ""], ["Yang", "Tianren", ""]]}, {"id": "2010.04552", "submitter": "Osama N. Hassan", "authors": "Osama N. Hassan, Serhat Sahin, Vahid Mohammadzadeh, Xiaohe Yang, Navid\n  Amini, Apoorva Mylavarapu, Jack Martinyan, Tae Hong, Golnoush Mahmoudinezhad,\n  Daniel Rueckert, Kouros Nouri-Mahdavi, and Fabien Scalzo", "title": "Conditional GAN for Prediction of Glaucoma Progression with Macular\n  Optical Coherence Tomography", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The estimation of glaucoma progression is a challenging task as the rate of\ndisease progression varies among individuals in addition to other factors such\nas measurement variability and the lack of standardization in defining\nprogression. Structural tests, such as thickness measurements of the retinal\nnerve fiber layer or the macula with optical coherence tomography (OCT), are\nable to detect anatomical changes in glaucomatous eyes. Such changes may be\nobserved before any functional damage. In this work, we built a generative deep\nlearning model using the conditional GAN architecture to predict glaucoma\nprogression over time. The patient's OCT scan is predicted from three or two\nprior measurements. The predicted images demonstrate high similarity with the\nground truth images. In addition, our results suggest that OCT scans obtained\nfrom only two prior visits may actually be sufficient to predict the next OCT\nscan of the patient after six months.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2020 22:24:46 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Hassan", "Osama N.", ""], ["Sahin", "Serhat", ""], ["Mohammadzadeh", "Vahid", ""], ["Yang", "Xiaohe", ""], ["Amini", "Navid", ""], ["Mylavarapu", "Apoorva", ""], ["Martinyan", "Jack", ""], ["Hong", "Tae", ""], ["Mahmoudinezhad", "Golnoush", ""], ["Rueckert", "Daniel", ""], ["Nouri-Mahdavi", "Kouros", ""], ["Scalzo", "Fabien", ""]]}, {"id": "2010.04565", "submitter": "Ajoy Mondal Dr.", "authors": "Sachin Raja, Ajoy Mondal, and C. V. Jawahar", "title": "Table Structure Recognition using Top-Down and Bottom-Up Cues", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Tables are information-rich structured objects in document images. While\nsignificant work has been done in localizing tables as graphic objects in\ndocument images, only limited attempts exist on table structure recognition.\nMost existing literature on structure recognition depends on extraction of\nmeta-features from the PDF document or on the optical character recognition\n(OCR) models to extract low-level layout features from the image. However,\nthese methods fail to generalize well because of the absence of meta-features\nor errors made by the OCR when there is a significant variance in table layouts\nand text organization. In our work, we focus on tables that have complex\nstructures, dense content, and varying layouts with no dependency on\nmeta-features and/or OCR.\n  We present an approach for table structure recognition that combines cell\ndetection and interaction modules to localize the cells and predict their row\nand column associations with other detected cells. We incorporate structural\nconstraints as additional differential components to the loss function for cell\ndetection. We empirically validate our method on the publicly available\nreal-world datasets - ICDAR-2013, ICDAR-2019 (cTDaR) archival, UNLV, SciTSR,\nSciTSR-COMP, TableBank, and PubTabNet. Our attempt opens up a new direction for\ntable structure recognition by combining top-down (table cells detection) and\nbottom-up (structure recognition) cues in visually understanding the tables.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 13:32:53 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Raja", "Sachin", ""], ["Mondal", "Ajoy", ""], ["Jawahar", "C. V.", ""]]}, {"id": "2010.04586", "submitter": "Shilpa Mayannavar", "authors": "Shilpa Mayannavar, Uday Wali, and V M Aparanji", "title": "A Novel ANN Structure for Image Recognition", "comments": "9 pages, 10 images", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper presents Multi-layer Auto Resonance Networks (ARN), a new neural\nmodel, for image recognition. Neurons in ARN, called Nodes, latch on to an\nincoming pattern and resonate when the input is within its 'coverage.'\nResonance allows the neuron to be noise tolerant and tunable. Coverage of nodes\ngives them an ability to approximate the incoming pattern. Its latching\ncharacteristics allow it to respond to episodic events without disturbing the\nexisting trained network. These networks are capable of addressing problems in\nvaried fields but have not been sufficiently explored. Implementation of an\nimage classification and identification system using two-layer ARN is discussed\nin this paper. Recognition accuracy of 94% has been achieved for MNIST dataset\nwith only two layers of neurons and just 50 samples per numeral, making it\nuseful in computing at the edge of cloud infrastructure.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 14:07:29 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Mayannavar", "Shilpa", ""], ["Wali", "Uday", ""], ["Aparanji", "V M", ""]]}, {"id": "2010.04595", "submitter": "Bo Yang", "authors": "Alex Trevithick, Bo Yang", "title": "GRF: Learning a General Radiance Field for 3D Scene Representation and\n  Rendering", "comments": "Code and data are available at: https://github.com/alextrevithick/GRF", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a simple yet powerful implicit neural function that can represent\nand render arbitrarily complex 3D scenes in a single network only from 2D\nobservations. The function models 3D scenes as a general radiance field, which\ntakes a set of posed 2D images with camera poses and intrinsics as input,\nconstructs an internal representation for each 3D point of the scene, and\nrenders the corresponding appearance and geometry of any 3D point viewing from\nan arbitrary angle. The key to our approach is to explicitly integrate the\nprinciple of multi-view geometry to obtain the internal representations from\nobserved 2D views, such that the learned implicit representations empirically\nremain multi-view consistent. In addition, we introduce an effective neural\nmodule to learn general features for each pixel in 2D images, allowing the\nconstructed internal 3D representations to be general as well. Extensive\nexperiments demonstrate the superiority of our approach.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 14:21:43 GMT"}, {"version": "v2", "created": "Sun, 29 Nov 2020 06:33:25 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Trevithick", "Alex", ""], ["Yang", "Bo", ""]]}, {"id": "2010.04611", "submitter": "Min Zhao", "authors": "Min Zhao, Tiande Gao, Jie Chen, Wei Chen", "title": "Hyperspectral Unmixing via Nonnegative Matrix Factorization with\n  Handcrafted and Learnt Priors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, nonnegative matrix factorization (NMF) based methods have been\nwidely applied to blind spectral unmixing. Introducing proper regularizers to\nNMF is crucial for mathematically constraining the solutions and physically\nexploiting spectral and spatial properties of images. Generally, properly\nhandcrafting regularizers and solving the associated complex optimization\nproblem are non-trivial tasks. In our work, we propose an NMF based unmixing\nframework which jointly uses a handcrafting regularizer and a learnt\nregularizer from data. we plug learnt priors of abundances where the associated\nsubproblem can be addressed using various image denoisers, and we consider an\nl_2,1-norm regularizer to the abundance matrix to promote sparse unmixing\nresults. The proposed framework is flexible and extendable. Both synthetic data\nand real airborne data are conducted to confirm the effectiveness of our\nmethod.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 14:40:20 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Zhao", "Min", ""], ["Gao", "Tiande", ""], ["Chen", "Jie", ""], ["Chen", "Wei", ""]]}, {"id": "2010.04634", "submitter": "Vibhu Bhatia", "authors": "Vibhu Bhatia, Yatender Kumar", "title": "Attaining Real-Time Super-Resolution for Microscopic Images Using GAN", "comments": "10 pages, 10 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last few years, several deep learning models, especially Generative\nAdversarial Networks have received a lot of attention for the task of Single\nImage Super-Resolution (SISR). These methods focus on building an end-to-end\nframework, which produce a high resolution(SR) image from a given low\nresolution(LR) image in a single step to achieve state-of-the-art performance.\nThis paper focuses on improving an existing deep-learning based method to\nperform Super-Resolution Microscopy in real-time using a standard GPU. For\nthis, we first propose a tiling strategy, which takes advantage of parallelism\nprovided by a GPU to speed up the network training process. Further, we suggest\nsimple changes to the architecture of the generator and the discriminator of\nSRGAN. Subsequently, We compare the quality and the running time for the\noutputs produced by our model, opening its applications in different areas like\nlow-end benchtop and even mobile microscopy. Finally, we explore the\npossibility of the trained network to produce High-Resolution HR outputs for\ndifferent domains.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 15:26:21 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Bhatia", "Vibhu", ""], ["Kumar", "Yatender", ""]]}, {"id": "2010.04638", "submitter": "Christopher Hahne", "authors": "Christopher Hahne, Amar Aggoun, Vladan Velisavljevic, Susanne Fiebig,\n  Matthias Pesch", "title": "Baseline and Triangulation Geometry in a Standard Plenoptic Camera", "comments": "clarified remarks around Eqs.(16-17)", "journal-ref": "International Journal of Computer Vision, volume 126, pages 21-35\n  (2018)", "doi": "10.1007/s11263-017-1036-4", "report-no": null, "categories": "cs.IR cs.CG cs.CV physics.optics", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we demonstrate light field triangulation to determine depth\ndistances and baselines in a plenoptic camera. Advances in micro lenses and\nimage sensors have enabled plenoptic cameras to capture a scene from different\nviewpoints with sufficient spatial resolution. While object distances can be\ninferred from disparities in a stereo viewpoint pair using triangulation, this\nconcept remains ambiguous when applied in the case of plenoptic cameras. We\npresent a geometrical light field model allowing the triangulation to be\napplied to a plenoptic camera in order to predict object distances or specify\nbaselines as desired. It is shown that distance estimates from our novel method\nmatch those of real objects placed in front of the camera. Additional benchmark\ntests with an optical design software further validate the model's accuracy\nwith deviations of less than +-0.33 % for several main lens types and focus\nsettings. A variety of applications in the automotive and robotics field can\nbenefit from this estimation model.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 15:31:14 GMT"}, {"version": "v2", "created": "Wed, 20 Jan 2021 12:02:36 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Hahne", "Christopher", ""], ["Aggoun", "Amar", ""], ["Velisavljevic", "Vladan", ""], ["Fiebig", "Susanne", ""], ["Pesch", "Matthias", ""]]}, {"id": "2010.04642", "submitter": "Loic Landrieu", "authors": "Thomas Chaton, Nicolas Chaulet, Sofiane Horache, Loic Landrieu", "title": "Torch-Points3D: A Modular Multi-Task Frameworkfor Reproducible Deep\n  Learning on 3D Point Clouds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Torch-Points3D, an open-source framework designed to facilitate\nthe use of deep networks on3D data. Its modular design, efficient\nimplementation, and user-friendly interfaces make it a relevant tool for\nresearch and productization alike. Beyond multiple quality-of-life features,\nour goal is to standardize a higher level of transparency and reproducibility\nin 3D deep learning research, and to lower its barrier to entry. In this paper,\nwe present the design principles of Torch-Points3D, as well as extensive\nbenchmarks of multiple state-of-the-art algorithms and inference schemes across\nseveral datasets and tasks. The modularity of Torch-Points3D allows us to\ndesign fair and rigorous experimental protocols in which all methods are\nevaluated in the same conditions. The Torch-Points3D repository\n:https://github.com/nicolas-chaulet/torch-points3d\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 15:34:32 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Chaton", "Thomas", ""], ["Chaulet", "Nicolas", ""], ["Horache", "Sofiane", ""], ["Landrieu", "Loic", ""]]}, {"id": "2010.04647", "submitter": "Bo Li", "authors": "Bo Li and Yezhen Wang, Shanghang Zhang, Dongsheng Li, Trevor Darrell,\n  Kurt Keutzer, Han Zhao", "title": "Learning Invariant Representations and Risks for Semi-supervised Domain\n  Adaptation", "comments": "Accepted to CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The success of supervised learning hinges on the assumption that the training\nand test data come from the same underlying distribution, which is often not\nvalid in practice due to potential distribution shift. In light of this, most\nexisting methods for unsupervised domain adaptation focus on achieving\ndomain-invariant representations and small source domain error. However, recent\nworks have shown that this is not sufficient to guarantee good generalization\non the target domain, and in fact, is provably detrimental under label\ndistribution shift. Furthermore, in many real-world applications it is often\nfeasible to obtain a small amount of labeled data from the target domain and\nuse them to facilitate model training with source data. Inspired by the above\nobservations, in this paper we propose the first method that aims to\nsimultaneously learn invariant representations and risks under the setting of\nsemi-supervised domain adaptation (Semi-DA). First, we provide a finite sample\nbound for both classification and regression problems under Semi-DA. The bound\nsuggests a principled way to obtain target generalization, i.e. by aligning\nboth the marginal and conditional distributions across domains in feature\nspace. Motivated by this, we then introduce the LIRR algorithm for jointly\n\\textbf{L}earning \\textbf{I}nvariant \\textbf{R}epresentations and\n\\textbf{R}isks. Finally, extensive experiments are conducted on both\nclassification and regression tasks, which demonstrates LIRR consistently\nachieves state-of-the-art performance and significant improvements compared\nwith the methods that only learn invariant representations or invariant risks.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 15:42:35 GMT"}, {"version": "v2", "created": "Mon, 12 Oct 2020 12:53:32 GMT"}, {"version": "v3", "created": "Sun, 4 Apr 2021 18:10:56 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Li", "Bo", ""], ["Wang", "Yezhen", ""], ["Zhang", "Shanghang", ""], ["Li", "Dongsheng", ""], ["Darrell", "Trevor", ""], ["Keutzer", "Kurt", ""], ["Zhao", "Han", ""]]}, {"id": "2010.04683", "submitter": "Jovita Lukasik", "authors": "Jovita Lukasik and David Friede and Arber Zela and Frank Hutter and\n  Margret Keuper", "title": "Smooth Variational Graph Embeddings for Efficient Neural Architecture\n  Search", "comments": "8 pages, 3 figures, 5 tables. Camera-Ready Version for IJCNN 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural architecture search (NAS) has recently been addressed from various\ndirections, including discrete, sampling-based methods and efficient\ndifferentiable approaches. While the former are notoriously expensive, the\nlatter suffer from imposing strong constraints on the search space.\nArchitecture optimization from a learned embedding space for example through\ngraph neural network based variational autoencoders builds a middle ground and\nleverages advantages from both sides. Such approaches have recently shown good\nperformance on several benchmarks. Yet, their stability and predictive power\nheavily depends on their capacity to reconstruct networks from the embedding\nspace. In this paper, we propose a two-sided variational graph autoencoder,\nwhich allows to smoothly encode and accurately reconstruct neural architectures\nfrom various search spaces. We evaluate the proposed approach on neural\narchitectures defined by the ENAS approach, the NAS-Bench-101 and the\nNAS-Bench-201 search space and show that our smooth embedding space allows to\ndirectly extrapolate the performance prediction to architectures outside the\nseen domain (e.g. with more operations). Thus, it facilitates to predict good\nnetwork architectures even without expensive Bayesian optimization or\nreinforcement learning.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 17:05:41 GMT"}, {"version": "v2", "created": "Tue, 8 Dec 2020 14:50:56 GMT"}, {"version": "v3", "created": "Wed, 12 May 2021 12:44:54 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Lukasik", "Jovita", ""], ["Friede", "David", ""], ["Zela", "Arber", ""], ["Hutter", "Frank", ""], ["Keuper", "Margret", ""]]}, {"id": "2010.04689", "submitter": "Gregory Kahn", "authors": "Gregory Kahn, Pieter Abbeel, Sergey Levine", "title": "LaND: Learning to Navigate from Disengagements", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consistently testing autonomous mobile robots in real world scenarios is a\nnecessary aspect of developing autonomous navigation systems. Each time the\nhuman safety monitor disengages the robot's autonomy system due to the robot\nperforming an undesirable maneuver, the autonomy developers gain insight into\nhow to improve the autonomy system. However, we believe that these\ndisengagements not only show where the system fails, which is useful for\ntroubleshooting, but also provide a direct learning signal by which the robot\ncan learn to navigate. We present a reinforcement learning approach for\nlearning to navigate from disengagements, or LaND. LaND learns a neural network\nmodel that predicts which actions lead to disengagements given the current\nsensory observation, and then at test time plans and executes actions that\navoid disengagements. Our results demonstrate LaND can successfully learn to\nnavigate in diverse, real world sidewalk environments, outperforming both\nimitation learning and reinforcement learning approaches. Videos, code, and\nother material are available on our website\nhttps://sites.google.com/view/sidewalk-learning\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 17:21:42 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Kahn", "Gregory", ""], ["Abbeel", "Pieter", ""], ["Levine", "Sergey", ""]]}, {"id": "2010.04690", "submitter": "Shaifali Parashar", "authors": "Shaifali Parashar, Adrien Bartoli and Daniel Pizarro", "title": "Robust Isometric Non-Rigid Structure-from-Motion", "comments": "Accepted in TPAMI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Non-Rigid Structure-from-Motion (NRSfM) reconstructs a deformable 3D object\nfrom the correspondences established between monocular 2D images. Current NRSfM\nmethods lack statistical robustness, which is the ability to cope with\ncorrespondence errors.This prevents one to use automatically established\ncorrespondences, which are prone to errors, thereby strongly limiting the scope\nof NRSfM. We propose a three-step automatic pipeline to solve NRSfM robustly by\nexploiting isometry. Step 1 computes the optical flow from correspondences,\nstep 2 reconstructs each 3D point's normal vector using multiple reference\nimages and integrates them to form surfaces with the best reference and step 3\nrejects the 3D points that break isometry in their local neighborhood.\nImportantly, each step is designed to discard or flag erroneous\ncorrespondences. Our contributions include the robustification of optical flow\nby warp estimation, new fast analytic solutions to local normal reconstruction\nand their robustification, and a new scale-independent measure of 3D local\nisometric coherence. Experimental results show that our robust NRSfM method\nconsistently outperforms existing methods on both synthetic and real datasets.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 17:25:00 GMT"}, {"version": "v2", "created": "Wed, 2 Jun 2021 12:23:42 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Parashar", "Shaifali", ""], ["Bartoli", "Adrien", ""], ["Pizarro", "Daniel", ""]]}, {"id": "2010.04713", "submitter": "Farzin Negahbani", "authors": "Farzin Negahbani, Rasool Sabzi, Bita Pakniyat Jahromi, Fateme\n  Movahedi, Mahsa Kohandel Shirazi, Shayan Majidi, Dena Firouzabadi, and\n  Amirreza Dehghanian", "title": "PathoNet: Deep learning assisted evaluation of Ki-67 and tumor\n  infiltrating lymphocytes (TILs) as prognostic factors in breast cancer; A\n  large dataset and baseline", "comments": "This is a preprint of an article published in Nature Publishing\n  Group, Scientific Reports. The final authenticated version is available\n  online at: https://www.nature.com/articles/s41598-021-86912-w", "journal-ref": "Sci Rep 11, 8489 (2021)", "doi": "10.1038/s41598-021-86912-w", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The nuclear protein Ki-67 and Tumor infiltrating lymphocytes (TILs) have been\nintroduced as prognostic factors in predicting tumor progression and its\ntreatment response. The value of the Ki-67 index and TILs in approach to\nheterogeneous tumors such as Breast cancer (BC), known as the most common\ncancer in women worldwide, has been highlighted in the literature. Due to the\nindeterminable and subjective nature of Ki-67 as well as TILs scoring,\nautomated methods using machine learning, specifically approaches based on deep\nlearning, have attracted attention. Yet, deep learning methods need\nconsiderable annotated data. In the absence of publicly available benchmarks\nfor BC Ki-67 stained cell detection and further annotated classification of\ncells, we propose SHIDC-BC-Ki-67 as a dataset for the aforementioned purpose.\nWe also introduce a novel pipeline and a backend, namely PathoNet for Ki-67\nimmunostained cell detection and classification and simultaneous determination\nof intratumoral TILs score. Further, we show that despite facing challenges,\nour proposed backend, PathoNet, outperforms the state of the art methods\nproposed to date in the harmonic mean measure.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 17:55:52 GMT"}, {"version": "v2", "created": "Tue, 20 Oct 2020 23:55:31 GMT"}, {"version": "v3", "created": "Wed, 28 Apr 2021 22:20:20 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Negahbani", "Farzin", ""], ["Sabzi", "Rasool", ""], ["Jahromi", "Bita Pakniyat", ""], ["Movahedi", "Fateme", ""], ["Shirazi", "Mahsa Kohandel", ""], ["Majidi", "Shayan", ""], ["Firouzabadi", "Dena", ""], ["Dehghanian", "Amirreza", ""]]}, {"id": "2010.04717", "submitter": "Jaime Simarro", "authors": "Jaime Simarro, Ezequiel de la Rosa, Thijs Vande Vyvere, David Robben\n  and Diana M. Sima", "title": "Unsupervised 3D Brain Anomaly Detection", "comments": "Accepted at BrainLes Workshop in MICCAI 2020", "journal-ref": "In: Brainlesion: Glioma, Multiple Sclerosis, Stroke and Traumatic\n  Brain Injuries. BrainLes 2020. Lecture Notes in Computer Science, vol 12658.\n  Springer, Cham (2021)", "doi": "10.1007/978-3-030-72084-1_13", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anomaly detection (AD) is the identification of data samples that do not fit\na learned data distribution. As such, AD systems can help physicians to\ndetermine the presence, severity, and extension of a pathology. Deep generative\nmodels, such as Generative Adversarial Networks (GANs), can be exploited to\ncapture anatomical variability. Consequently, any outlier (i.e., sample falling\noutside of the learned distribution) can be detected as an abnormality in an\nunsupervised fashion. By using this method, we can not only detect expected or\nknown lesions, but we can even unveil previously unrecognized biomarkers. To\nthe best of our knowledge, this study exemplifies the first AD approach that\ncan efficiently handle volumetric data and detect 3D brain anomalies in one\nsingle model. Our proposal is a volumetric and high-detail extension of the 2D\nf-AnoGAN model obtained by combining a state-of-the-art 3D GAN with refinement\ntraining steps. In experiments using non-contrast computed tomography images\nfrom traumatic brain injury (TBI) patients, the model detects and localizes TBI\nabnormalities with an area under the ROC curve of ~75%. Moreover, we test the\npotential of the method for detecting other anomalies such as low quality\nimages, preprocessing inaccuracies, artifacts, and even the presence of\npost-operative signs (such as a craniectomy or a brain shunt). The method has\npotential for rapidly labeling abnormalities in massive imaging datasets, as\nwell as identifying new biomarkers.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 17:59:17 GMT"}, {"version": "v2", "created": "Fri, 9 Apr 2021 11:43:09 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Simarro", "Jaime", ""], ["de la Rosa", "Ezequiel", ""], ["Vyvere", "Thijs Vande", ""], ["Robben", "David", ""], ["Sima", "Diana M.", ""]]}, {"id": "2010.04757", "submitter": "Adrian Dalca", "authors": "Adrian V. Dalca, Ramesh Sridharan, Mert R. Sabuncu, Polina Golland", "title": "Predictive Modeling of Anatomy with Genetic and Clinical Data", "comments": "MICCAI 2015. Keywords: Neuroimaging, Anatomical prediction,\n  Synthesis, Simulation, Genetics, Generative model, Linear mixed effects,\n  Kernel Machines", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a semi-parametric generative model for predicting anatomy of a\npatient in subsequent scans following a single baseline image. Such predictive\nmodeling promises to facilitate novel analyses in both voxel-level studies and\nlongitudinal biomarker evaluation. We capture anatomical change through a\ncombination of population-wide regression and a non-parametric model of the\nsubject's health based on individual genetic and clinical indicators. In\ncontrast to classical correlation and longitudinal analysis, we focus on\npredicting new observations from a single subject observation. We demonstrate\nprediction of follow-up anatomical scans in the ADNI cohort, and illustrate a\nnovel analysis approach that compares a patient's scans to the predicted\nsubject-specific healthy anatomical trajectory. The code is available at\nhttps://github.com/adalca/voxelorb.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 18:30:15 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Dalca", "Adrian V.", ""], ["Sridharan", "Ramesh", ""], ["Sabuncu", "Mert R.", ""], ["Golland", "Polina", ""]]}, {"id": "2010.04767", "submitter": "Tanmay Samak", "authors": "Tanmay Vilas Samak, Chinmay Vilas Samak and Sivanathan Kandhasamy", "title": "Robust Behavioral Cloning for Autonomous Vehicles using End-to-End\n  Imitation Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present a lightweight pipeline for robust behavioral cloning\nof a human driver using end-to-end imitation learning. The proposed pipeline\nwas employed to train and deploy three distinct driving behavior models onto a\nsimulated vehicle. The training phase comprised of data collection, balancing,\naugmentation, preprocessing and training a neural network, following which, the\ntrained model was deployed onto the ego vehicle to predict steering commands\nbased on the feed from an onboard camera. A novel coupled control law was\nformulated to generate longitudinal control commands on-the-go based on the\npredicted steering angle and other parameters such as actual speed of the ego\nvehicle and the prescribed constraints for speed and steering. We analyzed\ncomputational efficiency of the pipeline and evaluated robustness of the\ntrained models through exhaustive experimentation during the deployment phase.\nWe also compared our approach against state-of-the-art implementation in order\nto comment on its validity.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 19:03:15 GMT"}, {"version": "v2", "created": "Mon, 18 Jan 2021 18:07:14 GMT"}, {"version": "v3", "created": "Sat, 13 Mar 2021 09:33:08 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Samak", "Tanmay Vilas", ""], ["Samak", "Chinmay Vilas", ""], ["Kandhasamy", "Sivanathan", ""]]}, {"id": "2010.04794", "submitter": "Sarah Ryan", "authors": "Sarah Ryan, Nichole Carlson, Harris Butler, Tasha Fingerlin, Lisa\n  Maier, Fuyong Xing", "title": "Cluster Activation Mapping with Applications to Medical Imaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  An open question in deep clustering is how to understand what in the image is\ncreating the cluster assignments. This visual understanding is essential to be\nable to trust the results of an inherently complex algorithm like deep\nlearning, especially when the derived cluster assignments may be used to inform\ndecision-making or create new disease sub-types. In this work, we developed\nnovel methodology to generate CLuster Activation Mapping (CLAM) which combines\nan unsupervised deep clustering framework with a modification of Score-CAM, an\napproach for discriminative localization in the supervised setting. We\nevaluated our approach using a simulation study based on computed tomography\nscans of the lung, and applied it to 3D CT scans from a sarcoidosis population\nto identify new clusters of sarcoidosis based purely on CT scan presentation.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 20:37:09 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Ryan", "Sarah", ""], ["Carlson", "Nichole", ""], ["Butler", "Harris", ""], ["Fingerlin", "Tasha", ""], ["Maier", "Lisa", ""], ["Xing", "Fuyong", ""]]}, {"id": "2010.04812", "submitter": "Xiang Deng", "authors": "Xiang Deng and Zhongfei (Mark) Zhang", "title": "Locally Linear Region Knowledge Distillation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge distillation (KD) is an effective technique to transfer knowledge\nfrom one neural network (teacher) to another (student), thus improving the\nperformance of the student. To make the student better mimic the behavior of\nthe teacher, the existing work focuses on designing different criteria to align\ntheir logits or representations. Different from these efforts, we address\nknowledge distillation from a novel data perspective. We argue that\ntransferring knowledge at sparse training data points cannot enable the student\nto well capture the local shape of the teacher function. To address this issue,\nwe propose locally linear region knowledge distillation ($\\rm L^2$RKD) which\ntransfers the knowledge in local, linear regions from a teacher to a student.\nThis is achieved by enforcing the student to mimic the outputs of the teacher\nfunction in local, linear regions. To the end, the student is able to better\ncapture the local shape of the teacher function and thus achieves a better\nperformance. Despite its simplicity, extensive experiments demonstrate that\n$\\rm L^2$RKD is superior to the original KD in many aspects as it outperforms\nKD and the other state-of-the-art approaches by a large margin, shows\nrobustness and superiority under few-shot settings, and is more compatible with\nthe existing distillation approaches to further improve their performances\nsignificantly.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 21:23:53 GMT"}, {"version": "v2", "created": "Mon, 19 Oct 2020 08:47:58 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Deng", "Xiang", "", "Mark"], ["Zhongfei", "", "", "Mark"], ["Zhang", "", ""]]}, {"id": "2010.04821", "submitter": "Ziyuan Zhong", "authors": "Ziyuan Zhong, Yuchi Tian, Baishakhi Ray", "title": "Understanding Local Robustness of Deep Neural Networks under Natural\n  Variations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks (DNNs) are being deployed in a wide range of settings\ntoday, from safety-critical applications like autonomous driving to commercial\napplications involving image classifications. However, recent research has\nshown that DNNs can be brittle to even slight variations of the input data.\nTherefore, rigorous testing of DNNs has gained widespread attention.\n  While DNN robustness under norm-bound perturbation got significant attention\nover the past few years, our knowledge is still limited when natural variants\nof the input images come. These natural variants, e.g. a rotated or a rainy\nversion of the original input, are especially concerning as they can occur\nnaturally in the field without any active adversary and may lead to undesirable\nconsequences. Thus, it is important to identify the inputs whose small\nvariations may lead to erroneous DNN behaviors. The very few studies that\nlooked at DNN's robustness under natural variants, however, focus on estimating\nthe overall robustness of DNNs across all the test data rather than localizing\nsuch error-producing points. This work aims to bridge this gap.\n  To this end, we study the local per-input robustness properties of the DNNs\nand leverage those properties to build a white-box (DeepRobust-W) and a\nblack-box (DeepRobust-B) tool to automatically identify the non-robust points.\nOur evaluation of these methods on three DNN models spanning three widely used\nimage classification datasets shows that they are effective in flagging points\nof poor robustness. In particular, DeepRobust-W and DeepRobust-B are able to\nachieve an F1 score of up to 91.4% and 99.1%, respectively. We further show\nthat DeepRobust-W can be applied to a regression problem in another domain. Our\nevaluation on three self-driving car models demonstrates that DeepRobust-W is\neffective in identifying points of poor robustness with F1 score up to 78.9%.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 21:42:16 GMT"}, {"version": "v2", "created": "Sat, 23 Jan 2021 02:46:18 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Zhong", "Ziyuan", ""], ["Tian", "Yuchi", ""], ["Ray", "Baishakhi", ""]]}, {"id": "2010.04837", "submitter": "Manoj Bhat", "authors": "Iljoo Baek, Tzu-Chieh Tai, Manoj Bhat, Karun Ellango, Tarang Shah,\n  Kamal Fuseini, Ragunathan (Raj) Rajkumar", "title": "CurbScan: Curb Detection and Tracking Using Multi-Sensor Fusion", "comments": "Accepted to IEEE ITSC-2020 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.RO eess.SP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Reliable curb detection is critical for safe autonomous driving in urban\ncontexts. Curb detection and tracking are also useful in vehicle localization\nand path planning. Past work utilized a 3D LiDAR sensor to determine accurate\ndistance information and the geometric attributes of curbs. However, such an\napproach requires dense point cloud data and is also vulnerable to false\npositives from obstacles present on both road and off-road areas. In this\npaper, we propose an approach to detect and track curbs by fusing together data\nfrom multiple sensors: sparse LiDAR data, a mono camera and low-cost ultrasonic\nsensors. The detection algorithm is based on a single 3D LiDAR and a mono\ncamera sensor used to detect candidate curb features and it effectively removes\nfalse positives arising from surrounding static and moving obstacles. The\ndetection accuracy of the tracking algorithm is boosted by using Kalman\nfilter-based prediction and fusion with lateral distance information from\nlow-cost ultrasonic sensors. We next propose a line-fitting algorithm that\nyields robust results for curb locations. Finally, we demonstrate the practical\nfeasibility of our solution by testing in different road environments and\nevaluating our implementation in a real vehicle\\footnote{Demo video clips\ndemonstrating our algorithm have been uploaded to Youtube:\nhttps://www.youtube.com/watch?v=w5MwsdWhcy4,\nhttps://www.youtube.com/watch?v=Gd506RklfG8.}. Our algorithm maintains over\n90\\% accuracy within 4.5-22 meters and 0-14 meters for the KITTI dataset and\nour dataset respectively, and its average processing time per frame is\napproximately 10 ms on Intel i7 x86 and 100ms on NVIDIA Xavier board.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 22:48:20 GMT"}, {"version": "v2", "created": "Tue, 13 Oct 2020 00:28:21 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Baek", "Iljoo", "", "Raj"], ["Tai", "Tzu-Chieh", "", "Raj"], ["Bhat", "Manoj", "", "Raj"], ["Ellango", "Karun", "", "Raj"], ["Shah", "Tarang", "", "Raj"], ["Fuseini", "Kamal", "", "Raj"], ["Ragunathan", "", "", "Raj"], ["Rajkumar", "", ""]]}, {"id": "2010.04871", "submitter": "Kai Han", "authors": "Kai Han, Yunhe Wang, Yixing Xu, Chunjing Xu, Enhua Wu, Chang Xu", "title": "Training Binary Neural Networks through Learning with Noisy Supervision", "comments": "ICML 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper formalizes the binarization operations over neural networks from a\nlearning perspective. In contrast to classical hand crafted rules (\\eg hard\nthresholding) to binarize full-precision neurons, we propose to learn a mapping\nfrom full-precision neurons to the target binary ones. Each individual weight\nentry will not be binarized independently. Instead, they are taken as a whole\nto accomplish the binarization, just as they work together in generating\nconvolution features. To help the training of the binarization mapping, the\nfull-precision neurons after taking sign operations is regarded as some\nauxiliary supervision signal, which is noisy but still has valuable guidance.\nAn unbiased estimator is therefore introduced to mitigate the influence of the\nsupervision noise. Experimental results on benchmark datasets indicate that the\nproposed binarization technique attains consistent improvements over baselines.\n", "versions": [{"version": "v1", "created": "Sat, 10 Oct 2020 01:59:39 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Han", "Kai", ""], ["Wang", "Yunhe", ""], ["Xu", "Yixing", ""], ["Xu", "Chunjing", ""], ["Wu", "Enhua", ""], ["Xu", "Chang", ""]]}, {"id": "2010.04873", "submitter": "Yueming Yin", "authors": "Yueming Yin, Zhen Yang (Senior Member, IEEE), Xiaofu Wu, and Haifeng\n  Hu", "title": "Unveiling Class-Labeling Structure for Universal Domain Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a more practical setting for unsupervised domain adaptation, Universal\nDomain Adaptation (UDA) is recently introduced, where the target label set is\nunknown. One of the big challenges in UDA is how to determine the common label\nset shared by source and target domains, as there is simply no labeling\navailable in the target domain. In this paper, we employ a probabilistic\napproach for locating the common label set, where each source class may come\nfrom the common label set with a probability. In particular, we propose a novel\napproach for evaluating the probability of each source class from the common\nlabel set, where this probability is computed by the prediction margin\naccumulated over the whole target domain. Then, we propose a simple universal\nadaptation network (S-UAN) by incorporating the probabilistic structure for the\ncommon label set. Finally, we analyse the generalization bound focusing on the\ncommon label set and explore the properties on the target risk for UDA.\nExtensive experiments indicate that S-UAN works well in different UDA settings\nand outperforms the state-of-the-art methods by large margins.\n", "versions": [{"version": "v1", "created": "Sat, 10 Oct 2020 02:13:02 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Yin", "Yueming", "", "Senior Member, IEEE"], ["Yang", "Zhen", "", "Senior Member, IEEE"], ["Wu", "Xiaofu", ""], ["Hu", "Haifeng", ""]]}, {"id": "2010.04879", "submitter": "Wenxiao Wang", "authors": "Wenxiao Wang, Minghao Chen, Shuai Zhao, Long Chen, Jinming Hu, Haifeng\n  Liu, Deng Cai, Xiaofei He, Wei Liu", "title": "Accelerate CNNs from Three Dimensions: A Comprehensive Pruning Framework", "comments": "11 pages, 5 figures. Accepted by ICML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most neural network pruning methods, such as filter-level and layer-level\nprunings, prune the network model along one dimension (depth, width, or\nresolution) solely to meet a computational budget. However, such a pruning\npolicy often leads to excessive reduction of that dimension, thus inducing a\nhuge accuracy loss. To alleviate this issue, we argue that pruning should be\nconducted along three dimensions comprehensively. For this purpose, our pruning\nframework formulates pruning as an optimization problem. Specifically, it first\ncasts the relationships between a certain model's accuracy and\ndepth/width/resolution into a polynomial regression and then maximizes the\npolynomial to acquire the optimal values for the three dimensions. Finally, the\nmodel is pruned along the three optimal dimensions accordingly. In this\nframework, since collecting too much data for training the regression is very\ntime-costly, we propose two approaches to lower the cost: 1) specializing the\npolynomial to ensure an accurate regression even with less training data; 2)\nemploying iterative pruning and fine-tuning to collect the data faster.\nExtensive experiments show that our proposed algorithm surpasses\nstate-of-the-art pruning algorithms and even neural architecture search-based\nalgorithms.\n", "versions": [{"version": "v1", "created": "Sat, 10 Oct 2020 02:30:47 GMT"}, {"version": "v2", "created": "Sun, 7 Feb 2021 03:32:54 GMT"}, {"version": "v3", "created": "Tue, 15 Jun 2021 13:02:09 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Wang", "Wenxiao", ""], ["Chen", "Minghao", ""], ["Zhao", "Shuai", ""], ["Chen", "Long", ""], ["Hu", "Jinming", ""], ["Liu", "Haifeng", ""], ["Cai", "Deng", ""], ["He", "Xiaofei", ""], ["Liu", "Wei", ""]]}, {"id": "2010.04889", "submitter": "Soufiane Belharbi", "authors": "Soufiane Belharbi, Ismail Ben Ayed, Luke McCaffrey, Eric Granger", "title": "Deep Active Learning for Joint Classification & Segmentation with Weak\n  Annotator", "comments": "20 pages, 12 figures, WACV 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  CNN visualization and interpretation methods, like class-activation maps\n(CAMs), are typically used to highlight the image regions linked to class\npredictions. These models allow to simultaneously classify images and extract\nclass-dependent saliency maps, without the need for costly pixel-level\nannotations. However, they typically yield segmentations with high\nfalse-positive rates and, therefore, coarse visualisations, more so when\nprocessing challenging images, as encountered in histology. To mitigate this\nissue, we propose an active learning (AL) framework, which progressively\nintegrates pixel-level annotations during training. Given training data with\nglobal image-level labels, our deep weakly-supervised learning model jointly\nperforms supervised image-level classification and active learning for\nsegmentation, integrating pixel annotations by an oracle. Unlike standard AL\nmethods that focus on sample selection, we also leverage large numbers of\nunlabeled images via pseudo-segmentations (i.e., self-learning at the pixel\nlevel), and integrate them with the oracle-annotated samples during training.\nWe report extensive experiments over two challenging benchmarks --\nhigh-resolution medical images (histology GlaS data for colon cancer) and\nnatural images (CUB-200-2011 for bird species). Our results indicate that, by\nsimply using random sample selection, the proposed approach can significantly\noutperform state-of the-art CAMs and AL methods, with an identical\noracle-supervision budget. Our code is publicly available.\n", "versions": [{"version": "v1", "created": "Sat, 10 Oct 2020 03:25:54 GMT"}, {"version": "v2", "created": "Sat, 14 Nov 2020 04:09:55 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Belharbi", "Soufiane", ""], ["Ayed", "Ismail Ben", ""], ["McCaffrey", "Luke", ""], ["Granger", "Eric", ""]]}, {"id": "2010.04904", "submitter": "Qifei Wang", "authors": "Qifei Wang, Junjie Ke, Joshua Greaves, Grace Chu, Gabriel Bender,\n  Luciano Sbaiz, Alec Go, Andrew Howard, Feng Yang, Ming-Hsuan Yang, Jeff\n  Gilbert, and Peyman Milanfar", "title": "Multi-path Neural Networks for On-device Multi-domain Visual\n  Classification", "comments": "WACV 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning multiple domains/tasks with a single model is important for\nimproving data efficiency and lowering inference cost for numerous vision\ntasks, especially on resource-constrained mobile devices. However,\nhand-crafting a multi-domain/task model can be both tedious and challenging.\nThis paper proposes a novel approach to automatically learn a multi-path\nnetwork for multi-domain visual classification on mobile devices. The proposed\nmulti-path network is learned from neural architecture search by applying one\nreinforcement learning controller for each domain to select the best path in\nthe super-network created from a MobileNetV3-like search space. An adaptive\nbalanced domain prioritization algorithm is proposed to balance optimizing the\njoint model on multiple domains simultaneously. The determined multi-path model\nselectively shares parameters across domains in shared nodes while keeping\ndomain-specific parameters within non-shared nodes in individual domain paths.\nThis approach effectively reduces the total number of parameters and FLOPS,\nencouraging positive knowledge transfer while mitigating negative interference\nacross domains. Extensive evaluations on the Visual Decathlon dataset\ndemonstrate that the proposed multi-path model achieves state-of-the-art\nperformance in terms of accuracy, model size, and FLOPS against other\napproaches using MobileNetV3-like architectures. Furthermore, the proposed\nmethod improves average accuracy over learning single-domain models\nindividually, and reduces the total number of parameters and FLOPS by 78% and\n32% respectively, compared to the approach that simply bundles single-domain\nmodels for multi-domain learning.\n", "versions": [{"version": "v1", "created": "Sat, 10 Oct 2020 05:13:49 GMT"}, {"version": "v2", "created": "Fri, 8 Jan 2021 08:02:15 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Wang", "Qifei", ""], ["Ke", "Junjie", ""], ["Greaves", "Joshua", ""], ["Chu", "Grace", ""], ["Bender", "Gabriel", ""], ["Sbaiz", "Luciano", ""], ["Go", "Alec", ""], ["Howard", "Andrew", ""], ["Yang", "Feng", ""], ["Yang", "Ming-Hsuan", ""], ["Gilbert", "Jeff", ""], ["Milanfar", "Peyman", ""]]}, {"id": "2010.04913", "submitter": "Ruixue Tang", "authors": "Ruixue Tang, Chao Ma", "title": "Interpretable Neural Computation for Real-World Compositional Visual\n  Question Answering", "comments": "PRCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are two main lines of research on visual question answering (VQA):\ncompositional model with explicit multi-hop reasoning, and monolithic network\nwith implicit reasoning in the latent feature space. The former excels in\ninterpretability and compositionality but fails on real-world images, while the\nlatter usually achieves better performance due to model flexibility and\nparameter efficiency. We aim to combine the two to build an interpretable\nframework for real-world compositional VQA. In our framework, images and\nquestions are disentangled into scene graphs and programs, and a symbolic\nprogram executor runs on them with full transparency to select the attention\nregions, which are then iteratively passed to a visual-linguistic pre-trained\nencoder to predict answers. Experiments conducted on the GQA benchmark\ndemonstrate that our framework outperforms the compositional prior arts and\nachieves competitive accuracy among monolithic ones. With respect to the\nvalidity, plausibility and distribution metrics, our framework surpasses others\nby a considerable margin.\n", "versions": [{"version": "v1", "created": "Sat, 10 Oct 2020 05:46:22 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Tang", "Ruixue", ""], ["Ma", "Chao", ""]]}, {"id": "2010.04920", "submitter": "QiKui Zhu", "authors": "Qikui Zhu, Liang Li, Jiangnan Hao, Yunfei Zha, Yan Zhang, Yanxiang\n  Cheng, Fei Liao, Pingxiang Li", "title": "Selective Information Passing for MR/CT Image Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated medical image segmentation plays an important role in many clinical\napplications, which however is a very challenging task, due to complex\nbackground texture, lack of clear boundary and significant shape and texture\nvariation between images. Many researchers proposed an encoder-decoder\narchitecture with skip connections to combine low-level feature maps from the\nencoder path with high-level feature maps from the decoder path for\nautomatically segmenting medical images. The skip connections have been shown\nto be effective in recovering fine-grained details of the target objects and\nmay facilitate the gradient back-propagation. However, not all the feature maps\ntransmitted by those connections contribute positively to the network\nperformance. In this paper, to adaptively select useful information to pass\nthrough those skip connections, we propose a novel 3D network with\nself-supervised function, named selective information passing network\n(SIP-Net). We evaluate our proposed model on the MICCAI Prostate MR Image\nSegmentation 2012 Grant Challenge dataset, TCIA Pancreas CT-82 and MICCAI 2017\nLiver Tumor Segmentation (LiTS) Challenge dataset. The experimental results\nacross these data sets show that our model achieved improved segmentation\nresults and outperformed other state-of-the-art methods. The source code of\nthis work is available at https://github.com/ahukui/SIPNet.\n", "versions": [{"version": "v1", "created": "Sat, 10 Oct 2020 06:47:53 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Zhu", "Qikui", ""], ["Li", "Liang", ""], ["Hao", "Jiangnan", ""], ["Zha", "Yunfei", ""], ["Zhang", "Yan", ""], ["Cheng", "Yanxiang", ""], ["Liao", "Fei", ""], ["Li", "Pingxiang", ""]]}, {"id": "2010.04928", "submitter": "Haoming Li", "authors": "Haoming Li, Xin Yang, Jiamin Liang, Wenlong Shi, Chaoyu Chen, Haoran\n  Dou, Rui Li, Rui Gao, Guangquan Zhou, Jinghui Fang, Xiaowen Liang, Ruobing\n  Huang, Alejandro Frangi, Zhiyi Chen, Dong Ni", "title": "Contrastive Rendering for Ultrasound Image Segmentation", "comments": "10 pages, 5 figures, 2 tables, 13 references", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ultrasound (US) image segmentation embraced its significant improvement in\ndeep learning era. However, the lack of sharp boundaries in US images still\nremains an inherent challenge for segmentation. Previous methods often resort\nto global context, multi-scale cues or auxiliary guidance to estimate the\nboundaries. It is hard for these methods to approach pixel-level learning for\nfine-grained boundary generating. In this paper, we propose a novel and\neffective framework to improve boundary estimation in US images. Our work has\nthree highlights. First, we propose to formulate the boundary estimation as a\nrendering task, which can recognize ambiguous points (pixels/voxels) and\ncalibrate the boundary prediction via enriched feature representation learning.\nSecond, we introduce point-wise contrastive learning to enhance the similarity\nof points from the same class and contrastively decrease the similarity of\npoints from different classes. Boundary ambiguities are therefore further\naddressed. Third, both rendering and contrastive learning tasks contribute to\nconsistent improvement while reducing network parameters. As a\nproof-of-concept, we performed validation experiments on a challenging dataset\nof 86 ovarian US volumes. Results show that our proposed method outperforms\nstate-of-the-art methods and has the potential to be used in clinical practice.\n", "versions": [{"version": "v1", "created": "Sat, 10 Oct 2020 07:14:03 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Li", "Haoming", ""], ["Yang", "Xin", ""], ["Liang", "Jiamin", ""], ["Shi", "Wenlong", ""], ["Chen", "Chaoyu", ""], ["Dou", "Haoran", ""], ["Li", "Rui", ""], ["Gao", "Rui", ""], ["Zhou", "Guangquan", ""], ["Fang", "Jinghui", ""], ["Liang", "Xiaowen", ""], ["Huang", "Ruobing", ""], ["Frangi", "Alejandro", ""], ["Chen", "Zhiyi", ""], ["Ni", "Dong", ""]]}, {"id": "2010.04936", "submitter": "Ramtin Babaeipour", "authors": "Ramtin Babaeipour, Elham Azizi, Hassan Khotanlou", "title": "An Empirical Study on Detecting COVID-19 in Chest X-ray Images Using\n  Deep Learning Based Methods", "comments": "This paper is submitted to the The IKT 2020 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spreading of COVID-19 virus has increased the efforts to provide testing\nkits. Not only the preparation of these kits had been hard, rare, and expensive\nbut also using them is another issue. Results have shown that these kits take\nsome crucial time to recognize the virus, in addition to the fact that they\nencounter with 30% loss. In this paper, we have studied the usage of x-ray\npictures which are ubiquitous, for the classification of COVID-19 chest Xray\nimages, by the existing convolutional neural networks (CNNs). We intend to\ntrain chest x-rays of infected and not infected ones with different CNNs\narchitectures including VGG19, Densnet-121, and Xception. Training these\narchitectures resulted in different accuracies which were much faster and more\nprecise than usual ways of testing.\n", "versions": [{"version": "v1", "created": "Sat, 10 Oct 2020 08:07:19 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Babaeipour", "Ramtin", ""], ["Azizi", "Elham", ""], ["Khotanlou", "Hassan", ""]]}, {"id": "2010.04947", "submitter": "Yong Guo", "authors": "Yong Guo, Qingyao Wu, Chaorui Deng, Jian Chen, Mingkui Tan", "title": "Double Forward Propagation for Memorized Batch Normalization", "comments": "AAAI2018, 8 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Batch Normalization (BN) has been a standard component in designing deep\nneural networks (DNNs). Although the standard BN can significantly accelerate\nthe training of DNNs and improve the generalization performance, it has several\nunderlying limitations which may hamper the performance in both training and\ninference. In the training stage, BN relies on estimating the mean and variance\nof data using a single minibatch. Consequently, BN can be unstable when the\nbatch size is very small or the data is poorly sampled. In the inference stage,\nBN often uses the so called moving mean and moving variance instead of batch\nstatistics, i.e., the training and inference rules in BN are not consistent.\nRegarding these issues, we propose a memorized batch normalization (MBN), which\nconsiders multiple recent batches to obtain more accurate and robust\nstatistics. Note that after the SGD update for each batch, the model parameters\nwill change, and the features will change accordingly, leading to the\nDistribution Shift before and after the update for the considered batch. To\nalleviate this issue, we present a simple Double-Forward scheme in MBN which\ncan further improve the performance. Compared to related methods, the proposed\nMBN exhibits consistent behaviors in both training and inference. Empirical\nresults show that the MBN based models trained with the Double-Forward scheme\ngreatly reduce the sensitivity of data and significantly improve the\ngeneralization performance.\n", "versions": [{"version": "v1", "created": "Sat, 10 Oct 2020 08:48:41 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Guo", "Yong", ""], ["Wu", "Qingyao", ""], ["Deng", "Chaorui", ""], ["Chen", "Jian", ""], ["Tan", "Mingkui", ""]]}, {"id": "2010.04962", "submitter": "Congchong Nie", "authors": "Yanwen Chong, Congchong Nie, Yulong Tao, Xiaoshu Chen, Shaoming Pan", "title": "HCNet: Hierarchical Context Network for Semantic Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Global context information is vital in visual understanding problems,\nespecially in pixel-level semantic segmentation. The mainstream methods adopt\nthe self-attention mechanism to model global context information. However,\npixels belonging to different classes usually have weak feature correlation.\nModeling the global pixel-level correlation matrix indiscriminately is\nextremely redundant in the self-attention mechanism. In order to solve the\nabove problem, we propose a hierarchical context network to differentially\nmodel homogeneous pixels with strong correlations and heterogeneous pixels with\nweak correlations. Specifically, we first propose a multi-scale guided\npre-segmentation module to divide the entire feature map into different\nclassed-based homogeneous regions. Within each homogeneous region, we design\nthe pixel context module to capture pixel-level correlations. Subsequently,\ndifferent from the self-attention mechanism that still models weak\nheterogeneous correlations in a dense pixel-level manner, the region context\nmodule is proposed to model sparse region-level dependencies using a unified\nrepresentation of each region. Through aggregating fine-grained pixel context\nfeatures and coarse-grained region context features, our proposed network can\nnot only hierarchically model global context information but also harvest\nmulti-granularity representations to more robustly identify multi-scale\nobjects. We evaluate our approach on Cityscapes and the ISPRS Vaihingen\ndataset. Without Bells or Whistles, our approach realizes a mean IoU of 82.8%\nand overall accuracy of 91.4% on Cityscapes and ISPRS Vaihingen test set,\nachieving state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Sat, 10 Oct 2020 09:51:17 GMT"}, {"version": "v2", "created": "Tue, 20 Oct 2020 03:06:36 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Chong", "Yanwen", ""], ["Nie", "Congchong", ""], ["Tao", "Yulong", ""], ["Chen", "Xiaoshu", ""], ["Pan", "Shaoming", ""]]}, {"id": "2010.04968", "submitter": "Keren Fu", "authors": "Keren Fu, Yao Jiang, Ge-Peng Ji, Tao Zhou, Qijun Zhao, Deng-Ping Fan", "title": "Light Field Salient Object Detection: A Review and Benchmark", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Salient object detection (SOD) is a long-standing research topic in computer\nvision and has drawn an increasing amount of research interest in the past\ndecade. This paper provides the first comprehensive review and benchmark for\nlight field SOD, which has long been lacking in the saliency community.\nFirstly, we introduce preliminary knowledge on light fields, including theory\nand data forms, and then review existing studies on light field SOD, covering\nten traditional models, seven deep learning-based models, one comparative\nstudy, and one brief review. Existing datasets for light field SOD are also\nsummarized with detailed information and statistical analyses. Secondly, we\nbenchmark nine representative light field SOD models together with several\ncutting-edge RGB-D SOD models on four widely used light field datasets, from\nwhich insightful discussions and analyses, including a comparison between light\nfield SOD and RGB-D SOD models, are achieved. Besides, due to the inconsistency\nof datasets in their current forms, we further generate complete data and\nsupplement focal stacks, depth maps and multi-view images for the inconsistent\ndatasets, making them consistent and unified. Our supplemental data makes a\nuniversal benchmark possible. Lastly, because light field SOD is quite a\nspecial problem attributed to its diverse data representations and high\ndependency on acquisition hardware, making it differ greatly from other\nsaliency detection tasks, we provide nine hints into the challenges and future\ndirections, and outline several open issues. We hope our review and\nbenchmarking could help advance research in this field. All the materials\nincluding collected models, datasets, benchmarking results, and supplemented\nlight field datasets will be publicly available on our project site\nhttps://github.com/kerenfu/LFSOD-Survey.\n", "versions": [{"version": "v1", "created": "Sat, 10 Oct 2020 10:30:40 GMT"}, {"version": "v2", "created": "Sun, 18 Oct 2020 13:40:45 GMT"}, {"version": "v3", "created": "Sat, 23 Jan 2021 06:35:26 GMT"}, {"version": "v4", "created": "Sat, 24 Jul 2021 14:23:26 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Fu", "Keren", ""], ["Jiang", "Yao", ""], ["Ji", "Ge-Peng", ""], ["Zhou", "Tao", ""], ["Zhao", "Qijun", ""], ["Fan", "Deng-Ping", ""]]}, {"id": "2010.04979", "submitter": "Simone Fontana", "authors": "Simone Fontana and Domenico G. Sorrenti", "title": "A Termination Criterion for Probabilistic PointClouds Registration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Probabilistic Point Clouds Registration (PPCR) is an algorithm that, in its\nmulti-iteration version, outperformed state of the art algorithms for local\npoint clouds registration. However, its performances have been tested using a\nfixed high number of iterations. To be of practical usefulness, we think that\nthe algorithm should decide by itself when to stop, to avoid an excessive\nnumber of iterations and, therefore, wasting computational time. With this\nwork, we compare different termination criterion on several datasets and prove\nthat the chosen one produce very good results that are comparable to those\nobtained using a very high number of iterations while saving computational\ntime.\n", "versions": [{"version": "v1", "created": "Sat, 10 Oct 2020 12:02:18 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Fontana", "Simone", ""], ["Sorrenti", "Domenico G.", ""]]}, {"id": "2010.05007", "submitter": "Timofey Grigoryev", "authors": "Denis Kuzminykh, Laida Kushnareva, Timofey Grigoryev, Alexander\n  Zatolokin", "title": "Category-Learning with Context-Augmented Autoencoder", "comments": "11 pages, 12 figures", "journal-ref": "Information Technologies and Computing Systems 3/2020, pp. 30-39", "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding an interpretable non-redundant representation of real-world data is\none of the key problems in Machine Learning. Biological neural networks are\nknown to solve this problem quite well in unsupervised manner, yet unsupervised\nartificial neural networks either struggle to do it or require fine tuning for\neach task individually. We associate this with the fact that a biological brain\nlearns in the context of the relationships between observations, while an\nartificial network does not. We also notice that, though a naive data\naugmentation technique can be very useful for supervised learning problems,\nautoencoders typically fail to generalize transformations from data\naugmentations. Thus, we believe that providing additional knowledge about\nrelationships between data samples will improve model's capability of finding\nuseful inner data representation. More formally, we consider a dataset not as a\nmanifold, but as a category, where the examples are objects. Two these objects\nare connected by a morphism, if they actually represent different\ntransformations of the same entity. Following this formalism, we propose a\nnovel method of using data augmentations when training autoencoders. We train a\nVariational Autoencoder in such a way, that it makes transformation outcome\npredictable by auxiliary network in terms of the hidden representation. We\nbelieve that the classification accuracy of a linear classifier on the learned\nrepresentation is a good metric to measure its interpretability. In our\nexperiments, present approach outperforms $\\beta$-VAE and is comparable with\nGaussian-mixture VAE.\n", "versions": [{"version": "v1", "created": "Sat, 10 Oct 2020 14:04:44 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Kuzminykh", "Denis", ""], ["Kushnareva", "Laida", ""], ["Grigoryev", "Timofey", ""], ["Zatolokin", "Alexander", ""]]}, {"id": "2010.05013", "submitter": "Lidia Talavera-Martinez", "authors": "Lidia Talavera-Mart\\'inez, Pedro Bibiloni, Manuel Gonz\\'alez-Hidalgo", "title": "An Encoder-Decoder CNN for Hair Removal in Dermoscopic Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The process of removing occluding hair has a relevant role in the early and\naccurate diagnosis of skin cancer. It consists of detecting hairs and restore\nthe texture below them, which is sporadically occluded. In this work, we\npresent a model based on convolutional neural networks for hair removal in\ndermoscopic images. During the network's training, we use a combined loss\nfunction to improve the restoration ability of the proposed model. In order to\ntrain the CNN and to quantitatively validate their performance, we simulate the\npresence of skin hair in hairless images extracted from publicly known datasets\nsuch as the PH2, dermquest, dermis, EDRA2002, and the ISIC Data Archive. As far\nas we know, there is no other hair removal method based on deep learning. Thus,\nwe compare our results with six state-of-the-art algorithms based on\ntraditional computer vision techniques by means of similarity measures that\ncompare the reference hairless image and the one with hair simulated. Finally,\na statistical test is used to compare the methods. Both qualitative and\nquantitative results demonstrate the effectiveness of our network.\n", "versions": [{"version": "v1", "created": "Sat, 10 Oct 2020 14:28:10 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Talavera-Mart\u00ednez", "Lidia", ""], ["Bibiloni", "Pedro", ""], ["Gonz\u00e1lez-Hidalgo", "Manuel", ""]]}, {"id": "2010.05027", "submitter": "Jun Wang", "authors": "Jun Wang, Qianying Liu, Haotian Xie, Zhaogang Yang, Hefeng Zhou", "title": "Boosted EfficientNet: Detection of Lymph Node Metastases in Breast\n  Cancer Using Convolutional Neural Network", "comments": "17 pages, 7 figures, 2 tables, journal", "journal-ref": null, "doi": "10.3390/cancers13040661", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, advances in the development of whole-slide images have laid\na foundation for the utilization of digital images in pathology. With the\nassistance of computer images analysis that automatically identifies tissue or\ncell types, they have greatly improved the histopathologic interpretation and\ndiagnosis accuracy. In this paper, the Convolutional Neutral Network (CNN) has\nbeen adapted to predict and classify lymph node metastasis in breast cancer.\nUnlike traditional image cropping methods that are only suitable for large\nresolution images, we propose a novel data augmentation method named Random\nCenter Cropping (RCC) to facilitate small resolution images. RCC enriches the\ndatasets while retaining the image resolution and the center area of images. In\naddition, we reduce the downsampling scale of the network to further facilitate\nsmall resolution images better. Moreover, Attention and Feature Fusion (FF)\nmechanisms are employed to improve the semantic information of images.\nExperiments demonstrate that our methods boost performances of basic CNN\narchitectures. And the best-performed method achieves an accuracy of 97.96% and\nan AUC of 99.68% on RPCam datasets, respectively.\n", "versions": [{"version": "v1", "created": "Sat, 10 Oct 2020 15:18:56 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Wang", "Jun", ""], ["Liu", "Qianying", ""], ["Xie", "Haotian", ""], ["Yang", "Zhaogang", ""], ["Zhou", "Hefeng", ""]]}, {"id": "2010.05045", "submitter": "Quanshi Zhang", "authors": "Hao Zhang, Yichen Xie, Longjie Zheng, Die Zhang, Quanshi Zhang", "title": "Interpreting Multivariate Shapley Interactions in DNNs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper aims to explain deep neural networks (DNNs) from the perspective\nof multivariate interactions. In this paper, we define and quantify the\nsignificance of interactions among multiple input variables of the DNN. Input\nvariables with strong interactions usually form a coalition and reflect\nprototype features, which are memorized and used by the DNN for inference. We\ndefine the significance of interactions based on the Shapley value, which is\ndesigned to assign the attribution value of each input variable to the\ninference. We have conducted experiments with various DNNs. Experimental\nresults have demonstrated the effectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Sat, 10 Oct 2020 17:02:51 GMT"}, {"version": "v2", "created": "Thu, 15 Oct 2020 05:51:48 GMT"}, {"version": "v3", "created": "Sat, 16 Jan 2021 15:14:41 GMT"}, {"version": "v4", "created": "Wed, 3 Feb 2021 09:12:47 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Zhang", "Hao", ""], ["Xie", "Yichen", ""], ["Zheng", "Longjie", ""], ["Zhang", "Die", ""], ["Zhang", "Quanshi", ""]]}, {"id": "2010.05063", "submitter": "Yaoyao Liu", "authors": "Yaoyao Liu, Bernt Schiele, Qianru Sun", "title": "Adaptive Aggregation Networks for Class-Incremental Learning", "comments": "Accepted to CVPR 2021. Code:\n  https://github.com/yaoyao-liu/class-incremental-learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Class-Incremental Learning (CIL) aims to learn a classification model with\nthe number of classes increasing phase-by-phase. An inherent problem in CIL is\nthe stability-plasticity dilemma between the learning of old and new classes,\ni.e., high-plasticity models easily forget old classes, but high-stability\nmodels are weak to learn new classes. We alleviate this issue by proposing a\nnovel network architecture called Adaptive Aggregation Networks (AANets), in\nwhich we explicitly build two types of residual blocks at each residual level\n(taking ResNet as the baseline architecture): a stable block and a plastic\nblock. We aggregate the output feature maps from these two blocks and then feed\nthe results to the next-level blocks. We adapt the aggregation weights in order\nto balance these two types of blocks, i.e., to balance stability and\nplasticity, dynamically. We conduct extensive experiments on three CIL\nbenchmarks: CIFAR-100, ImageNet-Subset, and ImageNet, and show that many\nexisting CIL methods can be straightforwardly incorporated into the\narchitecture of AANets to boost their performances.\n", "versions": [{"version": "v1", "created": "Sat, 10 Oct 2020 18:24:24 GMT"}, {"version": "v2", "created": "Wed, 16 Dec 2020 12:19:15 GMT"}, {"version": "v3", "created": "Mon, 29 Mar 2021 22:09:07 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Liu", "Yaoyao", ""], ["Schiele", "Bernt", ""], ["Sun", "Qianru", ""]]}, {"id": "2010.05069", "submitter": "Fatemeh Azimi", "authors": "Fatemeh Azimi and Stanislav Frolov and Federico Raue and Joern Hees\n  and Andreas Dengel", "title": "Hybrid-S2S: Video Object Segmentation with Recurrent Networks and\n  Correspondence Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One-shot Video Object Segmentation~(VOS) is the task of pixel-wise tracking\nan object of interest within a video sequence, where the segmentation mask of\nthe first frame is given at inference time. In recent years, Recurrent Neural\nNetworks~(RNNs) have been widely used for VOS tasks, but they often suffer from\nlimitations such as drift and error propagation. In this work, we study an\nRNN-based architecture and address some of these issues by proposing a hybrid\nsequence-to-sequence architecture named HS2S, utilizing a dual mask propagation\nstrategy that allows incorporating the information obtained from correspondence\nmatching. Our experiments show that augmenting the RNN with correspondence\nmatching is a highly effective solution to reduce the drift problem. The\nadditional information helps the model to predict more accurate masks and makes\nit robust against error propagation. We evaluate our HS2S model on the\nDAVIS2017 dataset as well as Youtube-VOS. On the latter, we achieve an\nimprovement of 11.2pp in the overall segmentation accuracy over RNN-based\nstate-of-the-art methods in VOS. We analyze our model's behavior in challenging\ncases such as occlusion and long sequences and show that our hybrid\narchitecture significantly enhances the segmentation quality in these difficult\nscenarios.\n", "versions": [{"version": "v1", "created": "Sat, 10 Oct 2020 19:00:43 GMT"}, {"version": "v2", "created": "Sat, 7 Nov 2020 09:33:51 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Azimi", "Fatemeh", ""], ["Frolov", "Stanislav", ""], ["Raue", "Federico", ""], ["Hees", "Joern", ""], ["Dengel", "Andreas", ""]]}, {"id": "2010.05099", "submitter": "Thomas Tanay", "authors": "Thomas Tanay, Aivar Sootla, Matteo Maggioni, Puneet K. Dokania, Philip\n  Torr, Ales Leonardis and Gregory Slabaugh", "title": "Diagnosing and Preventing Instabilities in Recurrent Video Processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent models are becoming a popular choice for video enhancement tasks\nsuch as video denoising. In this work, we focus on their stability as dynamical\nsystems and show that they tend to fail catastrophically at inference time on\nlong video sequences. To address this issue, we (1) introduce a diagnostic tool\nwhich produces adversarial input sequences optimized to trigger instabilities\nand that can be interpreted as visualizations of spatio-temporal receptive\nfields, and (2) propose two approaches to enforce the stability of a model:\nconstraining the spectral norm or constraining the stable rank of its\nconvolutional layers. We then introduce Stable Rank Normalization of the Layers\n(SRNL), a new algorithm that enforces these constraints, and verify\nexperimentally that it successfully results in stable recurrent video\nprocessing.\n", "versions": [{"version": "v1", "created": "Sat, 10 Oct 2020 21:39:28 GMT"}, {"version": "v2", "created": "Sat, 17 Oct 2020 14:44:21 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Tanay", "Thomas", ""], ["Sootla", "Aivar", ""], ["Maggioni", "Matteo", ""], ["Dokania", "Puneet K.", ""], ["Torr", "Philip", ""], ["Leonardis", "Ales", ""], ["Slabaugh", "Gregory", ""]]}, {"id": "2010.05119", "submitter": "Ad\\'in Ram\\'irez Rivera", "authors": "Ad\\'in Ram\\'irez Rivera, Adil Khan, Imad E. I. Bekkouch, Taimoor S.\n  Sheikh", "title": "Anomaly Detection based on Zero-Shot Outlier Synthesis and Hierarchical\n  Feature Distillation", "comments": "To appear in IEEE Trans. on Neural Networks and Learning Systems", "journal-ref": null, "doi": "10.1109/TNNLS.2020.3027667", "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anomaly detection suffers from unbalanced data since anomalies are quite\nrare. Synthetically generated anomalies are a solution to such ill or not fully\ndefined data. However, synthesis requires an expressive representation to\nguarantee the quality of the generated data. In this paper, we propose a\ntwo-level hierarchical latent space representation that distills inliers'\nfeature-descriptors (through autoencoders) into more robust representations\nbased on a variational family of distributions (through a variational\nautoencoder) for zero-shot anomaly generation. From the learned latent\ndistributions, we select those that lie on the outskirts of the training data\nas synthetic-outlier generators. And, we synthesize from them, i.e., generate\nnegative samples without seen them before, to train binary classifiers. We\nfound that the use of the proposed hierarchical structure for feature\ndistillation and fusion creates robust and general representations that allow\nus to synthesize pseudo outlier samples. And in turn, train robust binary\nclassifiers for true outlier detection (without the need for actual outliers\nduring training). We demonstrate the performance of our proposal on several\nbenchmarks for anomaly detection.\n", "versions": [{"version": "v1", "created": "Sat, 10 Oct 2020 23:34:02 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Rivera", "Ad\u00edn Ram\u00edrez", ""], ["Khan", "Adil", ""], ["Bekkouch", "Imad E. I.", ""], ["Sheikh", "Taimoor S.", ""]]}, {"id": "2010.05123", "submitter": "Jatin Sharma", "authors": "Jatin Sharma and Jon Campbell and Pete Ansell and Jay Beavers and\n  Christopher O'Dowd", "title": "Towards Hardware-Agnostic Gaze-Trackers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaze-tracking is a novel way of interacting with computers which allows new\nscenarios, such as enabling people with motor-neuron disabilities to control\ntheir computers or doctors to interact with patient information without\ntouching screen or keyboard. Further, there are emerging applications of\ngaze-tracking in interactive gaming, user experience research, human attention\nanalysis and behavioral studies. Accurate estimation of the gaze may involve\naccounting for head-pose, head-position, eye rotation, distance from the object\nas well as operating conditions such as illumination, occlusion, background\nnoise and various biological aspects of the user. Commercially available\ngaze-trackers utilize specialized sensor assemblies that usually consist of an\ninfrared light source and camera. There are several challenges in the universal\nproliferation of gaze-tracking as accessibility technologies, specifically its\naffordability, reliability, and ease-of-use. In this paper, we try to address\nthese challenges through the development of a hardware-agnostic gaze-tracker.\nWe present a deep neural network architecture as an appearance-based method for\nconstrained gaze-tracking that utilizes facial imagery captured on an ordinary\nRGB camera ubiquitous in all modern computing devices. Our system achieved an\nerror of 1.8073cm on GazeCapture dataset without any calibration or device\nspecific fine-tuning. This research shows promise that one day soon any\ncomputer, tablet, or phone will be controllable using just your eyes due to the\nprediction capabilities of deep neutral networks.\n", "versions": [{"version": "v1", "created": "Sun, 11 Oct 2020 00:53:57 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Sharma", "Jatin", ""], ["Campbell", "Jon", ""], ["Ansell", "Pete", ""], ["Beavers", "Jay", ""], ["O'Dowd", "Christopher", ""]]}, {"id": "2010.05131", "submitter": "Yuanxin Wu", "authors": "Shengjie Li, Qi Cai and Yuanxin Wu", "title": "Segmenting Epipolar Line", "comments": "5 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying feature correspondence between two images is a fundamental\nprocedure in three-dimensional computer vision. Usually the feature search\nspace is confined by the epipolar line. Using the cheirality constraint, this\npaper finds that the feature search space can be restrained to one of two or\nthree segments of the epipolar line that are defined by the epipole and a\nso-called virtual infinity point.\n", "versions": [{"version": "v1", "created": "Sun, 11 Oct 2020 01:20:55 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Li", "Shengjie", ""], ["Cai", "Qi", ""], ["Wu", "Yuanxin", ""]]}, {"id": "2010.05133", "submitter": "Xiaoli Liu", "authors": "Xiaoli Liu and Jianqin Yin", "title": "SDMTL: Semi-Decoupled Multi-grained Trajectory Learning for 3D human\n  motion prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting future human motion is critical for intelligent robots to interact\nwith humans in the real world, and human motion has the nature of\nmulti-granularity. However, most of the existing work either implicitly modeled\nmulti-granularity information via fixed modes or focused on modeling a single\ngranularity, making it hard to well capture this nature for accurate\npredictions. In contrast, we propose a novel end-to-end network, Semi-Decoupled\nMulti-grained Trajectory Learning network (SDMTL), to predict future poses,\nwhich not only flexibly captures rich multi-grained trajectory information but\nalso aggregates multi-granularity information for predictions. Specifically, we\nfirst introduce a Brain-inspired Semi-decoupled Motion-sensitive Encoding\nmodule (BSME), effectively capturing spatiotemporal features in a\nsemi-decoupled manner. Then, we capture the temporal dynamics of motion\ntrajectory at multi-granularity, including fine granularity and coarse\ngranularity. We learn multi-grained trajectory information using BSMEs\nhierarchically and further capture the information of temporal evolutional\ndirections at each granularity by gathering the outputs of BSMEs at each\ngranularity and applying temporal convolutions along the motion trajectory.\nNext, the captured motion dynamics can be further enhanced by aggregating the\ninformation of multi-granularity with a weighted summation scheme. Finally,\nexperimental results on two benchmarks, including Human3.6M and CMU-Mocap, show\nthat our method achieves state-of-the-art performance, demonstrating the\neffectiveness of our proposed method. The code will be available if the paper\nis accepted.\n", "versions": [{"version": "v1", "created": "Sun, 11 Oct 2020 01:29:21 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Liu", "Xiaoli", ""], ["Yin", "Jianqin", ""]]}, {"id": "2010.05149", "submitter": "Yanlin Qian", "authors": "Yanlin Qian and Sibo Feng and Kang Qian and Miaofeng Wang", "title": "SDE-AWB: a Generic Solution for 2nd International Illumination\n  Estimation Challenge", "comments": "6 pages; challenge paper for 2nd International Illumination\n  Estimation Challenge, held in International Conference on Machine Vision 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a neural network-based solution for three different tracks of 2nd\nInternational Illumination Estimation Challenge (chromaticity.iitp.ru). Our\nmethod is built on pre-trained Squeeze-Net backbone, differential 2D chroma\nhistogram layer and a shallow MLP utilizing Exif information. By combining\nsemantic feature, color feature and Exif metadata, the resulting method --\nSDE-AWB -- obtains 1st place in both indoor and two-illuminant tracks and 2nd\nplace in general track.\n", "versions": [{"version": "v1", "created": "Sun, 11 Oct 2020 03:31:49 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Qian", "Yanlin", ""], ["Feng", "Sibo", ""], ["Qian", "Kang", ""], ["Wang", "Miaofeng", ""]]}, {"id": "2010.05177", "submitter": "Cyril Zakka", "authors": "Cyril Zakka, Ghida Saheb, Elie Najem, Ghina Berjawi", "title": "MammoGANesis: Controlled Generation of High-Resolution Mammograms for\n  Radiology Education", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  During their formative years, radiology trainees are required to interpret\nhundreds of mammograms per month, with the objective of becoming apt at\ndiscerning the subtle patterns differentiating benign from malignant lesions.\nUnfortunately, medico-legal and technical hurdles make it difficult to access\nand query medical images for training.\n  In this paper we train a generative adversarial network (GAN) to synthesize\n512 x 512 high-resolution mammograms. The resulting model leads to the\nunsupervised separation of high-level features (e.g. the standard mammography\nviews and the nature of the breast lesions), with stochastic variation in the\ngenerated images (e.g. breast adipose tissue, calcification), enabling\nuser-controlled global and local attribute-editing of the synthesized images.\n  We demonstrate the model's ability to generate anatomically and medically\nrelevant mammograms by achieving an average AUC of 0.54 in a double-blind study\non four expert mammography radiologists to distinguish between generated and\nreal images, ascribing to the high visual quality of the synthesized and edited\nmammograms, and to their potential use in advancing and facilitating medical\neducation.\n", "versions": [{"version": "v1", "created": "Sun, 11 Oct 2020 06:47:56 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Zakka", "Cyril", ""], ["Saheb", "Ghida", ""], ["Najem", "Elie", ""], ["Berjawi", "Ghina", ""]]}, {"id": "2010.05185", "submitter": "Chenhui Chu", "authors": "Chenhui Chu, Yuto Takebayashi, Mishra Vipul, Yuta Nakashima", "title": "Constructing a Visual Relationship Authenticity Dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A visual relationship denotes a relationship between two objects in an image,\nwhich can be represented as a triplet of (subject; predicate; object). Visual\nrelationship detection is crucial for scene understanding in images. Existing\nvisual relationship detection datasets only contain true relationships that\ncorrectly describe the content in an image. However, distinguishing false\nvisual relationships from true ones is also crucial for image understanding and\ngrounded natural language processing. In this paper, we construct a visual\nrelationship authenticity dataset, where both true and false relationships\namong all objects appeared in the captions in the Flickr30k entities image\ncaption dataset are annotated. The dataset is available at\nhttps://github.com/codecreator2053/VR_ClassifiedDataset. We hope that this\ndataset can promote the study on both vision and language understanding.\n", "versions": [{"version": "v1", "created": "Sun, 11 Oct 2020 07:38:33 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Chu", "Chenhui", ""], ["Takebayashi", "Yuto", ""], ["Vipul", "Mishra", ""], ["Nakashima", "Yuta", ""]]}, {"id": "2010.05210", "submitter": "Zhuotao Tian", "authors": "Zhuotao Tian, Xin Lai, Li Jiang, Michelle Shu, Hengshuang Zhao, Jiaya\n  Jia", "title": "Generalized Few-Shot Semantic Segmentation", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training semantic segmentation models requires a large amount of finely\nannotated data, making it hard to quickly adapt to novel classes not satisfying\nthis condition. Few-Shot Segmentation (FS-Seg) tackles this problem with many\nconstraints. In this paper, we introduce a new benchmark, called Generalized\nFew-Shot Semantic Segmentation (GFS-Seg), to analyze the generalization ability\nof segmentation models to simultaneously recognize novel categories with very\nfew examples as well as base categories with sufficient examples. Previous\nstate-of-the-art FS-Seg methods fall short in GFS-Seg and the performance\ndiscrepancy mainly comes from the constrained training setting of FS-Seg. To\nmake GFS-Seg tractable, we set up a GFS-Seg baseline that achieves decent\nperformance without structural change on the original model. Then, as context\nis the key for boosting performance on semantic segmentation, we propose the\nContext-Aware Prototype Learning (CAPL) that significantly improves performance\nby leveraging the contextual information to update class prototypes with\naligned features. Extensive experiments on Pascal-VOC and COCO manifest the\neffectiveness of CAPL, and CAPL also generalizes well to FS-Seg.\n", "versions": [{"version": "v1", "created": "Sun, 11 Oct 2020 10:13:21 GMT"}, {"version": "v2", "created": "Thu, 19 Nov 2020 12:18:13 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Tian", "Zhuotao", ""], ["Lai", "Xin", ""], ["Jiang", "Li", ""], ["Shu", "Michelle", ""], ["Zhao", "Hengshuang", ""], ["Jia", "Jiaya", ""]]}, {"id": "2010.05212", "submitter": "Ushasi Chaudhuri", "authors": "Ushasi Chaudhuri, Syomantak Chaudhuri, Subhasis Chaudhuri", "title": "GuCNet: A Guided Clustering-based Network for Improved Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": "Accepted in International Conference on Pattern Recognition (ICPR)\n  2020", "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We deal with the problem of semantic classification of challenging and\nhighly-cluttered dataset. We present a novel, and yet a very simple\nclassification technique by leveraging the ease of classifiability of any\nexisting well separable dataset for guidance. Since the guide dataset which may\nor may not have any semantic relationship with the experimental dataset, forms\nwell separable clusters in the feature set, the proposed network tries to embed\nclass-wise features of the challenging dataset to those distinct clusters of\nthe guide set, making them more separable. Depending on the availability, we\npropose two types of guide sets: one using texture (image) guides and another\nusing prototype vectors representing cluster centers. Experimental results\nobtained on the challenging benchmark RSSCN, LSUN, and TU-Berlin datasets\nestablish the efficacy of the proposed method as we outperform the existing\nstate-of-the-art techniques by a considerable margin.\n", "versions": [{"version": "v1", "created": "Sun, 11 Oct 2020 10:22:03 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Chaudhuri", "Ushasi", ""], ["Chaudhuri", "Syomantak", ""], ["Chaudhuri", "Subhasis", ""]]}, {"id": "2010.05222", "submitter": "Xiang An", "authors": "Xiang An, Xuhan Zhu, Yang Xiao, Lan Wu, Ming Zhang, Yuan Gao, Bin Qin,\n  Debing Zhang, Ying Fu", "title": "Partial FC: Training 10 Million Identities on a Single Machine", "comments": "8 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face recognition has been an active and vital topic among computer vision\ncommunity for a long time. Previous researches mainly focus on loss functions\nused for facial feature extraction network, among which the improvements of\nsoftmax-based loss functions greatly promote the performance of face\nrecognition. However, the contradiction between the drastically increasing\nnumber of face identities and the shortage of GPU memories is gradually\nbecoming irreconcilable. In this paper, we thoroughly analyze the optimization\ngoal of softmax-based loss functions and the difficulty of training massive\nidentities. We find that the importance of negative classes in softmax function\nin face representation learning is not as high as we previously thought. The\nexperiment demonstrates no loss of accuracy when training with only 10\\%\nrandomly sampled classes for the softmax-based loss functions, compared with\ntraining with full classes using state-of-the-art models on mainstream\nbenchmarks. We also implement a very efficient distributed sampling algorithm,\ntaking into account model accuracy and training efficiency, which uses only\neight NVIDIA RTX2080Ti to complete classification tasks with tens of millions\nof identities. The code of this paper has been made available\nhttps://github.com/deepinsight/insightface/tree/master/recognition/partial_fc.\n", "versions": [{"version": "v1", "created": "Sun, 11 Oct 2020 11:15:26 GMT"}, {"version": "v2", "created": "Sat, 23 Jan 2021 05:25:06 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["An", "Xiang", ""], ["Zhu", "Xuhan", ""], ["Xiao", "Yang", ""], ["Wu", "Lan", ""], ["Zhang", "Ming", ""], ["Gao", "Yuan", ""], ["Qin", "Bin", ""], ["Zhang", "Debing", ""], ["Fu", "Ying", ""]]}, {"id": "2010.05250", "submitter": "Jian Liang", "authors": "Jian Liang, Yuren Cao, Shuang Li, Bing Bai, Hao Li, Fei Wang, Kun Bai", "title": "Domain Agnostic Learning for Unbiased Authentication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Authentication is the task of confirming the matching relationship between a\ndata instance and a given identity. Typical examples of authentication problems\ninclude face recognition and person re-identification. Data-driven\nauthentication could be affected by undesired biases, i.e., the models are\noften trained in one domain (e.g., for people wearing spring outfits) while\napplied in other domains (e.g., they change the clothes to summer outfits).\nPrevious works have made efforts to eliminate domain-difference. They typically\nassume domain annotations are provided, and all the domains share classes.\nHowever, for authentication, there could be a large number of domains shared by\ndifferent identities/classes, and it is impossible to annotate these domains\nexhaustively. It could make domain-difference challenging to model and\neliminate. In this paper, we propose a domain-agnostic method that eliminates\ndomain-difference without domain labels. We alternately perform latent domain\ndiscovery and domain-difference elimination until our model no longer detects\ndomain-difference. In our approach, the latent domains are discovered by\nlearning the heterogeneous predictive relationships between inputs and outputs.\nThen domain-difference is eliminated in both class-dependent and\nclass-independent spaces to improve robustness of elimination. We further\nextend our method to a meta-learning framework to pursue more thorough\ndomain-difference elimination. Comprehensive empirical evaluation results are\nprovided to demonstrate the effectiveness and superiority of our proposed\nmethod.\n", "versions": [{"version": "v1", "created": "Sun, 11 Oct 2020 14:05:16 GMT"}, {"version": "v2", "created": "Mon, 23 Nov 2020 09:13:33 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Liang", "Jian", ""], ["Cao", "Yuren", ""], ["Li", "Shuang", ""], ["Bai", "Bing", ""], ["Li", "Hao", ""], ["Wang", "Fei", ""], ["Bai", "Kun", ""]]}, {"id": "2010.05259", "submitter": "Lei Luo", "authors": "Lei Luo, William Hsu, and Shangxian Wang", "title": "Shape-aware Generative Adversarial Networks for Attribute Transfer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative adversarial networks (GANs) have been successfully applied to\ntransfer visual attributes in many domains, including that of human face\nimages. This success is partly attributable to the facts that human faces have\nsimilar shapes and the positions of eyes, noses, and mouths are fixed among\ndifferent people. Attribute transfer is more challenging when the source and\ntarget domain share different shapes. In this paper, we introduce a shape-aware\nGAN model that is able to preserve shape when transferring attributes, and\npropose its application to some real-world domains. Compared to other\nstate-of-art GANs-based image-to-image translation models, the model we propose\nis able to generate more visually appealing results while maintaining the\nquality of results from transfer learning.\n", "versions": [{"version": "v1", "created": "Sun, 11 Oct 2020 14:52:32 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Luo", "Lei", ""], ["Hsu", "William", ""], ["Wang", "Shangxian", ""]]}, {"id": "2010.05260", "submitter": "Chao Ma", "authors": "Chao Ma, Guohua Gu, Xin Miao, Minjie Wan, Weixian Qian, Kan Ren, and\n  Qian Chen", "title": "Infrared target tracking based on proximal robust principal component\n  analysis method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Infrared target tracking plays an important role in both civil and military\nfields. The main challenges in designing a robust and high-precision tracker\nfor infrared sequences include overlap, occlusion and appearance change. To\nthis end, this paper proposes an infrared target tracker based on proximal\nrobust principal component analysis method. Firstly, the observation matrix is\ndecomposed into a sparse occlusion matrix and a low-rank target matrix, and the\nconstraint optimization is carried out with an approaching proximal norm which\nis better than L1-norm. To solve this convex optimization problem, Alternating\nDirection Method of Multipliers (ADMM) is employed to estimate the variables\nalternately. Finally, the framework of particle filter with model update\nstrategy is exploited to locate the target. Through a series of experiments on\nreal infrared target sequences, the effectiveness and robustness of our\nalgorithm are proved.\n", "versions": [{"version": "v1", "created": "Sun, 11 Oct 2020 14:54:00 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Ma", "Chao", ""], ["Gu", "Guohua", ""], ["Miao", "Xin", ""], ["Wan", "Minjie", ""], ["Qian", "Weixian", ""], ["Ren", "Kan", ""], ["Chen", "Qian", ""]]}, {"id": "2010.05264", "submitter": "Junfu Pu", "authors": "Junfu Pu, Wengang Zhou, Hezhen Hu, Houqiang Li", "title": "Boosting Continuous Sign Language Recognition via Cross Modality\n  Augmentation", "comments": "Accepted to ACM Multimedia 2020", "journal-ref": null, "doi": "10.1145/3394171.3413931", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continuous sign language recognition (SLR) deals with unaligned video-text\npair and uses the word error rate (WER), i.e., edit distance, as the main\nevaluation metric. Since it is not differentiable, we usually instead optimize\nthe learning model with the connectionist temporal classification (CTC)\nobjective loss, which maximizes the posterior probability over the sequential\nalignment. Due to the optimization gap, the predicted sentence with the highest\ndecoding probability may not be the best choice under the WER metric. To tackle\nthis issue, we propose a novel architecture with cross modality augmentation.\nSpecifically, we first augment cross-modal data by simulating the calculation\nprocedure of WER, i.e., substitution, deletion and insertion on both text label\nand its corresponding video. With these real and generated pseudo video-text\npairs, we propose multiple loss terms to minimize the cross modality distance\nbetween the video and ground truth label, and make the network distinguish the\ndifference between real and pseudo modalities. The proposed framework can be\neasily extended to other existing CTC based continuous SLR architectures.\nExtensive experiments on two continuous SLR benchmarks, i.e.,\nRWTH-PHOENIX-Weather and CSL, validate the effectiveness of our proposed\nmethod.\n", "versions": [{"version": "v1", "created": "Sun, 11 Oct 2020 15:07:50 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Pu", "Junfu", ""], ["Zhou", "Wengang", ""], ["Hu", "Hezhen", ""], ["Li", "Houqiang", ""]]}, {"id": "2010.05272", "submitter": "Ziyi Wu", "authors": "Ziyi Wu, Yueqi Duan, He Wang, Qingnan Fan, Leonidas J. Guibas", "title": "IF-Defense: 3D Adversarial Point Cloud Defense via Implicit Function\n  based Restoration", "comments": "17 pages, 8 figures. Update several experimental results compared\n  with v2, e.g. cross dataset evaluation and baseline results of adversarial\n  training", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point cloud is an important 3D data representation widely used in many\nessential applications. Leveraging deep neural networks, recent works have\nshown great success in processing 3D point clouds. However, those deep neural\nnetworks are vulnerable to various 3D adversarial attacks, which can be\nsummarized as two primary types: point perturbation that affects local point\ndistribution, and surface distortion that causes dramatic changes in geometry.\nIn this paper, we simultaneously address both the aforementioned attacks by\nlearning to restore the clean point clouds from the attacked ones. More\nspecifically, we propose an IF-Defense framework to directly optimize the\ncoordinates of input points with geometry-aware and distribution-aware\nconstraints. The former aims to recover the surface of point cloud through\nimplicit function, while the latter encourages evenly-distributed points. Our\nexperimental results show that IF-Defense achieves the state-of-the-art defense\nperformance against existing 3D adversarial attacks on PointNet, PointNet++,\nDGCNN, PointConv and RS-CNN. For example, compared with previous methods,\nIF-Defense presents 20.02% improvement in classification accuracy against\nsalient point dropping attack and 16.29% against LG-GAN attack on PointNet. Our\ncode is available at https://github.com/Wuziyi616/IF-Defense.\n", "versions": [{"version": "v1", "created": "Sun, 11 Oct 2020 15:36:40 GMT"}, {"version": "v2", "created": "Tue, 17 Nov 2020 04:57:19 GMT"}, {"version": "v3", "created": "Thu, 18 Mar 2021 14:43:20 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Wu", "Ziyi", ""], ["Duan", "Yueqi", ""], ["Wang", "He", ""], ["Fan", "Qingnan", ""], ["Guibas", "Leonidas J.", ""]]}, {"id": "2010.05300", "submitter": "Yulin Wang", "authors": "Yulin Wang, Kangchen Lv, Rui Huang, Shiji Song, Le Yang, Gao Huang", "title": "Glance and Focus: a Dynamic Approach to Reducing Spatial Redundancy in\n  Image Classification", "comments": "Accepted by NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The accuracy of deep convolutional neural networks (CNNs) generally improves\nwhen fueled with high resolution images. However, this often comes at a high\ncomputational cost and high memory footprint. Inspired by the fact that not all\nregions in an image are task-relevant, we propose a novel framework that\nperforms efficient image classification by processing a sequence of relatively\nsmall inputs, which are strategically selected from the original image with\nreinforcement learning. Such a dynamic decision process naturally facilitates\nadaptive inference at test time, i.e., it can be terminated once the model is\nsufficiently confident about its prediction and thus avoids further redundant\ncomputation. Notably, our framework is general and flexible as it is compatible\nwith most of the state-of-the-art light-weighted CNNs (such as MobileNets,\nEfficientNets and RegNets), which can be conveniently deployed as the backbone\nfeature extractor. Experiments on ImageNet show that our method consistently\nimproves the computational efficiency of a wide variety of deep models. For\nexample, it further reduces the average latency of the highly efficient\nMobileNet-V3 on an iPhone XS Max by 20% without sacrificing accuracy. Code and\npre-trained models are available at\nhttps://github.com/blackfeather-wang/GFNet-Pytorch.\n", "versions": [{"version": "v1", "created": "Sun, 11 Oct 2020 17:55:06 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Wang", "Yulin", ""], ["Lv", "Kangchen", ""], ["Huang", "Rui", ""], ["Song", "Shiji", ""], ["Yang", "Le", ""], ["Huang", "Gao", ""]]}, {"id": "2010.05302", "submitter": "Enric Corona", "authors": "Wen Guo, Enric Corona, Francesc Moreno-Noguer, Xavier Alameda-Pineda", "title": "PI-Net: Pose Interacting Network for Multi-Person Monocular 3D Pose\n  Estimation", "comments": "Accepted at WACV 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent literature addressed the monocular 3D pose estimation task very\nsatisfactorily. In these studies, different persons are usually treated as\nindependent pose instances to estimate. However, in many every-day situations,\npeople are interacting, and the pose of an individual depends on the pose of\nhis/her interactees. In this paper, we investigate how to exploit this\ndependency to enhance current - and possibly future - deep networks for 3D\nmonocular pose estimation. Our pose interacting network, or PI-Net, inputs the\ninitial pose estimates of a variable number of interactees into a recurrent\narchitecture used to refine the pose of the person-of-interest. Evaluating such\na method is challenging due to the limited availability of public annotated\nmulti-person 3D human pose datasets. We demonstrate the effectiveness of our\nmethod in the MuPoTS dataset, setting the new state-of-the-art on it.\nQualitative results on other multi-person datasets (for which 3D pose\nground-truth is not available) showcase the proposed PI-Net. PI-Net is\nimplemented in PyTorch and the code will be made available upon acceptance of\nthe paper.\n", "versions": [{"version": "v1", "created": "Sun, 11 Oct 2020 17:58:24 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Guo", "Wen", ""], ["Corona", "Enric", ""], ["Moreno-Noguer", "Francesc", ""], ["Alameda-Pineda", "Xavier", ""]]}, {"id": "2010.05309", "submitter": "Peri Akiva", "authors": "Peri Akiva, Matthew Purri, Kristin Dana, Beth Tellman, Tyler Anderson", "title": "H2O-Net: Self-Supervised Flood Segmentation via Adversarial Domain\n  Adaptation and Label Refinement", "comments": "Submitted to WACV2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate flood detection in near real time via high resolution, high latency\nsatellite imagery is essential to prevent loss of lives by providing quick and\nactionable information. Instruments and sensors useful for flood detection are\nonly available in low resolution, low latency satellites with region re-visit\nperiods of up to 16 days, making flood alerting systems that use such\nsatellites unreliable. This work presents H2O-Network, a self supervised deep\nlearning method to segment floods from satellites and aerial imagery by\nbridging domain gap between low and high latency satellite and coarse-to-fine\nlabel refinement. H2O-Net learns to synthesize signals highly correlative with\nwater presence as a domain adaptation step for semantic segmentation in high\nresolution satellite imagery. Our work also proposes a self-supervision\nmechanism, which does not require any hand annotation, used during training to\ngenerate high quality ground truth data. We demonstrate that H2O-Net\noutperforms the state-of-the-art semantic segmentation methods on satellite\nimagery by 10% and 12% pixel accuracy and mIoU respectively for the task of\nflood segmentation. We emphasize the generalizability of our model by\ntransferring model weights trained on satellite imagery to drone imagery, a\nhighly different sensor and domain.\n", "versions": [{"version": "v1", "created": "Sun, 11 Oct 2020 18:35:03 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Akiva", "Peri", ""], ["Purri", "Matthew", ""], ["Dana", "Kristin", ""], ["Tellman", "Beth", ""], ["Anderson", "Tyler", ""]]}, {"id": "2010.05322", "submitter": "Hieu Vu", "authors": "Hieu M. Vu, Diep Thi-Ngoc Nguyen", "title": "Revising FUNSD dataset for key-value detection in document images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  FUNSD is one of the limited publicly available datasets for information\nextraction from document im-ages. The information in the FUNSD dataset is\ndefined by text areas of four categories (\"key\", \"value\", \"header\", \"other\",\nand \"background\") and connectivity between areas as key-value relations.\nIn-specting FUNSD, we found several inconsistency in labeling, which impeded\nits applicability to thekey-value extraction problem. In this report, we\ndescribed some labeling issues in FUNSD and therevision we made to the dataset.\nWe also reported our implementation of for key-value detection onFUNSD using a\nUNet model as baseline results and an improved UNet model with\nChannel-InvariantDeformable Convolution.\n", "versions": [{"version": "v1", "created": "Sun, 11 Oct 2020 19:06:56 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Vu", "Hieu M.", ""], ["Nguyen", "Diep Thi-Ngoc", ""]]}, {"id": "2010.05334", "submitter": "Justin Pinkney", "authors": "Justin N. M. Pinkney and Doron Adler", "title": "Resolution Dependent GAN Interpolation for Controllable Image Synthesis\n  Between Domains", "comments": "2 pages, 3 figures. Accepted to Machine Learning for Creativity and\n  Design NeurIPS 2020 Workshop; Corrected typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  GANs can generate photo-realistic images from the domain of their training\ndata. However, those wanting to use them for creative purposes often want to\ngenerate imagery from a truly novel domain, a task which GANs are inherently\nunable to do. It is also desirable to have a level of control so that there is\na degree of artistic direction rather than purely curation of random results.\nHere we present a method for interpolating between generative models of the\nStyleGAN architecture in a resolution dependent manner. This allows us to\ngenerate images from an entirely novel domain and do this with a degree of\ncontrol over the nature of the output.\n", "versions": [{"version": "v1", "created": "Sun, 11 Oct 2020 20:10:36 GMT"}, {"version": "v2", "created": "Tue, 20 Oct 2020 12:21:00 GMT"}, {"version": "v3", "created": "Sat, 21 Nov 2020 14:31:00 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Pinkney", "Justin N. M.", ""], ["Adler", "Doron", ""]]}, {"id": "2010.05340", "submitter": "Taras Lehinevych", "authors": "Ihor Protsenko, Taras Lehinevych, Dmytro Voitekh, Ihor Kroosh, Nick\n  Hasty, Anthony Johnson", "title": "Self-attention aggregation network for video face representation and\n  recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Models based on self-attention mechanisms have been successful in analyzing\ntemporal data and have been widely used in the natural language domain. We\npropose a new model architecture for video face representation and recognition\nbased on a self-attention mechanism. Our approach could be used for video with\nsingle and multiple identities. To the best of our knowledge, no one has\nexplored the aggregation approaches that consider the video with multiple\nidentities. The proposed approach utilizes existing models to get the face\nrepresentation for each video frame, e.g., ArcFace and MobileFaceNet, and the\naggregation module produces the aggregated face representation vector for video\nby taking into consideration the order of frames and their quality scores. We\ndemonstrate empirical results on a public dataset for video face recognition\ncalled IJB-C to indicate that the self-attention aggregation network (SAAN)\noutperforms naive average pooling. Moreover, we introduce a new multi-identity\nvideo dataset based on the publicly available UMDFaces dataset and collected\nGIFs from Giphy. We show that SAAN is capable of producing a compact face\nrepresentation for both single and multiple identities in a video. The dataset\nand source code will be publicly available.\n", "versions": [{"version": "v1", "created": "Sun, 11 Oct 2020 20:57:46 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Protsenko", "Ihor", ""], ["Lehinevych", "Taras", ""], ["Voitekh", "Dmytro", ""], ["Kroosh", "Ihor", ""], ["Hasty", "Nick", ""], ["Johnson", "Anthony", ""]]}, {"id": "2010.05350", "submitter": "Bo Liu", "authors": "Qishen Ha, Bo Liu, Fuxu Liu, Peiyuan Liao", "title": "Google Landmark Recognition 2020 Competition Third Place Solution", "comments": "5 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present our third place solution to the Google Landmark Recognition 2020\ncompetition. It is an ensemble of global features only Sub-center ArcFace\nmodels. We introduce dynamic margins for ArcFace loss, a family of tune-able\nmargin functions of class size, designed to deal with the extreme imbalance in\nGLDv2 dataset. Progressive finetuning and careful postprocessing are also key\nto the solution. Our two submissions scored 0.6344 and 0.6289 on private\nleaderboard, both ranking third place out of 736 teams.\n", "versions": [{"version": "v1", "created": "Sun, 11 Oct 2020 21:30:43 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Ha", "Qishen", ""], ["Liu", "Bo", ""], ["Liu", "Fuxu", ""], ["Liao", "Peiyuan", ""]]}, {"id": "2010.05351", "submitter": "Bo Liu", "authors": "Qishen Ha, Bo Liu, Fuxu Liu", "title": "Identifying Melanoma Images using EfficientNet Ensemble: Winning\n  Solution to the SIIM-ISIC Melanoma Classification Challenge", "comments": "6 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present our winning solution to the SIIM-ISIC Melanoma Classification\nChallenge. It is an ensemble of convolutions neural network (CNN) models with\ndifferent backbones and input sizes, most of which are image-only models while\na few of them used image-level and patient-level metadata. The keys to our\nwinning are: (1) stable validation scheme (2) good choice of model target (3)\ncarefully tuned pipeline and (4) ensembling with very diverse models. The\nwinning submission scored 0.9600 AUC on cross validation and 0.9490 AUC on\nprivate leaderboard.\n", "versions": [{"version": "v1", "created": "Sun, 11 Oct 2020 21:38:04 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Ha", "Qishen", ""], ["Liu", "Bo", ""], ["Liu", "Fuxu", ""]]}, {"id": "2010.05352", "submitter": "Pranav Rajpurkar", "authors": "Hari Sowrirajan, Jingbo Yang, Andrew Y. Ng, Pranav Rajpurkar", "title": "MoCo-CXR: MoCo Pretraining Improves Representation and Transferability\n  of Chest X-ray Models", "comments": "Accepted at Medical Imaging with Deep Learning (MIDL) Conference 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Contrastive learning is a form of self-supervision that can leverage\nunlabeled data to produce pretrained models. While contrastive learning has\ndemonstrated promising results on natural image classification tasks, its\napplication to medical imaging tasks like chest X-ray interpretation has been\nlimited. In this work, we propose MoCo-CXR, which is an adaptation of the\ncontrastive learning method Momentum Contrast (MoCo), to produce models with\nbetter representations and initializations for the detection of pathologies in\nchest X-rays. In detecting pleural effusion, we find that linear models trained\non MoCo-CXR-pretrained representations outperform those without\nMoCo-CXR-pretrained representations, indicating that MoCo-CXR-pretrained\nrepresentations are of higher-quality. End-to-end fine-tuning experiments\nreveal that a model initialized via MoCo-CXR-pretraining outperforms its\nnon-MoCo-CXR-pretrained counterpart. We find that MoCo-CXR-pretraining provides\nthe most benefit with limited labeled training data. Finally, we demonstrate\nsimilar results on a target Tuberculosis dataset unseen during pretraining,\nindicating that MoCo-CXR-pretraining endows models with representations and\ntransferability that can be applied across chest X-ray datasets and tasks.\n", "versions": [{"version": "v1", "created": "Sun, 11 Oct 2020 21:42:10 GMT"}, {"version": "v2", "created": "Sun, 21 Feb 2021 02:54:06 GMT"}, {"version": "v3", "created": "Mon, 17 May 2021 20:39:42 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Sowrirajan", "Hari", ""], ["Yang", "Jingbo", ""], ["Ng", "Andrew Y.", ""], ["Rajpurkar", "Pranav", ""]]}, {"id": "2010.05355", "submitter": "Vishnu Bashyam", "authors": "Vishnu M. Bashyam, Jimit Doshi, Guray Erus, Dhivya Srinivasan, Ahmed\n  Abdulkadir, Mohamad Habes, Yong Fan, Colin L. Masters, Paul Maruff, Chuanjun\n  Zhuo, Henry V\\\"olzke, Sterling C. Johnson, Jurgen Fripp, Nikolaos\n  Koutsouleris, Theodore D. Satterthwaite, Daniel H. Wolf, Raquel E. Gur, Ruben\n  C. Gur, John C. Morris, Marilyn S. Albert, Hans J. Grabe, Susan M. Resnick,\n  R. Nick Bryan, David A. Wolk, Haochang Shou, Ilya M. Nasrallah, and Christos\n  Davatzikos", "title": "Medical Image Harmonization Using Deep Learning Based Canonical Mapping:\n  Toward Robust and Generalizable Learning in Imaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional and deep learning-based methods have shown great potential in\nthe medical imaging domain, as means for deriving diagnostic, prognostic, and\npredictive biomarkers, and by contributing to precision medicine. However,\nthese methods have yet to see widespread clinical adoption, in part due to\nlimited generalization performance across various imaging devices, acquisition\nprotocols, and patient populations. In this work, we propose a new paradigm in\nwhich data from a diverse range of acquisition conditions are \"harmonized\" to a\ncommon reference domain, where accurate model learning and prediction can take\nplace. By learning an unsupervised image to image canonical mapping from\ndiverse datasets to a reference domain using generative deep learning models,\nwe aim to reduce confounding data variation while preserving semantic\ninformation, thereby rendering the learning task easier in the reference\ndomain. We test this approach on two example problems, namely MRI-based brain\nage prediction and classification of schizophrenia, leveraging pooled cohorts\nof neuroimaging MRI data spanning 9 sites and 9701 subjects. Our results\nindicate a substantial improvement in these tasks in out-of-sample data, even\nwhen training is restricted to a single site.\n", "versions": [{"version": "v1", "created": "Sun, 11 Oct 2020 22:01:37 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Bashyam", "Vishnu M.", ""], ["Doshi", "Jimit", ""], ["Erus", "Guray", ""], ["Srinivasan", "Dhivya", ""], ["Abdulkadir", "Ahmed", ""], ["Habes", "Mohamad", ""], ["Fan", "Yong", ""], ["Masters", "Colin L.", ""], ["Maruff", "Paul", ""], ["Zhuo", "Chuanjun", ""], ["V\u00f6lzke", "Henry", ""], ["Johnson", "Sterling C.", ""], ["Fripp", "Jurgen", ""], ["Koutsouleris", "Nikolaos", ""], ["Satterthwaite", "Theodore D.", ""], ["Wolf", "Daniel H.", ""], ["Gur", "Raquel E.", ""], ["Gur", "Ruben C.", ""], ["Morris", "John C.", ""], ["Albert", "Marilyn S.", ""], ["Grabe", "Hans J.", ""], ["Resnick", "Susan M.", ""], ["Bryan", "R. Nick", ""], ["Wolk", "David A.", ""], ["Shou", "Haochang", ""], ["Nasrallah", "Ilya M.", ""], ["Davatzikos", "Christos", ""]]}, {"id": "2010.05360", "submitter": "Donsub Rim", "authors": "Weilin Li, Kui Ren, Donsub Rim", "title": "A range characterization of the single-quadrant ADRT", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.CV cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work characterizes the range of the single-quadrant approximate discrete\nRadon transform (ADRT) of square images. The characterization is given in the\nform of linear constraints that ensure the exact and fast inversion formula\n[Rim, Appl. Math. Lett. 102 106159, 2020] yields a square image in a stable\nmanner. The range characterization is obtained by first showing that the\ntransform is a bijection between images supported on infinite half-strips, then\nidentifying the linear subspaces that stay finitely supported under the\ninversion formula.\n", "versions": [{"version": "v1", "created": "Sun, 11 Oct 2020 22:14:22 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Li", "Weilin", ""], ["Ren", "Kui", ""], ["Rim", "Donsub", ""]]}, {"id": "2010.05379", "submitter": "Qinxin Wang", "authors": "Qinxin Wang, Hao Tan, Sheng Shen, Michael W. Mahoney, Zhewei Yao", "title": "MAF: Multimodal Alignment Framework for Weakly-Supervised Phrase\n  Grounding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Phrase localization is a task that studies the mapping from textual phrases\nto regions of an image. Given difficulties in annotating phrase-to-object\ndatasets at scale, we develop a Multimodal Alignment Framework (MAF) to\nleverage more widely-available caption-image datasets, which can then be used\nas a form of weak supervision. We first present algorithms to model\nphrase-object relevance by leveraging fine-grained visual representations and\nvisually-aware language representations. By adopting a contrastive objective,\nour method uses information in caption-image pairs to boost the performance in\nweakly-supervised scenarios. Experiments conducted on the widely-adopted\nFlickr30k dataset show a significant improvement over existing\nweakly-supervised methods. With the help of the visually-aware language\nrepresentations, we can also improve the previous best unsupervised result by\n5.56%. We conduct ablation studies to show that both our novel model and our\nweakly-supervised strategies significantly contribute to our strong results.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 00:43:52 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Wang", "Qinxin", ""], ["Tan", "Hao", ""], ["Shen", "Sheng", ""], ["Mahoney", "Michael W.", ""], ["Yao", "Zhewei", ""]]}, {"id": "2010.05382", "submitter": "Kyrollos Yanny", "authors": "Kyrollos Yanny, Nick Antipa, William Liberti, Sam Dehaeck, Kristina\n  Monakhova, Fanglin Linda Liu, Konlin Shen, Ren Ng and Laura Waller", "title": "Miniscope3D: optimized single-shot miniature 3D fluorescence microscopy", "comments": "Published with Nature Springer in Light: Science and Applications", "journal-ref": "Light: Science & Applications 9.1 (2020): 1-13", "doi": "10.1038/s41377-020-00403-7", "report-no": null, "categories": "eess.IV cs.CV physics.optics", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Miniature fluorescence microscopes are a standard tool in systems biology.\nHowever, widefield miniature microscopes capture only 2D information, and\nmodifications that enable 3D capabilities increase the size and weight and have\npoor resolution outside a narrow depth range. Here, we achieve the 3D\ncapability by replacing the tube lens of a conventional 2D Miniscope with an\noptimized multifocal phase mask at the objective's aperture stop. Placing the\nphase mask at the aperture stop significantly reduces the size of the device,\nand varying the focal lengths enables a uniform resolution across a wide depth\nrange. The phase mask encodes the 3D fluorescence intensity into a single 2D\nmeasurement, and the 3D volume is recovered by solving a sparsity-constrained\ninverse problem. We provide methods for designing and fabricating the phase\nmask and an efficient forward model that accounts for the field-varying\naberrations in miniature objectives. We demonstrate a prototype that is 17 mm\ntall and weighs 2.5 grams, achieving 2.76 $\\mu$m lateral, and 15 $\\mu$m axial\nresolution across most of the 900x700x390 $\\mu m^3$ volume at 40 volumes per\nsecond. The performance is validated experimentally on resolution targets,\ndynamic biological samples, and mouse brain tissue. Compared with existing\nminiature single-shot volume-capture implementations, our system is smaller and\nlighter and achieves a more than 2x better lateral and axial resolution\nthroughout a 10x larger usable depth range. Our microscope design provides\nsingle-shot 3D imaging for applications where a compact platform matters, such\nas volumetric neural imaging in freely moving animals and 3D motion studies of\ndynamic samples in incubators and lab-on-a-chip devices.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 01:19:31 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Yanny", "Kyrollos", ""], ["Antipa", "Nick", ""], ["Liberti", "William", ""], ["Dehaeck", "Sam", ""], ["Monakhova", "Kristina", ""], ["Liu", "Fanglin Linda", ""], ["Shen", "Konlin", ""], ["Ng", "Ren", ""], ["Waller", "Laura", ""]]}, {"id": "2010.05391", "submitter": "William Beksi", "authors": "Mohammad Samiul Arshad and William J. Beksi", "title": "A Progressive Conditional Generative Adversarial Network for Generating\n  Dense and Colored 3D Point Clouds", "comments": "To be published in the 2020 International Conference on 3D Vision\n  (3DV)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a novel conditional generative adversarial\nnetwork that creates dense 3D point clouds, with color, for assorted classes of\nobjects in an unsupervised manner. To overcome the difficulty of capturing\nintricate details at high resolutions, we propose a point transformer that\nprogressively grows the network through the use of graph convolutions. The\nnetwork is composed of a leaf output layer and an initial set of branches.\nEvery training iteration evolves a point vector into a point cloud of\nincreasing resolution. After a fixed number of iterations, the number of\nbranches is increased by replicating the last branch. Experimental results show\nthat our network is capable of learning and mimicking a 3D data distribution,\nand produces colored point clouds with fine details at multiple resolutions.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 01:32:13 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Arshad", "Mohammad Samiul", ""], ["Beksi", "William J.", ""]]}, {"id": "2010.05395", "submitter": "Zhiyang Lu", "authors": "Zhiyang Lu, Jun Li, Zheng Li, Hongjian He, Jun Shi", "title": "Reconstruction of Quantitative Susceptibility Maps from Phase of\n  Susceptibility Weighted Imaging with Cross-Connected $\\Psi$-Net", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantitative Susceptibility Mapping (QSM) is a new phase-based technique for\nquantifying magnetic susceptibility. The existing QSM reconstruction methods\ngenerally require complicated pre-processing on high-quality phase data. In\nthis work, we propose to explore a new value of the high-pass filtered phase\ndata generated in susceptibility weighted imaging (SWI), and develop an\nend-to-end Cross-connected $\\Psi$-Net (C$\\Psi$-Net) to reconstruct QSM directly\nfrom these phase data in SWI without additional pre-processing. C$\\Psi$-Net\nadds an intermediate branch in the classical U-Net to form a $\\Psi$-like\nstructure. The specially designed dilated interaction block is embedded in each\nlevel of this branch to enlarge the receptive fields for capturing more\nsusceptibility information from a wider spatial range of phase images.\nMoreover, the crossed connections are utilized between branches to implement a\nmulti-resolution feature fusion scheme, which helps C$\\Psi$-Net capture rich\ncontextual information for accurate reconstruction. The experimental results on\na human dataset show that C$\\Psi$-Net achieves superior performance in our task\nover other QSM reconstruction algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 01:53:21 GMT"}, {"version": "v2", "created": "Wed, 14 Oct 2020 11:50:13 GMT"}, {"version": "v3", "created": "Tue, 12 Jan 2021 13:37:59 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Lu", "Zhiyang", ""], ["Li", "Jun", ""], ["Li", "Zheng", ""], ["He", "Hongjian", ""], ["Shi", "Jun", ""]]}, {"id": "2010.05397", "submitter": "Yun Yue", "authors": "Yun Yue, Ming Li, Venkatesh Saligrama, Ziming Zhang", "title": "RNN Training along Locally Optimal Trajectories via Frank-Wolfe\n  Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel and efficient training method for RNNs by iteratively\nseeking a local minima on the loss surface within a small region, and leverage\nthis directional vector for the update, in an outer-loop. We propose to utilize\nthe Frank-Wolfe (FW) algorithm in this context. Although, FW implicitly\ninvolves normalized gradients, which can lead to a slow convergence rate, we\ndevelop a novel RNN training method that, surprisingly, even with the\nadditional cost, the overall training cost is empirically observed to be lower\nthan back-propagation. Our method leads to a new Frank-Wolfe method, that is in\nessence an SGD algorithm with a restart scheme. We prove that under certain\nconditions our algorithm has a sublinear convergence rate of $O(1/\\epsilon)$\nfor $\\epsilon$ error. We then conduct empirical experiments on several\nbenchmark datasets including those that exhibit long-term dependencies, and\nshow significant performance improvement. We also experiment with deep RNN\narchitectures and show efficient training performance. Finally, we demonstrate\nthat our training method is robust to noisy data.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 01:59:18 GMT"}, {"version": "v2", "created": "Tue, 13 Oct 2020 01:15:45 GMT"}, {"version": "v3", "created": "Thu, 15 Oct 2020 16:02:28 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Yue", "Yun", ""], ["Li", "Ming", ""], ["Saligrama", "Venkatesh", ""], ["Zhang", "Ziming", ""]]}, {"id": "2010.05406", "submitter": "Mingzhe Li", "authors": "Mingzhe Li, Xiuying Chen, Shen Gao, Zhangming Chan, Dongyan Zhao and\n  Rui Yan", "title": "VMSMO: Learning to Generate Multimodal Summary for Video-based News\n  Articles", "comments": "Accepted by The 2020 Conference on Empirical Methods in Natural\n  Language Processing (EMNLP 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A popular multimedia news format nowadays is providing users with a lively\nvideo and a corresponding news article, which is employed by influential news\nmedia including CNN, BBC, and social media including Twitter and Weibo. In such\na case, automatically choosing a proper cover frame of the video and generating\nan appropriate textual summary of the article can help editors save time, and\nreaders make the decision more effectively. Hence, in this paper, we propose\nthe task of Video-based Multimodal Summarization with Multimodal Output (VMSMO)\nto tackle such a problem. The main challenge in this task is to jointly model\nthe temporal dependency of video with semantic meaning of article. To this end,\nwe propose a Dual-Interaction-based Multimodal Summarizer (DIMS), consisting of\na dual interaction module and multimodal generator. In the dual interaction\nmodule, we propose a conditional self-attention mechanism that captures local\nsemantic information within video and a global-attention mechanism that handles\nthe semantic relationship between news text and video from a high level.\nExtensive experiments conducted on a large-scale real-world VMSMO dataset show\nthat DIMS achieves the state-of-the-art performance in terms of both automatic\nmetrics and human evaluations.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 02:19:16 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Li", "Mingzhe", ""], ["Chen", "Xiuying", ""], ["Gao", "Shen", ""], ["Chan", "Zhangming", ""], ["Zhao", "Dongyan", ""], ["Yan", "Rui", ""]]}, {"id": "2010.05435", "submitter": "Rodolfo Quispe", "authors": "Rodolfo Quispe and Helio Pedrini", "title": "Top-DB-Net: Top DropBlock for Activation Enhancement in Person\n  Re-Identification", "comments": "Accepted on 25th International Conference on Pattern Recognition\n  (ICPR2020)", "journal-ref": "ICPR 2020", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person Re-Identification is a challenging task that aims to retrieve all\ninstances of a query image across a system of non-overlapping cameras. Due to\nthe various extreme changes of view, it is common that local regions that could\nbe used to match people are suppressed, which leads to a scenario where\napproaches have to evaluate the similarity of images based on less informative\nregions. In this work, we introduce the Top-DB-Net, a method based on Top\nDropBlock that pushes the network to learn to focus on the scene foreground,\nwith special emphasis on the most task-relevant regions and, at the same time,\nencodes low informative regions to provide high discriminability. The\nTop-DB-Net is composed of three streams: (i) a global stream encodes rich image\ninformation from a backbone, (ii) the Top DropBlock stream encourages the\nbackbone to encode low informative regions with high discriminative features,\nand (iii) a regularization stream helps to deal with the noise created by the\ndropping process of the second stream, when testing the first two streams are\nused. Vast experiments on three challenging datasets show the capabilities of\nour approach against state-of-the-art methods. Qualitative results demonstrate\nthat our method exhibits better activation maps focusing on reliable parts of\nthe input images.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 03:49:58 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Quispe", "Rodolfo", ""], ["Pedrini", "Helio", ""]]}, {"id": "2010.05466", "submitter": "Di Hu", "authors": "Di Hu, Rui Qian, Minyue Jiang, Xiao Tan, Shilei Wen, Errui Ding,\n  Weiyao Lin and Dejing Dou", "title": "Discriminative Sounding Objects Localization via Self-supervised\n  Audiovisual Matching", "comments": "To appear in NeurIPS 2020. Previous Title: Learning to\n  Discriminatively Localize Sounding Objects in a Cocktail-party Scenario", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discriminatively localizing sounding objects in cocktail-party, i.e., mixed\nsound scenes, is commonplace for humans, but still challenging for machines. In\nthis paper, we propose a two-stage learning framework to perform\nself-supervised class-aware sounding object localization. First, we propose to\nlearn robust object representations by aggregating the candidate sound\nlocalization results in the single source scenes. Then, class-aware object\nlocalization maps are generated in the cocktail-party scenarios by referring\nthe pre-learned object knowledge, and the sounding objects are accordingly\nselected by matching audio and visual object category distributions, where the\naudiovisual consistency is viewed as the self-supervised signal. Experimental\nresults in both realistic and synthesized cocktail-party videos demonstrate\nthat our model is superior in filtering out silent objects and pointing out the\nlocation of sounding objects of different classes. Code is available at\nhttps://github.com/DTaoo/Discriminative-Sounding-Objects-Localization.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 05:51:55 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Hu", "Di", ""], ["Qian", "Rui", ""], ["Jiang", "Minyue", ""], ["Tan", "Xiao", ""], ["Wen", "Shilei", ""], ["Ding", "Errui", ""], ["Lin", "Weiyao", ""], ["Dou", "Dejing", ""]]}, {"id": "2010.05468", "submitter": "Dongxu Li", "authors": "Dongxu Li, Chenchen Xu, Xin Yu, Kaihao Zhang, Ben Swift, Hanna\n  Suominen, Hongdong Li", "title": "TSPNet: Hierarchical Feature Learning via Temporal Semantic Pyramid for\n  Sign Language Translation", "comments": "NeurIPS 2020 preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sign language translation (SLT) aims to interpret sign video sequences into\ntext-based natural language sentences. Sign videos consist of continuous\nsequences of sign gestures with no clear boundaries in between. Existing SLT\nmodels usually represent sign visual features in a frame-wise manner so as to\navoid needing to explicitly segmenting the videos into isolated signs. However,\nthese methods neglect the temporal information of signs and lead to substantial\nambiguity in translation. In this paper, we explore the temporal semantic\nstructures of signvideos to learn more discriminative features. To this end, we\nfirst present a novel sign video segment representation which takes into\naccount multiple temporal granularities, thus alleviating the need for accurate\nvideo segmentation. Taking advantage of the proposed segment representation, we\ndevelop a novel hierarchical sign video feature learning method via a temporal\nsemantic pyramid network, called TSPNet. Specifically, TSPNet introduces an\ninter-scale attention to evaluate and enhance local semantic consistency of\nsign segments and an intra-scale attention to resolve semantic ambiguity by\nusing non-local video context. Experiments show that our TSPNet outperforms the\nstate-of-the-art with significant improvements on the BLEU score (from 9.58 to\n13.41) and ROUGE score (from 31.80 to 34.96)on the largest commonly-used SLT\ndataset. Our implementation is available at\nhttps://github.com/verashira/TSPNet.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 05:58:09 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Li", "Dongxu", ""], ["Xu", "Chenchen", ""], ["Yu", "Xin", ""], ["Zhang", "Kaihao", ""], ["Swift", "Ben", ""], ["Suominen", "Hanna", ""], ["Li", "Hongdong", ""]]}, {"id": "2010.05469", "submitter": "Ze-Yu Song", "authors": "Zeyu Song, Dongliang Chang, Zhanyu Ma, Xiaoxu Li, Zheng-Hua Tan", "title": "CC-Loss: Channel Correlation Loss For Image Classification", "comments": "accepted by ICPR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The loss function is a key component in deep learning models. A commonly used\nloss function for classification is the cross entropy loss, which is a simple\nyet effective application of information theory for classification problems.\nBased on this loss, many other loss functions have been proposed,~\\emph{e.g.},\nby adding intra-class and inter-class constraints to enhance the discriminative\nability of the learned features. However, these loss functions fail to consider\nthe connections between the feature distribution and the model structure.\nAiming at addressing this problem, we propose a channel correlation loss\n(CC-Loss) that is able to constrain the specific relations between classes and\nchannels as well as maintain the intra-class and the inter-class separability.\nCC-Loss uses a channel attention module to generate channel attention of\nfeatures for each sample in the training stage. Next, an Euclidean distance\nmatrix is calculated to make the channel attention vectors associated with the\nsame class become identical and to increase the difference between different\nclasses. Finally, we obtain a feature embedding with good intra-class\ncompactness and inter-class separability.Experimental results show that two\ndifferent backbone models trained with the proposed CC-Loss outperform the\nstate-of-the-art loss functions on three image classification datasets.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 05:59:06 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Song", "Zeyu", ""], ["Chang", "Dongliang", ""], ["Ma", "Zhanyu", ""], ["Li", "Xiaoxu", ""], ["Tan", "Zheng-Hua", ""]]}, {"id": "2010.05495", "submitter": "Christoph Kamann", "authors": "Christoph Kamann, Burkhard G\\\"ussefeld, Robin Hutmacher, Jan Hendrik\n  Metzen, Carsten Rother", "title": "Increasing the Robustness of Semantic Segmentation Models with\n  Painting-by-Numbers", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-58607-2_22", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For safety-critical applications such as autonomous driving, CNNs have to be\nrobust with respect to unavoidable image corruptions, such as image noise.\nWhile previous works addressed the task of robust prediction in the context of\nfull-image classification, we consider it for dense semantic segmentation. We\nbuild upon an insight from image classification that output robustness can be\nimproved by increasing the network-bias towards object shapes. We present a new\ntraining schema that increases this shape bias. Our basic idea is to\nalpha-blend a portion of the RGB training images with faked images, where each\nclass-label is given a fixed, randomly chosen color that is not likely to\nappear in real imagery. This forces the network to rely more strongly on shape\ncues. We call this data augmentation technique ``Painting-by-Numbers''. We\ndemonstrate the effectiveness of our training schema for DeepLabv3+ with\nvarious network backbones, MobileNet-V2, ResNets, and Xception, and evaluate it\non the Cityscapes dataset. With respect to our 16 different types of image\ncorruptions and 5 different network backbones, we are in 74% better than\ntraining with clean data. For cases where we are worse than a model trained\nwithout our training schema, it is mostly only marginally worse. However, for\nsome image corruptions such as images with noise, we see a considerable\nperformance gain of up to 25%.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 07:42:39 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Kamann", "Christoph", ""], ["G\u00fcssefeld", "Burkhard", ""], ["Hutmacher", "Robin", ""], ["Metzen", "Jan Hendrik", ""], ["Rother", "Carsten", ""]]}, {"id": "2010.05501", "submitter": "Haotong Qin", "authors": "Haotong Qin, Zhongang Cai, Mingyuan Zhang, Yifu Ding, Haiyu Zhao,\n  Shuai Yi, Xianglong Liu, Hao Su", "title": "BiPointNet: Binary Neural Network for Point Clouds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To alleviate the resource constraint for real-time point cloud applications\nthat run on edge devices, in this paper we present BiPointNet, the first model\nbinarization approach for efficient deep learning on point clouds. We discover\nthat the immense performance drop of binarized models for point clouds mainly\nstems from two challenges: aggregation-induced feature homogenization that\nleads to a degradation of information entropy, and scale distortion that\nhinders optimization and invalidates scale-sensitive structures. With\ntheoretical justifications and in-depth analysis, our BiPointNet introduces\nEntropy-Maximizing Aggregation (EMA) to modulate the distribution before\naggregation for the maximum information entropy, and Layer-wise Scale Recovery\n(LSR) to efficiently restore feature representation capacity. Extensive\nexperiments show that BiPointNet outperforms existing binarization methods by\nconvincing margins, at the level even comparable with the full precision\ncounterpart. We highlight that our techniques are generic, guaranteeing\nsignificant improvements on various fundamental tasks and mainstream backbones.\nMoreover, BiPointNet gives an impressive 14.7x speedup and 18.9x storage saving\non real-world resource-constrained devices.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 07:54:51 GMT"}, {"version": "v2", "created": "Mon, 25 Jan 2021 18:16:08 GMT"}, {"version": "v3", "created": "Tue, 2 Mar 2021 02:02:57 GMT"}, {"version": "v4", "created": "Fri, 11 Jun 2021 15:03:56 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Qin", "Haotong", ""], ["Cai", "Zhongang", ""], ["Zhang", "Mingyuan", ""], ["Ding", "Yifu", ""], ["Zhao", "Haiyu", ""], ["Yi", "Shuai", ""], ["Liu", "Xianglong", ""], ["Su", "Hao", ""]]}, {"id": "2010.05507", "submitter": "Hao Xue", "authors": "Hao Xue, Du Q.Huynh, Mark Reynolds", "title": "Scene Gated Social Graph: Pedestrian Trajectory Prediction Based on\n  Dynamic Social Graphs and Scene Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pedestrian trajectory prediction is valuable for understanding human motion\nbehaviors and it is challenging because of the social influence from other\npedestrians, the scene constraints and the multimodal possibilities of\npredicted trajectories. Most existing methods only focus on two of the above\nthree key elements. In order to jointly consider all these elements, we propose\na novel trajectory prediction method named Scene Gated Social Graph (SGSG). In\nthe proposed SGSG, dynamic graphs are used to describe the social relationship\namong pedestrians. The social and scene influences are taken into account\nthrough the scene gated social graph features which combine the encoded social\ngraph features and semantic scene features. In addition, a VAE module is\nincorporated to learn the scene gated social feature and sample latent\nvariables for generating multiple trajectories that are socially and\nenvironmentally acceptable. We compare our SGSG against twenty state-of-the-art\npedestrian trajectory prediction methods and the results show that the proposed\nmethod achieves superior performance on two widely used trajectory prediction\nbenchmarks.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 08:04:05 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Xue", "Hao", ""], ["Huynh", "Du Q.", ""], ["Reynolds", "Mark", ""]]}, {"id": "2010.05508", "submitter": "Lingbo Yang", "authors": "Lingbo Yang, Pan Wang, Zhanning Gao, Shanshe Wang, Peiran Ren, Siwei\n  Ma, Wen Gao", "title": "Implicit Subspace Prior Learning for Dual-Blind Face Restoration", "comments": "TPAMI submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Face restoration is an inherently ill-posed problem, where additional prior\nconstraints are typically considered crucial for mitigating such pathology.\nHowever, real-world image prior are often hard to simulate with precise\nmathematical models, which inevitably limits the performance and generalization\nability of existing prior-regularized restoration methods. In this paper, we\nstudy the problem of face restoration under a more practical ``dual blind''\nsetting, i.e., without prior assumptions or hand-crafted regularization terms\non the degradation profile or image contents.\n  To this end, a novel implicit subspace prior learning (ISPL) framework is\nproposed as a generic solution to dual-blind face restoration, with two key\nelements: 1) an implicit formulation to circumvent the ill-defined restoration\nmapping and 2) a subspace prior decomposition and fusion mechanism to\ndynamically handle inputs at varying degradation levels with consistent\nhigh-quality restoration results.\n  Experimental results demonstrate significant perception-distortion\nimprovement of ISPL against existing state-of-the-art methods for a variety of\nrestoration subtasks, including a 3.69db PSNR and 45.8% FID gain against\nESRGAN, the 2018 NTIRE SR challenge winner. Overall, we prove that it is\npossible to capture and utilize prior knowledge without explicitly formulating\nit, which will help inspire new research paradigms towards low-level vision\ntasks.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 08:04:24 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Yang", "Lingbo", ""], ["Wang", "Pan", ""], ["Gao", "Zhanning", ""], ["Wang", "Shanshe", ""], ["Ren", "Peiran", ""], ["Ma", "Siwei", ""], ["Gao", "Wen", ""]]}, {"id": "2010.05512", "submitter": "Weisi Guo", "authors": "Lili Lu, Weisi Guo", "title": "Automatic Quantification of Settlement Damage using Deep Learning of\n  Satellite Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humanitarian disasters and political violence cause significant damage to our\nliving space. The reparation cost to homes, infrastructure, and the ecosystem\nis often difficult to quantify in real-time. Real-time quantification is\ncritical to both informing relief operations, but also planning ahead for\nrebuilding. Here, we use satellite images before and after major crisis around\nthe world to train a robust baseline Residual Network (ResNet) and a disaster\nquantification Pyramid Scene Parsing Network (PSPNet). ResNet offers robustness\nto poor image quality and can identify areas of destruction with high accuracy\n(92\\%), whereas PSPNet offers contextualised quantification of built\nenvironment damage with good accuracy (84\\%). As there are multiple damage\ndimensions to consider (e.g. economic loss and fatalities), we fit a\nmulti-linear regression model to quantify the overall damage. To validate our\ncombined system of deep learning and regression modeling, we successfully match\nour prediction to the ongoing recovery in the 2020 Beirut port explosion. These\ninnovations provide a better quantification of overall disaster magnitude and\ninform intelligent humanitarian systems of unfolding disasters.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 08:06:33 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Lu", "Lili", ""], ["Guo", "Weisi", ""]]}, {"id": "2010.05517", "submitter": "Junyu Gao", "authors": "Tao Han, Junyu Gao, Yuan Yuan, Qi Wang", "title": "Unsupervised Semantic Aggregation and Deformable Template Matching for\n  Semi-Supervised Learning", "comments": "accepted by NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unlabeled data learning has attracted considerable attention recently.\nHowever, it is still elusive to extract the expected high-level semantic\nfeature with mere unsupervised learning. In the meantime, semi-supervised\nlearning (SSL) demonstrates a promising future in leveraging few samples. In\nthis paper, we combine both to propose an Unsupervised Semantic Aggregation and\nDeformable Template Matching (USADTM) framework for SSL, which strives to\nimprove the classification performance with few labeled data and then reduce\nthe cost in data annotating. Specifically, unsupervised semantic aggregation\nbased on Triplet Mutual Information (T-MI) loss is explored to generate\nsemantic labels for unlabeled data. Then the semantic labels are aligned to the\nactual class by the supervision of labeled data. Furthermore, a feature pool\nthat stores the labeled samples is dynamically updated to assign proxy labels\nfor unlabeled data, which are used as targets for cross-entropy minimization.\nExtensive experiments and analysis across four standard semi-supervised\nlearning benchmarks validate that USADTM achieves top performance (e.g.,\n90.46$\\%$ accuracy on CIFAR-10 with 40 labels and 95.20$\\%$ accuracy with 250\nlabels). The code is released at https://github.com/taohan10200/USADTM.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 08:17:56 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Han", "Tao", ""], ["Gao", "Junyu", ""], ["Yuan", "Yuan", ""], ["Wang", "Qi", ""]]}, {"id": "2010.05537", "submitter": "Nian Liu", "authors": "Nian Liu, Ni Zhang, Ling Shao, Junwei Han", "title": "Learning Selective Mutual Attention and Contrast for RGB-D Saliency\n  Detection", "comments": "Journal extension of our CVPR paper entitled \"Learning Selective\n  Self-Mutual Attention for RGB-D Saliency Detection\" appeared in CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How to effectively fuse cross-modal information is the key problem for RGB-D\nsalient object detection. Early fusion and the result fusion schemes fuse RGB\nand depth information at the input and output stages, respectively, hence incur\nthe problem of distribution gap or information loss. Many models use the\nfeature fusion strategy but are limited by the low-order point-to-point fusion\nmethods. In this paper, we propose a novel mutual attention model by fusing\nattention and contexts from different modalities. We use the non-local\nattention of one modality to propagate long-range contextual dependencies for\nthe other modality, thus leveraging complementary attention cues to perform\nhigh-order and trilinear cross-modal interaction. We also propose to induce\ncontrast inference from the mutual attention and obtain a unified model.\nConsidering low-quality depth data may detriment the model performance, we\nfurther propose selective attention to reweight the added depth cues. We embed\nthe proposed modules in a two-stream CNN for RGB-D SOD. Experimental results\nhave demonstrated the effectiveness of our proposed model. Moreover, we also\nconstruct a new challenging large-scale RGB-D SOD dataset with high-quality,\nthus can both promote the training and evaluation of deep models.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 08:50:10 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Liu", "Nian", ""], ["Zhang", "Ni", ""], ["Shao", "Ling", ""], ["Han", "Junwei", ""]]}, {"id": "2010.05562", "submitter": "Linchao Bao", "authors": "Linchao Bao, Xiangkai Lin, Yajing Chen, Haoxian Zhang, Sheng Wang,\n  Xuefei Zhe, Di Kang, Haozhi Huang, Xinwei Jiang, Jue Wang, Dong Yu, Zhengyou\n  Zhang", "title": "High-Fidelity 3D Digital Human Head Creation from RGB-D Selfies", "comments": "Code: https://github.com/tencent-ailab/hifi3dface", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a fully automatic system that can produce high-fidelity,\nphoto-realistic 3D digital human heads with a consumer RGB-D selfie camera. The\nsystem only needs the user to take a short selfie RGB-D video while rotating\nhis/her head, and can produce a high quality head reconstruction in less than\n30 seconds. Our main contribution is a new facial geometry modeling and\nreflectance synthesis procedure that significantly improves the\nstate-of-the-art. Specifically, given the input video a two-stage frame\nselection procedure is first employed to select a few high-quality frames for\nreconstruction. Then a differentiable renderer based 3D Morphable Model (3DMM)\nfitting algorithm is applied to recover facial geometries from multiview RGB-D\ndata, which takes advantages of a powerful 3DMM basis constructed with\nextensive data generation and perturbation. Our 3DMM has much larger expressive\ncapacities than conventional 3DMM, allowing us to recover more accurate facial\ngeometry using merely linear basis. For reflectance synthesis, we present a\nhybrid approach that combines parametric fitting and CNNs to synthesize\nhigh-resolution albedo/normal maps with realistic hair/pore/wrinkle details.\nResults show that our system can produce faithful 3D digital human faces with\nextremely realistic details. The main code and the newly constructed 3DMM basis\nis publicly available.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 09:31:53 GMT"}, {"version": "v2", "created": "Tue, 29 Jun 2021 09:51:51 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Bao", "Linchao", ""], ["Lin", "Xiangkai", ""], ["Chen", "Yajing", ""], ["Zhang", "Haoxian", ""], ["Wang", "Sheng", ""], ["Zhe", "Xuefei", ""], ["Kang", "Di", ""], ["Huang", "Haozhi", ""], ["Jiang", "Xinwei", ""], ["Wang", "Jue", ""], ["Yu", "Dong", ""], ["Zhang", "Zhengyou", ""]]}, {"id": "2010.05599", "submitter": "Lilang Lin", "authors": "Lilang Lin, Sijie Song, Wenhan Yan and Jiaying Liu", "title": "MS$^2$L: Multi-Task Self-Supervised Learning for Skeleton Based Action\n  Recognition", "comments": "Accepted by ACMMM 2020", "journal-ref": null, "doi": "10.1145/3394171.3413548", "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address self-supervised representation learning from human\nskeletons for action recognition. Previous methods, which usually learn feature\npresentations from a single reconstruction task, may come across the\noverfitting problem, and the features are not generalizable for action\nrecognition. Instead, we propose to integrate multiple tasks to learn more\ngeneral representations in a self-supervised manner. To realize this goal, we\nintegrate motion prediction, jigsaw puzzle recognition, and contrastive\nlearning to learn skeleton features from different aspects. Skeleton dynamics\ncan be modeled through motion prediction by predicting the future sequence. And\ntemporal patterns, which are critical for action recognition, are learned\nthrough solving jigsaw puzzles. We further regularize the feature space by\ncontrastive learning. Besides, we explore different training strategies to\nutilize the knowledge from self-supervised tasks for action recognition. We\nevaluate our multi-task self-supervised learning approach with action\nclassifiers trained under different configurations, including unsupervised,\nsemi-supervised and fully-supervised settings. Our experiments on the NW-UCLA,\nNTU RGB+D, and PKUMMD datasets show remarkable performance for action\nrecognition, demonstrating the superiority of our method in learning more\ndiscriminative and general features. Our project website is available at\nhttps://langlandslin.github.io/projects/MSL/.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 11:09:44 GMT"}, {"version": "v2", "created": "Wed, 14 Oct 2020 07:07:05 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Lin", "Lilang", ""], ["Song", "Sijie", ""], ["Yan", "Wenhan", ""], ["Liu", "Jiaying", ""]]}, {"id": "2010.05600", "submitter": "Takao Yamanaka", "authors": "Keisuke Okubo and Takao Yamanaka", "title": "Omni-Directional Image Generation from Single Snapshot Image", "comments": "SMC2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An omni-directional image (ODI) is the image that has a field of view\ncovering the entire sphere around the camera. The ODIs have begun to be used in\na wide range of fields such as virtual reality (VR), robotics, and social\nnetwork services. Although the contents using ODI have increased, the available\nimages and videos are still limited, compared with widespread snapshot images.\nA large number of ODIs are desired not only for the VR contents, but also for\ntraining deep learning models for ODI. For these purposes, a novel computer\nvision task to generate ODI from a single snapshot image is proposed in this\npaper. To tackle this problem, the conditional generative adversarial network\nwas applied in combination with class-conditioned convolution layers. With this\nnovel task, VR images and videos will be easily created even with a smartphone\ncamera.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 11:12:04 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Okubo", "Keisuke", ""], ["Yamanaka", "Takao", ""]]}, {"id": "2010.05605", "submitter": "Yutao Shen", "authors": "YuTao Shen and Ying Wen", "title": "Convolutional Neural Network optimization via Channel Reassessment\n  Attention module", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance of convolutional neural networks (CNNs) can be improved by\nadjusting the interrelationship between channels with attention mechanism.\nHowever, attention mechanism in recent advance has not fully utilized spatial\ninformation of feature maps, which makes a great difference to the results of\ngenerated channel attentions. In this paper, we propose a novel network\noptimization module called Channel Reassessment Attention (CRA) module which\nuses channel attentions with spatial information of feature maps to enhance\nrepresentational power of networks. We employ CRA module to assess channel\nattentions based on feature maps in different channels, then the final features\nare refined adaptively by product between channel attentions and feature\nmaps.CRA module is a computational lightweight module and it can be embedded\ninto any architectures of CNNs. The experiments on ImageNet, CIFAR and MS COCO\ndatasets demonstrate that the embedding of CRA module on various networks\neffectively improves the performance under different evaluation standards.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 11:27:17 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Shen", "YuTao", ""], ["Wen", "Ying", ""]]}, {"id": "2010.05631", "submitter": "Vishal Kaushal", "authors": "Vishal Kaushal, Suraj Kothawade, Ganesh Ramakrishnan, Jeff Bilmes,\n  Himanshu Asnani, Rishabh Iyer", "title": "A Unified Framework for Generic, Query-Focused, Privacy Preserving and\n  Update Summarization using Submodular Information Measures", "comments": "35 pages, 14 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study submodular information measures as a rich framework for generic,\nquery-focused, privacy sensitive, and update summarization tasks. While past\nwork generally treats these problems differently ({\\em e.g.}, different models\nare often used for generic and query-focused summarization), the submodular\ninformation measures allow us to study each of these problems via a unified\napproach. We first show that several previous query-focused and update\nsummarization techniques have, unknowingly, used various instantiations of the\naforesaid submodular information measures, providing evidence for the benefit\nand naturalness of these models. We then carefully study and demonstrate the\nmodelling capabilities of the proposed functions in different settings and\nempirically verify our findings on both a synthetic dataset and an existing\nreal-world image collection dataset (that has been extended by adding concept\nannotations to each image making it suitable for this task) and will be\npublicly released. We employ a max-margin framework to learn a mixture model\nbuilt using the proposed instantiations of submodular information measures and\ndemonstrate the effectiveness of our approach. While our experiments are in the\ncontext of image summarization, our framework is generic and can be easily\nextended to other summarization settings (e.g., videos or documents).\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 12:03:03 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Kaushal", "Vishal", ""], ["Kothawade", "Suraj", ""], ["Ramakrishnan", "Ganesh", ""], ["Bilmes", "Jeff", ""], ["Asnani", "Himanshu", ""], ["Iyer", "Rishabh", ""]]}, {"id": "2010.05654", "submitter": "Francesco Ragusa", "authors": "Francesco Ragusa and Antonino Furnari and Salvatore Livatino and\n  Giovanni Maria Farinella", "title": "The MECCANO Dataset: Understanding Human-Object Interactions from\n  Egocentric Videos in an Industrial-like Domain", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wearable cameras allow to collect images and videos of humans interacting\nwith the world. While human-object interactions have been thoroughly\ninvestigated in third person vision, the problem has been understudied in\negocentric settings and in industrial scenarios. To fill this gap, we introduce\nMECCANO, the first dataset of egocentric videos to study human-object\ninteractions in industrial-like settings. MECCANO has been acquired by 20\nparticipants who were asked to build a motorbike model, for which they had to\ninteract with tiny objects and tools. The dataset has been explicitly labeled\nfor the task of recognizing human-object interactions from an egocentric\nperspective. Specifically, each interaction has been labeled both temporally\n(with action segments) and spatially (with active object bounding boxes). With\nthe proposed dataset, we investigate four different tasks including 1) action\nrecognition, 2) active object detection, 3) active object recognition and 4)\negocentric human-object interaction detection, which is a revisited version of\nthe standard human-object interaction detection task. Baseline results show\nthat the MECCANO dataset is a challenging benchmark to study egocentric\nhuman-object interactions in industrial-like scenarios. We publicy release the\ndataset at https://iplab.dmi.unict.it/MECCANO.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 12:50:30 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Ragusa", "Francesco", ""], ["Furnari", "Antonino", ""], ["Livatino", "Salvatore", ""], ["Farinella", "Giovanni Maria", ""]]}, {"id": "2010.05657", "submitter": "Yuyuan Yu", "authors": "Yuyuan Yu, Guoxu Zhou, Ning Zheng, Shengli Xie and Qibin Zhao", "title": "Graph Regularized Nonnegative Tensor Ring Decomposition for Multiway\n  Representation Learning", "comments": "13 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tensor ring (TR) decomposition is a powerful tool for exploiting the low-rank\nnature of multiway data and has demonstrated great potential in a variety of\nimportant applications. In this paper, nonnegative tensor ring (NTR)\ndecomposition and graph regularized NTR (GNTR) decomposition are proposed,\nwhere the former equips TR decomposition with local feature extraction by\nimposing nonnegativity on the core tensors and the latter is additionally able\nto capture manifold geometry information of tensor data, both significantly\nextend the applications of TR decomposition for nonnegative multiway\nrepresentation learning. Accelerated proximal gradient based methods are\nderived for NTR and GNTR. The experimental result demonstrate that the proposed\nalgorithms can extract parts-based basis with rich colors and rich lines from\ntensor objects that provide more interpretable and meaningful representation,\nand hence yield better performance than the state-of-the-art tensor based\nmethods in clustering and classification tasks.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 12:54:20 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Yu", "Yuyuan", ""], ["Zhou", "Guoxu", ""], ["Zheng", "Ning", ""], ["Xie", "Shengli", ""], ["Zhao", "Qibin", ""]]}, {"id": "2010.05687", "submitter": "Kunping Yang", "authors": "Kunping Yang, Gui-Song Xia, Zicheng Liu, Bo Du, Wen Yang, Marcello\n  Pelillo, Liangpei Zhang", "title": "Semantic Change Detection with Asymmetric Siamese Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given two multi-temporal aerial images, semantic change detection aims to\nlocate the land-cover variations and identify their change types with\npixel-wise boundaries. This problem is vital in many earth vision related\ntasks, such as precise urban planning and natural resource management. Existing\nstate-of-the-art algorithms mainly identify the changed pixels by applying\nhomogeneous operations on each input image and comparing the extracted\nfeatures. However, in changed regions, totally different land-cover\ndistributions often require heterogeneous features extraction procedures w.r.t\neach input. In this paper, we present an asymmetric siamese network (ASN) to\nlocate and identify semantic changes through feature pairs obtained from\nmodules of widely different structures, which involve areas of various sizes\nand apply different quantities of parameters to factor in the discrepancy\nacross different land-cover distributions. To better train and evaluate our\nmodel, we create a large-scale well-annotated SEmantic Change detectiON Dataset\n(SECOND), while an Adaptive Threshold Learning (ATL) module and a Separated\nKappa (SeK) coefficient are proposed to alleviate the influences of label\nimbalance in model training and evaluation. The experimental results\ndemonstrate that the proposed model can stably outperform the state-of-the-art\nalgorithms with different encoder backbones.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 13:26:30 GMT"}, {"version": "v2", "created": "Sat, 8 May 2021 16:23:46 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Yang", "Kunping", ""], ["Xia", "Gui-Song", ""], ["Liu", "Zicheng", ""], ["Du", "Bo", ""], ["Yang", "Wen", ""], ["Pelillo", "Marcello", ""], ["Zhang", "Liangpei", ""]]}, {"id": "2010.05690", "submitter": "Lalith Bharadwaj B", "authors": "Lalith Bharadwaj B, Rohit Boddeda, Sai Vardhan K and Madhu G", "title": "COVID-19 Classification Using Staked Ensembles: A Comprehensive Analysis", "comments": "The paper is accepted in CMC (Computers, Materials and Continua)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The issue of COVID-19, increasing with a massive mortality rate. This led to\nthe WHO declaring it as a pandemic. In this situation, it is crucial to perform\nefficient and fast diagnosis. The reverse transcript polymerase chain reaction\n(RTPCR) test is conducted to detect the presence of SARS-CoV-2. This test is\ntime-consuming and instead chest CT (or Chest X-ray) can be used for a fast and\naccurate diagnosis. Automated diagnosis is considered to be important as it\nreduces human effort and provides accurate and low-cost tests. The\ncontributions of our research are three-fold. First, it is aimed to analyse the\nbehaviour and performance of variant vision models ranging from Inception to\nNAS networks with the appropriate fine-tuning procedure. Second, the behaviour\nof these models is visually analysed by plotting CAMs for individual networks\nand determining classification performance with AUCROC curves. Thirdly, stacked\nensembles techniques are imparted to provide higher generalisation on combining\nthe fine-tuned models, in which six ensemble neural networks are designed by\ncombining the existing fine-tuned networks. Implying these stacked ensembles\nprovides a great generalization to the models. The ensemble model designed by\ncombining all the fine-tuned networks obtained a state-of-the-art accuracy\nscore of 99.17%. The precision and recall for the COVID-19 class are 99.99% and\n89.79% respectively, which resembles the robustness of the stacked ensembles.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 07:43:57 GMT"}, {"version": "v2", "created": "Fri, 11 Jun 2021 07:56:40 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["B", "Lalith Bharadwaj", ""], ["Boddeda", "Rohit", ""], ["K", "Sai Vardhan", ""], ["G", "Madhu", ""]]}, {"id": "2010.05713", "submitter": "Jialu Huang", "authors": "Jialu Huang, Jing Liao, Sam Kwong", "title": "Unsupervised Image-to-Image Translation via Pre-trained StyleGAN2\n  Network", "comments": "2020 IEEE. Personal use of this material is permitted. Permission\n  from IEEE must be obtained for all other uses, in any current or future\n  media, including reprinting/republishing this material for advertising or\n  promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image-to-Image (I2I) translation is a heated topic in academia, and it also\nhas been applied in real-world industry for tasks like image synthesis,\nsuper-resolution, and colorization. However, traditional I2I translation\nmethods train data in two or more domains together. This requires lots of\ncomputation resources. Moreover, the results are of lower quality, and they\ncontain many more artifacts. The training process could be unstable when the\ndata in different domains are not balanced, and modal collapse is more likely\nto happen. We proposed a new I2I translation method that generates a new model\nin the target domain via a series of model transformations on a pre-trained\nStyleGAN2 model in the source domain. After that, we proposed an inversion\nmethod to achieve the conversion between an image and its latent vector. By\nfeeding the latent vector into the generated model, we can perform I2I\ntranslation between the source domain and target domain. Both qualitative and\nquantitative evaluations were conducted to prove that the proposed method can\nachieve outstanding performance in terms of image quality, diversity and\nsemantic similarity to the input and reference images compared to\nstate-of-the-art works.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 13:51:40 GMT"}, {"version": "v2", "created": "Tue, 27 Oct 2020 01:18:01 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Huang", "Jialu", ""], ["Liao", "Jing", ""], ["Kwong", "Sam", ""]]}, {"id": "2010.05759", "submitter": "Alexander Katzmann", "authors": "Alexander Katzmann, Oliver Taubmann, Stephen Ahmad, Alexander\n  M\\\"uhlberg, Michael S\\\"uhling, Horst-Michael Gro{\\ss}", "title": "Explaining Clinical Decision Support Systems in Medical Imaging using\n  Cycle-Consistent Activation Maximization", "comments": "32 pages, 9 figures, 18 pages appendix, metadata typo corrected", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clinical decision support using deep neural networks has become a topic of\nsteadily growing interest. While recent work has repeatedly demonstrated that\ndeep learning offers major advantages for medical image classification over\ntraditional methods, clinicians are often hesitant to adopt the technology\nbecause its underlying decision-making process is considered to be\nintransparent and difficult to comprehend. In recent years, this has been\naddressed by a variety of approaches that have successfully contributed to\nproviding deeper insight. Most notably, additive feature attribution methods\nare able to propagate decisions back into the input space by creating a\nsaliency map which allows the practitioner to \"see what the network sees.\"\nHowever, the quality of the generated maps can become poor and the images noisy\nif only limited data is available - a typical scenario in clinical contexts. We\npropose a novel decision explanation scheme based on CycleGAN activation\nmaximization which generates high-quality visualizations of classifier\ndecisions even in smaller data sets. We conducted a user study in which these\nvisualizations significantly outperformed existing methods on the LIDC dataset\nfor lung lesion malignancy classification. With our approach we make a\nsignificant contribution to a better understanding of clinical decision support\nsystems based on deep neural networks and thus aim to foster overall clinical\nacceptance.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 14:39:27 GMT"}, {"version": "v2", "created": "Tue, 13 Oct 2020 11:57:53 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Katzmann", "Alexander", ""], ["Taubmann", "Oliver", ""], ["Ahmad", "Stephen", ""], ["M\u00fchlberg", "Alexander", ""], ["S\u00fchling", "Michael", ""], ["Gro\u00df", "Horst-Michael", ""]]}, {"id": "2010.05760", "submitter": "Antonio Busson", "authors": "Antonio J G Busson, Paulo R C Mendes, Daniel de S Moraes, \\'Alvaro M\n  da Veiga, \\'Alan L V Guedes and S\\'ergio Colcher", "title": "Video Quality Enhancement Using Deep Learning-Based Prediction Models\n  for Quantized DCT Coefficients in MPEG I-frames", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent works have successfully applied some types of Convolutional Neural\nNetworks (CNNs) to reduce the noticeable distortion resulting from the lossy\nJPEG/MPEG compression technique. Most of them are built upon the processing\nmade on the spatial domain. In this work, we propose a MPEG video decoder that\nis purely based on the frequency-to-frequency domain: it reads the quantized\nDCT coefficients received from a low-quality I-frames bitstream and, using a\ndeep learning-based model, predicts the missing coefficients in order to\nrecompose the same frames with enhanced quality. In experiments with a video\ndataset, our best model was able to improve from frames with quantized DCT\ncoefficients corresponding to a Quality Factor (QF) of 10 to enhanced quality\nframes with QF slightly near to 20.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 16:41:18 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Busson", "Antonio J G", ""], ["Mendes", "Paulo R C", ""], ["Moraes", "Daniel de S", ""], ["da Veiga", "\u00c1lvaro M", ""], ["Guedes", "\u00c1lan L V", ""], ["Colcher", "S\u00e9rgio", ""]]}, {"id": "2010.05762", "submitter": "Guangming Wang", "authors": "Guangming Wang, Xinrui Wu, Zhe Liu, and Hesheng Wang", "title": "Hierarchical Attention Learning of Scene Flow in 3D Point Clouds", "comments": "13 pages, 7 figures, under review", "journal-ref": null, "doi": "10.1109/TIP.2021.3079796", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scene flow represents the 3D motion of every point in the dynamic\nenvironments. Like the optical flow that represents the motion of pixels in 2D\nimages, 3D motion representation of scene flow benefits many applications, such\nas autonomous driving and service robot. This paper studies the problem of\nscene flow estimation from two consecutive 3D point clouds. In this paper, a\nnovel hierarchical neural network with double attention is proposed for\nlearning the correlation of point features in adjacent frames and refining\nscene flow from coarse to fine layer by layer. The proposed network has a new\nmore-for-less hierarchical architecture. The more-for-less means that the\nnumber of input points is greater than the number of output points for scene\nflow estimation, which brings more input information and balances the precision\nand resource consumption. In this hierarchical architecture, scene flow of\ndifferent levels is generated and supervised respectively. A novel attentive\nembedding module is introduced to aggregate the features of adjacent points\nusing a double attention method in a patch-to-patch manner. The proper layers\nfor flow embedding and flow supervision are carefully considered in our network\ndesignment. Experiments show that the proposed network outperforms the\nstate-of-the-art performance of 3D scene flow estimation on the FlyingThings3D\nand KITTI Scene Flow 2015 datasets. We also apply the proposed network to\nrealistic LiDAR odometry task, which is an key problem in autonomous driving.\nThe experiment results demonstrate that our proposed network can outperform the\nICP-based method and shows the good practical application ability.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 14:56:08 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Wang", "Guangming", ""], ["Wu", "Xinrui", ""], ["Liu", "Zhe", ""], ["Wang", "Hesheng", ""]]}, {"id": "2010.05784", "submitter": "Anqi Liu", "authors": "Haoxuan Wang, Anqi Liu, Zhiding Yu, Yisong Yue, Anima Anandkumar", "title": "Distributionally Robust Learning for Unsupervised Domain Adaptation", "comments": "Preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a distributionally robust learning (DRL) method for unsupervised\ndomain adaptation (UDA) that scales to modern computer vision benchmarks. DRL\ncan be naturally formulated as a competitive two-player game between a\npredictor and an adversary that is allowed to corrupt the labels, subject to\ncertain constraints, and reduces to incorporating a density ratio between the\nsource and target domains (under the standard log loss). This formulation\nmotivates the use of two neural networks that are jointly trained - a\ndiscriminative network between the source and target domains for density-ratio\nestimation, in addition to the standard classification network. The use of a\ndensity ratio in DRL prevents the model from being overconfident on target\ninputs far away from the source domain. Thus, DRL provides conservative\nconfidence estimation in the target domain, even when the target labels are not\navailable. This conservatism motivates the use of DRL in self-training for\nsample selection, and we term the approach distributionally robust\nself-training (DRST). In our experiments, DRST generates more calibrated\nprobabilities and achieves state-of-the-art self-training accuracy on benchmark\ndatasets. We demonstrate that DRST captures shape features more effectively,\nand reduces the extent of distributional shift during self-training.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 02:10:54 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Wang", "Haoxuan", ""], ["Liu", "Anqi", ""], ["Yu", "Zhiding", ""], ["Yue", "Yisong", ""], ["Anandkumar", "Anima", ""]]}, {"id": "2010.05785", "submitter": "Oren Nuriel", "authors": "Oren Nuriel, Sagie Benaim, Lior Wolf", "title": "Permuted AdaIN: Reducing the Bias Towards Global Statistics in Image\n  Classification", "comments": "8 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work has shown that convolutional neural network classifiers overly\nrely on texture at the expense of shape cues. We make a similar but different\ndistinction between shape and local image cues, on the one hand, and global\nimage statistics, on the other. Our method, called Permuted Adaptive Instance\nNormalization (pAdaIN), reduces the representation of global statistics in the\nhidden layers of image classifiers. pAdaIN samples a random permutation $\\pi$\nthat rearranges the samples in a given batch. Adaptive Instance Normalization\n(AdaIN) is then applied between the activations of each (non-permuted) sample\n$i$ and the corresponding activations of the sample $\\pi(i)$, thus swapping\nstatistics between the samples of the batch. Since the global image statistics\nare distorted, this swapping procedure causes the network to rely on cues, such\nas shape or texture. By choosing the random permutation with probability $p$\nand the identity permutation otherwise, one can control the effect's strength.\n  With the correct choice of $p$, fixed apriori for all experiments and\nselected without considering test data, our method consistently outperforms\nbaselines in multiple settings. In image classification, our method improves on\nboth CIFAR100 and ImageNet using multiple architectures. In the setting of\nrobustness, our method improves on both ImageNet-C and Cifar-100-C for multiple\narchitectures. In the setting of domain adaptation and domain generalization,\nour method achieves state of the art results on the transfer learning task from\nGTAV to Cityscapes and on the PACS benchmark.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 16:38:38 GMT"}, {"version": "v2", "created": "Tue, 8 Dec 2020 15:21:29 GMT"}, {"version": "v3", "created": "Wed, 23 Jun 2021 17:04:49 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Nuriel", "Oren", ""], ["Benaim", "Sagie", ""], ["Wolf", "Lior", ""]]}, {"id": "2010.05796", "submitter": "Simone Zamboni", "authors": "Simone Zamboni, Zekarias Tilahun Kefato, Sarunas Girdzijauskas, Noren\n  Christoffer, Laura Dal Col", "title": "Pedestrian Trajectory Prediction with Convolutional Neural Networks", "comments": "33 pages, 10 figures. This article is a preprint submitted to Pattern\n  Recognition Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting the future trajectories of pedestrians is a challenging problem\nthat has a range of application, from crowd surveillance to autonomous driving.\nIn literature, methods to approach pedestrian trajectory prediction have\nevolved, transitioning from physics-based models to data-driven models based on\nrecurrent neural networks. In this work, we propose a new approach to\npedestrian trajectory prediction, with the introduction of a novel 2D\nconvolutional model. This new model outperforms recurrent models, and it\nachieves state-of-the-art results on the ETH and TrajNet datasets. We also\npresent an effective system to represent pedestrian positions and powerful data\naugmentation techniques, such as the addition of Gaussian noise and the use of\nrandom rotations, which can be applied to any model. As an additional\nexploratory analysis, we present experimental results on the inclusion of\noccupancy methods to model social information, which empirically show that\nthese methods are ineffective in capturing social interaction.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 15:51:01 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Zamboni", "Simone", ""], ["Kefato", "Zekarias Tilahun", ""], ["Girdzijauskas", "Sarunas", ""], ["Christoffer", "Noren", ""], ["Col", "Laura Dal", ""]]}, {"id": "2010.05810", "submitter": "Tsai-Shien Chen", "authors": "Tsai-Shien Chen, Man-Yu Lee, Chih-Ting Liu, Shao-Yi Chien", "title": "Viewpoint-Aware Channel-Wise Attentive Network for Vehicle\n  Re-Identification", "comments": "CVPR Workshop 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vehicle re-identification (re-ID) matches images of the same vehicle across\ndifferent cameras. It is fundamentally challenging because the dramatically\ndifferent appearance caused by different viewpoints would make the framework\nfail to match two vehicles of the same identity. Most existing works solved the\nproblem by extracting viewpoint-aware feature via spatial attention mechanism,\nwhich, yet, usually suffers from noisy generated attention map or otherwise\nrequires expensive keypoint labels to improve the quality. In this work, we\npropose Viewpoint-aware Channel-wise Attention Mechanism (VCAM) by observing\nthe attention mechanism from a different aspect. Our VCAM enables the feature\nlearning framework channel-wisely reweighing the importance of each feature\nmaps according to the \"viewpoint\" of input vehicle. Extensive experiments\nvalidate the effectiveness of the proposed method and show that we perform\nfavorably against state-of-the-arts methods on the public VeRi-776 dataset and\nobtain promising results on the 2020 AI City Challenge. We also conduct other\nexperiments to demonstrate the interpretability of how our VCAM practically\nassists the learning framework.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 16:05:41 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Chen", "Tsai-Shien", ""], ["Lee", "Man-Yu", ""], ["Liu", "Chih-Ting", ""], ["Chien", "Shao-Yi", ""]]}, {"id": "2010.05821", "submitter": "Yiming Li", "authors": "Yiming Li, Ziqi Zhang, Jiawang Bai, Baoyuan Wu, Yong Jiang, Shu-Tao\n  Xia", "title": "Open-sourced Dataset Protection via Backdoor Watermarking", "comments": "Accepted by the NeurIPS Workshop on Dataset Curation and Security,\n  2020. 6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rapid development of deep learning has benefited from the release of some\nhigh-quality open-sourced datasets ($e.g.$, ImageNet), which allows researchers\nto easily verify the effectiveness of their algorithms. Almost all existing\nopen-sourced datasets require that they can only be adopted for academic or\neducational purposes rather than commercial purposes, whereas there is still no\ngood way to protect them. In this paper, we propose a \\emph{backdoor embedding\nbased dataset watermarking} method to protect an open-sourced\nimage-classification dataset by verifying whether it is used for training a\nthird-party model. Specifically, the proposed method contains two main\nprocesses, including \\emph{dataset watermarking} and \\emph{dataset\nverification}. We adopt classical poisoning-based backdoor attacks ($e.g.$,\nBadNets) for dataset watermarking, ie, generating some poisoned samples by\nadding a certain trigger ($e.g.$, a local patch) onto some benign samples,\nlabeled with a pre-defined target class. Based on the proposed backdoor-based\nwatermarking, we use a hypothesis test guided method for dataset verification\nbased on the posterior probability generated by the suspicious third-party\nmodel of the benign samples and their correspondingly watermarked samples\n($i.e.$, images with trigger) on the target class. Experiments on some\nbenchmark datasets are conducted, which verify the effectiveness of the\nproposed method.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 16:16:27 GMT"}, {"version": "v2", "created": "Sun, 15 Nov 2020 08:32:27 GMT"}, {"version": "v3", "created": "Thu, 19 Nov 2020 04:51:13 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Li", "Yiming", ""], ["Zhang", "Ziqi", ""], ["Bai", "Jiawang", ""], ["Wu", "Baoyuan", ""], ["Jiang", "Yong", ""], ["Xia", "Shu-Tao", ""]]}, {"id": "2010.05838", "submitter": "Royson Lee", "authors": "Royson Lee, Stylianos I. Venieris, Nicholas D. Lane", "title": "Neural Enhancement in Content Delivery Systems: The State-of-the-Art and\n  Future Directions", "comments": "Accepted at the 1st Workshop on Distributed Machine Learning at\n  CoNEXT 2020 (DistributedML 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DC eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Internet-enabled smartphones and ultra-wide displays are transforming a\nvariety of visual apps spanning from on-demand movies and 360-degree videos to\nvideo-conferencing and live streaming. However, robustly delivering visual\ncontent under fluctuating networking conditions on devices of diverse\ncapabilities remains an open problem. In recent years, advances in the field of\ndeep learning on tasks such as super-resolution and image enhancement have led\nto unprecedented performance in generating high-quality images from low-quality\nones, a process we refer to as neural enhancement. In this paper, we survey\nstate-of-the-art content delivery systems that employ neural enhancement as a\nkey component in achieving both fast response time and high visual quality. We\nfirst present the deployment challenges of neural enhancement models. We then\ncover systems targeting diverse use-cases and analyze their design decisions in\novercoming technical challenges. Moreover, we present promising directions\nbased on the latest insights from deep learning research to further boost the\nquality of experience of these systems.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 16:41:29 GMT"}, {"version": "v2", "created": "Thu, 22 Oct 2020 12:42:00 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Lee", "Royson", ""], ["Venieris", "Stylianos I.", ""], ["Lane", "Nicholas D.", ""]]}, {"id": "2010.05855", "submitter": "Chuanbo Wang", "authors": "Chuanbo Wang, DM Anisuzzaman, Victor Williamson, Mrinal Kanti Dhar,\n  Behrouz Rostami, Jeffrey Niezgoda, Sandeep Gopalakrishnan and Zeyun Yu", "title": "Fully Automatic Wound Segmentation with Deep Convolutional Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Acute and chronic wounds have varying etiologies and are an economic burden\nto healthcare systems around the world. The advanced wound care market is\nexpected to exceed $22 billion by 2024. Wound care professionals rely heavily\non images and image documentation for proper diagnosis and treatment.\nUnfortunately lack of expertise can lead to improper diagnosis of wound\netiology and inaccurate wound management and documentation. Fully automatic\nsegmentation of wound areas in natural images is an important part of the\ndiagnosis and care protocol since it is crucial to measure the area of the\nwound and provide quantitative parameters in the treatment. Various deep\nlearning models have gained success in image analysis including semantic\nsegmentation. Particularly, MobileNetV2 stands out among others due to its\nlightweight architecture and uncompromised performance. This manuscript\nproposes a novel convolutional framework based on MobileNetV2 and connected\ncomponent labelling to segment wound regions from natural images. We build an\nannotated wound image dataset consisting of 1,109 foot ulcer images from 889\npatients to train and test the deep learning models. We demonstrate the\neffectiveness and mobility of our method by conducting comprehensive\nexperiments and analyses on various segmentation neural networks.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 17:02:48 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Wang", "Chuanbo", ""], ["Anisuzzaman", "DM", ""], ["Williamson", "Victor", ""], ["Dhar", "Mrinal Kanti", ""], ["Rostami", "Behrouz", ""], ["Niezgoda", "Jeffrey", ""], ["Gopalakrishnan", "Sandeep", ""], ["Yu", "Zeyun", ""]]}, {"id": "2010.05858", "submitter": "Mark Fonaryov", "authors": "Mark Fonaryov and Michael Lindenbaum", "title": "On the Minimal Recognizable Image Patch", "comments": "Submitted to ICPR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In contrast to human vision, common recognition algorithms often fail on\npartially occluded images. We propose characterizing, empirically, the\nalgorithmic limits by finding a minimal recognizable patch (MRP) that is by\nitself sufficient to recognize the image. A specialized deep network allows us\nto find the most informative patches of a given size, and serves as an\nexperimental tool. A human vision study recently characterized related (but\ndifferent) minimally recognizable configurations (MIRCs) [1], for which we\nspecify computational analogues (denoted cMIRCs). The drop in human decision\naccuracy associated with size reduction of these MIRCs is substantial and\nsharp. Interestingly, such sharp reductions were also found for the\ncomputational versions we specified.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 17:06:16 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Fonaryov", "Mark", ""], ["Lindenbaum", "Michael", ""]]}, {"id": "2010.05862", "submitter": "Yogesh Balaji", "authors": "Yogesh Balaji, Rama Chellappa and Soheil Feizi", "title": "Robust Optimal Transport with Applications in Generative Modeling and\n  Domain Adaptation", "comments": "Accepted in NeurIPS 2020. Code available at\n  https://github.com/yogeshbalaji/robustOT", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Optimal Transport (OT) distances such as Wasserstein have been used in\nseveral areas such as GANs and domain adaptation. OT, however, is very\nsensitive to outliers (samples with large noise) in the data since in its\nobjective function, every sample, including outliers, is weighed similarly due\nto the marginal constraints. To remedy this issue, robust formulations of OT\nwith unbalanced marginal constraints have previously been proposed. However,\nemploying these methods in deep learning problems such as GANs and domain\nadaptation is challenging due to the instability of their dual optimization\nsolvers. In this paper, we resolve these issues by deriving a\ncomputationally-efficient dual form of the robust OT optimization that is\namenable to modern deep learning applications. We demonstrate the effectiveness\nof our formulation in two applications of GANs and domain adaptation. Our\napproach can train state-of-the-art GAN models on noisy datasets corrupted with\noutlier distributions. In particular, our optimization computes weights for\ntraining samples reflecting how difficult it is for those samples to be\ngenerated in the model. In domain adaptation, our robust OT formulation leads\nto improved accuracy compared to the standard adversarial adaptation methods.\nOur code is available at https://github.com/yogeshbalaji/robustOT.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 17:13:40 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Balaji", "Yogesh", ""], ["Chellappa", "Rama", ""], ["Feizi", "Soheil", ""]]}, {"id": "2010.05864", "submitter": "Jingkang Yang", "authors": "Jingkang Yang, Weirong Chen, Litong Feng, Xiaopeng Yan, Huabin Zheng,\n  Wayne Zhang", "title": "Webly Supervised Image Classification with Metadata: Automatic Noisy\n  Label Correction via Visual-Semantic Graph", "comments": "Accepted to ACM Multimedia 2020 (Oral)", "journal-ref": null, "doi": "10.1145/3394171.3413952", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Webly supervised learning becomes attractive recently for its efficiency in\ndata expansion without expensive human labeling. However, adopting search\nqueries or hashtags as web labels of images for training brings massive noise\nthat degrades the performance of DNNs. Especially, due to the semantic\nconfusion of query words, the images retrieved by one query may contain\ntremendous images belonging to other concepts. For example, searching `tiger\ncat' on Flickr will return a dominating number of tiger images rather than the\ncat images. These realistic noisy samples usually have clear visual semantic\nclusters in the visual space that mislead DNNs from learning accurate semantic\nlabels. To correct real-world noisy labels, expensive human annotations seem\nindispensable. Fortunately, we find that metadata can provide extra knowledge\nto discover clean web labels in a labor-free fashion, making it feasible to\nautomatically provide correct semantic guidance among the massive label-noisy\nweb data. In this paper, we propose an automatic label corrector VSGraph-LC\nbased on the visual-semantic graph. VSGraph-LC starts from anchor selection\nreferring to the semantic similarity between metadata and correct label\nconcepts, and then propagates correct labels from anchors on a visual graph\nusing graph neural network (GNN). Experiments on realistic webly supervised\nlearning datasets Webvision-1000 and NUS-81-Web show the effectiveness and\nrobustness of VSGraph-LC. Moreover, VSGraph-LC reveals its advantage on the\nopen-set validation set.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 17:15:51 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Yang", "Jingkang", ""], ["Chen", "Weirong", ""], ["Feng", "Litong", ""], ["Yan", "Xiaopeng", ""], ["Zheng", "Huabin", ""], ["Zhang", "Wayne", ""]]}, {"id": "2010.05903", "submitter": "Niv Cohen", "authors": "Tal Reiss, Niv Cohen, Liron Bergman and Yedid Hoshen", "title": "PANDA: Adapting Pretrained Features for Anomaly Detection and\n  Segmentation", "comments": "CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anomaly detection methods require high-quality features. In recent years, the\nanomaly detection community has attempted to obtain better features using\nadvances in deep self-supervised feature learning. Surprisingly, a very\npromising direction, using pretrained deep features, has been mostly\noverlooked. In this paper, we first empirically establish the perhaps expected,\nbut unreported result, that combining pretrained features with simple anomaly\ndetection and segmentation methods convincingly outperforms, much more complex,\nstate-of-the-art methods.\n  In order to obtain further performance gains in anomaly detection, we adapt\npretrained features to the target distribution. Although transfer learning\nmethods are well established in multi-class classification problems, the\none-class classification (OCC) setting is not as well explored. It turns out\nthat naive adaptation methods, which typically work well in supervised\nlearning, often result in catastrophic collapse (feature deterioration) and\nreduce performance in OCC settings. A popular OCC method, DeepSVDD, advocates\nusing specialized architectures, but this limits the adaptation performance\ngain. We propose two methods for combating collapse: i) a variant of early\nstopping that dynamically learns the stopping iteration ii) elastic\nregularization inspired by continual learning. Our method, PANDA, outperforms\nthe state-of-the-art in the OCC, outlier exposure and anomaly segmentation\nsettings by large margins.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 17:52:50 GMT"}, {"version": "v2", "created": "Thu, 6 May 2021 17:56:04 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Reiss", "Tal", ""], ["Cohen", "Niv", ""], ["Bergman", "Liron", ""], ["Hoshen", "Yedid", ""]]}, {"id": "2010.05907", "submitter": "Anand Bhattad", "authors": "Anand Bhattad and David A. Forsyth", "title": "Cut-and-Paste Neural Rendering", "comments": "Project page: https://anandbhattad.github.io/projects/reshading tldr:\n  Convincing cut-and-paste reshading by consistent image decomposition\n  inferences", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cut-and-paste methods take an object from one image and insert it into\nanother. Doing so often results in unrealistic looking images because the\ninserted object's shading is inconsistent with the target scene's shading.\nExisting reshading methods require a geometric and physical model of the\ninserted object, which is then rendered using environment parameters.\nAccurately constructing such a model only from a single image is beyond the\ncurrent understanding of computer vision. We describe an alternative procedure\n-- cut-and-paste neural rendering, to render the inserted fragment's shading\nfield consistent with the target scene. We use a Deep Image Prior (DIP) as a\nneural renderer trained to render an image with consistent image decomposition\ninferences. The resulting rendering from DIP should have an albedo consistent\nwith composite albedo; it should have a shading field that, outside the\ninserted fragment, is the same as the target scene's shading field; and\ncomposite surface normals are consistent with the final rendering's shading\nfield. The result is a simple procedure that produces convincing and realistic\nshading. Moreover, our procedure does not require rendered images or\nimage-decomposition from real images in the training or labeled annotations. In\nfact, our only use of simulated ground truth is our use of a pre-trained normal\nestimator. Qualitative results are strong, supported by a user study comparing\nagainst the state-of-the-art image harmonization baseline.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 17:59:55 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Bhattad", "Anand", ""], ["Forsyth", "David A.", ""]]}, {"id": "2010.05949", "submitter": "Daniel Groos", "authors": "Daniel Groos, Lars Adde, Ragnhild St{\\o}en, Heri Ramampiaro, Espen A.\n  F. Ihlen", "title": "Towards human performance on automatic motion tracking of infant\n  spontaneous movements", "comments": "This work will be submitted to Computerized Medical Imaging and\n  Graphics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Assessment of spontaneous movements can predict the long-term developmental\noutcomes in high-risk infants. In order to develop algorithms for automated\nprediction of later function based on early motor repertoire, high-precision\ntracking of segments and joints are required. Four types of convolutional\nneural networks were investigated on a novel infant pose dataset, covering the\nlarge variation in 1 424 videos from a clinical international community. The\nprecision level of the networks was evaluated as the deviation between the\nestimated keypoint positions and human expert annotations. The computational\nefficiency was also assessed to determine the feasibility of the neural\nnetworks in clinical practice. The study shows that the precision of the best\nperforming infant motion tracker is similar to the inter-rater error of human\nexperts, while still operating efficiently. In conclusion, the proposed\ntracking of infant movements can pave the way for early detection of motor\ndisorders in children with perinatal brain injuries by quantifying infant\nmovements from video recordings with human precision.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 18:17:47 GMT"}, {"version": "v2", "created": "Thu, 22 Oct 2020 07:40:31 GMT"}, {"version": "v3", "created": "Sat, 31 Oct 2020 08:30:54 GMT"}, {"version": "v4", "created": "Sun, 15 Nov 2020 18:21:21 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Groos", "Daniel", ""], ["Adde", "Lars", ""], ["St\u00f8en", "Ragnhild", ""], ["Ramampiaro", "Heri", ""], ["Ihlen", "Espen A. F.", ""]]}, {"id": "2010.05957", "submitter": "Wei Xu", "authors": "Wei Xu, Dongjiao He, Yixi Cai, Fu Zhang", "title": "Robots State Estimation and Observability Analysis Based on Statistical\n  Motion Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a generic motion model to capture mobile robots' dynamic\nbehaviors (translation and rotation). The model is based on statistical models\ndriven by white random processes and is formulated into a full state estimation\nalgorithm based on the error-state extended Kalman filtering framework (ESEKF).\nMajor benefits of this method are its versatility, being applicable to\ndifferent robotic systems without accurately modeling the robots' specific\ndynamics, and ability to estimate the robot's (angular) acceleration, jerk, or\nhigher-order dynamic states with low delay. Mathematical analysis with\nnumerical simulations are presented to show the properties of the statistical\nmodel-based estimation framework and to reveal its connection to existing\nlow-pass filters. Furthermore, a new paradigm is developed for robots\nobservability analysis by developing Lie derivatives and associated partial\ndifferentiation directly on manifolds. It is shown that this new paradigm is\nmuch simpler and more natural than existing methods based on quaternion\nparameterizations. It is also scalable to high dimensional systems. A novel\n\\textbf{\\textit{thin}} set concept is introduced to characterize the\nunobservable subset of the system states, providing the theoretical foundation\nto observability analysis of robotic systems operating on manifolds and in high\ndimension. Finally, extensive experiments including full state estimation and\nextrinsic calibration (both POS-IMU and IMU-IMU) on a quadrotor UAV, a handheld\nplatform and a ground vehicle are conducted. Comparisons with existing methods\nshow that the proposed method can effectively estimate all extrinsic\nparameters, the robot's translation/angular acceleration and other state\nvariables (e.g., position, velocity, attitude) of high accuracy and low delay.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 18:35:33 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Xu", "Wei", ""], ["He", "Dongjiao", ""], ["Cai", "Yixi", ""], ["Zhang", "Fu", ""]]}, {"id": "2010.05970", "submitter": "Jonathan Hersh", "authors": "Hannes Mueller, Andre Groger, Jonathan Hersh, Andrea Matranga and Joan\n  Serrat", "title": "Monitoring War Destruction from Space: A Machine Learning Approach", "comments": null, "journal-ref": null, "doi": "10.1073/pnas.2025400118", "report-no": null, "categories": "econ.GN cs.AI cs.CV q-fin.EC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing data on building destruction in conflict zones rely on eyewitness\nreports or manual detection, which makes it generally scarce, incomplete and\npotentially biased. This lack of reliable data imposes severe limitations for\nmedia reporting, humanitarian relief efforts, human rights monitoring,\nreconstruction initiatives, and academic studies of violent conflict. This\narticle introduces an automated method of measuring destruction in\nhigh-resolution satellite images using deep learning techniques combined with\ndata augmentation to expand training samples. We apply this method to the\nSyrian civil war and reconstruct the evolution of damage in major cities across\nthe country. The approach allows generating destruction data with unprecedented\nscope, resolution, and frequency - only limited by the available satellite\nimagery - which can alleviate data limitations decisively.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 19:01:20 GMT"}, {"version": "v2", "created": "Wed, 14 Oct 2020 03:47:28 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Mueller", "Hannes", ""], ["Groger", "Andre", ""], ["Hersh", "Jonathan", ""], ["Matranga", "Andrea", ""], ["Serrat", "Joan", ""]]}, {"id": "2010.05981", "submitter": "Yingwei Li", "authors": "Yingwei Li, Qihang Yu, Mingxing Tan, Jieru Mei, Peng Tang, Wei Shen,\n  Alan Yuille, Cihang Xie", "title": "Shape-Texture Debiased Neural Network Training", "comments": "ICLR 2021. The code is available here:\n  https://github.com/LiYingwei/ShapeTextureDebiasedTraining", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shape and texture are two prominent and complementary cues for recognizing\nobjects. Nonetheless, Convolutional Neural Networks are often biased towards\neither texture or shape, depending on the training dataset. Our ablation shows\nthat such bias degenerates model performance. Motivated by this observation, we\ndevelop a simple algorithm for shape-texture debiased learning. To prevent\nmodels from exclusively attending on a single cue in representation learning,\nwe augment training data with images with conflicting shape and texture\ninformation (eg, an image of chimpanzee shape but with lemon texture) and, most\nimportantly, provide the corresponding supervisions from shape and texture\nsimultaneously.\n  Experiments show that our method successfully improves model performance on\nseveral image recognition benchmarks and adversarial robustness. For example,\nby training on ImageNet, it helps ResNet-152 achieve substantial improvements\non ImageNet (+1.2%), ImageNet-A (+5.2%), ImageNet-C (+8.3%) and\nStylized-ImageNet (+11.1%), and on defending against FGSM adversarial attacker\non ImageNet (+14.4%). Our method also claims to be compatible with other\nadvanced data augmentation strategies, eg, Mixup, and CutMix. The code is\navailable here: https://github.com/LiYingwei/ShapeTextureDebiasedTraining.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 19:16:12 GMT"}, {"version": "v2", "created": "Tue, 30 Mar 2021 19:16:30 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Li", "Yingwei", ""], ["Yu", "Qihang", ""], ["Tan", "Mingxing", ""], ["Mei", "Jieru", ""], ["Tang", "Peng", ""], ["Shen", "Wei", ""], ["Yuille", "Alan", ""], ["Xie", "Cihang", ""]]}, {"id": "2010.06000", "submitter": "Sanjay Subramanian", "authors": "Sanjay Subramanian, Lucy Lu Wang, Sachin Mehta, Ben Bogin, Madeleine\n  van Zuylen, Sravanthi Parasa, Sameer Singh, Matt Gardner, Hannaneh Hajishirzi", "title": "MedICaT: A Dataset of Medical Images, Captions, and Textual References", "comments": "EMNLP-Findings 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the relationship between figures and text is key to scientific\ndocument understanding. Medical figures in particular are quite complex, often\nconsisting of several subfigures (75% of figures in our dataset), with detailed\ntext describing their content. Previous work studying figures in scientific\npapers focused on classifying figure content rather than understanding how\nimages relate to the text. To address challenges in figure retrieval and\nfigure-to-text alignment, we introduce MedICaT, a dataset of medical images in\ncontext. MedICaT consists of 217K images from 131K open access biomedical\npapers, and includes captions, inline references for 74% of figures, and\nmanually annotated subfigures and subcaptions for a subset of figures. Using\nMedICaT, we introduce the task of subfigure to subcaption alignment in compound\nfigures and demonstrate the utility of inline references in image-text\nmatching. Our data and code can be accessed at\nhttps://github.com/allenai/medicat.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 19:56:08 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Subramanian", "Sanjay", ""], ["Wang", "Lucy Lu", ""], ["Mehta", "Sachin", ""], ["Bogin", "Ben", ""], ["van Zuylen", "Madeleine", ""], ["Parasa", "Sravanthi", ""], ["Singh", "Sameer", ""], ["Gardner", "Matt", ""], ["Hajishirzi", "Hannaneh", ""]]}, {"id": "2010.06027", "submitter": "Tejas Sudharshan Mathai", "authors": "Tejas Sudharshan Mathai, Yi Wang, Nathan Cross", "title": "Assessing Lesion Segmentation Bias of Neural Networks on Motion\n  Corrupted Brain MRI", "comments": "First two authors contributed equally. Accepted at MICCAI BrainLes\n  2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG physics.med-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Patient motion during the magnetic resonance imaging (MRI) acquisition\nprocess results in motion artifacts, which limits the ability of radiologists\nto provide a quantitative assessment of a condition visualized. Often times,\nradiologists either \"see through\" the artifacts with reduced diagnostic\nconfidence, or the MR scans are rejected and patients are asked to be recalled\nand re-scanned. Presently, there are many published approaches that focus on\nMRI artifact detection and correction. However, the key question of the bias\nexhibited by these algorithms on motion corrupted MRI images is still\nunanswered. In this paper, we seek to quantify the bias in terms of the impact\nthat different levels of motion artifacts have on the performance of neural\nnetworks engaged in a lesion segmentation task. Additionally, we explore the\neffect of a different learning strategy, curriculum learning, on the\nsegmentation performance. Our results suggest that a network trained using\ncurriculum learning is effective at compensating for different levels of motion\nartifacts, and improved the segmentation performance by ~9%-15% (p < 0.05) when\ncompared against a conventional shuffled learning strategy on the same motion\ndata. Within each motion category, it either improved or maintained the dice\nscore. To the best of our knowledge, we are the first to quantitatively assess\nthe segmentation bias on various levels of motion artifacts present in a brain\nMRI image.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 21:06:40 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Mathai", "Tejas Sudharshan", ""], ["Wang", "Yi", ""], ["Cross", "Nathan", ""]]}, {"id": "2010.06034", "submitter": "Sharib Ali Dr.", "authors": "Sharib Ali, Mariia Dmitrieva, Noha Ghatwary, Sophia Bano, Gorkem\n  Polat, Alptekin Temizel, Adrian Krenzer, Amar Hekalo, Yun Bo Guo, Bogdan\n  Matuszewski, Mourad Gridach, Irina Voiculescu, Vishnusai Yoganand, Arnav\n  Chavan, Aryan Raj, Nhan T. Nguyen, Dat Q. Tran, Le Duy Huynh, Nicolas Boutry,\n  Shahadate Rezvy, Haijian Chen, Yoon Ho Choi, Anand Subramanian, Velmurugan\n  Balasubramanian, Xiaohong W. Gao, Hongyu Hu, Yusheng Liao, Danail Stoyanov,\n  Christian Daul, Stefano Realdon, Renato Cannizzaro, Dominique Lamarque, Terry\n  Tran-Nguyen, Adam Bailey, Barbara Braden, James East and Jens Rittscher", "title": "Deep learning for detection and segmentation of artefact and disease\n  instances in gastrointestinal endoscopy", "comments": "32 pages", "journal-ref": null, "doi": "10.1016/j.media.2021.102002", "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The Endoscopy Computer Vision Challenge (EndoCV) is a crowd-sourcing\ninitiative to address eminent problems in developing reliable computer aided\ndetection and diagnosis endoscopy systems and suggest a pathway for clinical\ntranslation of technologies. Whilst endoscopy is a widely used diagnostic and\ntreatment tool for hollow-organs, there are several core challenges often faced\nby endoscopists, mainly: 1) presence of multi-class artefacts that hinder their\nvisual interpretation, and 2) difficulty in identifying subtle precancerous\nprecursors and cancer abnormalities. Artefacts often affect the robustness of\ndeep learning methods applied to the gastrointestinal tract organs as they can\nbe confused with tissue of interest. EndoCV2020 challenges are designed to\naddress research questions in these remits. In this paper, we present a summary\nof methods developed by the top 17 teams and provide an objective comparison of\nstate-of-the-art methods and methods designed by the participants for two\nsub-challenges: i) artefact detection and segmentation (EAD2020), and ii)\ndisease detection and segmentation (EDD2020). Multi-center, multi-organ,\nmulti-class, and multi-modal clinical endoscopy datasets were compiled for both\nEAD2020 and EDD2020 sub-challenges. The out-of-sample generalization ability of\ndetection algorithms was also evaluated. Whilst most teams focused on accuracy\nimprovements, only a few methods hold credibility for clinical usability. The\nbest performing teams provided solutions to tackle class imbalance, and\nvariabilities in size, origin, modality and occurrences by exploring data\naugmentation, data fusion, and optimal class thresholding techniques.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 21:22:37 GMT"}, {"version": "v2", "created": "Wed, 17 Feb 2021 17:49:32 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Ali", "Sharib", ""], ["Dmitrieva", "Mariia", ""], ["Ghatwary", "Noha", ""], ["Bano", "Sophia", ""], ["Polat", "Gorkem", ""], ["Temizel", "Alptekin", ""], ["Krenzer", "Adrian", ""], ["Hekalo", "Amar", ""], ["Guo", "Yun Bo", ""], ["Matuszewski", "Bogdan", ""], ["Gridach", "Mourad", ""], ["Voiculescu", "Irina", ""], ["Yoganand", "Vishnusai", ""], ["Chavan", "Arnav", ""], ["Raj", "Aryan", ""], ["Nguyen", "Nhan T.", ""], ["Tran", "Dat Q.", ""], ["Huynh", "Le Duy", ""], ["Boutry", "Nicolas", ""], ["Rezvy", "Shahadate", ""], ["Chen", "Haijian", ""], ["Choi", "Yoon Ho", ""], ["Subramanian", "Anand", ""], ["Balasubramanian", "Velmurugan", ""], ["Gao", "Xiaohong W.", ""], ["Hu", "Hongyu", ""], ["Liao", "Yusheng", ""], ["Stoyanov", "Danail", ""], ["Daul", "Christian", ""], ["Realdon", "Stefano", ""], ["Cannizzaro", "Renato", ""], ["Lamarque", "Dominique", ""], ["Tran-Nguyen", "Terry", ""], ["Bailey", "Adam", ""], ["Braden", "Barbara", ""], ["East", "James", ""], ["Rittscher", "Jens", ""]]}, {"id": "2010.06045", "submitter": "Thomas Vandal", "authors": "Thomas Vandal, Daniel McDuff, Weile Wang, Andrew Michaelis,\n  Ramakrishna Nemani", "title": "Spectral Synthesis for Satellite-to-Satellite Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Earth observing satellites carrying multi-spectral sensors are widely used to\nmonitor the physical and biological states of the atmosphere, land, and oceans.\nThese satellites have different vantage points above the earth and different\nspectral imaging bands resulting in inconsistent imagery from one to another.\nThis presents challenges in building downstream applications. What if we could\ngenerate synthetic bands for existing satellites from the union of all domains?\nWe tackle the problem of generating synthetic spectral imagery for\nmultispectral sensors as an unsupervised image-to-image translation problem\nwith partial labels and introduce a novel shared spectral reconstruction loss.\nSimulated experiments performed by dropping one or more spectral bands show\nthat cross-domain reconstruction outperforms measurements obtained from a\nsecond vantage point. On a downstream cloud detection task, we show that\ngenerating synthetic bands with our model improves segmentation performance\nbeyond our baseline. Our proposed approach enables synchronization of\nmultispectral data and provides a basis for more homogeneous remote sensing\ndatasets.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 21:36:39 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Vandal", "Thomas", ""], ["McDuff", "Daniel", ""], ["Wang", "Weile", ""], ["Michaelis", "Andrew", ""], ["Nemani", "Ramakrishna", ""]]}, {"id": "2010.06073", "submitter": "Lior Shamir", "authors": "Hunter Goddard, Lior Shamir", "title": "A catalog of broad morphology of Pan-STARRS galaxies based on deep\n  learning", "comments": "ApJS, accepted", "journal-ref": null, "doi": "10.3847/1538-4365/abc0ed", "report-no": null, "categories": "astro-ph.GA astro-ph.IM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous digital sky surveys such as Pan-STARRS have the ability to image a\nvery large number of galactic and extra-galactic objects, and the large and\ncomplex nature of the image data reinforces the use of automation. Here we\ndescribe the design and implementation of a data analysis process for automatic\nbroad morphology annotation of galaxies, and applied it to the data of\nPan-STARRS DR1. The process is based on filters followed by a two-step\nconvolutional neural network (CNN) classification. Training samples are\ngenerated by using an augmented and balanced set of manually classified\ngalaxies. Results are evaluated for accuracy by comparison to the annotation of\nPan-STARRS included in a previous broad morphology catalog of SDSS galaxies.\nOur analysis shows that a CNN combined with several filters is an effective\napproach for annotating the galaxies and removing unclean images. The catalog\ncontains morphology labels for 1,662,190 galaxies with ~95% accuracy. The\naccuracy can be further improved by selecting labels above certain confidence\nthresholds. The catalog is publicly available.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 23:20:35 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Goddard", "Hunter", ""], ["Shamir", "Lior", ""]]}, {"id": "2010.06087", "submitter": "Yash Kant", "authors": "Yash Kant, Abhinav Moudgil, Dhruv Batra, Devi Parikh, Harsh Agrawal", "title": "Contrast and Classify: Training Robust VQA Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent Visual Question Answering (VQA) models have shown impressive\nperformance on the VQA benchmark but remain sensitive to small linguistic\nvariations in input questions. Existing approaches address this by augmenting\nthe dataset with question paraphrases from visual question generation models or\nadversarial perturbations. These approaches use the combined data to learn an\nanswer classifier by minimizing the standard cross-entropy loss. To more\neffectively leverage augmented data, we build on the recent success in\ncontrastive learning. We propose a novel training paradigm (ConClaT) that\noptimizes both cross-entropy and contrastive losses. The contrastive loss\nencourages representations to be robust to linguistic variations in questions\nwhile the cross-entropy loss preserves the discriminative power of\nrepresentations for answer prediction.\n  We find that optimizing both losses -- either alternately or jointly -- is\nkey to effective training. On the VQA-Rephrasings benchmark, which measures the\nVQA model's answer consistency across human paraphrases of a question, ConClaT\nimproves Consensus Score by 1 .63% over an improved baseline. In addition, on\nthe standard VQA 2.0 benchmark, we improve the VQA accuracy by 0.78% overall.\nWe also show that ConClaT is agnostic to the type of data-augmentation strategy\nused.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 00:23:59 GMT"}, {"version": "v2", "created": "Mon, 19 Apr 2021 03:45:27 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Kant", "Yash", ""], ["Moudgil", "Abhinav", ""], ["Batra", "Dhruv", ""], ["Parikh", "Devi", ""], ["Agrawal", "Harsh", ""]]}, {"id": "2010.06096", "submitter": "Sunny Verma", "authors": "Sunny Verma, Chen Wang, Liming Zhu, and Wei Liu", "title": "Attn-HybridNet: Improving Discriminability of Hybrid Features with\n  Attention Fusion", "comments": "Under minor review at IEEE Transactions on Cybernetics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The principal component analysis network (PCANet) is an unsupervised\nparsimonious deep network, utilizing principal components as filters in its\nconvolution layers. Albeit powerful, the PCANet consists of basic operations\nsuch as principal components and spatial pooling, which suffers from two\nfundamental problems. First, the principal components obtain information by\ntransforming it to column vectors (which we call the amalgamated view), which\nincurs the loss of the spatial information in the data. Second, the generalized\nspatial pooling utilized in the PCANet induces feature redundancy and also\nfails to accommodate spatial statistics of natural images. In this research, we\nfirst propose a tensor-factorization based deep network called the Tensor\nFactorization Network (TFNet). The TFNet extracts features from the spatial\nstructure of the data (which we call the minutiae view). We then show that the\ninformation obtained by the PCANet and the TFNet are distinctive and\nnon-trivial but individually insufficient. This phenomenon necessitates the\ndevelopment of proposed HybridNet, which integrates the information discovery\nwith the two views of the data. To enhance the discriminability of hybrid\nfeatures, we propose Attn-HybridNet, which alleviates the feature redundancy by\nperforming attention-based feature fusion. The significance of our proposed\nAttn-HybridNet is demonstrated on multiple real-world datasets where the\nfeatures obtained with Attn-HybridNet achieves better classification\nperformance over other popular baseline methods, demonstrating the\neffectiveness of the proposed technique.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 00:52:57 GMT"}, {"version": "v2", "created": "Wed, 14 Oct 2020 12:44:41 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Verma", "Sunny", ""], ["Wang", "Chen", ""], ["Zhu", "Liming", ""], ["Liu", "Wei", ""]]}, {"id": "2010.06097", "submitter": "Feihu Huang", "authors": "Feihu Huang, Shangqian Gao and Heng Huang", "title": "Gradient Descent Ascent for Min-Max Problems on Riemannian Manifolds", "comments": "32 pages. We have updated the theoretical results of our methods in\n  this new revision. E.g., our MVR-RSGDA algorithm achieves a lower sample\n  complexity. arXiv admin note: text overlap with arXiv:2008.08170", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the paper, we study a class of useful non-convex minimax optimization\nproblems on Riemanian manifolds and propose a class of Riemanian gradient\ndescent ascent algorithms to solve these minimax problems. Specifically, we\npropose a new Riemannian gradient descent ascent (RGDA) algorithm for the\n\\textbf{deterministic} minimax optimization. Moreover, we prove that the RGDA\nhas a sample complexity of $O(\\kappa^2\\epsilon^{-2})$ for finding an\n$\\epsilon$-stationary point of the nonconvex strongly-concave minimax problems,\nwhere $\\kappa$ denotes the condition number. At the same time, we introduce a\nRiemannian stochastic gradient descent ascent (RSGDA) algorithm for the\n\\textbf{stochastic} minimax optimization. In the theoretical analysis, we prove\nthat the RSGDA can achieve a sample complexity of $O(\\kappa^3\\epsilon^{-4})$.\nTo further reduce the sample complexity, we propose a novel momentum\nvariance-reduced Riemannian stochastic gradient descent ascent (MVR-RSGDA)\nalgorithm based on the momentum-based variance-reduced technique of STORM. We\nprove that the MVR-RSGDA algorithm achieves a lower sample complexity of\n$\\tilde{O}(\\kappa^{(3-\\nu/2)}\\epsilon^{-3})$ for $\\nu \\geq 0$, which reaches\nthe best known sample complexity for its Euclidean counterpart. Extensive\nexperimental results on the robust deep neural networks training over Stiefel\nmanifold demonstrate the efficiency of our proposed algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 00:54:00 GMT"}, {"version": "v2", "created": "Tue, 24 Nov 2020 03:43:15 GMT"}, {"version": "v3", "created": "Wed, 17 Mar 2021 15:09:46 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Huang", "Feihu", ""], ["Gao", "Shangqian", ""], ["Huang", "Heng", ""]]}, {"id": "2010.06099", "submitter": "Felipe Farias Mr.", "authors": "Felipe Farias, Teresa Ludermir, Carmelo Bastos-Filho", "title": "Similarity Based Stratified Splitting: an approach to train better\n  classifiers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a Similarity-Based Stratified Splitting (SBSS) technique, which\nuses both the output and input space information to split the data. The splits\nare generated using similarity functions among samples to place similar samples\nin different splits. This approach allows for a better representation of the\ndata in the training phase. This strategy leads to a more realistic performance\nestimation when used in real-world applications. We evaluate our proposal in\ntwenty-two benchmark datasets with classifiers such as Multi-Layer Perceptron,\nSupport Vector Machine, Random Forest and K-Nearest Neighbors, and five\nsimilarity functions Cityblock, Chebyshev, Cosine, Correlation, and Euclidean.\nAccording to the Wilcoxon Sign-Rank test, our approach consistently\noutperformed ordinary stratified 10-fold cross-validation in 75\\% of the\nassessed scenarios.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 01:07:48 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Farias", "Felipe", ""], ["Ludermir", "Teresa", ""], ["Bastos-Filho", "Carmelo", ""]]}, {"id": "2010.06100", "submitter": "Sarah Ostadabbas", "authors": "Xiaofei Huang, Nihang Fu, Shuangjun Liu, Sarah Ostadabbas", "title": "Invariant Representation Learning for Infant Pose Estimation with Small\n  Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Infant motion analysis is a topic with critical importance in early childhood\ndevelopment studies. However, while the applications of human pose estimation\nhave become more and more broad, models trained on large-scale adult pose\ndatasets are barely successful in estimating infant poses due to the\nsignificant differences in their body ratio and the versatility of their poses.\nMoreover, the privacy and security considerations hinder the availability of\nadequate infant pose data required for training of a robust model from scratch.\nTo address this problem, this paper presents (1) building and publicly\nreleasing a hybrid synthetic and real infant pose (SyRIP) dataset with small\nyet diverse real infant images as well as generated synthetic infant poses and\n(2) a multi-stage invariant representation learning strategy that could\ntransfer the knowledge from the adjacent domains of adult poses and synthetic\ninfant images into our fine-tuned domain-adapted infant pose (FiDIP) estimation\nmodel. In our ablation study, with identical network structure, models trained\non SyRIP dataset show noticeable improvement over the ones trained on the only\nother public infant pose datasets. Integrated with pose estimation backbone\nnetworks with varying complexity, FiDIP performs consistently better than the\nfine-tuned versions of those models. One of our best infant pose estimation\nperformers on the state-of-the-art DarkPose model shows mean average precision\n(mAP) of 93.6.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 01:10:14 GMT"}, {"version": "v2", "created": "Tue, 1 Dec 2020 01:29:16 GMT"}, {"version": "v3", "created": "Wed, 2 Dec 2020 01:43:49 GMT"}, {"version": "v4", "created": "Sun, 30 May 2021 01:45:50 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Huang", "Xiaofei", ""], ["Fu", "Nihang", ""], ["Liu", "Shuangjun", ""], ["Ostadabbas", "Sarah", ""]]}, {"id": "2010.06107", "submitter": "Xiaoman Zhang", "authors": "Xiaoman Zhang, Ya Zhang, Xiaoyun Zhang, and Yanfeng Wang", "title": "Universal Model for 3D Medical Image Analysis", "comments": "Submitted to IEEE Transactions on Medical Imaging", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Learning-based methods recently have achieved remarkable progress in\nmedical image analysis, but heavily rely on massive amounts of labeled training\ndata. Transfer learning from pre-trained models has been proposed as a standard\npipeline on medical image analysis to address this bottleneck. Despite their\nsuccess, the existing pre-trained models are mostly not tuned for multi-modal\nmulti-task generalization in medical domains. Specifically, their training data\nare either from non-medical domain or in single modality, failing to attend to\nthe problem of performance degradation with cross-modal transfer. Furthermore,\nthere is no effort to explicitly extract multi-level features required by a\nvariety of downstream tasks. To overcome these limitations, we propose\nUniversal Model, a transferable and generalizable pre-trained model for 3D\nmedical image analysis. A unified self-supervised learning scheme is leveraged\nto learn representations from multiple unlabeled source datasets with different\nmodalities and distinctive scan regions. A modality invariant adversarial\nlearning module is further introduced to improve the cross-modal\ngeneralization. To fit a wide range of tasks, a simple yet effective scale\nclassifier is incorporated to capture multi-level visual representations. To\nvalidate the effectiveness of the Universal Model, we perform extensive\nexperimental analysis on five target tasks, covering multiple imaging\nmodalities, distinctive scan regions, and different analysis tasks. Compared\nwith both public 3D pre-trained models and newly investigated 3D\nself-supervised learning methods, Universal Model demonstrates superior\ngeneralizability, manifested by its higher performance, stronger robustness and\nfaster convergence. The pre-trained Universal Model is available at:\n\\href{https://github.com/xm-cmic/Universal-Model}{https://github.com/xm-cmic/Universal-Model}.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 01:23:17 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Zhang", "Xiaoman", ""], ["Zhang", "Ya", ""], ["Zhang", "Xiaoyun", ""], ["Wang", "Yanfeng", ""]]}, {"id": "2010.06117", "submitter": "Bing Zha", "authors": "Bing Zha, Alper Yilmaz", "title": "Map-Based Temporally Consistent Geolocalization through Learning Motion\n  Trajectories", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel trajectory learning method that exploits\nmotion trajectories on topological map using recurrent neural network for\ntemporally consistent geolocalization of object. Inspired by human's ability to\nboth be aware of distance and direction of self-motion in navigation, our\ntrajectory learning method learns a pattern representation of trajectories\nencoded as a sequence of distances and turning angles to assist\nself-localization. We pose the learning process as a conditional sequence\nprediction problem in which each output locates the object on a traversable\npath in a map. Considering the prediction sequence ought to be topologically\nconnected in the graph-structured map, we adopt two different hypotheses\ngeneration and elimination strategies to eliminate disconnected sequence\nprediction. We demonstrate our approach on the KITTI stereo visual odometry\ndataset which is a city-scale environment and can generate trajectory with\nmetric information. The key benefits of our approach to geolocalization are\nthat 1) we take advantage of powerful sequence modeling ability of recurrent\nneural network and its robustness to noisy input, 2) only require a map in the\nform of a graph and simply use an affordable sensor that generates motion\ntrajectory and 3) do not need initial position. The experiments show that the\nmotion trajectories can be learned by training an recurrent neural network, and\ntemporally consistent geolocation can be predicted with both of the proposed\nstrategies.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 02:08:45 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Zha", "Bing", ""], ["Yilmaz", "Alper", ""]]}, {"id": "2010.06131", "submitter": "He Zhao", "authors": "He Zhao, Trung Le, Paul Montague, Olivier De Vel, Tamas Abraham, Dinh\n  Phung", "title": "Towards Understanding Pixel Vulnerability under Adversarial Attacks for\n  Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Deep neural network image classifiers are reported to be susceptible to\nadversarial evasion attacks, which use carefully crafted images created to\nmislead a classifier. Recently, various kinds of adversarial attack methods\nhave been proposed, most of which focus on adding small perturbations to all of\nthe pixels of a real image. We find that a considerable amount of the\nperturbations on an image generated by some widely-used attacks may contribute\nlittle in attacking a classifier. However, they usually result in a more easily\ndetectable adversarial image by both humans and adversarial attack detection\nalgorithms. Therefore, it is important to impose the perturbations on the most\nvulnerable pixels of an image that can change the predictions of classifiers\nmore readily. With the pixel vulnerability, given an existing attack, we can\nmake its adversarial images more realistic and less detectable with fewer\nperturbations but keep its attack performance the same. Moreover, the\ndiscovered vulnerability assists to get a better understanding of the weakness\nof deep classifiers. Derived from the information-theoretic perspective, we\npropose a probabilistic approach for automatically finding the pixel\nvulnerability of an image, which is compatible with and improves over many\nexisting adversarial attacks.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 02:51:10 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Zhao", "He", ""], ["Le", "Trung", ""], ["Montague", "Paul", ""], ["De Vel", "Olivier", ""], ["Abraham", "Tamas", ""], ["Phung", "Dinh", ""]]}, {"id": "2010.06163", "submitter": "Yichi Zhang", "authors": "Yichi Zhang, Qingcheng Liao, Jicong Zhang", "title": "Exploring Efficient Volumetric Medical Image Segmentation Using 2.5D\n  Method: An Empirical Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the unprecedented developments in deep learning, many methods are\nproposed and have achieved great success for medical image segmentation.\nHowever, unlike segmentation of natural images, most medical images such as MRI\nand CT are volumetric data. In order to make full use of volumetric\ninformation, 3D CNNs are widely used. However, 3D CNNs suffer from higher\ninference time and computation cost, which hinders their further clinical\napplications. Additionally, with the increased number of parameters, the risk\nof overfitting is higher, especially for medical images where data and\nannotations are expensive to acquire. To issue this problem, many 2.5D\nsegmentation methods have been proposed to make use of volumetric spatial\ninformation with less computation cost. Despite these works lead to\nimprovements on a variety of segmentation tasks, to the best of our knowledge,\nthere has not previously been a large-scale empirical comparison of these\nmethods. In this paper, we aim to present a review of the latest developments\nof 2.5D methods for volumetric medical image segmentation. Additionally, to\ncompare the performance and effectiveness of these methods, we provide an\nempirical study of these methods on three representative segmentation tasks\ninvolving different modalities and targets. Our experimental results highlight\nthat 3D CNNs may not always be the best choice. Besides, although all these\n2.5D methods can bring performance gains to 2D baseline, not all the methods\nhold the benefits on different datasets. We hope the results and conclusions of\nour study will prove useful for the community on exploring and developing\nefficient volumetric medical image segmentation methods.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 04:12:28 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Zhang", "Yichi", ""], ["Liao", "Qingcheng", ""], ["Zhang", "Jicong", ""]]}, {"id": "2010.06176", "submitter": "Yibo Yang", "authors": "Yibo Yang, Hongyang Li, Shan You, Fei Wang, Chen Qian, Zhouchen Lin", "title": "ISTA-NAS: Efficient and Consistent Neural Architecture Search by Sparse\n  Coding", "comments": "NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural architecture search (NAS) aims to produce the optimal sparse solution\nfrom a high-dimensional space spanned by all candidate connections. Current\ngradient-based NAS methods commonly ignore the constraint of sparsity in the\nsearch phase, but project the optimized solution onto a sparse one by\npost-processing. As a result, the dense super-net for search is inefficient to\ntrain and has a gap with the projected architecture for evaluation. In this\npaper, we formulate neural architecture search as a sparse coding problem. We\nperform the differentiable search on a compressed lower-dimensional space that\nhas the same validation loss as the original sparse solution space, and recover\nan architecture by solving the sparse coding problem. The differentiable search\nand architecture recovery are optimized in an alternate manner. By doing so,\nour network for search at each update satisfies the sparsity constraint and is\nefficient to train. In order to also eliminate the depth and width gap between\nthe network in search and the target-net in evaluation, we further propose a\nmethod to search and evaluate in one stage under the target-net settings. When\ntraining finishes, architecture variables are absorbed into network weights.\nThus we get the searched architecture and optimized parameters in a single run.\nIn experiments, our two-stage method on CIFAR-10 requires only 0.05 GPU-day for\nsearch. Our one-stage method produces state-of-the-art performances on both\nCIFAR-10 and ImageNet at the cost of only evaluation time.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 04:34:24 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Yang", "Yibo", ""], ["Li", "Hongyang", ""], ["You", "Shan", ""], ["Wang", "Fei", ""], ["Qian", "Chen", ""], ["Lin", "Zhouchen", ""]]}, {"id": "2010.06177", "submitter": "Anwaar Ulhaq Dr", "authors": "Anwaar Ulhaq, Oliver Burmeister", "title": "COVID-19 Imaging Data Privacy by Federated Learning Design: A\n  Theoretical Framework", "comments": "2 images, 0 Table,8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To address COVID-19 healthcare challenges, we need frequent sharing of health\ndata, knowledge and resources at a global scale. However, in this digital age,\ndata privacy is a big concern that requires the secure embedding of privacy\nassurance into the design of all technological solutions that use health data.\nIn this paper, we introduce differential privacy by design (dPbD) framework and\ndiscuss its embedding into the federated machine learning system. To limit the\nscope of our paper, we focus on the problem scenario of COVID-19 imaging data\nprivacy for disease diagnosis by computer vision and deep learning approaches.\nWe discuss the evaluation of the proposed design of federated machine learning\nsystems and discuss how differential privacy by design (dPbD) framework can\nenhance data privacy in federated learning systems with scalability and\nrobustness. We argue that scalable differentially private federated learning\ndesign is a promising solution for building a secure, private and collaborative\nmachine learning model such as required to combat COVID19 challenge.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 04:34:30 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Ulhaq", "Anwaar", ""], ["Burmeister", "Oliver", ""]]}, {"id": "2010.06188", "submitter": "Takayuki Nishio", "authors": "Takayuki Nishio, Yusuke Koda, Jihong Park, Mehdi Bennis, Klaus Doppler", "title": "When Wireless Communications Meet Computer Vision in Beyond 5G", "comments": "7 pages, 4 figures; This work has been submitted to IEEE\n  Communications Standards Magazine for possible publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article articulates the emerging paradigm, sitting at the confluence of\ncomputer vision and wireless communication, to enable beyond-5G/6G\nmission-critical applications (autonomous/remote-controlled vehicles,\nvisuo-haptic VR, and other cyber-physical applications). First, drawing on\nrecent advances in machine learning and the availability of non-RF data,\nvision-aided wireless networks are shown to significantly enhance the\nreliability of wireless communication without sacrificing spectral efficiency.\nIn particular, we demonstrate how computer vision enables {look-ahead}\nprediction in a millimeter-wave channel blockage scenario, before the blockage\nactually happens. From a computer vision perspective, we highlight how radio\nfrequency (RF) based sensing and imaging are instrumental in robustifying\ncomputer vision applications against occlusion and failure. This is\ncorroborated via an RF-based image reconstruction use case, showcasing a\nreceiver-side image failure correction resulting in reduced retransmission and\nlatency. Taken together, this article sheds light on the much-needed\nconvergence of RF and non-RF modalities to enable ultra-reliable communication\nand truly intelligent 6G networks.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 05:25:35 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Nishio", "Takayuki", ""], ["Koda", "Yusuke", ""], ["Park", "Jihong", ""], ["Bennis", "Mehdi", ""], ["Doppler", "Klaus", ""]]}, {"id": "2010.06201", "submitter": "Heliang Huang", "authors": "He-Liang Huang, Yuxuan Du, Ming Gong, Youwei Zhao, Yulin Wu, Chaoyue\n  Wang, Shaowei Li, Futian Liang, Jin Lin, Yu Xu, Rui Yang, Tongliang Liu,\n  Min-Hsiu Hsieh, Hui Deng, Hao Rong, Cheng-Zhi Peng, Chao-Yang Lu, Yu-Ao Chen,\n  Dacheng Tao, Xiaobo Zhu, Jian-Wei Pan", "title": "Experimental Quantum Generative Adversarial Networks for Image\n  Generation", "comments": "Our first version was submitted to the journal in January 2020.\n  Comments are welcome", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantum machine learning is expected to be one of the first practical\napplications of near-term quantum devices. Pioneer theoretical works suggest\nthat quantum generative adversarial networks (GANs) may exhibit a potential\nexponential advantage over classical GANs, thus attracting widespread\nattention. However, it remains elusive whether quantum GANs implemented on\nnear-term quantum devices can actually solve real-world learning tasks. Here,\nwe devise a flexible quantum GAN scheme to narrow this knowledge gap, which\ncould accomplish image generation with arbitrarily high-dimensional features,\nand could also take advantage of quantum superposition to train multiple\nexamples in parallel. For the first time, we experimentally achieve the\nlearning and generation of real-world hand-written digit images on a\nsuperconducting quantum processor. Moreover, we utilize a gray-scale bar\ndataset to exhibit the competitive performance between quantum GANs and the\nclassical GANs based on multilayer perceptron and convolutional neural network\narchitectures, respectively, benchmarked by the Fr\\'echet Distance score. Our\nwork provides guidance for developing advanced quantum generative models on\nnear-term quantum devices and opens up an avenue for exploring quantum\nadvantages in various GAN-related learning tasks.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 06:57:17 GMT"}, {"version": "v2", "created": "Wed, 21 Oct 2020 02:09:10 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Huang", "He-Liang", ""], ["Du", "Yuxuan", ""], ["Gong", "Ming", ""], ["Zhao", "Youwei", ""], ["Wu", "Yulin", ""], ["Wang", "Chaoyue", ""], ["Li", "Shaowei", ""], ["Liang", "Futian", ""], ["Lin", "Jin", ""], ["Xu", "Yu", ""], ["Yang", "Rui", ""], ["Liu", "Tongliang", ""], ["Hsieh", "Min-Hsiu", ""], ["Deng", "Hui", ""], ["Rong", "Hao", ""], ["Peng", "Cheng-Zhi", ""], ["Lu", "Chao-Yang", ""], ["Chen", "Yu-Ao", ""], ["Tao", "Dacheng", ""], ["Zhu", "Xiaobo", ""], ["Pan", "Jian-Wei", ""]]}, {"id": "2010.06208", "submitter": "Shujun Wang", "authors": "Shujun Wang, Lequan Yu, Kang Li, Xin Yang, Chi-Wing Fu, and Pheng-Ann\n  Heng", "title": "DoFE: Domain-oriented Feature Embedding for Generalizable Fundus Image\n  Segmentation on Unseen Datasets", "comments": "Accepted at IEEE-TMI 2020", "journal-ref": null, "doi": "10.1109/TMI.2020.3015224", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks have significantly boosted the performance\nof fundus image segmentation when test datasets have the same distribution as\nthe training datasets. However, in clinical practice, medical images often\nexhibit variations in appearance for various reasons, e.g., different scanner\nvendors and image quality. These distribution discrepancies could lead the deep\nnetworks to over-fit on the training datasets and lack generalization ability\non the unseen test datasets. To alleviate this issue, we present a novel\nDomain-oriented Feature Embedding (DoFE) framework to improve the\ngeneralization ability of CNNs on unseen target domains by exploring the\nknowledge from multiple source domains. Our DoFE framework dynamically enriches\nthe image features with additional domain prior knowledge learned from\nmulti-source domains to make the semantic features more discriminative.\nSpecifically, we introduce a Domain Knowledge Pool to learn and memorize the\nprior information extracted from multi-source domains. Then the original image\nfeatures are augmented with domain-oriented aggregated features, which are\ninduced from the knowledge pool based on the similarity between the input image\nand multi-source domain images. We further design a novel domain code\nprediction branch to infer this similarity and employ an attention-guided\nmechanism to dynamically combine the aggregated features with the semantic\nfeatures. We comprehensively evaluate our DoFE framework on two fundus image\nsegmentation tasks, including the optic cup and disc segmentation and vessel\nsegmentation. Our DoFE framework generates satisfying segmentation results on\nunseen datasets and surpasses other domain generalization and network\nregularization methods.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 07:28:39 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Wang", "Shujun", ""], ["Yu", "Lequan", ""], ["Li", "Kang", ""], ["Yang", "Xin", ""], ["Fu", "Chi-Wing", ""], ["Heng", "Pheng-Ann", ""]]}, {"id": "2010.06215", "submitter": "Congqi Cao", "authors": "Congqi Cao, Yajuan Li, Qinyi Lv, Peng Wang, Yanning Zhang", "title": "Few-shot Action Recognition with Implicit Temporal Alignment and Pair\n  Similarity Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Few-shot learning aims to recognize instances from novel classes with few\nlabeled samples, which has great value in research and application. Although\nthere has been a lot of work in this area recently, most of the existing work\nis based on image classification tasks. Video-based few-shot action recognition\nhas not been explored well and remains challenging: 1) the differences of\nimplementation details among different papers make a fair comparison difficult;\n2) the wide variations and misalignment of temporal sequences make the\nvideo-level similarity comparison difficult; 3) the scarcity of labeled data\nmakes the optimization difficult. To solve these problems, this paper presents\n1) a specific setting to evaluate the performance of few-shot action\nrecognition algorithms; 2) an implicit sequence-alignment algorithm for better\nvideo-level similarity comparison; 3) an advanced loss for few-shot learning to\noptimize pair similarity with limited data. Specifically, we propose a novel\nfew-shot action recognition framework that uses long short-term memory\nfollowing 3D convolutional layers for sequence modeling and alignment. Circle\nloss is introduced to maximize the within-class similarity and minimize the\nbetween-class similarity flexibly towards a more definite convergence target.\nInstead of using random or ambiguous experimental settings, we set a concrete\ncriterion analogous to the standard image-based few-shot learning setting for\nfew-shot action recognition evaluation. Extensive experiments on two datasets\ndemonstrate the effectiveness of our proposed method.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 07:56:06 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Cao", "Congqi", ""], ["Li", "Yajuan", ""], ["Lv", "Qinyi", ""], ["Wang", "Peng", ""], ["Zhang", "Yanning", ""]]}, {"id": "2010.06218", "submitter": "Simon Jenni", "authors": "Simon Jenni, Paolo Favaro", "title": "Self-Supervised Multi-View Synchronization Learning for 3D Pose\n  Estimation", "comments": "ACCV 2020 (oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current state-of-the-art methods cast monocular 3D human pose estimation as a\nlearning problem by training neural networks on large data sets of images and\ncorresponding skeleton poses. In contrast, we propose an approach that can\nexploit small annotated data sets by fine-tuning networks pre-trained via\nself-supervised learning on (large) unlabeled data sets. To drive such networks\ntowards supporting 3D pose estimation during the pre-training step, we\nintroduce a novel self-supervised feature learning task designed to focus on\nthe 3D structure in an image. We exploit images extracted from videos captured\nwith a multi-view camera system. The task is to classify whether two images\ndepict two views of the same scene up to a rigid transformation. In a\nmulti-view data set, where objects deform in a non-rigid manner, a rigid\ntransformation occurs only between two views taken at the exact same time,\ni.e., when they are synchronized. We demonstrate the effectiveness of the\nsynchronization task on the Human3.6M data set and achieve state-of-the-art\nresults in 3D human pose estimation.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 08:01:24 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Jenni", "Simon", ""], ["Favaro", "Paolo", ""]]}, {"id": "2010.06224", "submitter": "Shixiang Feng", "authors": "Shixiang Feng, Beibei Liu, Ya Zhang, Xiaoyun Zhang, Yuehua Li", "title": "Two-Stream Compare and Contrast Network for Vertebral Compression\n  Fracture Diagnosis", "comments": "submitted to TMI", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Differentiating Vertebral Compression Fractures (VCFs) associated with trauma\nand osteoporosis (benign VCFs) or those caused by metastatic cancer (malignant\nVCFs) are critically important for treatment decisions. So far, automatic VCFs\ndiagnosis is solved in a two-step manner, i.e. first identify VCFs and then\nclassify it into benign or malignant. In this paper, we explore to model VCFs\ndiagnosis as a three-class classification problem, i.e. normal vertebrae,\nbenign VCFs, and malignant VCFs. However, VCFs recognition and classification\nrequire very different features, and both tasks are characterized by high\nintra-class variation and high inter-class similarity. Moreover, the dataset is\nextremely class-imbalanced. To address the above challenges, we propose a novel\nTwo-Stream Compare and Contrast Network (TSCCN) for VCFs diagnosis. This\nnetwork consists of two streams, a recognition stream which learns to identify\nVCFs through comparing and contrasting between adjacent vertebra, and a\nclassification stream which compares and contrasts between intra-class and\ninter-class to learn features for fine-grained classification. The two streams\nare integrated via a learnable weight control module which adaptively sets\ntheir contribution. The TSCCN is evaluated on a dataset consisting of 239 VCFs\npatients and achieves the average sensitivity and specificity of 92.56\\% and\n96.29\\%, respectively.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 08:12:19 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Feng", "Shixiang", ""], ["Liu", "Beibei", ""], ["Zhang", "Ya", ""], ["Zhang", "Xiaoyun", ""], ["Li", "Yuehua", ""]]}, {"id": "2010.06235", "submitter": "Bin Zhang", "authors": "Qi Shen, Shengjie Zhao, Rongqing Zhang, Bin Zhang", "title": "Robust Two-Stream Multi-Feature Network for Driver Drowsiness Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Drowsiness driving is a major cause of traffic accidents and thus numerous\nprevious researches have focused on driver drowsiness detection. Many drive\nrelevant factors have been taken into consideration for fatigue detection and\ncan lead to high precision, but there are still several serious constraints,\nsuch as most existing models are environmentally susceptible. In this paper,\nfatigue detection is considered as temporal action detection problem instead of\nimage classification. The proposed detection system can be divided into four\nparts: (1) Localize the key patches of the detected driver picture which are\ncritical for fatigue detection and calculate the corresponding optical flow.\n(2) Contrast Limited Adaptive Histogram Equalization (CLAHE) is used in our\nsystem to reduce the impact of different light conditions. (3) Three individual\ntwo-stream networks combined with attention mechanism are designed for each\nfeature to extract temporal information. (4) The outputs of the three\nsub-networks will be concatenated and sent to the fully-connected network,\nwhich judges the status of the driver. The drowsiness detection system is\ntrained and evaluated on the famous Nation Tsing Hua University Driver\nDrowsiness Detection (NTHU-DDD) dataset and we obtain an accuracy of 94.46%,\nwhich outperforms most existing fatigue detection models.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 08:49:35 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Shen", "Qi", ""], ["Zhao", "Shengjie", ""], ["Zhang", "Rongqing", ""], ["Zhang", "Bin", ""]]}, {"id": "2010.06245", "submitter": "Benjamin Berkels", "authors": "Johanna C. Clauser, Judith Maas, Jutta Arens, Thomas Schmitz-Rode,\n  Ulrich Steinseifer, Benjamin Berkels", "title": "Automation of Hemocompatibility Analysis Using Image Segmentation and a\n  Random Forest", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The hemocompatibility of blood-contacting medical devices remains one of the\nmajor challenges in biomedical engineering and makes research in the field of\nnew and improved materials inevitable. However, current in-vitro test and\nanalysis methods are still lacking standardization and comparability, which\nimpedes advances in material design. For example, the optical platelet analysis\nof material in-vitro hemocompatibility tests is carried out manually or\nsemi-manually by each research group individually.\n  As a step towards standardization, this paper proposes an automation approach\nfor the optical platelet count and analysis. To this end, fluorescence images\nare segmented using Zach's convexification of the multiphase-phase piecewise\nconstant Mumford--Shah model. The resulting connected components of the\nnon-background segments then need to be classified as platelet or no platelet.\nTherefore, a supervised random forest is applied to feature vectors derived\nfrom the components using features like area, perimeter and circularity. With\nan overall high accuracy and low error rates, the random forest achieves\nreliable results. This is supported by high areas under the receiver-operator\nand the prediction-recall curve, respectively.\n  We developed a new method for a fast, user-independent and reproducible\nanalysis of material hemocompatibility tests, which is therefore a unique and\npowerful tool for advances in biomaterial research.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 09:13:00 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Clauser", "Johanna C.", ""], ["Maas", "Judith", ""], ["Arens", "Jutta", ""], ["Schmitz-Rode", "Thomas", ""], ["Steinseifer", "Ulrich", ""], ["Berkels", "Benjamin", ""]]}, {"id": "2010.06255", "submitter": "Fangqiang Ding", "authors": "Changhong Fu, Bowen Li, Fangqiang Ding, Fuling Lin and Geng Lu", "title": "Correlation Filters for Unmanned Aerial Vehicle-Based Aerial Tracking: A\n  Review and Experimental Evaluation", "comments": "28 pages, 10 figures, submitted to GRSM", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aerial tracking, which has exhibited its omnipresent dedication and splendid\nperformance, is one of the most active applications in the remote sensing\nfield. Especially, unmanned aerial vehicle (UAV)-based remote sensing system,\nequipped with a visual tracking approach, has been widely used in aviation,\nnavigation, agriculture,transportation, and public security, etc. As is\nmentioned above, the UAV-based aerial tracking platform has been gradually\ndeveloped from research to practical application stage, reaching one of the\nmain aerial remote sensing technologies in the future. However, due to the\nreal-world onerous situations, e.g., harsh external challenges, the vibration\nof the UAV mechanical structure (especially under strong wind conditions), the\nmaneuvering flight in complex environment, and the limited computation\nresources onboard, accuracy, robustness, and high efficiency are all crucial\nfor the onboard tracking methods. Recently, the discriminative correlation\nfilter (DCF)-based trackers have stood out for their high computational\nefficiency and appealing robustness on a single CPU, and have flourished in the\nUAV visual tracking community. In this work, the basic framework of the\nDCF-based trackers is firstly generalized, based on which, 23 state-of-the-art\nDCF-based trackers are orderly summarized according to their innovations for\nsolving various issues. Besides, exhaustive and quantitative experiments have\nbeen extended on various prevailing UAV tracking benchmarks, i.e., UAV123,\nUAV123@10fps, UAV20L, UAVDT, DTB70, and VisDrone2019-SOT, which contain 371,903\nframes in total. The experiments show the performance, verify the feasibility,\nand demonstrate the current challenges of DCF-based trackers onboard UAV\ntracking.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 09:35:40 GMT"}, {"version": "v2", "created": "Thu, 22 Oct 2020 07:30:40 GMT"}, {"version": "v3", "created": "Sat, 31 Oct 2020 17:32:00 GMT"}, {"version": "v4", "created": "Tue, 16 Feb 2021 15:24:00 GMT"}, {"version": "v5", "created": "Sat, 5 Jun 2021 04:52:11 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Fu", "Changhong", ""], ["Li", "Bowen", ""], ["Ding", "Fangqiang", ""], ["Lin", "Fuling", ""], ["Lu", "Geng", ""]]}, {"id": "2010.06260", "submitter": "Cristian Rodriguez", "authors": "Cristian Rodriguez-Opazo and Edison Marrese-Taylor and Basura Fernando\n  and Hongdong Li and Stephen Gould", "title": "DORi: Discovering Object Relationship for Moment Localization of a\n  Natural-Language Query in Video", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the task of temporal moment localization in a long\nuntrimmed video using natural language query. Given a query sentence, the goal\nis to determine the start and end of the relevant segment within the video. Our\nkey innovation is to learn a video feature embedding through a\nlanguage-conditioned message-passing algorithm suitable for temporal moment\nlocalization which captures the relationships between humans, objects and\nactivities in the video. These relationships are obtained by a spatial\nsub-graph that contextualizes the scene representation using detected objects\nand human features conditioned in the language query. Moreover, a temporal\nsub-graph captures the activities within the video through time. Our method is\nevaluated on three standard benchmark datasets, and we also introduce YouCookII\nas a new benchmark for this task. Experiments show our method outperforms\nstate-of-the-art methods on these datasets, confirming the effectiveness of our\napproach.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 09:50:29 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Rodriguez-Opazo", "Cristian", ""], ["Marrese-Taylor", "Edison", ""], ["Fernando", "Basura", ""], ["Li", "Hongdong", ""], ["Gould", "Stephen", ""]]}, {"id": "2010.06264", "submitter": "Thanh Hong Phuoc", "authors": "Thanh Hong-Phuoc and Ling Guan", "title": "A Scale and Rotational Invariant Key-point Detector based on Sparse\n  Coding", "comments": "A novel scale and rotational invariant key-point detector was\n  proposed", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most popular hand-crafted key-point detectors such as Harris corner, SIFT,\nSURF aim to detect corners, blobs, junctions or other human defined structures\nin images. Though being robust with some geometric transformations, unintended\nscenarios or non-uniform lighting variations could significantly degrade their\nperformance. Hence, a new detector that is flexible with context change and\nsimultaneously robust with both geometric and non-uniform illumination\nvariations is very desirable. In this paper, we propose a solution to this\nchallenging problem by incorporating Scale and Rotation Invariant design (named\nSRI-SCK) into a recently developed Sparse Coding based Key-point detector\n(SCK). The SCK detector is flexible in different scenarios and fully invariant\nto affine intensity change, yet it is not designed to handle images with\ndrastic scale and rotation changes. In SRI-SCK, the scale invariance is\nimplemented with an image pyramid technique while the rotation invariance is\nrealized by combining multiple rotated versions of the dictionary used in the\nsparse coding step of SCK. Techniques for calculation of key-points'\ncharacteristic scales and their sub-pixel accuracy positions are also proposed.\nExperimental results on three public datasets demonstrate that significantly\nhigh repeatability and matching score are achieved.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 10:08:12 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Hong-Phuoc", "Thanh", ""], ["Guan", "Ling", ""]]}, {"id": "2010.06285", "submitter": "Eleni Charou Dr", "authors": "Vasilis Pollatos, Loukas Kouvaras and Eleni Charou", "title": "Land Cover Semantic Segmentation Using ResUNet", "comments": "21 pages , 17 figures, presented in AI in Natural Science and\n  Technology (https://ainst.scify.org/) Workshop of SETN 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper we present our work on developing an automated system for land\ncover classification. This system takes a multiband satellite image of an area\nas input and outputs the land cover map of the area at the same resolution as\nthe input. For this purpose convolutional machine learning models were trained\nin the task of predicting the land cover semantic segmentation of satellite\nimages. This is a case of supervised learning. The land cover label data were\ntaken from the CORINE Land Cover inventory and the satellite images were taken\nfrom the Copernicus hub. As for the model, U-Net architecture variations were\napplied. Our area of interest are the Ionian islands (Greece). We created a\ndataset from scratch covering this particular area. In addition, transfer\nlearning from the BigEarthNet dataset [1] was performed. In [1] simple\nclassification of satellite images into the classes of CLC is performed but not\nsegmentation as we do. However, their models have been trained into a dataset\nmuch bigger than ours, so we applied transfer learning using their pretrained\nmodels as the first part of out network, utilizing the ability these networks\nhave developed to extract useful features from the satellite images (we\ntransferred a pretrained ResNet50 into a U-Res-Net). Apart from transfer\nlearning other techniques were applied in order to overcome the limitations set\nby the small size of our area of interest. We used data augmentation (cutting\nimages into overlapping patches, applying random transformations such as\nrotations and flips) and cross validation. The results are tested on the 3 CLC\nclass hierarchy levels and a comparative study is made on the results of\ndifferent approaches.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 10:56:09 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Pollatos", "Vasilis", ""], ["Kouvaras", "Loukas", ""], ["Charou", "Eleni", ""]]}, {"id": "2010.06291", "submitter": "Delia Velasco-Montero", "authors": "Th\\'eo Benoit-Cattin, Delia Velasco-Montero and Jorge\n  Fern\\'andez-Berni", "title": "Impact of Thermal Throttling on Long-Term Visual Inference in a\n  CPU-based Edge Device", "comments": "14 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many application scenarios of edge visual inference, e.g., robotics or\nenvironmental monitoring, eventually require long periods of continuous\noperation. In such periods, the processor temperature plays a critical role to\nkeep a prescribed frame rate. Particularly, the heavy computational load of\nconvolutional neural networks (CNNs) may lead to thermal throttling and hence\nperformance degradation in few seconds. In this paper, we report and analyze\nthe long-term performance of 80 different cases resulting from running 5 CNN\nmodels on 4 software frameworks and 2 operating systems without and with active\ncooling. This comprehensive study was conducted on a low-cost edge platform,\nnamely Raspberry Pi 4B (RPi4B), under stable indoor conditions. The results\nshow that hysteresis-based active cooling prevented thermal throttling in all\ncases, thereby improving the throughput up to approximately 90% versus no\ncooling. Interestingly, the range of fan usage during active cooling varied\nfrom 33% to 65%. Given the impact of the fan on the power consumption of the\nsystem as a whole, these results stress the importance of a suitable selection\nof CNN model and software components. To assess the performance in outdoor\napplications, we integrated an external temperature sensor with the RPi4B and\nconducted a set of experiments with no active cooling in a wide interval of\nambient temperature, ranging from 22 {\\deg}C to 36 {\\deg}C. Variations up to\n27.7% were measured with respect to the maximum throughput achieved in that\ninterval. This demonstrates that ambient temperature is a critical parameter in\ncase active cooling cannot be applied.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 11:15:17 GMT"}, {"version": "v2", "created": "Thu, 3 Dec 2020 10:37:01 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Benoit-Cattin", "Th\u00e9o", ""], ["Velasco-Montero", "Delia", ""], ["Fern\u00e1ndez-Berni", "Jorge", ""]]}, {"id": "2010.06300", "submitter": "Sungnyun Kim", "authors": "Sungnyun Kim, Gihun Lee, Sangmin Bae, Se-Young Yun", "title": "MixCo: Mix-up Contrastive Learning for Visual Representation", "comments": "accepted in NeurIPS 2020 Workshop on Self-Supervised Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contrastive learning has shown remarkable results in recent self-supervised\napproaches for visual representation. By learning to contrast positive pairs'\nrepresentation from the corresponding negatives pairs, one can train good\nvisual representations without human annotations. This paper proposes Mix-up\nContrast (MixCo), which extends the contrastive learning concept to\nsemi-positives encoded from the mix-up of positive and negative images. MixCo\naims to learn the relative similarity of representations, reflecting how much\nthe mixed images have the original positives. We validate the efficacy of MixCo\nwhen applied to the recent self-supervised learning algorithms under the\nstandard linear evaluation protocol on TinyImageNet, CIFAR10, and CIFAR100. In\nthe experiments, MixCo consistently improves test accuracy. Remarkably, the\nimprovement is more significant when the learning capacity (e.g., model size)\nis limited, suggesting that MixCo might be more useful in real-world scenarios.\nThe code is available at: https://github.com/Lee-Gihun/MixCo-Mixup-Contrast.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 11:34:25 GMT"}, {"version": "v2", "created": "Sun, 15 Nov 2020 08:29:07 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Kim", "Sungnyun", ""], ["Lee", "Gihun", ""], ["Bae", "Sangmin", ""], ["Yun", "Se-Young", ""]]}, {"id": "2010.06307", "submitter": "Julia Dietlmeier", "authors": "Julia Dietlmeier, Joseph Antony, Kevin McGuinness, Noel E. O'Connor", "title": "How important are faces for person re-identification?", "comments": "25th International Conference on Pattern Recognition (ICPR2020),\n  Milan, Italy, 10-15 January 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the dependence of existing state-of-the-art person\nre-identification models on the presence and visibility of human faces. We\napply a face detection and blurring algorithm to create anonymized versions of\nseveral popular person re-identification datasets including Market1501,\nDukeMTMC-reID, CUHK03, Viper, and Airport. Using a cross-section of existing\nstate-of-the-art models that range in accuracy and computational efficiency, we\nevaluate the effect of this anonymization on re-identification performance\nusing standard metrics. Perhaps surprisingly, the effect on mAP is very small,\nand accuracy is recovered by simply training on the anonymized versions of the\ndata rather than the original data. These findings are consistent across\nmultiple models and datasets. These results indicate that datasets can be\nsafely anonymized by blurring faces without significantly impacting the\nperformance of person reidentification systems, and may allow for the release\nof new richer re-identification datasets where previously there were privacy or\ndata protection concerns.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 11:47:16 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Dietlmeier", "Julia", ""], ["Antony", "Joseph", ""], ["McGuinness", "Kevin", ""], ["O'Connor", "Noel E.", ""]]}, {"id": "2010.06318", "submitter": "Akiyoshi Kurobe", "authors": "Akiyoshi Kurobe, Yoshikatsu Nakajima, Hideo Saito, Kris Kitani", "title": "Audio-Visual Self-Supervised Terrain Type Discovery for Mobile Platforms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to both recognize and discover terrain characteristics is an\nimportant function required for many autonomous ground robots such as social\nrobots, assistive robots, autonomous vehicles, and ground exploration robots.\nRecognizing and discovering terrain characteristics is challenging because\nsimilar terrains may have very different appearances (e.g., carpet comes in\nmany colors), while terrains with very similar appearance may have very\ndifferent physical properties (e.g. mulch versus dirt). In order to address the\ninherent ambiguity in vision-based terrain recognition and discovery, we\npropose a multi-modal self-supervised learning technique that switches between\naudio features extracted from a mic attached to the underside of a mobile\nplatform and image features extracted by a camera on the platform to cluster\nterrain types. The terrain cluster labels are then used to train an image-based\nconvolutional neural network to predict changes in terrain types. Through\nexperiments, we demonstrate that the proposed self-supervised terrain type\ndiscovery method achieves over 80% accuracy, which greatly outperforms several\nbaselines and suggests strong potential for assistive applications.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 11:56:48 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Kurobe", "Akiyoshi", ""], ["Nakajima", "Yoshikatsu", ""], ["Saito", "Hideo", ""], ["Kitani", "Kris", ""]]}, {"id": "2010.06323", "submitter": "Patrick Wenzel", "authors": "Lukas von Stumberg, Patrick Wenzel, Nan Yang, Daniel Cremers", "title": "LM-Reloc: Levenberg-Marquardt Based Direct Visual Relocalization", "comments": "International Conference on 3D Vision (3DV), 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present LM-Reloc -- a novel approach for visual relocalization based on\ndirect image alignment. In contrast to prior works that tackle the problem with\na feature-based formulation, the proposed method does not rely on feature\nmatching and RANSAC. Hence, the method can utilize not only corners but any\nregion of the image with gradients. In particular, we propose a loss\nformulation inspired by the classical Levenberg-Marquardt algorithm to train\nLM-Net. The learned features significantly improve the robustness of direct\nimage alignment, especially for relocalization across different conditions. To\nfurther improve the robustness of LM-Net against large image baselines, we\npropose a pose estimation network, CorrPoseNet, which regresses the relative\npose to bootstrap the direct image alignment. Evaluations on the CARLA and\nOxford RobotCar relocalization tracking benchmark show that our approach\ndelivers more accurate results than previous state-of-the-art methods while\nbeing comparable in terms of robustness.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 12:15:20 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["von Stumberg", "Lukas", ""], ["Wenzel", "Patrick", ""], ["Yang", "Nan", ""], ["Cremers", "Daniel", ""]]}, {"id": "2010.06349", "submitter": "Zongxin Yang", "authors": "Zongxin Yang, Yunchao Wei, Yi Yang", "title": "Collaborative Video Object Segmentation by Multi-Scale\n  Foreground-Background Integration", "comments": "Accepted by TPAMI; Journal extension of arXiv:2003.08333 (ECCV 2020,\n  Spotlight)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the principles of embedding learning to tackle the\nchallenging semi-supervised video object segmentation. Unlike previous\npractices that focus on exploring the embedding learning of foreground object\n(s), we consider background should be equally treated. Thus, we propose a\nCollaborative video object segmentation by Foreground-Background Integration\n(CFBI) approach. CFBI separates the feature embedding into the foreground\nobject region and its corresponding background region, implicitly promoting\nthem to be more contrastive and improving the segmentation results accordingly.\nMoreover, CFBI performs both pixel-level matching processes and instance-level\nattention mechanisms between the reference and the predicted sequence, making\nCFBI robust to various object scales. Based on CFBI, we introduce a multi-scale\nmatching structure and propose an Atrous Matching strategy, resulting in a more\nrobust and efficient framework, CFBI+. We conduct extensive experiments on two\npopular benchmarks, i.e., DAVIS and YouTube-VOS. Without applying any simulated\ndata for pre-training, our CFBI+ achieves the performance (J&F) of 82.9% and\n82.8%, outperforming all the other state-of-the-art methods. Code:\nhttps://github.com/z-x-yang/CFBI.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 13:06:10 GMT"}, {"version": "v2", "created": "Sun, 16 May 2021 11:21:08 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Yang", "Zongxin", ""], ["Wei", "Yunchao", ""], ["Yang", "Yi", ""]]}, {"id": "2010.06362", "submitter": "Jinting Wu", "authors": "Jinting Wu, Yujia Zhang, Xiaoguang Zhao and Wenbin Gao", "title": "A Generalized Zero-Shot Framework for Emotion Recognition from Body\n  Gestures", "comments": "The new version adds a co-author and revises the layout of Fig.3", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although automatic emotion recognition from facial expressions and speech has\nmade remarkable progress, emotion recognition from body gestures has not been\nthoroughly explored. People often use a variety of body language to express\nemotions, and it is difficult to enumerate all emotional body gestures and\ncollect enough samples for each category. Therefore, recognizing new emotional\nbody gestures is critical for better understanding human emotions. However, the\nexisting methods fail to accurately determine which emotional state a new body\ngesture belongs to. In order to solve this problem, we introduce a Generalized\nZero-Shot Learning (GZSL) framework, which consists of three branches to infer\nthe emotional state of the new body gestures with only their semantic\ndescriptions. The first branch is a Prototype-Based Detector (PBD) which is\nused to determine whether an sample belongs to a seen body gesture category and\nobtain the prediction results of the samples from the seen categories. The\nsecond branch is a Stacked AutoEncoder (StAE) with manifold regularization,\nwhich utilizes semantic representations to predict samples from unseen\ncategories. Note that both of the above branches are for body gesture\nrecognition. We further add an emotion classifier with a softmax layer as the\nthird branch in order to better learn the feature representations for this\nemotion classification task. The input features for these three branches are\nlearned by a shared feature extraction network, i.e., a Bidirectional Long\nShort-Term Memory Networks (BLSTM) with a self-attention module. We treat these\nthree branches as subtasks and use multi-task learning strategies for joint\ntraining. The performance of our framework on an emotion recognition dataset is\nsignificantly superior to the traditional method of emotion classification and\nstate-of-the-art zero-shot learning methods.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 13:16:38 GMT"}, {"version": "v2", "created": "Tue, 20 Oct 2020 08:15:45 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Wu", "Jinting", ""], ["Zhang", "Yujia", ""], ["Zhao", "Xiaoguang", ""], ["Gao", "Wenbin", ""]]}, {"id": "2010.06363", "submitter": "Tong Wu", "authors": "Jianrong Wang and Tong Wu and Shanyu Wang and Mei Yu and Qiang Fang\n  and Ju Zhang and Li Liu", "title": "Three-Dimensional Lip Motion Network for Text-Independent Speaker\n  Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lip motion reflects behavior characteristics of speakers, and thus can be\nused as a new kind of biometrics in speaker recognition. In the literature,\nlots of works used two-dimensional (2D) lip images to recognize speaker in a\ntextdependent context. However, 2D lip easily suffers from various face\norientations. To this end, in this work, we present a novel end-to-end 3D lip\nmotion Network (3LMNet) by utilizing the sentence-level 3D lip motion (S3DLM)\nto recognize speakers in both the text-independent and text-dependent contexts.\nA new regional feedback module (RFM) is proposed to obtain attentions in\ndifferent lip regions. Besides, prior knowledge of lip motion is investigated\nto complement RFM, where landmark-level and frame-level features are merged to\nform a better feature representation. Moreover, we present two methods, i.e.,\ncoordinate transformation and face posture correction to pre-process the LSD-AV\ndataset, which contains 68 speakers and 146 sentences per speaker. The\nevaluation results on this dataset demonstrate that our proposed 3LMNet is\nsuperior to the baseline models, i.e., LSTM, VGG-16 and ResNet-34, and\noutperforms the state-of-the-art using 2D lip image as well as the 3D face. The\ncode of this work is released at\nhttps://github.com/wutong18/Three-Dimensional-Lip-\nMotion-Network-for-Text-Independent-Speaker-Recognition.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 13:18:33 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Wang", "Jianrong", ""], ["Wu", "Tong", ""], ["Wang", "Shanyu", ""], ["Yu", "Mei", ""], ["Fang", "Qiang", ""], ["Zhang", "Ju", ""], ["Liu", "Li", ""]]}, {"id": "2010.06374", "submitter": "Raphael Mitteau", "authors": "R Mitteau (IRFM), J Spruytte (IRFM), S Vallet (IRFM), J Trav\\`ere\n  (IRFM), D Guilhem (IRFM), C Brosset (IRFM)", "title": "A Possible Method of Carbon Deposit Mapping on Plasma Facing Components\n  Using Infrared Thermography", "comments": "Journal of Nuclear Materials, Elsevier, 2007", "journal-ref": null, "doi": "10.1016/j.jnucmat.2007.01.00", "report-no": null, "categories": "physics.ins-det cs.CV physics.class-ph physics.plasm-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The material eroded from the surface of plasma facing components is\nredeposited partly close to high heat flux areas. At these locations, the\ndeposit is heated by the plasma and the deposition pattern evolves depending on\nthe operation parameters. The mapping of the deposit is still a matter of\nintense scientific activity, especially during the course of experimental\ncampaigns. A method based on the comparison of surface temperature maps,\nobtained in situ by infrared cameras and by theoretical modelling is proposed.\nThe difference between the two is attributed to the thermal resistance added by\ndeposited material, and expressed as a deposit thickness. The method benefits\nof elaborated imaging techniques such as possibility theory and fuzzy logics.\nThe results are consistent with deposit maps obtained by visual inspection\nduring shutdowns.\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2020 12:04:50 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Mitteau", "R", "", "IRFM"], ["Spruytte", "J", "", "IRFM"], ["Vallet", "S", "", "IRFM"], ["Trav\u00e8re", "J", "", "IRFM"], ["Guilhem", "D", "", "IRFM"], ["Brosset", "C", "", "IRFM"]]}, {"id": "2010.06379", "submitter": "Jingfei Chang", "authors": "Jingfei Chang", "title": "Coarse and fine-grained automatic cropping deep convolutional neural\n  network", "comments": "12 pages, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The existing convolutional neural network pruning algorithms can be divided\ninto two categories: coarse-grained clipping and fine-grained clipping. This\npaper proposes a coarse and fine-grained automatic pruning algorithm, which can\nachieve more efficient and accurate compression acceleration for convolutional\nneural networks. First, cluster the intermediate feature maps of the\nconvolutional neural network to obtain the network structure after\ncoarse-grained clipping, and then use the particle swarm optimization algorithm\nto iteratively search and optimize the structure. Finally, the optimal network\ntailoring substructure is obtained.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 13:36:33 GMT"}, {"version": "v2", "created": "Wed, 14 Oct 2020 02:42:31 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Chang", "Jingfei", ""]]}, {"id": "2010.06402", "submitter": "Cedric Renggli", "authors": "Cedric Renggli, Andr\\'e Susano Pinto, Luka Rimanic, Joan Puigcerver,\n  Carlos Riquelme, Ce Zhang, Mario Lucic", "title": "Which Model to Transfer? Finding the Needle in the Growing Haystack", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transfer learning has been recently popularized as a data-efficient\nalternative to training models from scratch, in particular in vision and NLP\nwhere it provides a remarkably solid baseline. The emergence of rich model\nrepositories, such as TensorFlow Hub, enables the practitioners and researchers\nto unleash the potential of these models across a wide range of downstream\ntasks. As these repositories keep growing exponentially, efficiently selecting\na good model for the task at hand becomes paramount. We provide a formalization\nof this problem through a familiar notion of regret and introduce the\npredominant strategies, namely task-agnostic (e.g. picking the highest scoring\nImageNet model) and task-aware search strategies (such as linear or kNN\nevaluation). We conduct a large-scale empirical study and show that both\ntask-agnostic and task-aware methods can yield high regret. We then propose a\nsimple and computationally efficient hybrid search strategy which outperforms\nthe existing approaches. We highlight the practical benefits of the proposed\nsolution on a set of 19 diverse vision tasks.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 14:00:22 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Renggli", "Cedric", ""], ["Pinto", "Andr\u00e9 Susano", ""], ["Rimanic", "Luka", ""], ["Puigcerver", "Joan", ""], ["Riquelme", "Carlos", ""], ["Zhang", "Ce", ""], ["Lucic", "Mario", ""]]}, {"id": "2010.06407", "submitter": "Giulia Orr\\`u", "authors": "Giulia Orr\\`u, Davide Ghiani, Maura Pintor, Gian Luca Marcialis, Fabio\n  Roli", "title": "Detecting Anomalies from Video-Sequences: a Novel Descriptor", "comments": "Accepted for the 25th International Conference on Pattern Recognition\n  (ICPR 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel descriptor for crowd behavior analysis and anomaly\ndetection. The goal is to measure by appropriate patterns the speed of\nformation and disintegration of groups in the crowd. This descriptor is\ninspired by the concept of one-dimensional local binary patterns: in our case,\nsuch patterns depend on the number of group observed in a time window. An\nappropriate measurement unit, named \"trit\" (trinary digit), represents three\npossible dynamic states of groups on a certain frame. Our hypothesis is that\nabrupt variations of the groups' number may be due to an anomalous event that\ncan be accordingly detected, by translating these variations on temporal\ntrit-based sequence of strings which are significantly different from the one\ndescribing the \"no-anomaly\" one. Due to the peculiarity of the rationale behind\nthis work, relying on the number of groups, three different methods of people\ngroup's extraction are compared. Experiments are carried out on the\nMotion-Emotion benchmark data set. Reported results point out in which cases\nthe trit-based measurement of group dynamics allows us to detect the anomaly.\nBesides the promising performance of our approach, we show how it is correlated\nwith the anomaly typology and the camera's perspective to the crowd's flow\n(frontal, lateral).\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 14:07:43 GMT"}, {"version": "v2", "created": "Sat, 17 Oct 2020 14:40:54 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Orr\u00f9", "Giulia", ""], ["Ghiani", "Davide", ""], ["Pintor", "Maura", ""], ["Marcialis", "Gian Luca", ""], ["Roli", "Fabio", ""]]}, {"id": "2010.06412", "submitter": "Giulia Orr\\`u", "authors": "Giulia Orr\\`u, Marco Micheletto, Fabio Terranova, Gian Luca Marcialis", "title": "Electroencephalography signal processing based on textural features for\n  monitoring the driver's state by a Brain-Computer Interface", "comments": "Accepted for the 25th International Conference on Pattern Recognition\n  (ICPR 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study we investigate a textural processing method of\nelectroencephalography (EEG) signal as an indicator to estimate the driver's\nvigilance in a hypothetical Brain-Computer Interface (BCI) system. The novelty\nof the solution proposed relies on employing the one-dimensional Local Binary\nPattern (1D-LBP) algorithm for feature extraction from pre-processed EEG data.\nFrom the resulting feature vector, the classification is done according to\nthree vigilance classes: awake, tired and drowsy. The claim is that the class\ntransitions can be detected by describing the variations of the micro-patterns'\noccurrences along the EEG signal. The 1D-LBP is able to describe them by\ndetecting mutual variations of the signal temporarily \"close\" as a short\nbit-code. Our analysis allows to conclude that the 1D-LBP adoption has led to\nsignificant performance improvement. Moreover, capturing the class transitions\nfrom the EEG signal is effective, although the overall performance is not yet\ngood enough to develop a BCI for assessing the driver's vigilance in real\nenvironments.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 14:16:00 GMT"}, {"version": "v2", "created": "Sat, 17 Oct 2020 14:46:00 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Orr\u00f9", "Giulia", ""], ["Micheletto", "Marco", ""], ["Terranova", "Fabio", ""], ["Marcialis", "Gian Luca", ""]]}, {"id": "2010.06418", "submitter": "Farzad Khalvati", "authors": "Saman Motamed, Patrik Rogalla, Farzad Khalvati", "title": "RANDGAN: Randomized Generative Adversarial Network for Detection of\n  COVID-19 in Chest X-ray", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  COVID-19 spread across the globe at an immense rate has left healthcare\nsystems incapacitated to diagnose and test patients at the needed rate. Studies\nhave shown promising results for detection of COVID-19 from viral bacterial\npneumonia in chest X-rays. Automation of COVID-19 testing using medical images\ncan speed up the testing process of patients where health care systems lack\nsufficient numbers of the reverse-transcription polymerase chain reaction\n(RT-PCR) tests. Supervised deep learning models such as convolutional neural\nnetworks (CNN) need enough labeled data for all classes to correctly learn the\ntask of detection. Gathering labeled data is a cumbersome task and requires\ntime and resources which could further strain health care systems and\nradiologists at the early stages of a pandemic such as COVID-19. In this study,\nwe propose a randomized generative adversarial network (RANDGAN) that detects\nimages of an unknown class (COVID-19) from known and labelled classes (Normal\nand Viral Pneumonia) without the need for labels and training data from the\nunknown class of images (COVID-19). We used the largest publicly available\nCOVID-19 chest X-ray dataset, COVIDx, which is comprised of Normal, Pneumonia,\nand COVID-19 images from multiple public databases. In this work, we use\ntransfer learning to segment the lungs in the COVIDx dataset. Next, we show why\nsegmentation of the region of interest (lungs) is vital to correctly learn the\ntask of classification, specifically in datasets that contain images from\ndifferent resources as it is the case for the COVIDx dataset. Finally, we show\nimproved results in detection of COVID-19 cases using our generative model\n(RANDGAN) compared to conventional generative adversarial networks (GANs) for\nanomaly detection in medical images, improving the area under the ROC curve\nfrom 0.71 to 0.77.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 15:58:09 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Motamed", "Saman", ""], ["Rogalla", "Patrik", ""], ["Khalvati", "Farzad", ""]]}, {"id": "2010.06421", "submitter": "Menghan Hu", "authors": "Yuzhen Chen, Menghan Hu, Chunjun Hua, Guangtao Zhai, Jian Zhang,\n  Qingli Li, Simon X. Yang", "title": "Face Mask Assistant: Detection of Face Mask Service Stage Based on\n  Mobile Phone", "comments": "11 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coronavirus Disease 2019 (COVID-19) has spread all over the world since it\nbroke out massively in December 2019, which has caused a large loss to the\nwhole world. Both the confirmed cases and death cases have reached a relatively\nfrightening number. Syndrome coronaviruses 2 (SARS-CoV-2), the cause of\nCOVID-19, can be transmitted by small respiratory droplets. To curb its spread\nat the source, wearing masks is a convenient and effective measure. In most\ncases, people use face masks in a high-frequent but short-time way. Aimed at\nsolving the problem that we don't know which service stage of the mask belongs\nto, we propose a detection system based on the mobile phone. We first extract\nfour features from the GLCMs of the face mask's micro-photos. Next, a\nthree-result detection system is accomplished by using KNN algorithm. The\nresults of validation experiments show that our system can reach a precision of\n82.87% (standard deviation=8.5%) on the testing dataset. In future work, we\nplan to expand the detection objects to more mask types. This work demonstrates\nthat the proposed mobile microscope system can be used as an assistant for face\nmask being used, which may play a positive role in fighting against COVID-19.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 08:49:52 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Chen", "Yuzhen", ""], ["Hu", "Menghan", ""], ["Hua", "Chunjun", ""], ["Zhai", "Guangtao", ""], ["Zhang", "Jian", ""], ["Li", "Qingli", ""], ["Yang", "Simon X.", ""]]}, {"id": "2010.06440", "submitter": "Shujun Wang", "authors": "Shujun Wang, Yaxi Zhu, Lequan Yu, Hao Chen, Huangjing Lin, Xiangbo\n  Wan, Xinjuan Fan, and Pheng-Ann Hen", "title": "RMDL: Recalibrated multi-instance deep learning for whole slide gastric\n  image classification", "comments": "Accepted at Medical Image Analysis. Code:\n  https://github.com/EmmaW8/RMDL", "journal-ref": null, "doi": "10.1016/j.media.2019.101549", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The whole slide histopathology images (WSIs) play a critical role in gastric\ncancer diagnosis. However, due to the large scale of WSIs and various sizes of\nthe abnormal area, how to select informative regions and analyze them are quite\nchallenging during the automatic diagnosis process. The multi-instance learning\nbased on the most discriminative instances can be of great benefit for whole\nslide gastric image diagnosis. In this paper, we design a recalibrated\nmulti-instance deep learning method (RMDL) to address this challenging problem.\nWe first select the discriminative instances, and then utilize these instances\nto diagnose diseases based on the proposed RMDL approach. The designed RMDL\nnetwork is capable of capturing instance-wise dependencies and recalibrating\ninstance features according to the importance coefficient learned from the\nfused features. Furthermore, we build a large whole-slide gastric\nhistopathology image dataset with detailed pixel-level annotations.\nExperimental results on the constructed gastric dataset demonstrate the\nsignificant improvement on the accuracy of our proposed framework compared with\nother state-of-the-art multi-instance learning methods. Moreover, our method is\ngeneral and can be extended to other diagnosis tasks of different cancer types\nbased on WSIs.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 14:55:47 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Wang", "Shujun", ""], ["Zhu", "Yaxi", ""], ["Yu", "Lequan", ""], ["Chen", "Hao", ""], ["Lin", "Huangjing", ""], ["Wan", "Xiangbo", ""], ["Fan", "Xinjuan", ""], ["Hen", "Pheng-Ann", ""]]}, {"id": "2010.06449", "submitter": "Yann Desmarais", "authors": "Yann Desmarais, Denis Mottet, Pierre Slangen, Philippe Montesinos", "title": "A review of 3D human pose estimation algorithms for markerless motion\n  capture", "comments": "25 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Human pose estimation is a very active research field, stimulated by its\nimportant applications in robotics, entertainment or health and sports\nsciences, among others. Advances in convolutional networks triggered noticeable\nimprovements in 2D pose estimation, leading modern 3D markerless motion capture\ntechniques to an average error per joint of 20 mm. However, with the\nproliferation of methods, it is becoming increasingly difficult to make an\ninformed choice. Here, we review the leading human pose estimation methods of\nthe past five years, focusing on metrics, benchmarks and method structures. We\npropose a taxonomy based on accuracy, speed and robustness that we use to\nclassify de methods and derive directions for future research.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 15:07:01 GMT"}, {"version": "v2", "created": "Mon, 19 Oct 2020 15:30:14 GMT"}, {"version": "v3", "created": "Mon, 12 Jul 2021 17:07:05 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Desmarais", "Yann", ""], ["Mottet", "Denis", ""], ["Slangen", "Pierre", ""], ["Montesinos", "Philippe", ""]]}, {"id": "2010.06453", "submitter": "Tarik Ayaou", "authors": "Tarik Ayaou, Mourad Boussaid, Karim Afdel, Abdellah Amghar", "title": "Improving Road Signs Detection performance by Combining the Features of\n  Hough Transform and Texture", "comments": "6 pages", "journal-ref": null, "doi": "10.5120/12767-8795", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the large uses of the intelligent systems in different domains, and in\norder to increase the drivers and pedestrians safety, the road and traffic sign\nrecognition system has been a challenging issue and an important task for many\nyears. But studies, done in this field of detection and recognition of traffic\nsigns in an image, which are interested in the Arab context, are still\ninsufficient. Detection of the road signs present in the scene is the one of\nthe main stages of the traffic sign detection and recognition. In this paper,\nan efficient solution to enhance road signs detection, including Arabic\ncontext, performance based on color segmentation, Randomized Hough Transform\nand the combination of Zernike moments and Haralick features has been made.\nSegmentation stage is useful to determine the Region of Interest (ROI) in the\nimage. The Randomized Hough Transform (RHT) is used to detect the circular and\noctagonal shapes. This stage is improved by the extraction of the Haralick\nfeatures and Zernike moments. Furthermore, we use it as input of a classifier\nbased on SVM. Experimental results show that the proposed approach allows us to\nperform the measurements precision.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 15:09:29 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Ayaou", "Tarik", ""], ["Boussaid", "Mourad", ""], ["Afdel", "Karim", ""], ["Amghar", "Abdellah", ""]]}, {"id": "2010.06454", "submitter": "Hao Dong", "authors": "Jian Mei and Hao Dong", "title": "The DongNiao International Birds 10000 Dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  DongNiao International Birds 10000 (DIB-10K) is a challenging image dataset\nwhich has more than 10 thousand different types of birds. It was created to\nenable the study of machine learning and also ornithology research. DIB-10K\ndoes not own the copyright of these images. It only provides thumbnails of\nimages, in a way similar to ImageNet.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2020 04:19:27 GMT"}, {"version": "v2", "created": "Thu, 5 Nov 2020 05:52:08 GMT"}], "update_date": "2020-11-06", "authors_parsed": [["Mei", "Jian", ""], ["Dong", "Hao", ""]]}, {"id": "2010.06469", "submitter": "Clemens-Alexander Brust", "authors": "Clemens-Alexander Brust and Bj\\\"orn Barz and Joachim Denzler", "title": "Making Every Label Count: Handling Semantic Imprecision by Integrating\n  Domain Knowledge", "comments": "9 pages pre-print. Accepted for publication at ICPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Noisy data, crawled from the web or supplied by volunteers such as Mechanical\nTurkers or citizen scientists, is considered an alternative to professionally\nlabeled data. There has been research focused on mitigating the effects of\nlabel noise. It is typically modeled as inaccuracy, where the correct label is\nreplaced by an incorrect label from the same set. We consider an additional\ndimension of label noise: imprecision. For example, a non-breeding snow bunting\nis labeled as a bird. This label is correct, but not as precise as the task\nrequires.\n  Standard softmax classifiers cannot learn from such a weak label because they\nconsider all classes mutually exclusive, which non-breeding snow bunting and\nbird are not. We propose CHILLAX (Class Hierarchies for Imprecise Label\nLearning and Annotation eXtrapolation), a method based on hierarchical\nclassification, to fully utilize labels of any precision.\n  Experiments on noisy variants of NABirds and ILSVRC2012 show that our method\noutperforms strong baselines by as much as 16.4 percentage points, and the\ncurrent state of the art by up to 3.9 percentage points.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 15:21:14 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Brust", "Clemens-Alexander", ""], ["Barz", "Bj\u00f6rn", ""], ["Denzler", "Joachim", ""]]}, {"id": "2010.06497", "submitter": "Mark Pritt", "authors": "Mark Pritt and Gary Chern", "title": "Satellite Image Classification with Deep Learning", "comments": "7 pages, 18 figures, 2017 IEEE Applied Imagery Pattern Recognition\n  Workshop (AIPR)", "journal-ref": "2017 IEEE Applied Imagery Pattern Recognition Workshop (AIPR),\n  Washington, DC, USA, 2017, pp. 1-7", "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Satellite imagery is important for many applications including disaster\nresponse, law enforcement, and environmental monitoring. These applications\nrequire the manual identification of objects and facilities in the imagery.\nBecause the geographic expanses to be covered are great and the analysts\navailable to conduct the searches are few, automation is required. Yet\ntraditional object detection and classification algorithms are too inaccurate\nand unreliable to solve the problem. Deep learning is a family of machine\nlearning algorithms that have shown promise for the automation of such tasks.\nIt has achieved success in image understanding by means of convolutional neural\nnetworks. In this paper we apply them to the problem of object and facility\nrecognition in high-resolution, multi-spectral satellite imagery. We describe a\ndeep learning system for classifying objects and facilities from the IARPA\nFunctional Map of the World (fMoW) dataset into 63 different classes. The\nsystem consists of an ensemble of convolutional neural networks and additional\nneural networks that integrate satellite metadata with image features. It is\nimplemented in Python using the Keras and TensorFlow deep learning libraries\nand runs on a Linux server with an NVIDIA Titan X graphics card. At the time of\nwriting the system is in 2nd place in the fMoW TopCoder competition. Its total\naccuracy is 83%, the F1 score is 0.797, and it classifies 15 of the classes\nwith accuracies of 95% or better.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 15:56:58 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Pritt", "Mark", ""], ["Chern", "Gary", ""]]}, {"id": "2010.06499", "submitter": "Quan Huu Cap", "authors": "Quan Huu Cap, Hiroki Tani, Hiroyuki Uga, Satoshi Kagiwada and Hitoshi\n  Iyatomi", "title": "LASSR: Effective Super-Resolution Method for Plant Disease Diagnosis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The collection of high-resolution training data is crucial in building robust\nplant disease diagnosis systems, since such data have a significant impact on\ndiagnostic performance. However, they are very difficult to obtain and are not\nalways available in practice. Deep learning-based techniques, and particularly\ngenerative adversarial networks (GANs), can be applied to generate high-quality\nsuper-resolution images, but these methods often produce unexpected artifacts\nthat can lower the diagnostic performance. In this paper, we propose a novel\nartifact-suppression super-resolution method that is specifically designed for\ndiagnosing leaf disease, called Leaf Artifact-Suppression Super Resolution\n(LASSR). Thanks to its own artifact removal module that detects and suppresses\nartifacts to a considerable extent, LASSR can generate much more pleasing,\nhigh-quality images compared to the state-of-the-art ESRGAN model. Experiments\nbased on a five-class cucumber disease (including healthy) discrimination model\nshow that training with data generated by LASSR significantly boosts the\nperformance on an unseen test dataset by nearly 22% compared with the baseline,\nand that our approach is more than 2% better than a model trained with images\ngenerated by ESRGAN.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 02:33:49 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Cap", "Quan Huu", ""], ["Tani", "Hiroki", ""], ["Uga", "Hiroyuki", ""], ["Kagiwada", "Satoshi", ""], ["Iyatomi", "Hitoshi", ""]]}, {"id": "2010.06510", "submitter": "Behzad Ghazanfari", "authors": "Behzad Ghazanfari, Fatemeh Afghah, Sixian Zhang", "title": "Piece-wise Matching Layer in Representation Learning for ECG\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes piece-wise matching layer as a novel layer in\nrepresentation learning methods for electrocardiogram (ECG) classification.\nDespite the remarkable performance of representation learning methods in the\nanalysis of time series, there are still several challenges associated with\nthese methods ranging from the complex structures of methods, the lack of\ngenerality of solutions, the need for expert knowledge, and large-scale\ntraining datasets. We introduce the piece-wise matching layer that works based\non two levels to address some of the aforementioned challenges. At the first\nlevel, a set of morphological, statistical, and frequency features and\ncomparative forms of them are computed based on each periodic part and its\nneighbors. At the second level, these features are modified by predefined\ntransformation functions based on a receptive field scenario. Several scenarios\nof offline processing, incremental processing, fixed sliding receptive field,\nand event-based triggering receptive field can be implemented based on the\nchoice of length and mechanism of indicating the receptive field. We propose\ndynamic time wrapping as a mechanism that indicates a receptive field based on\nevent triggering tactics. To evaluate the performance of this method in time\nseries analysis, we applied the proposed layer in two publicly available\ndatasets of PhysioNet competitions in 2015 and 2017 where the input data is ECG\nsignal. We compared the performance of our method against a variety of known\ntuned methods from expert knowledge, machine learning, deep learning methods,\nand the combination of them. The proposed approach improves the state of the\nart in two known completions 2015 and 2017 around 4% and 7% correspondingly\nwhile it does not rely on in advance knowledge of the classes or the possible\nplaces of arrhythmia.\n", "versions": [{"version": "v1", "created": "Sat, 26 Sep 2020 00:49:34 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Ghazanfari", "Behzad", ""], ["Afghah", "Fatemeh", ""], ["Zhang", "Sixian", ""]]}, {"id": "2010.06520", "submitter": "Mark Pritt", "authors": "Mark Pritt", "title": "Deep Learning for Recognizing Mobile Targets in Satellite Imagery", "comments": "7 pages, 15 figures, 2018 IEEE Applied Imagery Pattern Recognition\n  Workshop (AIPR)", "journal-ref": "2018 IEEE Applied Imagery Pattern Recognition Workshop (AIPR),\n  Washington, DC, USA, 2018, pp. 1-7", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is an increasing demand for software that automatically detects and\nclassifies mobile targets such as airplanes, cars, and ships in satellite\nimagery. Applications of such automated target recognition (ATR) software\ninclude economic forecasting, traffic planning, maritime law enforcement, and\ndisaster response. This paper describes the extension of a convolutional neural\nnetwork (CNN) for classification to a sliding window algorithm for detection.\nIt is evaluated on mobile targets of the xView dataset, on which it achieves\ndetection and classification accuracies higher than 95%.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 16:26:42 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Pritt", "Mark", ""]]}, {"id": "2010.06536", "submitter": "Sasan Tavakkol", "authors": "Sasan Tavakkol, Feng Han, Brandon Mayer, Mark Phillips, Cyrus Shahabi,\n  Yao-Yi Chiang and Raimondas Kiveris", "title": "Kartta Labs: Collaborative Time Travel", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce the modular and scalable design of Kartta Labs, an open source,\nopen data, and scalable system for virtually reconstructing cities from\nhistorical maps and photos. Kartta Labs relies on crowdsourcing and artificial\nintelligence consisting of two major modules: Maps and 3D models. Each module,\nin turn, consists of sub-modules that enable the system to reconstruct a city\nfrom historical maps and photos. The result is a spatiotemporal reference that\ncan be used to integrate various collected data (curated, sensed, or\ncrowdsourced) for research, education, and entertainment purposes. The system\nempowers the users to experience collaborative time travel such that they work\ntogether to reconstruct the past and experience it on an open source and open\ndata platform.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 02:19:32 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Tavakkol", "Sasan", ""], ["Han", "Feng", ""], ["Mayer", "Brandon", ""], ["Phillips", "Mark", ""], ["Shahabi", "Cyrus", ""], ["Chiang", "Yao-Yi", ""], ["Kiveris", "Raimondas", ""]]}, {"id": "2010.06572", "submitter": "Jack Hessel", "authors": "Jack Hessel and Lillian Lee", "title": "Does my multimodal model learn cross-modal interactions? It's harder to\n  tell than you might think!", "comments": null, "journal-ref": "Published in EMNLP 2020", "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling expressive cross-modal interactions seems crucial in multimodal\ntasks, such as visual question answering. However, sometimes high-performing\nblack-box algorithms turn out to be mostly exploiting unimodal signals in the\ndata. We propose a new diagnostic tool, empirical multimodally-additive\nfunction projection (EMAP), for isolating whether or not cross-modal\ninteractions improve performance for a given model on a given task. This\nfunction projection modifies model predictions so that cross-modal interactions\nare eliminated, isolating the additive, unimodal structure. For seven\nimage+text classification tasks (on each of which we set new state-of-the-art\nbenchmarks), we find that, in many cases, removing cross-modal interactions\nresults in little to no performance degradation. Surprisingly, this holds even\nwhen expressive models, with capacity to consider interactions, otherwise\noutperform less expressive models; thus, performance improvements, even when\npresent, often cannot be attributed to consideration of cross-modal feature\ninteractions. We hence recommend that researchers in multimodal machine\nlearning report the performance not only of unimodal baselines, but also the\nEMAP of their best-performing model.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 17:45:28 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Hessel", "Jack", ""], ["Lee", "Lillian", ""]]}, {"id": "2010.06580", "submitter": "Daniel Fremont", "authors": "Daniel J. Fremont and Edward Kim and Tommaso Dreossi and Shromona\n  Ghosh and Xiangyu Yue and Alberto L. Sangiovanni-Vincentelli and Sanjit A.\n  Seshia", "title": "Scenic: A Language for Scenario Specification and Data Generation", "comments": "Supercedes arXiv:1809.09310", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new probabilistic programming language for the design and\nanalysis of cyber-physical systems, especially those based on machine learning.\nSpecifically, we consider the problems of training a system to be robust to\nrare events, testing its performance under different conditions, and debugging\nfailures. We show how a probabilistic programming language can help address\nthese problems by specifying distributions encoding interesting types of\ninputs, then sampling these to generate specialized training and test data.\nMore generally, such languages can be used to write environment models, an\nessential prerequisite to any formal analysis. In this paper, we focus on\nsystems like autonomous cars and robots, whose environment at any point in time\nis a 'scene', a configuration of physical objects and agents. We design a\ndomain-specific language, Scenic, for describing scenarios that are\ndistributions over scenes and the behaviors of their agents over time. As a\nprobabilistic programming language, Scenic allows assigning distributions to\nfeatures of the scene, as well as declaratively imposing hard and soft\nconstraints over the scene. We develop specialized techniques for sampling from\nthe resulting distribution, taking advantage of the structure provided by\nScenic's domain-specific syntax. Finally, we apply Scenic in a case study on a\nconvolutional neural network designed to detect cars in road images, improving\nits performance beyond that achieved by state-of-the-art synthetic data\ngeneration methods.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 17:58:31 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Fremont", "Daniel J.", ""], ["Kim", "Edward", ""], ["Dreossi", "Tommaso", ""], ["Ghosh", "Shromona", ""], ["Yue", "Xiangyu", ""], ["Sangiovanni-Vincentelli", "Alberto L.", ""], ["Seshia", "Sanjit A.", ""]]}, {"id": "2010.06610", "submitter": "Marton Havasi", "authors": "Marton Havasi, Rodolphe Jenatton, Stanislav Fort, Jeremiah Zhe Liu,\n  Jasper Snoek, Balaji Lakshminarayanan, Andrew M. Dai, Dustin Tran", "title": "Training independent subnetworks for robust prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent approaches to efficiently ensemble neural networks have shown that\nstrong robustness and uncertainty performance can be achieved with a negligible\ngain in parameters over the original network. However, these methods still\nrequire multiple forward passes for prediction, leading to a significant\ncomputational cost. In this work, we show a surprising result: the benefits of\nusing multiple predictions can be achieved `for free' under a single model's\nforward pass. In particular, we show that, using a multi-input multi-output\n(MIMO) configuration, one can utilize a single model's capacity to train\nmultiple subnetworks that independently learn the task at hand. By ensembling\nthe predictions made by the subnetworks, we improve model robustness without\nincreasing compute. We observe a significant improvement in negative\nlog-likelihood, accuracy, and calibration error on CIFAR10, CIFAR100, ImageNet,\nand their out-of-distribution variants compared to previous methods.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 18:05:13 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Havasi", "Marton", ""], ["Jenatton", "Rodolphe", ""], ["Fort", "Stanislav", ""], ["Liu", "Jeremiah Zhe", ""], ["Snoek", "Jasper", ""], ["Lakshminarayanan", "Balaji", ""], ["Dai", "Andrew M.", ""], ["Tran", "Dustin", ""]]}, {"id": "2010.06626", "submitter": "Raul de Queiroz Mendes", "authors": "Raul de Queiroz Mendes, Eduardo Godinho Ribeiro, Nicolas dos Santos\n  Rosa, Valdir Grassi Jr", "title": "On Deep Learning Techniques to Boost Monocular Depth Estimation for\n  Autonomous Navigation", "comments": "29 pages, 16 figures. Preprint published in the Elsevier's Robotics\n  and Autonomous Systems journal on November 23, 2020", "journal-ref": "Journal: Robotics and Autonomous Systems, publisher: Elsevier,\n  volume number: 136, year: 2020, page number: 103701", "doi": "10.1016/j.robot.2020.103701", "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inferring the depth of images is a fundamental inverse problem within the\nfield of Computer Vision since depth information is obtained through 2D images,\nwhich can be generated from infinite possibilities of observed real scenes.\nBenefiting from the progress of Convolutional Neural Networks (CNNs) to explore\nstructural features and spatial image information, Single Image Depth\nEstimation (SIDE) is often highlighted in scopes of scientific and\ntechnological innovation, as this concept provides advantages related to its\nlow implementation cost and robustness to environmental conditions. In the\ncontext of autonomous vehicles, state-of-the-art CNNs optimize the SIDE task by\nproducing high-quality depth maps, which are essential during the autonomous\nnavigation process in different locations. However, such networks are usually\nsupervised by sparse and noisy depth data, from Light Detection and Ranging\n(LiDAR) laser scans, and are carried out at high computational cost, requiring\nhigh-performance Graphic Processing Units (GPUs). Therefore, we propose a new\nlightweight and fast supervised CNN architecture combined with novel feature\nextraction models which are designed for real-world autonomous navigation. We\nalso introduce an efficient surface normals module, jointly with a simple\ngeometric 2.5D loss function, to solve SIDE problems. We also innovate by\nincorporating multiple Deep Learning techniques, such as the use of\ndensification algorithms and additional semantic, surface normals and depth\ninformation to train our framework. The method introduced in this work focuses\non robotic applications in indoor and outdoor environments and its results are\nevaluated on the competitive and publicly available NYU Depth V2 and KITTI\nDepth datasets.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 18:37:38 GMT"}, {"version": "v2", "created": "Tue, 29 Dec 2020 01:09:57 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Mendes", "Raul de Queiroz", ""], ["Ribeiro", "Eduardo Godinho", ""], ["Rosa", "Nicolas dos Santos", ""], ["Grassi", "Valdir", "Jr"]]}, {"id": "2010.06647", "submitter": "Matthew Hutchinson", "authors": "Matthew Hutchinson and Vijay Gadepally", "title": "Video Action Understanding: A Tutorial", "comments": "Shortened version submitted to ACM CSUR", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many believe that the successes of deep learning on image understanding\nproblems can be replicated in the realm of video understanding. However, the\nspan of video action problems and the set of proposed deep learning solutions\nis arguably wider and more diverse than those of their 2D image siblings.\nFinding, identifying, and predicting actions are a few of the most salient\ntasks in video action understanding. This tutorial clarifies a taxonomy of\nvideo action problems, highlights datasets and metrics used to baseline each\nproblem, describes common data preparation methods, and presents the building\nblocks of state-of-the-art deep learning model architectures.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 19:29:41 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Hutchinson", "Matthew", ""], ["Gadepally", "Vijay", ""]]}, {"id": "2010.06663", "submitter": "Teruo Matos Maruyama", "authors": "Teruo M. Maruyama, Luiz S. Oliveira, Alceu S. Britto Jr, Robert\n  Sabourin", "title": "Intrapersonal Parameter Optimization for Offline Handwritten Signature\n  Augmentation", "comments": "16 pages, 11 figures, To appear in the IEEE Transactions on\n  Information Forensics & Security", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Usually, in a real-world scenario, few signature samples are available to\ntrain an automatic signature verification system (ASVS). However, such systems\ndo indeed need a lot of signatures to achieve an acceptable performance.\nNeuromotor signature duplication methods and feature space augmentation methods\nmay be used to meet the need for an increase in the number of samples. Such\ntechniques manually or empirically define a set of parameters to introduce a\ndegree of writer variability. Therefore, in the present study, a method to\nautomatically model the most common writer variability traits is proposed. The\nmethod is used to generate offline signatures in the image and the feature\nspace and train an ASVS. We also introduce an alternative approach to evaluate\nthe quality of samples considering their feature vectors. We evaluated the\nperformance of an ASVS with the generated samples using three well-known\noffline signature datasets: GPDS, MCYT-75, and CEDAR. In GPDS-300, when the SVM\nclassifier was trained using one genuine signature per writer and the\nduplicates generated in the image space, the Equal Error Rate (EER) decreased\nfrom 5.71% to 1.08%. Under the same conditions, the EER decreased to 1.04%\nusing the feature space augmentation technique. We also verified that the model\nthat generates duplicates in the image space reproduces the most common writer\nvariability traits in the three different datasets.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 19:54:02 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Maruyama", "Teruo M.", ""], ["Oliveira", "Luiz S.", ""], ["Britto", "Alceu S.", "Jr"], ["Sabourin", "Robert", ""]]}, {"id": "2010.06668", "submitter": "Qun Liu", "authors": "Qun Liu, Matthew Shreve, Raja Bala", "title": "LiDAM: Semi-Supervised Learning with Localized Domain Adaptation and\n  Iterative Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although data is abundant, data labeling is expensive. Semi-supervised\nlearning methods combine a few labeled samples with a large corpus of unlabeled\ndata to effectively train models. This paper introduces our proposed method\nLiDAM, a semi-supervised learning approach rooted in both domain adaptation and\nself-paced learning. LiDAM first performs localized domain shifts to extract\nbetter domain-invariant features for the model that results in more accurate\nclusters and pseudo-labels. These pseudo-labels are then aligned with real\nclass labels in a self-paced fashion using a novel iterative matching technique\nthat is based on majority consistency over high-confidence predictions.\nSimultaneously, a final classifier is trained to predict ground-truth labels\nuntil convergence. LiDAM achieves state-of-the-art performance on the CIFAR-100\ndataset, outperforming FixMatch (73.50% vs. 71.82%) when using 2500 labels.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 19:57:32 GMT"}, {"version": "v2", "created": "Mon, 23 Nov 2020 22:42:54 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Liu", "Qun", ""], ["Shreve", "Matthew", ""], ["Bala", "Raja", ""]]}, {"id": "2010.06671", "submitter": "Pedram Hosseini", "authors": "Lily Li, Or Levi, Pedram Hosseini, David A. Broniatowski", "title": "A Multi-Modal Method for Satire Detection using Textual and Visual Cues", "comments": "Accepted to the Third Workshop on NLP for Internet Freedom (NLP4IF):\n  Censorship, Disinformation, and Propaganda. Co-located with COLING 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Satire is a form of humorous critique, but it is sometimes misinterpreted by\nreaders as legitimate news, which can lead to harmful consequences. We observe\nthat the images used in satirical news articles often contain absurd or\nridiculous content and that image manipulation is used to create fictional\nscenarios. While previous work have studied text-based methods, in this work we\npropose a multi-modal approach based on state-of-the-art visiolinguistic model\nViLBERT. To this end, we create a new dataset consisting of images and\nheadlines of regular and satirical news for the task of satire detection. We\nfine-tune ViLBERT on the dataset and train a convolutional neural network that\nuses an image forensics technique. Evaluation on the dataset shows that our\nproposed multi-modal approach outperforms image-only, text-only, and simple\nfusion baselines.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 20:08:29 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Li", "Lily", ""], ["Levi", "Or", ""], ["Hosseini", "Pedram", ""], ["Broniatowski", "David A.", ""]]}, {"id": "2010.06682", "submitter": "Ari Morcos", "authors": "Tiffany Tianhui Cai, Jonathan Frankle, David J. Schwab, and Ari S.\n  Morcos", "title": "Are all negatives created equal in contrastive instance discrimination?", "comments": "Fixed author name error", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-supervised learning has recently begun to rival supervised learning on\ncomputer vision tasks. Many of the recent approaches have been based on\ncontrastive instance discrimination (CID), in which the network is trained to\nrecognize two augmented versions of the same instance (a query and positive)\nwhile discriminating against a pool of other instances (negatives). The learned\nrepresentation is then used on downstream tasks such as image classification.\nUsing methodology from MoCo v2 (Chen et al., 2020), we divided negatives by\ntheir difficulty for a given query and studied which difficulty ranges were\nmost important for learning useful representations. We found a minority of\nnegatives -- the hardest 5% -- were both necessary and sufficient for the\ndownstream task to reach nearly full accuracy. Conversely, the easiest 95% of\nnegatives were unnecessary and insufficient. Moreover, the very hardest 0.1% of\nnegatives were unnecessary and sometimes detrimental. Finally, we studied the\nproperties of negatives that affect their hardness, and found that hard\nnegatives were more semantically similar to the query, and that some negatives\nwere more consistently easy or hard than we would expect by chance. Together,\nour results indicate that negatives vary in importance and that CID may benefit\nfrom more intelligent negative treatment.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 20:52:10 GMT"}, {"version": "v2", "created": "Sun, 25 Oct 2020 16:55:39 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Cai", "Tiffany Tianhui", ""], ["Frankle", "Jonathan", ""], ["Schwab", "David J.", ""], ["Morcos", "Ari S.", ""]]}, {"id": "2010.06693", "submitter": "Hamdi Yahia", "authors": "Yahia Hamdi, Hanen Akouaydi, Houcine Boubaker, Adel M. Alimi", "title": "Handwriting Quality Analysis using Online-Offline Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work is part of an innovative e-learning project allowing the\ndevelopment of an advanced digital educational tool that provides feedback\nduring the process of learning handwriting for young school children (three to\neight years old). In this paper, we describe a new method for children\nhandwriting quality analysis. It automatically detects mistakes, gives\nreal-time on-line feedback for children's writing, and helps teachers\ncomprehend and evaluate children's writing skills. The proposed method adjudges\nfive main criteria shape, direction, stroke order, position respect to the\nreference lines, and kinematics of the trace. It analyzes the handwriting\nquality and automatically gives feedback based on the combination of three\nextracted models: Beta-Elliptic Model (BEM) using similarity detection (SD) and\ndissimilarity distance (DD) measure, Fourier Descriptor Model (FDM), and\nperceptive Convolutional Neural Network (CNN) with Support Vector Machine (SVM)\ncomparison engine. The originality of our work lies partly in the system\narchitecture which apprehends complementary dynamic, geometric, and visual\nrepresentation of the examined handwritten scripts and in the efficient\nselected features adapted to various handwriting styles and multiple script\nlanguages such as Arabic, Latin, digits, and symbol drawing. The application\noffers two interactive interfaces respectively dedicated to learners,\neducators, experts or teachers and allows them to adapt it easily to the\nspecificity of their disciples. The evaluation of our framework is enhanced by\na database collected in Tunisia primary school with 400 children. Experimental\nresults show the efficiency and robustness of our suggested framework that\nhelps teachers and children by offering positive feedback throughout the\nhandwriting learning process using tactile digital devices.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 14:33:56 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Hamdi", "Yahia", ""], ["Akouaydi", "Hanen", ""], ["Boubaker", "Houcine", ""], ["Alimi", "Adel M.", ""]]}, {"id": "2010.06715", "submitter": "Liam Fowl", "authors": "Liam Fowl, Micah Goldblum, Arjun Gupta, Amr Sharaf, Tom Goldstein", "title": "Random Network Distillation as a Diversity Metric for Both Image and\n  Text Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative models are increasingly able to produce remarkably high quality\nimages and text. The community has developed numerous evaluation metrics for\ncomparing generative models. However, these metrics do not effectively quantify\ndata diversity. We develop a new diversity metric that can readily be applied\nto data, both synthetic and natural, of any type. Our method employs random\nnetwork distillation, a technique introduced in reinforcement learning. We\nvalidate and deploy this metric on both images and text. We further explore\ndiversity in few-shot image generation, a setting which was previously\ndifficult to evaluate.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 22:03:52 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Fowl", "Liam", ""], ["Goldblum", "Micah", ""], ["Gupta", "Arjun", ""], ["Sharaf", "Amr", ""], ["Goldstein", "Tom", ""]]}, {"id": "2010.06740", "submitter": "Yanjun  Qi Dr.", "authors": "Jake Grigsby, Yanjun Qi", "title": "Measuring Visual Generalization in Continuous Control from Pixels", "comments": "A total of 20 pages, 8 pages as the main text", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-supervised learning and data augmentation have significantly reduced the\nperformance gap between state and image-based reinforcement learning agents in\ncontinuous control tasks. However, it is still unclear whether current\ntechniques can face a variety of visual conditions required by real-world\nenvironments. We propose a challenging benchmark that tests agents' visual\ngeneralization by adding graphical variety to existing continuous control\ndomains. Our empirical analysis shows that current methods struggle to\ngeneralize across a diverse set of visual changes, and we examine the specific\nfactors of variation that make these tasks difficult. We find that data\naugmentation techniques outperform self-supervised learning approaches and that\nmore significant image transformations provide better visual generalization\n\\footnote{The benchmark and our augmented actor-critic implementation are\nopen-sourced @ https://github.com/QData/dmc_remastered)\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 23:42:40 GMT"}, {"version": "v2", "created": "Fri, 27 Nov 2020 20:33:03 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Grigsby", "Jake", ""], ["Qi", "Yanjun", ""]]}, {"id": "2010.06773", "submitter": "Ruwan Tennakoon", "authors": "Joshua Thorpe, Ruwan Tennakoon, Alireza Bab-Hadiashar", "title": "Rotation Averaging with Attention Graph Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a real-time and robust solution to large-scale\nmultiple rotation averaging. Until recently, Multiple rotation averaging\nproblem had been solved using conventional iterative optimization algorithms.\nSuch methods employed robust cost functions that were chosen based on\nassumptions made about the sensor noise and outlier distribution. In practice,\nthese assumptions do not always fit real datasets very well. A recent work\nshowed that the noise distribution could be learnt using a graph neural\nnetwork. This solution required a second network for outlier detection and\nremoval as the averaging network was sensitive to a poor initialization. In\nthis paper we propose a single-stage graph neural network that can robustly\nperform rotation averaging in the presence of noise and outliers. Our method\nuses all observations, suppressing outliers effects through the use of weighted\naveraging and an attention mechanism within the network design. The result is a\nnetwork that is faster, more robust and can be trained with less samples than\nthe previous neural approach, ultimately outperforming conventional iterative\nalgorithms in accuracy and in inference times.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 02:07:19 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Thorpe", "Joshua", ""], ["Tennakoon", "Ruwan", ""], ["Bab-Hadiashar", "Alireza", ""]]}, {"id": "2010.06775", "submitter": "Hao Tan", "authors": "Hao Tan, Mohit Bansal", "title": "Vokenization: Improving Language Understanding with Contextualized,\n  Visual-Grounded Supervision", "comments": "EMNLP 2020 (15 pages)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans learn language by listening, speaking, writing, reading, and also, via\ninteraction with the multimodal real world. Existing language pre-training\nframeworks show the effectiveness of text-only self-supervision while we\nexplore the idea of a visually-supervised language model in this paper. We find\nthat the main reason hindering this exploration is the large divergence in\nmagnitude and distributions between the visually-grounded language datasets and\npure-language corpora. Therefore, we develop a technique named \"vokenization\"\nthat extrapolates multimodal alignments to language-only data by contextually\nmapping language tokens to their related images (which we call \"vokens\"). The\n\"vokenizer\" is trained on relatively small image captioning datasets and we\nthen apply it to generate vokens for large language corpora. Trained with these\ncontextually generated vokens, our visually-supervised language models show\nconsistent improvements over self-supervised alternatives on multiple\npure-language tasks such as GLUE, SQuAD, and SWAG. Code and pre-trained models\npublicly available at https://github.com/airsplay/vokenization\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 02:11:51 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Tan", "Hao", ""], ["Bansal", "Mohit", ""]]}, {"id": "2010.06777", "submitter": "Peng Peng", "authors": "Peng Peng, Jiugen Wang", "title": "Ferrograph image classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has been challenging to identify ferrograph images with a small dataset\nand various scales of wear particle. A novel model is proposed in this study to\ncope with these challenging problems. For the problem of insufficient samples,\nwe first proposed a data augmentation algorithm based on the permutation of\nimage patches. Then, an auxiliary loss function of image patch permutation\nrecognition was proposed to identify the image generated by the data\naugmentation algorithm. Moreover, we designed a feature extraction loss\nfunction to force the proposed model to extract more abundant features and to\nreduce redundant representations. As for the challenge of large change range of\nwear particle size, we proposed a multi-scale feature extraction block to\nobtain the multi-scale representations of wear particles. We carried out\nexperiments on a ferrograph image dataset and a mini-CIFAR-10 dataset.\nExperimental results show that the proposed model can improve the accuracy of\nthe two datasets by 9% and 20% respectively compared with the baseline.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 02:19:32 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Peng", "Peng", ""], ["Wang", "Jiugen", ""]]}, {"id": "2010.06784", "submitter": "Bardia Yousefi", "authors": "Bardia Yousefi, Clemente Ibarra Castanedo, Xavier P.V. Maldague", "title": "Low-rank Convex/Sparse Thermal Matrix Approximation for Infrared-based\n  Diagnostic System", "comments": "Authors version", "journal-ref": "IEEE Transactions on Instrumentation and Measurement 2020", "doi": "10.1109/TIM.2020.3031129", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Active and passive thermography are two efficient techniques extensively used\nto measure heterogeneous thermal patterns leading to subsurface defects for\ndiagnostic evaluations. This study conducts a comparative analysis on low-rank\nmatrix approximation methods in thermography with applications of semi-,\nconvex-, and sparse- non-negative matrix factorization (NMF) methods for\ndetecting subsurface thermal patterns. These methods inherit the advantages of\nprincipal component thermography (PCT) and sparse PCT, whereas tackle negative\nbases in sparse PCT with non-negative constraints, and exhibit clustering\nproperty in processing data. The practicality and efficiency of these methods\nare demonstrated by the experimental results for subsurface defect detection in\nthree specimens (for different depth and size defects) and preserving thermal\nheterogeneity for distinguishing breast abnormality in breast cancer screening\ndataset (accuracy of 74.1%, 75.8%, and 77.8%).\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 02:53:19 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Yousefi", "Bardia", ""], ["Castanedo", "Clemente Ibarra", ""], ["Maldague", "Xavier P. V.", ""]]}, {"id": "2010.06808", "submitter": "Zhao Chen", "authors": "Zhao Chen, Jiquan Ngiam, Yanping Huang, Thang Luong, Henrik\n  Kretzschmar, Yuning Chai, Dragomir Anguelov", "title": "Just Pick a Sign: Optimizing Deep Multitask Models with Gradient Sign\n  Dropout", "comments": "Conference on Neural Information Processing Systems (NeurIPS) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The vast majority of deep models use multiple gradient signals, typically\ncorresponding to a sum of multiple loss terms, to update a shared set of\ntrainable weights. However, these multiple updates can impede optimal training\nby pulling the model in conflicting directions. We present Gradient Sign\nDropout (GradDrop), a probabilistic masking procedure which samples gradients\nat an activation layer based on their level of consistency. GradDrop is\nimplemented as a simple deep layer that can be used in any deep net and\nsynergizes with other gradient balancing approaches. We show that GradDrop\noutperforms the state-of-the-art multiloss methods within traditional multitask\nand transfer learning settings, and we discuss how GradDrop reveals links\nbetween optimal multiloss training and gradient stochasticity.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 04:42:18 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Chen", "Zhao", ""], ["Ngiam", "Jiquan", ""], ["Huang", "Yanping", ""], ["Luong", "Thang", ""], ["Kretzschmar", "Henrik", ""], ["Chai", "Yuning", ""], ["Anguelov", "Dragomir", ""]]}, {"id": "2010.06821", "submitter": "Dong Li", "authors": "Dong Li, Sitong Chen, Xudong Liu, Yunda Sun and Li Zhang", "title": "Towards Optimal Filter Pruning with Balanced Performance and Pruning\n  Speed", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Filter pruning has drawn more attention since resource constrained platform\nrequires more compact model for deployment. However, current pruning methods\nsuffer either from the inferior performance of one-shot methods, or the\nexpensive time cost of iterative training methods. In this paper, we propose a\nbalanced filter pruning method for both performance and pruning speed. Based on\nthe filter importance criteria, our method is able to prune a layer with\napproximate layer-wise optimal pruning rate at preset loss variation. The\nnetwork is pruned in the layer-wise way without the time consuming\nprune-retrain iteration. If a pre-defined pruning rate for the entire network\nis given, we also introduce a method to find the corresponding loss variation\nthreshold with fast converging speed. Moreover, we propose the layer group\npruning and channel selection mechanism for channel alignment in network with\nshort connections. The proposed pruning method is widely applicable to common\narchitectures and does not involve any additional training except the final\nfine-tuning. Comprehensive experiments show that our method outperforms many\nstate-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 06:17:09 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Li", "Dong", ""], ["Chen", "Sitong", ""], ["Liu", "Xudong", ""], ["Sun", "Yunda", ""], ["Zhang", "Li", ""]]}, {"id": "2010.06824", "submitter": "Martijn Starmans", "authors": "Martijn P.A. Starmans, Milea J.M. Timbergen, Melissa Vos, Michel\n  Renckens, Dirk J. Gr\\\"unhagen, Geert J.L.H. van Leenders, Roy S. Dwarkasing,\n  Fran\\c{c}ois E. J. A. Willemssen, Wiro J. Niessen, Cornelis Verhoef, Stefan\n  Sleijfer, Jacob J. Visser, and Stefan Klein", "title": "Differential diagnosis and molecular stratification of gastrointestinal\n  stromal tumors on CT images using a radiomics approach", "comments": "Martijn P.A. Starmans and Milea J.M. Timbergen contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distinguishing gastrointestinal stromal tumors (GISTs) from other\nintra-abdominal tumors and GISTs molecular analysis is necessary for treatment\nplanning, but challenging due to its rarity. The aim of this study was to\nevaluate radiomics for distinguishing GISTs from other intra-abdominal tumors,\nand in GISTs, predict the c-KIT, PDGFRA,BRAF mutational status and mitotic\nindex (MI). All 247 included patients (125 GISTS, 122 non-GISTs) underwent a\ncontrast-enhanced venous phase CT. The GIST vs. non-GIST radiomics model,\nincluding imaging, age, sex and location, had a mean area under the curve (AUC)\nof 0.82. Three radiologists had an AUC of 0.69, 0.76, and 0.84, respectively.\nThe radiomics model had an AUC of 0.52 for c-KIT, 0.56 for c-KIT exon 11, and\n0.52 for the MI. Hence, our radiomics model was able to distinguish GIST from\nnon-GISTS with a performance similar to three radiologists, but was not able to\npredict the c-KIT mutation or MI.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 06:27:45 GMT"}, {"version": "v2", "created": "Thu, 15 Oct 2020 06:12:55 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Starmans", "Martijn P. A.", ""], ["Timbergen", "Milea J. M.", ""], ["Vos", "Melissa", ""], ["Renckens", "Michel", ""], ["Gr\u00fcnhagen", "Dirk J.", ""], ["van Leenders", "Geert J. L. H.", ""], ["Dwarkasing", "Roy S.", ""], ["Willemssen", "Fran\u00e7ois E. J. A.", ""], ["Niessen", "Wiro J.", ""], ["Verhoef", "Cornelis", ""], ["Sleijfer", "Stefan", ""], ["Visser", "Jacob J.", ""], ["Klein", "Stefan", ""]]}, {"id": "2010.06844", "submitter": "Yu Cheng", "authors": "Cheng Yu, Bo Wang, Bo Yang, Robby T. Tan", "title": "Multi-Scale Networks for 3D Human Pose Estimation with Inference Stage\n  Optimization", "comments": "14 pages, 13 figures. arXiv admin note: substantial text overlap with\n  arXiv:2004.11822", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating 3D human poses from a monocular video is still a challenging task.\nMany existing methods' performance drops when the target person is occluded by\nother objects, or the motion is too fast/slow relative to the scale and speed\nof the training data. Moreover, many of these methods are not designed or\ntrained under severe occlusion explicitly, making their performance on handling\nocclusion compromised. Addressing these problems, we introduce a\nspatio-temporal network for robust 3D human pose estimation. As humans in\nvideos may appear in different scales and have various motion speeds, we apply\nmulti-scale spatial features for 2D joints or keypoints prediction in each\nindividual frame, and multi-stride temporal convolutional networks (TCNs) to\nestimate 3D joints or keypoints. Furthermore, we design a spatio-temporal\ndiscriminator based on body structures as well as limb motions to assess\nwhether the predicted pose forms a valid pose and a valid movement. During\ntraining, we explicitly mask out some keypoints to simulate various occlusion\ncases, from minor to severe occlusion, so that our network can learn better and\nbecomes robust to various degrees of occlusion. As there are limited 3D\nground-truth data, we further utilize 2D video data to inject a semi-supervised\nlearning capability to our network. Moreover, we observe that there is a\ndiscrepancy between 3D pose prediction and 2D pose estimation due to different\npose variations between video and image training datasets. We, therefore\npropose a confidence-based inference stage optimization to adaptively enforce\n3D pose projection to match 2D pose estimation to further improve final pose\nprediction accuracy. Experiments on public datasets validate the effectiveness\nof our method, and our ablation studies show the strengths of our network's\nindividual submodules.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 15:24:28 GMT"}, {"version": "v2", "created": "Fri, 16 Oct 2020 19:42:53 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Yu", "Cheng", ""], ["Wang", "Bo", ""], ["Yang", "Bo", ""], ["Tan", "Robby T.", ""]]}, {"id": "2010.06855", "submitter": "Hui Liu", "authors": "Hui Liu, Bo Zhao, Jiabao Guo, Yang An, Peng Liu", "title": "GreedyFool: Multi-Factor Imperceptibility and Its Application to\n  Designing Black-box Adversarial Example Attack", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) are inherently vulnerable to well-designed input\nsamples called adversarial examples. The adversary can easily fool DNNs by\nadding slight perturbations to the input. In this paper, we propose a novel\nblack-box adversarial example attack named GreedyFool, which synthesizes\nadversarial examples based on the differential evolution and the greedy\napproximation. The differential evolution is utilized to evaluate the effects\nof perturbed pixels on the confidence of the DNNs-based classifier. The greedy\napproximation is an approximate optimization algorithm to automatically get\nadversarial perturbations. Existing works synthesize the adversarial examples\nby leveraging simple metrics to penalize the perturbations, which lack\nsufficient consideration of the human visual system (HVS), resulting in\nnoticeable artifacts. In order to sufficient imperceptibility, we launch a lot\nof investigations into the HVS and design an integrated metric considering just\nnoticeable distortion (JND), Weber-Fechner law, texture masking and channel\nmodulation, which is proven to be a better metric to measure the perceptual\ndistance between the benign examples and the adversarial ones. The experimental\nresults demonstrate that the GreedyFool has several remarkable properties\nincluding black-box, 100% success rate, flexibility, automation and can\nsynthesize the more imperceptible adversarial examples than the\nstate-of-the-art pixel-wise methods.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 07:45:06 GMT"}, {"version": "v2", "created": "Sat, 24 Oct 2020 16:29:44 GMT"}, {"version": "v3", "created": "Mon, 23 Nov 2020 08:40:32 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Liu", "Hui", ""], ["Zhao", "Bo", ""], ["Guo", "Jiabao", ""], ["An", "Yang", ""], ["Liu", "Peng", ""]]}, {"id": "2010.06866", "submitter": "Basil Mustafa", "authors": "Basil Mustafa and Carlos Riquelme and Joan Puigcerver and Andr\\'e\n  Susano Pinto and Daniel Keysers and Neil Houlsby", "title": "Deep Ensembles for Low-Data Transfer Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the low-data regime, it is difficult to train good supervised models from\nscratch. Instead practitioners turn to pre-trained models, leveraging transfer\nlearning. Ensembling is an empirically and theoretically appealing way to\nconstruct powerful predictive models, but the predominant approach of training\nmultiple deep networks with different random initialisations collides with the\nneed for transfer via pre-trained weights. In this work, we study different\nways of creating ensembles from pre-trained models. We show that the nature of\npre-training itself is a performant source of diversity, and propose a\npractical algorithm that efficiently identifies a subset of pre-trained models\nfor any downstream dataset. The approach is simple: Use nearest-neighbour\naccuracy to rank pre-trained models, fine-tune the best ones with a small\nhyperparameter sweep, and greedily construct an ensemble to minimise validation\ncross-entropy. When evaluated together with strong baselines on 19 different\ndownstream tasks (the Visual Task Adaptation Benchmark), this achieves\nstate-of-the-art performance at a much lower inference budget, even when\nselecting from over 2,000 pre-trained models. We also assess our ensembles on\nImageNet variants and show improved robustness to distribution shift.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 07:59:00 GMT"}, {"version": "v2", "created": "Mon, 19 Oct 2020 10:59:20 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Mustafa", "Basil", ""], ["Riquelme", "Carlos", ""], ["Puigcerver", "Joan", ""], ["Pinto", "Andr\u00e9 Susano", ""], ["Keysers", "Daniel", ""], ["Houlsby", "Neil", ""]]}, {"id": "2010.06876", "submitter": "Shuo Wang", "authors": "Xudong Lv, Boya Wang, Dong Ye, and Shuo Wang", "title": "Semantic Flow-guided Motion Removal Method for Robust Mapping", "comments": "8 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Moving objects in scenes are still a severe challenge for the SLAM system.\nMany efforts have tried to remove the motion regions in the images by detecting\nmoving objects. In this way, the keypoints belonging to motion regions will be\nignored in the later calculations. In this paper, we proposed a novel motion\nremoval method, leveraging semantic information and optical flow to extract\nmotion regions. Different from previous works, we don't predict moving objects\nor motion regions directly from image sequences. We computed rigid optical\nflow, synthesized by the depth and pose, and compared it against the estimated\noptical flow to obtain initial motion regions. Then, we utilized K-means to\nfinetune the motion region masks with instance segmentation masks. The\nORB-SLAM2 integrated with the proposed motion removal method achieved the best\nperformance in both indoor and outdoor dynamic environments.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 08:40:16 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Lv", "Xudong", ""], ["Wang", "Boya", ""], ["Ye", "Dong", ""], ["Wang", "Shuo", ""]]}, {"id": "2010.06879", "submitter": "Zijue Chen", "authors": "Zijue Chen, David Ting, Rhys Newbury, Chao Chen", "title": "Semantic Segmentation for Partially Occluded Apple Trees Based on Deep\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fruit tree pruning and fruit thinning require a powerful vision system that\ncan provide high resolution segmentation of the fruit trees and their branches.\nHowever, recent works only consider the dormant season, where there are minimal\nocclusions on the branches or fit a polynomial curve to reconstruct branch\nshape and hence, losing information about branch thickness. In this work, we\napply two state-of-the-art supervised learning models U-Net and DeepLabv3, and\na conditional Generative Adversarial Network Pix2Pix (with and without the\ndiscriminator) to segment partially occluded 2D-open-V apple trees. Binary\naccuracy, Mean IoU, Boundary F1 score and Occluded branch recall were used to\nevaluate the performances of the models. DeepLabv3 outperforms the other models\nat Binary accuracy, Mean IoU and Boundary F1 score, but is surpassed by Pix2Pix\n(without discriminator) and U-Net in Occluded branch recall. We define two\ndifficulty indices to quantify the difficulty of the task: (1) Occlusion\nDifficulty Index and (2) Depth Difficulty Index. We analyze the worst 10 images\nin both difficulty indices by means of Branch Recall and Occluded Branch\nRecall. U-Net outperforms the other two models in the current metrics. On the\nother hand, Pix2Pix (without discriminator) provides more information on branch\npaths, which are not reflected by the metrics. This highlights the need for\nmore specific metrics on recovering occluded information. Furthermore, this\nshows the usefulness of image-transfer networks for hallucination behind\nocclusions. Future work is required to further enhance the models to recover\nmore information from occlusions such that this technology can be applied to\nautomating agricultural tasks in a commercial environment.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 08:43:22 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Chen", "Zijue", ""], ["Ting", "David", ""], ["Newbury", "Rhys", ""], ["Chen", "Chao", ""]]}, {"id": "2010.06890", "submitter": "Rahaf Aljundi", "authors": "Rahaf Aljundi, Nikolay Chumerin and Daniel Olmeda Reino", "title": "Identifying Wrongly Predicted Samples: A Method for Active Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art machine learning models require access to significant amount\nof annotated data in order to achieve the desired level of performance. While\nunlabelled data can be largely available and even abundant, annotation process\ncan be quite expensive and limiting. Under the assumption that some samples are\nmore important for a given task than others, active learning targets the\nproblem of identifying the most informative samples that one should acquire\nannotations for. Instead of the conventional reliance on model uncertainty as a\nproxy to leverage new unknown labels, in this work we propose a simple sample\nselection criterion that moves beyond uncertainty. By first accepting the model\nprediction and then judging its effect on the generalization error, we can\nbetter identify wrongly predicted samples. We further present an approximation\nto our criterion that is very efficient and provides a similarity based\ninterpretation. In addition to evaluating our method on the standard benchmarks\nof active learning, we consider the challenging yet realistic scenario of\nimbalanced data where categories are not equally represented. We show\nstate-of-the-art results and better rates at identifying wrongly predicted\nsamples. Our method is simple, model agnostic and relies on the current model\nstatus without the need for re-training from scratch.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 09:00:42 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Aljundi", "Rahaf", ""], ["Chumerin", "Nikolay", ""], ["Reino", "Daniel Olmeda", ""]]}, {"id": "2010.06897", "submitter": "Valerio Paolicelli", "authors": "Gabriele Moreno Berton, Valerio Paolicelli, Carlo Masone and Barbara\n  Caputo", "title": "Adaptive-Attentive Geolocalization from few queries: a hybrid approach", "comments": "The pytorch code is available at\n  https://github.com/valeriopaolicelli/AdAGeo", "journal-ref": "Proceedings of the IEEE/CVF Winter Conference on Applications of\n  Computer Vision (WACV), 2021, pp. 2918-2927", "doi": "10.1109/WACV48630.2021.00296", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the task of cross-domain visual place recognition, where the goal\nis to geolocalize a given query image against a labeled gallery, in the case\nwhere the query and the gallery belong to different visual domains. To achieve\nthis, we focus on building a domain robust deep network by leveraging over an\nattention mechanism combined with few-shot unsupervised domain adaptation\ntechniques, where we use a small number of unlabeled target domain images to\nlearn about the target distribution. With our method, we are able to outperform\nthe current state of the art while using two orders of magnitude less target\ndomain images. Finally we propose a new large-scale dataset for cross-domain\nvisual place recognition, called SVOX. The pytorch code is available at\nhttps://github.com/valeriopaolicelli/AdAGeo .\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 09:14:02 GMT"}, {"version": "v2", "created": "Wed, 6 Jan 2021 20:05:53 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Berton", "Gabriele Moreno", ""], ["Paolicelli", "Valerio", ""], ["Masone", "Carlo", ""], ["Caputo", "Barbara", ""]]}, {"id": "2010.06900", "submitter": "Kyeongchan Jang", "authors": "Yong-Gu Lee, Seong-Jae Lee, Sang-Jin Lee, Tae-Seung Baek, Dong-Whan\n  Lee, Kyeong-Chan Jang, Ho-Jin Sohn, Jin-Soo Kim", "title": "Development of Open Informal Dataset Affecting Autonomous Driving", "comments": "26 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This document is a document that has written procedures and methods for\ncollecting objects and unstructured dynamic data on the road for the\ndevelopment of object recognition technology for self-driving cars, and\noutlines the methods of collecting data, annotation data, object classifier\ncriteria, and data processing methods. On-road object and unstructured dynamic\ndata were collected in various environments, such as weather, time and traffic\nconditions, and additional reception calls for police and safety personnel were\ncollected. Finally, 100,000 images of various objects existing on pedestrians\nand roads, 200,000 images of police and traffic safety personnel, 5,000 images\nof police and traffic safety personnel, and data sets consisting of 5,000 image\ndata were collected and built.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 09:21:45 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Lee", "Yong-Gu", ""], ["Lee", "Seong-Jae", ""], ["Lee", "Sang-Jin", ""], ["Baek", "Tae-Seung", ""], ["Lee", "Dong-Whan", ""], ["Jang", "Kyeong-Chan", ""], ["Sohn", "Ho-Jin", ""], ["Kim", "Jin-Soo", ""]]}, {"id": "2010.06907", "submitter": "Nanyu Li", "authors": "Nanyu Li, Charles C. Zhou", "title": "AMPA-Net: Optimization-Inspired Attention Neural Network for Deep\n  Compressed Sensing", "comments": "7 pages,7 figures", "journal-ref": "2020 IEEE 20th International Conference on Communication\n  Technology(Oral)", "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compressed sensing (CS) is a challenging problem in image processing due to\nreconstructing an almost complete image from a limited measurement. To achieve\nfast and accurate CS reconstruction, we synthesize the advantages of two\nwell-known methods (neural network and optimization algorithm) to propose a\nnovel optimization inspired neural network which dubbed AMP-Net. AMP-Net\nrealizes the fusion of the Approximate Message Passing (AMP) algorithm and\nneural network. All of its parameters are learned automatically. Furthermore,\nwe propose an AMPA-Net which uses three attention networks to improve the\nrepresentation ability of AMP-Net. Finally, We demonstrate the effectiveness of\nAMP-Net and AMPA-Net on four standard CS reconstruction benchmark data sets.\nOur code is available on https://github.com/puallee/AMPA-Net.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 09:39:22 GMT"}, {"version": "v2", "created": "Thu, 15 Oct 2020 05:19:29 GMT"}, {"version": "v3", "created": "Wed, 21 Oct 2020 09:49:15 GMT"}, {"version": "v4", "created": "Thu, 22 Oct 2020 12:31:44 GMT"}, {"version": "v5", "created": "Mon, 9 Nov 2020 03:14:20 GMT"}, {"version": "v6", "created": "Mon, 16 Nov 2020 09:57:39 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Li", "Nanyu", ""], ["Zhou", "Charles C.", ""]]}, {"id": "2010.06931", "submitter": "Stefan Ploner", "authors": "Stefan B. Ploner, Martin F. Kraus, Eric M. Moult, Lennart Husvogt,\n  Julia Schottenhamml, A. Yasin Alibhai, Nadia K. Waheed, Jay S. Duker, James\n  G. Fujimoto, Andreas K. Maier", "title": "Efficient and high accuracy 3-D OCT angiography motion correction in\n  pathology", "comments": "22 pages, 11 figures, 4 tables", "journal-ref": null, "doi": "10.1364/BOE.411117", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel method for non-rigid 3-D motion correction of orthogonally\nraster-scanned optical coherence tomography angiography volumes. This is the\nfirst approach that aligns predominantly axial structural features like retinal\nlayers and transverse angiographic vascular features in a joint optimization.\nCombined with the use of orthogonal scans and favorization of kinematically\nmore plausible displacements, the approach allows subpixel alignment and\nmicrometer-scale distortion correction in all 3 dimensions. As no specific\nstructures or layers are segmented, the approach is by design robust to\npathologic changes. It is furthermore designed for highly parallel\nimplementation and brief runtime, allowing its integration in clinical routine\neven for high density or wide-field scans. We evaluated the algorithm with\nmetrics related to clinically relevant features in a large-scale quantitative\nevaluation based on 204 volumetric scans of 17 subjects including both a wide\nrange of pathologies and healthy controls. Using this method, we achieve\nstate-of-the-art axial performance and show significant advances in both\ntransverse co-alignment and distortion correction, especially in the pathologic\nsubgroup.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 10:20:17 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Ploner", "Stefan B.", ""], ["Kraus", "Martin F.", ""], ["Moult", "Eric M.", ""], ["Husvogt", "Lennart", ""], ["Schottenhamml", "Julia", ""], ["Alibhai", "A. Yasin", ""], ["Waheed", "Nadia K.", ""], ["Duker", "Jay S.", ""], ["Fujimoto", "James G.", ""], ["Maier", "Andreas K.", ""]]}, {"id": "2010.06932", "submitter": "An Tran", "authors": "An Tran, Ali Zonoozi, Jagannadan Varadarajan, Hannes Kruppa", "title": "PP-LinkNet: Improving Semantic Segmentation of High Resolution Satellite\n  Imagery with Multi-stage Training", "comments": null, "journal-ref": null, "doi": "10.1145/3423323.3423407", "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Road network and building footprint extraction is essential for many\napplications such as updating maps, traffic regulations, city planning,\nride-hailing, disaster response \\textit{etc}. Mapping road networks is\ncurrently both expensive and labor-intensive. Recently, improvements in image\nsegmentation through the application of deep neural networks has shown\npromising results in extracting road segments from large scale, high resolution\nsatellite imagery. However, significant challenges remain due to lack of enough\nlabeled training data needed to build models for industry grade applications.\nIn this paper, we propose a two-stage transfer learning technique to improve\nrobustness of semantic segmentation for satellite images that leverages noisy\npseudo ground truth masks obtained automatically (without human labor) from\ncrowd-sourced OpenStreetMap (OSM) data. We further propose Pyramid\nPooling-LinkNet (PP-LinkNet), an improved deep neural network for segmentation\nthat uses focal loss, poly learning rate, and context module. We demonstrate\nthe strengths of our approach through evaluations done on three popular\ndatasets over two tasks, namely, road extraction and building foot-print\ndetection. Specifically, we obtain 78.19\\% meanIoU on SpaceNet building\nfootprint dataset, 67.03\\% and 77.11\\% on the road topology metric on SpaceNet\nand DeepGlobe road extraction dataset, respectively.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 10:23:48 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Tran", "An", ""], ["Zonoozi", "Ali", ""], ["Varadarajan", "Jagannadan", ""], ["Kruppa", "Hannes", ""]]}, {"id": "2010.06935", "submitter": "Yuzhi Wang", "authors": "Yuzhi Wang, Haibin Huang, Qin Xu, Jiaming Liu, Yiqun Liu, Jue Wang", "title": "Practical Deep Raw Image Denoising on Mobile Devices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep learning-based image denoising approaches have been extensively studied\nin recent years, prevailing in many public benchmark datasets. However, the\nstat-of-the-art networks are computationally too expensive to be directly\napplied on mobile devices. In this work, we propose a light-weight, efficient\nneural network-based raw image denoiser that runs smoothly on mainstream mobile\ndevices, and produces high quality denoising results. Our key insights are\ntwofold: (1) by measuring and estimating sensor noise level, a smaller network\ntrained on synthetic sensor-specific data can out-perform larger ones trained\non general data; (2) the large noise level variation under different ISO\nsettings can be removed by a novel k-Sigma Transform, allowing a small network\nto efficiently handle a wide range of noise levels. We conduct extensive\nexperiments to demonstrate the efficiency and accuracy of our approach. Our\nproposed mobile-friendly denoising model runs at ~70 milliseconds per megapixel\non Qualcomm Snapdragon 855 chipset, and it is the basis of the night shot\nfeature of several flagship smartphones released in 2019.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 10:30:32 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Wang", "Yuzhi", ""], ["Huang", "Haibin", ""], ["Xu", "Qin", ""], ["Liu", "Jiaming", ""], ["Liu", "Yiqun", ""], ["Wang", "Jue", ""]]}, {"id": "2010.06939", "submitter": "G\\\"orkem Algan", "authors": "G\\\"orkem Algan, Ilkay Ulusoy, \\c{S}aban G\\\"on\\\"ul, Banu Turgut, Berker\n  Bakbak", "title": "Deep Learning from Small Amount of Medical Data with Noisy Labels: A\n  Meta-Learning Approach", "comments": "Accepted by ICRPOA 2021: International Conference on Retinopathy of\n  Prematurity Ophthalmologic Approach", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer vision systems recently made a big leap thanks to deep neural\nnetworks. However, these systems require correctly labeled large datasets in\norder to be trained properly, which is very difficult to obtain for medical\napplications. Two main reasons for label noise in medical applications are the\nhigh complexity of the data and conflicting opinions of experts. Moreover,\nmedical imaging datasets are commonly tiny, which makes each data very\nimportant in learning. As a result, if not handled properly, label noise\nsignificantly degrades the performance. Therefore, a label-noise-robust\nlearning algorithm that makes use of the meta-learning paradigm is proposed in\nthis article. The proposed solution is tested on retinopathy of prematurity\n(ROP) dataset with a very high label noise of 68%. Results show that the\nproposed algorithm significantly improves the classification algorithm's\nperformance in the presence of noisy labels.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 10:39:44 GMT"}, {"version": "v2", "created": "Mon, 15 Feb 2021 08:42:57 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Algan", "G\u00f6rkem", ""], ["Ulusoy", "Ilkay", ""], ["G\u00f6n\u00fcl", "\u015eaban", ""], ["Turgut", "Banu", ""], ["Bakbak", "Berker", ""]]}, {"id": "2010.06944", "submitter": "Alican Mertan", "authors": "Alican Mertan, Damien Jade Duff, Gozde Unal", "title": "Relative Depth Estimation as a Ranking Problem", "comments": "Accepted at SIU 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a formulation of the relative depth estimation from a single image\nproblem, as a ranking problem. By reformulating the problem this way, we were\nable to utilize literature on the ranking problem, and apply the existing\nknowledge to achieve better results. To this end, we have introduced a listwise\nranking loss borrowed from ranking literature, weighted ListMLE, to the\nrelative depth estimation problem. We have also brought a new metric which\nconsiders pixel depth ranking accuracy, on which our method is stronger.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 10:46:33 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Mertan", "Alican", ""], ["Duff", "Damien Jade", ""], ["Unal", "Gozde", ""]]}, {"id": "2010.06950", "submitter": "Dominik Hirner", "authors": "Dominik Hirner, Friedrich Fraundorfer", "title": "FC-DCNN: A densely connected neural network for stereo estimation", "comments": "This paper has been accepted to the ICPR 2020 conference in Milan\n  which will be held on the 10-15 January 2021. Therefore this work has not yet\n  been presented", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel lightweight network for stereo estimation. Our network\nconsists of a fully-convolutional densely connected neural network (FC-DCNN)\nthat computes matching costs between rectified image pairs. Our FC-DCNN method\nlearns expressive features and performs some simple but effective\npost-processing steps. The densely connected layer structure connects the\noutput of each layer to the input of each subsequent layer. This network\nstructure and the fact that we do not use any fully-connected layers or 3D\nconvolutions leads to a very lightweight network. The output of this network is\nused in order to calculate matching costs and create a cost-volume. Instead of\nusing time and memory-inefficient cost-aggregation methods such as semi-global\nmatching or conditional random fields in order to improve the result, we rely\non filtering techniques, namely median filter and guided filter. By computing a\nleft-right consistency check we get rid of inconsistent values. Afterwards we\nuse a watershed foreground-background segmentation on the disparity image with\nremoved inconsistencies. This mask is then used to refine the final prediction.\nWe show that our method works well for both challenging indoor and outdoor\nscenes by evaluating it on the Middlebury, KITTI and ETH3D benchmarks\nrespectively. Our full framework is available at\nhttps://github.com/thedodo/FC-DCNN\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 10:59:09 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Hirner", "Dominik", ""], ["Fraundorfer", "Friedrich", ""]]}, {"id": "2010.06988", "submitter": "Stefano Ermon", "authors": "Marshall Burke, Anne Driscoll, David B. Lobell, Stefano Ermon", "title": "Using satellite imagery to understand and promote sustainable\n  development", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate and comprehensive measurements of a range of sustainable development\noutcomes are fundamental inputs into both research and policy. We synthesize\nthe growing literature that uses satellite imagery to understand these\noutcomes, with a focus on approaches that combine imagery with machine\nlearning. We quantify the paucity of ground data on key human-related outcomes\nand the growing abundance and resolution (spatial, temporal, and spectral) of\nsatellite imagery. We then review recent machine learning approaches to\nmodel-building in the context of scarce and noisy training data, highlighting\nhow this noise often leads to incorrect assessment of models' predictive\nperformance. We quantify recent model performance across multiple sustainable\ndevelopment domains, discuss research and policy applications, explore\nconstraints to future progress, and highlight key research directions for the\nfield.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2020 05:20:00 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Burke", "Marshall", ""], ["Driscoll", "Anne", ""], ["Lobell", "David B.", ""], ["Ermon", "Stefano", ""]]}, {"id": "2010.07002", "submitter": "David Bouget", "authors": "David Bouget, Andr\\'e Pedersen, Sayied Abdol Mohieb Hosainey, Johanna\n  Vanel, Ole Solheim, Ingerid Reinertsen", "title": "Fast meningioma segmentation in T1-weighted MRI volumes using a\n  lightweight 3D deep learning architecture", "comments": "15 pages, 7 figures, submitted to SPIE journal of Medical Imaging", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic and consistent meningioma segmentation in T1-weighted MRI volumes\nand corresponding volumetric assessment is of use for diagnosis, treatment\nplanning, and tumor growth evaluation. In this paper, we optimized the\nsegmentation and processing speed performances using a large number of both\nsurgically treated meningiomas and untreated meningiomas followed at the\noutpatient clinic. We studied two different 3D neural network architectures:\n(i) a simple encoder-decoder similar to a 3D U-Net, and (ii) a lightweight\nmulti-scale architecture (PLS-Net). In addition, we studied the impact of\ndifferent training schemes. For the validation studies, we used 698 T1-weighted\nMR volumes from St. Olav University Hospital, Trondheim, Norway. The models\nwere evaluated in terms of detection accuracy, segmentation accuracy and\ntraining/inference speed. While both architectures reached a similar Dice score\nof 70% on average, the PLS-Net was more accurate with an F1-score of up to 88%.\nThe highest accuracy was achieved for the largest meningiomas. Speed-wise, the\nPLS-Net architecture tended to converge in about 50 hours while 130 hours were\nnecessary for U-Net. Inference with PLS-Net takes less than a second on GPU and\nabout 15 seconds on CPU. Overall, with the use of mixed precision training, it\nwas possible to train competitive segmentation models in a relatively short\namount of time using the lightweight PLS-Net architecture. In the future, the\nfocus should be brought toward the segmentation of small meningiomas (less than\n2ml) to improve clinical relevance for automatic and early diagnosis as well as\nspeed of growth estimates.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 12:26:53 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Bouget", "David", ""], ["Pedersen", "Andr\u00e9", ""], ["Hosainey", "Sayied Abdol Mohieb", ""], ["Vanel", "Johanna", ""], ["Solheim", "Ole", ""], ["Reinertsen", "Ingerid", ""]]}, {"id": "2010.07021", "submitter": "Jan Bedna\\v{r}\\'ik", "authors": "Zhantao Deng, Jan Bedna\\v{r}\\'ik, Mathieu Salzmann, Pascal Fua", "title": "Better Patch Stitching for Parametric Surface Reconstruction", "comments": "Accepted to 3DV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, parametric mappings have emerged as highly effective surface\nrepresentations, yielding low reconstruction error. In particular, the latest\nworks represent the target shape as an atlas of multiple mappings, which can\nclosely encode object parts. Atlas representations, however, suffer from one\nmajor drawback: The individual mappings are not guaranteed to be consistent,\nwhich results in holes in the reconstructed shape or in jagged surface areas.\n  We introduce an approach that explicitly encourages global consistency of the\nlocal mappings. To this end, we introduce two novel loss terms. The first term\nexploits the surface normals and requires that they remain locally consistent\nwhen estimated within and across the individual mappings. The second term\nfurther encourages better spatial configuration of the mappings by minimizing\nnovel stitching error. We show on standard benchmarks that the use of normal\nconsistency requirement outperforms the baselines quantitatively while\nenforcing better stitching leads to much better visual quality of the\nreconstructed objects as compared to the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 12:37:57 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Deng", "Zhantao", ""], ["Bedna\u0159\u00edk", "Jan", ""], ["Salzmann", "Mathieu", ""], ["Fua", "Pascal", ""]]}, {"id": "2010.07023", "submitter": "David Leslie", "authors": "David Leslie", "title": "Understanding bias in facial recognition technologies", "comments": "49 pages", "journal-ref": null, "doi": "10.5281/zenodo.4050457", "report-no": null, "categories": "cs.CY cs.CV cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Over the past couple of years, the growing debate around automated facial\nrecognition has reached a boiling point. As developers have continued to\nswiftly expand the scope of these kinds of technologies into an almost\nunbounded range of applications, an increasingly strident chorus of critical\nvoices has sounded concerns about the injurious effects of the proliferation of\nsuch systems. Opponents argue that the irresponsible design and use of facial\ndetection and recognition technologies (FDRTs) threatens to violate civil\nliberties, infringe on basic human rights and further entrench structural\nracism and systemic marginalisation. They also caution that the gradual creep\nof face surveillance infrastructures into every domain of lived experience may\neventually eradicate the modern democratic forms of life that have long\nprovided cherished means to individual flourishing, social solidarity and human\nself-creation. Defenders, by contrast, emphasise the gains in public safety,\nsecurity and efficiency that digitally streamlined capacities for facial\nidentification, identity verification and trait characterisation may bring. In\nthis explainer, I focus on one central aspect of this debate: the role that\ndynamics of bias and discrimination play in the development and deployment of\nFDRTs. I examine how historical patterns of discrimination have made inroads\ninto the design and implementation of FDRTs from their very earliest moments.\nAnd, I explain the ways in which the use of biased FDRTs can lead\ndistributional and recognitional injustices. The explainer concludes with an\nexploration of broader ethical questions around the potential proliferation of\npervasive face-based surveillance infrastructures and makes some\nrecommendations for cultivating more responsible approaches to the development\nand governance of these technologies.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 20:45:46 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Leslie", "David", ""]]}, {"id": "2010.07045", "submitter": "Eva Schnider", "authors": "Eva Schnider, Antal Horv\\'ath, Georg Rauter, Azhar Zam, Magdalena\n  M\\\"uller-Gerbl, Philippe C. Cattin", "title": "3D Segmentation Networks for Excessive Numbers of Classes: Distinct Bone\n  Segmentation in Upper Bodies", "comments": "10 pages, 3 figures, 2 tables, accepted into MICCAI 2020\n  International Workshop on Machine Learning in Medical Imaging", "journal-ref": "Machine Learning in Medical Imaging. MLMI 2020. Lecture Notes in\n  Computer Science, vol 12436. Springer, Cham", "doi": "10.1007/978-3-030-59861-7_5", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Segmentation of distinct bones plays a crucial role in diagnosis, planning,\nnavigation, and the assessment of bone metastasis. It supplies semantic\nknowledge to visualisation tools for the planning of surgical interventions and\nthe education of health professionals. Fully supervised segmentation of 3D data\nusing Deep Learning methods has been extensively studied for many tasks but is\nusually restricted to distinguishing only a handful of classes. With 125\ndistinct bones, our case includes many more labels than typical 3D segmentation\ntasks. For this reason, the direct adaptation of most established methods is\nnot possible. This paper discusses the intricacies of training a 3D\nsegmentation network in a many-label setting and shows necessary modifications\nin network architecture, loss function, and data augmentation. As a result, we\ndemonstrate the robustness of our method by automatically segmenting over one\nhundred distinct bones simultaneously in an end-to-end learnt fashion from a\nCT-scan.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 12:54:15 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Schnider", "Eva", ""], ["Horv\u00e1th", "Antal", ""], ["Rauter", "Georg", ""], ["Zam", "Azhar", ""], ["M\u00fcller-Gerbl", "Magdalena", ""], ["Cattin", "Philippe C.", ""]]}, {"id": "2010.07047", "submitter": "Chaoqing Xu", "authors": "Chaoqing Xu, Tyson Neuroth, Takanori Fujiwara, Ronghua Liang, and\n  Kwan-Liu Ma", "title": "A Predictive Visual Analytics System for Studying Neurodegenerative\n  Disease based on DTI Fiber Tracts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diffusion tensor imaging (DTI) has been used to study the effects of\nneurodegenerative diseases on neural pathways, which may lead to more reliable\nand early diagnosis of these diseases as well as a better understanding of how\nthey affect the brain. We introduce an intelligent visual analytics system for\nstudying patient groups based on their labeled DTI fiber tract data and\ncorresponding statistics. The system's AI-augmented interface guides the user\nthrough an organized and holistic analysis space, including the statistical\nfeature space, the physical space, and the space of patients over different\ngroups. We use a custom machine learning pipeline to help narrow down this\nlarge analysis space, and then explore it pragmatically through a range of\nlinked visualizations. We conduct several case studies using real data from the\nresearch database of Parkinson's Progression Markers Initiative.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 06:34:45 GMT"}, {"version": "v2", "created": "Tue, 3 Nov 2020 16:04:08 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Xu", "Chaoqing", ""], ["Neuroth", "Tyson", ""], ["Fujiwara", "Takanori", ""], ["Liang", "Ronghua", ""], ["Ma", "Kwan-Liu", ""]]}, {"id": "2010.07091", "submitter": "Alican Mertan", "authors": "Alican Mertan, Yusuf Huseyin Sahin, Damien Jade Duff, Gozde Unal", "title": "A New Distributional Ranking Loss With Uncertainty: Illustrated in\n  Relative Depth Estimation", "comments": "Accepted at 3DV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new approach for the problem of relative depth estimation from a\nsingle image. Instead of directly regressing over depth scores, we formulate\nthe problem as estimation of a probability distribution over depth and aim to\nlearn the parameters of the distributions which maximize the likelihood of the\ngiven data. To train our model, we propose a new ranking loss, Distributional\nLoss, which tries to increase the probability of farther pixel's depth being\ngreater than the closer pixel's depth. Our proposed approach allows our model\nto output confidence in its estimation in the form of standard deviation of the\ndistribution. We achieve state of the art results against a number of baselines\nwhile providing confidence in our estimations. Our analysis show that estimated\nconfidence is actually a good indicator of accuracy. We investigate the usage\nof confidence information in a downstream task of metric depth estimation, to\nincrease its performance.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 13:47:18 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Mertan", "Alican", ""], ["Sahin", "Yusuf Huseyin", ""], ["Duff", "Damien Jade", ""], ["Unal", "Gozde", ""]]}, {"id": "2010.07092", "submitter": "Renkun Ni", "authors": "Renkun Ni, Micah Goldblum, Amr Sharaf, Kezhi Kong, Tom Goldstein", "title": "Data Augmentation for Meta-Learning", "comments": "15 pages, 3 figures, Accepted to ICML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional image classifiers are trained by randomly sampling mini-batches\nof images. To achieve state-of-the-art performance, practitioners use\nsophisticated data augmentation schemes to expand the amount of training data\navailable for sampling. In contrast, meta-learning algorithms sample support\ndata, query data, and tasks on each training step. In this complex sampling\nscenario, data augmentation can be used not only to expand the number of images\navailable per class, but also to generate entirely new classes/tasks. We\nsystematically dissect the meta-learning pipeline and investigate the distinct\nways in which data augmentation can be integrated at both the image and class\nlevels. Our proposed meta-specific data augmentation significantly improves the\nperformance of meta-learners on few-shot classification benchmarks.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 13:48:22 GMT"}, {"version": "v2", "created": "Tue, 22 Jun 2021 16:06:36 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Ni", "Renkun", ""], ["Goldblum", "Micah", ""], ["Sharaf", "Amr", ""], ["Kong", "Kezhi", ""], ["Goldstein", "Tom", ""]]}, {"id": "2010.07110", "submitter": "Keval Doshi", "authors": "Keval Doshi, Yasin Yilmaz", "title": "Online Anomaly Detection in Surveillance Videos with Asymptotic Bounds\n  on False Alarm Rate", "comments": "Submitted to Pattern Recognition. arXiv admin note: substantial text\n  overlap with arXiv:2004.07941", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anomaly detection in surveillance videos is attracting an increasing amount\nof attention. Despite the competitive performance of recent methods, they lack\ntheoretical performance analysis, particularly due to the complex deep neural\nnetwork architectures used in decision making. Additionally, online decision\nmaking is an important but mostly neglected factor in this domain. Much of the\nexisting methods that claim to be online, depend on batch or offline processing\nin practice. Motivated by these research gaps, we propose an online anomaly\ndetection method in surveillance videos with asymptotic bounds on the false\nalarm rate, which in turn provides a clear procedure for selecting a proper\ndecision threshold that satisfies the desired false alarm rate. Our proposed\nalgorithm consists of a multi-objective deep learning module along with a\nstatistical anomaly detection module, and its effectiveness is demonstrated on\nseveral publicly available data sets where we outperform the state-of-the-art\nalgorithms. All codes are available at\nhttps://github.com/kevaldoshi17/Prediction-based-Video-Anomaly-Detection-.\n", "versions": [{"version": "v1", "created": "Sat, 10 Oct 2020 04:46:16 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Doshi", "Keval", ""], ["Yilmaz", "Yasin", ""]]}, {"id": "2010.07151", "submitter": "Jean-Baptiste Boin", "authors": "Jean-Baptiste Boin, Nat Roth, Jigar Doshi, Pablo Llueca, Nicolas\n  Borensztein", "title": "Multi-class segmentation under severe class imbalance: A case study in\n  roof damage assessment", "comments": "Submitted to the Artificial Intelligence for Humanitarian Assistance\n  and Disaster Response Workshop at NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of roof damage classification and segmentation from overhead imagery\npresents unique challenges. In this work we choose to address the challenge\nposed due to strong class imbalance. We propose four distinct techniques that\naim at mitigating this problem. Through a new scheme that feeds the data to the\nnetwork by oversampling the minority classes, and three other network\narchitectural improvements, we manage to boost the macro-averaged F1-score of a\nmodel by 39.9 percentage points, thus achieving improved segmentation\nperformance, especially on the minority classes.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 15:08:54 GMT"}, {"version": "v2", "created": "Wed, 18 Nov 2020 19:00:48 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Boin", "Jean-Baptiste", ""], ["Roth", "Nat", ""], ["Doshi", "Jigar", ""], ["Llueca", "Pablo", ""], ["Borensztein", "Nicolas", ""]]}, {"id": "2010.07160", "submitter": "Xiangwei Shi", "authors": "Xiangwei Shi, Yunqiang Li, Xin Liu, Jan van Gemert", "title": "WeightAlign: Normalizing Activations by Weight Alignment", "comments": "The first three authors contributed equally; accepted by ICPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Batch normalization (BN) allows training very deep networks by normalizing\nactivations by mini-batch sample statistics which renders BN unstable for small\nbatch sizes. Current small-batch solutions such as Instance Norm, Layer Norm,\nand Group Norm use channel statistics which can be computed even for a single\nsample. Such methods are less stable than BN as they critically depend on the\nstatistics of a single input sample. To address this problem, we propose a\nnormalization of activation without sample statistics. We present WeightAlign:\na method that normalizes the weights by the mean and scaled standard derivation\ncomputed within a filter, which normalizes activations without computing any\nsample statistics. Our proposed method is independent of batch size and stable\nover a wide range of batch sizes. Because weight statistics are orthogonal to\nsample statistics, we can directly combine WeightAlign with any method for\nactivation normalization. We experimentally demonstrate these benefits for\nclassification on CIFAR-10, CIFAR-100, ImageNet, for semantic segmentation on\nPASCAL VOC 2012 and for domain adaptation on Office-31.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 15:25:39 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Shi", "Xiangwei", ""], ["Li", "Yunqiang", ""], ["Liu", "Xin", ""], ["van Gemert", "Jan", ""]]}, {"id": "2010.07184", "submitter": "Zhiwen Cao", "authors": "Zhiwen Cao, Zongcheng Chu, Dongfang Liu, Yingjie Chen", "title": "A Vector-based Representation to Enhance Head Pose Estimation", "comments": "Proceeding with IEEE Winter Conference on Applications of Computer\n  Vision (WACV 2021); 10 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes to use the three vectors in a rotation matrix as the\nrepresentation in head pose estimation and develops a new neural network based\non the characteristic of such representation. We address two potential issues\nexisted in current head pose estimation works: 1. Public datasets for head pose\nestimation use either Euler angles or quaternions to annotate data samples.\nHowever, both of these annotations have the issue of discontinuity and thus\ncould result in some performance issues in neural network training. 2. Most\nresearch works report Mean Absolute Error (MAE) of Euler angles as the\nmeasurement of performance. We show that MAE may not reflect the actual\nbehavior especially for the cases of profile views. To solve these two\nproblems, we propose a new annotation method which uses three vectors to\ndescribe head poses and a new measurement Mean Absolute Error of Vectors (MAEV)\nto assess the performance. We also train a new neural network to predict the\nthree vectors with the constraints of orthogonality. Our proposed method\nachieves state-of-the-art results on both AFLW2000 and BIWI datasets.\nExperiments show our vector-based annotation method can effectively reduce\nprediction errors for large pose angles.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 15:57:29 GMT"}, {"version": "v2", "created": "Tue, 8 Dec 2020 21:44:34 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Cao", "Zhiwen", ""], ["Chu", "Zongcheng", ""], ["Liu", "Dongfang", ""], ["Chen", "Yingjie", ""]]}, {"id": "2010.07210", "submitter": "Yiding Yang", "authors": "Yiding Yang, Jiayan Qiu, Mingli Song, Dacheng Tao, Xinchao Wang", "title": "Learning Propagation Rules for Attribution Map Generation", "comments": "Accepted by ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prior gradient-based attribution-map methods rely on handcrafted propagation\nrules for the non-linear/activation layers during the backward pass, so as to\nproduce gradients of the input and then the attribution map. Despite the\npromising results achieved, such methods are sensitive to the non-informative\nhigh-frequency components and lack adaptability for various models and samples.\nIn this paper, we propose a dedicated method to generate attribution maps that\nallow us to learn the propagation rules automatically, overcoming the flaws of\nthe handcrafted ones. Specifically, we introduce a learnable plugin module,\nwhich enables adaptive propagation rules for each pixel, to the non-linear\nlayers during the backward pass for mask generating. The masked input image is\nthen fed into the model again to obtain new output that can be used as a\nguidance when combined with the original one. The introduced learnable module\ncan be trained under any auto-grad framework with higher-order differential\nsupport. As demonstrated on five datasets and six network architectures, the\nproposed method yields state-of-the-art results and gives cleaner and more\nvisually plausible attribution maps.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 16:23:58 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Yang", "Yiding", ""], ["Qiu", "Jiayan", ""], ["Song", "Mingli", ""], ["Tao", "Dacheng", ""], ["Wang", "Xinchao", ""]]}, {"id": "2010.07215", "submitter": "Dinghao Yang", "authors": "Dinghao Yang, Wei Gao", "title": "PointManifold: Using Manifold Learning for Point Cloud Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a point cloud classification method based on graph\nneural network and manifold learning. Different from the conventional point\ncloud analysis methods, this paper uses manifold learning algorithms to embed\npoint cloud features for better considering the geometric continuity on the\nsurface. Then, the nature of point cloud can be acquired in low dimensional\nspace, and after being concatenated with features in the original\nthree-dimensional (3D)space, both the capability of feature representation and\nthe classification network performance can be improved. We pro-pose two\nmanifold learning modules, where one is based on locally linear embedding\nalgorithm, and the other is a non-linear projection method based on neural\nnetwork architecture. Both of them can obtain better performances than the\nstate-of-the-art baseline. Afterwards, the graph model is constructed by using\nthe k nearest neighbors algorithm, where the edge features are effectively\naggregated for the implementation of point cloud classification. Experiments\nshow that the proposed point cloud classification methods obtain the mean class\naccuracy (mA) of 90.2% and the overall accuracy (oA)of 93.2%, which reach\ncompetitive performances compared with the existing state-of-the-art related\nmethods.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 16:28:19 GMT"}, {"version": "v2", "created": "Fri, 16 Oct 2020 06:32:05 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Yang", "Dinghao", ""], ["Gao", "Wei", ""]]}, {"id": "2010.07217", "submitter": "Xinyu Yang", "authors": "Xinyu Yang, Majid Mirmehdi, Tilo Burghardt", "title": "Back to the Future: Cycle Encoding Prediction for Self-supervised\n  Contrastive Video Representation Learning", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we show that learning video feature spaces in which temporal\ncycles are maximally predictable benefits action classification. In particular,\nwe propose a novel learning approach termed Cycle Encoding Prediction (CEP)\nthat is able to effectively represent high-level spatio-temporal structure of\nunlabelled video content. CEP builds a latent space wherein the concept of\nclosed forward-backward as well as backward-forward temporal loops is\napproximately preserved. As a self-supervision signal, CEP leverages the\nbi-directional temporal coherence of the video stream and applies loss\nfunctions that encourage both temporal cycle closure as well as contrastive\nfeature separation. Architecturally, the underpinning network structure\nutilises a single feature encoder for all video snippets, adding two predictive\nmodules that learn temporal forward and backward transitions. We apply our\nframework for pretext training of networks for action recognition tasks. We\nreport significantly improved results for the standard datasets UCF101 and\nHMDB51. Detailed ablation studies support the effectiveness of the proposed\ncomponents. We publish source code for the CEP components in full with this\npaper.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 16:31:12 GMT"}, {"version": "v2", "created": "Thu, 15 Oct 2020 06:30:06 GMT"}, {"version": "v3", "created": "Tue, 17 Nov 2020 13:30:05 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Yang", "Xinyu", ""], ["Mirmehdi", "Majid", ""], ["Burghardt", "Tilo", ""]]}, {"id": "2010.07219", "submitter": "Vinicius Mesquita de Pinho", "authors": "Vinicius M. de Pinho, Marcello L. R. de Campos, Luis Uzeda Garcia and\n  Dalia Popescu", "title": "Vision-Aided Radio: User Identity Match in Radio and Video Domains Using\n  Machine Learning", "comments": "Accepted for publication in the IEEE Access", "journal-ref": "in IEEE Access, vol. 8, pp. 209619-209629, 2020", "doi": "10.1109/ACCESS.2020.3038926", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  5G is designed to be an essential enabler and a leading infrastructure\nprovider in the communication technology industry by supporting the demand for\nthe growing data traffic and a variety of services with distinct requirements.\nThe use of deep learning and computer vision tools has the means to increase\nthe environmental awareness of the network with information from visual data.\nInformation extracted via computer vision tools such as user position, movement\ndirection, and speed can be promptly available for the network. However, the\nnetwork must have a mechanism to match the identity of a user in both visual\nand radio systems. This mechanism is absent in the present literature.\nTherefore, we propose a framework to match the information from both visual and\nradio domains. This is an essential step to practical applications of computer\nvision tools in communications. We detail the proposed framework training and\ndeployment phases for a presented setup. We carried out practical experiments\nusing data collected in different types of environments. The work compares the\nuse of Deep Neural Network and Random Forest classifiers and shows that the\nformer performed better across all experiments, achieving classification\naccuracy greater than 99%.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 16:32:22 GMT"}, {"version": "v2", "created": "Mon, 16 Nov 2020 12:57:57 GMT"}, {"version": "v3", "created": "Mon, 14 Dec 2020 20:47:52 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["de Pinho", "Vinicius M.", ""], ["de Campos", "Marcello L. R.", ""], ["Garcia", "Luis Uzeda", ""], ["Popescu", "Dalia", ""]]}, {"id": "2010.07222", "submitter": "Ekaterina Kondrateva", "authors": "Ekaterina Kondrateva, Marina Pominova, Elena Popova, Maxim Sharaev,\n  Alexander Bernstein, Evgeny Burnaev", "title": "Domain Shift in Computer Vision models for MRI data analysis: An\n  Overview", "comments": "8 pages, 1 figure", "journal-ref": "ICMV2020", "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Machine learning and computer vision methods are showing good performance in\nmedical imagery analysis. Yetonly a few applications are now in clinical use\nand one of the reasons for that is poor transferability of themodels to data\nfrom different sources or acquisition domains. Development of new methods and\nalgorithms forthe transfer of training and adaptation of the domain in\nmulti-modal medical imaging data is crucial for thedevelopment of accurate\nmodels and their use in clinics. In present work, we overview methods used to\ntackle thedomain shift problem in machine learning and computer vision. The\nalgorithms discussed in this survey includeadvanced data processing, model\narchitecture enhancing and featured training, as well as predicting in\ndomaininvariant latent space. The application of the autoencoding neural\nnetworks and their domain-invariant variationsare heavily discussed in a\nsurvey. We observe the latest methods applied to the magnetic resonance\nimaging(MRI) data analysis and conclude on their performance as well as propose\ndirections for further research.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 16:34:21 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Kondrateva", "Ekaterina", ""], ["Pominova", "Marina", ""], ["Popova", "Elena", ""], ["Sharaev", "Maxim", ""], ["Bernstein", "Alexander", ""], ["Burnaev", "Evgeny", ""]]}, {"id": "2010.07233", "submitter": "Ekaterina Kondrateva", "authors": "Marina Pominova, Ekaterina Kondrateva, Maxim Sharaev, Alexander\n  Bernstein, Evgeny Burnaev", "title": "Fader Networks for domain adaptation on fMRI: ABIDE-II study", "comments": null, "journal-ref": "ICMV2020", "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  ABIDE is the largest open-source autism spectrum disorder database with both\nfMRI data and full phenotype description. These data were extensively studied\nbased on functional connectivity analysis as well as with deep learning on raw\ndata, with top models accuracy close to 75\\% for separate scanning sites. Yet\nthere is still a problem of models transferability between different scanning\nsites within ABIDE. In the current paper, we for the first time perform domain\nadaptation for brain pathology classification problem on raw neuroimaging data.\nWe use 3D convolutional autoencoders to build the domain irrelevant latent\nspace image representation and demonstrate this method to outperform existing\napproaches on ABIDE data.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 16:50:50 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Pominova", "Marina", ""], ["Kondrateva", "Ekaterina", ""], ["Sharaev", "Maxim", ""], ["Bernstein", "Alexander", ""], ["Burnaev", "Evgeny", ""]]}, {"id": "2010.07258", "submitter": "Ali Varamesh", "authors": "Ali Varamesh, Ali Diba, Tinne Tuytelaars, Luc Van Gool", "title": "Self-Supervised Ranking for Representation Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new framework for self-supervised representation learning by\nformulating it as a ranking problem in an image retrieval context on a large\nnumber of random views (augmentations) obtained from images. Our work is based\non two intuitions: first, a good representation of images must yield a\nhigh-quality image ranking in a retrieval task; second, we would expect random\nviews of an image to be ranked closer to a reference view of that image than\nrandom views of other images. Hence, we model representation learning as a\nlearning to rank problem for image retrieval. We train a representation encoder\nby maximizing average precision (AP) for ranking, where random views of an\nimage are considered positively related, and that of the other images\nconsidered negatives. The new framework, dubbed S2R2, enables computing a\nglobal objective on multiple views, compared to the local objective in the\npopular contrastive learning framework, which is calculated on pairs of views.\nIn principle, by using a ranking criterion, we eliminate reliance on\nobject-centric curated datasets. When trained on STL10 and MS-COCO, S2R2\noutperforms SimCLR and the clustering-based contrastive learning model, SwAV,\nwhile being much simpler both conceptually and at implementation. On MS-COCO,\nS2R2 outperforms both SwAV and SimCLR with a larger margin than on STl10. This\nindicates that S2R2 is more effective on diverse scenes and could eliminate the\nneed for an object-centric large training dataset for self-supervised\nrepresentation learning.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 17:24:56 GMT"}, {"version": "v2", "created": "Fri, 20 Nov 2020 15:20:30 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Varamesh", "Ali", ""], ["Diba", "Ali", ""], ["Tuytelaars", "Tinne", ""], ["Van Gool", "Luc", ""]]}, {"id": "2010.07259", "submitter": "Stefan Zwaard S.L.W.", "authors": "Stefan Zwaard, Henk-Jan Boele, Hani Alers, Christos Strydis, Casey\n  Lew-Williams, and Zaid Al-Ars", "title": "Privacy-Preserving Object Detection & Localization Using Distributed\n  Machine Learning: A Case Study of Infant Eyeblink Conditioning", "comments": "This is a preprint version of \"Privacy-Preserving Object Detection &\n  Localization Using Distributed Machine Learning: A Case Study of Infant\n  Eyeblink Conditioning\". This work consists of 12 pages including refs and, 4\n  tables and 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed machine learning is becoming a popular model-training method due\nto privacy, computational scalability, and bandwidth capacities. In this work,\nwe explore scalable distributed-training versions of two algorithms commonly\nused in object detection. A novel distributed training algorithm using Mean\nWeight Matrix Aggregation (MWMA) is proposed for Linear Support Vector Machine\n(L-SVM) object detection based in Histogram of Orientated Gradients (HOG). In\naddition, a novel Weighted Bin Aggregation (WBA) algorithm is proposed for\ndistributed training of Ensemble of Regression Trees (ERT) landmark\nlocalization. Both algorithms do not restrict the location of model aggregation\nand allow custom architectures for model distribution. For this work, a\nPool-Based Local Training and Aggregation (PBLTA) architecture for both\nalgorithms is explored. The application of both algorithms in the medical field\nis examined using a paradigm from the fields of psychology and neuroscience -\neyeblink conditioning with infants - where models need to be trained on facial\nimages while protecting participant privacy. Using distributed learning, models\ncan be trained without sending image data to other nodes. The custom software\nhas been made available for public use on GitHub:\nhttps://github.com/SLWZwaard/DMT. Results show that the aggregation of models\nfor the HOG algorithm using MWMA not only preserves the accuracy of the model\nbut also allows for distributed learning with an accuracy increase of 0.9%\ncompared with traditional learning. Furthermore, WBA allows for ERT model\naggregation with an accuracy increase of 8% when compared to single-node\nmodels.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 17:33:28 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Zwaard", "Stefan", ""], ["Boele", "Henk-Jan", ""], ["Alers", "Hani", ""], ["Strydis", "Christos", ""], ["Lew-Williams", "Casey", ""], ["Al-Ars", "Zaid", ""]]}, {"id": "2010.07284", "submitter": "Vincenzo Ciancia", "authors": "Laura Bussi, Vincenzo Ciancia, Fabio Gadducci", "title": "A spatial model checker in GPU (extended version)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The tool voxlogica merges the state-of-the-art library of computational\nimaging algorithms ITK with the combination of declarative specification and\noptimised execution provided by spatial logic model checking. The analysis of\nan existing benchmark for segmentation of brain tumours via a simple logical\nspecification reached state-of-the-art accuracy. We present a new, GPU-based\nversion of voxlogica and discuss its implementation, scalability, and\napplications.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 17:58:28 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Bussi", "Laura", ""], ["Ciancia", "Vincenzo", ""], ["Gadducci", "Fabio", ""]]}, {"id": "2010.07290", "submitter": "Zaccharie Ramzi", "authors": "Zaccharie Ramzi, Philippe Ciuciu, Jean-Luc Starck", "title": "XPDNet for MRI Reconstruction: an application to the 2020 fastMRI\n  challenge", "comments": "8 pages, 3 figures, presented as an oral to the 2021 ISMRM conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG physics.med-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new neural network, the XPDNet, for MRI reconstruction from\nperiodically under-sampled multi-coil data. We inform the design of this\nnetwork by taking best practices from MRI reconstruction and computer vision.\nWe show that this network can achieve state-of-the-art reconstruction results,\nas shown by its ranking of second in the fastMRI 2020 challenge.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 14:45:00 GMT"}, {"version": "v2", "created": "Wed, 7 Jul 2021 08:57:52 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Ramzi", "Zaccharie", ""], ["Ciuciu", "Philippe", ""], ["Starck", "Jean-Luc", ""]]}, {"id": "2010.07334", "submitter": "Chen Zhu", "authors": "Chen Zhu, Zheng Xu, Ali Shafahi, Manli Shu, Amin Ghiasi, Tom Goldstein", "title": "Towards Accurate Quantization and Pruning via Data-free Knowledge\n  Transfer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When large scale training data is available, one can obtain compact and\naccurate networks to be deployed in resource-constrained environments\neffectively through quantization and pruning. However, training data are often\nprotected due to privacy concerns and it is challenging to obtain compact\nnetworks without data. We study data-free quantization and pruning by\ntransferring knowledge from trained large networks to compact networks.\nAuxiliary generators are simultaneously and adversarially trained with the\ntargeted compact networks to generate synthetic inputs that maximize the\ndiscrepancy between the given large network and its quantized or pruned\nversion. We show theoretically that the alternating optimization for the\nunderlying minimax problem converges under mild conditions for pruning and\nquantization. Our data-free compact networks achieve competitive accuracy to\nnetworks trained and fine-tuned with training data. Our quantized and pruned\nnetworks achieve good performance while being more compact and lightweight.\nFurther, we demonstrate that the compact structure and corresponding\ninitialization from the Lottery Ticket Hypothesis can also help in data-free\ntraining.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 18:02:55 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Zhu", "Chen", ""], ["Xu", "Zheng", ""], ["Shafahi", "Ali", ""], ["Shu", "Manli", ""], ["Ghiasi", "Amin", ""], ["Goldstein", "Tom", ""]]}, {"id": "2010.07347", "submitter": "Changjiang Cai", "authors": "Changjiang Cai, Matteo Poggi, Stefano Mattoccia, Philippos Mordohai", "title": "Matching-space Stereo Networks for Cross-domain Generalization", "comments": "14 pages, 8 figures, International Conference on 3D Vision\n  (3DV'2020), Github code at https://github.com/ccj5351/MS-Nets", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  End-to-end deep networks represent the state of the art for stereo matching.\nWhile excelling on images framing environments similar to the training set,\nmajor drops in accuracy occur in unseen domains (e.g., when moving from\nsynthetic to real scenes). In this paper we introduce a novel family of\narchitectures, namely Matching-Space Networks (MS-Nets), with improved\ngeneralization properties. By replacing learning-based feature extraction from\nimage RGB values with matching functions and confidence measures from\nconventional wisdom, we move the learning process from the color space to the\nMatching Space, avoiding over-specialization to domain specific features.\nExtensive experimental results on four real datasets highlight that our\nproposal leads to superior generalization to unseen environments over\nconventional deep architectures, keeping accuracy on the source domain almost\nunaltered. Our code is available at https://github.com/ccj5351/MS-Nets.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 18:29:20 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Cai", "Changjiang", ""], ["Poggi", "Matteo", ""], ["Mattoccia", "Stefano", ""], ["Mordohai", "Philippos", ""]]}, {"id": "2010.07350", "submitter": "Changjiang Cai", "authors": "Changjiang Cai, Philippos Mordohai", "title": "Do End-to-end Stereo Algorithms Under-utilize Information?", "comments": "13 pages, 10 figures, International Conference on 3D Vision\n  (3DV'2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep networks for stereo matching typically leverage 2D or 3D convolutional\nencoder-decoder architectures to aggregate cost and regularize the cost volume\nfor accurate disparity estimation. Due to content-insensitive convolutions and\ndown-sampling and up-sampling operations, these cost aggregation mechanisms do\nnot take full advantage of the information available in the images. Disparity\nmaps suffer from over-smoothing near occlusion boundaries, and erroneous\npredictions in thin structures. In this paper, we show how deep adaptive\nfiltering and differentiable semi-global aggregation can be integrated in\nexisting 2D and 3D convolutional networks for end-to-end stereo matching,\nleading to improved accuracy. The improvements are due to utilizing RGB\ninformation from the images as a signal to dynamically guide the matching\nprocess, in addition to being the signal we attempt to match across the images.\nWe show extensive experimental results on the KITTI 2015 and Virtual KITTI 2\ndatasets comparing four stereo networks (DispNetC, GCNet, PSMNet and GANet)\nafter integrating four adaptive filters (segmentation-aware bilateral\nfiltering, dynamic filtering networks, pixel adaptive convolution and\nsemi-global aggregation) into their architectures. Our code is available at\nhttps://github.com/ccj5351/DAFStereoNets.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 18:32:39 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Cai", "Changjiang", ""], ["Mordohai", "Philippos", ""]]}, {"id": "2010.07356", "submitter": "Luis Monta\\~nez", "authors": "L. E. Monta\\~nez, L. M. Valent\\'in-Coronado, D. Moctezuma, G. Flores", "title": "Photovoltaic module segmentation and thermal analysis tool from thermal\n  images", "comments": "7 pages, 12 Figures", "journal-ref": null, "doi": "10.1109/ROPEC50909.2020.9258760", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The growing interest in the use of clean energy has led to the construction\nof increasingly large photovoltaic systems. Consequently, monitoring the proper\nfunctioning of these systems has become a highly relevant issue.In this paper,\nautomatic detection, and analysis of photovoltaic modules are proposed. To\nperform the analysis, a module identification step, based on a digital image\nprocessing algorithm, is first carried out. This algorithm consists of image\nenhancement (contrast enhancement, noise reduction, etc.), followed by\nsegmentation of the photovoltaic module. Subsequently, a statistical analysis\nbased on the temperature values of the segmented module is performed.Besides, a\ngraphical user interface has been designed as a potential tool that provides\nrelevant information of the photovoltaic modules.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 18:43:01 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Monta\u00f1ez", "L. E.", ""], ["Valent\u00edn-Coronado", "L. M.", ""], ["Moctezuma", "D.", ""], ["Flores", "G.", ""]]}, {"id": "2010.07360", "submitter": "Hongliang Li", "authors": "Hongliang Li, Manish Bhatt, Zhen Qu, Shiming Zhang, Martin C. Hartel,\n  Ali Khademhosseini, Guy Cloutier", "title": "Deep Learning in Ultrasound Elastography Imaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is known that changes in the mechanical properties of tissues are\nassociated with the onset and progression of certain diseases. Ultrasound\nelastography is a technique to characterize tissue stiffness using ultrasound\nimaging either by measuring tissue strain using quasi-static elastography or\nnatural organ pulsation elastography, or by tracing a propagated shear wave\ninduced by a source or a natural vibration using dynamic elastography. In\nrecent years, deep learning has begun to emerge in ultrasound elastography\nresearch. In this review, several common deep learning frameworks in the\ncomputer vision community, such as multilayer perceptron, convolutional neural\nnetwork, and recurrent neural network are described. Then, recent advances in\nultrasound elastography using such deep learning techniques are revisited in\nterms of algorithm development and clinical diagnosis. Finally, the current\nchallenges and future developments of deep learning in ultrasound elastography\nare prospected.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 18:50:40 GMT"}, {"version": "v2", "created": "Sat, 31 Oct 2020 18:59:13 GMT"}], "update_date": "2020-11-22", "authors_parsed": [["Li", "Hongliang", ""], ["Bhatt", "Manish", ""], ["Qu", "Zhen", ""], ["Zhang", "Shiming", ""], ["Hartel", "Martin C.", ""], ["Khademhosseini", "Ali", ""], ["Cloutier", "Guy", ""]]}, {"id": "2010.07367", "submitter": "Shi-Jie Li", "authors": "Shijie Li, Jinhui Yi, Yazan Abu Farha and Juergen Gall", "title": "Pose Refinement Graph Convolutional Network for Skeleton-based Action\n  Recognition", "comments": "Accepted for publication in IEEE Robotics and Automation Letters\n  (RA-L)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advances in capturing 2D or 3D skeleton data, skeleton-based action\nrecognition has received an increasing interest over the last years. As\nskeleton data is commonly represented by graphs, graph convolutional networks\nhave been proposed for this task. While current graph convolutional networks\naccurately recognize actions, they are too expensive for robotics applications\nwhere limited computational resources are available. In this paper, we\ntherefore propose a highly efficient graph convolutional network that addresses\nthe limitations of previous works. This is achieved by a parallel structure\nthat gradually fuses motion and spatial information and by reducing the\ntemporal resolution as early as possible. Furthermore, we explicitly address\nthe issue that human poses can contain errors. To this end, the network first\nrefines the poses before they are further processed to recognize the action. We\ntherefore call the network Pose Refinement Graph Convolutional Network.\nCompared to other graph convolutional networks, our network requires 86\\%-93\\%\nless parameters and reduces the floating point operations by 89%-96% while\nachieving a comparable accuracy. It therefore provides a much better trade-off\nbetween accuracy, memory footprint and processing time, which makes it suitable\nfor robotics applications.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 19:06:23 GMT"}, {"version": "v2", "created": "Mon, 18 Jan 2021 16:15:31 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Li", "Shijie", ""], ["Yi", "Jinhui", ""], ["Farha", "Yazan Abu", ""], ["Gall", "Juergen", ""]]}, {"id": "2010.07411", "submitter": "Eleni Chiou", "authors": "Eleni Chiou, Francesco Giganti, Shonit Punwani, Iasonas Kokkinos,\n  Eleftheria Panagiotaki", "title": "Harnessing Uncertainty in Domain Adaptation for MRI Prostate Lesion\n  Segmentation", "comments": "Accepted at MICCAI 2020. Code is available at\n  https://github.com/elchiou/DA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The need for training data can impede the adoption of novel imaging\nmodalities for learning-based medical image analysis. Domain adaptation methods\npartially mitigate this problem by translating training data from a related\nsource domain to a novel target domain, but typically assume that a one-to-one\ntranslation is possible. Our work addresses the challenge of adapting to a more\ninformative target domain where multiple target samples can emerge from a\nsingle source sample. In particular we consider translating from mp-MRI to\nVERDICT, a richer MRI modality involving an optimized acquisition protocol for\ncancer characterization. We explicitly account for the inherent uncertainty of\nthis mapping and exploit it to generate multiple outputs conditioned on a\nsingle input. Our results show that this allows us to extract systematically\nbetter image representations for the target domain, when used in tandem with\nboth simple, CycleGAN-based baselines, as well as more powerful approaches that\nintegrate discriminative segmentation losses and/or residual adapters. When\ncompared to its deterministic counterparts, our approach yields substantial\nimprovements across a broad range of dataset sizes, increasingly strong\nbaselines, and evaluation measures.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 21:30:27 GMT"}, {"version": "v2", "created": "Mon, 18 Jan 2021 18:54:13 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Chiou", "Eleni", ""], ["Giganti", "Francesco", ""], ["Punwani", "Shonit", ""], ["Kokkinos", "Iasonas", ""], ["Panagiotaki", "Eleftheria", ""]]}, {"id": "2010.07428", "submitter": "Yinyu Nie", "authors": "Yinyu Nie, Yiqun Lin, Xiaoguang Han, Shihui Guo, Jian Chang, Shuguang\n  Cui, Jian Jun Zhang", "title": "Skeleton-bridged Point Completion: From Global Inference to Local\n  Adjustment", "comments": "Accepted by NeurIPS 2020; Project Page:\n  https://yinyunie.github.io/SKPCN-page/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point completion refers to complete the missing geometries of objects from\npartial point clouds. Existing works usually estimate the missing shape by\ndecoding a latent feature encoded from the input points. However, real-world\nobjects are usually with diverse topologies and surface details, which a latent\nfeature may fail to represent to recover a clean and complete surface. To this\nend, we propose a skeleton-bridged point completion network (SK-PCN) for shape\ncompletion. Given a partial scan, our method first predicts its 3D skeleton to\nobtain the global structure, and completes the surface by learning\ndisplacements from skeletal points. We decouple the shape completion into\nstructure estimation and surface reconstruction, which eases the learning\ndifficulty and benefits our method to obtain on-surface details. Besides,\nconsidering the missing features during encoding input points, SK-PCN adopts a\nlocal adjustment strategy that merges the input point cloud to our predictions\nfor surface refinement. Comparing with previous methods, our skeleton-bridged\nmanner better supports point normal estimation to obtain the full surface mesh\nbeyond point clouds. The qualitative and quantitative experiments on both point\ncloud and mesh completion show that our approach outperforms the existing\nmethods on various object categories.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 22:49:30 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Nie", "Yinyu", ""], ["Lin", "Yiqun", ""], ["Han", "Xiaoguang", ""], ["Guo", "Shihui", ""], ["Chang", "Jian", ""], ["Cui", "Shuguang", ""], ["Zhang", "Jian Jun", ""]]}, {"id": "2010.07432", "submitter": "Alex Tamkin", "authors": "Alex Tamkin, Mike Wu, Noah Goodman", "title": "Viewmaker Networks: Learning Views for Unsupervised Representation\n  Learning", "comments": "ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many recent methods for unsupervised representation learning train models to\nbe invariant to different \"views,\" or distorted versions of an input. However,\ndesigning these views requires considerable trial and error by human experts,\nhindering widespread adoption of unsupervised representation learning methods\nacross domains and modalities. To address this, we propose viewmaker networks:\ngenerative models that learn to produce useful views from a given input.\nViewmakers are stochastic bounded adversaries: they produce views by generating\nand then adding an $\\ell_p$-bounded perturbation to the input, and are trained\nadversarially with respect to the main encoder network. Remarkably, when\npretraining on CIFAR-10, our learned views enable comparable transfer accuracy\nto the well-tuned SimCLR augmentations -- despite not including transformations\nlike cropping or color jitter. Furthermore, our learned views significantly\noutperform baseline augmentations on speech recordings (+9% points, on average)\nand wearable sensor data (+17% points). Viewmakers can also be combined with\nhandcrafted views: they improve robustness to common image corruptions and can\nincrease transfer performance in cases where handcrafted views are less\nexplored. These results suggest that viewmakers may provide a path towards more\ngeneral representation learning algorithms -- reducing the domain expertise and\neffort needed to pretrain on a much wider set of domains. Code is available at\nhttps://github.com/alextamkin/viewmaker.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 23:03:31 GMT"}, {"version": "v2", "created": "Mon, 29 Mar 2021 06:49:09 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Tamkin", "Alex", ""], ["Wu", "Mike", ""], ["Goodman", "Noah", ""]]}, {"id": "2010.07441", "submitter": "Yunhai Han", "authors": "Yunhai Han, Yuhan Liu, David Paz, Henrik Christensen", "title": "Auto-calibration Method Using Stop Signs for Urban Autonomous Driving\n  Applications", "comments": "7 pages, 7 figures, 1 table, Accepted to ICRA 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Calibration of sensors is fundamental to robust performance for intelligent\nvehicles. In natural environments, disturbances can easily challenge\ncalibration. One possibility is to use natural objects of known shape to\nrecalibrate sensors. An approach based on recognition of traffic signs, such as\nstop signs, and use of them for recalibration of cameras is presented. The\napproach is based on detection, geometry estimation, calibration, and recursive\nupdating. Results from natural environments are presented that clearly show\nconvergence and improved performance.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 23:56:47 GMT"}, {"version": "v2", "created": "Thu, 18 Mar 2021 19:01:05 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Han", "Yunhai", ""], ["Liu", "Yuhan", ""], ["Paz", "David", ""], ["Christensen", "Henrik", ""]]}, {"id": "2010.07442", "submitter": "Hera Siddiqui", "authors": "Hera Siddiqui, Ajita Rattani, Dakshina Ranjan Kisku, Tanner Dean", "title": "AI-based BMI Inference from Facial Images: An Application to Weight\n  Monitoring", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-diagnostic image-based methods for healthy weight monitoring is gaining\nincreased interest following the alarming trend of obesity. Only a handful of\nacademic studies exist that investigate AI-based methods for Body Mass Index\n(BMI) inference from facial images as a solution to healthy weight monitoring\nand management. To promote further research and development in this area, we\nevaluate and compare the performance of five different deep-learning based\nConvolutional Neural Network (CNN) architectures i.e., VGG19, ResNet50,\nDenseNet, MobileNet, and lightCNN for BMI inference from facial images.\nExperimental results on the three publicly available BMI annotated facial image\ndatasets assembled from social media, namely, VisualBMI, VIP-Attributes, and\nBollywood datasets, suggest the efficacy of the deep learning methods in BMI\ninference from face images with minimum Mean Absolute Error (MAE) of $1.04$\nobtained using ResNet50.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 00:00:40 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Siddiqui", "Hera", ""], ["Rattani", "Ajita", ""], ["Kisku", "Dakshina Ranjan", ""], ["Dean", "Tanner", ""]]}, {"id": "2010.07445", "submitter": "Fantine Huot", "authors": "Fantine Huot, R. Lily Hu, Matthias Ihme, Qing Wang, John Burge,\n  Tianjian Lu, Jason Hickey, Yi-Fan Chen, John Anderson", "title": "Deep Learning Models for Predicting Wildfires from Historical\n  Remote-Sensing Data", "comments": "Presented at 34th Conference on Neural Information Processing Systems\n  (NeurIPS 2020), Artificial Intelligence for Humani- tarian Assistance and\n  Disaster Response Workshop, Vancouver, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying regions that have high likelihood for wildfires is a key\ncomponent of land and forestry management and disaster preparedness. We create\na data set by aggregating nearly a decade of remote-sensing data and historical\nfire records to predict wildfires. This prediction problem is framed as three\nmachine learning tasks. Results are compared and analyzed for four different\ndeep learning models to estimate wildfire likelihood. The results demonstrate\nthat deep learning models can successfully identify areas of high fire\nlikelihood using aggregated data about vegetation, weather, and topography with\nan AUC of 83%.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 00:27:22 GMT"}, {"version": "v2", "created": "Tue, 1 Dec 2020 00:11:08 GMT"}, {"version": "v3", "created": "Wed, 10 Feb 2021 14:52:42 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Huot", "Fantine", ""], ["Hu", "R. Lily", ""], ["Ihme", "Matthias", ""], ["Wang", "Qing", ""], ["Burge", "John", ""], ["Lu", "Tianjian", ""], ["Hickey", "Jason", ""], ["Chen", "Yi-Fan", ""], ["Anderson", "John", ""]]}, {"id": "2010.07468", "submitter": "Juntang Zhuang", "authors": "Juntang Zhuang, Tommy Tang, Yifan Ding, Sekhar Tatikonda, Nicha\n  Dvornek, Xenophon Papademetris, James S. Duncan", "title": "AdaBelief Optimizer: Adapting Stepsizes by the Belief in Observed\n  Gradients", "comments": null, "journal-ref": "NeurIPS 2020", "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most popular optimizers for deep learning can be broadly categorized as\nadaptive methods (e.g. Adam) and accelerated schemes (e.g. stochastic gradient\ndescent (SGD) with momentum). For many models such as convolutional neural\nnetworks (CNNs), adaptive methods typically converge faster but generalize\nworse compared to SGD; for complex settings such as generative adversarial\nnetworks (GANs), adaptive methods are typically the default because of their\nstability.We propose AdaBelief to simultaneously achieve three goals: fast\nconvergence as in adaptive methods, good generalization as in SGD, and training\nstability. The intuition for AdaBelief is to adapt the stepsize according to\nthe \"belief\" in the current gradient direction. Viewing the exponential moving\naverage (EMA) of the noisy gradient as the prediction of the gradient at the\nnext time step, if the observed gradient greatly deviates from the prediction,\nwe distrust the current observation and take a small step; if the observed\ngradient is close to the prediction, we trust it and take a large step. We\nvalidate AdaBelief in extensive experiments, showing that it outperforms other\nmethods with fast convergence and high accuracy on image classification and\nlanguage modeling. Specifically, on ImageNet, AdaBelief achieves comparable\naccuracy to SGD. Furthermore, in the training of a GAN on Cifar10, AdaBelief\ndemonstrates high stability and improves the quality of generated samples\ncompared to a well-tuned Adam optimizer. Code is available at\nhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 01:46:13 GMT"}, {"version": "v2", "created": "Sat, 24 Oct 2020 00:04:24 GMT"}, {"version": "v3", "created": "Sun, 22 Nov 2020 16:29:28 GMT"}, {"version": "v4", "created": "Sat, 28 Nov 2020 03:01:41 GMT"}, {"version": "v5", "created": "Sun, 20 Dec 2020 22:30:36 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Zhuang", "Juntang", ""], ["Tang", "Tommy", ""], ["Ding", "Yifan", ""], ["Tatikonda", "Sekhar", ""], ["Dvornek", "Nicha", ""], ["Papademetris", "Xenophon", ""], ["Duncan", "James S.", ""]]}, {"id": "2010.07469", "submitter": "Xiangrui Li", "authors": "Yuan Zhou, Xiangrui Li", "title": "Unsupervised Self-training Algorithm Based on Deep Learning for Optical\n  Aerial Images Change Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optical aerial images change detection is an important task in earth\nobservation and has been extensively investigated in the past few decades.\nGenerally, the supervised change detection methods with superior performance\nrequire a large amount of labeled training data which is obtained by manual\nannotation with high cost. In this paper, we present a novel unsupervised\nself-training algorithm (USTA) for optical aerial images change detection. The\ntraditional method such as change vector analysis is used to generate the\npseudo labels. We use these pseudo labels to train a well designed\nconvolutional neural network. The network is used as a teacher to classify the\noriginal multitemporal images to generate another set of pseudo labels. Then\ntwo set of pseudo labels are used to jointly train a student network with the\nsame structure as the teacher. The final change detection result can be\nobtained by the trained student network. Besides, we design an image filter to\ncontrol the usage of change information in the pseudo labels in the training\nprocess of the network. The whole process of the algorithm is an unsupervised\nprocess without manually marked labels. Experimental results on the real\ndatasets demonstrate competitive performance of our proposed method.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 01:51:46 GMT"}, {"version": "v2", "created": "Thu, 22 Oct 2020 07:28:12 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Zhou", "Yuan", ""], ["Li", "Xiangrui", ""]]}, {"id": "2010.07485", "submitter": "Jia Guo", "authors": "Jia Guo, Minghao Chen, Yao Hu, Chen Zhu, Xiaofei He, Deng Cai", "title": "Reducing the Teacher-Student Gap via Spherical Knowledge Disitllation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge distillation aims at obtaining a compact and effective model by\nlearning the mapping function from a much larger one. Due to the limited\ncapacity of the student, the student would underfit the teacher. Therefore,\nstudent performance would unexpectedly drop when distilling from an oversized\nteacher, termed the capacity gap problem. We investigate this problem by study\nthe gap of confidence between teacher and student. We find that the magnitude\nof confidence is not necessary for knowledge distillation and could harm the\nstudent performance if the student are forced to learn confidence. We propose\nSpherical Knowledge Distillation to eliminate this gap explicitly, which eases\nthe underfitting problem. We find this novel knowledge representation can\nimprove compact models with much larger teachers and is robust to temperature.\nWe conducted experiments on both CIFAR100 and ImageNet, and achieve significant\nimprovement. Specifically, we train ResNet18 to 73.0 accuracy, which is a\nsubstantial improvement over previous SOTA and is on par with resnet34 almost\ntwice the student size. The implementation has been shared at\nhttps://github.com/forjiuzhou/Spherical-Knowledge-Distillation.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 03:03:36 GMT"}, {"version": "v2", "created": "Thu, 29 Oct 2020 09:19:46 GMT"}, {"version": "v3", "created": "Thu, 19 Nov 2020 10:19:36 GMT"}, {"version": "v4", "created": "Mon, 21 Dec 2020 12:34:30 GMT"}, {"version": "v5", "created": "Tue, 12 Jan 2021 08:46:08 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Guo", "Jia", ""], ["Chen", "Minghao", ""], ["Hu", "Yao", ""], ["Zhu", "Chen", ""], ["He", "Xiaofei", ""], ["Cai", "Deng", ""]]}, {"id": "2010.07486", "submitter": "Lei Mou", "authors": "Lei Mou, Yitian Zhao, Huazhu Fu, Yonghuai Liu, Jun Cheng, Yalin Zheng,\n  Pan Su, Jianlong Yang, Li Chen, Alejandro F Frang, Masahiro Akiba, Jiang Liu", "title": "CS2-Net: Deep Learning Segmentation of Curvilinear Structures in Medical\n  Imaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated detection of curvilinear structures, e.g., blood vessels or nerve\nfibres, from medical and biomedical images is a crucial early step in automatic\nimage interpretation associated to the management of many diseases. Precise\nmeasurement of the morphological changes of these curvilinear organ structures\ninforms clinicians for understanding the mechanism, diagnosis, and treatment of\ne.g. cardiovascular, kidney, eye, lung, and neurological conditions. In this\nwork, we propose a generic and unified convolution neural network for the\nsegmentation of curvilinear structures and illustrate in several 2D/3D medical\nimaging modalities. We introduce a new curvilinear structure segmentation\nnetwork (CS2-Net), which includes a self-attention mechanism in the encoder and\ndecoder to learn rich hierarchical representations of curvilinear structures.\nTwo types of attention modules - spatial attention and channel attention - are\nutilized to enhance the inter-class discrimination and intra-class\nresponsiveness, to further integrate local features with their global\ndependencies and normalization, adaptively. Furthermore, to facilitate the\nsegmentation of curvilinear structures in medical images, we employ a 1x3 and a\n3x1 convolutional kernel to capture boundary features. ...\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 03:06:37 GMT"}, {"version": "v2", "created": "Mon, 19 Oct 2020 14:39:41 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Mou", "Lei", ""], ["Zhao", "Yitian", ""], ["Fu", "Huazhu", ""], ["Liu", "Yonghuai", ""], ["Cheng", "Jun", ""], ["Zheng", "Yalin", ""], ["Su", "Pan", ""], ["Yang", "Jianlong", ""], ["Chen", "Li", ""], ["Frang", "Alejandro F", ""], ["Akiba", "Masahiro", ""], ["Liu", "Jiang", ""]]}, {"id": "2010.07488", "submitter": "Shounak Datta", "authors": "Shounak Datta and Eduardo B. Mariottoni and David Dov and Alessandro\n  A. Jammal and Lawrence Carin and Felipe A. Medeiros", "title": "RetiNerveNet: Using Recursive Deep Learning to Estimate Pointwise 24-2\n  Visual Field Data based on Retinal Structure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Glaucoma is the leading cause of irreversible blindness in the world,\naffecting over 70 million people. The cumbersome Standard Automated Perimetry\n(SAP) test is most frequently used to detect visual loss due to glaucoma. Due\nto the SAP test's innate difficulty and its high test-retest variability, we\npropose the RetiNerveNet, a deep convolutional recursive neural network for\nobtaining estimates of the SAP visual field. RetiNerveNet uses information from\nthe more objective Spectral-Domain Optical Coherence Tomography (SDOCT).\nRetiNerveNet attempts to trace-back the arcuate convergence of the retinal\nnerve fibers, starting from the Retinal Nerve Fiber Layer (RNFL) thickness\naround the optic disc, to estimate individual age-corrected 24-2 SAP values.\nRecursive passes through the proposed network sequentially yield estimates of\nthe visual locations progressively farther from the optic disc. While all the\nmethods used for our experiments exhibit lower performance for the advanced\ndisease group, the proposed network is observed to be more accurate than all\nthe baselines for estimating the individual visual field values. We further\naugment RetiNerveNet to additionally predict the SAP Mean Deviation values and\nalso create an ensemble of RetiNerveNets that further improves the performance,\nby increasingly weighting-up underrepresented parts of the training data.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 03:09:08 GMT"}, {"version": "v2", "created": "Sun, 20 Jun 2021 00:00:13 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Datta", "Shounak", ""], ["Mariottoni", "Eduardo B.", ""], ["Dov", "David", ""], ["Jammal", "Alessandro A.", ""], ["Carin", "Lawrence", ""], ["Medeiros", "Felipe A.", ""]]}, {"id": "2010.07492", "submitter": "Kai Zhang", "authors": "Kai Zhang, Gernot Riegler, Noah Snavely, Vladlen Koltun", "title": "NeRF++: Analyzing and Improving Neural Radiance Fields", "comments": "Code is available at https://github.com/Kai-46/nerfplusplus; fix a\n  minor formatting issue in Fig. 4", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural Radiance Fields (NeRF) achieve impressive view synthesis results for a\nvariety of capture settings, including 360 capture of bounded scenes and\nforward-facing capture of bounded and unbounded scenes. NeRF fits multi-layer\nperceptrons (MLPs) representing view-invariant opacity and view-dependent color\nvolumes to a set of training images, and samples novel views based on volume\nrendering techniques. In this technical report, we first remark on radiance\nfields and their potential ambiguities, namely the shape-radiance ambiguity,\nand analyze NeRF's success in avoiding such ambiguities. Second, we address a\nparametrization issue involved in applying NeRF to 360 captures of objects\nwithin large-scale, unbounded 3D scenes. Our method improves view synthesis\nfidelity in this challenging scenario. Code is available at\nhttps://github.com/Kai-46/nerfplusplus.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 03:24:14 GMT"}, {"version": "v2", "created": "Wed, 21 Oct 2020 18:53:21 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Zhang", "Kai", ""], ["Riegler", "Gernot", ""], ["Snavely", "Noah", ""], ["Koltun", "Vladlen", ""]]}, {"id": "2010.07510", "submitter": "Guanjie Huang", "authors": "Shao Wei Wang, Guan Jie Huang, Xiang Yu Luo", "title": "A Human Eye-based Text Color Scheme Generation Method for Image\n  Synthesis", "comments": "8 pages, 6 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Synthetic data used for scene text detection and recognition tasks have\nproven effective. However, there are still two problems: First, the color\nschemes used for text coloring in the existing methods are relatively fixed\ncolor key-value pairs learned from real datasets. The dirty data in real\ndatasets may cause the problem that the colors of text and background are too\nsimilar to be distinguished from each other. Second, the generated texts are\nuniformly limited to the same depth of a picture, while there are special cases\nin the real world that text may appear across depths. To address these\nproblems, in this paper we design a novel method to generate color schemes,\nwhich are consistent with the characteristics of human eyes to observe things.\nThe advantages of our method are as follows: (1) overcomes the color confusion\nproblem between text and background caused by dirty data; (2) the texts\ngenerated are allowed to appear in most locations of any image, even across\ndepths; (3) avoids analyzing the depth of background, such that the performance\nof our method exceeds the state-of-the-art methods; (4) the speed of generating\nimages is fast, nearly one picture generated per three milliseconds. The\neffectiveness of our method is verified on several public datasets.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 04:24:08 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Wang", "Shao Wei", ""], ["Huang", "Guan Jie", ""], ["Luo", "Xiang Yu", ""]]}, {"id": "2010.07524", "submitter": "MyeongAh Cho", "authors": "MyeongAh Cho, Taeoh Kim, Ig-Jae Kim and Sangyoun Lee", "title": "Unsupervised Video Anomaly Detection via Normalizing Flows with Implicit\n  Latent Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Surveillance anomaly detection searches for anomalous events, such as crimes\nor accidents, among normal scenes. Because it occurs rarely, most training data\nconsists of unlabeled, normal videos, which makes the task challenging. Most\nexisting methods use an autoencoder (AE) to learn reconstructing normal videos\nand detect anomalies by a failure to reconstruct the appearance of abnormal\nscenes. However, because anomalies are distinguished by appearance or motion,\nmany previous approaches have explicitly separated appearance and motion\ninformation--for example, using a pre-trained optical flow model. This explicit\nseparation restricts reciprocal representation capabilities between two\ninformation. In contrast, we propose an implicit two-path AE (ITAE), a\nstructure in which two encoders implicitly model appearance and motion\nfeatures, and a single decoder that combines them to learn normal video\npatterns. For the complex distribution of normal scenes, we suggest normal\ndensity estimation of ITAE features through normalizing flow (NF)-based\ngenerative models to learn the tractable likelihoods and find anomalies using\nout-of-distribution detection. NF models intensify ITAE performance by learning\nnormality through implicitly learned features. Finally, we demonstrate the\neffectiveness of ITAE and its feature distribution modeling in three\nbenchmarks, especially on the Shanghai Tech Campus (ST) database composed of\nvarious anomalies in real-world scenarios.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 05:02:02 GMT"}, {"version": "v2", "created": "Fri, 19 Mar 2021 09:09:27 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Cho", "MyeongAh", ""], ["Kim", "Taeoh", ""], ["Kim", "Ig-Jae", ""], ["Lee", "Sangyoun", ""]]}, {"id": "2010.07526", "submitter": "Ana Marasovi\\'c", "authors": "Ana Marasovi\\'c, Chandra Bhagavatula, Jae Sung Park, Ronan Le Bras,\n  Noah A. Smith, Yejin Choi", "title": "Natural Language Rationales with Full-Stack Visual Reasoning: From\n  Pixels to Semantic Frames to Commonsense Graphs", "comments": "Accepted to Findings of EMNLP", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural language rationales could provide intuitive, higher-level\nexplanations that are easily understandable by humans, complementing the more\nbroadly studied lower-level explanations based on gradients or attention\nweights. We present the first study focused on generating natural language\nrationales across several complex visual reasoning tasks: visual commonsense\nreasoning, visual-textual entailment, and visual question answering. The key\nchallenge of accurate rationalization is comprehensive image understanding at\nall levels: not just their explicit content at the pixel level, but their\ncontextual contents at the semantic and pragmatic levels. We present\nRationale^VT Transformer, an integrated model that learns to generate free-text\nrationales by combining pretrained language models with object recognition,\ngrounded visual semantic frames, and visual commonsense graphs. Our experiments\nshow that the base pretrained language model benefits from visual adaptation\nand that free-text rationalization is a promising research direction to\ncomplement model interpretability for complex visual-textual reasoning tasks.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 05:08:56 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Marasovi\u0107", "Ana", ""], ["Bhagavatula", "Chandra", ""], ["Park", "Jae Sung", ""], ["Bras", "Ronan Le", ""], ["Smith", "Noah A.", ""], ["Choi", "Yejin", ""]]}, {"id": "2010.07539", "submitter": "Jiaolong Xu", "authors": "L. Xiao, J. Xu, D. Zhao, Z. Wang, L. Wang, Y. Nie, B. Dai", "title": "Self-Supervised Domain Adaptation with Consistency Training", "comments": "ICPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of unsupervised domain adaptation for image\nclassification. To learn target-domain-aware features from the unlabeled data,\nwe create a self-supervised pretext task by augmenting the unlabeled data with\na certain type of transformation (specifically, image rotation) and ask the\nlearner to predict the properties of the transformation. However, the obtained\nfeature representation may contain a large amount of irrelevant information\nwith respect to the main task. To provide further guidance, we force the\nfeature representation of the augmented data to be consistent with that of the\noriginal data. Intuitively, the consistency introduces additional constraints\nto representation learning, therefore, the learned representation is more\nlikely to focus on the right information about the main task. Our experimental\nresults validate the proposed method and demonstrate state-of-the-art\nperformance on classical domain adaptation benchmarks. Code is available at\nhttps://github.com/Jiaolong/ss-da-consistency.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 06:03:47 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Xiao", "L.", ""], ["Xu", "J.", ""], ["Zhao", "D.", ""], ["Wang", "Z.", ""], ["Wang", "L.", ""], ["Nie", "Y.", ""], ["Dai", "B.", ""]]}, {"id": "2010.07544", "submitter": "Masakazu Yoshimura", "authors": "Masakazu Yoshimura and Satoshi Ogata", "title": "FOSS: Multi-Person Age Estimation with Focusing on Objects and Still\n  Seeing Surroundings", "comments": "The precision and speed are improved from previous version with\n  modified loss function and image size", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Age estimation from images can be used in many practical scenes. Most of the\nprevious works targeted on the estimation from images in which only one face\nexists. Also, most of the open datasets for age estimation contain images like\nthat. However, in some situations, age estimation in the wild and for\nmulti-person is needed. Usually, such situations were solved by two separate\nmodels; one is a face detector model which crops facial regions and the other\nis an age estimation model which estimates from cropped images. In this work,\nwe propose a method that can detect and estimate the age of multi-person with a\nsingle model which estimates age with focusing on faces and still seeing\nsurroundings. Also, we propose a training method which enables the model to\nestimate multi-person well despite trained with images in which only one face\nis photographed. In the experiments, we evaluated our proposed method compared\nwith the traditional approach using two separate models. As the result, the\naccuracy could be enhanced with our proposed method. We also adapted our\nproposed model to commonly used single person photographed age estimation\ndatasets and it is proved that our method is also effective to those images and\noutperforms the state of the art accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 06:38:16 GMT"}, {"version": "v2", "created": "Fri, 19 Feb 2021 03:47:55 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Yoshimura", "Masakazu", ""], ["Ogata", "Satoshi", ""]]}, {"id": "2010.07548", "submitter": "Patrick Dendorfer", "authors": "Patrick Dendorfer and Aljo\\v{s}a O\\v{s}ep and Anton Milan and Konrad\n  Schindler and Daniel Cremers and Ian Reid and Stefan Roth and Laura\n  Leal-Taix\\'e", "title": "MOTChallenge: A Benchmark for Single-Camera Multiple Target Tracking", "comments": "Accepted at IJCV", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Standardized benchmarks have been crucial in pushing the performance of\ncomputer vision algorithms, especially since the advent of deep learning.\nAlthough leaderboards should not be over-claimed, they often provide the most\nobjective measure of performance and are therefore important guides for\nresearch. We present MOTChallenge, a benchmark for single-camera Multiple\nObject Tracking (MOT) launched in late 2014, to collect existing and new data,\nand create a framework for the standardized evaluation of multiple object\ntracking methods. The benchmark is focused on multiple people tracking, since\npedestrians are by far the most studied object in the tracking community, with\napplications ranging from robot navigation to self-driving cars. This paper\ncollects the first three releases of the benchmark: (i) MOT15, along with\nnumerous state-of-the-art results that were submitted in the last years, (ii)\nMOT16, which contains new challenging videos, and (iii) MOT17, that extends\nMOT16 sequences with more precise labels and evaluates tracking performance on\nthree different object detectors. The second and third release not only offers\na significant increase in the number of labeled boxes but also provide labels\nfor multiple object classes beside pedestrians, as well as the level of\nvisibility for every single object of interest. We finally provide a\ncategorization of state-of-the-art trackers and a broad error analysis. This\nwill help newcomers understand the related work and research trends in the MOT\ncommunity, and hopefully shed some light on potential future research\ndirections.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 06:52:16 GMT"}, {"version": "v2", "created": "Tue, 8 Dec 2020 09:10:53 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Dendorfer", "Patrick", ""], ["O\u0161ep", "Aljo\u0161a", ""], ["Milan", "Anton", ""], ["Schindler", "Konrad", ""], ["Cremers", "Daniel", ""], ["Reid", "Ian", ""], ["Roth", "Stefan", ""], ["Leal-Taix\u00e9", "Laura", ""]]}, {"id": "2010.07556", "submitter": "Evgenii Sovetkin", "authors": "Evgenii Sovetkin and Elbert Jan Achterberg and Thomas Weber, and Bart\n  E. Pieters", "title": "Encoder-decoder semantic segmentation models for electroluminescence\n  images of thin-film photovoltaic modules", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a series of image segmentation methods based on the deep neural\nnetworks in order to perform semantic segmentation of electroluminescence (EL)\nimages of thin-film modules. We utilize the encoder-decoder deep neural network\narchitecture. The framework is general such that it can easily be extended to\nother types of images (e.g. thermography) or solar cell technologies (e.g.\ncrystalline silicon modules). The networks are trained and tested on a sample\nof images from a database with 6000 EL images of Copper Indium Gallium\nDiselenide (CIGS) thin film modules. We selected two types of features to\nextract, shunts and so called \"droplets\". The latter feature is often observed\nin the set of images. Several models are tested using various combinations of\nencoder-decoder layers, and a procedure is proposed to select the best model.\nWe show exemplary results with the best selected model. Furthermore, we applied\nthe best model to the full set of 6000 images and demonstrate that the\nautomated segmentation of EL images can reveal many subtle features which\ncannot be inferred from studying a small sample of images. We believe these\nfeatures can contribute to process optimization and quality control.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 07:09:16 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Sovetkin", "Evgenii", ""], ["Achterberg", "Elbert Jan", ""], ["Weber", "Thomas", ""], ["Pieters", "Bart E.", ""]]}, {"id": "2010.07581", "submitter": "Mazeyar Moeini Feizabadi", "authors": "Mazeyar Moeini Feizabadi, Ali Mohammed Shujjat, Sarah Shahid, Zainab\n  Hasnain (Habib University)", "title": "Interactive Latent Interpolation on MNIST Dataset", "comments": "For associated demonstration and code repository, see\n  https://mazy1998.github.io/browserGAN/ and\n  https://github.com/mazy1998/browserGAN respectively", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  This paper will discuss the potential of dimensionality reduction with a\nweb-based use of GANs. Throughout a variety of experiments, we show\nsynthesizing visually-appealing samples, interpolating meaningfully between\nsamples, and performing linear arithmetic with latent vectors. GANs have proved\nto be a remarkable technique to produce computer-generated images, very similar\nto an original image. This is primarily useful when coupled with dimensionality\nreduction as an effective application of our algorithm. We proposed a new\narchitecture for GANs, which ended up not working for mathematical reasons\nlater explained. We then proposed a new web-based GAN that still takes\nadvantage of dimensionality reduction to speed generation in the browser to .2\nmilliseconds. Lastly, we made a modern UI with linear interpolation to present\nthe work. With the speedy generation, we can generate so fast that we can\ncreate an animation type effect that we have never seen before that works on\nboth web and mobile.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 08:04:48 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Feizabadi", "Mazeyar Moeini", "", "Habib University"], ["Shujjat", "Ali Mohammed", "", "Habib University"], ["Shahid", "Sarah", "", "Habib University"], ["Hasnain", "Zainab", "", "Habib University"]]}, {"id": "2010.07591", "submitter": "Ziqi Wang", "authors": "Ziqi Wang, Marco Loog, Jan van Gemert", "title": "Respecting Domain Relations: Hypothesis Invariance for Domain\n  Generalization", "comments": "accepted at ICPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In domain generalization, multiple labeled non-independent and\nnon-identically distributed source domains are available during training while\nneither the data nor the labels of target domains are. Currently, learning\nso-called domain invariant representations (DIRs) is the prevalent approach to\ndomain generalization. In this work, we define DIRs employed by existing works\nin probabilistic terms and show that by learning DIRs, overly strict\nrequirements are imposed concerning the invariance. Particularly, DIRs aim to\nperfectly align representations of different domains, i.e. their input\ndistributions. This is, however, not necessary for good generalization to a\ntarget domain and may even dispose of valuable classification information. We\npropose to learn so-called hypothesis invariant representations (HIRs), which\nrelax the invariance assumptions by merely aligning posteriors, instead of\naligning representations. We report experimental results on public domain\ngeneralization datasets to show that learning HIRs is more effective than\nlearning DIRs. In fact, our approach can even compete with approaches using\nprior knowledge about domains.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 08:26:08 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Wang", "Ziqi", ""], ["Loog", "Marco", ""], ["van Gemert", "Jan", ""]]}, {"id": "2010.07605", "submitter": "Yuan Liu", "authors": "Yuan Liu, Ruoteng Li, Robby T. Tan, Yu Cheng, Xiubao Sui", "title": "Object Tracking Using Spatio-Temporal Future Prediction", "comments": "12 pages, 11 figures, journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Occlusion is a long-standing problem that causes many modern tracking methods\nto be erroneous. In this paper, we address the occlusion problem by exploiting\nthe current and future possible locations of the target object from its past\ntrajectory. To achieve this, we introduce a learning-based tracking method that\ntakes into account background motion modeling and trajectory prediction. Our\ntrajectory prediction module predicts the target object's locations in the\ncurrent and future frames based on the object's past trajectory. Since, in the\ninput video, the target object's trajectory is not only affected by the object\nmotion but also the camera motion, our background motion module estimates the\ncamera motion. So that the object's trajectory can be made independent from it.\nTo dynamically switch between the appearance-based tracker and the trajectory\nprediction, we employ a network that can assess how good a tracking prediction\nis, and we use the assessment scores to choose between the appearance-based\ntracker's prediction and the trajectory-based prediction. Comprehensive\nevaluations show that the proposed method sets a new state-of-the-art\nperformance on commonly used tracking benchmarks.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 09:02:50 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Liu", "Yuan", ""], ["Li", "Ruoteng", ""], ["Tan", "Robby T.", ""], ["Cheng", "Yu", ""], ["Sui", "Xiubao", ""]]}, {"id": "2010.07608", "submitter": "Bo Pang", "authors": "Bo Pang, Deming Zhai, Junjun Jiang, Xianming Liu", "title": "Fully Unsupervised Person Re-identification viaSelective Contrastive\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification (ReID) aims at searching the same identity person\namong images captured by various cameras. Unsupervised person ReID attracts a\nlot of attention recently, due to it works without intensive manual annotation\nand thus shows great potential of adapting to new conditions. Representation\nlearning plays a critical role in unsupervised person ReID. In this work, we\npropose a novel selective contrastive learning framework for unsupervised\nfeature learning. Specifically, different from traditional contrastive learning\nstrategies, we propose to use multiple positives and adaptively sampled\nnegatives for defining the contrastive loss, enabling to learn a feature\nembedding model with stronger identity discriminative representation. Moreover,\nwe propose to jointly leverage global and local features to construct three\ndynamic dictionaries, among which the global and local memory banks are used\nfor pairwise similarity computation and the mixture memory bank are used for\ncontrastive loss definition. Experimental results demonstrate the superiority\nof our method in unsupervised person ReID compared with the state-of-the-arts.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 09:09:23 GMT"}, {"version": "v2", "created": "Thu, 4 Mar 2021 02:37:28 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Pang", "Bo", ""], ["Zhai", "Deming", ""], ["Jiang", "Junjun", ""], ["Liu", "Xianming", ""]]}, {"id": "2010.07614", "submitter": "Estephe Arnaud", "authors": "Estephe Arnaud, Arnaud Dapogny, Kevin Bailly", "title": "THIN: THrowable Information Networks and Application for Facial\n  Expression Recognition In The Wild", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a number of machine learning problems, an exogenous variable can be\nidentified such that it heavily influences the appearance of the different\nclasses, and an ideal classifier should be invariant to this variable. An\nexample of such exogenous variable is identity if facial expression recognition\n(FER) is considered. In this paper, we propose a dual exogenous/endogenous\nrepresentation. The former captures the exogenous variable whereas the second\none models the task at hand (e.g. facial expression). We design a prediction\nlayer that uses a tree-gated deep ensemble conditioned by the exogenous\nrepresentation. We also propose an exogenous dispelling loss to remove the\nexogenous information from the endogenous representation. Thus, the exogenous\ninformation is used two times in a throwable fashion, first as a conditioning\nvariable for the target task, and second to create invariance within the\nendogenous representation. We call this method THIN, standing for THrowable\nInformation Networks. We experimentally validate THIN in several contexts where\nan exogenous information can be identified, such as digit recognition under\nlarge rotations and shape recognition at multiple scales. We also apply it to\nFER with identity as the exogenous variable. We demonstrate that THIN\nsignificantly outperforms state-of-the-art approaches on several challenging\ndatasets.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 09:20:31 GMT"}, {"version": "v2", "created": "Wed, 21 Apr 2021 08:49:22 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Arnaud", "Estephe", ""], ["Dapogny", "Arnaud", ""], ["Bailly", "Kevin", ""]]}, {"id": "2010.07621", "submitter": "Pengcheng Yuan", "authors": "Pengcheng Yuan, Shufei Lin, Cheng Cui, Yuning Du, Ruoyu Guo, Dongliang\n  He, Errui Ding and Shumin Han", "title": "HS-ResNet: Hierarchical-Split Block on Convolutional Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses representational block named Hierarchical-Split Block,\nwhich can be taken as a plug-and-play block to upgrade existing convolutional\nneural networks, improves model performance significantly in a network.\nHierarchical-Split Block contains many hierarchical split and concatenate\nconnections within one single residual block. We find multi-scale features is\nof great importance for numerous vision tasks. Moreover, Hierarchical-Split\nblock is very flexible and efficient, which provides a large space of potential\nnetwork architectures for different applications. In this work, we present a\ncommon backbone based on Hierarchical-Split block for tasks: image\nclassification, object detection, instance segmentation and semantic image\nsegmentation/parsing. Our approach shows significant improvements over all\nthese core tasks in comparison with the baseline. As shown in Figure1, for\nimage classification, our 50-layers network(HS-ResNet50) achieves 81.28% top-1\naccuracy with competitive latency on ImageNet-1k dataset. It also outperforms\nmost state-of-the-art models. The source code and models will be available on:\nhttps://github.com/PaddlePaddle/PaddleClas\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 09:32:38 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Yuan", "Pengcheng", ""], ["Lin", "Shufei", ""], ["Cui", "Cheng", ""], ["Du", "Yuning", ""], ["Guo", "Ruoyu", ""], ["He", "Dongliang", ""], ["Ding", "Errui", ""], ["Han", "Shumin", ""]]}, {"id": "2010.07646", "submitter": "Berta Besc\\'os Torcal", "authors": "Berta Bescos, Cesar Cadena, Jose Neira", "title": "Empty Cities: a Dynamic-Object-Invariant Space for Visual SLAM", "comments": null, "journal-ref": null, "doi": "10.1109/TRO.2020.3031267", "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a data-driven approach to obtain the static image of\na scene, eliminating dynamic objects that might have been present at the time\nof traversing the scene with a camera. The general objective is to improve\nvision-based localization and mapping tasks in dynamic environments, where the\npresence (or absence) of different dynamic objects in different moments makes\nthese tasks less robust. We introduce an end-to-end deep learning framework to\nturn images of an urban environment that include dynamic content, such as\nvehicles or pedestrians, into realistic static frames suitable for localization\nand mapping. This objective faces two main challenges: detecting the dynamic\nobjects, and inpainting the static occluded back-ground. The first challenge is\naddressed by the use of a convolutional network that learns a multi-class\nsemantic segmentation of the image. The second challenge is approached with a\ngenerative adversarial model that, taking as input the original dynamic image\nand the computed dynamic/static binary mask, is capable of generating the final\nstatic image. This framework makes use of two new losses, one based on image\nsteganalysis techniques, useful to improve the inpainting quality, and another\none based on ORB features, designed to enhance feature matching between real\nand hallucinated image regions. To validate our approach, we perform an\nextensive evaluation on different tasks that are affected by dynamic entities,\ni.e., visual odometry, place recognition and multi-view stereo, with the\nhallucinated images. Code has been made available on\nhttps://github.com/bertabescos/EmptyCities_SLAM.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 10:31:12 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Bescos", "Berta", ""], ["Cadena", "Cesar", ""], ["Neira", "Jose", ""]]}, {"id": "2010.07661", "submitter": "Xin Jin", "authors": "Xin Jin, Xiqiao Li, Heng Huang, Xiaodong Li, and Xinghui Zhou", "title": "A Deep Drift-Diffusion Model for Image Aesthetic Score Distribution\n  Prediction", "comments": "13 pages, 8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of aesthetic quality assessment is complicated due to its\nsubjectivity. In recent years, the target representation of image aesthetic\nquality has changed from a one-dimensional binary classification label or\nnumerical score to a multi-dimensional score distribution. According to current\nmethods, the ground truth score distributions are straightforwardly regressed.\nHowever, the subjectivity of aesthetics is not taken into account, that is to\nsay, the psychological processes of human beings are not taken into\nconsideration, which limits the performance of the task. In this paper, we\npropose a Deep Drift-Diffusion (DDD) model inspired by psychologists to predict\naesthetic score distribution from images. The DDD model can describe the\npsychological process of aesthetic perception instead of traditional modeling\nof the results of assessment. We use deep convolution neural networks to\nregress the parameters of the drift-diffusion model. The experimental results\nin large scale aesthetic image datasets reveal that our novel DDD model is\nsimple but efficient, which outperforms the state-of-the-art methods in\naesthetic score distribution prediction. Besides, different psychological\nprocesses can also be predicted by our model.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 11:01:46 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Jin", "Xin", ""], ["Li", "Xiqiao", ""], ["Huang", "Heng", ""], ["Li", "Xiaodong", ""], ["Zhou", "Xinghui", ""]]}, {"id": "2010.07675", "submitter": "Xiaofei Mao", "authors": "Xiaofei Mao, Jiahao Cao, Dongfang Li, Xia Jia, Qingfang Zheng", "title": "Integrating Coarse Granularity Part-level Features with Supervised\n  Global-level Features for Person Re-identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Holistic person re-identification (Re-ID) and partial person\nre-identification have achieved great progress respectively in recent years.\nHowever, scenarios in reality often include both holistic and partial\npedestrian images, which makes single holistic or partial person Re-ID hard to\nwork. In this paper, we propose a robust coarse granularity part-level person\nRe-ID network (CGPN), which not only extracts robust regional level body\nfeatures, but also integrates supervised global features for both holistic and\npartial person images. CGPN gains two-fold benefit toward higher accuracy for\nperson Re-ID. On one hand, CGPN learns to extract effective body part features\nfor both holistic and partial person images. On the other hand, compared with\nextracting global features directly by backbone network, CGPN learns to extract\nmore accurate global features with a supervision strategy. The single model\ntrained on three Re-ID datasets including Market-1501, DukeMTMC-reID and CUHK03\nachieves state-of-the-art performances and outperforms any existing approaches.\nEspecially on CUHK03, which is the most challenging dataset for person Re-ID,\nin single query mode, we obtain a top result of Rank-1/mAP=87.1\\%/83.6\\% with\nthis method without re-ranking, outperforming the current best method by\n+7.0\\%/+6.7\\%.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 11:49:20 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Mao", "Xiaofei", ""], ["Cao", "Jiahao", ""], ["Li", "Dongfang", ""], ["Jia", "Xia", ""], ["Zheng", "Qingfang", ""]]}, {"id": "2010.07680", "submitter": "Pankesh Patel", "authors": "Bhavin Joshi and Tapan Pathak and Vatsal Patel and Sarth Kanani and\n  Pankesh Patel and Muhammad Intizar Ali and John Breslin", "title": "Demonstration of a Cloud-based Software Framework for Video Analytics\n  Application using Low-Cost IoT Devices", "comments": "arXiv admin note: substantial text overlap with arXiv:2009.09065", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CV cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The design of products and services such as a Smart doorbell, demonstrating\nvideo analytics software/algorithm functionality, is expected to address a new\nkind of requirements such as designing a scalable solution while considering\nthe trade-off between cost and accuracy; a flexible architecture to deploy new\nAI-based models or update existing models, as user requirements evolve; as well\nas seamlessly integrating different kinds of user interfaces and devices. To\naddress these challenges, we propose a smart doorbell that orchestrates video\nanalytics across Edge and Cloud resources. The proposal uses AWS as a base\nplatform for implementation and leverages Commercially Available\nOff-The-Shelf(COTS) affordable devices such as Raspberry Pi in the form of an\nEdge device.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 06:05:32 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Joshi", "Bhavin", ""], ["Pathak", "Tapan", ""], ["Patel", "Vatsal", ""], ["Kanani", "Sarth", ""], ["Patel", "Pankesh", ""], ["Ali", "Muhammad Intizar", ""], ["Breslin", "John", ""]]}, {"id": "2010.07693", "submitter": "Matthew Leavitt", "authors": "Matthew L. Leavitt, Ari Morcos", "title": "Linking average- and worst-case perturbation robustness via class\n  selectivity and dimensionality", "comments": "arXiv admin note: text overlap with arXiv:2007.04440", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Representational sparsity is known to affect robustness to input\nperturbations in deep neural networks (DNNs), but less is known about how the\nsemantic content of representations affects robustness. Class selectivity-the\nvariability of a unit's responses across data classes or dimensions-is one way\nof quantifying the sparsity of semantic representations. Given recent evidence\nthat class selectivity may not be necessary for, and in some cases can impair\ngeneralization, we investigate whether it also confers robustness (or\nvulnerability) to perturbations of input data. We found that networks\nregularized to have lower levels of class selectivity were more robust to\naverage-case (naturalistic) perturbations, while networks with higher class\nselectivity are more vulnerable. In contrast, class selectivity increases\nrobustness to multiple types of worst-case (i.e. white box adversarial)\nperturbations, suggesting that while decreasing class selectivity is helpful\nfor average-case perturbations, it is harmful for worst-case perturbations. To\nexplain this difference, we studied the dimensionality of the networks'\nrepresentations: we found that the dimensionality of early-layer\nrepresentations is inversely proportional to a network's class selectivity, and\nthat adversarial samples cause a larger increase in early-layer dimensionality\nthan corrupted samples. Furthermore, the input-unit gradient is more variable\nacross samples and units in high-selectivity networks compared to\nlow-selectivity networks. These results lead to the conclusion that units\nparticipate more consistently in low-selectivity regimes compared to\nhigh-selectivity regimes, effectively creating a larger attack surface and\nhence vulnerability to worst-case perturbations.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 00:45:29 GMT"}, {"version": "v2", "created": "Mon, 29 Mar 2021 22:49:58 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Leavitt", "Matthew L.", ""], ["Morcos", "Ari", ""]]}, {"id": "2010.07704", "submitter": "Alisha Sharma", "authors": "Alisha Sharma, Ryan Nett, and Jonathan Ventura", "title": "Unsupervised Learning of Depth and Ego-Motion from Cylindrical Panoramic\n  Video with Applications for Virtual Reality", "comments": "Expansion on arXiv:1901.00979 for IJSC SI; correct table 1 and 3\n  headings, reduce file size", "journal-ref": "Int.J.Semantic Computing 14(3) (2020) 315-322", "doi": "10.1142/S1793351X20400139", "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a convolutional neural network model for unsupervised learning\nof depth and ego-motion from cylindrical panoramic video. Panoramic depth\nestimation is an important technology for applications such as virtual reality,\n3D modeling, and autonomous robotic navigation. In contrast to previous\napproaches for applying convolutional neural networks to panoramic imagery, we\nuse the cylindrical panoramic projection which allows for the use of the\ntraditional CNN layers such as convolutional filters and max pooling without\nmodification. Our evaluation of synthetic and real data shows that unsupervised\nlearning of depth and ego-motion on cylindrical panoramic images can produce\nhigh-quality depth maps and that an increased field-of-view improves ego-motion\nestimation accuracy. We create two new datasets to evaluate our approach: a\nsynthetic dataset created using the CARLA simulator, and Headcam, a novel\ndataset of panoramic video collected from a helmet-mounted camera while biking\nin an urban setting. We also apply our network to the problem of converting\nmonocular panoramas to stereo panoramas.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 16:41:33 GMT"}, {"version": "v2", "created": "Tue, 10 Nov 2020 00:35:33 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Sharma", "Alisha", ""], ["Nett", "Ryan", ""], ["Ventura", "Jonathan", ""]]}, {"id": "2010.07726", "submitter": "BenLei Cui", "authors": "Benlei Cui, XueMei Dong, Qiaoqiao Zhan, Jiangtao Peng, Weiwei Sun", "title": "LiteDepthwiseNet: An Extreme Lightweight Network for Hyperspectral Image\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning methods have shown considerable potential for hyperspectral\nimage (HSI) classification, which can achieve high accuracy compared with\ntraditional methods. However, they often need a large number of training\nsamples and have a lot of parameters and high computational overhead. To solve\nthese problems, this paper proposes a new network architecture,\nLiteDepthwiseNet, for HSI classification. Based on 3D depthwise convolution,\nLiteDepthwiseNet can decompose standard convolution into depthwise convolution\nand pointwise convolution, which can achieve high classification performance\nwith minimal parameters. Moreover, we remove the ReLU layer and Batch\nNormalization layer in the original 3D depthwise convolution, which\nsignificantly improves the overfitting phenomenon of the model on small sized\ndatasets. In addition, focal loss is used as the loss function to improve the\nmodel's attention on difficult samples and unbalanced data, and its training\nperformance is significantly better than that of cross-entropy loss or balanced\ncross-entropy loss. Experiment results on three benchmark hyperspectral\ndatasets show that LiteDepthwiseNet achieves state-of-the-art performance with\na very small number of parameters and low computational cost.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 13:12:17 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Cui", "Benlei", ""], ["Dong", "XueMei", ""], ["Zhan", "Qiaoqiao", ""], ["Peng", "Jiangtao", ""], ["Sun", "Weiwei", ""]]}, {"id": "2010.07734", "submitter": "Cheng Perng Phoo", "authors": "Cheng Perng Phoo, Bharath Hariharan", "title": "Self-training for Few-shot Transfer Across Extreme Task Differences", "comments": "Published as a conference paper at ICLR 2021(oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most few-shot learning techniques are pre-trained on a large, labeled \"base\ndataset\". In problem domains where such large labeled datasets are not\navailable for pre-training (e.g., X-ray, satellite images), one must resort to\npre-training in a different \"source\" problem domain (e.g., ImageNet), which can\nbe very different from the desired target task. Traditional few-shot and\ntransfer learning techniques fail in the presence of such extreme differences\nbetween the source and target tasks. In this paper, we present a simple and\neffective solution to tackle this extreme domain gap: self-training a source\ndomain representation on unlabeled data from the target domain. We show that\nthis improves one-shot performance on the target domain by 2.9 points on\naverage on the challenging BSCD-FSL benchmark consisting of datasets from\nmultiple domains. Our code is available at https://github.com/cpphoo/STARTUP.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 13:23:59 GMT"}, {"version": "v2", "created": "Wed, 17 Mar 2021 16:11:57 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Phoo", "Cheng Perng", ""], ["Hariharan", "Bharath", ""]]}, {"id": "2010.07769", "submitter": "Kelum Gajamannage", "authors": "Kelum Gajamannage, Randy Paffenroth, Anura P. Jayasumana", "title": "A Patch-based Image Denoising Method Using Eigenvectors of the\n  Geodesics' Gramian Matrix", "comments": "16 pages, 5 figures, submitted into Pattern Recognition", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the sophisticated modern technology in the camera industry, the demand\nfor accurate and visually pleasing images is increasing. However, the quality\nof images captured by cameras are inevitably degraded by noise. Thus, some\nprocessing on images is required to filter out the noise without losing vital\nimage features such as edges, corners, etc. Even though the current literature\noffers a variety of denoising methods, fidelity and efficiency of their\ndenoising are sometimes uncertain. Thus, here we propose a novel and\ncomputationally efficient image denoising method that is capable of producing\nan accurate output. This method inputs patches partitioned from the image\nrather than pixels that are well known for preserving image smoothness. Then,\nit performs denoising on the manifold underlying the patch-space rather than\nthat in the image domain to better preserve the features across the whole\nimage. We validate the performance of this method against benchmark image\nprocessing methods.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 04:07:24 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Gajamannage", "Kelum", ""], ["Paffenroth", "Randy", ""], ["Jayasumana", "Anura P.", ""]]}, {"id": "2010.07783", "submitter": "Sebastian Schrom", "authors": "Sebastian Schrom and Stephan Hasler and J\\\"urgen Adamy", "title": "Improved Multi-Source Domain Adaptation by Preservation of Factors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain Adaptation (DA) is a highly relevant research topic when it comes to\nimage classification with deep neural networks. Combining multiple source\ndomains in a sophisticated way to optimize a classification model can improve\nthe generalization to a target domain. Here, the difference in data\ndistributions of source and target image datasets plays a major role. In this\npaper, we describe based on a theory of visual factors how real-world scenes\nappear in images in general and how recent DA datasets are composed of such. We\nshow that different domains can be described by a set of so called domain\nfactors, whose values are consistent within a domain, but can change across\ndomains. Many DA approaches try to remove all domain factors from the feature\nrepresentation to be domain invariant. In this paper we show that this can lead\nto negative transfer since task-informative factors can get lost as well. To\naddress this, we propose Factor-Preserving DA (FP-DA), a method to train a deep\nadversarial unsupervised DA model, which is able to preserve specific task\nrelevant factors in a multi-domain scenario. We demonstrate on CORe50, a\ndataset with many domains, how such factors can be identified by standard\none-to-one transfer experiments between single domains combined with PCA. By\napplying FP-DA, we show that the highest average and minimum performance can be\nachieved.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 14:19:57 GMT"}, {"version": "v2", "created": "Fri, 16 Oct 2020 07:15:58 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Schrom", "Sebastian", ""], ["Hasler", "Stephan", ""], ["Adamy", "J\u00fcrgen", ""]]}, {"id": "2010.07788", "submitter": "Yanghao Zhang", "authors": "Yanghao Zhang, Wenjie Ruan, Fu Wang, Xiaowei Huang", "title": "Generalizing Universal Adversarial Attacks Beyond Additive Perturbations", "comments": "A short version of this work will appear in the ICDM 2020 conference\n  proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The previous study has shown that universal adversarial attacks can fool deep\nneural networks over a large set of input images with a single human-invisible\nperturbation. However, current methods for universal adversarial attacks are\nbased on additive perturbation, which cause misclassification when the\nperturbation is directly added to the input images. In this paper, for the\nfirst time, we show that a universal adversarial attack can also be achieved\nvia non-additive perturbation (e.g., spatial transformation). More importantly,\nto unify both additive and non-additive perturbations, we propose a novel\nunified yet flexible framework for universal adversarial attacks, called GUAP,\nwhich is able to initiate attacks by additive perturbation, non-additive\nperturbation, or the combination of both. Extensive experiments are conducted\non CIFAR-10 and ImageNet datasets with six deep neural network models including\nGoogleLeNet, VGG16/19, ResNet101/152, and DenseNet121. The empirical\nexperiments demonstrate that GUAP can obtain up to 90.9% and 99.24% successful\nattack rates on CIFAR-10 and ImageNet datasets, leading to over 15% and 19%\nimprovements respectively than current state-of-the-art universal adversarial\nattacks. The code for reproducing the experiments in this paper is available at\nhttps://github.com/TrustAI/GUAP.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 14:25:58 GMT"}, {"version": "v2", "created": "Thu, 29 Oct 2020 18:20:21 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Zhang", "Yanghao", ""], ["Ruan", "Wenjie", ""], ["Wang", "Fu", ""], ["Huang", "Xiaowei", ""]]}, {"id": "2010.07804", "submitter": "Xiao Luo", "authors": "Xiao Luo, Daqing Wu, Zeyu Ma, Chong Chen, Huasong Zhong, Minghua Deng,\n  Jianqiang Huang and Xian-sheng Hua", "title": "CIMON: Towards High-quality Hash Codes", "comments": "8 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, hashing is widely-used in approximate nearest neighbor search for\nits storage and computational efficiency. Due to the lack of labeled data in\npractice, many studies focus on unsupervised hashing. Most of the unsupervised\nhashing methods learn to map images into semantic similarity-preserving hash\ncodes by constructing local semantic similarity structure from the pre-trained\nmodel as guiding information, i.e., treating each point pair similar if their\ndistance is small in feature space. However, due to the inefficient\nrepresentation ability of the pre-trained model, many false positives and\nnegatives in local semantic similarity will be introduced and lead to error\npropagation during hash code learning. Moreover, most of hashing methods ignore\nthe basic characteristics of hash codes such as collisions, which will cause\ninstability of hash codes to disturbance. In this paper, we propose a new\nmethod named Comprehensive sImilarity Mining and cOnsistency learNing (CIMON).\nFirst, we use global constraint learning and similarity statistical\ndistribution to obtain reliable and smooth guidance. Second, image augmentation\nand consistency learning will be introduced to explore both semantic and\ncontrastive consistency to derive robust hash codes with fewer collisions.\nExtensive experiments on several benchmark datasets show that the proposed\nmethod consistently outperforms a wide range of state-of-the-art methods in\nboth retrieval performance and robustness.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 14:47:14 GMT"}, {"version": "v2", "created": "Fri, 16 Oct 2020 09:18:50 GMT"}, {"version": "v3", "created": "Thu, 5 Nov 2020 08:44:26 GMT"}], "update_date": "2020-11-06", "authors_parsed": [["Luo", "Xiao", ""], ["Wu", "Daqing", ""], ["Ma", "Zeyu", ""], ["Chen", "Chong", ""], ["Zhong", "Huasong", ""], ["Deng", "Minghua", ""], ["Huang", "Jianqiang", ""], ["Hua", "Xian-sheng", ""]]}, {"id": "2010.07810", "submitter": "Amil Merchant", "authors": "Amil Merchant, Barret Zoph, Ekin Dogus Cubuk", "title": "Does Data Augmentation Benefit from Split BatchNorms", "comments": "9 pages (+ 3 for references)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data augmentation has emerged as a powerful technique for improving the\nperformance of deep neural networks and led to state-of-the-art results in\ncomputer vision. However, state-of-the-art data augmentation strongly distorts\ntraining images, leading to a disparity between examples seen during training\nand inference. In this work, we explore a recently proposed training paradigm\nin order to correct for this disparity: using an auxiliary BatchNorm for the\npotentially out-of-distribution, strongly augmented images. Our experiments\nthen focus on how to define the BatchNorm parameters that are used at\nevaluation. To eliminate the train-test disparity, we experiment with using the\nbatch statistics defined by clean training images only, yet surprisingly find\nthat this does not yield improvements in model performance. Instead, we\ninvestigate using BatchNorm parameters defined by weak augmentations and find\nthat this method significantly improves the performance of common image\nclassification benchmarks such as CIFAR-10, CIFAR-100, and ImageNet. We then\nexplore a fundamental trade-off between accuracy and robustness coming from\nusing different BatchNorm parameters, providing greater insight into the\nbenefits of data augmentation on model performance.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 15:00:43 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Merchant", "Amil", ""], ["Zoph", "Barret", ""], ["Cubuk", "Ekin Dogus", ""]]}, {"id": "2010.07811", "submitter": "Ching-Hui Chen", "authors": "Bardia Doosti, Ching-Hui Chen, Raviteja Vemulapalli, Xuhui Jia, Yukun\n  Zhu, Bradley Green", "title": "Boosting Image-based Mutual Gaze Detection using Pseudo 3D Gaze", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mutual gaze detection, i.e., predicting whether or not two people are looking\nat each other, plays an important role in understanding human interactions. In\nthis work, we focus on the task of image-based mutual gaze detection, and\npropose a simple and effective approach to boost the performance by using an\nauxiliary 3D gaze estimation task during the training phase. We achieve the\nperformance boost without additional labeling cost by training the 3D gaze\nestimation branch using pseudo 3D gaze labels deduced from mutual gaze labels.\nBy sharing the head image encoder between the 3D gaze estimation and the mutual\ngaze detection branches, we achieve better head features than learned by\ntraining the mutual gaze detection branch alone. Experimental results on three\nimage datasets show that the proposed approach improves the detection\nperformance significantly without additional annotations. This work also\nintroduces a new image dataset that consists of 33.1K pairs of humans annotated\nwith mutual gaze labels in 29.2K images.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 15:01:41 GMT"}, {"version": "v2", "created": "Tue, 22 Dec 2020 17:20:59 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Doosti", "Bardia", ""], ["Chen", "Ching-Hui", ""], ["Vemulapalli", "Raviteja", ""], ["Jia", "Xuhui", ""], ["Zhu", "Yukun", ""], ["Green", "Bradley", ""]]}, {"id": "2010.07820", "submitter": "Berta Besc\\'os Torcal", "authors": "Berta Bescos, Carlos Campos, Juan D. Tard\\'os, Jos\\'e Neira", "title": "DynaSLAM II: Tightly-Coupled Multi-Object Tracking and SLAM", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The assumption of scene rigidity is common in visual SLAM algorithms.\nHowever, it limits their applicability in populated real-world environments.\nFurthermore, most scenarios including autonomous driving, multi-robot\ncollaboration and augmented/virtual reality, require explicit motion\ninformation of the surroundings to help with decision making and scene\nunderstanding. We present in this paper DynaSLAM II, a visual SLAM system for\nstereo and RGB-D configurations that tightly integrates the multi-object\ntracking capability.\n  DynaSLAM II makes use of instance semantic segmentation and of ORB features\nto track dynamic objects. The structure of the static scene and of the dynamic\nobjects is optimized jointly with the trajectories of both the camera and the\nmoving agents within a novel bundle adjustment proposal. The 3D bounding boxes\nof the objects are also estimated and loosely optimized within a fixed temporal\nwindow. We demonstrate that tracking dynamic objects does not only provide rich\nclues for scene understanding but is also beneficial for camera tracking.\n  The project code will be released upon acceptance.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 15:25:30 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Bescos", "Berta", ""], ["Campos", "Carlos", ""], ["Tard\u00f3s", "Juan D.", ""], ["Neira", "Jos\u00e9", ""]]}, {"id": "2010.07827", "submitter": "C\\'esar Soto-Valero", "authors": "Gustaf Halvardsson, Johanna Peterson, C\\'esar Soto-Valero, Benoit\n  Baudry", "title": "Interpretation of Swedish Sign Language using Convolutional Neural\n  Networks and Transfer Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The automatic interpretation of sign languages is a challenging task, as it\nrequires the usage of high-level vision and high-level motion processing\nsystems for providing accurate image perception. In this paper, we use\nConvolutional Neural Networks (CNNs) and transfer learning in order to make\ncomputers able to interpret signs of the Swedish Sign Language (SSL) hand\nalphabet. Our model consist of the implementation of a pre-trained InceptionV3\nnetwork, and the usage of the mini-batch gradient descent optimization\nalgorithm. We rely on transfer learning during the pre-training of the model\nand its data. The final accuracy of the model, based on 8 study subjects and\n9,400 images, is 85%. Our results indicate that the usage of CNNs is a\npromising approach to interpret sign languages, and transfer learning can be\nused to achieve high testing accuracy despite using a small training dataset.\nFurthermore, we describe the implementation details of our model to interpret\nsigns as a user-friendly web application.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 15:34:09 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Halvardsson", "Gustaf", ""], ["Peterson", "Johanna", ""], ["Soto-Valero", "C\u00e9sar", ""], ["Baudry", "Benoit", ""]]}, {"id": "2010.07830", "submitter": "Javiera Castillo-Navarro", "authors": "Javiera Castillo-Navarro, Bertrand Le Saux, Alexandre Boulch, Nicolas\n  Audebert and S\\'ebastien Lef\\`evre", "title": "Semi-Supervised Semantic Segmentation in Earth Observation: The\n  MiniFrance Suite, Dataset Analysis and Multi-task Network Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The development of semi-supervised learning techniques is essential to\nenhance the generalization capacities of machine learning algorithms. Indeed,\nraw image data are abundant while labels are scarce, therefore it is crucial to\nleverage unlabeled inputs to build better models. The availability of large\ndatabases have been key for the development of learning algorithms with high\nlevel performance.\n  Despite the major role of machine learning in Earth Observation to derive\nproducts such as land cover maps, datasets in the field are still limited,\neither because of modest surface coverage, lack of variety of scenes or\nrestricted classes to identify. We introduce a novel large-scale dataset for\nsemi-supervised semantic segmentation in Earth Observation, the MiniFrance\nsuite. MiniFrance has several unprecedented properties: it is large-scale,\ncontaining over 2000 very high resolution aerial images, accounting for more\nthan 200 billions samples (pixels); it is varied, covering 16 conurbations in\nFrance, with various climates, different landscapes, and urban as well as\ncountryside scenes; and it is challenging, considering land use classes with\nhigh-level semantics. Nevertheless, the most distinctive quality of MiniFrance\nis being the only dataset in the field especially designed for semi-supervised\nlearning: it contains labeled and unlabeled images in its training partition,\nwhich reproduces a life-like scenario. Along with this dataset, we present\ntools for data representativeness analysis in terms of appearance similarity\nand a thorough study of MiniFrance data, demonstrating that it is suitable for\nlearning and generalizes well in a semi-supervised setting. Finally, we present\nsemi-supervised deep architectures based on multi-task learning and the first\nexperiments on MiniFrance.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 15:36:58 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Castillo-Navarro", "Javiera", ""], ["Saux", "Bertrand Le", ""], ["Boulch", "Alexandre", ""], ["Audebert", "Nicolas", ""], ["Lef\u00e8vre", "S\u00e9bastien", ""]]}, {"id": "2010.07849", "submitter": "Guanbin Li", "authors": "Hongjun Wang, Guanbin Li, Xiaobai Liu and Liang Lin", "title": "A Hamiltonian Monte Carlo Method for Probabilistic Adversarial Attack\n  and Learning", "comments": "Accepted as a Regular Paper in IEEE Transactions on Pattern Analysis\n  and Machine Intelligence", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although deep convolutional neural networks (CNNs) have demonstrated\nremarkable performance on multiple computer vision tasks, researches on\nadversarial learning have shown that deep models are vulnerable to adversarial\nexamples, which are crafted by adding visually imperceptible perturbations to\nthe input images. Most of the existing adversarial attack methods only create a\nsingle adversarial example for the input, which just gives a glimpse of the\nunderlying data manifold of adversarial examples. An attractive solution is to\nexplore the solution space of the adversarial examples and generate a diverse\nbunch of them, which could potentially improve the robustness of real-world\nsystems and help prevent severe security threats and vulnerabilities. In this\npaper, we present an effective method, called Hamiltonian Monte Carlo with\nAccumulated Momentum (HMCAM), aiming to generate a sequence of adversarial\nexamples. To improve the efficiency of HMC, we propose a new regime to\nautomatically control the length of trajectories, which allows the algorithm to\nmove with adaptive step sizes along the search direction at different\npositions. Moreover, we revisit the reason for high computational cost of\nadversarial training under the view of MCMC and design a new generative method\ncalled Contrastive Adversarial Training (CAT), which approaches equilibrium\ndistribution of adversarial examples with only few iterations by building from\nsmall modifications of the standard Contrastive Divergence (CD) and achieve a\ntrade-off between efficiency and accuracy. Both quantitative and qualitative\nanalysis on several natural image datasets and practical systems have confirmed\nthe superiority of the proposed algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 16:07:26 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Wang", "Hongjun", ""], ["Li", "Guanbin", ""], ["Liu", "Xiaobai", ""], ["Lin", "Liang", ""]]}, {"id": "2010.07881", "submitter": "Mehdi Miah", "authors": "Mehdi Miah, Justine Pepin, Nicolas Saunier and Guillaume-Alexandre\n  Bilodeau", "title": "An Empirical Analysis of Visual Features for Multiple Object Tracking in\n  Urban Scenes", "comments": "Accepted on 25th International Conference on Pattern Recognition\n  (ICPR 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of selecting appearance features for\nmultiple object tracking (MOT) in urban scenes. Over the years, a large number\nof features has been used for MOT. However, it is not clear whether some of\nthem are better than others. Commonly used features are color histograms,\nhistograms of oriented gradients, deep features from convolutional neural\nnetworks and re-identification (ReID) features. In this study, we assess how\ngood these features are at discriminating objects enclosed by a bounding box in\nurban scene tracking scenarios. Several affinity measures, namely the\n$\\mathrm{L}_1$, $\\mathrm{L}_2$ and the Bhattacharyya distances, Rank-1 counts\nand the cosine similarity, are also assessed for their impact on the\ndiscriminative power of the features. Results on several datasets show that\nfeatures from ReID networks are the best for discriminating instances from one\nanother regardless of the quality of the detector. If a ReID model is not\navailable, color histograms may be selected if the detector has a good recall\nand there are few occlusions; otherwise, deep features are more robust to\ndetectors with lower recall. The project page is\nhttp://www.mehdimiah.com/visual_features.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 16:57:13 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Miah", "Mehdi", ""], ["Pepin", "Justine", ""], ["Saunier", "Nicolas", ""], ["Bilodeau", "Guillaume-Alexandre", ""]]}, {"id": "2010.07922", "submitter": "Jovana Mitrovic", "authors": "Jovana Mitrovic, Brian McWilliams, Jacob Walker, Lars Buesing, Charles\n  Blundell", "title": "Representation Learning via Invariant Causal Mechanisms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-supervised learning has emerged as a strategy to reduce the reliance on\ncostly supervised signal by pretraining representations only using unlabeled\ndata. These methods combine heuristic proxy classification tasks with data\naugmentations and have achieved significant success, but our theoretical\nunderstanding of this success remains limited. In this paper we analyze\nself-supervised representation learning using a causal framework. We show how\ndata augmentations can be more effectively utilized through explicit invariance\nconstraints on the proxy classifiers employed during pretraining. Based on\nthis, we propose a novel self-supervised objective, Representation Learning via\nInvariant Causal Mechanisms (ReLIC), that enforces invariant prediction of\nproxy targets across augmentations through an invariance regularizer which\nyields improved generalization guarantees. Further, using causality we\ngeneralize contrastive learning, a particular kind of self-supervised method,\nand provide an alternative theoretical explanation for the success of these\nmethods. Empirically, ReLIC significantly outperforms competing methods in\nterms of robustness and out-of-distribution generalization on ImageNet, while\nalso significantly outperforming these methods on Atari achieving above\nhuman-level performance on $51$ out of $57$ games.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 17:53:37 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Mitrovic", "Jovana", ""], ["McWilliams", "Brian", ""], ["Walker", "Jacob", ""], ["Buesing", "Lars", ""], ["Blundell", "Charles", ""]]}, {"id": "2010.07930", "submitter": "Jifeng Dai", "authors": "Hao Li, Chenxin Tao, Xizhou Zhu, Xiaogang Wang, Gao Huang, Jifeng Dai", "title": "Auto Seg-Loss: Searching Metric Surrogates for Semantic Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Designing proper loss functions is essential in training deep networks.\nEspecially in the field of semantic segmentation, various evaluation metrics\nhave been proposed for diverse scenarios. Despite the success of the widely\nadopted cross-entropy loss and its variants, the mis-alignment between the loss\nfunctions and evaluation metrics degrades the network performance. Meanwhile,\nmanually designing loss functions for each specific metric requires expertise\nand significant manpower. In this paper, we propose to automate the design of\nmetric-specific loss functions by searching differentiable surrogate losses for\neach metric. We substitute the non-differentiable operations in the metrics\nwith parameterized functions, and conduct parameter search to optimize the\nshape of loss surfaces. Two constraints are introduced to regularize the search\nspace and make the search efficient. Extensive experiments on PASCAL VOC and\nCityscapes demonstrate that the searched surrogate losses outperform the\nmanually designed loss functions consistently. The searched losses can\ngeneralize well to other datasets and networks. Code shall be released.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 17:59:08 GMT"}, {"version": "v2", "created": "Thu, 3 Dec 2020 05:05:15 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Li", "Hao", ""], ["Tao", "Chenxin", ""], ["Zhu", "Xizhou", ""], ["Wang", "Xiaogang", ""], ["Huang", "Gao", ""], ["Dai", "Jifeng", ""]]}, {"id": "2010.07931", "submitter": "YingQiao Wang", "authors": "YingQiao Wang", "title": "LTN: Long-Term Network for Long-Term Motion Prediction", "comments": "Under Review For ICRA/RA-L 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Making accurate motion prediction of surrounding agents such as pedestrians\nand vehicles is a critical task when robots are trying to perform autonomous\nnavigation tasks. Recent research on multi-modal trajectory prediction,\nincluding regression and classification approaches, perform very well at\nshort-term prediction. However, when it comes to long-term prediction, most\nLong Short-Term Memory (LSTM) based models tend to diverge far away from the\nground truth. Therefore, in this work, we present a two-stage framework for\nlong-term trajectory prediction, which is named as Long-Term Network (LTN). Our\nLong-Term Network integrates both the regression and classification approaches.\nWe first generate a set of proposed trajectories with our proposed distribution\nusing a Conditional Variational Autoencoder (CVAE), and then classify them with\nbinary labels, and output the trajectories with the highest score. We\ndemonstrate our Long-Term Network's performance with experiments on two\nreal-world pedestrian datasets: ETH/UCY, Stanford Drone Dataset (SDD), and one\nchallenging real-world driving forecasting dataset: nuScenes. The results show\nthat our method outperforms multiple state-of-the-art approaches in long-term\ntrajectory prediction in terms of accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 17:59:09 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Wang", "YingQiao", ""]]}, {"id": "2010.07936", "submitter": "Tomasz Szandala", "authors": "Tomasz Szandala", "title": "Convolutional Neural Network for Blur Images Detection as an Alternative\n  for Laplacian Method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  With the prevalence of digital cameras, the number of digital images\nincreases quickly, which raises the demand for non-manual image quality\nassessment. While there are many methods considered useful for detecting\nblurriness, in this paper we propose and evaluate a new method that uses a deep\nconvolutional neural network, which can determine whether an image is blurry or\nnot. Experimental results demonstrate the effectiveness of the proposed scheme\nand are compared to deterministic methods using the confusion matrix.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 11:13:22 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Szandala", "Tomasz", ""]]}, {"id": "2010.07954", "submitter": "Alexander Ku", "authors": "Alexander Ku and Peter Anderson and Roma Patel and Eugene Ie and Jason\n  Baldridge", "title": "Room-Across-Room: Multilingual Vision-and-Language Navigation with Dense\n  Spatiotemporal Grounding", "comments": "EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Room-Across-Room (RxR), a new Vision-and-Language Navigation\n(VLN) dataset. RxR is multilingual (English, Hindi, and Telugu) and larger\n(more paths and instructions) than other VLN datasets. It emphasizes the role\nof language in VLN by addressing known biases in paths and eliciting more\nreferences to visible entities. Furthermore, each word in an instruction is\ntime-aligned to the virtual poses of instruction creators and validators. We\nestablish baseline scores for monolingual and multilingual settings and\nmultitask learning when including Room-to-Room annotations. We also provide\nresults for a model that learns from synchronized pose traces by focusing only\non portions of the panorama attended to in human demonstrations. The size,\nscope and detail of RxR dramatically expands the frontier for research on\nembodied language agents in simulated, photo-realistic environments.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 18:01:15 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Ku", "Alexander", ""], ["Anderson", "Peter", ""], ["Patel", "Roma", ""], ["Ie", "Eugene", ""], ["Baldridge", "Jason", ""]]}, {"id": "2010.07958", "submitter": "Yongqing Liang", "authors": "Yongqing Liang, Xin Li, Navid Jafari, Qin Chen", "title": "Video Object Segmentation with Adaptive Feature Bank and\n  Uncertain-Region Refinement", "comments": "Preprint version. Accepted by NeurIPS 2020", "journal-ref": "NeurIPS 2020", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new matching-based framework for semi-supervised video object\nsegmentation (VOS). Recently, state-of-the-art VOS performance has been\nachieved by matching-based algorithms, in which feature banks are created to\nstore features for region matching and classification. However, how to\neffectively organize information in the continuously growing feature bank\nremains under-explored, and this leads to inefficient design of the bank. We\nintroduce an adaptive feature bank update scheme to dynamically absorb new\nfeatures and discard obsolete features. We also design a new confidence loss\nand a fine-grained segmentation module to enhance the segmentation accuracy in\nuncertain regions. On public benchmarks, our algorithm outperforms existing\nstate-of-the-arts.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 18:04:46 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Liang", "Yongqing", ""], ["Li", "Xin", ""], ["Jafari", "Navid", ""], ["Chen", "Qin", ""]]}, {"id": "2010.07979", "submitter": "John Howard", "authors": "John J. Howard, Yevgeniy B. Sirotin, Jerry L. Tipton, and Arun R.\n  Vemury", "title": "Quantifying the Extent to Which Race and Gender Features Determine\n  Identity in Commercial Face Recognition Algorithms", "comments": "8 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human face features can be used to determine individual identity as well as\ndemographic information like gender and race. However, the extent to which\nblack-box commercial face recognition algorithms (CFRAs) use gender and race\nfeatures to determine identity is poorly understood despite increasing\ndeployments by government and industry. In this study, we quantified the degree\nto which gender and race features influenced face recognition similarity scores\nbetween different people, i.e. non-mated scores. We ran this study using five\ndifferent CFRAs and a sample of 333 diverse test subjects. As a control, we\ncompared the behavior of these non-mated distributions to a commercial iris\nrecognition algorithm (CIRA). Confirming prior work, all CFRAs produced higher\nsimilarity scores for people of the same gender and race, an effect known as\n\"broad homogeneity\". No such effect was observed for the CIRA. Next, we applied\nprincipal components analysis (PCA) to similarity score matrices. We show that\nsome principal components (PCs) of CFRAs cluster people by gender and race, but\nthe majority do not. Demographic clustering in the PCs accounted for only 10 %\nof the total CFRA score variance. No clustering was observed for the CIRA. This\ndemonstrates that, although CFRAs use some gender and race features to\nestablish identity, most features utilized by current CFRAs are unrelated to\ngender and race, similar to the iris texture patterns utilized by the CIRA.\nFinally, reconstruction of similarity score matrices using only PCs that showed\nno demographic clustering reduced broad homogeneity effects, but also decreased\nthe separation between mated and non-mated scores. This suggests it's possible\nfor CFRAs to operate on features unrelated to gender and race, albeit with\nsomewhat lower recognition accuracy, but that this is not the current\ncommercial practice.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 18:52:36 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Howard", "John J.", ""], ["Sirotin", "Yevgeniy B.", ""], ["Tipton", "Jerry L.", ""], ["Vemury", "Arun R.", ""]]}, {"id": "2010.07982", "submitter": "Saurabh Hinduja", "authors": "Saurabh Hinduja, Shaun Canavan, Saandeep Aathreya", "title": "Impact of Action Unit Occurrence Patterns on Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting action units is an important task in face analysis, especially in\nfacial expression recognition. This is due, in part, to the idea that\nexpressions can be decomposed into multiple action units. In this paper we\ninvestigate the impact of action unit occurrence patterns on detection of\naction units. To facilitate this investigation, we review state of the art\nliterature, for AU detection, on 2 state-of-the-art face databases that are\ncommonly used for this task, namely DISFA, and BP4D. Our findings, from this\nliterature review, suggest that action unit occurrence patterns strongly impact\nevaluation metrics (e.g. F1-binary). Along with the literature review, we also\nconduct multi and single action unit detection, as well as propose a new\napproach to explicitly train deep neural networks using the occurrence patterns\nto boost the accuracy of action unit detection. These experiments validate that\naction unit patterns directly impact the evaluation metrics.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 19:03:05 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Hinduja", "Saurabh", ""], ["Canavan", "Shaun", ""], ["Aathreya", "Saandeep", ""]]}, {"id": "2010.07999", "submitter": "Jie Lei", "authors": "Jie Lei, Licheng Yu, Tamara L. Berg, Mohit Bansal", "title": "What is More Likely to Happen Next? Video-and-Language Future Event\n  Prediction", "comments": "EMNLP 2020 (17 pages)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a video with aligned dialogue, people can often infer what is more\nlikely to happen next. Making such predictions requires not only a deep\nunderstanding of the rich dynamics underlying the video and dialogue, but also\na significant amount of commonsense knowledge. In this work, we explore whether\nAI models are able to learn to make such multimodal commonsense next-event\npredictions. To support research in this direction, we collect a new dataset,\nnamed Video-and-Language Event Prediction (VLEP), with 28,726 future event\nprediction examples (along with their rationales) from 10,234 diverse TV Show\nand YouTube Lifestyle Vlog video clips. In order to promote the collection of\nnon-trivial challenging examples, we employ an adversarial\nhuman-and-model-in-the-loop data collection procedure. We also present a strong\nbaseline incorporating information from video, dialogue, and commonsense\nknowledge. Experiments show that each type of information is useful for this\nchallenging task, and that compared to the high human performance on VLEP, our\nmodel provides a good starting point but leaves large room for future work. Our\ndataset and code are available at:\nhttps://github.com/jayleicn/VideoLanguageFuturePred\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 19:56:47 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Lei", "Jie", ""], ["Yu", "Licheng", ""], ["Berg", "Tamara L.", ""], ["Bansal", "Mohit", ""]]}, {"id": "2010.08001", "submitter": "Long Zhao", "authors": "Long Zhao, Ting Liu, Xi Peng, Dimitris Metaxas", "title": "Maximum-Entropy Adversarial Data Augmentation for Improved\n  Generalization and Robustness", "comments": "Accepted to NeurIPS 2020. Code is available at\n  https://github.com/garyzhao/ME-ADA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial data augmentation has shown promise for training robust deep\nneural networks against unforeseen data shifts or corruptions. However, it is\ndifficult to define heuristics to generate effective fictitious target\ndistributions containing \"hard\" adversarial perturbations that are largely\ndifferent from the source distribution. In this paper, we propose a novel and\neffective regularization term for adversarial data augmentation. We\ntheoretically derive it from the information bottleneck principle, which\nresults in a maximum-entropy formulation. Intuitively, this regularization term\nencourages perturbing the underlying source distribution to enlarge predictive\nuncertainty of the current model, so that the generated \"hard\" adversarial\nperturbations can improve the model robustness during training. Experimental\nresults on three standard benchmarks demonstrate that our method consistently\noutperforms the existing state of the art by a statistically significant\nmargin.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 20:02:23 GMT"}, {"version": "v2", "created": "Fri, 18 Dec 2020 03:37:02 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Zhao", "Long", ""], ["Liu", "Ting", ""], ["Peng", "Xi", ""], ["Metaxas", "Dimitris", ""]]}, {"id": "2010.08006", "submitter": "Siyi Tang", "authors": "Siyi Tang, Amirata Ghorbani, Rikiya Yamashita, Sameer Rehman, Jared A.\n  Dunnmon, James Zou, Daniel L. Rubin", "title": "Data Valuation for Medical Imaging Using Shapley Value: Application on A\n  Large-scale Chest X-ray Dataset", "comments": null, "journal-ref": null, "doi": "10.1038/s41598-021-87762-2", "report-no": null, "categories": "cs.LG cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The reliability of machine learning models can be compromised when trained on\nlow quality data. Many large-scale medical imaging datasets contain low quality\nlabels extracted from sources such as medical reports. Moreover, images within\na dataset may have heterogeneous quality due to artifacts and biases arising\nfrom equipment or measurement errors. Therefore, algorithms that can\nautomatically identify low quality data are highly desired. In this study, we\nused data Shapley, a data valuation metric, to quantify the value of training\ndata to the performance of a pneumonia detection algorithm in a large chest\nX-ray dataset. We characterized the effectiveness of data Shapley in\nidentifying low quality versus valuable data for pneumonia detection. We found\nthat removing training data with high Shapley values decreased the pneumonia\ndetection performance, whereas removing data with low Shapley values improved\nthe model performance. Furthermore, there were more mislabeled examples in low\nShapley value data and more true pneumonia cases in high Shapley value data.\nOur results suggest that low Shapley value indicates mislabeled or poor quality\nimages, whereas high Shapley value indicates data that are valuable for\npneumonia detection. Our method can serve as a framework for using data Shapley\nto denoise large-scale medical imaging datasets.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 20:18:35 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Tang", "Siyi", ""], ["Ghorbani", "Amirata", ""], ["Yamashita", "Rikiya", ""], ["Rehman", "Sameer", ""], ["Dunnmon", "Jared A.", ""], ["Zou", "James", ""], ["Rubin", "Daniel L.", ""]]}, {"id": "2010.08020", "submitter": "Wei Chen", "authors": "Wei Chen and Yu Liu and Weiping Wang and Tinne Tuytelaars and Erwin M.\n  Bakker and Michael Lew", "title": "On the Exploration of Incremental Learning for Fine-grained Image\n  Retrieval", "comments": "BMVC2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the problem of fine-grained image retrieval in an\nincremental setting, when new categories are added over time. On the one hand,\nrepeatedly training the representation on the extended dataset is\ntime-consuming. On the other hand, fine-tuning the learned representation only\nwith the new classes leads to catastrophic forgetting. To this end, we propose\nan incremental learning method to mitigate retrieval performance degradation\ncaused by the forgetting issue. Without accessing any samples of the original\nclasses, the classifier of the original network provides soft \"labels\" to\ntransfer knowledge to train the adaptive network, so as to preserve the\nprevious capability for classification. More importantly, a regularization\nfunction based on Maximum Mean Discrepancy is devised to minimize the\ndiscrepancy of new classes features from the original network and the adaptive\nnetwork, respectively. Extensive experiments on two datasets show that our\nmethod effectively mitigates the catastrophic forgetting on the original\nclasses while achieving high performance on the new classes.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 21:07:44 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Chen", "Wei", ""], ["Liu", "Yu", ""], ["Wang", "Weiping", ""], ["Tuytelaars", "Tinne", ""], ["Bakker", "Erwin M.", ""], ["Lew", "Michael", ""]]}, {"id": "2010.08021", "submitter": "Udit Arora", "authors": "Aman Khullar, Udit Arora", "title": "MAST: Multimodal Abstractive Summarization with Trimodal Hierarchical\n  Attention", "comments": "To appear in the first EMNLP Workshop on NLP Beyond Text, 2020. Aman\n  Khullar and Udit Arora have equal contribution", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents MAST, a new model for Multimodal Abstractive Text\nSummarization that utilizes information from all three modalities -- text,\naudio and video -- in a multimodal video. Prior work on multimodal abstractive\ntext summarization only utilized information from the text and video\nmodalities. We examine the usefulness and challenges of deriving information\nfrom the audio modality and present a sequence-to-sequence trimodal\nhierarchical attention-based model that overcomes these challenges by letting\nthe model pay more attention to the text modality. MAST outperforms the current\nstate of the art model (video-text) by 2.51 points in terms of Content F1 score\nand 1.00 points in terms of Rouge-L score on the How2 dataset for multimodal\nlanguage understanding.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 21:08:20 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Khullar", "Aman", ""], ["Arora", "Udit", ""]]}, {"id": "2010.08031", "submitter": "Luca Parisi", "authors": "L. Parisi, D. Neagu, R. Ma, F. Campean", "title": "QReLU and m-QReLU: Two novel quantum activation functions to aid medical\n  diagnostics", "comments": "30 pages, 4 listings/Python code snippets, 2 figures, 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The ReLU activation function (AF) has been extensively applied in deep neural\nnetworks, in particular Convolutional Neural Networks (CNN), for image\nclassification despite its unresolved dying ReLU problem, which poses\nchallenges to reliable applications. This issue has obvious important\nimplications for critical applications, such as those in healthcare. Recent\napproaches are just proposing variations of the activation function within the\nsame unresolved dying ReLU challenge. This contribution reports a different\nresearch direction by investigating the development of an innovative quantum\napproach to the ReLU AF that avoids the dying ReLU problem by disruptive\ndesign. The Leaky ReLU was leveraged as a baseline on which the two quantum\nprinciples of entanglement and superposition were applied to derive the\nproposed Quantum ReLU (QReLU) and the modified-QReLU (m-QReLU) activation\nfunctions. Both QReLU and m-QReLU are implemented and made freely available in\nTensorFlow and Keras. This original approach is effective and validated\nextensively in case studies that facilitate the detection of COVID-19 and\nParkinson Disease (PD) from medical images. The two novel AFs were evaluated in\na two-layered CNN against nine ReLU-based AFs on seven benchmark datasets,\nincluding images of spiral drawings taken via graphic tablets from patients\nwith Parkinson Disease and healthy subjects, and point-of-care ultrasound\nimages on the lungs of patients with COVID-19, those with pneumonia and healthy\ncontrols. Despite a higher computational cost, results indicated an overall\nhigher classification accuracy, precision, recall and F1-score brought about by\neither quantum AFs on five of the seven bench-mark datasets, thus demonstrating\nits potential to be the new benchmark or gold standard AF in CNNs and aid image\nclassification tasks involved in critical applications, such as medical\ndiagnoses of COVID-19 and PD.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 21:38:36 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Parisi", "L.", ""], ["Neagu", "D.", ""], ["Ma", "R.", ""], ["Campean", "F.", ""]]}, {"id": "2010.08034", "submitter": "Liyuan Liu", "authors": "Zichao Li and Liyuan Liu and Chengyu Dong and Jingbo Shang", "title": "Overfitting or Underfitting? Understand Robustness Drop in Adversarial\n  Training", "comments": "Work in Progress", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our goal is to understand why the robustness drops after conducting\nadversarial training for too long. Although this phenomenon is commonly\nexplained as overfitting, our analysis suggest that its primary cause is\nperturbation underfitting. We observe that after training for too long,\nFGSM-generated perturbations deteriorate into random noise. Intuitively, since\nno parameter updates are made to strengthen the perturbation generator, once\nthis process collapses, it could be trapped in such local optima. Also,\nsophisticating this process could mostly avoid the robustness drop, which\nsupports that this phenomenon is caused by underfitting instead of overfitting.\nIn the light of our analyses, we propose APART, an adaptive adversarial\ntraining framework, which parameterizes perturbation generation and\nprogressively strengthens them. Shielding perturbations from underfitting\nunleashes the potential of our framework. In our experiments, APART provides\ncomparable or even better robustness than PGD-10, with only about 1/4 of its\ncomputational cost.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 21:43:07 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Li", "Zichao", ""], ["Liu", "Liyuan", ""], ["Dong", "Chengyu", ""], ["Shang", "Jingbo", ""]]}, {"id": "2010.08038", "submitter": "Wenchi Ma", "authors": "Wenchi Ma, Miao Yu, Kaidong Li, Guanghui Wang", "title": "Why Layer-Wise Learning is Hard to Scale-up and a Possible Solution via\n  Accelerated Downsampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Layer-wise learning, as an alternative to global back-propagation, is easy to\ninterpret, analyze, and it is memory efficient. Recent studies demonstrate that\nlayer-wise learning can achieve state-of-the-art performance in image\nclassification on various datasets. However, previous studies of layer-wise\nlearning are limited to networks with simple hierarchical structures, and the\nperformance decreases severely for deeper networks like ResNet. This paper, for\nthe first time, reveals the fundamental reason that impedes the scale-up of\nlayer-wise learning is due to the relatively poor separability of the feature\nspace in shallow layers. This argument is empirically verified by controlling\nthe intensity of the convolution operation in local layers. We discover that\nthe poorly-separable features from shallow layers are mismatched with the\nstrong supervision constraint throughout the entire network, making the\nlayer-wise learning sensitive to network depth. The paper further proposes a\ndownsampling acceleration approach to weaken the poor learning of shallow\nlayers so as to transfer the learning emphasis to deep feature space where the\nseparability matches better with the supervision restraint. Extensive\nexperiments have been conducted to verify the new finding and demonstrate the\nadvantages of the proposed downsampling acceleration in improving the\nperformance of layer-wise learning.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 21:51:43 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Ma", "Wenchi", ""], ["Yu", "Miao", ""], ["Li", "Kaidong", ""], ["Wang", "Guanghui", ""]]}, {"id": "2010.08045", "submitter": "Keshav Bhandari", "authors": "Keshav Bhandari, Ziliang Zong, Yan Yan", "title": "Revisiting Optical Flow Estimation in 360 Videos", "comments": "8 Pages, 7 figures, 1 Table, 5 Equations, 25th International\n  Conference on Pattern Recognition Milan, Italy", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays 360 video analysis has become a significant research topic in the\nfield since the appearance of high-quality and low-cost 360 wearable devices.\nIn this paper, we propose a novel LiteFlowNet360 architecture for 360 videos\noptical flow estimation. We design LiteFlowNet360 as a domain adaptation\nframework from perspective video domain to 360 video domain. We adapt it from\nsimple kernel transformation techniques inspired by Kernel Transformer Network\n(KTN) to cope with inherent distortion in 360 videos caused by the\nsphere-to-plane projection. First, we apply an incremental transformation of\nconvolution layers in feature pyramid network and show that further\ntransformation in inference and regularization layers are not important, hence\nreducing the network growth in terms of size and computation cost. Second, we\nrefine the network by training with augmented data in a supervised manner. We\nperform data augmentation by projecting the images in a sphere and\nre-projecting to a plane. Third, we train LiteFlowNet360 in a self-supervised\nmanner using target domain 360 videos. Experimental results show the promising\nresults of 360 video optical flow estimation using the proposed novel\narchitecture.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 22:22:21 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Bhandari", "Keshav", ""], ["Zong", "Ziliang", ""], ["Yan", "Yan", ""]]}, {"id": "2010.08054", "submitter": "Jingpei Lu", "authors": "Jingpei Lu, Florian Richter, Michael Yip", "title": "Robust Keypoint Detection and Pose Estimation of Robot Manipulators with\n  Self-Occlusions via Sim-to-Real Transfer", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Keypoint detection is an essential building block for many robotic\napplications like motion capture and pose estimation. Historically, keypoints\nare detected using uniquely engineered markers such as checkerboards,\nfiducials, or markers. More recently, deep learning methods have been explored\nas they have the ability to detect user-defined keypoints in a marker-less\nmanner. However, deep neural network (DNN) detectors can have an uneven\nperformance for different manually selected keypoints along the kinematic\nchain. An example of this can be found on symmetric robotic tools where DNN\ndetectors cannot solve the correspondence problem correctly. In this work, we\npropose a new and autonomous way to define the keypoint locations that\novercomes these challenges. The approach involves finding the optimal set of\nkeypoints on robotic manipulators for robust visual detection. Using a robotic\nsimulator as a medium, our algorithm utilizes synthetic data for DNN training,\nand the proposed algorithm is used to optimize the selection of keypoints\nthrough an iterative approach. The results show that when using the optimized\nkeypoints, the detection performance of the DNNs improved so significantly that\nthey can even be detected in cases of self-occlusion. We further use the\noptimized keypoints for real robotic applications by using domain randomization\nto bridge the reality gap between the simulator and the physical world. The\nphysical world experiments show how the proposed method can be applied to the\nwide-breadth of robotic applications that require visual feedback, such as\ncamera-to-robot calibration, robotic tool tracking, and whole-arm pose\nestimation.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 22:38:37 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Lu", "Jingpei", ""], ["Richter", "Florian", ""], ["Yip", "Michael", ""]]}, {"id": "2010.08055", "submitter": "Keshav Bhandari", "authors": "Keshav Bhandari, Mario A. DeLaGarza, Ziliang Zong, Hugo Latapie, Yan\n  Yan", "title": "Egok360: A 360 Egocentric Kinetic Human Activity Video Dataset", "comments": "5 pages, 5 figures, 1 table, 2020 IEEE International Conference on\n  Image Processing (ICIP)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, there has been a growing interest in wearable sensors which\nprovides new research perspectives for 360 {\\deg} video analysis. However, the\nlack of 360 {\\deg} datasets in literature hinders the research in this field.\nTo bridge this gap, in this paper we propose a novel Egocentric (first-person)\n360{\\deg} Kinetic human activity video dataset (EgoK360). The EgoK360 dataset\ncontains annotations of human activity with different sub-actions, e.g.,\nactivity Ping-Pong with four sub-actions which are pickup-ball, hit,\nbounce-ball and serve. To the best of our knowledge, EgoK360 is the first\ndataset in the domain of first-person activity recognition with a 360{\\deg}\nenvironmental setup, which will facilitate the egocentric 360 {\\deg} video\nunderstanding. We provide experimental results and comprehensive analysis of\nvariants of the two-stream network for 360 egocentric activity recognition. The\nEgoK360 dataset can be downloaded from https://egok360.github.io/.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 22:40:55 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Bhandari", "Keshav", ""], ["DeLaGarza", "Mario A.", ""], ["Zong", "Ziliang", ""], ["Latapie", "Hugo", ""], ["Yan", "Yan", ""]]}, {"id": "2010.08066", "submitter": "Nafees Mansoor PhD", "authors": "Abrar Hasin Kamal, Md. Asifuzzaman Jishan, and Nafees Mansoor", "title": "TextMage: The Automated Bangla Caption Generator Based On Deep Learning", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural Networks and Deep Learning have seen an upsurge of research in the\npast decade due to the improved results. Generates text from the given image is\na crucial task that requires the combination of both sectors which are computer\nvision and natural language processing in order to understand an image and\nrepresent it using a natural language. However existing works have all been\ndone on a particular lingual domain and on the same set of data. This leads to\nthe systems being developed to perform poorly on images that belong to specific\nlocales' geographical context. TextMage is a system that is capable of\nunderstanding visual scenes that belong to the Bangladeshi geographical context\nand use its knowledge to represent what it understands in Bengali. Hence, we\nhave trained a model on our previously developed and published dataset named\nBanglaLekhaImageCaptions. This dataset contains 9,154 images along with two\nannotations for each image. In order to access performance, the proposed model\nhas been implemented and evaluated.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 23:24:15 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Kamal", "Abrar Hasin", ""], ["Jishan", "Md. Asifuzzaman", ""], ["Mansoor", "Nafees", ""]]}, {"id": "2010.08087", "submitter": "Willie Huang", "authors": "W. H. Huang", "title": "Performance evaluation and application of computation based low-cost\n  homogeneous machine learning model algorithm for image classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The image classification machine learning model was trained with the\nintention to predict the category of the input image. While multiple\nstate-of-the-art ensemble model methodologies are openly available, this paper\nevaluates the performance of a low-cost, simple algorithm that would integrate\nseamlessly into modern production-grade cloud-based applications. The\nhomogeneous models, trained with the full instead of subsets of data, contains\nvarying hyper-parameters and neural layers from one another. These models'\ninferences will be processed by the new algorithm, which is loosely based on\nconditional probability theories. The final output will be evaluated.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 01:05:49 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Huang", "W. H.", ""]]}, {"id": "2010.08092", "submitter": "Tao Zhong", "authors": "Tao Zhong, Wonjik Kim, Masayuki Tanaka and Masatoshi Okutomi", "title": "Human Segmentation with Dynamic LiDAR Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consecutive LiDAR scans compose dynamic 3D sequences, which contain more\nabundant information than a single frame. Similar to the development history of\nimage and video perception, dynamic 3D sequence perception starts to come into\nsight after inspiring research on static 3D data perception. This work proposes\na spatio-temporal neural network for human segmentation with the dynamic LiDAR\npoint clouds. It takes a sequence of depth images as input. It has a two-branch\nstructure, i.e., the spatial segmentation branch and the temporal velocity\nestimation branch. The velocity estimation branch is designed to capture motion\ncues from the input sequence and then propagates them to the other branch. So\nthat the segmentation branch segments humans according to both spatial and\ntemporal features. These two branches are jointly learned on a generated\ndynamic point cloud dataset for human recognition. Our works fill in the blank\nof dynamic point cloud perception with the spherical representation of point\ncloud and achieves high accuracy. The experiments indicate that the\nintroduction of temporal feature benefits the segmentation of dynamic point\ncloud.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 01:26:35 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Zhong", "Tao", ""], ["Kim", "Wonjik", ""], ["Tanaka", "Masayuki", ""], ["Okutomi", "Masatoshi", ""]]}, {"id": "2010.08103", "submitter": "Bjorn Lutjens", "authors": "Bj\\\"orn L\\\"utjens, Brandon Leshchinskiy, Christian Requena-Mesa,\n  Farrukh Chishtie, Natalia D\\'iaz-Rodriguez, Oc\\'eane Boulais, Aaron Pi\\~na,\n  Dava Newman, Alexander Lavin, Yarin Gal, Chedy Ra\\\"issi", "title": "Physics-informed GANs for Coastal Flood Visualization", "comments": "Under Review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As climate change increases the intensity of natural disasters, society needs\nbetter tools for adaptation. Floods, for example, are the most frequent natural\ndisaster, but during hurricanes the area is largely covered by clouds and\nemergency managers must rely on nonintuitive flood visualizations for mission\nplanning. To assist these emergency managers, we have created a deep learning\npipeline that generates visual satellite images of current and future coastal\nflooding. We advanced a state-of-the-art GAN called pix2pixHD, such that it\nproduces imagery that is physically-consistent with the output of an\nexpert-validated storm surge model (NOAA SLOSH). By evaluating the imagery\nrelative to physics-based flood maps, we find that our proposed framework\noutperforms baseline models in both physical-consistency and photorealism.\nWhile this work focused on the visualization of coastal floods, we envision the\ncreation of a global visualization of how climate change will shape our earth.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 02:15:34 GMT"}, {"version": "v2", "created": "Fri, 12 Feb 2021 06:26:46 GMT"}], "update_date": "2021-02-15", "authors_parsed": [["L\u00fctjens", "Bj\u00f6rn", ""], ["Leshchinskiy", "Brandon", ""], ["Requena-Mesa", "Christian", ""], ["Chishtie", "Farrukh", ""], ["D\u00edaz-Rodriguez", "Natalia", ""], ["Boulais", "Oc\u00e9ane", ""], ["Pi\u00f1a", "Aaron", ""], ["Newman", "Dava", ""], ["Lavin", "Alexander", ""], ["Gal", "Yarin", ""], ["Ra\u00efssi", "Chedy", ""]]}, {"id": "2010.08115", "submitter": "Sanjay Kumar Sonbhadra Mr.", "authors": "Sanjay Kumar Sonbhadra, Sonali Agarwal and P. Nagabhushan", "title": "Pinball-OCSVM for early-stage COVID-19 diagnosis with limited\n  posteroanterior chest X-ray images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The infection of respiratory coronavirus disease 2019 (COVID-19) starts with\nthe upper respiratory tract and as the virus grows, the infection can progress\nto lungs and develop pneumonia. The conventional way of COVID-19 diagnosis is\nreverse transcription polymerase chain reaction (RT-PCR), which is less\nsensitive during early stages; especially if the patient is asymptomatic, which\nmay further cause more severe pneumonia. In this context, several deep learning\nmodels have been proposed to identify pulmonary infections using publicly\navailable chest X-ray (CXR) image datasets for early diagnosis, better\ntreatment and quick cure. In these datasets, presence of less number of\nCOVID-19 positive samples compared to other classes (normal, pneumonia and\nTuberculosis) raises the challenge for unbiased learning of deep learning\nmodels. All deep learning models opted class balancing techniques to solve this\nissue; which however should be avoided in any medical diagnosis process.\nMoreover, the deep learning models are also data hungry and need massive\ncomputation resources. Therefore for quicker diagnosis, this research proposes\na novel pinball loss function based one-class support vector machine\n(PB-OCSVM), that can work in presence of limited COVID-19 positive CXR samples\nwith objectives to maximize the learning efficiency and to minimize the false\npredictions. The performance of the proposed model is compared with\nconventional OCSVM and existing deep learning models, and the experimental\nresults prove that the proposed model outperformed over state-of-the-art\nmethods. To validate the robustness of the proposed model, experiments are also\nperformed with noisy CXR images and UCI benchmark datasets.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 02:34:15 GMT"}, {"version": "v2", "created": "Sat, 5 Jun 2021 06:32:07 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Sonbhadra", "Sanjay Kumar", ""], ["Agarwal", "Sonali", ""], ["Nagabhushan", "P.", ""]]}, {"id": "2010.08127", "submitter": "Preetum Nakkiran", "authors": "Preetum Nakkiran, Behnam Neyshabur, Hanie Sedghi", "title": "The Deep Bootstrap Framework: Good Online Learners are Good Offline\n  Generalizers", "comments": "Accepted to ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new framework for reasoning about generalization in deep\nlearning. The core idea is to couple the Real World, where optimizers take\nstochastic gradient steps on the empirical loss, to an Ideal World, where\noptimizers take steps on the population loss. This leads to an alternate\ndecomposition of test error into: (1) the Ideal World test error plus (2) the\ngap between the two worlds. If the gap (2) is universally small, this reduces\nthe problem of generalization in offline learning to the problem of\noptimization in online learning. We then give empirical evidence that this gap\nbetween worlds can be small in realistic deep learning settings, in particular\nsupervised image classification. For example, CNNs generalize better than MLPs\non image distributions in the Real World, but this is \"because\" they optimize\nfaster on the population loss in the Ideal World. This suggests our framework\nis a useful tool for understanding generalization in deep learning, and lays a\nfoundation for future research in the area.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 03:07:49 GMT"}, {"version": "v2", "created": "Fri, 19 Feb 2021 03:24:24 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Nakkiran", "Preetum", ""], ["Neyshabur", "Behnam", ""], ["Sedghi", "Hanie", ""]]}, {"id": "2010.08128", "submitter": "Jianfeng He", "authors": "Jianfeng He, Xuchao Zhang, Shuo Lei, Shuhui Wang, Qingming Huang,\n  Chang-Tien Lu, Bei Xiao", "title": "Semantic Editing On Segmentation Map Via Multi-Expansion Loss", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic editing on segmentation map has been proposed as an intermediate\ninterface for image generation, because it provides flexible and strong\nassistance in various image generation tasks. This paper aims to improve\nquality of edited segmentation map conditioned on semantic inputs. Even though\nrecent studies apply global and local adversarial losses extensively to\ngenerate images for higher image quality, we find that they suffer from the\nmisalignment of the boundary area in the mask area. To address this, we propose\nMExGAN for semantic editing on segmentation map, which uses a novel\nMulti-Expansion (MEx) loss implemented by adversarial losses on MEx areas. Each\nMEx area has the mask area of the generation as the majority and the boundary\nof original context as the minority. To boost convenience and stability of MEx\nloss, we further propose an Approximated MEx (A-MEx) loss. Besides, in contrast\nto previous model that builds training data for semantic editing on\nsegmentation map with part of the whole image, which leads to model performance\ndegradation, MExGAN applies the whole image to build the training data.\nExtensive experiments on semantic editing on segmentation map and natural image\ninpainting show competitive results on four datasets.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 03:12:26 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["He", "Jianfeng", ""], ["Zhang", "Xuchao", ""], ["Lei", "Shuo", ""], ["Wang", "Shuhui", ""], ["Huang", "Qingming", ""], ["Lu", "Chang-Tien", ""], ["Xiao", "Bei", ""]]}, {"id": "2010.08138", "submitter": "Anh Tran", "authors": "Anh Nguyen and Anh Tran", "title": "Input-Aware Dynamic Backdoor Attack", "comments": "Accepted to NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, neural backdoor attack has been considered to be a potential\nsecurity threat to deep learning systems. Such systems, while achieving the\nstate-of-the-art performance on clean data, perform abnormally on inputs with\npredefined triggers. Current backdoor techniques, however, rely on uniform\ntrigger patterns, which are easily detected and mitigated by current defense\nmethods. In this work, we propose a novel backdoor attack technique in which\nthe triggers vary from input to input. To achieve this goal, we implement an\ninput-aware trigger generator driven by diversity loss. A novel cross-trigger\ntest is applied to enforce trigger nonreusablity, making backdoor verification\nimpossible. Experiments show that our method is efficient in various attack\nscenarios as well as multiple datasets. We further demonstrate that our\nbackdoor can bypass the state of the art defense methods. An analysis with a\nfamous neural network inspector again proves the stealthiness of the proposed\nattack. Our code is publicly available at\nhttps://github.com/VinAIResearch/input-aware-backdoor-attack-release.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 03:57:12 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Nguyen", "Anh", ""], ["Tran", "Anh", ""]]}, {"id": "2010.08145", "submitter": "Suncheng Xiang", "authors": "Suncheng Xiang, Yuzhuo Fu, Guanjie You, Ting Liu", "title": "Taking A Closer Look at Synthesis: Fine-grained Attribute Analysis for\n  Person Re-Identification", "comments": "Accepted as conference paper in ICASSP 2021. arXiv admin note: text\n  overlap with arXiv:2006.07139", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification (re-ID) plays an important role in applications such\nas public security and video surveillance. Recently, learning from synthetic\ndata, which benefits from the popularity of synthetic data engine, has achieved\nremarkable performance. However, in pursuit of high accuracy, researchers in\nthe academic always focus on training with large-scale datasets at a high cost\nof time and label expenses, while neglect to explore the potential of\nperforming efficient training from millions of synthetic data. To facilitate\ndevelopment in this field, we reviewed the previously developed synthetic\ndataset GPR and built an improved one (GPR+) with larger number of identities\nand distinguished attributes. Based on it, we quantitatively analyze the\ninfluence of dataset attribute on re-ID system. To our best knowledge, we are\namong the first attempts to explicitly dissect person re-ID from the aspect of\nattribute on synthetic dataset. This research helps us have a deeper\nunderstanding of the fundamental problems in person re-ID, which also provides\nuseful insights for dataset building and future practical usage.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 02:47:06 GMT"}, {"version": "v2", "created": "Thu, 29 Oct 2020 09:15:20 GMT"}, {"version": "v3", "created": "Tue, 6 Apr 2021 03:44:34 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Xiang", "Suncheng", ""], ["Fu", "Yuzhuo", ""], ["You", "Guanjie", ""], ["Liu", "Ting", ""]]}, {"id": "2010.08164", "submitter": "Anshul Shah", "authors": "Anshul Shah, Shlok Mishra, Ankan Bansal, Jun-Cheng Chen, Rama\n  Chellappa, Abhinav Shrivastava", "title": "Pose And Joint-Aware Action Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most human action recognition systems typically consider static appearances\nand motion as independent streams of information. In this paper, we consider\nthe evolution of human pose and propose a method to better capture\ninterdependence among skeleton joints. Our model extracts motion information\nfrom each joint independently, reweighs the information and finally performs\ninter-joint reasoning. The effectiveness of pose and joint-based\nrepresentations is strengthened using a geometry-aware data augmentation\ntechnique which jitters pose heatmaps while retaining the dynamics of the\naction. Our best model gives an absolute improvement of 8.19% on JHMDB, 4.31%\non HMDB and 1.55 mAP on Charades datasets over state-of-the-art methods using\npose heat-maps alone. Fusing with RGB and flow streams leads to improvement\nover state-of-the-art. Our model also outperforms the baseline on Mimetics, a\ndataset with out-of-context videos by 1.14% while using only pose heatmaps.\nFurther, to filter out clips irrelevant for action recognition, we re-purpose\nour model for clip selection guided by pose information and show improved\nperformance using fewer clips.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 04:43:34 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Shah", "Anshul", ""], ["Mishra", "Shlok", ""], ["Bansal", "Ankan", ""], ["Chen", "Jun-Cheng", ""], ["Chellappa", "Rama", ""], ["Shrivastava", "Abhinav", ""]]}, {"id": "2010.08168", "submitter": "Esther Rolf", "authors": "Esther Rolf, Jonathan Proctor, Tamma Carleton, Ian Bolliger, Vaishaal\n  Shankar, Miyabi Ishihara, Benjamin Recht, Solomon Hsiang", "title": "A Generalizable and Accessible Approach to Machine Learning with Global\n  Satellite Imagery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Combining satellite imagery with machine learning (SIML) has the potential to\naddress global challenges by remotely estimating socioeconomic and\nenvironmental conditions in data-poor regions, yet the resource requirements of\nSIML limit its accessibility and use. We show that a single encoding of\nsatellite imagery can generalize across diverse prediction tasks (e.g. forest\ncover, house price, road length). Our method achieves accuracy competitive with\ndeep neural networks at orders of magnitude lower computational cost, scales\nglobally, delivers label super-resolution predictions, and facilitates\ncharacterizations of uncertainty. Since image encodings are shared across\ntasks, they can be centrally computed and distributed to unlimited researchers,\nwho need only fit a linear regression to their own ground truth data in order\nto achieve state-of-the-art SIML performance.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 05:00:39 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Rolf", "Esther", ""], ["Proctor", "Jonathan", ""], ["Carleton", "Tamma", ""], ["Bolliger", "Ian", ""], ["Shankar", "Vaishaal", ""], ["Ishihara", "Miyabi", ""], ["Recht", "Benjamin", ""], ["Hsiang", "Solomon", ""]]}, {"id": "2010.08175", "submitter": "Xuanhong Chen", "authors": "Xuanhong Chen, Xirui Yan, Naiyuan Liu, Ting Qiu and Bingbing Ni", "title": "Anisotropic Stroke Control for Multiple Artists Style Transfer", "comments": "ACMMM2020", "journal-ref": "ACM Multimedia 2020", "doi": "10.1145/3394171.3413770", "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Though significant progress has been made in artistic style transfer,\nsemantic information is usually difficult to be preserved in a fine-grained\nlocally consistent manner by most existing methods, especially when multiple\nartists styles are required to transfer within one single model. To circumvent\nthis issue, we propose a Stroke Control Multi-Artist Style Transfer framework.\nOn the one hand, we develop a multi-condition single-generator structure which\nfirst performs multi-artist style transfer. On the one hand, we design an\nAnisotropic Stroke Module (ASM) which realizes the dynamic adjustment of\nstyle-stroke between the non-trivial and the trivial regions. ASM endows the\nnetwork with the ability of adaptive semantic-consistency among various styles.\nOn the other hand, we present an novel Multi-Scale Projection Discriminator} to\nrealize the texture-level conditional generation. In contrast to the\nsingle-scale conditional discriminator, our discriminator is able to capture\nmulti-scale texture clue to effectively distinguish a wide range of artistic\nstyles. Extensive experimental results well demonstrate the feasibility and\neffectiveness of our approach. Our framework can transform a photograph into\ndifferent artistic style oil painting via only ONE single model. Furthermore,\nthe results are with distinctive artistic style and retain the anisotropic\nsemantic information. The code is already available on github:\nhttps://github.com/neuralchen/ASMAGAN.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 05:32:26 GMT"}, {"version": "v2", "created": "Mon, 14 Jun 2021 14:25:27 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Chen", "Xuanhong", ""], ["Yan", "Xirui", ""], ["Liu", "Naiyuan", ""], ["Qiu", "Ting", ""], ["Ni", "Bingbing", ""]]}, {"id": "2010.08186", "submitter": "Saleh Shahinfar", "authors": "Saleh Shahinfar, Paul Meek, Greg Falzon", "title": "How many images do I need? Understanding how sample size per class\n  affects deep learning model performance metrics for balanced designs in\n  autonomous wildlife monitoring", "comments": null, "journal-ref": "Ecological Informatics, 2020, 57:101085", "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning (DL) algorithms are the state of the art in automated\nclassification of wildlife camera trap images. The challenge is that the\necologist cannot know in advance how many images per species they need to\ncollect for model training in order to achieve their desired classification\naccuracy. In fact there is limited empirical evidence in the context of camera\ntrapping to demonstrate that increasing sample size will lead to improved\naccuracy. In this study we explore in depth the issues of deep learning model\nperformance for progressively increasing per class (species) sample sizes. We\nalso provide ecologists with an approximation formula to estimate how many\nimages per animal species they need for certain accuracy level a priori. This\nwill help ecologists for optimal allocation of resources, work and efficient\nstudy design. In order to investigate the effect of number of training images;\nseven training sets with 10, 20, 50, 150, 500, 1000 images per class were\ndesigned. Six deep learning architectures namely ResNet-18, ResNet-50,\nResNet-152, DnsNet-121, DnsNet-161, and DnsNet-201 were trained and tested on a\ncommon exclusive testing set of 250 images per class. The whole experiment was\nrepeated on three similar datasets from Australia, Africa and North America and\nthe results were compared. Simple regression equations for use by practitioners\nto approximate model performance metrics are provided. Generalized additive\nmodels (GAM) are shown to be effective in modelling DL performance metrics\nbased on the number of training images per class, tuning scheme and dataset.\n  Key-words: Camera Traps, Deep Learning, Ecological Informatics, Generalised\nAdditive Models, Learning Curves, Predictive Modelling, Wildlife.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 06:28:35 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Shahinfar", "Saleh", ""], ["Meek", "Paul", ""], ["Falzon", "Greg", ""]]}, {"id": "2010.08188", "submitter": "Sunghyun Park", "authors": "Sunghyun Park, Kangyeol Kim, Junsoo Lee, Jaegul Choo, Joonseok Lee,\n  Sookyung Kim, Edward Choi", "title": "Vid-ODE: Continuous-Time Video Generation with Neural Ordinary\n  Differential Equation", "comments": "Accepted to AAAI 2021, 22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video generation models often operate under the assumption of fixed frame\nrates, which leads to suboptimal performance when it comes to handling flexible\nframe rates (e.g., increasing the frame rate of the more dynamic portion of the\nvideo as well as handling missing video frames). To resolve the restricted\nnature of existing video generation models' ability to handle arbitrary\ntimesteps, we propose continuous-time video generation by combining neural ODE\n(Vid-ODE) with pixel-level video processing techniques. Using ODE-ConvGRU as an\nencoder, a convolutional version of the recently proposed neural ODE, which\nenables us to learn continuous-time dynamics, Vid-ODE can learn the\nspatio-temporal dynamics of input videos of flexible frame rates. The decoder\nintegrates the learned dynamics function to synthesize video frames at any\ngiven timesteps, where the pixel-level composition technique is used to\nmaintain the sharpness of individual frames. With extensive experiments on four\nreal-world video datasets, we verify that the proposed Vid-ODE outperforms\nstate-of-the-art approaches under various video generation settings, both\nwithin the trained time range (interpolation) and beyond the range\n(extrapolation). To the best of our knowledge, Vid-ODE is the first work\nsuccessfully performing continuous-time video generation using real-world\nvideos.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 06:50:47 GMT"}, {"version": "v2", "created": "Tue, 30 Mar 2021 13:17:23 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Park", "Sunghyun", ""], ["Kim", "Kangyeol", ""], ["Lee", "Junsoo", ""], ["Choo", "Jaegul", ""], ["Lee", "Joonseok", ""], ["Kim", "Sookyung", ""], ["Choi", "Edward", ""]]}, {"id": "2010.08189", "submitter": "Wei Chen", "authors": "Wei Chen and Weiping Wang and Li Liu and Michael S. Lew", "title": "New Ideas and Trends in Deep Multimodal Content Understanding: A Review", "comments": "Accepted by Neurocomputing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The focus of this survey is on the analysis of two modalities of multimodal\ndeep learning: image and text. Unlike classic reviews of deep learning where\nmonomodal image classifiers such as VGG, ResNet and Inception module are\ncentral topics, this paper will examine recent multimodal deep models and\nstructures, including auto-encoders, generative adversarial nets and their\nvariants. These models go beyond the simple image classifiers in which they can\ndo uni-directional (e.g. image captioning, image generation) and bi-directional\n(e.g. cross-modal retrieval, visual question answering) multimodal tasks.\nBesides, we analyze two aspects of the challenge in terms of better content\nunderstanding in deep multimodal applications. We then introduce current ideas\nand trends in deep multimodal feature learning, such as feature embedding\napproaches and objective function design, which are crucial in overcoming the\naforementioned challenges. Finally, we include several promising directions for\nfuture research.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 06:50:54 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Chen", "Wei", ""], ["Wang", "Weiping", ""], ["Liu", "Li", ""], ["Lew", "Michael S.", ""]]}, {"id": "2010.08190", "submitter": "Yuang Shi", "authors": "Yuang Shi, Chen Zu, Mei Hong, Luping Zhou, Lei Wang, Xi Wu, Jiliu\n  Zhou, Daoqiang Zhang, Yan Wang", "title": "ASMFS: Adaptive-Similarity-based Multi-modality Feature Selection for\n  Classification of Alzheimer's Disease", "comments": "27 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the increasing amounts of high-dimensional heterogeneous data to be\nprocessed, multi-modality feature selection has become an important research\ndirection in medical image analysis. Traditional methods usually depict the\ndata structure using fixed and predefined similarity matrix for each modality\nseparately, without considering the potential relationship structure across\ndifferent modalities. In this paper, we propose a novel multi-modality feature\nselection method, which performs feature selection and local similarity\nlearning simultaniously. Specially, a similarity matrix is learned by jointly\nconsidering different imaging modalities. And at the same time, feature\nselection is conducted by imposing sparse l_{2, 1} norm constraint. The\neffectiveness of our proposed joint learning method can be well demonstrated by\nthe experimental results on Alzheimer's Disease Neuroimaging Initiative (ADNI)\ndataset, which outperforms existing the state-of-the-art multi-modality\napproaches.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 06:53:27 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Shi", "Yuang", ""], ["Zu", "Chen", ""], ["Hong", "Mei", ""], ["Zhou", "Luping", ""], ["Wang", "Lei", ""], ["Wu", "Xi", ""], ["Zhou", "Jiliu", ""], ["Zhang", "Daoqiang", ""], ["Wang", "Yan", ""]]}, {"id": "2010.08201", "submitter": "Asifullah Khan", "authors": "Muhammad Abbas, Asifullah Khan, Aqsa Saeed Qureshi, Muhammad Waleed\n  Khan", "title": "Extracting Signals of Higgs Boson From Background Noise Using Deep\n  Neural Networks", "comments": "Figures: 2, Table: 1", "journal-ref": null, "doi": null, "report-no": null, "categories": "hep-ph cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Higgs boson is a fundamental particle, and the classification of Higgs\nsignals is a well-known problem in high energy physics. The identification of\nthe Higgs signal is a challenging task because its signal has a resemblance to\nthe background signals. This study proposes a Higgs signal classification using\na novel combination of random forest, auto encoder and deep auto encoder to\nbuild a robust and generalized Higgs boson prediction system to discriminate\nthe Higgs signal from the background noise. The proposed ensemble technique is\nbased on achieving diversity in the decision space, and the results show good\ndiscrimination power on the private leaderboard; achieving an area under the\nReceiver Operating Characteristic curve of 0.9 and an Approximate Median\nSignificance score of 3.429.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 07:19:39 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Abbas", "Muhammad", ""], ["Khan", "Asifullah", ""], ["Qureshi", "Aqsa Saeed", ""], ["Khan", "Muhammad Waleed", ""]]}, {"id": "2010.08202", "submitter": "Xiaotong Chen", "authors": "Xiaotong Chen, Kaizhi Zheng, Zhen Zeng, Shreshtha Basu, James Cooney,\n  Jana Pavlasek, Odest Chadwicke Jenkins", "title": "Manipulation-Oriented Object Perception in Clutter through Affordance\n  Coordinate Frames", "comments": "8 pages, 10 figures, submitted to RA-L, video link:\n  https://www.youtube.com/watch?v=7P9_O9wveYk, github link:\n  https://github.com/cxt98/ACF_perception", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to enable robust operation in unstructured environments, robots\nshould be able to generalize manipulation actions to novel object instances.\nFor example, to pour and serve a drink, a robot should be able to recognize\nnovel containers which afford the task. Most importantly, robots should be able\nto manipulate these novel containers to fulfill the task. To achieve this, we\naim to provide robust and generalized perception of object affordances and\ntheir associated manipulation poses for reliable manipulation. In this work, we\ncombine the notions of affordance and category-level pose, and introduce the\nAffordance Coordinate Frame (ACF). With ACF, we represent each object class in\nterms of individual affordance parts and the compatibility between them, where\neach part is associated with a part category-level pose for robot manipulation.\nIn our experiments, we demonstrate that ACF outperforms state-of-the-art\nmethods for object detection, as well as category-level pose estimation for\nobject parts. We further demonstrate the applicability of ACF to robot\nmanipulation tasks through experiments in a simulated environment.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 07:24:32 GMT"}, {"version": "v2", "created": "Sun, 11 Apr 2021 20:11:10 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Chen", "Xiaotong", ""], ["Zheng", "Kaizhi", ""], ["Zeng", "Zhen", ""], ["Basu", "Shreshtha", ""], ["Cooney", "James", ""], ["Pavlasek", "Jana", ""], ["Jenkins", "Odest Chadwicke", ""]]}, {"id": "2010.08209", "submitter": "Ruohua Shi", "authors": "Ruohua Shi, Wenyao Wang, Zhixuan Li, Liuyuan He, Kaiwen Sheng, Lei Ma,\n  Kai Du, Tingting Jiang, Tiejun Huang", "title": "Human Perception-based Evaluation Criterion for Ultra-high Resolution\n  Cell Membrane Segmentation", "comments": "submitted to ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer vision technology is widely used in biological and medical data\nanalysis and understanding. However, there are still two major bottlenecks in\nthe field of cell membrane segmentation, which seriously hinder further\nresearch: lack of sufficient high-quality data and lack of suitable evaluation\ncriteria. In order to solve these two problems, this paper first proposes an\nUltra-high Resolution Image Segmentation dataset for the Cell membrane, called\nU-RISC, the largest annotated Electron Microscopy (EM) dataset for the Cell\nmembrane with multiple iterative annotations and uncompressed high-resolution\nraw data. During the analysis process of the U-RISC, we found that the current\npopular segmentation evaluation criteria are inconsistent with human\nperception. This interesting phenomenon is confirmed by a subjective experiment\ninvolving twenty people. Furthermore, to resolve this inconsistency, we propose\na new evaluation criterion called Perceptual Hausdorff Distance (PHD) to\nmeasure the quality of cell membrane segmentation results. Detailed performance\ncomparison and discussion of classic segmentation methods along with two\niterative manual annotation results under existing evaluation criteria and PHD\nis given.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 07:39:17 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Shi", "Ruohua", ""], ["Wang", "Wenyao", ""], ["Li", "Zhixuan", ""], ["He", "Liuyuan", ""], ["Sheng", "Kaiwen", ""], ["Ma", "Lei", ""], ["Du", "Kai", ""], ["Jiang", "Tingting", ""], ["Huang", "Tiejun", ""]]}, {"id": "2010.08219", "submitter": "Yuge Zhang", "authors": "Yuge Zhang, Quanlu Zhang, Yaming Yang", "title": "How Does Supernet Help in Neural Architecture Search?", "comments": "Accepted by 2nd Workshop on Neural Architecture Search at ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weight sharing, as an approach to speed up architecture performance\nestimation has received wide attention. Instead of training each architecture\nseparately, weight sharing builds a supernet that assembles all the\narchitectures as its submodels. However, there has been debate over whether the\nNAS process actually benefits from weight sharing, due to the gap between\nsupernet optimization and the objective of NAS. To further understand the\neffect of weight sharing on NAS, we conduct a comprehensive analysis on five\nsearch spaces, including NAS-Bench-101, NAS-Bench-201, DARTS-CIFAR10,\nDARTS-PTB, and ProxylessNAS. We find that weight sharing works well on some\nsearch spaces but fails on others. Taking a step forward, we further identified\nbiases accounting for such phenomenon and the capacity of weight sharing. Our\nwork is expected to inspire future NAS researchers to better leverage the power\nof weight sharing.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 08:07:03 GMT"}, {"version": "v2", "created": "Wed, 5 May 2021 07:26:48 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Zhang", "Yuge", ""], ["Zhang", "Quanlu", ""], ["Yang", "Yaming", ""]]}, {"id": "2010.08221", "submitter": "Michael F\\\"urst", "authors": "Michael F\\\"urst, Shriya T. P. Gupta, Ren\\'e Schuster, Oliver\n  Wasenm\\\"uller, Didier Stricker", "title": "HPERL: 3D Human Pose Estimation from RGB and LiDAR", "comments": "7 pages, 6 figures, 4 tables, LiDAR and RGB Fusion", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In-the-wild human pose estimation has a huge potential for various fields,\nranging from animation and action recognition to intention recognition and\nprediction for autonomous driving. The current state-of-the-art is focused only\non RGB and RGB-D approaches for predicting the 3D human pose. However, not\nusing precise LiDAR depth information limits the performance and leads to very\ninaccurate absolute pose estimation. With LiDAR sensors becoming more\naffordable and common on robots and autonomous vehicle setups, we propose an\nend-to-end architecture using RGB and LiDAR to predict the absolute 3D human\npose with unprecedented precision. Additionally, we introduce a\nweakly-supervised approach to generate 3D predictions using 2D pose annotations\nfrom PedX [1]. This allows for many new opportunities in the field of 3D human\npose estimation.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 08:09:49 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["F\u00fcrst", "Michael", ""], ["Gupta", "Shriya T. P.", ""], ["Schuster", "Ren\u00e9", ""], ["Wasenm\u00fcller", "Oliver", ""], ["Stricker", "Didier", ""]]}, {"id": "2010.08243", "submitter": "Cristiano Saltori", "authors": "Cristiano Saltori, St\\'ephane Lathuili\\'ere, Nicu Sebe, Elisa Ricci,\n  Fabio Galasso", "title": "SF-UDA$^{3D}$: Source-Free Unsupervised Domain Adaptation for\n  LiDAR-Based 3D Object Detection", "comments": "Accepted paper at 3DV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  3D object detectors based only on LiDAR point clouds hold the\nstate-of-the-art on modern street-view benchmarks. However, LiDAR-based\ndetectors poorly generalize across domains due to domain shift. In the case of\nLiDAR, in fact, domain shift is not only due to changes in the environment and\nin the object appearances, as for visual data from RGB cameras, but is also\nrelated to the geometry of the point clouds (e.g., point density variations).\nThis paper proposes SF-UDA$^{3D}$, the first Source-Free Unsupervised Domain\nAdaptation (SF-UDA) framework to domain-adapt the state-of-the-art PointRCNN 3D\ndetector to target domains for which we have no annotations (unsupervised),\nneither we hold images nor annotations of the source domain (source-free).\nSF-UDA$^{3D}$ is novel on both aspects. Our approach is based on\npseudo-annotations, reversible scale-transformations and motion coherency.\nSF-UDA$^{3D}$ outperforms both previous domain adaptation techniques based on\nfeatures alignment and state-of-the-art 3D object detection methods which\nadditionally use few-shot target annotations or target annotation statistics.\nThis is demonstrated by extensive experiments on two large-scale datasets,\ni.e., KITTI and nuScenes.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 08:44:49 GMT"}, {"version": "v2", "created": "Mon, 19 Oct 2020 14:19:30 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Saltori", "Cristiano", ""], ["Lathuili\u00e9re", "St\u00e9phane", ""], ["Sebe", "Nicu", ""], ["Ricci", "Elisa", ""], ["Galasso", "Fabio", ""]]}, {"id": "2010.08244", "submitter": "Baifeng Shi", "authors": "Baifeng Shi, Judy Hoffman, Kate Saenko, Trevor Darrell, Huijuan Xu", "title": "Auxiliary Task Reweighting for Minimum-data Learning", "comments": "NeurIPS 2020. Project page:\n  https://sites.google.com/view/auxiliary-task-reweighting/home", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Supervised learning requires a large amount of training data, limiting its\napplication where labeled data is scarce. To compensate for data scarcity, one\npossible method is to utilize auxiliary tasks to provide additional supervision\nfor the main task. Assigning and optimizing the importance weights for\ndifferent auxiliary tasks remains an crucial and largely understudied research\nquestion. In this work, we propose a method to automatically reweight auxiliary\ntasks in order to reduce the data requirement on the main task. Specifically,\nwe formulate the weighted likelihood function of auxiliary tasks as a surrogate\nprior for the main task. By adjusting the auxiliary task weights to minimize\nthe divergence between the surrogate prior and the true prior of the main task,\nwe obtain a more accurate prior estimation, achieving the goal of minimizing\nthe required amount of training data for the main task and avoiding a costly\ngrid search. In multiple experimental settings (e.g. semi-supervised learning,\nmulti-label classification), we demonstrate that our algorithm can effectively\nutilize limited labeled data of the main task with the benefit of auxiliary\ntasks compared with previous task reweighting methods. We also show that under\nextreme cases with only a few extra examples (e.g. few-shot domain adaptation),\nour algorithm results in significant improvement over the baseline.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 08:45:37 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Shi", "Baifeng", ""], ["Hoffman", "Judy", ""], ["Saenko", "Kate", ""], ["Darrell", "Trevor", ""], ["Xu", "Huijuan", ""]]}, {"id": "2010.08262", "submitter": "Bernd Illing", "authors": "Bernd Illing, Jean Ventura, Guillaume Bellec, Wulfram Gerstner", "title": "Local plasticity rules can learn deep representations using\n  self-supervised contrastive predictions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.AR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning in the brain is poorly understood and learning rules that respect\nbiological constraints, yet yield deep hierarchical representations, are still\nunknown. Here, we propose a learning rule that takes inspiration from\nneuroscience and recent advances in self-supervised deep learning. Learning\nminimizes a simple layer-specific loss function and does not need to\nback-propagate error signals within or between layers. Instead, weight updates\nfollow a local, Hebbian, learning rule that only depends on pre- and\npost-synaptic neuronal activity, predictive dendritic input and widely\nbroadcasted modulation factors which are identical for large groups of neurons.\nThe learning rule applies contrastive predictive learning to a causal,\nbiological setting using saccades (i.e. rapid shifts in gaze direction). We\nfind that networks trained with this self-supervised and local rule build deep\nhierarchical representations of images, speech and video.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 09:32:35 GMT"}, {"version": "v2", "created": "Fri, 12 Mar 2021 10:13:54 GMT"}, {"version": "v3", "created": "Fri, 19 Mar 2021 10:51:10 GMT"}, {"version": "v4", "created": "Wed, 16 Jun 2021 13:30:39 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Illing", "Bernd", ""], ["Ventura", "Jean", ""], ["Bellec", "Guillaume", ""], ["Gerstner", "Wulfram", ""]]}, {"id": "2010.08276", "submitter": "Biao Zhang", "authors": "Biao Zhang, Peter Wonka", "title": "Training Data Generating Networks: Linking 3D Shapes and Few-Shot\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CG cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel 3d shape representation for 3d shape reconstruction from a\nsingle image. Rather than predicting a shape directly, we train a network to\ngenerate a training set which will be feed into another learning algorithm to\ndefine the shape. Training data generating networks establish a link between\nfew-shot learning and 3d shape analysis. We propose a novel meta-learning\nframework to jointly train the data generating network and other components. We\nimprove upon recent work on standard benchmarks for 3d shape reconstruction,\nbut our novel shape representation has many applications.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 09:52:13 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Zhang", "Biao", ""], ["Wonka", "Peter", ""]]}, {"id": "2010.08278", "submitter": "Cian Ryan", "authors": "Cian Ryan, Brian O Sullivan, Amr Elrasad, Joe Lemley, Paul Kielty,\n  Christoph Posch and Etienne Perot", "title": "Real-Time Face & Eye Tracking and Blink Detection using Event Cameras", "comments": "20 Pages, 8 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Event cameras contain emerging, neuromorphic vision sensors that capture\nlocal light intensity changes at each pixel, generating a stream of\nasynchronous events. This way of acquiring visual information constitutes a\ndeparture from traditional frame based cameras and offers several significant\nadvantages: low energy consumption, high temporal resolution, high dynamic\nrange and low latency. Driver monitoring systems (DMS) are in-cabin safety\nsystems designed to sense and understand a drivers physical and cognitive\nstate. Event cameras are particularly suited to DMS due to their inherent\nadvantages. This paper proposes a novel method to simultaneously detect and\ntrack faces and eyes for driver monitoring. A unique, fully convolutional\nrecurrent neural network architecture is presented. To train this network, a\nsynthetic event-based dataset is simulated with accurate bounding box\nannotations, called Neuromorphic HELEN. Additionally, a method to detect and\nanalyse drivers eye blinks is proposed, exploiting the high temporal resolution\nof event cameras. Behaviour of blinking provides greater insights into a driver\nlevel of fatigue or drowsiness. We show that blinks have a unique temporal\nsignature that can be better captured by event cameras.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 10:02:41 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Ryan", "Cian", ""], ["Sullivan", "Brian O", ""], ["Elrasad", "Amr", ""], ["Lemley", "Joe", ""], ["Kielty", "Paul", ""], ["Posch", "Christoph", ""], ["Perot", "Etienne", ""]]}, {"id": "2010.08296", "submitter": "Rhys Newbury", "authors": "Keenan Granland, Rhys Newbury, David Ting and Chao Chen", "title": "Minimizing Labeling Effort for Tree Skeleton Segmentation using an\n  Automated Iterative Training Methodology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training of convolutional neural networks for semantic segmentation requires\naccurate pixel-wise labeling which requires large amounts of human effort. The\nhuman-in-the-loop method reduces labeling effort; however, it requires human\nintervention for each image. This paper describes a general iterative training\nmethodology for semantic segmentation, Automating-the-Loop. This aims to\nreplicate the manual adjustments of the human-in-the-loop method with an\nautomated process, hence, drastically reducing labeling effort. Using the\napplication of detecting partially occluded apple tree segmentation, we compare\nmanually labeled annotations, self-training, human-in-the-loop, and\nAutomating-the-Loop methods in both the quality of the trained convolutional\nneural networks, and the effort needed to create them. The convolutional neural\nnetwork (U-Net) performance is analyzed using traditional metrics and a new\nmetric, Complete Grid Scan, which promotes connectivity and low noise. It is\nshown that in our application, the new Automating-the-Loop method greatly\nreduces the labeling effort while producing comparable performance to both\nhuman-in-the-loop and complete manual labeling methods.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 10:38:43 GMT"}, {"version": "v2", "created": "Wed, 14 Apr 2021 12:23:55 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Granland", "Keenan", ""], ["Newbury", "Rhys", ""], ["Ting", "David", ""], ["Chen", "Chao", ""]]}, {"id": "2010.08310", "submitter": "David Gillsj\\\"o", "authors": "David Gillsj\\\"o, Kalle {\\AA}str\\\"om", "title": "In Depth Bayesian Semantic Scene Completion", "comments": "Accepted at ICPR2020, 10 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work studies Semantic Scene Completion which aims to predict a 3D\nsemantic segmentation of our surroundings, even though some areas are occluded.\nFor this we construct a Bayesian Convolutional Neural Network (BCNN), which is\nnot only able to perform the segmentation, but also predict model uncertainty.\nThis is an important feature not present in standard CNNs.\n  We show on the MNIST dataset that the Bayesian approach performs equal or\nbetter to the standard CNN when processing digits unseen in the training phase\nwhen looking at accuracy, precision and recall. With the added benefit of\nhaving better calibrated scores and the ability to express model uncertainty.\n  We then show results for the Semantic Scene Completion task where a category\nis introduced at test time on the SUNCG dataset. In this more complex task the\nBayesian approach outperforms the standard CNN. Showing better Intersection\nover Union score and excels in Average Precision and separation scores.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 11:05:31 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Gillsj\u00f6", "David", ""], ["\u00c5str\u00f6m", "Kalle", ""]]}, {"id": "2010.08321", "submitter": "Yichen Qian", "authors": "Yichen Qian, Zhiyu Tan, Xiuyu Sun, Ming Lin, Dongyang Li, Zhenhong\n  Sun, Hao Li, Rong Jin", "title": "Learning Accurate Entropy Model with Global Reference for Image\n  Compression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent deep image compression neural networks, the entropy model plays a\ncritical role in estimating the prior distribution of deep image encodings.\nExisting methods combine hyperprior with local context in the entropy\nestimation function. This greatly limits their performance due to the absence\nof a global vision. In this work, we propose a novel Global Reference Model for\nimage compression to effectively leverage both the local and the global context\ninformation, leading to an enhanced compression rate. The proposed method scans\ndecoded latents and then finds the most relevant latent to assist the\ndistribution estimating of the current latent. A by-product of this work is the\ninnovation of a mean-shifting GDN module that further improves the performance.\nExperimental results demonstrate that the proposed model outperforms the\nrate-distortion performance of most of the state-of-the-art methods in the\nindustry.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 11:27:46 GMT"}, {"version": "v2", "created": "Thu, 29 Oct 2020 06:30:13 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Qian", "Yichen", ""], ["Tan", "Zhiyu", ""], ["Sun", "Xiuyu", ""], ["Lin", "Ming", ""], ["Li", "Dongyang", ""], ["Sun", "Zhenhong", ""], ["Li", "Hao", ""], ["Jin", "Rong", ""]]}, {"id": "2010.08350", "submitter": "Javier Hidalgo-Carri\\'o", "authors": "Javier Hidalgo-Carri\\'o, Daniel Gehrig and Davide Scaramuzza", "title": "Learning Monocular Dense Depth from Events", "comments": "IEEE International Conference on 3D Vision (3DV), 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Event cameras are novel sensors that output brightness changes in the form of\na stream of asynchronous events instead of intensity frames. Compared to\nconventional image sensors, they offer significant advantages: high temporal\nresolution, high dynamic range, no motion blur, and much lower bandwidth.\nRecently, learning-based approaches have been applied to event-based data, thus\nunlocking their potential and making significant progress in a variety of\ntasks, such as monocular depth prediction. Most existing approaches use\nstandard feed-forward architectures to generate network predictions, which do\nnot leverage the temporal consistency presents in the event stream. We propose\na recurrent architecture to solve this task and show significant improvement\nover standard feed-forward methods. In particular, our method generates dense\ndepth predictions using a monocular setup, which has not been shown previously.\nWe pretrain our model using a new dataset containing events and depth maps\nrecorded in the CARLA simulator. We test our method on the Multi Vehicle Stereo\nEvent Camera Dataset (MVSEC). Quantitative experiments show up to 50%\nimprovement in average depth error with respect to previous event-based\nmethods.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 12:36:23 GMT"}, {"version": "v2", "created": "Thu, 22 Oct 2020 08:33:43 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Hidalgo-Carri\u00f3", "Javier", ""], ["Gehrig", "Daniel", ""], ["Scaramuzza", "Davide", ""]]}, {"id": "2010.08357", "submitter": "Yinhao Li", "authors": "Yinhao Li, Yutaro Iwamoto, Lanfen Lin, Rui Xu, Yen-Wei Chen", "title": "VolumeNet: A Lightweight Parallel Network for Super-Resolution of\n  Medical Volumetric Data", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2021.3076285", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning-based super-resolution (SR) techniques have generally achieved\nexcellent performance in the computer vision field. Recently, it has been\nproven that three-dimensional (3D) SR for medical volumetric data delivers\nbetter visual results than conventional two-dimensional (2D) processing.\nHowever, deepening and widening 3D networks increases training difficulty\nsignificantly due to the large number of parameters and small number of\ntraining samples. Thus, we propose a 3D convolutional neural network (CNN) for\nSR of medical volumetric data called ParallelNet using parallel connections. We\nconstruct a parallel connection structure based on the group convolution and\nfeature aggregation to build a 3D CNN that is as wide as possible with few\nparameters. As a result, the model thoroughly learns more feature maps with\nlarger receptive fields. In addition, to further improve accuracy, we present\nan efficient version of ParallelNet (called VolumeNet), which reduces the\nnumber of parameters and deepens ParallelNet using a proposed lightweight\nbuilding block module called the Queue module. Unlike most lightweight CNNs\nbased on depthwise convolutions, the Queue module is primarily constructed\nusing separable 2D cross-channel convolutions. As a result, the number of\nnetwork parameters and computational complexity can be reduced significantly\nwhile maintaining accuracy due to full channel fusion. Experimental results\ndemonstrate that the proposed VolumeNet significantly reduces the number of\nmodel parameters and achieves high precision results compared to\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 12:53:15 GMT"}, {"version": "v2", "created": "Sat, 24 Oct 2020 06:00:55 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Li", "Yinhao", ""], ["Iwamoto", "Yutaro", ""], ["Lin", "Lanfen", ""], ["Xu", "Rui", ""], ["Chen", "Yen-Wei", ""]]}, {"id": "2010.08360", "submitter": "Wei Zhang", "authors": "Zhaowen Wang, Wei Zhang, Zhiming Wang", "title": "G-DARTS-A: Groups of Channel Parallel Sampling with Attention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Differentiable Architecture Search (DARTS) provides a baseline for searching\neffective network architectures based gradient, but it is accompanied by huge\ncomputational overhead in searching and training network architecture.\nRecently, many novel works have improved DARTS. Particularly,\nPartially-Connected DARTS(PC-DARTS) proposed the partial channel sampling\ntechnique which achieved good results. In this work, we found that the backbone\nprovided by DARTS is prone to overfitting. To mitigate this problem, we propose\nan approach named Group-DARTS with Attention (G-DARTS-A), using multiple groups\nof channels for searching. Inspired by the partially sampling strategy of\nPC-DARTS, we use groups channels to sample the super-network to perform a more\nefficient search while maintaining the relative integrity of the network\ninformation. In order to relieve the competition between channel groups and\nkeep channel balance, we follow the attention mechanism in\nSqueeze-and-Excitation Network. Each group of channels shares defined weights\nthence they can provide different suggestion for searching. The searched\narchitecture is more powerful and better adapted to different deployments.\nSpecifically, by only using the attention module on DARTS we achieved an error\nrate of 2.82%/16.36% on CIFAR10/100 with 0.3GPU-days for search process on\nCIFAR10. Apply our G-DARTS-A to DARTS/PC-DARTS, an error rate of 2.57%/2.61% on\nCIFAR10 with 0.5/0.4 GPU-days is achieved.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 12:58:08 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Wang", "Zhaowen", ""], ["Zhang", "Wei", ""], ["Wang", "Zhiming", ""]]}, {"id": "2010.08365", "submitter": "Li Yuan", "authors": "Li Yuan, Yichen Zhou, Shuning Chang, Ziyuan Huang, Yunpeng Chen,\n  Xuecheng Nie, Tao Wang, Jiashi Feng, Shuicheng Yan", "title": "Toward Accurate Person-level Action Recognition in Videos of Crowded\n  Scenes", "comments": "1'st Place in ACM Multimedia Grand Challenge: Human in Events,\n  Track4: Person-level Action Recognition in Complex Events", "journal-ref": "ACM Multimedia 2020", "doi": "10.1145/3394171.3416301", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting and recognizing human action in videos with crowded scenes is a\nchallenging problem due to the complex environment and diversity events. Prior\nworks always fail to deal with this problem in two aspects: (1) lacking\nutilizing information of the scenes; (2) lacking training data in the crowd and\ncomplex scenes. In this paper, we focus on improving spatio-temporal action\nrecognition by fully-utilizing the information of scenes and collecting new\ndata. A top-down strategy is used to overcome the limitations. Specifically, we\nadopt a strong human detector to detect the spatial location of each frame. We\nthen apply action recognition models to learn the spatio-temporal information\nfrom video frames on both the HIE dataset and new data with diverse scenes from\nthe internet, which can improve the generalization ability of our model.\nBesides, the scenes information is extracted by the semantic segmentation model\nto assistant the process. As a result, our method achieved an average 26.05\nwf\\_mAP (ranking 1st place in the ACM MM grand challenge 2020: Human in\nEvents).\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 13:08:50 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Yuan", "Li", ""], ["Zhou", "Yichen", ""], ["Chang", "Shuning", ""], ["Huang", "Ziyuan", ""], ["Chen", "Yunpeng", ""], ["Nie", "Xuecheng", ""], ["Wang", "Tao", ""], ["Feng", "Jiashi", ""], ["Yan", "Shuicheng", ""]]}, {"id": "2010.08377", "submitter": "Robert Geirhos", "authors": "Robert Geirhos, Kantharaju Narayanappa, Benjamin Mitzkus, Matthias\n  Bethge, Felix A. Wichmann, Wieland Brendel", "title": "On the surprising similarities between supervised and self-supervised\n  models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How do humans learn to acquire a powerful, flexible and robust representation\nof objects? While much of this process remains unknown, it is clear that humans\ndo not require millions of object labels. Excitingly, recent algorithmic\nadvancements in self-supervised learning now enable convolutional neural\nnetworks (CNNs) to learn useful visual object representations without\nsupervised labels, too. In the light of this recent breakthrough, we here\ncompare self-supervised networks to supervised models and human behaviour. We\ntested models on 15 generalisation datasets for which large-scale human\nbehavioural data is available (130K highly controlled psychophysical trials).\nSurprisingly, current self-supervised CNNs share four key characteristics of\ntheir supervised counterparts: (1.) relatively poor noise robustness (with the\nnotable exception of SimCLR), (2.) non-human category-level error patterns,\n(3.) non-human image-level error patterns (yet high similarity to supervised\nmodel errors) and (4.) a bias towards texture. Taken together, these results\nsuggest that the strategies learned through today's supervised and\nself-supervised training objectives end up being surprisingly similar, but\ndistant from human-like behaviour. That being said, we are clearly just at the\nbeginning of what could be called a self-supervised revolution of machine\nvision, and we are hopeful that future self-supervised models behave\ndifferently from supervised ones, and---perhaps---more similar to robust human\nobject recognition.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 13:28:13 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Geirhos", "Robert", ""], ["Narayanappa", "Kantharaju", ""], ["Mitzkus", "Benjamin", ""], ["Bethge", "Matthias", ""], ["Wichmann", "Felix A.", ""], ["Brendel", "Wieland", ""]]}, {"id": "2010.08390", "submitter": "Eleni Bohacek", "authors": "Eleni Bohacek, Andrew J. Coates, David R. Selviah", "title": "Volumetric Calculation of Quantization Error in 3-D Vision Systems", "comments": "As submitted to IEEE Transactions on Pattern Analysis and Machine\n  Intelligence on 4th September 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates how the inherent quantization of camera sensors\nintroduces uncertainty in the calculated position of an observed feature during\n3-D mapping. It is typically assumed that pixels and scene features are points,\nhowever, a pixel is a two-dimensional area that maps onto multiple points in\nthe scene. This uncertainty region is a bound for quantization error in the\ncalculated point positions. Earlier studies calculated the volume of two\nintersecting pixel views, approximated as a cuboid, by projecting pyramids and\ncones from the pixels into the scene. In this paper, we reverse this approach\nby generating an array of scene points and calculating which scene points are\ndetected by which pixel in each camera. This enables us to map the uncertainty\nregions for every pixel correspondence for a given camera system in one\ncalculation, without approximating the complex shapes. The dependence of the\nvolumes of the uncertainty regions on camera baseline length, focal length,\npixel size, and distance to object, shows that earlier studies overestimated\nthe quantization error by at least a factor of two. For static camera systems\nthe method can also be used to determine volumetric scene geometry without the\nneed to calculate disparity maps.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 13:48:30 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Bohacek", "Eleni", ""], ["Coates", "Andrew J.", ""], ["Selviah", "David R.", ""]]}, {"id": "2010.08391", "submitter": "Zihui Zhang", "authors": "Cuican Yu, Zihui Zhang, Huibin Li", "title": "Reconstructing A Large Scale 3D Face Dataset for Deep 3D Face\n  Identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning methods have brought many breakthroughs to computer vision,\nespecially in 2D face recognition. However, the bottleneck of deep learning\nbased 3D face recognition is that it is difficult to collect millions of 3D\nfaces, whether for industry or academia. In view of this situation, there are\nmany methods to generate more 3D faces from existing 3D faces through 3D face\ndata augmentation, which are used to train deep 3D face recognition models.\nHowever, to the best of our knowledge, there is no method to generate 3D faces\nfrom 2D face images for training deep 3D face recognition models. This letter\nfocuses on the role of reconstructed 3D facial surfaces in 3D face\nidentification and proposes a framework of 2D-aided deep 3D face\nidentification. In particular, we propose to reconstruct millions of 3D face\nscans from a large scale 2D face database (i.e.VGGFace2), using a deep learning\nbased 3D face reconstruction method (i.e.ExpNet). Then, we adopt a two-phase\ntraining approach: In the first phase, we use millions of face images to\npre-train the deep convolutional neural network (DCNN), and in the second\nphase, we use normal component images (NCI) of reconstructed 3D face scans to\ntrain the DCNN. Extensive experimental results illustrate that the proposed\napproach can greatly improve the rank-1 score of 3D face identification on the\nFRGC v2.0, the Bosphorus, and the BU-3DFE 3D face databases, compared to the\nmodel trained by 2D face images. Finally, our proposed approach achieves\nstate-of-the-art rank-1 scores on the FRGC v2.0 (97.6%), Bosphorus (98.4%), and\nBU-3DFE (98.8%) databases. The experimental results show that the reconstructed\n3D facial surfaces are useful and our 2D-aided deep 3D face identification\nframework is meaningful, facing the scarcity of 3D faces.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 13:48:38 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Yu", "Cuican", ""], ["Zhang", "Zihui", ""], ["Li", "Huibin", ""]]}, {"id": "2010.08402", "submitter": "Xiao Liu", "authors": "Xiao Liu, Jiajie Zhang, Siting Li, Zuotong Wu, Yang Yu", "title": "Difference-in-Differences: Bridging Normalization and Disentanglement in\n  PG-GAN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  What mechanisms causes GAN's entanglement? Although developing disentangled\nGAN has attracted sufficient attention, it is unclear how entanglement is\noriginated by GAN transformation. We in this research propose a\ndifference-in-difference (DID) counterfactual framework to design experiments\nfor analyzing the entanglement mechanism in on of the Progressive-growing GAN\n(PG-GAN). Our experiment clarify the mechanisms how pixel normalization causes\nPG-GAN entanglement during a input-unit-ablation transformation. We discover\nthat pixel normalization causes object entanglement by in-painting the area\noccupied by ablated objects. We also discover the unit-object relation\ndetermines whether and how pixel normalization causes objects entanglement. Our\nDID framework theoretically guarantees that the mechanisms that we discover is\nsolid, explainable and comprehensively.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 14:02:53 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Liu", "Xiao", ""], ["Zhang", "Jiajie", ""], ["Li", "Siting", ""], ["Wu", "Zuotong", ""], ["Yu", "Yang", ""]]}, {"id": "2010.08437", "submitter": "Chiayen Chiang", "authors": "Chia-Yen Chiang, Chloe Barnes, Plamen Angelov, and Richard Jiang", "title": "Deep Learning based Automated Forest Health Diagnosis from Aerial Images", "comments": "16 pages", "journal-ref": "IEEE Access, vol. 8, pp. 144064-144076, 2020", "doi": "10.1109/ACCESS.2020.3012417.", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Global climate change has had a drastic impact on our environment. Previous\nstudy showed that pest disaster occured from global climate change may cause a\ntremendous number of trees died and they inevitably became a factor of forest\nfire. An important portent of the forest fire is the condition of forests.\nAerial image-based forest analysis can give an early detection of dead trees\nand living trees. In this paper, we applied a synthetic method to enlarge\nimagery dataset and present a new framework for automated dead tree detection\nfrom aerial images using a re-trained Mask RCNN (Mask Region-based\nConvolutional Neural Network) approach, with a transfer learning scheme. We\napply our framework to our aerial imagery datasets,and compare eight fine-tuned\nmodels. The mean average precision score (mAP) for the best of these models\nreaches 54%. Following the automated detection, we are able to automatically\nproduce and calculate number of dead tree masks to label the dead trees in an\nimage, as an indicator of forest health that could be linked to the causal\nanalysis of environmental changes and the predictive likelihood of forest fire.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 15:07:56 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Chiang", "Chia-Yen", ""], ["Barnes", "Chloe", ""], ["Angelov", "Plamen", ""], ["Jiang", "Richard", ""]]}, {"id": "2010.08486", "submitter": "Maksim Levental", "authors": "Maksim Levental, Ryan Chard, Joseph A. Libera, Kyle Chard, Aarthi\n  Koripelly, Jakob R. Elias, Marcus Schwarting, Ben Blaiszik, Marius Stan,\n  Santanu Chaudhuri, Ian Foster", "title": "Towards Online Steering of Flame Spray Pyrolysis Nanoparticle Synthesis", "comments": null, "journal-ref": null, "doi": "10.1109/XLOOP51963.2020.00011", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Flame Spray Pyrolysis (FSP) is a manufacturing technique to mass produce\nengineered nanoparticles for applications in catalysis, energy materials,\ncomposites, and more. FSP instruments are highly dependent on a number of\nadjustable parameters, including fuel injection rate, fuel-oxygen mixtures, and\ntemperature, which can greatly affect the quality, quantity, and properties of\nthe yielded nanoparticles. Optimizing FSP synthesis requires monitoring,\nanalyzing, characterizing, and modifying experimental conditions.Here, we\npropose a hybrid CPU-GPU Difference of Gaussians (DoG)method for characterizing\nthe volume distribution of unburnt solution, so as to enable near-real-time\noptimization and steering of FSP experiments. Comparisons against standard\nimplementations show our method to be an order of magnitude more efficient.\nThis surrogate signal can be deployed as a component of an online end-to-end\npipeline that maximizes the synthesis yield.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 16:38:16 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Levental", "Maksim", ""], ["Chard", "Ryan", ""], ["Libera", "Joseph A.", ""], ["Chard", "Kyle", ""], ["Koripelly", "Aarthi", ""], ["Elias", "Jakob R.", ""], ["Schwarting", "Marcus", ""], ["Blaiszik", "Ben", ""], ["Stan", "Marius", ""], ["Chaudhuri", "Santanu", ""], ["Foster", "Ian", ""]]}, {"id": "2010.08534", "submitter": "Andrew Keyes", "authors": "Andrew Keyes, Nicky Bayat, Vahid Reza Khazaie, Yalda Mohsenzadeh", "title": "Latent Vector Recovery of Audio GANs", "comments": "7 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advanced Generative Adversarial Networks (GANs) are remarkable in generating\nintelligible audio from a random latent vector. In this paper, we examine the\ntask of recovering the latent vector of both synthesized and real audio.\nPrevious works recovered latent vectors of given audio through an auto-encoder\ninspired technique that trains an encoder network either in parallel with the\nGAN or after the generator is trained. With our approach, we train a deep\nresidual neural network architecture to project audio synthesized by WaveGAN\ninto the corresponding latent space with near identical reconstruction\nperformance. To accommodate for the lack of an original latent vector for real\naudio, we optimize the residual network on the perceptual loss between the real\naudio samples and the reconstructed audio of the predicted latent vectors. In\nthe case of synthesized audio, the Mean Squared Error (MSE) between the ground\ntruth and recovered latent vector is minimized as well. We further investigated\nthe audio reconstruction performance when several gradient optimization steps\nare applied to the predicted latent vector. Through our deep neural network\nbased method of training on real and synthesized audio, we are able to predict\na latent vector that corresponds to a reasonable reconstruction of real audio.\nEven though we evaluated our method on WaveGAN, our proposed method is\nuniversal and can be applied to any other GANs.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 17:45:35 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Keyes", "Andrew", ""], ["Bayat", "Nicky", ""], ["Khazaie", "Vahid Reza", ""], ["Mohsenzadeh", "Yalda", ""]]}, {"id": "2010.08539", "submitter": "Kiana Ehsani", "authors": "Kiana Ehsani, Daniel Gordon, Thomas Nguyen, Roozbeh Mottaghi, Ali\n  Farhadi", "title": "What Can You Learn from Your Muscles? Learning Visual Representation\n  from Human Interactions", "comments": "Published as a conference paper at ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning effective representations of visual data that generalize to a\nvariety of downstream tasks has been a long quest for computer vision. Most\nrepresentation learning approaches rely solely on visual data such as images or\nvideos. In this paper, we explore a novel approach, where we use human\ninteraction and attention cues to investigate whether we can learn better\nrepresentations compared to visual-only representations. For this study, we\ncollect a dataset of human interactions capturing body part movements and gaze\nin their daily lives. Our experiments show that our \"muscly-supervised\"\nrepresentation that encodes interaction and attention cues outperforms a\nvisual-only state-of-the-art method MoCo (He et al.,2020), on a variety of\ntarget tasks: scene classification (semantic), action recognition (temporal),\ndepth estimation (geometric), dynamics prediction (physics) and walkable\nsurface estimation (affordance). Our code and dataset are available at:\nhttps://github.com/ehsanik/muscleTorch.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 17:46:53 GMT"}, {"version": "v2", "created": "Sat, 6 Mar 2021 19:28:58 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Ehsani", "Kiana", ""], ["Gordon", "Daniel", ""], ["Nguyen", "Thomas", ""], ["Mottaghi", "Roozbeh", ""], ["Farhadi", "Ali", ""]]}, {"id": "2010.08582", "submitter": "Sarah Gerard", "authors": "Sarah E. Gerard and Jacob Herrmann and Yi Xin and Kevin T. Martin and\n  Emanuele Rezoagli and Davide Ippolito and Giacomo Bellani and Maurizio Cereda\n  and Junfeng Guo and Eric A. Hoffman and David W. Kaczka and Joseph M.\n  Reinhardt", "title": "CT Image Segmentation for Inflamed and Fibrotic Lungs Using a\n  Multi-Resolution Convolutional Neural Network", "comments": null, "journal-ref": "Sci Rep 11, 1455 (2021)", "doi": "10.1038/s41598-020-80936-4", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The purpose of this study was to develop a fully-automated segmentation\nalgorithm, robust to various density enhancing lung abnormalities, to\nfacilitate rapid quantitative analysis of computed tomography images. A\npolymorphic training approach is proposed, in which both specifically labeled\nleft and right lungs of humans with COPD, and nonspecifically labeled lungs of\nanimals with acute lung injury, were incorporated into training a single neural\nnetwork. The resulting network is intended for predicting left and right lung\nregions in humans with or without diffuse opacification and consolidation.\nPerformance of the proposed lung segmentation algorithm was extensively\nevaluated on CT scans of subjects with COPD, confirmed COVID-19, lung cancer,\nand IPF, despite no labeled training data of the latter three diseases. Lobar\nsegmentations were obtained using the left and right lung segmentation as input\nto the LobeNet algorithm. Regional lobar analysis was performed using\nhierarchical clustering to identify radiographic subtypes of COVID-19. The\nproposed lung segmentation algorithm was quantitatively evaluated using\nsemi-automated and manually-corrected segmentations in 87 COVID-19 CT images,\nachieving an average symmetric surface distance of $0.495 \\pm 0.309$ mm and\nDice coefficient of $0.985 \\pm 0.011$. Hierarchical clustering identified four\nradiographical phenotypes of COVID-19 based on lobar fractions of consolidated\nand poorly aerated tissue. Lower left and lower right lobes were consistently\nmore afflicted with poor aeration and consolidation. However, the most severe\ncases demonstrated involvement of all lobes. The polymorphic training approach\nwas able to accurately segment COVID-19 cases with diffuse consolidation\nwithout requiring COVID-19 cases for training.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 18:25:59 GMT"}, {"version": "v2", "created": "Thu, 14 Jan 2021 21:09:48 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Gerard", "Sarah E.", ""], ["Herrmann", "Jacob", ""], ["Xin", "Yi", ""], ["Martin", "Kevin T.", ""], ["Rezoagli", "Emanuele", ""], ["Ippolito", "Davide", ""], ["Bellani", "Giacomo", ""], ["Cereda", "Maurizio", ""], ["Guo", "Junfeng", ""], ["Hoffman", "Eric A.", ""], ["Kaczka", "David W.", ""], ["Reinhardt", "Joseph M.", ""]]}, {"id": "2010.08639", "submitter": "Andrea Apicella", "authors": "Andrea Apicella, Salvatore Giugliano, Francesco Isgr\\`o, Roberto\n  Prevete", "title": "A general approach to compute the relevance of middle-level input\n  features", "comments": "Presented on the Explainable Deep Learning/AI (EDL/AI) Workshop\n  during the 25th International Conference on Pattern Recognition (ICPR2020)", "journal-ref": "Pattern Recognition. ICPR International Workshops and Challenges,\n  2021, Springer International Publishing, pag. 189-203, vol III", "doi": "10.1007/978-3-030-68796-0_14", "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work proposes a novel general framework, in the context of eXplainable\nArtificial Intelligence (XAI), to construct explanations for the behaviour of\nMachine Learning (ML) models in terms of middle-level features. One can isolate\ntwo different ways to provide explanations in the context of XAI: low and\nmiddle-level explanations. Middle-level explanations have been introduced for\nalleviating some deficiencies of low-level explanations such as, in the context\nof image classification, the fact that human users are left with a significant\ninterpretive burden: starting from low-level explanations, one has to identify\nproperties of the overall input that are perceptually salient for the human\nvisual system. However, a general approach to correctly evaluate the elements\nof middle-level explanations with respect ML model responses has never been\nproposed in the literature.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 21:46:50 GMT"}, {"version": "v2", "created": "Thu, 28 Jan 2021 00:05:16 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Apicella", "Andrea", ""], ["Giugliano", "Salvatore", ""], ["Isgr\u00f2", "Francesco", ""], ["Prevete", "Roberto", ""]]}, {"id": "2010.08644", "submitter": "Seyran Khademi Mrs", "authors": "Xiangwei Shi, Seyran Khademi, Yunqiang Li, Jan van Gemert", "title": "Zoom-CAM: Generating Fine-grained Pixel Annotations from Image Labels", "comments": "ICPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current weakly supervised object localization and segmentation rely on\nclass-discriminative visualization techniques to generate pseudo-labels for\npixel-level training. Such visualization methods, including class activation\nmapping (CAM) and Grad-CAM, use only the deepest, lowest resolution\nconvolutional layer, missing all information in intermediate layers. We propose\nZoom-CAM: going beyond the last lowest resolution layer by integrating the\nimportance maps over all activations in intermediate layers. Zoom-CAM captures\nfine-grained small-scale objects for various discriminative class instances,\nwhich are commonly missed by the baseline visualization methods. We focus on\ngenerating pixel-level pseudo-labels from class labels. The quality of our\npseudo-labels evaluated on the ImageNet localization task exhibits more than\n2.8% improvement on top-1 error. For weakly supervised semantic segmentation\nour generated pseudo-labels improve a state of the art model by 1.1%.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 22:06:43 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Shi", "Xiangwei", ""], ["Khademi", "Seyran", ""], ["Li", "Yunqiang", ""], ["van Gemert", "Jan", ""]]}, {"id": "2010.08648", "submitter": "Tianyu Ma", "authors": "Tianyu Ma, Hang Zhang, Hanley Ong, Amar Vora, Thanh D. Nguyen, Ajay\n  Gupta, Yi Wang, Mert Sabuncu", "title": "Ensembling Low Precision Models for Binary Biomedical Image Segmentation", "comments": "10 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Segmentation of anatomical regions of interest such as vessels or small\nlesions in medical images is still a difficult problem that is often tackled\nwith manual input by an expert. One of the major challenges for this task is\nthat the appearance of foreground (positive) regions can be similar to\nbackground (negative) regions. As a result, many automatic segmentation\nalgorithms tend to exhibit asymmetric errors, typically producing more false\npositives than false negatives. In this paper, we aim to leverage this\nasymmetry and train a diverse ensemble of models with very high recall, while\nsacrificing their precision. Our core idea is straightforward: A diverse\nensemble of low precision and high recall models are likely to make different\nfalse positive errors (classifying background as foreground in different parts\nof the image), but the true positives will tend to be consistent. Thus, in\naggregate the false positive errors will cancel out, yielding high performance\nfor the ensemble. Our strategy is general and can be applied with any\nsegmentation model. In three different applications (carotid artery\nsegmentation in a neck CT angiography, myocardium segmentation in a\ncardiovascular MRI and multiple sclerosis lesion segmentation in a brain MRI),\nwe show how the proposed approach can significantly boost the performance of a\nbaseline segmentation method.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 22:12:20 GMT"}], "update_date": "2020-11-22", "authors_parsed": [["Ma", "Tianyu", ""], ["Zhang", "Hang", ""], ["Ong", "Hanley", ""], ["Vora", "Amar", ""], ["Nguyen", "Thanh D.", ""], ["Gupta", "Ajay", ""], ["Wang", "Yi", ""], ["Sabuncu", "Mert", ""]]}, {"id": "2010.08657", "submitter": "Federico Pernici", "authors": "Federico Pernici, Matteo Bruni, Claudio Baecchi, Francesco Turchini,\n  Alberto Del Bimbo", "title": "Class-incremental Learning with Pre-allocated Fixed Classifiers", "comments": "ICPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In class-incremental learning, a learning agent faces a stream of data with\nthe goal of learning new classes while not forgetting previous ones. Neural\nnetworks are known to suffer under this setting, as they forget previously\nacquired knowledge. To address this problem, effective methods exploit past\ndata stored in an episodic memory while expanding the final classifier nodes to\naccommodate the new classes.\n  In this work, we substitute the expanding classifier with a novel fixed\nclassifier in which a number of pre-allocated output nodes are subject to the\nclassification loss right from the beginning of the learning phase. Contrarily\nto the standard expanding classifier, this allows: (a) the output nodes of\nfuture unseen classes to firstly see negative samples since the beginning of\nlearning together with the positive samples that incrementally arrive; (b) to\nlearn features that do not change their geometric configuration as novel\nclasses are incorporated in the learning model.\n  Experiments with public datasets show that the proposed approach is as\neffective as the expanding classifier while exhibiting novel intriguing\nproperties of the internal feature representation that are otherwise\nnot-existent. Our ablation study on pre-allocating a large number of classes\nfurther validates the approach.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 22:40:28 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Pernici", "Federico", ""], ["Bruni", "Matteo", ""], ["Baecchi", "Claudio", ""], ["Turchini", "Francesco", ""], ["Del Bimbo", "Alberto", ""]]}, {"id": "2010.08661", "submitter": "Stephan Huckemann", "authors": "Robin Richter, Duy H. Thai and Stephan F. Huckemann", "title": "Generalized Intersection Algorithms with Fixpoints for Image\n  Decomposition Learning", "comments": "30 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In image processing, classical methods minimize a suitable functional that\nbalances between computational feasibility (convexity of the functional is\nideal) and suitable penalties reflecting the desired image decomposition. The\nfact that algorithms derived from such minimization problems can be used to\nconstruct (deep) learning architectures has spurred the development of\nalgorithms that can be trained for a specifically desired image decomposition,\ne.g. into cartoon and texture. While many such methods are very successful,\ntheoretical guarantees are only scarcely available. To this end, in this\ncontribution, we formalize a general class of intersection point problems\nencompassing a wide range of (learned) image decomposition models, and we give\nan existence result for a large subclass of such problems, i.e. giving the\nexistence of a fixpoint of the corresponding algorithm. This class generalizes\nclassical model-based variational problems, such as the TV-l2 -model or the\nmore general TV-Hilbert model. To illustrate the potential for learned\nalgorithms, novel (non learned) choices within our class show comparable\nresults in denoising and texture removal.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 22:55:34 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Richter", "Robin", ""], ["Thai", "Duy H.", ""], ["Huckemann", "Stephan F.", ""]]}, {"id": "2010.08666", "submitter": "Viraj Prabhu", "authors": "Viraj Prabhu, Arjun Chandrasekaran, Kate Saenko, Judy Hoffman", "title": "Active Domain Adaptation via Clustering Uncertainty-weighted Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalizing deep neural networks to new target domains is critical to their\nreal-world utility. In practice, it may be feasible to get some target data\nlabeled, but to be cost-effective it is desirable to select a\nmaximally-informative subset via active learning (AL). We study the problem of\nAL under a domain shift, called Active Domain Adaptation (Active DA). We\nempirically demonstrate how existing AL approaches based solely on model\nuncertainty or diversity sampling are suboptimal for Active DA. Our algorithm,\nActive Domain Adaptation via Clustering Uncertainty-weighted Embeddings\n(ADA-CLUE), i) identifies target instances for labeling that are both uncertain\nunder the model and diverse in feature space, and ii) leverages the available\nsource and target data for adaptation by optimizing a semi-supervised\nadversarial entropy loss that is complementary to our active sampling\nobjective. On standard image classification-based domain adaptation benchmarks,\nADA-CLUE consistently outperforms competing active adaptation, active learning,\nand domain adaptation methods across domain shifts of varying severity.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 23:37:44 GMT"}, {"version": "v2", "created": "Fri, 18 Dec 2020 23:23:48 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Prabhu", "Viraj", ""], ["Chandrasekaran", "Arjun", ""], ["Saenko", "Kate", ""], ["Hoffman", "Judy", ""]]}, {"id": "2010.08675", "submitter": "German Barquero", "authors": "Germ\\'an Barquero, Carles Fern\\'andez and Isabelle Hupont", "title": "Long-Term Face Tracking for Crowded Video-Surveillance Scenarios", "comments": "8 pages, 6 figures, 4 tables. Published in the International Joint\n  Conference on Biometrics, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most current multi-object trackers focus on short-term tracking, and are\nbased on deep and complex systems that do not operate in real-time, often\nmaking them impractical for video-surveillance. In this paper, we present a\nlong-term multi-face tracking architecture conceived for working in crowded\ncontexts, particularly unconstrained in terms of movement and occlusions, and\nwhere the face is often the only visible part of the person. Our system\nbenefits from advances in the fields of face detection and face recognition to\nachieve long-term tracking. It follows a tracking-by-detection approach,\ncombining a fast short-term visual tracker with a novel online tracklet\nreconnection strategy grounded on face verification. Additionally, a correction\nmodule is included to correct past track assignments with no extra\ncomputational cost. We present a series of experiments introducing novel,\nspecialized metrics for the evaluation of long-term tracking capabilities and a\nvideo dataset that we publicly release. Findings demonstrate that, in this\ncontext, our approach allows to obtain up to 50% longer tracks than\nstate-of-the-art deep learning trackers.\n", "versions": [{"version": "v1", "created": "Sat, 17 Oct 2020 00:11:13 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Barquero", "Germ\u00e1n", ""], ["Fern\u00e1ndez", "Carles", ""], ["Hupont", "Isabelle", ""]]}, {"id": "2010.08682", "submitter": "Rakesh Shrestha", "authors": "Rakesh Shrestha, Zhiwen Fan, Qingkun Su, Zuozhuo Dai, Siyu Zhu, Ping\n  Tan", "title": "MeshMVS: Multi-View Stereo Guided Mesh Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning based 3D shape generation methods generally utilize latent\nfeatures extracted from color images to encode the semantics of objects and\nguide the shape generation process. These color image semantics only implicitly\nencode 3D information, potentially limiting the accuracy of the generated\nshapes. In this paper we propose a multi-view mesh generation method which\nincorporates geometry information explicitly by using the features from\nintermediate depth representations of multi-view stereo and regularizing the 3D\nshapes against these depth images. First, our system predicts a coarse 3D\nvolume from the color images by probabilistically merging voxel occupancy grids\nfrom the prediction of individual views. Then the depth images from multi-view\nstereo along with the rendered depth images of the coarse shape are used as a\ncontrastive input whose features guide the refinement of the coarse shape\nthrough a series of graph convolution networks. Notably, we achieve superior\nresults than state-of-the-art multi-view shape generation methods with 34%\ndecrease in Chamfer distance to ground truth and 14% increase in F1-score on\nShapeNet dataset.Our source code is available at https://git.io/Jmalg\n", "versions": [{"version": "v1", "created": "Sat, 17 Oct 2020 00:51:21 GMT"}, {"version": "v2", "created": "Thu, 18 Mar 2021 16:15:23 GMT"}, {"version": "v3", "created": "Sun, 11 Apr 2021 16:14:31 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Shrestha", "Rakesh", ""], ["Fan", "Zhiwen", ""], ["Su", "Qingkun", ""], ["Dai", "Zuozhuo", ""], ["Zhu", "Siyu", ""], ["Tan", "Ping", ""]]}, {"id": "2010.08691", "submitter": "Elizabeth Munch", "authors": "Kayla Makela and Tim Ophelders and Michelle Quigley and Elizabeth\n  Munch and Daniel Chitwood and Asia Dowtin", "title": "Automatic Tree Ring Detection using Jacobi Sets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tree ring widths are an important source of climatic and historical data, but\nmeasuring these widths typically requires extensive manual work. Computer\nvision techniques provide promising directions towards the automation of tree\nring detection, but most automated methods still require a substantial amount\nof user interaction to obtain high accuracy. We perform analysis on 3D X-ray CT\nimages of a cross-section of a tree trunk, known as a tree disk. We present\nnovel automated methods for locating the pith (center) of a tree disk, and ring\nboundaries. Our methods use a combination of standard image processing\ntechniques and tools from topological data analysis. We evaluate the efficacy\nof our method for two different CT scans by comparing its results to manually\nlocated rings and centers and show that it is better than current automatic\nmethods in terms of correctly counting each ring and its location. Our methods\nhave several parameters, which we optimize experimentally by minimizing edit\ndistances to the manually obtained locations.\n", "versions": [{"version": "v1", "created": "Sat, 17 Oct 2020 01:28:16 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Makela", "Kayla", ""], ["Ophelders", "Tim", ""], ["Quigley", "Michelle", ""], ["Munch", "Elizabeth", ""], ["Chitwood", "Daniel", ""], ["Dowtin", "Asia", ""]]}, {"id": "2010.08705", "submitter": "Shuai Xie", "authors": "Shuai Xie, Zunlei Feng, Ying Chen, Songtao Sun, Chao Ma and Mingli\n  Song", "title": "DEAL: Difficulty-aware Active Learning for Semantic Segmentation", "comments": "Accepted by ACCV2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Active learning aims to address the paucity of labeled data by finding the\nmost informative samples. However, when applying to semantic segmentation,\nexisting methods ignore the segmentation difficulty of different semantic\nareas, which leads to poor performance on those hard semantic areas such as\ntiny or slender objects. To deal with this problem, we propose a semantic\nDifficulty-awarE Active Learning (DEAL) network composed of two branches: the\ncommon segmentation branch and the semantic difficulty branch. For the latter\nbranch, with the supervision of segmentation error between the segmentation\nresult and GT, a pixel-wise probability attention module is introduced to learn\nthe semantic difficulty scores for different semantic areas. Finally, two\nacquisition functions are devised to select the most valuable samples with\nsemantic difficulty. Competitive results on semantic segmentation benchmarks\ndemonstrate that DEAL achieves state-of-the-art active learning performance and\nimproves the performance of the hard semantic areas in particular.\n", "versions": [{"version": "v1", "created": "Sat, 17 Oct 2020 03:25:25 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Xie", "Shuai", ""], ["Feng", "Zunlei", ""], ["Chen", "Ying", ""], ["Sun", "Songtao", ""], ["Ma", "Chao", ""], ["Song", "Mingli", ""]]}, {"id": "2010.08708", "submitter": "Wei Han", "authors": "Hantao Huang, Tao Han, Wei Han, Deep Yap, Cheng-Ming Chiang", "title": "Answer-checking in Context: A Multi-modal FullyAttention Network for\n  Visual Question Answering", "comments": "Accepted in ICPR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual Question Answering (VQA) is challenging due to the complex cross-modal\nrelations. It has received extensive attention from the research community.\nFrom the human perspective, to answer a visual question, one needs to read the\nquestion and then refer to the image to generate an answer. This answer will\nthen be checked against the question and image again for the final\nconfirmation. In this paper, we mimic this process and propose a fully\nattention based VQA architecture. Moreover, an answer-checking module is\nproposed to perform a unified attention on the jointly answer, question and\nimage representation to update the answer. This mimics the human answer\nchecking process to consider the answer in the context. With answer-checking\nmodules and transferred BERT layers, our model achieves the state-of-the-art\naccuracy 71.57\\% using fewer parameters on VQA-v2.0 test-standard split.\n", "versions": [{"version": "v1", "created": "Sat, 17 Oct 2020 03:37:16 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Huang", "Hantao", ""], ["Han", "Tao", ""], ["Han", "Wei", ""], ["Yap", "Deep", ""], ["Chiang", "Cheng-Ming", ""]]}, {"id": "2010.08713", "submitter": "Linchen Qian", "authors": "Linchen Qian, Jiasong Chen, Timur Urakov, Weiyong Gu, Liang Liang", "title": "CQ-VAE: Coordinate Quantized VAE for Uncertainty Estimation with\n  Application to Disk Shape Analysis from Lumbar Spine MRI Images", "comments": "This paper is accepted by 19th IEEE International Conference on\n  Machine Learning and Applications (ICMLA2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ambiguity is inevitable in medical images, which often results in different\nimage interpretations (e.g. object boundaries or segmentation maps) from\ndifferent human experts. Thus, a model that learns the ambiguity and outputs a\nprobability distribution of the target, would be valuable for medical\napplications to assess the uncertainty of diagnosis. In this paper, we propose\na powerful generative model to learn a representation of ambiguity and to\ngenerate probabilistic outputs. Our model, named Coordinate Quantization\nVariational Autoencoder (CQ-VAE) employs a discrete latent space with an\ninternal discrete probability distribution by quantizing the coordinates of a\ncontinuous latent space. As a result, the output distribution from CQ-VAE is\ndiscrete. During training, Gumbel-Softmax sampling is used to enable\nbackpropagation through the discrete latent space. A matching algorithm is used\nto establish the correspondence between model-generated samples and\n\"ground-truth\" samples, which makes a trade-off between the ability to generate\nnew samples and the ability to represent training samples. Besides these\nprobabilistic components to generate possible outputs, our model has a\ndeterministic path to output the best estimation. We demonstrated our method on\na lumbar disk image dataset, and the results show that our CQ-VAE can learn\nlumbar disk shape variation and uncertainty.\n", "versions": [{"version": "v1", "created": "Sat, 17 Oct 2020 04:25:32 GMT"}, {"version": "v2", "created": "Tue, 20 Oct 2020 20:23:40 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Qian", "Linchen", ""], ["Chen", "Jiasong", ""], ["Urakov", "Timur", ""], ["Gu", "Weiyong", ""], ["Liang", "Liang", ""]]}, {"id": "2010.08719", "submitter": "Xiaogang Wang", "authors": "Xiaogang Wang, Marcelo H Ang Jr, Gim Hee Lee", "title": "A Self-supervised Cascaded Refinement Network for Point Cloud Completion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point clouds are often sparse and incomplete, which imposes difficulties for\nreal-world applications, such as 3D object classification, detection and\nsegmentation. Existing shape completion methods tend to generate coarse shapes\nof objects without fine-grained details. Moreover, current approaches require\nfully-complete ground truth, which are difficult to obtain in real-world\napplications. In view of these, we propose a self-supervised object completion\nmethod, which optimizes the training procedure solely on the partial input\nwithout utilizing the fully-complete ground truth. In order to generate\nhigh-quality objects with detailed geometric structures, we propose a cascaded\nrefinement network (CRN) with a coarse-to-fine strategy to synthesize the\ncomplete objects. Considering the local details of partial input together with\nthe adversarial training, we are able to learn the complicated distributions of\npoint clouds and generate the object details as realistic as possible. We\nverify our self-supervised method on both unsupervised and supervised\nexperimental settings and show superior performances. Quantitative and\nqualitative experiments on different datasets demonstrate that our method\nachieves more realistic outputs compared to existing state-of-the-art\napproaches on the 3D point cloud completion task.\n", "versions": [{"version": "v1", "created": "Sat, 17 Oct 2020 04:56:22 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Wang", "Xiaogang", ""], ["Ang", "Marcelo H", "Jr"], ["Lee", "Gim Hee", ""]]}, {"id": "2010.08720", "submitter": "Pengbo Zhao", "authors": "Pengbo Zhao, Zhenshen Qu, Yingjia Bu, Wenming Tan, Ye Ren, Shiliang Pu", "title": "PolarDet: A Fast, More Precise Detector for Rotated Target in Aerial\n  Images", "comments": "11 pages, 10 figures, 5 tables", "journal-ref": null, "doi": "10.1080/01431161.2021.1931535", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fast and precise object detection for high-resolution aerial images has been\na challenging task over the years. Due to the sharp variations on object scale,\nrotation, and aspect ratio, most existing methods are inefficient and\nimprecise. In this paper, we represent the oriented objects by polar method in\npolar coordinate and propose PolarDet, a fast and accurate one-stage object\ndetector based on that representation. Our detector introduces a sub-pixel\ncenter semantic structure to further improve classifying veracity. PolarDet\nachieves nearly all SOTA performance in aerial object detection tasks with\nfaster inference speed. In detail, our approach obtains the SOTA results on\nDOTA, UCAS-AOD, HRSC with 76.64\\% mAP, 97.01\\% mAP, and 90.46\\% mAP\nrespectively. Most noticeably, our PolarDet gets the best performance and\nreaches the fastest speed(32fps) at the UCAS-AOD dataset.\n", "versions": [{"version": "v1", "created": "Sat, 17 Oct 2020 05:16:46 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Zhao", "Pengbo", ""], ["Qu", "Zhenshen", ""], ["Bu", "Yingjia", ""], ["Tan", "Wenming", ""], ["Ren", "Ye", ""], ["Pu", "Shiliang", ""]]}, {"id": "2010.08722", "submitter": "Jun Wan", "authors": "Jun Wan, Zhihui Lai, Jun Liu, Jie Zhou, Can Gao", "title": "Robust Face Alignment by Multi-order High-precision Hourglass Network", "comments": "Accep by IEEE Transactions on Image Processing 2020.10.14", "journal-ref": null, "doi": "10.1109/TIP.2020.3032029", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Heatmap regression (HR) has become one of the mainstream approaches for face\nalignment and has obtained promising results under constrained environments.\nHowever, when a face image suffers from large pose variations, heavy occlusions\nand complicated illuminations, the performances of HR methods degrade greatly\ndue to the low resolutions of the generated landmark heatmaps and the exclusion\nof important high-order information that can be used to learn more\ndiscriminative features. To address the alignment problem for faces with\nextremely large poses and heavy occlusions, this paper proposes a heatmap\nsubpixel regression (HSR) method and a multi-order cross geometry-aware (MCG)\nmodel, which are seamlessly integrated into a novel multi-order high-precision\nhourglass network (MHHN). The HSR method is proposed to achieve high-precision\nlandmark detection by a well-designed subpixel detection loss (SDL) and\nsubpixel detection technology (SDT). At the same time, the MCG model is able to\nuse the proposed multi-order cross information to learn more discriminative\nrepresentations for enhancing facial geometric constraints and context\ninformation. To the best of our knowledge, this is the first study to explore\nheatmap subpixel regression for robust and high-precision face alignment. The\nexperimental results from challenging benchmark datasets demonstrate that our\napproach outperforms state-of-the-art methods in the literature.\n", "versions": [{"version": "v1", "created": "Sat, 17 Oct 2020 05:40:30 GMT"}], "update_date": "2020-12-30", "authors_parsed": [["Wan", "Jun", ""], ["Lai", "Zhihui", ""], ["Liu", "Jun", ""], ["Zhou", "Jie", ""], ["Gao", "Can", ""]]}, {"id": "2010.08725", "submitter": "Chenhui Chu", "authors": "Andrew Merritt, Chenhui Chu, Yuki Arase", "title": "A Corpus for English-Japanese Multimodal Neural Machine Translation with\n  Comparable Sentences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multimodal neural machine translation (NMT) has become an increasingly\nimportant area of research over the years because additional modalities, such\nas image data, can provide more context to textual data. Furthermore, the\nviability of training multimodal NMT models without a large parallel corpus\ncontinues to be investigated due to low availability of parallel sentences with\nimages, particularly for English-Japanese data. However, this void can be\nfilled with comparable sentences that contain bilingual terms and parallel\nphrases, which are naturally created through media such as social network posts\nand e-commerce product descriptions. In this paper, we propose a new multimodal\nEnglish-Japanese corpus with comparable sentences that are compiled from\nexisting image captioning datasets. In addition, we supplement our comparable\nsentences with a smaller parallel corpus for validation and test purposes. To\ntest the performance of this comparable sentence translation scenario, we train\nseveral baseline NMT models with our comparable corpus and evaluate their\nEnglish-Japanese translation performance. Due to low translation scores in our\nbaseline experiments, we believe that current multimodal NMT models are not\ndesigned to effectively utilize comparable sentence data. Despite this, we hope\nfor our corpus to be used to further research into multimodal NMT with\ncomparable sentences.\n", "versions": [{"version": "v1", "created": "Sat, 17 Oct 2020 06:12:25 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Merritt", "Andrew", ""], ["Chu", "Chenhui", ""], ["Arase", "Yuki", ""]]}, {"id": "2010.08727", "submitter": "Jiatong Li", "authors": "Jiatong Li, Fangda Han, Ricardo Guerrero, Vladimir Pavlovic", "title": "Picture-to-Amount (PITA): Predicting Relative Ingredient Amounts from\n  Food Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Increased awareness of the impact of food consumption on health and lifestyle\ntoday has given rise to novel data-driven food analysis systems. Although these\nsystems may recognize the ingredients, a detailed analysis of their amounts in\nthe meal, which is paramount for estimating the correct nutrition, is usually\nignored. In this paper, we study the novel and challenging problem of\npredicting the relative amount of each ingredient from a food image. We propose\nPITA, the Picture-to-Amount deep learning architecture to solve the problem.\nMore specifically, we predict the ingredient amounts using a domain-driven\nWasserstein loss from image-to-recipe cross-modal embeddings learned to align\nthe two views of food data. Experiments on a dataset of recipes collected from\nthe Internet show the model generates promising results and improves the\nbaselines on this challenging task. A demo of our system and our data is\navailableat: foodai.cs.rutgers.edu.\n", "versions": [{"version": "v1", "created": "Sat, 17 Oct 2020 06:43:18 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Li", "Jiatong", ""], ["Han", "Fangda", ""], ["Guerrero", "Ricardo", ""], ["Pavlovic", "Vladimir", ""]]}, {"id": "2010.08750", "submitter": "Mert Kilickaya", "authors": "Mert Kilickaya, Noureldien Hussein, Efstratios Gavves, Arnold\n  Smeulders", "title": "Self-Selective Context for Interaction Recognition", "comments": "Accepted at ICPR'20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human-object interaction recognition aims for identifying the relationship\nbetween a human subject and an object. Researchers incorporate global scene\ncontext into the early layers of deep Convolutional Neural Networks as a\nsolution. They report a significant increase in the performance since generally\ninteractions are correlated with the scene (\\ie riding bicycle on the city\nstreet). However, this approach leads to the following problems. It increases\nthe network size in the early layers, therefore not efficient. It leads to\nnoisy filter responses when the scene is irrelevant, therefore not accurate. It\nonly leverages scene context whereas human-object interactions offer a\nmultitude of contexts, therefore incomplete. To circumvent these issues, in\nthis work, we propose Self-Selective Context (SSC). SSC operates on the joint\nappearance of human-objects and context to bring the most discriminative\ncontext(s) into play for recognition. We devise novel contextual features that\nmodel the locality of human-object interactions and show that SSC can\nseamlessly integrate with the State-of-the-art interaction recognition models.\nOur experiments show that SSC leads to an important increase in interaction\nrecognition performance, while using much fewer parameters.\n", "versions": [{"version": "v1", "created": "Sat, 17 Oct 2020 09:06:12 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Kilickaya", "Mert", ""], ["Hussein", "Noureldien", ""], ["Gavves", "Efstratios", ""], ["Smeulders", "Arnold", ""]]}, {"id": "2010.08751", "submitter": "Boyuan Ma", "authors": "Boyuan Ma, Xiang Yin, Di Wu, Xiaojuan Ban", "title": "End-to-End Learning for Simultaneously Generating Decision Map and\n  Multi-Focus Image Fusion Result", "comments": "report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The general aim of multi-focus image fusion is to gather focused regions of\ndifferent images to generate a unique all-in-focus fused image. Deep learning\nbased methods become the mainstream of image fusion by virtue of its powerful\nfeature representation ability. However, most of the existing deep learning\nstructures failed to balance fusion quality and end-to-end implementation\nconvenience. End-to-end decoder design often leads to unrealistic result\nbecause of its non-linear mapping mechanism. On the other hand, generating an\nintermediate decision map achieves better quality for the fused image, but\nrelies on the rectification with empirical post-processing parameter choices.\nIn this work, to handle the requirements of both output image quality and\ncomprehensive simplicity of structure implementation, we propose a cascade\nnetwork to simultaneously generate decision map and fused result with an\nend-to-end training procedure. It avoids the dependence on empirical\npost-processing methods in the inference stage. To improve the fusion quality,\nwe introduce a gradient aware loss function to preserve gradient information in\noutput fused image. In addition, we design a decision calibration strategy to\ndecrease the time consumption in the application of multiple images fusion.\nExtensive experiments are conducted to compare with 19 different\nstate-of-the-art multi-focus image fusion structures with 6 assessment metrics.\nThe results prove that our designed structure can generally ameliorate the\noutput fused image quality, while implementation efficiency increases over 30\\%\nfor multiple images fusion.\n", "versions": [{"version": "v1", "created": "Sat, 17 Oct 2020 09:09:51 GMT"}, {"version": "v2", "created": "Thu, 21 Jan 2021 11:11:56 GMT"}, {"version": "v3", "created": "Tue, 23 Mar 2021 07:34:42 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Ma", "Boyuan", ""], ["Yin", "Xiang", ""], ["Wu", "Di", ""], ["Ban", "Xiaojuan", ""]]}, {"id": "2010.08755", "submitter": "Chenjia Bai", "authors": "Chenjia Bai, Peng Liu, Zhaoran Wang, Kaiyu Liu, Lingxiao Wang, Yingnan\n  Zhao", "title": "Variational Dynamic for Self-Supervised Exploration in Deep\n  Reinforcement Learning", "comments": "associated videos at https://sites.google.com/view/exploration-vdm", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient exploration remains a challenging problem in reinforcement\nlearning, especially for tasks where extrinsic rewards from environments are\nsparse or even totally disregarded. Significant advances based on intrinsic\nmotivation show promising results in simple environments but often get stuck in\nenvironments with multimodal and stochastic dynamics. In this work, we propose\na variational dynamic model based on the conditional variational inference to\nmodel the multimodality and stochasticity. We consider the environmental\nstate-action transition as a conditional generative process by generating the\nnext-state prediction under the condition of the current state, action, and\nlatent variable. We derive an upper bound of the negative log-likelihood of the\nenvironmental transition and use such an upper bound as the intrinsic reward\nfor exploration, which allows the agent to learn skills by self-supervised\nexploration without observing extrinsic rewards. We evaluate the proposed\nmethod on several image-based simulation tasks and a real robotic manipulating\ntask. Our method outperforms several state-of-the-art environment model-based\nexploration approaches.\n", "versions": [{"version": "v1", "created": "Sat, 17 Oct 2020 09:54:51 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Bai", "Chenjia", ""], ["Liu", "Peng", ""], ["Wang", "Zhaoran", ""], ["Liu", "Kaiyu", ""], ["Wang", "Lingxiao", ""], ["Zhao", "Yingnan", ""]]}, {"id": "2010.08764", "submitter": "Mohamed Ali Souibgui", "authors": "Mohamed Ali Souibgui and Yousri Kessentini", "title": "DE-GAN: A Conditional Generative Adversarial Network for Document\n  Enhancement", "comments": "Accepted in IEEE TPAMI", "journal-ref": null, "doi": "10.1109/TPAMI.2020.3022406", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Documents often exhibit various forms of degradation, which make it hard to\nbe read and substantially deteriorate the performance of an OCR system. In this\npaper, we propose an effective end-to-end framework named Document Enhancement\nGenerative Adversarial Networks (DE-GAN) that uses the conditional GANs (cGANs)\nto restore severely degraded document images. To the best of our knowledge,\nthis practice has not been studied within the context of generative adversarial\ndeep networks. We demonstrate that, in different tasks (document clean up,\nbinarization, deblurring and watermark removal), DE-GAN can produce an enhanced\nversion of the degraded document with a high quality. In addition, our approach\nprovides consistent improvements compared to state-of-the-art methods over the\nwidely used DIBCO 2013, DIBCO 2017 and H-DIBCO 2018 datasets, proving its\nability to restore a degraded document image to its ideal condition. The\nobtained results on a wide variety of degradation reveal the flexibility of the\nproposed model to be exploited in other document enhancement problems.\n", "versions": [{"version": "v1", "created": "Sat, 17 Oct 2020 10:54:49 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Souibgui", "Mohamed Ali", ""], ["Kessentini", "Yousri", ""]]}, {"id": "2010.08776", "submitter": "Urs Muller", "authors": "Mariusz Bojarski, Chenyi Chen, Joyjit Daw, Alperen De\\u{g}irmenci,\n  Joya Deri, Bernhard Firner, Beat Flepp, Sachin Gogri, Jesse Hong, Lawrence\n  Jackel, Zhenhua Jia, BJ Lee, Bo Liu, Fei Liu, Urs Muller, Samuel Payne,\n  Nischal Kota Nagendra Prasad, Artem Provodin, John Roach, Timur Rvachov, Neha\n  Tadimeti, Jesper van Engelen, Haiguang Wen, Eric Yang, and Zongyi Yang", "title": "The NVIDIA PilotNet Experiments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Four years ago, an experimental system known as PilotNet became the first\nNVIDIA system to steer an autonomous car along a roadway. This system\nrepresents a departure from the classical approach for self-driving in which\nthe process is manually decomposed into a series of modules, each performing a\ndifferent task. In PilotNet, on the other hand, a single deep neural network\n(DNN) takes pixels as input and produces a desired vehicle trajectory as\noutput; there are no distinct internal modules connected by human-designed\ninterfaces. We believe that handcrafted interfaces ultimately limit performance\nby restricting information flow through the system and that a learned approach,\nin combination with other artificial intelligence systems that add redundancy,\nwill lead to better overall performing systems. We continue to conduct research\ntoward that goal.\n  This document describes the PilotNet lane-keeping effort, carried out over\nthe past five years by our NVIDIA PilotNet group in Holmdel, New Jersey. Here\nwe present a snapshot of system status in mid-2020 and highlight some of the\nwork done by the PilotNet group.\n", "versions": [{"version": "v1", "created": "Sat, 17 Oct 2020 12:25:18 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Bojarski", "Mariusz", ""], ["Chen", "Chenyi", ""], ["Daw", "Joyjit", ""], ["De\u011firmenci", "Alperen", ""], ["Deri", "Joya", ""], ["Firner", "Bernhard", ""], ["Flepp", "Beat", ""], ["Gogri", "Sachin", ""], ["Hong", "Jesse", ""], ["Jackel", "Lawrence", ""], ["Jia", "Zhenhua", ""], ["Lee", "BJ", ""], ["Liu", "Bo", ""], ["Liu", "Fei", ""], ["Muller", "Urs", ""], ["Payne", "Samuel", ""], ["Prasad", "Nischal Kota Nagendra", ""], ["Provodin", "Artem", ""], ["Roach", "John", ""], ["Rvachov", "Timur", ""], ["Tadimeti", "Neha", ""], ["van Engelen", "Jesper", ""], ["Wen", "Haiguang", ""], ["Yang", "Eric", ""], ["Yang", "Zongyi", ""]]}, {"id": "2010.08788", "submitter": "Pradyumna Reddy", "authors": "Pradyumna Reddy, Paul Guerrero, Matt Fisher, Wilmot Li, Miloy J.Mitra", "title": "Discovering Pattern Structure Using Differentiable Compositing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Patterns, which are collections of elements arranged in regular or\nnear-regular arrangements, are an important graphic art form and widely used\ndue to their elegant simplicity and aesthetic appeal. When a pattern is encoded\nas a flat image without the underlying structure, manually editing the pattern\nis tedious and challenging as one has to both preserve the individual element\nshapes and their original relative arrangements. State-of-the-art deep learning\nframeworks that operate at the pixel level are unsuitable for manipulating such\npatterns. Specifically, these methods can easily disturb the shapes of the\nindividual elements or their arrangement, and thus fail to preserve the latent\nstructures of the input patterns. We present a novel differentiable compositing\noperator using pattern elements and use it to discover structures, in the form\nof a layered representation of graphical objects, directly from raw pattern\nimages. This operator allows us to adapt current deep learning based image\nmethods to effectively handle patterns. We evaluate our method on a range of\npatterns and demonstrate superiority in the context of pattern manipulations\nwhen compared against state-of-the-art\n", "versions": [{"version": "v1", "created": "Sat, 17 Oct 2020 13:39:12 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Reddy", "Pradyumna", ""], ["Guerrero", "Paul", ""], ["Fisher", "Matt", ""], ["Li", "Wilmot", ""], ["Mitra", "Miloy J.", ""]]}, {"id": "2010.08800", "submitter": "S Divakar Bhat", "authors": "Sayan Banerjee, S Divakar Bhat, Subhasis Chaudhuri, Rajbabu Velmurugan", "title": "Directed Variational Cross-encoder Network for Few-shot Multi-image\n  Co-segmentation", "comments": "Accepted at 2020 25th International Conference on Pattern Recognition\n  (ICPR)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel framework for multi-image co-segmentation\nusing class agnostic meta-learning strategy by generalizing to new classes\ngiven only a small number of training samples for each new class. We have\ndeveloped a novel encoder-decoder network termed as DVICE (Directed Variational\nInference Cross Encoder), which learns a continuous embedding space to ensure\nbetter similarity learning. We employ a combination of the proposed DVICE\nnetwork and a novel few-shot learning approach to tackle the small sample size\nproblem encountered in co-segmentation with small datasets like iCoseg and\nMSRC. Furthermore, the proposed framework does not use any semantic class\nlabels and is entirely class agnostic. Through exhaustive experimentation over\nmultiple datasets using only a small volume of training data, we have\ndemonstrated that our approach outperforms all existing state-of-the-art\ntechniques.\n", "versions": [{"version": "v1", "created": "Sat, 17 Oct 2020 14:38:57 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Banerjee", "Sayan", ""], ["Bhat", "S Divakar", ""], ["Chaudhuri", "Subhasis", ""], ["Velmurugan", "Rajbabu", ""]]}, {"id": "2010.08833", "submitter": "Neelanjan Bhowmik", "authors": "William Thomson, Neelanjan Bhowmik, Toby P. Breckon", "title": "Efficient and Compact Convolutional Neural Network Architectures for\n  Non-temporal Real-time Fire Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic visual fire detection is used to complement traditional fire\ndetection sensor systems (smoke/heat). In this work, we investigate different\nConvolutional Neural Network (CNN) architectures and their variants for the\nnon-temporal real-time bounds detection of fire pixel regions in video (or\nstill) imagery. Two reduced complexity compact CNN architectures\n(NasNet-A-OnFire and ShuffleNetV2-OnFire) are proposed through experimental\nanalysis to optimise the computational efficiency for this task. The results\nimprove upon the current state-of-the-art solution for fire detection,\nachieving an accuracy of 95% for full-frame binary classification and 97% for\nsuperpixel localisation. We notably achieve a classification speed up by a\nfactor of 2.3x for binary classification and 1.3x for superpixel localisation,\nwith runtime of 40 fps and 18 fps respectively, outperforming prior work in the\nfield presenting an efficient, robust and real-time solution for fire region\ndetection. Subsequent implementation on low-powered devices (Nvidia Xavier-NX,\nachieving 49 fps for full-frame classification via ShuffleNetV2-OnFire)\ndemonstrates our architectures are suitable for various real-world deployment\napplications.\n", "versions": [{"version": "v1", "created": "Sat, 17 Oct 2020 17:48:04 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Thomson", "William", ""], ["Bhowmik", "Neelanjan", ""], ["Breckon", "Toby P.", ""]]}, {"id": "2010.08841", "submitter": "Soufiane Lamghari", "authors": "Soufiane Lamghari, Guillaume-Alexandre Bilodeau, Nicolas Saunier", "title": "A Grid-based Representation for Human Action Recognition", "comments": "Accepted on 25th International Conference on Pattern Recognition\n  (ICPR 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human action recognition (HAR) in videos is a fundamental research topic in\ncomputer vision. It consists mainly in understanding actions performed by\nhumans based on a sequence of visual observations. In recent years, HAR have\nwitnessed significant progress, especially with the emergence of deep learning\nmodels. However, most of existing approaches for action recognition rely on\ninformation that is not always relevant for this task, and are limited in the\nway they fuse the temporal information. In this paper, we propose a novel\nmethod for human action recognition that encodes efficiently the most\ndiscriminative appearance information of an action with explicit attention on\nrepresentative pose features, into a new compact grid representation. Our GRAR\n(Grid-based Representation for Action Recognition) method is tested on several\nbenchmark datasets demonstrating that our model can accurately recognize human\nactions, despite intra-class appearance variations and occlusion challenges.\n", "versions": [{"version": "v1", "created": "Sat, 17 Oct 2020 18:25:00 GMT"}, {"version": "v2", "created": "Thu, 29 Oct 2020 14:39:08 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Lamghari", "Soufiane", ""], ["Bilodeau", "Guillaume-Alexandre", ""], ["Saunier", "Nicolas", ""]]}, {"id": "2010.08844", "submitter": "Jinghan Yang", "authors": "Jinghan Yang, Adith Boloor, Ayan Chakrabarti, Xuan Zhang, Yevgeniy\n  Vorobeychik", "title": "Finding Physical Adversarial Examples for Autonomous Driving with Fast\n  and Differentiable Image Compositing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is considerable evidence that deep neural networks are vulnerable to\nadversarial perturbations applied directly to their digital inputs. However, it\nremains an open question whether this translates to vulnerabilities in real\nsystems. For example, an attack on self-driving cars would in practice entail\nmodifying the driving environment, which then impacts the video inputs to the\ncar's controller, thereby indirectly leading to incorrect driving decisions.\nSuch attacks require accounting for system dynamics and tracking viewpoint\nchanges. We propose a scalable approach for finding adversarial modifications\nof a simulated autonomous driving environment using a differentiable\napproximation for the mapping from environmental modifications (rectangles on\nthe road) to the corresponding video inputs to the controller neural network.\nGiven the parameters of the rectangles, our proposed differentiable mapping\ncomposites them onto pre-recorded video streams of the original environment,\naccounting for geometric and color variations. Moreover, we propose a multiple\ntrajectory sampling approach that enables our attacks to be robust to a car's\nself-correcting behavior. When combined with a neural network-based controller,\nour approach allows the design of adversarial modifications through end-to-end\ngradient-based optimization. Using the Carla autonomous driving simulator, we\nshow that our approach is significantly more scalable and far more effective at\nidentifying autonomous vehicle vulnerabilities in simulation experiments than a\nstate-of-the-art approach based on Bayesian Optimization.\n", "versions": [{"version": "v1", "created": "Sat, 17 Oct 2020 18:35:32 GMT"}, {"version": "v2", "created": "Fri, 11 Jun 2021 00:42:30 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Yang", "Jinghan", ""], ["Boloor", "Adith", ""], ["Chakrabarti", "Ayan", ""], ["Zhang", "Xuan", ""], ["Vorobeychik", "Yevgeniy", ""]]}, {"id": "2010.08852", "submitter": "Panagiotis Eustratiadis", "authors": "Panagiotis Eustratiadis, Henry Gouk, Da Li, Timothy Hospedales", "title": "Weight-Covariance Alignment for Adversarially Robust Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic Neural Networks (SNNs) that inject noise into their hidden layers\nhave recently been shown to achieve strong robustness against adversarial\nattacks. However, existing SNNs are usually heuristically motivated, and often\nrely on adversarial training, which is computationally costly. We propose a new\nSNN that achieves state-of-the-art performance without relying on adversarial\ntraining, and enjoys solid theoretical justification. Specifically, while\nexisting SNNs inject learned or hand-tuned isotropic noise, our SNN learns an\nanisotropic noise distribution to optimize a learning-theoretic bound on\nadversarial robustness. We evaluate our method on a number of popular\nbenchmarks, show that it can be applied to different architectures, and that it\nprovides robustness to a variety of white-box and black-box attacks, while\nbeing simple and fast to train compared to existing alternatives.\n", "versions": [{"version": "v1", "created": "Sat, 17 Oct 2020 19:28:35 GMT"}, {"version": "v2", "created": "Mon, 24 May 2021 17:31:04 GMT"}, {"version": "v3", "created": "Wed, 26 May 2021 10:16:14 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Eustratiadis", "Panagiotis", ""], ["Gouk", "Henry", ""], ["Li", "Da", ""], ["Hospedales", "Timothy", ""]]}, {"id": "2010.08872", "submitter": "Manu Goyal", "authors": "Manu Goyal, Judith Austin-Strohbehn, Sean J. Sun, Karen Rodriguez,\n  Jessica M. Sin, Yvonne Y. Cheung and Saeed Hassanpour", "title": "Sensitivity and Specificity Evaluation of Deep Learning Models for\n  Detection of Pneumoperitoneum on Chest Radiographs", "comments": "21 Pages, 4 Tables and 6 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: Deep learning has great potential to assist with detecting and\ntriaging critical findings such as pneumoperitoneum on medical images. To be\nclinically useful, the performance of this technology still needs to be\nvalidated for generalizability across different types of imaging systems.\nMaterials and Methods: This retrospective study included 1,287 chest X-ray\nimages of patients who underwent initial chest radiography at 13 different\nhospitals between 2011 and 2019. The chest X-ray images were labelled\nindependently by four radiologist experts as positive or negative for\npneumoperitoneum. State-of-the-art deep learning models (ResNet101,\nInceptionV3, DenseNet161, and ResNeXt101) were trained on a subset of this\ndataset, and the automated classification performance was evaluated on the rest\nof the dataset by measuring the AUC, sensitivity, and specificity for each\nmodel. Furthermore, the generalizability of these deep learning models was\nassessed by stratifying the test dataset according to the type of the utilized\nimaging systems. Results: All deep learning models performed well for\nidentifying radiographs with pneumoperitoneum, while DenseNet161 achieved the\nhighest AUC of 95.7%, Specificity of 89.9%, and Sensitivity of 91.6%.\nDenseNet161 model was able to accurately classify radiographs from different\nimaging systems (Accuracy: 90.8%), while it was trained on images captured from\na specific imaging system from a single institution. This result suggests the\ngeneralizability of our model for learning salient features in chest X-ray\nimages to detect pneumoperitoneum, independent of the imaging system.\n", "versions": [{"version": "v1", "created": "Sat, 17 Oct 2020 21:41:53 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Goyal", "Manu", ""], ["Austin-Strohbehn", "Judith", ""], ["Sun", "Sean J.", ""], ["Rodriguez", "Karen", ""], ["Sin", "Jessica M.", ""], ["Cheung", "Yvonne Y.", ""], ["Hassanpour", "Saeed", ""]]}, {"id": "2010.08888", "submitter": "Tiancheng Sun", "authors": "Tiancheng Sun, Zexiang Xu, Xiuming Zhang, Sean Fanello, Christoph\n  Rhemann, Paul Debevec, Yun-Ta Tsai, Jonathan T. Barron, Ravi Ramamoorthi", "title": "Light Stage Super-Resolution: Continuous High-Frequency Relighting", "comments": "Siggraph Asia 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The light stage has been widely used in computer graphics for the past two\ndecades, primarily to enable the relighting of human faces. By capturing the\nappearance of the human subject under different light sources, one obtains the\nlight transport matrix of that subject, which enables image-based relighting in\nnovel environments. However, due to the finite number of lights in the stage,\nthe light transport matrix only represents a sparse sampling on the entire\nsphere. As a consequence, relighting the subject with a point light or a\ndirectional source that does not coincide exactly with one of the lights in the\nstage requires interpolation and resampling the images corresponding to nearby\nlights, and this leads to ghosting shadows, aliased specularities, and other\nartifacts. To ameliorate these artifacts and produce better results under\narbitrary high-frequency lighting, this paper proposes a learning-based\nsolution for the \"super-resolution\" of scans of human faces taken from a light\nstage. Given an arbitrary \"query\" light direction, our method aggregates the\ncaptured images corresponding to neighboring lights in the stage, and uses a\nneural network to synthesize a rendering of the face that appears to be\nilluminated by a \"virtual\" light source at the query location. This neural\nnetwork must circumvent the inherent aliasing and regularity of the light stage\ndata that was used for training, which we accomplish through the use of\nregularized traditional interpolation methods within our network. Our learned\nmodel is able to produce renderings for arbitrary light directions that exhibit\nrealistic shadows and specular highlights, and is able to generalize across a\nwide variety of subjects.\n", "versions": [{"version": "v1", "created": "Sat, 17 Oct 2020 23:40:43 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Sun", "Tiancheng", ""], ["Xu", "Zexiang", ""], ["Zhang", "Xiuming", ""], ["Fanello", "Sean", ""], ["Rhemann", "Christoph", ""], ["Debevec", "Paul", ""], ["Tsai", "Yun-Ta", ""], ["Barron", "Jonathan T.", ""], ["Ramamoorthi", "Ravi", ""]]}, {"id": "2010.08919", "submitter": "Xiaoyu Xiang", "authors": "Xiaoyu Xiang, Qian Lin, Jan P. Allebach", "title": "Boosting High-Level Vision with Joint Compression Artifacts Reduction\n  and Super-Resolution", "comments": "8 pages, 6 figures, 5 tables. Accepted by the 25th ICPR (2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the limits of bandwidth and storage space, digital images are usually\ndown-scaled and compressed when transmitted over networks, resulting in loss of\ndetails and jarring artifacts that can lower the performance of high-level\nvisual tasks. In this paper, we aim to generate an artifact-free\nhigh-resolution image from a low-resolution one compressed with an arbitrary\nquality factor by exploring joint compression artifacts reduction (CAR) and\nsuper-resolution (SR) tasks. First, we propose a context-aware joint CAR and SR\nneural network (CAJNN) that integrates both local and non-local features to\nsolve CAR and SR in one-stage. Finally, a deep reconstruction network is\nadopted to predict high quality and high-resolution images. Evaluation on CAR\nand SR benchmark datasets shows that our CAJNN model outperforms previous\nmethods and also takes 26.2% shorter runtime. Based on this model, we explore\naddressing two critical challenges in high-level computer vision: optical\ncharacter recognition of low-resolution texts, and extremely tiny face\ndetection. We demonstrate that CAJNN can serve as an effective image\npreprocessing method and improve the accuracy for real-scene text recognition\n(from 85.30% to 85.75%) and the average precision for tiny face detection (from\n0.317 to 0.611).\n", "versions": [{"version": "v1", "created": "Sun, 18 Oct 2020 04:17:08 GMT"}, {"version": "v2", "created": "Fri, 18 Dec 2020 03:26:40 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Xiang", "Xiaoyu", ""], ["Lin", "Qian", ""], ["Allebach", "Jan P.", ""]]}, {"id": "2010.08942", "submitter": "Hong-Xiang Chen", "authors": "Hong-Xiang Chen and Kunhong Li and Zhiheng Fu and Mengyi Liu and\n  Zonghao Chen and Yulan Guo", "title": "Distortion-aware Monocular Depth Estimation for Omnidirectional Images", "comments": "Preprint", "journal-ref": null, "doi": "10.1109/LSP.2021.3050712", "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A main challenge for tasks on panorama lies in the distortion of objects\namong images. In this work, we propose a Distortion-Aware Monocular\nOmnidirectional (DAMO) dense depth estimation network to address this challenge\non indoor panoramas with two steps. First, we introduce a distortion-aware\nmodule to extract calibrated semantic features from omnidirectional images.\nSpecifically, we exploit deformable convolution to adjust its sampling grids to\ngeometric variations of distorted objects on panoramas and then utilize a strip\npooling module to sample against horizontal distortion introduced by inverse\ngnomonic projection. Second, we further introduce a plug-and-play\nspherical-aware weight matrix for our objective function to handle the uneven\ndistribution of areas projected from a sphere. Experiments on the 360D dataset\nshow that the proposed method can effectively extract semantic features from\ndistorted panoramas and alleviate the supervision bias caused by distortion. It\nachieves state-of-the-art performance on the 360D dataset with high efficiency.\n", "versions": [{"version": "v1", "created": "Sun, 18 Oct 2020 08:47:57 GMT"}, {"version": "v2", "created": "Mon, 30 Nov 2020 01:41:15 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Chen", "Hong-Xiang", ""], ["Li", "Kunhong", ""], ["Fu", "Zhiheng", ""], ["Liu", "Mengyi", ""], ["Chen", "Zonghao", ""], ["Guo", "Yulan", ""]]}, {"id": "2010.08946", "submitter": "Federico Becattini", "authors": "Simone Undri Innocenti, Federico Becattini, Federico Pernici, Alberto\n  Del Bimbo", "title": "Temporal Binary Representation for Event-Based Action Recognition", "comments": "Accepted at ICPR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present an event aggregation strategy to convert the output\nof an event camera into frames processable by traditional Computer Vision\nalgorithms. The proposed method first generates sequences of intermediate\nbinary representations, which are then losslessly transformed into a compact\nformat by simply applying a binary-to-decimal conversion. This strategy allows\nus to encode temporal information directly into pixel values, which are then\ninterpreted by deep learning models. We apply our strategy, called Temporal\nBinary Representation, to the task of Gesture Recognition, obtaining state of\nthe art results on the popular DVS128 Gesture Dataset. To underline the\neffectiveness of the proposed method compared to existing ones, we also collect\nan extension of the dataset under more challenging conditions on which to\nperform experiments.\n", "versions": [{"version": "v1", "created": "Sun, 18 Oct 2020 09:20:45 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Innocenti", "Simone Undri", ""], ["Becattini", "Federico", ""], ["Pernici", "Federico", ""], ["Del Bimbo", "Alberto", ""]]}, {"id": "2010.08948", "submitter": "Federico Becattini", "authors": "Lorenzo Berlincioni, Federico Becattini, Lorenzo Seidenari, Alberto\n  Del Bimbo", "title": "Multiple Future Prediction Leveraging Synthetic Trajectories", "comments": "Accepted at ICPR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Trajectory prediction is an important task, especially in autonomous driving.\nThe ability to forecast the position of other moving agents can yield to an\neffective planning, ensuring safety for the autonomous vehicle as well for the\nobserved entities. In this work we propose a data driven approach based on\nMarkov Chains to generate synthetic trajectories, which are useful for training\na multiple future trajectory predictor. The advantages are twofold: on the one\nhand synthetic samples can be used to augment existing datasets and train more\neffective predictors; on the other hand, it allows to generate samples with\nmultiple ground truths, corresponding to diverse equally likely outcomes of the\nobserved trajectory. We define a trajectory prediction model and a loss that\nexplicitly address the multimodality of the problem and we show that combining\nsynthetic and real data leads to prediction improvements, obtaining state of\nthe art results.\n", "versions": [{"version": "v1", "created": "Sun, 18 Oct 2020 09:33:23 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Berlincioni", "Lorenzo", ""], ["Becattini", "Federico", ""], ["Seidenari", "Lorenzo", ""], ["Del Bimbo", "Alberto", ""]]}, {"id": "2010.08952", "submitter": "Sofie Tilborghs", "authors": "Sofie Tilborghs, Tom Dresselaers, Piet Claus, Jan Bogaert, Frederik\n  Maes", "title": "Shape Constrained CNN for Cardiac MR Segmentation with Simultaneous\n  Prediction of Shape and Pose Parameters", "comments": "Presented at the 11th Workshop on Statistical Atlases and\n  Computational Modeling of the Heart (STACOM 2020), held in conjunction with\n  MICCAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic segmentation using convolutional neural networks (CNNs) is the\nstate-of-the-art for many medical segmentation tasks including left ventricle\n(LV) segmentation in cardiac MR images. However, a drawback is that these CNNs\nlack explicit shape constraints, occasionally resulting in unrealistic\nsegmentations. In this paper, we perform LV and myocardial segmentation by\nregression of pose and shape parameters derived from a statistical shape model.\nThe integrated shape model regularizes predicted segmentations and guarantees\nrealistic shapes. Furthermore, in contrast to semantic segmentation, it allows\ndirect calculation of regional measures such as myocardial thickness. We\nenforce robustness of shape and pose prediction by simultaneously constructing\na segmentation distance map during training. We evaluated the proposed method\nin a fivefold cross validation on a in-house clinical dataset with 75 subjects\ncontaining a total of 1539 delineated short-axis slices covering LV from apex\nto base, and achieved a correlation of 99% for LV area, 94% for myocardial\narea, 98% for LV dimensions and 88% for regional wall thicknesses. The method\nwas additionally validated on the LVQuan18 and LVQuan19 public datasets and\nachieved state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Sun, 18 Oct 2020 09:51:04 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Tilborghs", "Sofie", ""], ["Dresselaers", "Tom", ""], ["Claus", "Piet", ""], ["Bogaert", "Jan", ""], ["Maes", "Frederik", ""]]}, {"id": "2010.08973", "submitter": "Ke Chen", "authors": "Maksymilian Wojtas and Ke Chen", "title": "Feature Importance Ranking for Deep Learning", "comments": "Accepted by NeurIPS 2020, 5 Figures and 1 Table in Main text, 10\n  Figures and 5 Tables in Supplementary Materials", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature importance ranking has become a powerful tool for explainable AI.\nHowever, its nature of combinatorial optimization poses a great challenge for\ndeep learning. In this paper, we propose a novel dual-net architecture\nconsisting of operator and selector for discovery of an optimal feature subset\nof a fixed size and ranking the importance of those features in the optimal\nsubset simultaneously. During learning, the operator is trained for a\nsupervised learning task via optimal feature subset candidates generated by the\nselector that learns predicting the learning performance of the operator\nworking on different optimal subset candidates. We develop an alternate\nlearning algorithm that trains two nets jointly and incorporates a stochastic\nlocal search procedure into learning to address the combinatorial optimization\nchallenge. In deployment, the selector generates an optimal feature subset and\nranks feature importance, while the operator makes predictions based on the\noptimal subset for test data. A thorough evaluation on synthetic, benchmark and\nreal data sets suggests that our approach outperforms several state-of-the-art\nfeature importance ranking and supervised feature selection methods. (Our\nsource code is available: https://github.com/maksym33/FeatureImportanceDL)\n", "versions": [{"version": "v1", "created": "Sun, 18 Oct 2020 12:20:27 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Wojtas", "Maksymilian", ""], ["Chen", "Ke", ""]]}, {"id": "2010.09009", "submitter": "Morris Klasen", "authors": "Morris Klasen, Dirk Ahrens, Jonas Eberle, and Volker Steinhage", "title": "Image-based Automated Species Identification: Can Virtual Data\n  Augmentation Overcome Problems of Insufficient Sampling?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated species identification and delimitation is challenging,\nparticularly in rare and thus often scarcely sampled species, which do not\nallow sufficient discrimination of infraspecific versus interspecific\nvariation. Typical problems arising from either low or exaggerated\ninterspecific morphological differentiation are best met by automated methods\nof machine learning that learn efficient and effective species identification\nfrom training samples. However, limited infraspecific sampling remains a key\nchallenge also in machine learning. 1In this study, we assessed whether a\ntwo-level data augmentation approach may help to overcome the problem of scarce\ntraining data in automated visual species identification. The first level of\nvisual data augmentation applies classic approaches of data augmentation and\ngeneration of faked images using a GAN approach. Descriptive feature vectors\nare derived from bottleneck features of a VGG-16 convolutional neural network\n(CNN) that are then stepwise reduced in dimensionality using Global Average\nPooling and PCA to prevent overfitting. The second level of data augmentation\nemploys synthetic additional sampling in feature space by an oversampling\nalgorithm in vector space (SMOTE). Applied on two challenging datasets of\nscarab beetles (Coleoptera), our augmentation approach outperformed a\nnon-augmented deep learning baseline approach as well as a traditional 2D\nmorphometric approach (Procrustes analysis).\n", "versions": [{"version": "v1", "created": "Sun, 18 Oct 2020 15:44:45 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Klasen", "Morris", ""], ["Ahrens", "Dirk", ""], ["Eberle", "Jonas", ""], ["Steinhage", "Volker", ""]]}, {"id": "2010.09015", "submitter": "Chaobing Shan", "authors": "Chaobing Shan, Chunbo Wei, Bing Deng, Jianqiang Huang, Xian-Sheng Hua,\n  Xiaoliang Cheng, Kewei Liang", "title": "Tracklets Predicting Based Adaptive Graph Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of the existing tracking methods link the detected boxes to the\ntracklets using a linear combination of feature cosine distances and box\noverlap. But the problem of inconsistent features of an object in two different\nframes still exists. In addition, when extracting features, only appearance\ninformation is utilized, neither the location relationship nor the information\nof the tracklets is considered. We present an accurate and end-to-end learning\nframework for multi-object tracking, namely \\textbf{TPAGT}. It re-extracts the\nfeatures of the tracklets in the current frame based on motion predicting,\nwhich is the key to solve the problem of features inconsistent. The adaptive\ngraph neural network in TPAGT is adopted to fuse locations, appearance, and\nhistorical information, and plays an important role in distinguishing different\nobjects. In the training phase, we propose the balanced MSE LOSS to\nsuccessfully overcome the unbalanced samples. Experiments show that our method\nreaches state-of-the-art performance. It achieves 76.5\\% MOTA on the MOT16\nchallenge and 76.2\\% MOTA on the MOT17 challenge.\n", "versions": [{"version": "v1", "created": "Sun, 18 Oct 2020 16:16:49 GMT"}, {"version": "v2", "created": "Wed, 4 Nov 2020 19:14:02 GMT"}, {"version": "v3", "created": "Thu, 19 Nov 2020 05:46:35 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Shan", "Chaobing", ""], ["Wei", "Chunbo", ""], ["Deng", "Bing", ""], ["Huang", "Jianqiang", ""], ["Hua", "Xian-Sheng", ""], ["Cheng", "Xiaoliang", ""], ["Liang", "Kewei", ""]]}, {"id": "2010.09016", "submitter": "Jeffrey Uhlmann", "authors": "Jeffrey Uhlmann", "title": "Covapixels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose and discuss the summarization of superpixel-type image\ntiles/patches using mean and covariance information. We refer to the resulting\nobjects as covapixels.\n", "versions": [{"version": "v1", "created": "Sun, 18 Oct 2020 16:16:50 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Uhlmann", "Jeffrey", ""]]}, {"id": "2010.09035", "submitter": "Lisha Chen", "authors": "Lisha Chen, Hui Su, Qiang Ji", "title": "Deep Structured Prediction for Facial Landmark Detection", "comments": "Accepted by NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing deep learning based facial landmark detection methods have achieved\nexcellent performance. These methods, however, do not explicitly embed the\nstructural dependencies among landmark points. They hence cannot preserve the\ngeometric relationships between landmark points or generalize well to\nchallenging conditions or unseen data. This paper proposes a method for deep\nstructured facial landmark detection based on combining a deep Convolutional\nNetwork with a Conditional Random Field. We demonstrate its superior\nperformance to existing state-of-the-art techniques in facial landmark\ndetection, especially a better generalization ability on challenging datasets\nthat include large pose and occlusion.\n", "versions": [{"version": "v1", "created": "Sun, 18 Oct 2020 17:09:24 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Chen", "Lisha", ""], ["Su", "Hui", ""], ["Ji", "Qiang", ""]]}, {"id": "2010.09066", "submitter": "Sudipta Paul", "authors": "Sudipta Paul, Shivkumar Chandrasekaran, B.S. Manjunath, Amit K.\n  Roy-Chowdhury", "title": "Exploiting Context for Robustness to Label Noise in Active Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several works in computer vision have demonstrated the effectiveness of\nactive learning for adapting the recognition model when new unlabeled data\nbecomes available. Most of these works consider that labels obtained from the\nannotator are correct. However, in a practical scenario, as the quality of the\nlabels depends on the annotator, some of the labels might be wrong, which\nresults in degraded recognition performance. In this paper, we address the\nproblems of i) how a system can identify which of the queried labels are wrong\nand ii) how a multi-class active learning system can be adapted to minimize the\nnegative impact of label noise. Towards solving the problems, we propose a\nnoisy label filtering based learning approach where the inter-relationship\n(context) that is quite common in natural data is utilized to detect the wrong\nlabels. We construct a graphical representation of the unlabeled data to encode\nthese relationships and obtain new beliefs on the graph when noisy labels are\navailable. Comparing the new beliefs with the prior relational information, we\ngenerate a dissimilarity score to detect the incorrect labels and update the\nrecognition model with correct labels which result in better recognition\nperformance. This is demonstrated in three different applications: scene\nclassification, activity classification, and document classification.\n", "versions": [{"version": "v1", "created": "Sun, 18 Oct 2020 18:59:44 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Paul", "Sudipta", ""], ["Chandrasekaran", "Shivkumar", ""], ["Manjunath", "B. S.", ""], ["Roy-Chowdhury", "Amit K.", ""]]}, {"id": "2010.09067", "submitter": "Kristijan Fugo\\v{s}i\\'c", "authors": "Kristijan Fugo\\v{s}i\\'c, Josip \\v{S}ari\\'c, Sini\\v{s}a \\v{S}egvi\\'c", "title": "Multimodal semantic forecasting based on conditional generation of\n  future features", "comments": "Accepted to German Conference on Pattern Recognition 2020. 24 pages,\n  11 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers semantic forecasting in road-driving scenes. Most\nexisting approaches address this problem as deterministic regression of future\nfeatures or future predictions given observed frames. However, such approaches\nignore the fact that future can not always be guessed with certainty. For\nexample, when a car is about to turn around a corner, the road which is\ncurrently occluded by buildings may turn out to be either free to drive, or\noccupied by people, other vehicles or roadworks. When a deterministic model\nconfronts such situation, its best guess is to forecast the most likely\noutcome. However, this is not acceptable since it defeats the purpose of\nforecasting to improve security. It also throws away valuable training data,\nsince a deterministic model is unable to learn any deviation from the norm. We\naddress this problem by providing more freedom to the model through allowing it\nto forecast different futures. We propose to formulate multimodal forecasting\nas sampling of a multimodal generative model conditioned on the observed\nframes. Experiments on the Cityscapes dataset reveal that our multimodal model\noutperforms its deterministic counterpart in short-term forecasting while\nperforming slightly worse in the mid-term case.\n", "versions": [{"version": "v1", "created": "Sun, 18 Oct 2020 18:59:52 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Fugo\u0161i\u0107", "Kristijan", ""], ["\u0160ari\u0107", "Josip", ""], ["\u0160egvi\u0107", "Sini\u0161a", ""]]}, {"id": "2010.09076", "submitter": "Marcel Sheeny", "authors": "Marcel Sheeny, Emanuele De Pellegrin, Saptarshi Mukherjee, Alireza\n  Ahrabian, Sen Wang, Andrew Wallace", "title": "RADIATE: A Radar Dataset for Automotive Perception in Bad Weather", "comments": "Accepted at IEEE International Conference on Robotics and Automation\n  2021 (ICRA 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Datasets for autonomous cars are essential for the development and\nbenchmarking of perception systems. However, most existing datasets are\ncaptured with camera and LiDAR sensors in good weather conditions. In this\npaper, we present the RAdar Dataset In Adverse weaThEr (RADIATE), aiming to\nfacilitate research on object detection, tracking and scene understanding using\nradar sensing for safe autonomous driving. RADIATE includes 3 hours of\nannotated radar images with more than 200K labelled road actors in total, on\naverage about 4.6 instances per radar image. It covers 8 different categories\nof actors in a variety of weather conditions (e.g., sun, night, rain, fog and\nsnow) and driving scenarios (e.g., parked, urban, motorway and suburban),\nrepresenting different levels of challenge. To the best of our knowledge, this\nis the first public radar dataset which provides high-resolution radar images\non public roads with a large amount of road actors labelled. The data collected\nin adverse weather, e.g., fog and snowfall, is unique. Some baseline results of\nradar based object detection and recognition are given to show that the use of\nradar data is promising for automotive applications in bad weather, where\nvision and LiDAR can fail. RADIATE also has stereo images, 32-channel LiDAR and\nGPS data, directed at other applications such as sensor fusion, localisation\nand mapping. The public dataset can be accessed at\nhttp://pro.hw.ac.uk/radiate/.\n", "versions": [{"version": "v1", "created": "Sun, 18 Oct 2020 19:33:27 GMT"}, {"version": "v2", "created": "Wed, 30 Dec 2020 13:04:30 GMT"}, {"version": "v3", "created": "Mon, 5 Apr 2021 14:00:22 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Sheeny", "Marcel", ""], ["De Pellegrin", "Emanuele", ""], ["Mukherjee", "Saptarshi", ""], ["Ahrabian", "Alireza", ""], ["Wang", "Sen", ""], ["Wallace", "Andrew", ""]]}, {"id": "2010.09079", "submitter": "Mahdi Saleh", "authors": "Mahdi Saleh, Shervin Dehghani, Benjamin Busam, Nassir Navab, Federico\n  Tombari", "title": "Graphite: GRAPH-Induced feaTure Extraction for Point Cloud Registration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D Point clouds are a rich source of information that enjoy growing\npopularity in the vision community. However, due to the sparsity of their\nrepresentation, learning models based on large point clouds is still a\nchallenge. In this work, we introduce Graphite, a GRAPH-Induced feaTure\nExtraction pipeline, a simple yet powerful feature transform and keypoint\ndetector. Graphite enables intensive down-sampling of point clouds with\nkeypoint detection accompanied by a descriptor. We construct a generic\ngraph-based learning scheme to describe point cloud regions and extract salient\npoints. To this end, we take advantage of 6D pose information and metric\nlearning to learn robust descriptions and keypoints across different scans. We\nReformulate the 3D keypoint pipeline with graph neural networks which allow\nefficient processing of the point set while boosting its descriptive power\nwhich ultimately results in more accurate 3D registrations. We demonstrate our\nlightweight descriptor on common 3D descriptor matching and point cloud\nregistration benchmarks and achieve comparable results with the state of the\nart. Describing 100 patches of a point cloud and detecting their keypoints\ntakes only ~0.018 seconds with our proposed network.\n", "versions": [{"version": "v1", "created": "Sun, 18 Oct 2020 19:41:09 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Saleh", "Mahdi", ""], ["Dehghani", "Shervin", ""], ["Busam", "Benjamin", ""], ["Navab", "Nassir", ""], ["Tombari", "Federico", ""]]}, {"id": "2010.09084", "submitter": "Alireza Sepas-Moghaddam", "authors": "Alireza Sepas-Moghaddam, Saeed Ghorbani, Nikolaus F. Troje, Ali Etemad", "title": "Gait Recognition using Multi-Scale Partial Representation Transformation\n  with Capsules", "comments": "Accepted to International Conference on Pattern Recognition (ICPR)\n  2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gait recognition, referring to the identification of individuals based on the\nmanner in which they walk, can be very challenging due to the variations in the\nviewpoint of the camera and the appearance of individuals. Current methods for\ngait recognition have been dominated by deep learning models, notably those\nbased on partial feature representations. In this context, we propose a novel\ndeep network, learning to transfer multi-scale partial gait representations\nusing capsules to obtain more discriminative gait features. Our network first\nobtains multi-scale partial representations using a state-of-the-art deep\npartial feature extractor. It then recurrently learns the correlations and\nco-occurrences of the patterns among the partial features in forward and\nbackward directions using Bi-directional Gated Recurrent Units (BGRU). Finally,\na capsule network is adopted to learn deeper part-whole relationships and\nassigns more weights to the more relevant features while ignoring the spurious\ndimensions. That way, we obtain final features that are more robust to both\nviewing and appearance changes. The performance of our method has been\nextensively tested on two gait recognition datasets, CASIA-B and OU-MVLP, using\nfour challenging test protocols. The results of our method have been compared\nto the state-of-the-art gait recognition solutions, showing the superiority of\nour model, notably when facing challenging viewing and carrying conditions.\n", "versions": [{"version": "v1", "created": "Sun, 18 Oct 2020 19:47:38 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Sepas-Moghaddam", "Alireza", ""], ["Ghorbani", "Saeed", ""], ["Troje", "Nikolaus F.", ""], ["Etemad", "Ali", ""]]}, {"id": "2010.09092", "submitter": "Alireza Sepas-Moghaddam", "authors": "Alireza Sepas-Moghaddam, Ali Etemad", "title": "View-Invariant Gait Recognition with Attentive Recurrent Learning of\n  Partial Representations", "comments": "Accepted in IEEE Transactions on Biometrics, Behavior, and Identity\n  Science (T-BIOM)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gait recognition refers to the identification of individuals based on\nfeatures acquired from their body movement during walking. Despite the recent\nadvances in gait recognition with deep learning, variations in data acquisition\nand appearance, namely camera angles, subject pose, occlusions, and clothing,\nare challenging factors that need to be considered for achieving accurate gait\nrecognition systems. In this paper, we propose a network that first learns to\nextract gait convolutional energy maps (GCEM) from frame-level convolutional\nfeatures. It then adopts a bidirectional recurrent neural network to learn from\nsplit bins of the GCEM, thus exploiting the relations between learned partial\nspatiotemporal representations. We then use an attention mechanism to\nselectively focus on important recurrently learned partial representations as\nidentity information in different scenarios may lie in different GCEM bins. Our\nproposed model has been extensively tested on two large-scale CASIA-B and\nOU-MVLP gait datasets using four different test protocols and has been compared\nto a number of state-of-the-art and baseline solutions. Additionally, a\ncomprehensive experiment has been performed to study the robustness of our\nmodel in the presence of six different synthesized occlusions. The experimental\nresults show the superiority of our proposed method, outperforming the\nstate-of-the-art, especially in scenarios where different clothing and carrying\nconditions are encountered. The results also revealed that our model is more\nrobust against different occlusions as compared to the state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Sun, 18 Oct 2020 20:20:43 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Sepas-Moghaddam", "Alireza", ""], ["Etemad", "Ali", ""]]}, {"id": "2010.09102", "submitter": "Harish RaviPrakash", "authors": "Harish RaviPrakash, Syed Muhammad Anwar, Ulas Bagci", "title": "Variational Capsule Encoder", "comments": "Accepted for publication in the 2020 25th International Conference on\n  Pattern Recognition (ICPR)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a novel capsule network based variational encoder architecture,\ncalled Bayesian capsules (B-Caps), to modulate the mean and standard deviation\nof the sampling distribution in the latent space. We hypothesized that this\napproach can learn a better representation of features in the latent space than\ntraditional approaches. Our hypothesis was tested by using the learned latent\nvariables for image reconstruction task, where for MNIST and Fashion-MNIST\ndatasets, different classes were separated successfully in the latent space\nusing our proposed model. Our experimental results have shown improved\nreconstruction and classification performances for both datasets adding\ncredence to our hypothesis. We also showed that by increasing the latent space\ndimension, the proposed B-Caps was able to learn a better representation when\ncompared to the traditional variational auto-encoders (VAE). Hence our results\nindicate the strength of capsule networks in representation learning which has\nnever been examined under the VAE settings before.\n", "versions": [{"version": "v1", "created": "Sun, 18 Oct 2020 20:52:16 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["RaviPrakash", "Harish", ""], ["Anwar", "Syed Muhammad", ""], ["Bagci", "Ulas", ""]]}, {"id": "2010.09103", "submitter": "Jose Principe", "authors": "Ryan Burt, Nina N. Thigpen, Andreas Keil, Jose C. Principe", "title": "Unsupervised Foveal Vision Neural Networks with Top-Down Attention", "comments": "29 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning architectures are an extremely powerful tool for recognizing\nand classifying images. However, they require supervised learning and normally\nwork on vectors the size of image pixels and produce the best results when\ntrained on millions of object images. To help mitigate these issues, we propose\nthe fusion of bottom-up saliency and top-down attention employing only\nunsupervised learning techniques, which helps the object recognition module to\nfocus on relevant data and learn important features that can later be\nfine-tuned for a specific task. In addition, by utilizing only relevant\nportions of the data, the training speed can be greatly improved. We test the\nperformance of the proposed Gamma saliency technique on the Toronto and CAT2000\ndatabases, and the foveated vision in the Street View House Numbers (SVHN)\ndatabase. The results in foveated vision show that Gamma saliency is comparable\nto the best and computationally faster. The results in SVHN show that our\nunsupervised cognitive architecture is comparable to fully supervised methods\nand that the Gamma saliency also improves CNN performance if desired. We also\ndevelop a topdown attention mechanism based on the Gamma saliency applied to\nthe top layer of CNNs to improve scene understanding in multi-object images or\nimages with strong background clutter. When we compare the results with human\nobservers in an image dataset of animals occluded in natural scenes, we show\nthat topdown attention is capable of disambiguating object from background and\nimproves system performance beyond the level of human observers.\n", "versions": [{"version": "v1", "created": "Sun, 18 Oct 2020 20:55:49 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Burt", "Ryan", ""], ["Thigpen", "Nina N.", ""], ["Keil", "Andreas", ""], ["Principe", "Jose C.", ""]]}, {"id": "2010.09105", "submitter": "Yuxin Hou", "authors": "Yuxin Hou, Muhammad Kamran Janjua, Juho Kannala, Arno Solin", "title": "Movement-induced Priors for Deep Stereo", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method for fusing stereo disparity estimation with\nmovement-induced prior information. Instead of independent inference\nframe-by-frame, we formulate the problem as a non-parametric learning task in\nterms of a temporal Gaussian process prior with a movement-driven kernel for\ninter-frame reasoning. We present a hierarchy of three Gaussian process kernels\ndepending on the availability of motion information, where our main focus is on\na new gyroscope-driven kernel for handheld devices with low-quality MEMS\nsensors, thus also relaxing the requirement of having full 6D camera poses\navailable. We show how our method can be combined with two state-of-the-art\ndeep stereo methods. The method either work in a plug-and-play fashion with\npre-trained deep stereo networks, or further improved by jointly training the\nkernels together with encoder-decoder architectures, leading to consistent\nimprovement.\n", "versions": [{"version": "v1", "created": "Sun, 18 Oct 2020 21:02:25 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Hou", "Yuxin", ""], ["Janjua", "Muhammad Kamran", ""], ["Kannala", "Juho", ""], ["Solin", "Arno", ""]]}, {"id": "2010.09125", "submitter": "Yuxuan Zhang", "authors": "Yuxuan Zhang, Wenzheng Chen, Huan Ling, Jun Gao, Yinan Zhang, Antonio\n  Torralba, Sanja Fidler", "title": "Image GANs meet Differentiable Rendering for Inverse Graphics and\n  Interpretable 3D Neural Rendering", "comments": "Accepted to ICLR 2021 as an Oral paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Differentiable rendering has paved the way to training neural networks to\nperform \"inverse graphics\" tasks such as predicting 3D geometry from monocular\nphotographs. To train high performing models, most of the current approaches\nrely on multi-view imagery which are not readily available in practice. Recent\nGenerative Adversarial Networks (GANs) that synthesize images, in contrast,\nseem to acquire 3D knowledge implicitly during training: object viewpoints can\nbe manipulated by simply manipulating the latent codes. However, these latent\ncodes often lack further physical interpretation and thus GANs cannot easily be\ninverted to perform explicit 3D reasoning. In this paper, we aim to extract and\ndisentangle 3D knowledge learned by generative models by utilizing\ndifferentiable renderers. Key to our approach is to exploit GANs as a\nmulti-view data generator to train an inverse graphics network using an\noff-the-shelf differentiable renderer, and the trained inverse graphics network\nas a teacher to disentangle the GAN's latent code into interpretable 3D\nproperties. The entire architecture is trained iteratively using cycle\nconsistency losses. We show that our approach significantly outperforms\nstate-of-the-art inverse graphics networks trained on existing datasets, both\nquantitatively and via user studies. We further showcase the disentangled GAN\nas a controllable 3D \"neural renderer\", complementing traditional graphics\nrenderers.\n", "versions": [{"version": "v1", "created": "Sun, 18 Oct 2020 22:29:07 GMT"}, {"version": "v2", "created": "Tue, 20 Apr 2021 18:06:17 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Zhang", "Yuxuan", ""], ["Chen", "Wenzheng", ""], ["Ling", "Huan", ""], ["Gao", "Jun", ""], ["Zhang", "Yinan", ""], ["Torralba", "Antonio", ""], ["Fidler", "Sanja", ""]]}, {"id": "2010.09140", "submitter": "Soumajit Majumder", "authors": "Soumajit Majumder, Angela Yao", "title": "Localized Interactive Instance Segmentation", "comments": "Preprint of the accepted paper at GCPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In current interactive instance segmentation works, the user is granted a\nfree hand when providing clicks to segment an object; clicks are allowed on\nbackground pixels and other object instances far from the target object. This\nform of interaction is highly inconsistent with the end goal of efficiently\nisolating objects of interest. In our work, we propose a clicking scheme\nwherein user interactions are restricted to the proximity of the object. In\naddition, we propose a novel transformation of the user-provided clicks to\ngenerate a weak localization prior on the object which is consistent with image\nstructures such as edges, textures etc. We demonstrate the effectiveness of our\nproposed clicking scheme and localization strategy through detailed\nexperimentation in which we raise state-of-the-art on several standard\ninteractive segmentation benchmarks.\n", "versions": [{"version": "v1", "created": "Sun, 18 Oct 2020 23:24:09 GMT"}, {"version": "v2", "created": "Tue, 20 Oct 2020 09:57:07 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Majumder", "Soumajit", ""], ["Yao", "Angela", ""]]}, {"id": "2010.09164", "submitter": "Masha Itkina", "authors": "Masha Itkina, Boris Ivanovic, Ransalu Senanayake, Mykel J.\n  Kochenderfer, and Marco Pavone", "title": "Evidential Sparsification of Multimodal Latent Spaces in Conditional\n  Variational Autoencoders", "comments": "21 pages, 15 figures, 34th Conference on Neural Information\n  Processing Systems (NeurIPS 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discrete latent spaces in variational autoencoders have been shown to\neffectively capture the data distribution for many real-world problems such as\nnatural language understanding, human intent prediction, and visual scene\nrepresentation. However, discrete latent spaces need to be sufficiently large\nto capture the complexities of real-world data, rendering downstream tasks\ncomputationally challenging. For instance, performing motion planning in a\nhigh-dimensional latent representation of the environment could be intractable.\nWe consider the problem of sparsifying the discrete latent space of a trained\nconditional variational autoencoder, while preserving its learned\nmultimodality. As a post hoc latent space reduction technique, we use\nevidential theory to identify the latent classes that receive direct evidence\nfrom a particular input condition and filter out those that do not. Experiments\non diverse tasks, such as image generation and human behavior prediction,\ndemonstrate the effectiveness of our proposed technique at reducing the\ndiscrete latent sample space size of a model while maintaining its learned\nmultimodality.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 01:27:21 GMT"}, {"version": "v2", "created": "Thu, 22 Oct 2020 22:28:54 GMT"}, {"version": "v3", "created": "Mon, 18 Jan 2021 18:34:32 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Itkina", "Masha", ""], ["Ivanovic", "Boris", ""], ["Senanayake", "Ransalu", ""], ["Kochenderfer", "Mykel J.", ""], ["Pavone", "Marco", ""]]}, {"id": "2010.09169", "submitter": "Zhi Qiao", "authors": "Zhi Qiao, Xugong Qin, Yu Zhou, Fei Yang, Weiping Wang", "title": "Gaussian Constrained Attention Network for Scene Text Recognition", "comments": "ICPR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scene text recognition has been a hot topic in computer vision. Recent\nmethods adopt the attention mechanism for sequence prediction which achieve\nconvincing results. However, we argue that the existing attention mechanism\nfaces the problem of attention diffusion, in which the model may not focus on a\ncertain character area. In this paper, we propose Gaussian Constrained\nAttention Network to deal with this problem. It is a 2D attention-based method\nintegrated with a novel Gaussian Constrained Refinement Module, which predicts\nan additional Gaussian mask to refine the attention weights. Different from\nadopting an additional supervision on the attention weights simply, our\nproposed method introduces an explicit refinement. In this way, the attention\nweights will be more concentrated and the attention-based recognition network\nachieves better performance. The proposed Gaussian Constrained Refinement\nModule is flexible and can be applied to existing attention-based methods\ndirectly. The experiments on several benchmark datasets demonstrate the\neffectiveness of our proposed method. Our code has been available at\nhttps://github.com/Pay20Y/GCAN.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 01:55:30 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Qiao", "Zhi", ""], ["Qin", "Xugong", ""], ["Zhou", "Yu", ""], ["Yang", "Fei", ""], ["Wang", "Weiping", ""]]}, {"id": "2010.09185", "submitter": "Vinit Chunilal Sarode", "authors": "Vinit Sarode, Animesh Dhagat, Rangaprasad Arun Srivatsan, Nicolas\n  Zevallos, Simon Lucey, Howie Choset", "title": "MaskNet: A Fully-Convolutional Network to Estimate Inlier Points", "comments": "Accepted at International Conference on 3D Vision (3DV, 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point clouds have grown in importance in the way computers perceive the\nworld. From LIDAR sensors in autonomous cars and drones to the time of flight\nand stereo vision systems in our phones, point clouds are everywhere. Despite\ntheir ubiquity, point clouds in the real world are often missing points because\nof sensor limitations or occlusions, or contain extraneous points from sensor\nnoise or artifacts. These problems challenge algorithms that require computing\ncorrespondences between a pair of point clouds. Therefore, this paper presents\na fully-convolutional neural network that identifies which points in one point\ncloud are most similar (inliers) to the points in another. We show improvements\nin learning-based and classical point cloud registration approaches when\nretrofitted with our network. We demonstrate these improvements on synthetic\nand real-world datasets. Finally, our network produces impressive results on\ntest datasets that were unseen during training, thus exhibiting\ngeneralizability. Code and videos are available at\nhttps://github.com/vinits5/masknet\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 03:18:35 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Sarode", "Vinit", ""], ["Dhagat", "Animesh", ""], ["Srivatsan", "Rangaprasad Arun", ""], ["Zevallos", "Nicolas", ""], ["Lucey", "Simon", ""], ["Choset", "Howie", ""]]}, {"id": "2010.09202", "submitter": "Hyunseung Chung", "authors": "Hyunseung Chung, Woo-Jeoung Nam, Seong-Whan Lee", "title": "Rotation Invariant Aerial Image Retrieval with Group Convolutional\n  Metric Learning", "comments": "8 pages, 5 figures, Accepted in ICPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Remote sensing image retrieval (RSIR) is the process of ranking database\nimages depending on the degree of similarity compared to the query image. As\nthe complexity of RSIR increases due to the diversity in shooting range, angle,\nand location of remote sensors, there is an increasing demand for methods to\naddress these issues and improve retrieval performance. In this work, we\nintroduce a novel method for retrieving aerial images by merging group\nconvolution with attention mechanism and metric learning, resulting in\nrobustness to rotational variations. For refinement and emphasis on important\nfeatures, we applied channel attention in each group convolution stage. By\nutilizing the characteristics of group convolution and channel-wise attention,\nit is possible to acknowledge the equality among rotated but identically\nlocated images. The training procedure has two main steps: (i) training the\nnetwork with Aerial Image Dataset (AID) for classification, (ii) fine-tuning\nthe network with triplet-loss for retrieval with Google Earth South Korea and\nNWPU-RESISC45 datasets. Results show that the proposed method performance\nexceeds other state-of-the-art retrieval methods in both rotated and original\nenvironments. Furthermore, we utilize class activation maps (CAM) to visualize\nthe distinct difference of main features between our method and baseline,\nresulting in better adaptability in rotated environments.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 04:12:36 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Chung", "Hyunseung", ""], ["Nam", "Woo-Jeoung", ""], ["Lee", "Seong-Whan", ""]]}, {"id": "2010.09211", "submitter": "Nakul Agarwal", "authors": "Nakul Agarwal, Yi-Ting Chen, Behzad Dariush, Ming-Hsuan Yang", "title": "Unsupervised Domain Adaptation for Spatio-Temporal Action Localization", "comments": "Accepted in BMVC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatio-temporal action localization is an important problem in computer\nvision that involves detecting where and when activities occur, and therefore\nrequires modeling of both spatial and temporal features. This problem is\ntypically formulated in the context of supervised learning, where the learned\nclassifiers operate on the premise that both training and test data are sampled\nfrom the same underlying distribution. However, this assumption does not hold\nwhen there is a significant domain shift, leading to poor generalization\nperformance on the test data. To address this, we focus on the hard and novel\ntask of generalizing training models to test samples without access to any\nlabels from the latter for spatio-temporal action localization by proposing an\nend-to-end unsupervised domain adaptation algorithm. We extend the\nstate-of-the-art object detection framework to localize and classify actions.\nIn order to minimize the domain shift, three domain adaptation modules at image\nlevel (temporal and spatial) and instance level (temporal) are designed and\nintegrated. We design a new experimental setup and evaluate the proposed method\nand different adaptation modules on the UCF-Sports, UCF-101 and JHMDB benchmark\ndatasets. We show that significant performance gain can be achieved when\nspatial and temporal features are adapted separately, or jointly for the most\neffective results.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 04:25:10 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Agarwal", "Nakul", ""], ["Chen", "Yi-Ting", ""], ["Dariush", "Behzad", ""], ["Yang", "Ming-Hsuan", ""]]}, {"id": "2010.09221", "submitter": "Ming Li", "authors": "Ming Li, Xinming Huang, Ziming Zhang", "title": "Discovering Discriminative Geometric Features with Self-Supervised\n  Attention for Vehicle Re-Identification and Beyond", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the literature of vehicle re-identification (ReID), intensive manual\nlabels such as landmarks, critical parts or semantic segmentation masks are\noften required to improve the performance. Such extra information helps to\ndetect locally geometric features as a part of representation learning for\nvehicles. In contrast, in this paper, we aim to address the challenge of {\\em\nautomatically} learning to detect geometric features as landmarks {\\em with no\nextra labels}. To the best of our knowledge, we are the {\\em first} to\nsuccessfully learn discriminative geometric features for vehicle ReID based on\nself-supervised attention. Specifically, we implement an end-to-end trainable\ndeep network architecture consisting of three branches: (1) a global branch as\nbackbone for image feature extraction, (2) an attentional branch for producing\nattention masks, and (3) a self-supervised branch for regularizing the\nattention learning with rotated images to locate geometric features. %Our\nnetwork design naturally leads to an end-to-end multi-task joint optimization.\nWe conduct comprehensive experiments on three benchmark datasets for vehicle\nReID, \\ie VeRi-776, CityFlow-ReID, and VehicleID, and demonstrate our\nstate-of-the-art performance. %of our approach with the capability of capturing\ninformative vehicle parts with no corresponding manual labels. We also show the\ngood generalization of our approach in other ReID tasks such as person ReID and\nmulti-target multi-camera (MTMC) vehicle tracking. {\\em Our demo code is\nattached in the supplementary file.}\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 04:43:56 GMT"}, {"version": "v2", "created": "Tue, 19 Jan 2021 06:26:52 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Li", "Ming", ""], ["Huang", "Xinming", ""], ["Zhang", "Ziming", ""]]}, {"id": "2010.09228", "submitter": "Tobias Fischer", "authors": "Timothy L. Molloy and Tobias Fischer and Michael Milford and Girish N.\n  Nair", "title": "Intelligent Reference Curation for Visual Place Recognition via Bayesian\n  Selective Fusion", "comments": "8 pages, 10 figures, accepted in the IEEE Robotics and Automation\n  Letters", "journal-ref": "IEEE Robotics and Automation Letters 6(2):588-595, 2021", "doi": "10.1109/LRA.2020.3047791", "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key challenge in visual place recognition (VPR) is recognizing places\ndespite drastic visual appearance changes due to factors such as time of day,\nseason, weather or lighting conditions. Numerous approaches based on\ndeep-learnt image descriptors, sequence matching, domain translation, and\nprobabilistic localization have had success in addressing this challenge, but\nmost rely on the availability of carefully curated representative reference\nimages of the possible places. In this paper, we propose a novel approach,\ndubbed Bayesian Selective Fusion, for actively selecting and fusing informative\nreference images to determine the best place match for a given query image. The\nselective element of our approach avoids the counterproductive fusion of every\nreference image and enables the dynamic selection of informative reference\nimages in environments with changing visual conditions (such as indoors with\nflickering lights, outdoors during sunshowers or over the day-night cycle). The\nprobabilistic element of our approach provides a means of fusing multiple\nreference images that accounts for their varying uncertainty via a novel\ntraining-free likelihood function for VPR. On difficult query images from two\nbenchmark datasets, we demonstrate that our approach matches and exceeds the\nperformance of several alternative fusion approaches along with\nstate-of-the-art techniques that are provided with prior (unfair) knowledge of\nthe best reference images. Our approach is well suited for long-term robot\nautonomy where dynamic visual environments are commonplace since it is\ntraining-free, descriptor-agnostic, and complements existing techniques such as\nsequence matching.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 05:17:35 GMT"}, {"version": "v2", "created": "Sun, 3 Jan 2021 22:28:28 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Molloy", "Timothy L.", ""], ["Fischer", "Tobias", ""], ["Milford", "Michael", ""], ["Nair", "Girish N.", ""]]}, {"id": "2010.09236", "submitter": "Joonhyuk Kim", "authors": "Joonhyuk Kim, Sahng-Min Yoo, Gyeong-Moon Park, Jong-Hwan Kim", "title": "Continual Unsupervised Domain Adaptation with Adversarial Learning", "comments": "9 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised Domain Adaptation (UDA) is essential for autonomous driving due\nto a lack of labeled real-world road images. Most of the existing UDA methods,\nhowever, have focused on a single-step domain adaptation (Synthetic-to-Real).\nThese methods overlook a change in environments in the real world as time goes\nby. Thus, developing a domain adaptation method for sequentially changing\ntarget domains without catastrophic forgetting is required for real-world\napplications. To deal with the problem above, we propose Continual Unsupervised\nDomain Adaptation with Adversarial learning (CUDA^2) framework, which can\ngenerally be applicable to other UDA methods conducting adversarial learning.\nCUDA^2 framework generates a sub-memory, called Target-specific Memory (TM) for\neach new target domain guided by Double Hinge Adversarial (DHA) loss. TM\nprevents catastrophic forgetting by storing target-specific information, and\nDHA loss induces a synergy between the existing network and the expanded TM. To\nthe best of our knowledge, we consider realistic autonomous driving scenarios\n(Synthetic-to-Real-to-Real) in UDA research for the first time. The model with\nour framework outperforms other state-of-the-art models under the same\nsettings. Besides, extensive experiments are conducted as ablation studies for\nin-depth analysis.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 05:59:48 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Kim", "Joonhyuk", ""], ["Yoo", "Sahng-Min", ""], ["Park", "Gyeong-Moon", ""], ["Kim", "Jong-Hwan", ""]]}, {"id": "2010.09241", "submitter": "Kohei Yamamichi", "authors": "Kohei Yamamichi, Xian-Hua Han", "title": "MCGKT-Net: Multi-level Context Gating Knowledge Transfer Network for\n  Single Image Deraining", "comments": "ACCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rain streak removal in a single image is a very challenging task due to its\nill-posed nature in essence. Recently, the end-to-end learning techniques with\ndeep convolutional neural networks (DCNN) have made great progress in this\ntask. However, the conventional DCNN-based deraining methods have struggled to\nexploit deeper and more complex network architectures for pursuing better\nperformance. This study proposes a novel MCGKT-Net for boosting deraining\nperformance, which is a naturally multi-scale learning framework being capable\nof exploring multi-scale attributes of rain streaks and different semantic\nstructures of the clear images. In order to obtain high representative features\ninside MCGKT-Net, we explore internal knowledge transfer module using ConvLSTM\nunit for conducting interaction learning between different layers and\ninvestigate external knowledge transfer module for leveraging the knowledge\nalready learned in other task domains. Furthermore, to dynamically select\nuseful features in learning procedure, we propose a multi-scale context gating\nmodule in the MCGKT-Net using squeeze-and-excitation block. Experiments on\nthree benchmark datasets: Rain100H, Rain100L, and Rain800, manifest impressive\nperformance compared with state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 06:21:07 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Yamamichi", "Kohei", ""], ["Han", "Xian-Hua", ""]]}, {"id": "2010.09245", "submitter": "Scott Leask", "authors": "Scott Leask, Vincent McDonell", "title": "Extraction of Discrete Spectra Modes from Video Data Using a Deep\n  Convolutional Koopman Network", "comments": "7 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV math.DS physics.flu-dyn", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent deep learning extensions in Koopman theory have enabled compact,\ninterpretable representations of nonlinear dynamical systems which are amenable\nto linear analysis. Deep Koopman networks attempt to learn the Koopman\neigenfunctions which capture the coordinate transformation to globally\nlinearize system dynamics. These eigenfunctions can be linked to underlying\nsystem modes which govern the dynamical behavior of the system. While many\nrelated techniques have demonstrated their efficacy on canonical systems and\ntheir associated state variables, in this work the system dynamics are observed\noptically (i.e. in video format). We demonstrate the ability of a deep\nconvolutional Koopman network (CKN) in automatically identifying independent\nmodes for dynamical systems with discrete spectra. Practically, this affords\nflexibility in system data collection as the data are easily obtainable\nobservable variables. The learned models are able to successfully and robustly\nidentify the underlying modes governing the system, even with a redundantly\nlarge embedding space. Modal disaggregation is encouraged using a simple\nmasking procedure. All of the systems analyzed in this work use an identical\nnetwork architecture.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 06:26:29 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Leask", "Scott", ""], ["McDonell", "Vincent", ""]]}, {"id": "2010.09273", "submitter": "Fabian Timm", "authors": "Michael Ulrich and Claudius Gl\\\"aser and Fabian Timm", "title": "DeepReflecs: Deep Learning for Automotive Object Classification with\n  Radar Reflections", "comments": "preprint, under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an novel object type classification method for automotive\napplications which uses deep learning with radar reflections. The method\nprovides object class information such as pedestrian, cyclist, car, or\nnon-obstacle. The method is both powerful and efficient, by using a\nlight-weight deep learning approach on reflection level radar data. It fills\nthe gap between low-performant methods of handcrafted features and\nhigh-performant methods with convolutional neural networks. The proposed\nnetwork exploits the specific characteristics of radar reflection data: It\nhandles unordered lists of arbitrary length as input and it combines both\nextraction of local and global features. In experiments with real data the\nproposed network outperforms existing methods of handcrafted or learned\nfeatures. An ablation study analyzes the impact of the proposed global context\nlayer.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 07:35:51 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Ulrich", "Michael", ""], ["Gl\u00e4ser", "Claudius", ""], ["Timm", "Fabian", ""]]}, {"id": "2010.09277", "submitter": "Yixin Wang", "authors": "Yixin Wang, Yao Zhang, Feng Hou, Yang Liu, Jiang Tian, Cheng Zhong,\n  Yang Zhang, Zhiqiang He", "title": "Modality-Pairing Learning for Brain Tumor Segmentation", "comments": "Second place of BraTS 2020 Challenge", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic brain tumor segmentation from multi-modality Magnetic Resonance\nImages (MRI) using deep learning methods plays an important role in assisting\nthe diagnosis and treatment of brain tumor. However, previous methods mostly\nignore the latent relationship among different modalities. In this work, we\npropose a novel end-to-end Modality-Pairing learning method for brain tumor\nsegmentation. Paralleled branches are designed to exploit different modality\nfeatures and a series of layer connections are utilized to capture complex\nrelationships and abundant information among modalities. We also use a\nconsistency loss to minimize the prediction variance between two branches.\nBesides, learning rate warmup strategy is adopted to solve the problem of the\ntraining instability and early over-fitting. Lastly, we use average ensemble of\nmultiple models and some post-processing techniques to get final results. Our\nmethod is tested on the BraTS 2020 online testing dataset, obtaining promising\nsegmentation performance, with average dice scores of 0.891, 0.842, 0.816 for\nthe whole tumor, tumor core and enhancing tumor, respectively. We won the\nsecond place of the BraTS 2020 Challenge for the tumor segmentation task.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 07:42:10 GMT"}, {"version": "v2", "created": "Tue, 29 Dec 2020 02:59:57 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Wang", "Yixin", ""], ["Zhang", "Yao", ""], ["Hou", "Feng", ""], ["Liu", "Yang", ""], ["Tian", "Jiang", ""], ["Zhong", "Cheng", ""], ["Zhang", "Yang", ""], ["He", "Zhiqiang", ""]]}, {"id": "2010.09278", "submitter": "Wen Fei", "authors": "Wen Fei, Wenrui Dai, Chenglin Li, Junni Zou, Hongkai Xiong", "title": "MimicNorm: Weight Mean and Last BN Layer Mimic the Dynamic of Batch\n  Normalization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Substantial experiments have validated the success of Batch Normalization\n(BN) Layer in benefiting convergence and generalization. However, BN requires\nextra memory and float-point calculation. Moreover, BN would be inaccurate on\nmicro-batch, as it depends on batch statistics. In this paper, we address these\nproblems by simplifying BN regularization while keeping two fundamental impacts\nof BN layers, i.e., data decorrelation and adaptive learning rate. We propose a\nnovel normalization method, named MimicNorm, to improve the convergence and\nefficiency in network training. MimicNorm consists of only two light\noperations, including modified weight mean operations (subtract mean values\nfrom weight parameter tensor) and one BN layer before loss function (last BN\nlayer). We leverage the neural tangent kernel (NTK) theory to prove that our\nweight mean operation whitens activations and transits network into the chaotic\nregime like BN layer, and consequently, leads to an enhanced convergence. The\nlast BN layer provides autotuned learning rates and also improves accuracy.\nExperimental results show that MimicNorm achieves similar accuracy for various\nnetwork structures, including ResNets and lightweight networks like ShuffleNet,\nwith a reduction of about 20% memory consumption. The code is publicly\navailable at https://github.com/Kid-key/MimicNorm.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 07:42:41 GMT"}, {"version": "v2", "created": "Sat, 24 Oct 2020 01:50:11 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Fei", "Wen", ""], ["Dai", "Wenrui", ""], ["Li", "Chenglin", ""], ["Zou", "Junni", ""], ["Xiong", "Hongkai", ""]]}, {"id": "2010.09290", "submitter": "Fang Tao Li", "authors": "Fangtao Li, Wenzhe Wang, Zihe Liu, Haoran Wang, Chenghao Yan, Bin Wu", "title": "Frame Aggregation and Multi-Modal Fusion Framework for Video-Based\n  Person Recognition", "comments": "Accepted by MMM 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video-based person recognition is challenging due to persons being blocked\nand blurred, and the variation of shooting angle. Previous research always\nfocused on person recognition on still images, ignoring similarity and\ncontinuity between video frames. To tackle the challenges above, we propose a\nnovel Frame Aggregation and Multi-Modal Fusion (FAMF) framework for video-based\nperson recognition, which aggregates face features and incorporates them with\nmulti-modal information to identify persons in videos. For frame aggregation,\nwe propose a novel trainable layer based on NetVLAD (named AttentionVLAD),\nwhich takes arbitrary number of features as input and computes a fixed-length\naggregation feature based on feature quality. We show that introducing an\nattention mechanism to NetVLAD can effectively decrease the impact of\nlow-quality frames. For the multi-model information of videos, we propose a\nMulti-Layer Multi-Modal Attention (MLMA) module to learn the correlation of\nmulti-modality by adaptively updating Gram matrix. Experimental results on\niQIYI-VID-2019 dataset show that our framework outperforms other\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 08:06:40 GMT"}, {"version": "v2", "created": "Wed, 30 Dec 2020 09:01:06 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Li", "Fangtao", ""], ["Wang", "Wenzhe", ""], ["Liu", "Zihe", ""], ["Wang", "Haoran", ""], ["Yan", "Chenghao", ""], ["Wu", "Bin", ""]]}, {"id": "2010.09291", "submitter": "Salman Khan Dr.", "authors": "Jathushan Rajasegaran, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan,\n  Mubarak Shah", "title": "Meta-learning the Learning Trends Shared Across Tasks", "comments": "Code will be released at https://github.com/brjathu/PAMELA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Meta-learning stands for 'learning to learn' such that generalization to new\ntasks is achieved. Among these methods, Gradient-based meta-learning algorithms\nare a specific sub-class that excel at quick adaptation to new tasks with\nlimited data. This demonstrates their ability to acquire transferable\nknowledge, a capability that is central to human learning. However, the\nexisting meta-learning approaches only depend on the current task information\nduring the adaptation, and do not share the meta-knowledge of how a similar\ntask has been adapted before. To address this gap, we propose a 'Path-aware'\nmodel-agnostic meta-learning approach. Specifically, our approach not only\nlearns a good initialization for adaptation, it also learns an optimal way to\nadapt these parameters to a set of task-specific parameters, with learnable\nupdate directions, learning rates and, most importantly, the way updates evolve\nover different time-steps. Compared to the existing meta-learning methods, our\napproach offers: (a) The ability to learn gradient-preconditioning at different\ntime-steps of the inner-loop, thereby modeling the dynamic learning behavior\nshared across tasks, and (b) The capability of aggregating the learning context\nthrough the provision of direct gradient-skip connections from the old\ntime-steps, thus avoiding overfitting and improving generalization. In essence,\nour approach not only learns a transferable initialization, but also models the\noptimal update directions, learning rates, and task-specific learning trends.\nSpecifically, in terms of learning trends, our approach determines the way\nupdate directions shape up as the task-specific learning progresses and how the\nprevious update history helps in the current update. Our approach is simple to\nimplement and demonstrates faster convergence. We report significant\nperformance improvements on a number of FSL datasets.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 08:06:47 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Rajasegaran", "Jathushan", ""], ["Khan", "Salman", ""], ["Hayat", "Munawar", ""], ["Khan", "Fahad Shahbaz", ""], ["Shah", "Mubarak", ""]]}, {"id": "2010.09294", "submitter": "Zhuo Su", "authors": "Zhuo Su, Linpu Fang, Deke Guo, Dewen Hu, Matti Pietik\\\"ainen, Li Liu", "title": "FTBNN: Rethinking Non-linearity for 1-bit CNNs and Going Beyond", "comments": "Openreview: https://openreview.net/forum?id=9wHe4F-lpp", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Binary neural networks (BNNs), where both weights and activations are\nbinarized into 1 bit, have been widely studied in recent years due to its great\nbenefit of highly accelerated computation and substantially reduced memory\nfootprint that appeal to the development of resource constrained devices. In\ncontrast to previous methods tending to reduce the quantization error for\ntraining BNN structures, we argue that the binarized convolution process owns\nan increasing linearity towards the target of minimizing such error, which in\nturn hampers BNN's discriminative ability. In this paper, we re-investigate and\ntune proper non-linear modules to fix that contradiction, leading to a strong\nbaseline which achieves state-of-the-art performance on the large-scale\nImageNet dataset in terms of accuracy and training efficiency. To go further,\nwe find that the proposed BNN model still has much potential to be compressed\nby making a better use of the efficient binary operations, without losing\naccuracy. In addition, the limited capacity of the BNN model can also be\nincreased with the help of group execution. Based on these insights, we are\nable to improve the baseline with an additional 4~5% top-1 accuracy gain even\nwith less computational cost. Our code will be made public at\nhttps://github.com/zhuogege1943/ftbnn.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 08:11:48 GMT"}, {"version": "v2", "created": "Thu, 29 Oct 2020 21:01:00 GMT"}, {"version": "v3", "created": "Mon, 30 Nov 2020 16:47:22 GMT"}, {"version": "v4", "created": "Wed, 30 Dec 2020 09:48:00 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Su", "Zhuo", ""], ["Fang", "Linpu", ""], ["Guo", "Deke", ""], ["Hu", "Dewen", ""], ["Pietik\u00e4inen", "Matti", ""], ["Liu", "Li", ""]]}, {"id": "2010.09297", "submitter": "Xiyue Guo", "authors": "Xiyue Guo, Junjie Hu, Junfeng Chen, Fuqin Deng, Tin Lun Lam", "title": "Semantic Histogram Based Graph Matching for Real-Time Multi-Robot Global\n  Localization in Large Scale Environment", "comments": null, "journal-ref": null, "doi": "10.1109/LRA.2021.3058935", "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The core problem of visual multi-robot simultaneous localization and mapping\n(MR-SLAM) is how to efficiently and accurately perform multi-robot global\nlocalization (MR-GL). The difficulties are two-fold. The first is the\ndifficulty of global localization for significant viewpoint difference.\nAppearance-based localization methods tend to fail under large viewpoint\nchanges. Recently, semantic graphs have been utilized to overcome the viewpoint\nvariation problem. However, the methods are highly time-consuming, especially\nin large-scale environments. This leads to the second difficulty, which is how\nto perform real-time global localization. In this paper, we propose a semantic\nhistogram-based graph matching method that is robust to viewpoint variation and\ncan achieve real-time global localization. Based on that, we develop a system\nthat can accurately and efficiently perform MR-GL for both homogeneous and\nheterogeneous robots. The experimental results show that our approach is about\n30 times faster than Random Walk based semantic descriptors. Moreover, it\nachieves an accuracy of 95% for global localization, while the accuracy of the\nstate-of-the-art method is 85%.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 08:18:42 GMT"}, {"version": "v2", "created": "Wed, 24 Feb 2021 09:51:22 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Guo", "Xiyue", ""], ["Hu", "Junjie", ""], ["Chen", "Junfeng", ""], ["Deng", "Fuqin", ""], ["Lam", "Tin Lun", ""]]}, {"id": "2010.09298", "submitter": "Yixin Wang", "authors": "Yixin Wang, Yao Zhang, Jiang Tian, Cheng Zhong, Zhongchao Shi, Yang\n  Zhang, Zhiqiang He", "title": "Double-Uncertainty Weighted Method for Semi-supervised Learning", "comments": "accepted by MICCAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Though deep learning has achieved advanced performance recently, it remains a\nchallenging task in the field of medical imaging, as obtaining reliable labeled\ntraining data is time-consuming and expensive. In this paper, we propose a\ndouble-uncertainty weighted method for semi-supervised segmentation based on\nthe teacher-student model. The teacher model provides guidance for the student\nmodel by penalizing their inconsistent prediction on both labeled and unlabeled\ndata. We train the teacher model using Bayesian deep learning to obtain\ndouble-uncertainty, i.e. segmentation uncertainty and feature uncertainty. It\nis the first to extend segmentation uncertainty estimation to feature\nuncertainty, which reveals the capability to capture information among\nchannels. A learnable uncertainty consistency loss is designed for the\nunsupervised learning process in an interactive manner between prediction and\nuncertainty. With no ground-truth for supervision, it can still incentivize\nmore accurate teacher's predictions and facilitate the model to reduce\nuncertain estimations. Furthermore, our proposed double-uncertainty serves as a\nweight on each inconsistency penalty to balance and harmonize supervised and\nunsupervised training processes. We validate the proposed feature uncertainty\nand loss function through qualitative and quantitative analyses. Experimental\nresults show that our method outperforms the state-of-the-art uncertainty-based\nsemi-supervised methods on two public medical datasets.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 08:20:18 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Wang", "Yixin", ""], ["Zhang", "Yao", ""], ["Tian", "Jiang", ""], ["Zhong", "Cheng", ""], ["Shi", "Zhongchao", ""], ["Zhang", "Yang", ""], ["He", "Zhiqiang", ""]]}, {"id": "2010.09304", "submitter": "Yicong Hong", "authors": "Yicong Hong, Cristian Rodriguez-Opazo, Yuankai Qi, Qi Wu, Stephen\n  Gould", "title": "Language and Visual Entity Relationship Graph for Agent Navigation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vision-and-Language Navigation (VLN) requires an agent to navigate in a\nreal-world environment following natural language instructions. From both the\ntextual and visual perspectives, we find that the relationships among the\nscene, its objects,and directional clues are essential for the agent to\ninterpret complex instructions and correctly perceive the environment. To\ncapture and utilize the relationships, we propose a novel Language and Visual\nEntity Relationship Graph for modelling the inter-modal relationships between\ntext and vision, and the intra-modal relationships among visual entities. We\npropose a message passing algorithm for propagating information between\nlanguage elements and visual entities in the graph, which we then combine to\ndetermine the next action to take. Experiments show that by taking advantage of\nthe relationships we are able to improve over state-of-the-art. On the\nRoom-to-Room (R2R) benchmark, our method achieves the new best performance on\nthe test unseen split with success rate weighted by path length (SPL) of 52%.\nOn the Room-for-Room (R4R) dataset, our method significantly improves the\nprevious best from 13% to 34% on the success weighted by normalized dynamic\ntime warping (SDTW). Code is available at:\nhttps://github.com/YicongHong/Entity-Graph-VLN.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 08:25:55 GMT"}, {"version": "v2", "created": "Fri, 25 Dec 2020 02:43:43 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Hong", "Yicong", ""], ["Rodriguez-Opazo", "Cristian", ""], ["Qi", "Yuankai", ""], ["Wu", "Qi", ""], ["Gould", "Stephen", ""]]}, {"id": "2010.09316", "submitter": "Junjie Hu", "authors": "Junjie Hu, Xiyue Guo, Junfeng Chen, Guanqi Liang, Fuqin Deng and Tin\n  lun Lam", "title": "A Two-stage Unsupervised Approach for Low light Image Enhancement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As vision based perception methods are usually built on the normal light\nassumption, there will be a serious safety issue when deploying them into low\nlight environments. Recently, deep learning based methods have been proposed to\nenhance low light images by penalizing the pixel-wise loss of low light and\nnormal light images. However, most of them suffer from the following problems:\n1) the need of pairs of low light and normal light images for training, 2) the\npoor performance for dark images, 3) the amplification of noise. To alleviate\nthese problems, in this paper, we propose a two-stage unsupervised method that\ndecomposes the low light image enhancement into a pre-enhancement and a\npost-refinement problem. In the first stage, we pre-enhance a low light image\nwith a conventional Retinex based method. In the second stage, we use a\nrefinement network learned with adversarial training for further improvement of\nthe image quality. The experimental results show that our method outperforms\nprevious methods on four benchmark datasets. In addition, we show that our\nmethod can significantly improve feature points matching and simultaneous\nlocalization and mapping in low light conditions.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 08:51:32 GMT"}, {"version": "v2", "created": "Tue, 20 Oct 2020 01:39:00 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Hu", "Junjie", ""], ["Guo", "Xiyue", ""], ["Chen", "Junfeng", ""], ["Liang", "Guanqi", ""], ["Deng", "Fuqin", ""], ["Lam", "Tin lun", ""]]}, {"id": "2010.09334", "submitter": "Marco De Nadai", "authors": "Pierfrancesco Ardino, Yahui Liu, Elisa Ricci, Bruno Lepri and Marco De\n  Nadai", "title": "Semantic-Guided Inpainting Network for Complex Urban Scenes Manipulation", "comments": "To appear in the Proceedings of IEEE ICPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Manipulating images of complex scenes to reconstruct, insert and/or remove\nspecific object instances is a challenging task. Complex scenes contain\nmultiple semantics and objects, which are frequently cluttered or ambiguous,\nthus hampering the performance of inpainting models. Conventional techniques\noften rely on structural information such as object contours in multi-stage\napproaches that generate unreliable results and boundaries. In this work, we\npropose a novel deep learning model to alter a complex urban scene by removing\na user-specified portion of the image and coherently inserting a new object\n(e.g. car or pedestrian) in that scene. Inspired by recent works on image\ninpainting, our proposed method leverages the semantic segmentation to model\nthe content and structure of the image, and learn the best shape and location\nof the object to insert. To generate reliable results, we design a new decoder\nblock that combines the semantic segmentation and generation task to guide\nbetter the generation of new objects and scenes, which have to be semantically\nconsistent with the image. Our experiments, conducted on two large-scale\ndatasets of urban scenes (Cityscapes and Indian Driving), show that our\nproposed approach successfully address the problem of semantically-guided\ninpainting of complex urban scene.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 09:17:17 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Ardino", "Pierfrancesco", ""], ["Liu", "Yahui", ""], ["Ricci", "Elisa", ""], ["Lepri", "Bruno", ""], ["De Nadai", "Marco", ""]]}, {"id": "2010.09342", "submitter": "Jiateng Liu", "authors": "Jiateng Liu, Wenming Zheng, Yuan Zong", "title": "SMA-STN: Segmented Movement-Attending Spatiotemporal Network\n  forMicro-Expression Recognition", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Correctly perceiving micro-expression is difficult since micro-expression is\nan involuntary, repressed, and subtle facial expression, and efficiently\nrevealing the subtle movement changes and capturing the significant segments in\na micro-expression sequence is the key to micro-expression recognition (MER).\nTo handle the crucial issue, in this paper, we firstly propose a dynamic\nsegmented sparse imaging module (DSSI) to compute dynamic images as\nlocal-global spatiotemporal descriptors under a unique sampling protocol, which\nreveals the subtle movement changes visually in an efficient way. Secondly, a\nsegmented movement-attending spatiotemporal network (SMA-STN) is proposed to\nfurther unveil imperceptible small movement changes, which utilizes a\nspatiotemporal movement-attending module (STMA) to capture long-distance\nspatial relation for facial expression and weigh temporal segments. Besides, a\ndeviation enhancement loss (DE-Loss) is embedded in the SMA-STN to enhance the\nrobustness of SMA-STN to subtle movement changes in feature level. Extensive\nexperiments on three widely used benchmarks, i.e., CASME II, SAMM, and SHIC,\nshow that the proposed SMA-STN achieves better MER performance than other\nstate-of-the-art methods, which proves that the proposed method is effective to\nhandle the challenging MER problem.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 09:23:24 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Liu", "Jiateng", ""], ["Zheng", "Wenming", ""], ["Zong", "Yuan", ""]]}, {"id": "2010.09343", "submitter": "Yan Xu", "authors": "Yan Xu, Zhaoyang Huang, Kwan-Yee Lin, Xinge Zhu, Jianping Shi, Hujun\n  Bao, Guofeng Zhang, Hongsheng Li", "title": "SelfVoxeLO: Self-supervised LiDAR Odometry with Voxel-based Deep Neural\n  Networks", "comments": "Accepted to CoRL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent learning-based LiDAR odometry methods have demonstrated their\ncompetitiveness. However, most methods still face two substantial challenges:\n1) the 2D projection representation of LiDAR data cannot effectively encode 3D\nstructures from the point clouds; 2) the needs for a large amount of labeled\ndata for training limit the application scope of these methods. In this paper,\nwe propose a self-supervised LiDAR odometry method, dubbed SelfVoxeLO, to\ntackle these two difficulties. Specifically, we propose a 3D convolution\nnetwork to process the raw LiDAR data directly, which extracts features that\nbetter encode the 3D geometric patterns. To suit our network to self-supervised\nlearning, we design several novel loss functions that utilize the inherent\nproperties of LiDAR point clouds. Moreover, an uncertainty-aware mechanism is\nincorporated in the loss functions to alleviate the interference of moving\nobjects/noises. We evaluate our method's performances on two large-scale\ndatasets, i.e., KITTI and Apollo-SouthBay. Our method outperforms\nstate-of-the-art unsupervised methods by 27%/32% in terms of\ntranslational/rotational errors on the KITTI dataset and also performs well on\nthe Apollo-SouthBay dataset. By including more unlabelled training data, our\nmethod can further improve performance comparable to the supervised methods.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 09:23:39 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Xu", "Yan", ""], ["Huang", "Zhaoyang", ""], ["Lin", "Kwan-Yee", ""], ["Zhu", "Xinge", ""], ["Shi", "Jianping", ""], ["Bao", "Hujun", ""], ["Zhang", "Guofeng", ""], ["Li", "Hongsheng", ""]]}, {"id": "2010.09350", "submitter": "Yiluan Guo", "authors": "Yiluan Guo, Holger Caesar, Oscar Beijbom, Jonah Philion, Sanja Fidler", "title": "The efficacy of Neural Planning Metrics: A meta-analysis of PKL on\n  nuScenes", "comments": "IROS 2020 Workshop on Benchmarking Progress in Autonomous Driving", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A high-performing object detection system plays a crucial role in autonomous\ndriving (AD). The performance, typically evaluated in terms of mean Average\nPrecision, does not take into account orientation and distance of the actors in\nthe scene, which are important for the safe AD. It also ignores environmental\ncontext. Recently, Philion et al. proposed a neural planning metric (PKL),\nbased on the KL divergence of a planner's trajectory and the groundtruth route,\nto accommodate these requirements. In this paper, we use this neural planning\nmetric to score all submissions of the nuScenes detection challenge and analyze\nthe results. We find that while somewhat correlated with mAP, the PKL metric\nshows different behavior to increased traffic density, ego velocity, road\ncurvature and intersections. Finally, we propose ideas to extend the neural\nplanning metric.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 09:32:48 GMT"}, {"version": "v2", "created": "Sat, 24 Oct 2020 02:15:23 GMT"}, {"version": "v3", "created": "Tue, 13 Jul 2021 03:53:53 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Guo", "Yiluan", ""], ["Caesar", "Holger", ""], ["Beijbom", "Oscar", ""], ["Philion", "Jonah", ""], ["Fidler", "Sanja", ""]]}, {"id": "2010.09355", "submitter": "Georgios Albanis", "authors": "Honglin Yuan, Remco C. Veltkamp, Georgios Albanis, Nikolaos Zioulis,\n  Dimitrios Zarpalas, Petros Daras", "title": "SHREC 2020 track: 6D Object Pose Estimation", "comments": null, "journal-ref": "Eurographics Workshop on 3D Object Retrieval (2020)", "doi": "10.2312/3dor.20201164", "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  6D pose estimation is crucial for augmented reality, virtual reality, robotic\nmanipulation and visual navigation. However, the problem is challenging due to\nthe variety of objects in the real world. They have varying 3D shape and their\nappearances in captured images are affected by sensor noise, changing lighting\nconditions and occlusions between objects. Different pose estimation methods\nhave different strengths and weaknesses, depending on feature representations\nand scene contents. At the same time, existing 3D datasets that are used for\ndata-driven methods to estimate 6D poses have limited view angles and low\nresolution. To address these issues, we organize the Shape Retrieval Challenge\nbenchmark on 6D pose estimation and create a physically accurate simulator that\nis able to generate photo-realistic color-and-depth image pairs with\ncorresponding ground truth 6D poses. From captured color and depth images, we\nuse this simulator to generate a 3D dataset which has 400 photo-realistic\nsynthesized color-and-depth image pairs with various view angles for training,\nand another 100 captured and synthetic images for testing. Five research groups\nregister in this track and two of them submitted their results. Data-driven\nmethods are the current trend in 6D object pose estimation and our evaluation\nresults show that approaches which fully exploit the color and geometric\nfeatures are more robust for 6D pose estimation of reflective and texture-less\nobjects and occlusion. This benchmark and comparative evaluation results have\nthe potential to further enrich and boost the research of 6D object pose\nestimation and its applications.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 09:45:42 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Yuan", "Honglin", ""], ["Veltkamp", "Remco C.", ""], ["Albanis", "Georgios", ""], ["Zioulis", "Nikolaos", ""], ["Zarpalas", "Dimitrios", ""], ["Daras", "Petros", ""]]}, {"id": "2010.09361", "submitter": "Domonkos Varga", "authors": "Domonkos Varga", "title": "A combined full-reference image quality assessment approach based on\n  convolutional activation maps", "comments": null, "journal-ref": "Algorithms 2020", "doi": "10.3390/a13120313", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The goal of full-reference image quality assessment (FR-IQA) is to predict\nthe quality of an image as perceived by human observers with using its\npristine, reference counterpart. In this study, we explore a novel, combined\napproach which predicts the perceptual quality of a distorted image by\ncompiling a feature vector from convolutional activation maps. More\nspecifically, a reference-distorted image pair is run through a pretrained\nconvolutional neural network and the activation maps are compared with a\ntraditional image similarity metric. Subsequently, the resulted feature vector\nis mapped onto perceptual quality scores with the help of a trained support\nvector regressor. A detailed parameter study is also presented in which the\ndesign choices of the proposed method is reasoned. Furthermore, we study the\nrelationship between the amount of training images and the prediction\nperformance. Specifically, it is demonstrated that the proposed method can be\ntrained with few amount of data to reach high prediction performance. Our best\nproposal - ActMapFeat - is compared to the state-of-the-art on six publicly\navailable benchmark IQA databases, such as KADID-10k, TID2013, TID2008, MDID,\nCSIQ, and VCL-FER. Specifically, our method is able to significantly outperform\nthe state-of-the-art on these benchmark databases.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 10:00:29 GMT"}, {"version": "v2", "created": "Sat, 28 Nov 2020 18:20:56 GMT"}, {"version": "v3", "created": "Thu, 3 Dec 2020 05:01:40 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Varga", "Domonkos", ""]]}, {"id": "2010.09391", "submitter": "Tobias Fechter", "authors": "Tobias Fechter, Sonja Adebahr, Anca-Ligia Grosu and Dimos Baltas", "title": "Measuring breathing induced oesophageal motion and its dosimetric impact", "comments": "The paper got accepted for publication in Physica Medica", "journal-ref": null, "doi": "10.1016/j.ejmp.2021.06.007", "report-no": null, "categories": "physics.med-ph cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stereotactic body radiation therapy allows for a precise and accurate dose\ndelivery. Organ motion during treatment bears the risk of undetected high dose\nhealthy tissue exposure. An organ very susceptible to high dose is the\noesophagus. Its low contrast on CT and the oblong shape renders motion\nestimation difficult. We tackle this issue by modern algorithms to measure the\noesophageal motion voxel-wise and to estimate motion related dosimetric impact.\nOesophageal motion was measured using deformable image registration and 4DCT of\n11 internal and 5 public datasets. Current clinical practice of contouring the\norgan on 3DCT was compared to timely resolved 4DCT contours. The dosimetric\nimpact of the motion was estimated by analysing the trajectory of each voxel in\nthe 4D dose distribution. Finally an organ motion model was built, allowing for\neasier patient-wise comparisons. Motion analysis showed mean absolute maximal\nmotion amplitudes of 4.55 +/- 1.81 mm left-right, 5.29 +/- 2.67 mm\nanterior-posterior and 10.78 +/- 5.30 mm superior-inferior. Motion between the\ncohorts differed significantly. In around 50 % of the cases the dosimetric\npassing criteria was violated. Contours created on 3DCT did not cover 14 % of\nthe organ for 50 % of the respiratory cycle and the 3D contour is around 38 %\nsmaller than the union of all 4D contours. The motion model revealed that the\nmaximal motion is not limited to the lower part of the organ. Our results\nshowed motion amplitudes higher than most reported values in the literature and\nthat motion is very heterogeneous across patients. Therefore, individual motion\ninformation should be considered in contouring and planning.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 11:25:05 GMT"}, {"version": "v2", "created": "Fri, 22 Jan 2021 12:48:45 GMT"}, {"version": "v3", "created": "Mon, 21 Jun 2021 09:32:46 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Fechter", "Tobias", ""], ["Adebahr", "Sonja", ""], ["Grosu", "Anca-Ligia", ""], ["Baltas", "Dimos", ""]]}, {"id": "2010.09409", "submitter": "Juan Jos\\'e G\\'omez Rodr\\'iguez", "authors": "Juan J. G\\'omez Rodr\\'iguez, Jos\\'e Lamarca, Javier Morlana, Juan D.\n  Tard\\'os, Jos\\'e M. M. Montiel", "title": "SD-DefSLAM: Semi-Direct Monocular SLAM for Deformable and Intracorporeal\n  Scenes", "comments": "10 pages, 8 figures. Submitted to RA-L with option to ICRA 2021.\n  Associated video: https://youtu.be/gkcC0IR3X6A", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional SLAM techniques strongly rely on scene rigidity to solve data\nassociation, ignoring dynamic parts of the scene. In this work we present\nSemi-Direct DefSLAM (SD-DefSLAM), a novel monocular deformable SLAM method able\nto map highly deforming environments, built on top of DefSLAM. To robustly\nsolve data association in challenging deforming scenes, SD-DefSLAM combines\ndirect and indirect methods: an enhanced illumination-invariant Lucas-Kanade\ntracker for data association, geometric Bundle Adjustment for pose and\ndeformable map estimation, and bag-of-words based on feature descriptors for\ncamera relocation. Dynamic objects are detected and segmented-out using a CNN\ntrained for the specific application domain. We thoroughly evaluate our system\nin two public datasets. The mandala dataset is a SLAM benchmark with\nincreasingly aggressive deformations. The Hamlyn dataset contains\nintracorporeal sequences that pose serious real-life challenges beyond\ndeformation like weak texture, specular reflections, surgical tools and\nocclusions. Our results show that SD-DefSLAM outperforms DefSLAM in point\ntracking, reconstruction accuracy and scale drift thanks to the improvement in\nall the data association steps, being the first system able to robustly perform\nSLAM inside the human body.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 12:07:07 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Rodr\u00edguez", "Juan J. G\u00f3mez", ""], ["Lamarca", "Jos\u00e9", ""], ["Morlana", "Javier", ""], ["Tard\u00f3s", "Juan D.", ""], ["Montiel", "Jos\u00e9 M. M.", ""]]}, {"id": "2010.09413", "submitter": "Du\\v{s}an Vari\\v{s}", "authors": "Du\\v{s}an Vari\\v{s}, Katsuhito Sudoh, and Satoshi Nakamura", "title": "Image Captioning with Visual Object Representations Grounded in the\n  Textual Modality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present our work in progress exploring the possibilities of a shared\nembedding space between textual and visual modality. Leveraging the textual\nnature of object detection labels and the hypothetical expressiveness of\nextracted visual object representations, we propose an approach opposite to the\ncurrent trend, grounding of the representations in the word embedding space of\nthe captioning system instead of grounding words or sentences in their\nassociated images. Based on the previous work, we apply additional grounding\nlosses to the image captioning training objective aiming to force visual object\nrepresentations to create more heterogeneous clusters based on their class\nlabel and copy a semantic structure of the word embedding space. In addition,\nwe provide an analysis of the learned object vector space projection and its\nimpact on the IC system performance. With only slight change in performance,\ngrounded models reach the stopping criterion during training faster than the\nunconstrained model, needing about two to three times less training updates.\nAdditionally, an improvement in structural correlation between the word\nembeddings and both original and projected object vectors suggests that the\ngrounding is actually mutual.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 12:21:38 GMT"}, {"version": "v2", "created": "Tue, 20 Oct 2020 12:24:39 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Vari\u0161", "Du\u0161an", ""], ["Sudoh", "Katsuhito", ""], ["Nakamura", "Satoshi", ""]]}, {"id": "2010.09414", "submitter": "Domonkos Varga", "authors": "Domonkos Varga", "title": "Comprehensive evaluation of no-reference image quality assessment\n  algorithms on KADID-10k database", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The main goal of objective image quality assessment is to devise\ncomputational, mathematical models which are able to predict perceptual image\nquality consistently with subjective evaluations. The evaluation of objective\nimage quality assessment algorithms is based on experiments conducted on\npublicly available benchmark databases. In this study, our goal is to give a\ncomprehensive evaluation about no-reference image quality assessment\nalgorithms, whose original source codes are available online, using the\nrecently published KADID-10k database which is one of the largest available\nbenchmark databases. Specifically, average PLCC, SROCC, and KROCC are reported\nwhich were measured over 100 random train-test splits. Furthermore, the\ndatabase was divided into a train (appx. 80\\% of images) and a test set (appx.\n20% of images) with respect to the reference images. So no semantic content\noverlap was between these two sets. Our evaluation results may be helpful to\nobtain a clear understanding about the status of state-of-the-art no-reference\nimage quality assessment methods.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 12:23:06 GMT"}, {"version": "v2", "created": "Mon, 9 Nov 2020 10:07:54 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Varga", "Domonkos", ""]]}, {"id": "2010.09425", "submitter": "Syed Waqas Zamir", "authors": "Nasir Hayat, Munawar Hayat, Shafin Rahman, Salman Khan, Syed Waqas\n  Zamir, Fahad Shahbaz Khan", "title": "Synthesizing the Unseen for Zero-shot Object Detection", "comments": "Accepted for publication at ACCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The existing zero-shot detection approaches project visual features to the\nsemantic domain for seen objects, hoping to map unseen objects to their\ncorresponding semantics during inference. However, since the unseen objects are\nnever visualized during training, the detection model is skewed towards seen\ncontent, thereby labeling unseen as background or a seen class. In this work,\nwe propose to synthesize visual features for unseen classes, so that the model\nlearns both seen and unseen objects in the visual domain. Consequently, the\nmajor challenge becomes, how to accurately synthesize unseen objects merely\nusing their class semantics? Towards this ambitious goal, we propose a novel\ngenerative model that uses class-semantics to not only generate the features\nbut also to discriminatively separate them. Further, using a unified model, we\nensure the synthesized features have high diversity that represents the\nintra-class differences and variable localization precision in the detected\nbounding boxes. We test our approach on three object detection benchmarks,\nPASCAL VOC, MSCOCO, and ILSVRC detection, under both conventional and\ngeneralized settings, showing impressive gains over the state-of-the-art\nmethods. Our codes are available at\nhttps://github.com/nasir6/zero_shot_detection.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 12:36:11 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Hayat", "Nasir", ""], ["Hayat", "Munawar", ""], ["Rahman", "Shafin", ""], ["Khan", "Salman", ""], ["Zamir", "Syed Waqas", ""], ["Khan", "Fahad Shahbaz", ""]]}, {"id": "2010.09456", "submitter": "Zhanwei Xu", "authors": "Zhanwei Xu, Yukun Cao, Cheng Jin, Guozhu Shao, Xiaoqing Liu, Jie Zhou,\n  Heshui Shi, Jianjiang Feng", "title": "GASNet: Weakly-supervised Framework for COVID-19 Lesion Segmentation", "comments": "10 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Segmentation of infected areas in chest CT volumes is of great significance\nfor further diagnosis and treatment of COVID-19 patients. Due to the complex\nshapes and varied appearances of lesions, a large number of voxel-level labeled\nsamples are generally required to train a lesion segmentation network, which is\na main bottleneck for developing deep learning based medical image segmentation\nalgorithms. In this paper, we propose a weakly-supervised lesion segmentation\nframework by embedding the Generative Adversarial training process into the\nSegmentation Network, which is called GASNet. GASNet is optimized to segment\nthe lesion areas of a COVID-19 CT by the segmenter, and to replace the abnormal\nappearance with a generated normal appearance by the generator, so that the\nrestored CT volumes are indistinguishable from healthy CT volumes by the\ndiscriminator. GASNet is supervised by chest CT volumes of many healthy and\nCOVID-19 subjects without voxel-level annotations. Experiments on three public\ndatabases show that when using as few as one voxel-level labeled sample, the\nperformance of GASNet is comparable to fully-supervised segmentation algorithms\ntrained on dozens of voxel-level labeled samples.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 13:06:23 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Xu", "Zhanwei", ""], ["Cao", "Yukun", ""], ["Jin", "Cheng", ""], ["Shao", "Guozhu", ""], ["Liu", "Xiaoqing", ""], ["Zhou", "Jie", ""], ["Shi", "Heshui", ""], ["Feng", "Jianjiang", ""]]}, {"id": "2010.09466", "submitter": "Liangzhi Li", "authors": "Bowen Wang, Liangzhi Li, Yuta Nakashima, Ryo Kawasaki, Hajime\n  Nagahara, Yasushi Yagi", "title": "Noisy-LSTM: Improving Temporal Awareness for Video Semantic Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic video segmentation is a key challenge for various applications. This\npaper presents a new model named Noisy-LSTM, which is trainable in an\nend-to-end manner, with convolutional LSTMs (ConvLSTMs) to leverage the\ntemporal coherency in video frames. We also present a simple yet effective\ntraining strategy, which replaces a frame in video sequence with noises. This\nstrategy spoils the temporal coherency in video frames during training and thus\nmakes the temporal links in ConvLSTMs unreliable, which may consequently\nimprove feature extraction from video frames, as well as serve as a regularizer\nto avoid overfitting, without requiring extra data annotation or computational\ncosts. Experimental results demonstrate that the proposed model can achieve\nstate-of-the-art performances in both the CityScapes and EndoVis2018 datasets.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 13:08:15 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Wang", "Bowen", ""], ["Li", "Liangzhi", ""], ["Nakashima", "Yuta", ""], ["Kawasaki", "Ryo", ""], ["Nagahara", "Hajime", ""], ["Yagi", "Yasushi", ""]]}, {"id": "2010.09498", "submitter": "Linhang Cai", "authors": "Linhang Cai, Zhulin An, Chuanguang Yang and Yongjun Xu", "title": "Softer Pruning, Incremental Regularization", "comments": "7 pages, ICPR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network pruning is widely used to compress Deep Neural Networks (DNNs). The\nSoft Filter Pruning (SFP) method zeroizes the pruned filters during training\nwhile updating them in the next training epoch. Thus the trained information of\nthe pruned filters is completely dropped. To utilize the trained pruned\nfilters, we proposed a SofteR Filter Pruning (SRFP) method and its variant,\nAsymptotic SofteR Filter Pruning (ASRFP), simply decaying the pruned weights\nwith a monotonic decreasing parameter. Our methods perform well across various\nnetworks, datasets and pruning rates, also transferable to weight pruning. On\nILSVRC-2012, ASRFP prunes 40% of the parameters on ResNet-34 with 1.63% top-1\nand 0.68% top-5 accuracy improvement. In theory, SRFP and ASRFP are an\nincremental regularization of the pruned filters. Besides, We note that SRFP\nand ASRFP pursue better results while slowing down the speed of convergence.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 13:37:19 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Cai", "Linhang", ""], ["An", "Zhulin", ""], ["Yang", "Chuanguang", ""], ["Xu", "Yongjun", ""]]}, {"id": "2010.09501", "submitter": "Xu Sun", "authors": "Xu Sun, Zhenfeng Fan, Zihao Zhang, Yingjie Guo, Shihong Xia", "title": "A Backbone Replaceable Fine-tuning Framework for Stable Face Alignment", "comments": "10 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Heatmap regression based face alignment has achieved prominent performance on\nstatic images. However, the stability and accuracy are remarkably discounted\nwhen applying the existing methods on dynamic videos. We attribute the\ndegradation to random noise and motion blur, which are common in videos. The\ntemporal information is critical to address this issue yet not fully considered\nin the existing works. In this paper, we visit the video-oriented face\nalignment problem in two perspectives: detection accuracy prefers lower error\nfor a single frame, and detection consistency forces better stability between\nadjacent frames. On this basis, we propose a Jitter loss function that\nleverages temporal information to suppress inaccurate as well as jittered\nlandmarks. The Jitter loss is involved in a novel framework with a fine-tuning\nConvLSTM structure over a backbone replaceable network. We further demonstrate\nthat accurate and stable landmarks are associated with different regions with\noverlaps in a canonical coordinate, based on which the proposed Jitter loss\nfacilitates the optimization process during training. The proposed framework\nachieves at least 40% improvement on stability evaluation metrics while\nenhancing detection accuracy versus state-of-the-art methods. Generally, it can\nswiftly convert a landmark detector for facial images to a better-performing\none for videos without retraining the entire model.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 13:40:39 GMT"}, {"version": "v2", "created": "Fri, 13 Nov 2020 02:33:01 GMT"}], "update_date": "2020-11-16", "authors_parsed": [["Sun", "Xu", ""], ["Fan", "Zhenfeng", ""], ["Zhang", "Zihao", ""], ["Guo", "Yingjie", ""], ["Xia", "Shihong", ""]]}, {"id": "2010.09522", "submitter": "Devamanyu Hazarika", "authors": "Shagun Uppal, Sarthak Bhagat, Devamanyu Hazarika, Navonil Majumdar,\n  Soujanya Poria, Roger Zimmermann, and Amir Zadeh", "title": "Multimodal Research in Vision and Language: A Review of Current and\n  Emerging Trends", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Deep Learning and its applications have cascaded impactful research and\ndevelopment with a diverse range of modalities present in the real-world data.\nMore recently, this has enhanced research interests in the intersection of the\nVision and Language arena with its numerous applications and fast-paced growth.\nIn this paper, we present a detailed overview of the latest trends in research\npertaining to visual and language modalities. We look at its applications in\ntheir task formulations and how to solve various problems related to semantic\nperception and content generation. We also address task-specific trends, along\nwith their evaluation strategies and upcoming challenges. Moreover, we shed\nsome light on multi-disciplinary patterns and insights that have emerged in the\nrecent past, directing this field towards more modular and transparent\nintelligent systems. This survey identifies key trends gravitating recent\nliterature in VisLang research and attempts to unearth directions that the\nfield is heading towards.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 13:55:10 GMT"}, {"version": "v2", "created": "Tue, 22 Dec 2020 04:43:20 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Uppal", "Shagun", ""], ["Bhagat", "Sarthak", ""], ["Hazarika", "Devamanyu", ""], ["Majumdar", "Navonil", ""], ["Poria", "Soujanya", ""], ["Zimmermann", "Roger", ""], ["Zadeh", "Amir", ""]]}, {"id": "2010.09524", "submitter": "Riqiang Gao", "authors": "Riqiang Gao, Yucheng Tang, Kaiwen Xu, Michael N. Kammer, Sanja L.\n  Antic, Steve Deppen, Kim L. Sandler, Pierre P. Massion, Yuankai Huo, Bennett\n  A. Landman", "title": "Deep Multi-path Network Integrating Incomplete Biomarker and Chest CT\n  Data for Evaluating Lung Cancer Risk", "comments": "RFW all-conference best paper finalist, SPIE2021 Medical Imaging", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clinical data elements (CDEs) (e.g., age, smoking history), blood markers and\nchest computed tomography (CT) structural features have been regarded as\neffective means for assessing lung cancer risk. These independent variables can\nprovide complementary information and we hypothesize that combining them will\nimprove the prediction accuracy. In practice, not all patients have all these\nvariables available. In this paper, we propose a new network design, termed as\nmulti-path multi-modal missing network (M3Net), to integrate the multi-modal\ndata (i.e., CDEs, biomarker and CT image) considering missing modality with\nmultiple paths neural network. Each path learns discriminative features of one\nmodality, and different modalities are fused in a second stage for an\nintegrated prediction. The network can be trained end-to-end with both medical\nimage features and CDEs/biomarkers, or make a prediction with single modality.\nWe evaluate M3Net with datasets including three sites from the Consortium for\nMolecular and Cellular Characterization of Screen-Detected Lesions (MCL)\nproject. Our method is cross validated within a cohort of 1291 subjects (383\nsubjects with complete CDEs/biomarkers and CT images), and externally validated\nwith a cohort of 99 subjects (99 with complete CDEs/biomarkers and CT images).\nBoth cross-validation and external-validation results show that combining\nmultiple modality significantly improves the predicting performance of single\nmodality. The results suggest that integrating subjects with missing either\nCDEs/biomarker or CT imaging features can contribute to the discriminatory\npower of our model (p < 0.05, bootstrap two-tailed test). In summary, the\nproposed M3Net framework provides an effective way to integrate image and\nnon-image data in the context of missing information.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 13:55:40 GMT"}, {"version": "v2", "created": "Wed, 10 Feb 2021 03:17:15 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Gao", "Riqiang", ""], ["Tang", "Yucheng", ""], ["Xu", "Kaiwen", ""], ["Kammer", "Michael N.", ""], ["Antic", "Sanja L.", ""], ["Deppen", "Steve", ""], ["Sandler", "Kim L.", ""], ["Massion", "Pierre P.", ""], ["Huo", "Yuankai", ""], ["Landman", "Bennett A.", ""]]}, {"id": "2010.09525", "submitter": "Hongxu Yang", "authors": "Hongxu Yang, Caifeng Shan, Alexander F. Kolen, Peter H. N. de With", "title": "Weakly-supervised Learning For Catheter Segmentation in 3D Frustum\n  Ultrasound", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Accurate and efficient catheter segmentation in 3D ultrasound (US) is\nessential for cardiac intervention. Currently, the state-of-the-art\nsegmentation algorithms are based on convolutional neural networks (CNNs),\nwhich achieved remarkable performances in a standard Cartesian volumetric data.\nNevertheless, these approaches suffer the challenges of low efficiency and GPU\nunfriendly image size. Therefore, such difficulties and expensive hardware\nrequirements become a bottleneck to build accurate and efficient segmentation\nmodels for real clinical application. In this paper, we propose a novel Frustum\nultrasound based catheter segmentation method. Specifically, Frustum ultrasound\nis a polar coordinate based image, which includes same information of standard\nCartesian image but has much smaller size, which overcomes the bottleneck of\nefficiency than conventional Cartesian images. Nevertheless, the irregular and\ndeformed Frustum images lead to more efforts for accurate voxel-level\nannotation. To address this limitation, a weakly supervised learning framework\nis proposed, which only needs 3D bounding box annotations overlaying the\nregion-of-interest to training the CNNs. Although the bounding box annotation\nincludes noise and inaccurate annotation to mislead to model, it is addressed\nby the proposed pseudo label generated scheme. The labels of training voxels\nare generated by incorporating class activation maps with line filtering, which\nis iteratively updated during the training. Our experimental results show the\nproposed method achieved the state-of-the-art performance with an efficiency of\n0.25 second per volume. More crucially, the Frustum image segmentation provides\na much faster and cheaper solution for segmentation in 3D US image, which meet\nthe demands of clinical applications.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 13:56:22 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Yang", "Hongxu", ""], ["Shan", "Caifeng", ""], ["Kolen", "Alexander F.", ""], ["de With", "Peter H. N.", ""]]}, {"id": "2010.09548", "submitter": "Zhe Ming Chng", "authors": "Zhe Ming Chng, Joseph Mun Hung Lew, Jimmy Addison Lee", "title": "RONELD: Robust Neural Network Output Enhancement for Active Lane\n  Detection", "comments": "Fixed typos; Accepted at ICPR 2020, 8 pages, 6 figures, code to be\n  published at http://github.com/czming/RONELD-Lane-Detection", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate lane detection is critical for navigation in autonomous vehicles,\nparticularly the active lane which demarcates the single road space that the\nvehicle is currently traveling on. Recent state-of-the-art lane detection\nalgorithms utilize convolutional neural networks (CNNs) to train deep learning\nmodels on popular benchmarks such as TuSimple and CULane. While each of these\nmodels works particularly well on train and test inputs obtained from the same\ndataset, the performance drops significantly on unseen datasets of different\nenvironments. In this paper, we present a real-time robust neural network\noutput enhancement for active lane detection (RONELD) method to identify,\ntrack, and optimize active lanes from deep learning probability map outputs. We\nfirst adaptively extract lane points from the probability map outputs, followed\nby detecting curved and straight lanes before using weighted least squares\nlinear regression on straight lanes to fix broken lane edges resulting from\nfragmentation of edge maps in real images. Lastly, we hypothesize true active\nlanes through tracking preceding frames. Experimental results demonstrate an up\nto two-fold increase in accuracy using RONELD on cross-dataset validation\ntests.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 14:22:47 GMT"}, {"version": "v2", "created": "Tue, 3 Nov 2020 02:16:21 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Chng", "Zhe Ming", ""], ["Lew", "Joseph Mun Hung", ""], ["Lee", "Jimmy Addison", ""]]}, {"id": "2010.09557", "submitter": "Milind Gajanan Padalkar", "authors": "Milind G. Padalkar, Carlos Beltr\\'an-Gonz\\'alez, Matteo Bustreo,\n  Alessio Del Bue and Vittorio Murino", "title": "A Versatile Crack Inspection Portable System based on Classifier\n  Ensemble and Controlled Illumination", "comments": "Accepted in ICPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel setup for automatic visual inspection of cracks\nin ceramic tile as well as studies the effect of various classifiers and\nheight-varying illumination conditions for this task. The intuition behind this\nsetup is that cracks can be better visualized under specific lighting\nconditions than others. Our setup, which is designed for field work with\nconstraints in its maximum dimensions, can acquire images for crack detection\nwith multiple lighting conditions using the illumination sources placed at\nmultiple heights. Crack detection is then performed by classifying patches\nextracted from the acquired images in a sliding window fashion. We study the\neffect of lights placed at various heights by training classifiers both on\ncustomized as well as state-of-the-art architectures and evaluate their\nperformance both at patch-level and image-level, demonstrating the\neffectiveness of our setup. More importantly, ours is the first study that\ndemonstrates how height-varying illumination conditions can affect crack\ndetection with the use of existing state-of-the-art classifiers. We provide an\ninsight about the illumination conditions that can help in improving crack\ndetection in a challenging real-world industrial environment.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 14:39:03 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Padalkar", "Milind G.", ""], ["Beltr\u00e1n-Gonz\u00e1lez", "Carlos", ""], ["Bustreo", "Matteo", ""], ["Del Bue", "Alessio", ""], ["Murino", "Vittorio", ""]]}, {"id": "2010.09561", "submitter": "Ci-Siang Lin", "authors": "Ci-Siang Lin, Yuan-Chia Cheng, Yu-Chiang Frank Wang", "title": "Domain Generalized Person Re-Identification via Cross-Domain Episodic\n  Learning", "comments": "Accepted to ICPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aiming at recognizing images of the same person across distinct camera views,\nperson re-identification (re-ID) has been among active research topics in\ncomputer vision. Most existing re-ID works require collection of a large amount\nof labeled image data from the scenes of interest. When the data to be\nrecognized are different from the source-domain training ones, a number of\ndomain adaptation approaches have been proposed. Nevertheless, one still needs\nto collect labeled or unlabelled target-domain data during training. In this\npaper, we tackle an even more challenging and practical setting, domain\ngeneralized (DG) person re-ID. That is, while a number of labeled source-domain\ndatasets are available, we do not have access to any target-domain training\ndata. In order to learn domain-invariant features without knowing the target\ndomain of interest, we present an episodic learning scheme which advances meta\nlearning strategies to exploit the observed source-domain labeled data. The\nlearned features would exhibit sufficient domain-invariant properties while not\noverfitting the source-domain data or ID labels. Our experiments on four\nbenchmark datasets confirm the superiority of our method over the\nstate-of-the-arts.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 14:42:29 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Lin", "Ci-Siang", ""], ["Cheng", "Yuan-Chia", ""], ["Wang", "Yu-Chiang Frank", ""]]}, {"id": "2010.09566", "submitter": "Jascha Kolberg", "authors": "Jascha Kolberg and Marta Gomez-Barrero and Christoph Busch", "title": "On the Generalisation Capabilities of Fingerprint Presentation Attack\n  Detection Methods in the Short Wave Infrared Domain", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, fingerprint-based biometric recognition systems are becoming\nincreasingly popular. However, in spite of their numerous advantages, biometric\ncapture devices are usually exposed to the public and thus vulnerable to\npresentation attacks (PAs). Therefore, presentation attack detection (PAD)\nmethods are of utmost importance in order to distinguish between bona fide and\nattack presentations. Due to the nearly unlimited possibilities to create new\npresentation attack instruments (PAIs), unknown attacks are a threat to\nexisting PAD algorithms. This fact motivates research on generalisation\ncapabilities in order to find PAD methods that are resilient to new attacks. In\nthis context, we evaluate the generalisability of multiple PAD algorithms on a\ndataset of 19,711 bona fide and 4,339 PA samples, including 45 different PAI\nspecies. The PAD data is captured in the short wave infrared domain and the\nresults discuss the advantages and drawbacks of this PAD technique regarding\nunknown attacks.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 14:50:24 GMT"}, {"version": "v2", "created": "Mon, 11 Jan 2021 16:45:36 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Kolberg", "Jascha", ""], ["Gomez-Barrero", "Marta", ""], ["Busch", "Christoph", ""]]}, {"id": "2010.09567", "submitter": "Vladimir Kolmogorov", "authors": "Vladimir Kolmogorov", "title": "Recursive Frank-Wolfe algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last decade there has been a resurgence of interest in Frank-Wolfe\n(FW) style methods for optimizing a smooth convex function over a polytope.\nExamples of recently developed techniques include {\\em Decomposition-invariant\nConditional Gradient} (DiCG), {\\em Blended Condition Gradient} (BCG), and {\\em\nFrank-Wolfe with in-face directions} (IF-FW) methods. We introduce two\nextensions of these techniques. First, we augment DiCG with the {\\em working\nset} strategy, and show how to optimize over the working set using {\\em shadow\nsimplex steps}. Second, we generalize in-face Frank-Wolfe directions to\npolytopes in which faces cannot be efficiently computed, and also describe a\ngeneric recursive procedure that can be used in conjunction with several\nFW-style techniques. Experimental results indicate that these extensions are\ncapable of speeding up original algorithms by orders of magnitude for certain\napplications.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 14:51:28 GMT"}, {"version": "v2", "created": "Fri, 27 Nov 2020 14:22:44 GMT"}, {"version": "v3", "created": "Tue, 19 Jan 2021 15:15:16 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Kolmogorov", "Vladimir", ""]]}, {"id": "2010.09572", "submitter": "Zhilei Liu", "authors": "Ruixin Xiao, Zhilei Liu, Baoyuan Wu", "title": "Teacher-Student Competition for Unsupervised Domain Adaptation", "comments": "Accepted by ICPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the supervision from source domain only in class-level, existing\nunsupervised domain adaptation (UDA) methods mainly learn the domain-invariant\nrepresentations from a shared feature extractor, which causes the source-bias\nproblem. This paper proposes an unsupervised domain adaptation approach with\nTeacher-Student Competition (TSC). In particular, a student network is\nintroduced to learn the target-specific feature space, and we design a novel\ncompetition mechanism to select more credible pseudo-labels for the training of\nstudent network. We introduce a teacher network with the structure of existing\nconventional UDA method, and both teacher and student networks compete to\nprovide target pseudo-labels to constrain every target sample's training in\nstudent network. Extensive experiments demonstrate that our proposed TSC\nframework significantly outperforms the state-of-the-art domain adaptation\nmethods on Office-31 and ImageCLEF-DA benchmarks.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 14:58:29 GMT"}, {"version": "v2", "created": "Tue, 20 Oct 2020 03:37:22 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Xiao", "Ruixin", ""], ["Liu", "Zhilei", ""], ["Wu", "Baoyuan", ""]]}, {"id": "2010.09582", "submitter": "Bo Yang", "authors": "Bo Yang", "title": "Learning to Reconstruct and Segment 3D Objects", "comments": "DPhil (PhD) Thesis 2020, University of Oxford\n  https://ora.ox.ac.uk/objects/uuid:5f9cd30d-0ee7-412d-ba49-44f5fd76bf28", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To endow machines with the ability to perceive the real-world in a three\ndimensional representation as we do as humans is a fundamental and\nlong-standing topic in Artificial Intelligence. Given different types of visual\ninputs such as images or point clouds acquired by 2D/3D sensors, one important\ngoal is to understand the geometric structure and semantics of the 3D\nenvironment. Traditional approaches usually leverage hand-crafted features to\nestimate the shape and semantics of objects or scenes. However, they are\ndifficult to generalize to novel objects and scenarios, and struggle to\novercome critical issues caused by visual occlusions. By contrast, we aim to\nunderstand scenes and the objects within them by learning general and robust\nrepresentations using deep neural networks, trained on large-scale real-world\n3D data. To achieve these aims, this thesis makes three core contributions from\nobject-level 3D shape estimation from single or multiple views to scene-level\nsemantic understanding.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 15:09:04 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Yang", "Bo", ""]]}, {"id": "2010.09586", "submitter": "Zicong Zhang", "authors": "Zicong Zhang, Kimerly Powell, Changchang Yin, Shilei Cao, Dani\n  Gonzalez, Yousef Hannawi, Ping Zhang", "title": "Brain Atlas Guided Attention U-Net for White Matter Hyperintensity\n  Segmentation", "comments": "Accepted by AMIA 2021 Virtual Informatics Summit", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  White Matter Hyperintensities (WMH) are the most common manifestation of\ncerebral small vessel disease (cSVD) on the brain MRI. Accurate WMH\nsegmentation algorithms are important to determine cSVD burden and its clinical\nconsequences. Most of existing WMH segmentation algorithms require both fluid\nattenuated inversion recovery (FLAIR) images and T1-weighted images as inputs.\nHowever, T1-weighted images are typically not part of standard clinicalscans\nwhich are acquired for patients with acute stroke. In this paper, we propose a\nnovel brain atlas guided attention U-Net (BAGAU-Net) that leverages only FLAIR\nimages with a spatially-registered white matter (WM) brain atlas to yield\ncompetitive WMH segmentation performance. Specifically, we designed a dual-path\nsegmentation model with two novel connecting mechanisms, namely multi-input\nattention module (MAM) and attention fusion module (AFM) to fuse the\ninformation from two paths for accurate results. Experiments on two publicly\navailable datasets show the effectiveness of the proposed BAGAU-Net. With only\nFLAIR images and WM brain atlas, BAGAU-Net outperforms the state-of-the-art\nmethod with T1-weighted images, paving the way for effective development of WMH\nsegmentation. Availability:https://github.com/Ericzhang1/BAGAU-Net\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 15:10:50 GMT"}, {"version": "v2", "created": "Tue, 22 Dec 2020 01:44:10 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Zhang", "Zicong", ""], ["Powell", "Kimerly", ""], ["Yin", "Changchang", ""], ["Cao", "Shilei", ""], ["Gonzalez", "Dani", ""], ["Hannawi", "Yousef", ""], ["Zhang", "Ping", ""]]}, {"id": "2010.09593", "submitter": "Behrouz Rostami", "authors": "Behrouz Rostami, D.M. Anisuzzaman, Chuanbo Wang, Sandeep\n  Gopalakrishnan, Jeffrey Niezgoda, Zeyun Yu", "title": "Multiclass Wound Image Classification using an Ensemble Deep CNN-based\n  Classifier", "comments": null, "journal-ref": "Computers in Biology and Medicine (2021)", "doi": "10.1016/j.compbiomed.2021.104536", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Acute and chronic wounds are a challenge to healthcare systems around the\nworld and affect many people's lives annually. Wound classification is a key\nstep in wound diagnosis that would help clinicians to identify an optimal\ntreatment procedure. Hence, having a high-performance classifier assists the\nspecialists in the field to classify the wounds with less financial and time\ncosts. Different machine learning and deep learning-based wound classification\nmethods have been proposed in the literature. In this study, we have developed\nan ensemble Deep Convolutional Neural Network-based classifier to classify\nwound images including surgical, diabetic, and venous ulcers, into\nmulti-classes. The output classification scores of two classifiers (patch-wise\nand image-wise) are fed into a Multi-Layer Perceptron to provide a superior\nclassification performance. A 5-fold cross-validation approach is used to\nevaluate the proposed method. We obtained maximum and average classification\naccuracy values of 96.4% and 94.28% for binary and 91.9\\% and 87.7\\% for\n3-class classification problems. The results show that our proposed method can\nbe used effectively as a decision support system in classification of wound\nimages or other related clinical applications.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 15:20:12 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Rostami", "Behrouz", ""], ["Anisuzzaman", "D. M.", ""], ["Wang", "Chuanbo", ""], ["Gopalakrishnan", "Sandeep", ""], ["Niezgoda", "Jeffrey", ""], ["Yu", "Zeyun", ""]]}, {"id": "2010.09594", "submitter": "Sarvesh Patil", "authors": "Sarvesh Patil, Chava Y P D Phani Rajanish, and Naveen Margankunte", "title": "Multi-Modal Super Resolution for Dense Microscopic Particle Size\n  Estimation", "comments": "11 pages, 10 figures, 6 tables, submitted to IEEE-TPAMI", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Particle Size Analysis (PSA) is an important process carried out in a number\nof industries, which can significantly influence the properties of the final\nproduct. A ubiquitous instrument for this purpose is the Optical Microscope\n(OM). However, OMs are often prone to drawbacks like low resolution, small\nfocal depth, and edge features being masked due to diffraction. We propose a\npowerful application of a combination of two Conditional Generative Adversarial\nNetworks (cGANs) that Super Resolve OM images to look like Scanning Electron\nMicroscope (SEM) images. We further demonstrate the use of a custom object\ndetection module that can perform efficient PSA of the super-resolved particles\non both, densely and sparsely packed images. The PSA results obtained from the\nsuper-resolved images have been benchmarked against human annotators, and\nresults obtained from the corresponding SEM images. The proposed models show a\ngeneralizable way of multi-modal image translation and super-resolution for\naccurate particle size estimation.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 15:20:40 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Patil", "Sarvesh", ""], ["Rajanish", "Chava Y P D Phani", ""], ["Margankunte", "Naveen", ""]]}, {"id": "2010.09624", "submitter": "Guillermo Ortiz-Jimenez", "authors": "Guillermo Ortiz-Jimenez, Apostolos Modas, Seyed-Mohsen\n  Moosavi-Dezfooli, Pascal Frossard", "title": "Optimism in the Face of Adversity: Understanding and Improving Deep\n  Learning through Adversarial Robustness", "comments": "24 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Driven by massive amounts of data and important advances in computational\nresources, new deep learning systems have achieved outstanding results in a\nlarge spectrum of applications. Nevertheless, our current theoretical\nunderstanding on the mathematical foundations of deep learning lags far behind\nits empirical success. Towards solving the vulnerability of neural networks,\nhowever, the field of adversarial robustness has recently become one of the\nmain sources of explanations of our deep models. In this article, we provide an\nin-depth review of the field of adversarial robustness in deep learning, and\ngive a self-contained introduction to its main notions. But, in contrast to the\nmainstream pessimistic perspective of adversarial robustness, we focus on the\nmain positive aspects that it entails. We highlight the intuitive connection\nbetween adversarial examples and the geometry of deep neural networks, and\neventually explore how the geometric study of adversarial examples can serve as\na powerful tool to understand deep learning. Furthermore, we demonstrate the\nbroad applicability of adversarial robustness, providing an overview of the\nmain emerging applications of adversarial robustness beyond security. The goal\nof this article is to provide readers with a set of new perspectives to\nunderstand deep learning, and to supply them with intuitive tools and insights\non how to use adversarial robustness to improve it.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 16:03:46 GMT"}, {"version": "v2", "created": "Thu, 28 Jan 2021 17:47:48 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Ortiz-Jimenez", "Guillermo", ""], ["Modas", "Apostolos", ""], ["Moosavi-Dezfooli", "Seyed-Mohsen", ""], ["Frossard", "Pascal", ""]]}, {"id": "2010.09648", "submitter": "Fan Zuo", "authors": "Ding Wang, Fan Zuo, Jingqin Gao, Yueshuai He, Zilin Bian, Suzana Duran\n  Bernardes, Chaekuk Na, Jingxing Wang, John Petinos, Kaan Ozbay, Joseph Y.J.\n  Chow, Shri Iyer, Hani Nassif, Xuegang Jeff Ban", "title": "Agent-based Simulation Model and Deep Learning Techniques to Evaluate\n  and Predict Transportation Trends around COVID-19", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.CV eess.IV physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The COVID-19 pandemic has affected travel behaviors and transportation system\noperations, and cities are grappling with what policies can be effective for a\nphased reopening shaped by social distancing. This edition of the white paper\nupdates travel trends and highlights an agent-based simulation model's results\nto predict the impact of proposed phased reopening strategies. It also\nintroduces a real-time video processing method to measure social distancing\nthrough cameras on city streets.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2020 05:37:15 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Wang", "Ding", ""], ["Zuo", "Fan", ""], ["Gao", "Jingqin", ""], ["He", "Yueshuai", ""], ["Bian", "Zilin", ""], ["Bernardes", "Suzana Duran", ""], ["Na", "Chaekuk", ""], ["Wang", "Jingxing", ""], ["Petinos", "John", ""], ["Ozbay", "Kaan", ""], ["Chow", "Joseph Y. J.", ""], ["Iyer", "Shri", ""], ["Nassif", "Hani", ""], ["Ban", "Xuegang Jeff", ""]]}, {"id": "2010.09662", "submitter": "Bernard Lange", "authors": "Bernard Lange, Masha Itkina and Mykel J. Kochenderfer", "title": "Attention Augmented ConvLSTM for Environment Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Safe and proactive planning in robotic systems generally requires accurate\npredictions of the environment. Prior work on environment prediction applied\nvideo frame prediction techniques to bird's-eye view environment\nrepresentations, such as occupancy grids. ConvLSTM-based frameworks used\npreviously often result in significant blurring and vanishing of moving\nobjects, thus hindering their applicability for use in safety-critical\napplications. In this work, we propose two extensions to the ConvLSTM to\naddress these issues. We present the Temporal Attention Augmented ConvLSTM\n(TAAConvLSTM) and Self-Attention Augmented ConvLSTM (SAAConvLSTM) frameworks\nfor spatiotemporal occupancy prediction, and demonstrate improved performance\nover baseline architectures on the real-world KITTI and Waymo datasets.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 16:57:24 GMT"}, {"version": "v2", "created": "Tue, 20 Oct 2020 01:10:47 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Lange", "Bernard", ""], ["Itkina", "Masha", ""], ["Kochenderfer", "Mykel J.", ""]]}, {"id": "2010.09670", "submitter": "Maksym Andriushchenko", "authors": "Francesco Croce, Maksym Andriushchenko, Vikash Sehwag, Edoardo\n  Debenedetti, Nicolas Flammarion, Mung Chiang, Prateek Mittal, Matthias Hein", "title": "RobustBench: a standardized adversarial robustness benchmark", "comments": "Version 2: 90+ evaluations, 60+ models, 5 leaderboards (Linf, L2,\n  common corruptions), significantly expanded analysis part (calibration,\n  fairness, privacy leakage, smoothness, transferability)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a research community, we are still lacking a systematic understanding of\nthe progress on adversarial robustness, which often makes it hard to identify\nthe most promising ideas in training robust models. A key challenge in\nbenchmarking robustness is that its evaluation is often error-prone, leading to\noverestimation of the true robustness of models. While adaptive attacks\ndesigned for a particular defense are a potential solution, they have to be\nhighly customized for particular models, which makes it difficult to compare\ndifferent methods. Our goal is to instead establish a standardized benchmark of\nadversarial robustness, which as accurately as possible reflects the robustness\nof the considered models within a reasonable computational budget. To evaluate\nthe robustness of models for our benchmark, we consider AutoAttack, an ensemble\nof white- and black-box attacks which was recently shown in a large-scale study\nto improve almost all robustness evaluations compared to the original\npublications. We also impose some restrictions on the admitted models to rule\nout defenses that only make gradient-based attacks ineffective without\nimproving actual robustness. Our leaderboard, hosted at\nhttps://robustbench.github.io/, contains evaluations of 90+ models and aims at\nreflecting the current state of the art on a set of well-defined tasks in\n$\\ell_\\infty$- and $\\ell_2$-threat models and on common corruptions, with\npossible extensions in the future. Additionally, we open-source the library\nhttps://github.com/RobustBench/robustbench that provides unified access to 60+\nrobust models to facilitate their downstream applications. Finally, based on\nthe collected models, we analyze the impact of robustness on the performance on\ndistribution shifts, calibration, out-of-distribution detection, fairness,\nprivacy leakage, smoothness, and transferability.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 17:06:18 GMT"}, {"version": "v2", "created": "Sat, 12 Jun 2021 13:50:59 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Croce", "Francesco", ""], ["Andriushchenko", "Maksym", ""], ["Sehwag", "Vikash", ""], ["Debenedetti", "Edoardo", ""], ["Flammarion", "Nicolas", ""], ["Chiang", "Mung", ""], ["Mittal", "Prateek", ""], ["Hein", "Matthias", ""]]}, {"id": "2010.09672", "submitter": "Ansh Khurana", "authors": "Soumajit Majumder, Ansh Khurana, Abhinav Rai, Angela Yao", "title": "Multi-Stage Fusion for One-Click Segmentation", "comments": "A preprint of the accepted paper at GCPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Segmenting objects of interest in an image is an essential building block of\napplications such as photo-editing and image analysis. Under interactive\nsettings, one should achieve good segmentations while minimizing user input.\nCurrent deep learning-based interactive segmentation approaches use early\nfusion and incorporate user cues at the image input layer. Since segmentation\nCNNs have many layers, early fusion may weaken the influence of user\ninteractions on the final prediction results. As such, we propose a new\nmulti-stage guidance framework for interactive segmentation. By incorporating\nuser cues at different stages of the network, we allow user interactions to\nimpact the final segmentation output in a more direct way. Our proposed\nframework has a negligible increase in parameter count compared to early-fusion\nframeworks. We perform extensive experimentation on the standard interactive\ninstance segmentation and one-click segmentation benchmarks and report\nstate-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 17:07:40 GMT"}, {"version": "v2", "created": "Tue, 20 Oct 2020 12:52:55 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Majumder", "Soumajit", ""], ["Khurana", "Ansh", ""], ["Rai", "Abhinav", ""], ["Yao", "Angela", ""]]}, {"id": "2010.09676", "submitter": "Supreeth Narasimhaswamy", "authors": "Supreeth Narasimhaswamy, Trung Nguyen, Minh Hoai", "title": "Detecting Hands and Recognizing Physical Contact in the Wild", "comments": "NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate a new problem of detecting hands and recognizing their\nphysical contact state in unconstrained conditions. This is a challenging\ninference task given the need to reason beyond the local appearance of hands.\nThe lack of training annotations indicating which object or parts of an object\nthe hand is in contact with further complicates the task. We propose a novel\nconvolutional network based on Mask-RCNN that can jointly learn to localize\nhands and predict their physical contact to address this problem. The network\nuses outputs from another object detector to obtain locations of objects\npresent in the scene. It uses these outputs and hand locations to recognize the\nhand's contact state using two attention mechanisms. The first attention\nmechanism is based on the hand and a region's affinity, enclosing the hand and\nthe object, and densely pools features from this region to the hand region. The\nsecond attention module adaptively selects salient features from this plausible\nregion of contact. To develop and evaluate our method's performance, we\nintroduce a large-scale dataset called ContactHands, containing unconstrained\nimages annotated with hand locations and contact states. The proposed network,\nincluding the parameters of attention modules, is end-to-end trainable. This\nnetwork achieves approximately 7\\% relative improvement over a baseline network\nthat was built on the vanilla Mask-RCNN architecture and trained for\nrecognizing hand contact states.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 17:11:41 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Narasimhaswamy", "Supreeth", ""], ["Nguyen", "Trung", ""], ["Hoai", "Minh", ""]]}, {"id": "2010.09680", "submitter": "Ehsan Nowroozi", "authors": "Ehsan Nowroozi, Ali Dehghantanha, Reza M. Parizi, Kim-Kwang Raymond\n  Choo", "title": "A Survey of Machine Learning Techniques in Adversarial Image Forensics", "comments": "37 pages, 24 figures, Accepted to the Journal Computer and Security\n  (Elsevier)", "journal-ref": "2020", "doi": null, "report-no": null, "categories": "cs.CR cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image forensic plays a crucial role in both criminal investigations (e.g.,\ndissemination of fake images to spread racial hate or false narratives about\nspecific ethnicity groups) and civil litigation (e.g., defamation).\nIncreasingly, machine learning approaches are also utilized in image forensics.\nHowever, there are also a number of limitations and vulnerabilities associated\nwith machine learning-based approaches, for example how to detect adversarial\n(image) examples, with real-world consequences (e.g., inadmissible evidence, or\nwrongful conviction). Therefore, with a focus on image forensics, this paper\nsurveys techniques that can be used to enhance the robustness of machine\nlearning-based binary manipulation detectors in various adversarial scenarios.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 17:16:38 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Nowroozi", "Ehsan", ""], ["Dehghantanha", "Ali", ""], ["Parizi", "Reza M.", ""], ["Choo", "Kim-Kwang Raymond", ""]]}, {"id": "2010.09689", "submitter": "Seyed Majid Azimi", "authors": "Seyed Majid Azimi, Maximilian Kraus, Reza Bahmanyar, Peter Reinartz", "title": "Multiple Pedestrians and Vehicles Tracking in Aerial Imagery: A\n  Comprehensive Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we address various challenges in multi-pedestrian and vehicle\ntracking in high-resolution aerial imagery by intensive evaluation of a number\nof traditional and Deep Learning based Single- and Multi-Object Tracking\nmethods. We also describe our proposed Deep Learning based Multi-Object\nTracking method AerialMPTNet that fuses appearance, temporal, and graphical\ninformation using a Siamese Neural Network, a Long Short-Term Memory, and a\nGraph Convolutional Neural Network module for a more accurate and stable\ntracking. Moreover, we investigate the influence of the Squeeze-and-Excitation\nlayers and Online Hard Example Mining on the performance of AerialMPTNet. To\nthe best of our knowledge, we are the first in using these two for a\nregression-based Multi-Object Tracking. Additionally, we studied and compared\nthe L1 and Huber loss functions. In our experiments, we extensively evaluate\nAerialMPTNet on three aerial Multi-Object Tracking datasets, namely AerialMPT\nand KIT AIS pedestrian and vehicle datasets. Qualitative and quantitative\nresults show that AerialMPTNet outperforms all previous methods for the\npedestrian datasets and achieves competitive results for the vehicle dataset.\nIn addition, Long Short-Term Memory and Graph Convolutional Neural Network\nmodules enhance the tracking performance. Moreover, using\nSqueeze-and-Excitation and Online Hard Example Mining significantly helps for\nsome cases while degrades the results for other cases. In addition, according\nto the results, L1 yields better results with respect to Huber loss for most of\nthe scenarios. The presented results provide a deep insight into challenges and\nopportunities of the aerial Multi-Object Tracking domain, paving the way for\nfuture research.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 17:26:09 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Azimi", "Seyed Majid", ""], ["Kraus", "Maximilian", ""], ["Bahmanyar", "Reza", ""], ["Reinartz", "Peter", ""]]}, {"id": "2010.09709", "submitter": "Tengda Han", "authors": "Tengda Han, Weidi Xie, Andrew Zisserman", "title": "Self-supervised Co-training for Video Representation Learning", "comments": "NeurIPS2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The objective of this paper is visual-only self-supervised video\nrepresentation learning. We make the following contributions: (i) we\ninvestigate the benefit of adding semantic-class positives to instance-based\nInfo Noise Contrastive Estimation (InfoNCE) training, showing that this form of\nsupervised contrastive learning leads to a clear improvement in performance;\n(ii) we propose a novel self-supervised co-training scheme to improve the\npopular infoNCE loss, exploiting the complementary information from different\nviews, RGB streams and optical flow, of the same data source by using one view\nto obtain positive class samples for the other; (iii) we thoroughly evaluate\nthe quality of the learnt representation on two different downstream tasks:\naction recognition and video retrieval. In both cases, the proposed approach\ndemonstrates state-of-the-art or comparable performance with other\nself-supervised approaches, whilst being significantly more efficient to train,\ni.e. requiring far less training data to achieve similar performance.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 17:59:01 GMT"}, {"version": "v2", "created": "Mon, 11 Jan 2021 20:53:18 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Han", "Tengda", ""], ["Xie", "Weidi", ""], ["Zisserman", "Andrew", ""]]}, {"id": "2010.09713", "submitter": "Yuliang Zou", "authors": "Yuliang Zou, Zizhao Zhang, Han Zhang, Chun-Liang Li, Xiao Bian,\n  Jia-Bin Huang, Tomas Pfister", "title": "PseudoSeg: Designing Pseudo Labels for Semantic Segmentation", "comments": "ICLR 2021. Project page: https://yuliang.vision/pseudo_seg/ Code:\n  https://github.com/googleinterns/wss", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in semi-supervised learning (SSL) demonstrate that a\ncombination of consistency regularization and pseudo-labeling can effectively\nimprove image classification accuracy in the low-data regime. Compared to\nclassification, semantic segmentation tasks require much more intensive\nlabeling costs. Thus, these tasks greatly benefit from data-efficient training\nmethods. However, structured outputs in segmentation render particular\ndifficulties (e.g., designing pseudo-labeling and augmentation) to apply\nexisting SSL strategies. To address this problem, we present a simple and novel\nre-design of pseudo-labeling to generate well-calibrated structured pseudo\nlabels for training with unlabeled or weakly-labeled data. Our proposed\npseudo-labeling strategy is network structure agnostic to apply in a one-stage\nconsistency training framework. We demonstrate the effectiveness of the\nproposed pseudo-labeling strategy in both low-data and high-data regimes.\nExtensive experiments have validated that pseudo labels generated from wisely\nfusing diverse sources and strong data augmentation are crucial to consistency\ntraining for segmentation. The source code is available at\nhttps://github.com/googleinterns/wss.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 17:59:30 GMT"}, {"version": "v2", "created": "Tue, 30 Mar 2021 17:54:47 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Zou", "Yuliang", ""], ["Zhang", "Zizhao", ""], ["Zhang", "Han", ""], ["Li", "Chun-Liang", ""], ["Bian", "Xiao", ""], ["Huang", "Jia-Bin", ""], ["Pfister", "Tomas", ""]]}, {"id": "2010.09714", "submitter": "Jonathan Barron", "authors": "Jonathan T. Barron", "title": "A Convenient Generalization of Schlick's Bias and Gain Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a generalization of Schlick's bias and gain functions -- simple\nparametric curve-shaped functions for inputs in [0, 1]. Our single function\nincludes both bias and gain as special cases, and is able to describe other\nsmooth and monotonic curves with variable degrees of asymmetry.\n", "versions": [{"version": "v1", "created": "Sat, 17 Oct 2020 03:25:55 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Barron", "Jonathan T.", ""]]}, {"id": "2010.09750", "submitter": "Jason Phang", "authors": "Jason Phang, Jungkyu Park and Krzysztof J. Geras", "title": "Investigating and Simplifying Masking-based Saliency Methods for Model\n  Interpretability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Saliency maps that identify the most informative regions of an image for a\nclassifier are valuable for model interpretability. A common approach to\ncreating saliency maps involves generating input masks that mask out portions\nof an image to maximally deteriorate classification performance, or mask in an\nimage to preserve classification performance. Many variants of this approach\nhave been proposed in the literature, such as counterfactual generation and\noptimizing over a Gumbel-Softmax distribution. Using a general formulation of\nmasking-based saliency methods, we conduct an extensive evaluation study of a\nnumber of recently proposed variants to understand which elements of these\nmethods meaningfully improve performance. Surprisingly, we find that a\nwell-tuned, relatively simple formulation of a masking-based saliency model\noutperforms many more complex approaches. We find that the most important\ningredients for high quality saliency map generation are (1) using both\nmasked-in and masked-out objectives and (2) training the classifier alongside\nthe masking model. Strikingly, we show that a masking model can be trained with\nas few as 10 examples per class and still generate saliency maps with only a\n0.7-point increase in localization error.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 18:00:36 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Phang", "Jason", ""], ["Park", "Jungkyu", ""], ["Geras", "Krzysztof J.", ""]]}, {"id": "2010.09774", "submitter": "Nitin Agarwal", "authors": "Nitin Agarwal and M Gopi", "title": "GAMesh: Guided and Augmented Meshing for Deep Point Networks", "comments": "Accepted to 3DV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CG cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new meshing algorithm called guided and augmented meshing,\nGAMesh, which uses a mesh prior to generate a surface for the output points of\na point network. By projecting the output points onto this prior and\nsimplifying the resulting mesh, GAMesh ensures a surface with the same topology\nas the mesh prior but whose geometric fidelity is controlled by the point\nnetwork. This makes GAMesh independent of both the density and distribution of\nthe output points, a common artifact in traditional surface reconstruction\nalgorithms. We show that such a separation of geometry from topology can have\nseveral advantages especially in single-view shape prediction, fair evaluation\nof point networks and reconstructing surfaces for networks which output sparse\npoint clouds. We further show that by training point networks with GAMesh, we\ncan directly optimize the vertex positions to generate adaptive meshes with\narbitrary topologies.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 18:23:53 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Agarwal", "Nitin", ""], ["Gopi", "M", ""]]}, {"id": "2010.09849", "submitter": "Siwei Zhang", "authors": "Siwei Zhang, Zhiwu Huang, Danda Pani Paudel, Luc Van Gool", "title": "Facial Emotion Recognition with Noisy Multi-task Annotations", "comments": "Accepted by 2021 WACV, camera-ready version with appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human emotions can be inferred from facial expressions. However, the\nannotations of facial expressions are often highly noisy in common emotion\ncoding models, including categorical and dimensional ones. To reduce human\nlabelling effort on multi-task labels, we introduce a new problem of facial\nemotion recognition with noisy multi-task annotations. For this new problem, we\nsuggest a formulation from the point of joint distribution match view, which\naims at learning more reliable correlations among raw facial images and\nmulti-task labels, resulting in the reduction of noise influence. In our\nformulation, we exploit a new method to enable the emotion prediction and the\njoint distribution learning in a unified adversarial learning game. Evaluation\nthroughout extensive experiments studies the real setups of the suggested new\nproblem, as well as the clear superiority of the proposed method over the\nstate-of-the-art competing methods on either the synthetic noisy labeled\nCIFAR-10 or practical noisy multi-task labeled RAF and AffectNet. The code is\navailable at https://github.com/sanweiliti/noisyFER.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 20:39:37 GMT"}, {"version": "v2", "created": "Tue, 24 Nov 2020 17:43:01 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Zhang", "Siwei", ""], ["Huang", "Zhiwu", ""], ["Paudel", "Danda Pani", ""], ["Van Gool", "Luc", ""]]}, {"id": "2010.09856", "submitter": "Behzad Bozorgtabar", "authors": "Behzad Bozorgtabar, Dwarikanath Mahapatra, Guillaume Vray,\n  Jean-Philippe Thiran", "title": "Anomaly Detection on X-Rays Using Self-Supervised Aggregation Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep anomaly detection models using a supervised mode of learning usually\nwork under a closed set assumption and suffer from overfitting to previously\nseen rare anomalies at training, which hinders their applicability in a real\nscenario. In addition, obtaining annotations for X-rays is very time consuming\nand requires extensive training of radiologists. Hence, training anomaly\ndetection in a fully unsupervised or self-supervised fashion would be\nadvantageous, allowing a significant reduction of time spent on the report by\nradiologists. In this paper, we present SALAD, an end-to-end deep\nself-supervised methodology for anomaly detection on X-Ray images. The proposed\nmethod is based on an optimization strategy in which a deep neural network is\nencouraged to represent prototypical local patterns of the normal data in the\nembedding space. During training, we record the prototypical patterns of normal\ntraining samples via a memory bank. Our anomaly score is then derived by\nmeasuring similarity to a weighted combination of normal prototypical patterns\nwithin a memory bank without using any anomalous patterns. We present extensive\nexperiments on the challenging NIH Chest X-rays and MURA dataset, which\nindicate that our algorithm improves state-of-the-art methods by a wide margin.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 20:49:34 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Bozorgtabar", "Behzad", ""], ["Mahapatra", "Dwarikanath", ""], ["Vray", "Guillaume", ""], ["Thiran", "Jean-Philippe", ""]]}, {"id": "2010.09865", "submitter": "Theodoros Tsiligkaridis", "authors": "Theodoros Tsiligkaridis", "title": "Failure Prediction by Confidence Estimation of Uncertainty-Aware\n  Dirichlet Networks", "comments": "preliminary version presented at ICML 2020 Workshop on Uncertainty\n  and Robustness in Deep Learning, submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reliably assessing model confidence in deep learning and predicting errors\nlikely to be made are key elements in providing safety for model deployment, in\nparticular for applications with dire consequences. In this paper, it is first\nshown that uncertainty-aware deep Dirichlet neural networks provide an improved\nseparation between the confidence of correct and incorrect predictions in the\ntrue class probability (TCP) metric. Second, as the true class is unknown at\ntest time, a new criterion is proposed for learning the true class probability\nby matching prediction confidence scores while taking imbalance and TCP\nconstraints into account for correct predictions and failures. Experimental\nresults show our method improves upon the maximum class probability (MCP)\nbaseline and predicted TCP for standard networks on several image\nclassification tasks with various network architectures.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 21:06:45 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Tsiligkaridis", "Theodoros", ""]]}, {"id": "2010.09870", "submitter": "Pengyu Chu", "authors": "Pengyu Chu, Zhaojian Li, Kyle Lammers, Renfu Lu, and Xiaoming Liu", "title": "DeepApple: Deep Learning-based Apple Detection using a Suppression Mask\n  R-CNN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robotic apple harvesting has received much research attention in the past few\nyears due to growing shortage and rising cost in labor. One key enabling\ntechnology towards automated harvesting is accurate and robust apple detection,\nwhich poses great challenges as a result of the complex orchard environment\nthat involves varying lighting conditions and foliage/branch occlusions. This\nletter reports on the development of a novel deep learning-based apple\ndetection framework named DeepApple. Specifically, we first collect a\ncomprehensive apple orchard dataset for 'Gala' and 'Blondee' apples, using a\ncolor camera, under different lighting conditions (sunny vs. overcast and front\nlighting vs. back lighting). We then develop a novel suppression Mask R-CNN for\napple detection, in which a suppression branch is added to the standard Mask\nR-CNN to suppress non-apple features generated by the original network.\nComprehensive evaluations are performed, which show that the developed\nsuppression Mask R-CNN network outperforms state-of-the-art models with a\nhigher F1-score of 0.905 and a detection time of 0.25 second per frame on a\nstandard desktop computer.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 21:07:46 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Chu", "Pengyu", ""], ["Li", "Zhaojian", ""], ["Lammers", "Kyle", ""], ["Lu", "Renfu", ""], ["Liu", "Xiaoming", ""]]}, {"id": "2010.09893", "submitter": "Nupur Kumari", "authors": "Parth Patel, Nupur Kumari, Mayank Singh, Balaji Krishnamurthy", "title": "LT-GAN: Self-Supervised GAN with Latent Transformation Detection", "comments": "Accepted at WACV2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Networks (GANs) coupled with self-supervised tasks\nhave shown promising results in unconditional and semi-supervised image\ngeneration. We propose a self-supervised approach (LT-GAN) to improve the\ngeneration quality and diversity of images by estimating the GAN-induced\ntransformation (i.e. transformation induced in the generated images by\nperturbing the latent space of generator). Specifically, given two pairs of\nimages where each pair comprises of a generated image and its transformed\nversion, the self-supervision task aims to identify whether the latent\ntransformation applied in the given pair is same to that of the other pair.\nHence, this auxiliary loss encourages the generator to produce images that are\ndistinguishable by the auxiliary network, which in turn promotes the synthesis\nof semantically consistent images with respect to latent transformations. We\nshow the efficacy of this pretext task by improving the image generation\nquality in terms of FID on state-of-the-art models for both conditional and\nunconditional settings on CIFAR-10, CelebA-HQ and ImageNet datasets. Moreover,\nwe empirically show that LT-GAN helps in improving controlled image editing for\nCelebA-HQ and ImageNet over baseline models. We experimentally demonstrate that\nour proposed LT self-supervision task can be effectively combined with other\nstate-of-the-art training techniques for added benefits. Consequently, we show\nthat our approach achieves the new state-of-the-art FID score of 9.8 on\nconditional CIFAR-10 image generation.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 22:09:45 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Patel", "Parth", ""], ["Kumari", "Nupur", ""], ["Singh", "Mayank", ""], ["Krishnamurthy", "Balaji", ""]]}, {"id": "2010.09907", "submitter": "Majid Harouni", "authors": "Majid Harouni and Hadi Yazdani Baghmaleki", "title": "Color Image Segmentation Metrics", "comments": "19 pages, 11 figures, 6 tables, 29 equations, book chapter, 2 authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  An automatic image segmentation procedure is an inevitable part of many image\nanalyses and computer vision which deeply affect the rest of the system;\ntherefore, a set of interactive segmentation evaluation methods can\nsubstantially simplify the system development process. This entry presents the\nstate of the art of quantitative evaluation metrics for color image\nsegmentation methods by performing an analytical and comparative review of the\nmeasures. The decision-making process in selecting a suitable evaluation metric\nis still very serious because each metric tends to favor a different\nsegmentation method for each benchmark dataset. Furthermore, a conceptual\ncomparison of these metrics is provided at a high level of abstraction and is\ndiscussed for understanding the quantitative changes in different image\nsegmentation results.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 22:47:32 GMT"}], "update_date": "2020-11-22", "authors_parsed": [["Harouni", "Majid", ""], ["Baghmaleki", "Hadi Yazdani", ""]]}, {"id": "2010.09908", "submitter": "Amit Moscovich", "authors": "Sharon Zhang, Amit Moscovich, Amit Singer", "title": "Product Manifold Learning", "comments": "10 pages, 4 figures", "journal-ref": "Proceedings of The 24th International Conference on Artificial\n  Intelligence and Statistics. 130 (2021) 3241-3249", "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider problems of dimensionality reduction and learning data\nrepresentations for continuous spaces with two or more independent degrees of\nfreedom. Such problems occur, for example, when observing shapes with several\ncomponents that move independently. Mathematically, if the parameter space of\neach continuous independent motion is a manifold, then their combination is\nknown as a product manifold. In this paper, we present a new paradigm for\nnon-linear independent component analysis called manifold factorization. Our\nfactorization algorithm is based on spectral graph methods for manifold\nlearning and the separability of the Laplacian operator on product spaces.\nRecovering the factors of a manifold yields meaningful lower-dimensional\nrepresentations and provides a new way to focus on particular aspects of the\ndata space while ignoring others. We demonstrate the potential use of our\nmethod for an important and challenging problem in structural biology: mapping\nthe motions of proteins and other large molecules using cryo-electron\nmicroscopy datasets.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 22:51:06 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Zhang", "Sharon", ""], ["Moscovich", "Amit", ""], ["Singer", "Amit", ""]]}, {"id": "2010.09925", "submitter": "Pingping Zhang Dr", "authors": "Yinjie Lei and Duo Peng and Pingping Zhang and Qiuhong Ke and Haifeng\n  Li", "title": "Hierarchical Paired Channel Fusion Network for Street Scene Change\n  Detection", "comments": "To appear in Transactions on Image Processing, including 13 pages, 13\n  figures, 9 tables", "journal-ref": null, "doi": "10.1109/TIP.2020.3031173", "report-no": null, "categories": "cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Street Scene Change Detection (SSCD) aims to locate the changed regions\nbetween a given street-view image pair captured at different times, which is an\nimportant yet challenging task in the computer vision community. The intuitive\nway to solve the SSCD task is to fuse the extracted image feature pairs, and\nthen directly measure the dissimilarity parts for producing a change map.\nTherefore, the key for the SSCD task is to design an effective feature fusion\nmethod that can improve the accuracy of the corresponding change maps. To this\nend, we present a novel Hierarchical Paired Channel Fusion Network (HPCFNet),\nwhich utilizes the adaptive fusion of paired feature channels. Specifically,\nthe features of a given image pair are jointly extracted by a Siamese\nConvolutional Neural Network (SCNN) and hierarchically combined by exploring\nthe fusion of channel pairs at multiple feature levels. In addition, based on\nthe observation that the distribution of scene changes is diverse, we further\npropose a Multi-Part Feature Learning (MPFL) strategy to detect diverse\nchanges. Based on the MPFL strategy, our framework achieves a novel approach to\nadapt to the scale and location diversities of the scene change regions.\nExtensive experiments on three public datasets (i.e., PCD, VL-CMU-CD and\nCDnet2014) demonstrate that the proposed framework achieves superior\nperformance which outperforms other state-of-the-art methods with a\nconsiderable margin.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 23:51:28 GMT"}], "update_date": "2020-12-30", "authors_parsed": [["Lei", "Yinjie", ""], ["Peng", "Duo", ""], ["Zhang", "Pingping", ""], ["Ke", "Qiuhong", ""], ["Li", "Haifeng", ""]]}, {"id": "2010.09953", "submitter": "Jeff Wang", "authors": "Qiong Xu, Jeff Wang, Hiroki Shirato, Lei Xing", "title": "Region-specific Dictionary Learning-based Low-dose Thoracic CT\n  Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.med-ph cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a dictionary learning-based method with region-specific\nimage patches to maximize the utility of the powerful sparse data processing\ntechnique for CT image reconstruction. Considering heterogeneous distributions\nof image features and noise in CT, region-specific customization of\ndictionaries is utilized in iterative reconstruction. Thoracic CT images are\npartitioned into several regions according to their structural and noise\ncharacteristics. Dictionaries specific to each region are then learned from the\nsegmented thoracic CT images and applied to subsequent image reconstruction of\nthe region. Parameters for dictionary learning and sparse representation are\ndetermined according to the structural and noise properties of each region. The\nproposed method results in better performance than the conventional\nreconstruction based on a single dictionary in recovering structures and\nsuppressing noise in both simulation and human CT imaging. Quantitatively, the\nsimulation study shows maximum improvement of image quality for the whole\nthorax can achieve 4.88% and 11.1% in terms of the Structure-SIMilarity (SSIM)\nand Root-Mean-Square Error (RMSE) indices, respectively. For human imaging\ndata, it is found that the structures in the lungs and heart can be better\nrecovered, while simultaneously decreasing noise around the vertebra\neffectively. The proposed strategy takes into account inherent regional\ndifferences inside of the reconstructed object and leads to improved images.\nThe method can be readily extended to CT imaging of other anatomical regions\nand other applications.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 01:41:45 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Xu", "Qiong", ""], ["Wang", "Jeff", ""], ["Shirato", "Hiroki", ""], ["Xing", "Lei", ""]]}, {"id": "2010.09978", "submitter": "Yi-Fan Song", "authors": "Yi-Fan Song, Zhang Zhang, Caifeng Shan and Liang Wang", "title": "Stronger, Faster and More Explainable: A Graph Convolutional Baseline\n  for Skeleton-based Action Recognition", "comments": "Accepted by ACM MultiMedia 2020, 9 pages, 4 figures, 5 tables", "journal-ref": null, "doi": "10.1145/3394171.3413802", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One essential problem in skeleton-based action recognition is how to extract\ndiscriminative features over all skeleton joints. However, the complexity of\nthe State-Of-The-Art (SOTA) models of this task tends to be exceedingly\nsophisticated and over-parameterized, where the low efficiency in model\ntraining and inference has obstructed the development in the field, especially\nfor large-scale action datasets. In this work, we propose an efficient but\nstrong baseline based on Graph Convolutional Network (GCN), where three main\nimprovements are aggregated, i.e., early fused Multiple Input Branches (MIB),\nResidual GCN (ResGCN) with bottleneck structure and Part-wise Attention\n(PartAtt) block. Firstly, an MIB is designed to enrich informative skeleton\nfeatures and remain compact representations at an early fusion stage. Then,\ninspired by the success of the ResNet architecture in Convolutional Neural\nNetwork (CNN), a ResGCN module is introduced in GCN to alleviate computational\ncosts and reduce learning difficulties in model training while maintain the\nmodel accuracy. Finally, a PartAtt block is proposed to discover the most\nessential body parts over a whole action sequence and obtain more explainable\nrepresentations for different skeleton action sequences. Extensive experiments\non two large-scale datasets, i.e., NTU RGB+D 60 and 120, validate that the\nproposed baseline slightly outperforms other SOTA models and meanwhile requires\nmuch fewer parameters during training and inference procedures, e.g., at most\n34 times less than DGNN, which is one of the best SOTA methods.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 02:56:58 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Song", "Yi-Fan", ""], ["Zhang", "Zhang", ""], ["Shan", "Caifeng", ""], ["Wang", "Liang", ""]]}, {"id": "2010.09982", "submitter": "Yuqian Fu", "authors": "Yuqian Fu, Li Zhang, Junke Wang, Yanwei Fu and Yu-Gang Jiang", "title": "Depth Guided Adaptive Meta-Fusion Network for Few-shot Video Recognition", "comments": "accepted by ACM Multimedia 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans can easily recognize actions with only a few examples given, while the\nexisting video recognition models still heavily rely on the large-scale labeled\ndata inputs. This observation has motivated an increasing interest in few-shot\nvideo action recognition, which aims at learning new actions with only very few\nlabeled samples. In this paper, we propose a depth guided Adaptive Meta-Fusion\nNetwork for few-shot video recognition which is termed as AMeFu-Net.\nConcretely, we tackle the few-shot recognition problem from three aspects:\nfirstly, we alleviate this extremely data-scarce problem by introducing depth\ninformation as a carrier of the scene, which will bring extra visual\ninformation to our model; secondly, we fuse the representation of original RGB\nclips with multiple non-strictly corresponding depth clips sampled by our\ntemporal asynchronization augmentation mechanism, which synthesizes new\ninstances at feature-level; thirdly, a novel Depth Guided Adaptive Instance\nNormalization (DGAdaIN) fusion module is proposed to fuse the two-stream\nmodalities efficiently. Additionally, to better mimic the few-shot recognition\nprocess, our model is trained in the meta-learning way. Extensive experiments\non several action recognition benchmarks demonstrate the effectiveness of our\nmodel.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 03:06:20 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Fu", "Yuqian", ""], ["Zhang", "Li", ""], ["Wang", "Junke", ""], ["Fu", "Yanwei", ""], ["Jiang", "Yu-Gang", ""]]}, {"id": "2010.09984", "submitter": "Charley Gros", "authors": "Charley Gros, Andreanne Lemay, Olivier Vincent, Lucas Rouhier, Anthime\n  Bucquet, Joseph Paul Cohen, Julien Cohen-Adad", "title": "ivadomed: A Medical Imaging Deep Learning Toolbox", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  ivadomed is an open-source Python package for designing, end-to-end training,\nand evaluating deep learning models applied to medical imaging data. The\npackage includes APIs, command-line tools, documentation, and tutorials.\nivadomed also includes pre-trained models such as spinal tumor segmentation and\nvertebral labeling. Original features of ivadomed include a data loader that\ncan parse image metadata (e.g., acquisition parameters, image contrast,\nresolution) and subject metadata (e.g., pathology, age, sex) for custom data\nsplitting or extra information during training and evaluation. Any dataset\nfollowing the Brain Imaging Data Structure (BIDS) convention will be compatible\nwith ivadomed without the need to manually organize the data, which is\ntypically a tedious task. Beyond the traditional deep learning methods,\nivadomed features cutting-edge architectures, such as FiLM and HeMis, as well\nas various uncertainty estimation methods (aleatoric and epistemic), and losses\nadapted to imbalanced classes and non-binary predictions. Each step is\nconveniently configurable via a single file. At the same time, the code is\nhighly modular to allow addition/modification of an architecture or\npre/post-processing steps. Example applications of ivadomed include MRI object\ndetection, segmentation, and labeling of anatomical and pathological\nstructures. Overall, ivadomed enables easy and quick exploration of the latest\nadvances in deep learning for medical imaging applications. ivadomed's main\nproject page is available at https://ivadomed.org.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 03:08:53 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Gros", "Charley", ""], ["Lemay", "Andreanne", ""], ["Vincent", "Olivier", ""], ["Rouhier", "Lucas", ""], ["Bucquet", "Anthime", ""], ["Cohen", "Joseph Paul", ""], ["Cohen-Adad", "Julien", ""]]}, {"id": "2010.09987", "submitter": "Zhen Xiang", "authors": "Zhen Xiang, David J. Miller, George Kesidis", "title": "L-RED: Efficient Post-Training Detection of Imperceptible Backdoor\n  Attacks without Access to the Training Set", "comments": null, "journal-ref": null, "doi": null, "report-no": "accepted by ICASSP 2021", "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Backdoor attacks (BAs) are an emerging form of adversarial attack typically\nagainst deep neural network image classifiers. The attacker aims to have the\nclassifier learn to classify to a target class when test images from one or\nmore source classes contain a backdoor pattern, while maintaining high accuracy\non all clean test images. Reverse-Engineering-based Defenses (REDs) against BAs\ndo not require access to the training set but only to an independent clean\ndataset. Unfortunately, most existing REDs rely on an unrealistic assumption\nthat all classes except the target class are source classes of the attack. REDs\nthat do not rely on this assumption often require a large set of clean images\nand heavy computation. In this paper, we propose a Lagrangian-based RED (L-RED)\nthat does not require knowledge of the number of source classes (or whether an\nattack is present). Our defense requires very few clean images to effectively\ndetect BAs and is computationally efficient. Notably, we detect 56 out of 60\nBAs using only two clean images per class in our experiments on CIFAR-10.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 03:17:20 GMT"}, {"version": "v2", "created": "Wed, 21 Oct 2020 23:32:01 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Xiang", "Zhen", ""], ["Miller", "David J.", ""], ["Kesidis", "George", ""]]}, {"id": "2010.09989", "submitter": "Amit Moscovich", "authors": "Rohan Rao, Amit Moscovich, Amit Singer", "title": "Wasserstein K-Means for Clustering Tomographic Projections", "comments": "11 pages, 5 figures, 1 table", "journal-ref": "Machine Learning for Structural Biology Workshop, NeurIPS 2020", "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the 2D class averaging problem in single-particle cryo-electron\nmicroscopy (cryo-EM), we present a k-means algorithm based on a\nrotationally-invariant Wasserstein metric for images. Unlike existing methods\nthat are based on Euclidean ($L_2$) distances, we prove that the Wasserstein\nmetric better accommodates for the out-of-plane angular differences between\ndifferent particle views. We demonstrate on a synthetic dataset that our method\ngives superior results compared to an $L_2$ baseline. Furthermore, there is\nlittle computational overhead, thanks to the use of a fast linear-time\napproximation to the Wasserstein-1 metric, also known as the Earthmover's\ndistance.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 03:28:17 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Rao", "Rohan", ""], ["Moscovich", "Amit", ""], ["Singer", "Amit", ""]]}, {"id": "2010.10000", "submitter": "Chien-Chuan Su", "authors": "Chien-Chuan Su, Ren Wang, Hung-Jin Lin, Yu-Lun Liu, Chia-Ping Chen,\n  Yu-Lin Chang and Soo-Chang Pei", "title": "Explorable Tone Mapping Operators", "comments": "To appear in ICPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tone-mapping plays an essential role in high dynamic range (HDR) imaging. It\naims to preserve visual information of HDR images in a medium with a limited\ndynamic range. Although many works have been proposed to provide tone-mapped\nresults from HDR images, most of them can only perform tone-mapping in a single\npre-designed way. However, the subjectivity of tone-mapping quality varies from\nperson to person, and the preference of tone-mapping style also differs from\napplication to application. In this paper, a learning-based multimodal\ntone-mapping method is proposed, which not only achieves excellent visual\nquality but also explores the style diversity. Based on the framework of\nBicycleGAN, the proposed method can provide a variety of expert-level\ntone-mapped results by manipulating different latent codes. Finally, we show\nthat the proposed method performs favorably against state-of-the-art\ntone-mapping algorithms both quantitatively and qualitatively.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 04:18:54 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Su", "Chien-Chuan", ""], ["Wang", "Ren", ""], ["Lin", "Hung-Jin", ""], ["Liu", "Yu-Lun", ""], ["Chen", "Chia-Ping", ""], ["Chang", "Yu-Lin", ""], ["Pei", "Soo-Chang", ""]]}, {"id": "2010.10001", "submitter": "Hai Wang", "authors": "Hai Wang, Wei-Shi Zheng, and Ling Yingbiao", "title": "Contextual Heterogeneous Graph Network for Human-Object Interaction\n  Detection", "comments": "Published on ECCV-2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human-object interaction(HOI) detection is an important task for\nunderstanding human activity. Graph structure is appropriate to denote the HOIs\nin the scene. Since there is an subordination between human and object---human\nplay subjective role and object play objective role in HOI, the relations\nbetween homogeneous entities and heterogeneous entities in the scene should\nalso not be equally the same. However, previous graph models regard human and\nobject as the same kind of nodes and do not consider that the messages are not\nequally the same between different entities. In this work, we address such a\nproblem for HOI task by proposing a heterogeneous graph network that models\nhumans and objects as different kinds of nodes and incorporates intra-class\nmessages between homogeneous nodes and inter-class messages between\nheterogeneous nodes. In addition, a graph attention mechanism based on the\nintra-class context and inter-class context is exploited to improve the\nlearning. Extensive experiments on the benchmark datasets V-COCO and HICO-DET\ndemonstrate that the intra-class and inter-class messages are very important in\nHOI detection and verify the effectiveness of our method.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 04:20:33 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Wang", "Hai", ""], ["Zheng", "Wei-Shi", ""], ["Yingbiao", "Ling", ""]]}, {"id": "2010.10004", "submitter": "Luis Leal", "authors": "Luis Leal, Marvin Castillo, Fernando Juarez, Erick Ramirez, Mildred\n  Aspuac, Diana Letona", "title": "Convolutional-LSTM for Multi-Image to Single Output Medical Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Medical head CT-scan imaging has been successfully combined with deep\nlearning for medical diagnostics of head diseases and lesions[1]. State of the\nart classification models and algorithms for this task usually are based on 3d\nconvolution layers for volumetric data on a supervised learning setting (1\ninput volume, 1 prediction per patient) or 2d convolution layers in a\nsupervised setting (1 input image, 1 prediction per image). However a very\ncommon scenario in developing countries is to have the volume metadata lost due\nmultiple reasons for example formatting conversion in images (for example\n.dicom to jpg), in this scenario the doctor analyses the collection of images\nand then emits a single diagnostic for the patient (with possibly an unfixed\nand variable number of images per patient) , this prevents it from being\npossible to use state of the art 3d models, but also is not possible to convert\nit to a supervised problem in a (1 image,1 diagnostic) setting because\ndifferent angles or positions of the images for a single patient may not\ncontain the disease or lesion. In this study we propose a solution for this\nscenario by combining 2d convolutional[2] models with sequence models which\ngenerate a prediction only after all images have been processed by the model\nfor a given patient \\(i\\), this creates a multi-image to single-diagnostic\nsetting \\(y^i=f(x_1,x_2,..,x_n)\\) where \\(n\\) may be different between\npatients. The experimental results demonstrate that it is possible to get a\nmulti-image to single diagnostic model which mimics human doctor diagnostic\nprocess: evaluate the collection of patient images and then use important\ninformation in memory to decide a single diagnostic for the patient.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 04:30:09 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Leal", "Luis", ""], ["Castillo", "Marvin", ""], ["Juarez", "Fernando", ""], ["Ramirez", "Erick", ""], ["Aspuac", "Mildred", ""], ["Letona", "Diana", ""]]}, {"id": "2010.10006", "submitter": "Long Chen", "authors": "Long Chen, Feixiang Zhou, Shengke Wang, Junyu Dong, Ning Li, Haiping\n  Ma, Xin Wang and Huiyu Zhou", "title": "SWIPENET: Object detection in noisy underwater images", "comments": "arXiv admin note: text overlap with arXiv:2005.11552", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, deep learning based object detection methods have achieved\npromising performance in controlled environments. However, these methods lack\nsufficient capabilities to handle underwater object detection due to these\nchallenges: (1) images in the underwater datasets and real applications are\nblurry whilst accompanying severe noise that confuses the detectors and (2)\nobjects in real applications are usually small. In this paper, we propose a\nnovel Sample-WeIghted hyPEr Network (SWIPENET), and a robust training paradigm\nnamed Curriculum Multi-Class Adaboost (CMA), to address these two problems at\nthe same time. Firstly, the backbone of SWIPENET produces multiple high\nresolution and semantic-rich Hyper Feature Maps, which significantly improve\nsmall object detection. Secondly, a novel sample-weighted detection loss\nfunction is designed for SWIPENET, which focuses on learning high weight\nsamples and ignore learning low weight samples. Moreover, inspired by the human\neducation process that drives the learning from easy to hard concepts, we here\npropose the CMA training paradigm that first trains a clean detector which is\nfree from the influence of noisy data. Then, based on the clean detector,\nmultiple detectors focusing on learning diverse noisy data are trained and\nincorporated into a unified deep ensemble of strong noise immunity. Experiments\non two underwater robot picking contest datasets (URPC2017 and URPC2018) show\nthat the proposed SWIPENET+CMA framework achieves better accuracy in object\ndetection against several state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 16:41:20 GMT"}, {"version": "v2", "created": "Mon, 14 Dec 2020 17:30:41 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Chen", "Long", ""], ["Zhou", "Feixiang", ""], ["Wang", "Shengke", ""], ["Dong", "Junyu", ""], ["Li", "Ning", ""], ["Ma", "Haiping", ""], ["Wang", "Xin", ""], ["Zhou", "Huiyu", ""]]}, {"id": "2010.10007", "submitter": "Li Yuan", "authors": "Li Yuan, Shuning Chang, Ziyuan Huang, Yichen Zhou, Yunpeng Chen,\n  Xuecheng Nie, Francis E.H. Tay, Jiashi Feng, Shuicheng Yan", "title": "A Simple Baseline for Pose Tracking in Videos of Crowded Scenes", "comments": "2nd Place in ACM Multimedia Grand Challenge: Human in Events, Track3:\n  Crowd Pose Tracking in Complex Events. ACM Multimedia 2020. arXiv admin note:\n  substantial text overlap with arXiv:2010.08365, arXiv:2010.10008", "journal-ref": null, "doi": "10.1145/3394171.3416300", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents our solution to ACM MM challenge: Large-scale\nHuman-centric Video Analysis in Complex Events\\cite{lin2020human};\nspecifically, here we focus on Track3: Crowd Pose Tracking in Complex Events.\nRemarkable progress has been made in multi-pose training in recent years.\nHowever, how to track the human pose in crowded and complex environments has\nnot been well addressed. We formulate the problem as several subproblems to be\nsolved. First, we use a multi-object tracking method to assign human ID to each\nbounding box generated by the detection model. After that, a pose is generated\nto each bounding box with ID. At last, optical flow is used to take advantage\nof the temporal information in the videos and generate the final pose tracking\nresult.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 13:06:21 GMT"}, {"version": "v2", "created": "Wed, 21 Oct 2020 03:37:18 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Yuan", "Li", ""], ["Chang", "Shuning", ""], ["Huang", "Ziyuan", ""], ["Zhou", "Yichen", ""], ["Chen", "Yunpeng", ""], ["Nie", "Xuecheng", ""], ["Tay", "Francis E. H.", ""], ["Feng", "Jiashi", ""], ["Yan", "Shuicheng", ""]]}, {"id": "2010.10008", "submitter": "Li Yuan", "authors": "Li Yuan, Shuning Chang, Xuecheng Nie, Ziyuan Huang, Yichen Zhou,\n  Yunpeng Chen, Jiashi Feng, Shuicheng Yan", "title": "Towards Accurate Human Pose Estimation in Videos of Crowded Scenes", "comments": "2nd Place in ACM Multimedia Grand Challenge: Human in Events, Track2:\n  Crowd Pose Estimation in Complex Events. ACM Multimedia 2020. arXiv admin\n  note: substantial text overlap with arXiv:2010.08365, arXiv:2010.10007", "journal-ref": null, "doi": "10.1145/3394171.3416299", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video-based human pose estimation in crowded scenes is a challenging problem\ndue to occlusion, motion blur, scale variation and viewpoint change, etc. Prior\napproaches always fail to deal with this problem because of (1) lacking of\nusage of temporal information; (2) lacking of training data in crowded scenes.\nIn this paper, we focus on improving human pose estimation in videos of crowded\nscenes from the perspectives of exploiting temporal context and collecting new\ndata. In particular, we first follow the top-down strategy to detect persons\nand perform single-person pose estimation for each frame. Then, we refine the\nframe-based pose estimation with temporal contexts deriving from the\noptical-flow. Specifically, for one frame, we forward the historical poses from\nthe previous frames and backward the future poses from the subsequent frames to\ncurrent frame, leading to stable and accurate human pose estimation in videos.\nIn addition, we mine new data of similar scenes to HIE dataset from the\nInternet for improving the diversity of training set. In this way, our model\nachieves best performance on 7 out of 13 videos and 56.33 average w\\_AP on test\ndataset of HIE challenge.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 13:19:11 GMT"}, {"version": "v2", "created": "Wed, 21 Oct 2020 03:37:40 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Yuan", "Li", ""], ["Chang", "Shuning", ""], ["Nie", "Xuecheng", ""], ["Huang", "Ziyuan", ""], ["Zhou", "Yichen", ""], ["Chen", "Yunpeng", ""], ["Feng", "Jiashi", ""], ["Yan", "Shuicheng", ""]]}, {"id": "2010.10019", "submitter": "Thao Minh Le", "authors": "Thao Minh Le, Vuong Le, Svetha Venkatesh, Truyen Tran", "title": "Hierarchical Conditional Relation Networks for Multimodal Video Question\n  Answering", "comments": "Major extension of our CVPR'20 paper to handle long video with text.\n  arXiv admin note: substantial text overlap with arXiv:2002.10698", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video QA challenges modelers in multiple fronts. Modeling video necessitates\nbuilding not only spatio-temporal models for the dynamic visual channel but\nalso multimodal structures for associated information channels such as\nsubtitles or audio. Video QA adds at least two more layers of complexity -\nselecting relevant content for each channel in the context of the linguistic\nquery, and composing spatio-temporal concepts and relations in response to the\nquery. To address these requirements, we start with two insights: (a) content\nselection and relation construction can be jointly encapsulated into a\nconditional computational structure, and (b) video-length structures can be\ncomposed hierarchically. For (a) this paper introduces a general-reusable\nneural unit dubbed Conditional Relation Network (CRN) taking as input a set of\ntensorial objects and translating into a new set of objects that encode\nrelations of the inputs. The generic design of CRN helps ease the common\ncomplex model building process of Video QA by simple block stacking with\nflexibility in accommodating input modalities and conditioning features across\nboth different domains. As a result, we realize insight (b) by introducing\nHierarchical Conditional Relation Networks (HCRN) for Video QA. The HCRN\nprimarily aims at exploiting intrinsic properties of the visual content of a\nvideo and its accompanying channels in terms of compositionality, hierarchy,\nand near and far-term relation. HCRN is then applied for Video QA in two forms,\nshort-form where answers are reasoned solely from the visual content, and\nlong-form where associated information, such as subtitles, presented. Our\nrigorous evaluations show consistent improvements over SOTAs on well-studied\nbenchmarks including large-scale real-world datasets such as TGIF-QA and TVQA,\ndemonstrating the strong capabilities of our CRN unit and the HCRN for complex\ndomains such as Video QA.\n", "versions": [{"version": "v1", "created": "Sun, 18 Oct 2020 02:31:06 GMT"}, {"version": "v2", "created": "Sun, 3 Jan 2021 07:11:23 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Le", "Thao Minh", ""], ["Le", "Vuong", ""], ["Venkatesh", "Svetha", ""], ["Tran", "Truyen", ""]]}, {"id": "2010.10024", "submitter": "Jovita Lukasik", "authors": "Jovita Lukasik, David Friede, Heiner Stuckenschmidt, Margret Keuper", "title": "Neural Architecture Performance Prediction Using Graph Neural Networks", "comments": "camera ready version for DAGM GCPR 2020. arXiv admin note:\n  substantial text overlap with arXiv:1912.05317", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In computer vision research, the process of automating architecture\nengineering, Neural Architecture Search (NAS), has gained substantial interest.\nDue to the high computational costs, most recent approaches to NAS as well as\nthe few available benchmarks only provide limited search spaces. In this paper\nwe propose a surrogate model for neural architecture performance prediction\nbuilt upon Graph Neural Networks (GNN). We demonstrate the effectiveness of\nthis surrogate model on neural architecture performance prediction for\nstructurally unknown architectures (i.e. zero shot prediction) by evaluating\nthe GNN on several experiments on the NAS-Bench-101 dataset.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 09:33:57 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Lukasik", "Jovita", ""], ["Friede", "David", ""], ["Stuckenschmidt", "Heiner", ""], ["Keuper", "Margret", ""]]}, {"id": "2010.10025", "submitter": "Victor Lorena de Farias Souza", "authors": "Victor L. F. Souza, Adriano L. I. Oliveira, Rafael M. O. Cruz and\n  Robert Sabourin", "title": "An Investigation of Feature Selection and Transfer Learning for\n  Writer-Independent Offline Handwritten Signature Verification", "comments": "arXiv admin note: text overlap with arXiv:2004.03373", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  SigNet is a state of the art model for feature representation used for\nhandwritten signature verification (HSV). This representation is based on a\nDeep Convolutional Neural Network (DCNN) and contains 2048 dimensions. When\ntransposed to a dissimilarity space generated by the dichotomy transformation\n(DT), related to the writer-independent (WI) approach, these features may\ninclude redundant information. This paper investigates the presence of\noverfitting when using Binary Particle Swarm Optimization (BPSO) to perform the\nfeature selection in a wrapper mode. We proposed a method based on a global\nvalidation strategy with an external archive to control overfitting during the\nsearch for the most discriminant representation. Moreover, an investigation is\nalso carried out to evaluate the use of the selected features in a transfer\nlearning context. The analysis is carried out on a writer-independent approach\non the CEDAR, MCYT and GPDS datasets. The experimental results showed the\npresence of overfitting when no validation is used during the optimization\nprocess and the improvement when the global validation strategy with an\nexternal archive is used. Also, the space generated after feature selection can\nbe used in a transfer learning context.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 12:18:51 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Souza", "Victor L. F.", ""], ["Oliveira", "Adriano L. I.", ""], ["Cruz", "Rafael M. O.", ""], ["Sabourin", "Robert", ""]]}, {"id": "2010.10027", "submitter": "Yi Tang", "authors": "Yi Tang and Yuanman Li and Wenbin Zou", "title": "Fast Video Salient Object Detection via Spatiotemporal Knowledge\n  Distillation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since the wide employment of deep learning frameworks in video salient object\ndetection, the accuracy of the recent approaches has made stunning progress.\nThese approaches mainly adopt the sequential modules, based on optical flow or\nrecurrent neural network (RNN), to learn robust spatiotemporal features. These\nmodules are effective but significantly increase the computational burden of\nthe corresponding deep models. In this paper, to simplify the network and\nmaintain the accuracy, we present a lightweight network tailored for video\nsalient object detection through the spatiotemporal knowledge distillation.\nSpecifically, in the spatial aspect, we combine a saliency guidance feature\nembedding structure and spatial knowledge distillation to refine the spatial\nfeatures. In the temporal aspect, we propose a temporal knowledge distillation\nstrategy, which allows the network to learn the robust temporal features\nthrough the infer-frame feature encoding and distilling information from\nadjacent frames. The experiments on widely used video datasets (e.g., DAVIS,\nDAVSOD, SegTrack-V2) prove that our approach achieves competitive performance.\nFurthermore, without the employment of the complex sequential modules, the\nproposed network can obtain high efficiency with 0.01s per frame.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 04:48:36 GMT"}, {"version": "v2", "created": "Wed, 17 Mar 2021 09:51:51 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Tang", "Yi", ""], ["Li", "Yuanman", ""], ["Zou", "Wenbin", ""]]}, {"id": "2010.10032", "submitter": "Rabab Abdelfattah", "authors": "Rabab Abdelfattah, Xiaofeng Wang, and Song Wang", "title": "TTPLA: An Aerial-Image Dataset for Detection and Segmentation of\n  Transmission Towers and Power Lines", "comments": "17 pages, 9 figures, 4 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate detection and segmentation of transmission towers~(TTs) and power\nlines~(PLs) from aerial images plays a key role in protecting power-grid\nsecurity and low-altitude UAV safety. Meanwhile, aerial images of TTs and PLs\npose a number of new challenges to the computer vision researchers who work on\nobject detection and segmentation -- PLs are long and thin, and may show\nsimilar color as the background; TTs can be of various shapes and most likely\nmade up of line structures of various sparsity; The background scene, lighting,\nand object sizes can vary significantly from one image to another. In this\npaper we collect and release a new TT/PL Aerial-image (TTPLA) dataset,\nconsisting of 1,100 images with the resolution of 3,840$\\times$2,160 pixels, as\nwell as manually labeled 8,987 instances of TTs and PLs. We develop novel\npolicies for collecting, annotating, and labeling the images in TTPLA.\nDifferent from other relevant datasets, TTPLA supports evaluation of instance\nsegmentation, besides detection and semantic segmentation. To build a baseline\nfor detection and segmentation tasks on TTPLA, we report the performance of\nseveral state-of-the-art deep learning models on our dataset. TTPLA dataset is\npublicly available at https://github.com/r3ab/ttpla_dataset\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 04:58:05 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Abdelfattah", "Rabab", ""], ["Wang", "Xiaofeng", ""], ["Wang", "Song", ""]]}, {"id": "2010.10038", "submitter": "Sameer Dharur", "authors": "Sameer Dharur, Purva Tendulkar, Dhruv Batra, Devi Parikh, Ramprasaath\n  R. Selvaraju", "title": "SOrT-ing VQA Models : Contrastive Gradient Learning for Improved\n  Consistency", "comments": "Accepted to the NeurIPS 2020 workshop on Interpretable Inductive\n  Biases and Physically Structured Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent research in Visual Question Answering (VQA) has revealed\nstate-of-the-art models to be inconsistent in their understanding of the world\n-- they answer seemingly difficult questions requiring reasoning correctly but\nget simpler associated sub-questions wrong. These sub-questions pertain to\nlower level visual concepts in the image that models ideally should understand\nto be able to answer the higher level question correctly. To address this, we\nfirst present a gradient-based interpretability approach to determine the\nquestions most strongly correlated with the reasoning question on an image, and\nuse this to evaluate VQA models on their ability to identify the relevant\nsub-questions needed to answer a reasoning question. Next, we propose a\ncontrastive gradient learning based approach called Sub-question Oriented\nTuning (SOrT) which encourages models to rank relevant sub-questions higher\nthan irrelevant questions for an <image, reasoning-question> pair. We show that\nSOrT improves model consistency by upto 6.5% points over existing baselines,\nwhile also improving visual grounding.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 05:15:48 GMT"}, {"version": "v2", "created": "Tue, 1 Dec 2020 02:11:13 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Dharur", "Sameer", ""], ["Tendulkar", "Purva", ""], ["Batra", "Dhruv", ""], ["Parikh", "Devi", ""], ["Selvaraju", "Ramprasaath R.", ""]]}, {"id": "2010.10047", "submitter": "Byungjoo Kim", "authors": "Byungjoo Kim, Bryce Chudomelka, Jinyoung Park, Jaewoo Kang, Youngjoon\n  Hong, Hyunwoo J. Kim", "title": "Robust Neural Networks inspired by Strong Stability Preserving\n  Runge-Kutta methods", "comments": "ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have achieved state-of-the-art performance in a variety\nof fields. Recent works observe that a class of widely used neural networks can\nbe viewed as the Euler method of numerical discretization. From the numerical\ndiscretization perspective, Strong Stability Preserving (SSP) methods are more\nadvanced techniques than the explicit Euler method that produce both accurate\nand stable solutions. Motivated by the SSP property and a generalized\nRunge-Kutta method, we propose Strong Stability Preserving networks (SSP\nnetworks) which improve robustness against adversarial attacks. We empirically\ndemonstrate that the proposed networks improve the robustness against\nadversarial examples without any defensive methods. Further, the SSP networks\nare complementary with a state-of-the-art adversarial training scheme. Lastly,\nour experiments show that SSP networks suppress the blow-up of adversarial\nperturbations. Our results open up a way to study robust architectures of\nneural networks leveraging rich knowledge from numerical discretization\nliterature.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 05:55:58 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Kim", "Byungjoo", ""], ["Chudomelka", "Bryce", ""], ["Park", "Jinyoung", ""], ["Kang", "Jaewoo", ""], ["Hong", "Youngjoon", ""], ["Kim", "Hyunwoo J.", ""]]}, {"id": "2010.10051", "submitter": "Jieqi Shi", "authors": "Jieqi Shi, Peiliang Li, Shaojie Shen", "title": "Tracking from Patterns: Learning Corresponding Patterns in Point Clouds\n  for 3D Object Tracking", "comments": "4 pages, ECCV2020 Workshop on Perception for Autonomous\n  Driving(PAD2020)", "journal-ref": "ECCV2020 Workshop on Perception for Autonomous Driving(PAD2020)", "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A robust 3D object tracker which continuously tracks surrounding objects and\nestimates their trajectories is key for self-driving vehicles. Most existing\ntracking methods employ a tracking-by-detection strategy, which usually\nrequires complex pair-wise similarity computation and neglects the nature of\ncontinuous object motion. In this paper, we propose to directly learn 3D object\ncorrespondences from temporal point cloud data and infer the motion information\nfrom correspondence patterns. We modify the standard 3D object detector to\nprocess two lidar frames at the same time and predict bounding box pairs for\nthe association and motion estimation tasks. We also equip our pipeline with a\nsimple yet effective velocity smoothing module to estimate consistent object\nmotion. Benifiting from the learned correspondences and motion refinement, our\nmethod exceeds the existing 3D tracking methods on both the KITTI and larger\nscale Nuscenes dataset.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 06:07:20 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Shi", "Jieqi", ""], ["Li", "Peiliang", ""], ["Shen", "Shaojie", ""]]}, {"id": "2010.10052", "submitter": "Prasan Shedligeri", "authors": "S Anupama, Prasan Shedligeri, Abhishek Pal, Kaushik Mitra", "title": "Video Reconstruction by Spatio-Temporal Fusion of Blurred-Coded Image\n  Pair", "comments": "8 pages, 7 figures, 3 tables, To appear at ICPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning-based methods have enabled the recovery of a video sequence from a\nsingle motion-blurred image or a single coded exposure image. Recovering video\nfrom a single motion-blurred image is a very ill-posed problem and the\nrecovered video usually has many artifacts. In addition to this, the direction\nof motion is lost and it results in motion ambiguity. However, it has the\nadvantage of fully preserving the information in the static parts of the scene.\nThe traditional coded exposure framework is better-posed but it only samples a\nfraction of the space-time volume, which is at best 50% of the space-time\nvolume. Here, we propose to use the complementary information present in the\nfully-exposed (blurred) image along with the coded exposure image to recover a\nhigh fidelity video without any motion ambiguity. Our framework consists of a\nshared encoder followed by an attention module to selectively combine the\nspatial information from the fully-exposed image with the temporal information\nfrom the coded image, which is then super-resolved to recover a non-ambiguous\nhigh-quality video. The input to our algorithm is a fully-exposed and coded\nimage pair. Such an acquisition system already exists in the form of a\nCoded-two-bucket (C2B) camera. We demonstrate that our proposed deep learning\napproach using blurred-coded image pair produces much better results than those\nfrom just a blurred image or just a coded image.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 06:08:42 GMT"}, {"version": "v2", "created": "Fri, 13 Nov 2020 10:06:06 GMT"}], "update_date": "2020-11-16", "authors_parsed": [["Anupama", "S", ""], ["Shedligeri", "Prasan", ""], ["Pal", "Abhishek", ""], ["Mitra", "Kaushik", ""]]}, {"id": "2010.10056", "submitter": "Xide Xia", "authors": "Xide Xia, Tianfan Xue, Wei-sheng Lai, Zheng Sun, Abby Chang, Brian\n  Kulis, Jiawen Chen", "title": "Real-time Localized Photorealistic Video Style Transfer", "comments": "16 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel algorithm for transferring artistic styles of semantically\nmeaningful local regions of an image onto local regions of a target video while\npreserving its photorealism. Local regions may be selected either fully\nautomatically from an image, through using video segmentation algorithms, or\nfrom casual user guidance such as scribbles. Our method, based on a deep neural\nnetwork architecture inspired by recent work in photorealistic style transfer,\nis real-time and works on arbitrary inputs without runtime optimization once\ntrained on a diverse dataset of artistic styles. By augmenting our video\ndataset with noisy semantic labels and jointly optimizing over style, content,\nmask, and temporal losses, our method can cope with a variety of imperfections\nin the input and produce temporally coherent videos without visual artifacts.\nWe demonstrate our method on a variety of style images and target videos,\nincluding the ability to transfer different styles onto multiple objects\nsimultaneously, and smoothly transition between styles in time.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 06:21:09 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Xia", "Xide", ""], ["Xue", "Tianfan", ""], ["Lai", "Wei-sheng", ""], ["Sun", "Zheng", ""], ["Chang", "Abby", ""], ["Kulis", "Brian", ""], ["Chen", "Jiawen", ""]]}, {"id": "2010.10095", "submitter": "Hung Le", "authors": "Hung Le, Doyen Sahoo, Nancy F. Chen, Steven C.H. Hoi", "title": "BiST: Bi-directional Spatio-Temporal Reasoning for Video-Grounded\n  Dialogues", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video-grounded dialogues are very challenging due to (i) the complexity of\nvideos which contain both spatial and temporal variations, and (ii) the\ncomplexity of user utterances which query different segments and/or different\nobjects in videos over multiple dialogue turns. However, existing approaches to\nvideo-grounded dialogues often focus on superficial temporal-level visual cues,\nbut neglect more fine-grained spatial signals from videos. To address this\ndrawback, we propose Bi-directional Spatio-Temporal Learning (BiST), a\nvision-language neural framework for high-resolution queries in videos based on\ntextual cues. Specifically, our approach not only exploits both spatial and\ntemporal-level information, but also learns dynamic information diffusion\nbetween the two feature spaces through spatial-to-temporal and\ntemporal-to-spatial reasoning. The bidirectional strategy aims to tackle the\nevolving semantics of user queries in the dialogue setting. The retrieved\nvisual cues are used as contextual information to construct relevant responses\nto the users. Our empirical results and comprehensive qualitative analysis show\nthat BiST achieves competitive performance and generates reasonable responses\non a large-scale AVSD benchmark. We also adapt our BiST models to the Video QA\nsetting, and substantially outperform prior approaches on the TGIF-QA\nbenchmark.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 07:43:00 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Le", "Hung", ""], ["Sahoo", "Doyen", ""], ["Chen", "Nancy F.", ""], ["Hoi", "Steven C. H.", ""]]}, {"id": "2010.10103", "submitter": "Sungho Suh", "authors": "Sungho Suh, Jihun Kim, Paul Lukowicz and Yong Oh Lee", "title": "Two-stage generative adversarial networks for document image\n  binarization with color noise and background removal", "comments": "Submitted to Pattern Recognition, Elsevier", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Document image enhancement and binarization methods are often used to improve\nthe accuracy and efficiency of document image analysis tasks such as text\nrecognition. Traditional non-machine-learning methods are constructed on\nlow-level features in an unsupervised manner but have difficulty with\nbinarization on documents with severely degraded backgrounds. Convolutional\nneural network-based methods focus only on grayscale images and on local\ntextual features. In this paper, we propose a two-stage color document image\nenhancement and binarization method using generative adversarial neural\nnetworks. In the first stage, four color-independent adversarial networks are\ntrained to extract color foreground information from an input image for\ndocument image enhancement. In the second stage, two independent adversarial\nnetworks with global and local features are trained for image binarization of\ndocuments of variable size. For the adversarial neural networks, we formulate\nloss functions between a discriminator and generators having an encoder-decoder\nstructure. Experimental results show that the proposed method achieves better\nperformance than many classical and state-of-the-art algorithms over the\nDocument Image Binarization Contest (DIBCO) datasets, the LRDE Document\nBinarization Dataset (LRDE DBD), and our shipping label image dataset. We plan\nto release the shipping label dataset as well as our implementation code at\ngithub.com/opensuh/DocumentBinarization/.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 07:51:50 GMT"}, {"version": "v2", "created": "Mon, 14 Dec 2020 16:50:09 GMT"}, {"version": "v3", "created": "Tue, 27 Apr 2021 08:17:44 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Suh", "Sungho", ""], ["Kim", "Jihun", ""], ["Lukowicz", "Paul", ""], ["Lee", "Yong Oh", ""]]}, {"id": "2010.10163", "submitter": "Menghan Hu", "authors": "Chang Yao, Jingyu Tang, Menghan Hu, Yue Wu, Wenyi Guo, Qingli Li,\n  Xiao-Ping Zhang", "title": "Claw U-Net: A Unet-based Network with Deep Feature Concatenation for\n  Scleral Blood Vessel Segmentation", "comments": "5 pages,4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sturge-Weber syndrome (SWS) is a vascular malformation disease, and it may\ncause blindness if the patient's condition is severe. Clinical results show\nthat SWS can be divided into two types based on the characteristics of scleral\nblood vessels. Therefore, how to accurately segment scleral blood vessels has\nbecome a significant problem in computer-aided diagnosis. In this research, we\npropose to continuously upsample the bottom layer's feature maps to preserve\nimage details, and design a novel Claw UNet based on UNet for scleral blood\nvessel segmentation. Specifically, the residual structure is used to increase\nthe number of network layers in the feature extraction stage to learn deeper\nfeatures. In the decoding stage, by fusing the features of the encoding,\nupsampling, and decoding parts, Claw UNet can achieve effective segmentation in\nthe fine-grained regions of scleral blood vessels. To effectively extract small\nblood vessels, we use the attention mechanism to calculate the attention\ncoefficient of each position in images. Claw UNet outperforms other UNet-based\nnetworks on scleral blood vessel image dataset.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 09:55:29 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Yao", "Chang", ""], ["Tang", "Jingyu", ""], ["Hu", "Menghan", ""], ["Wu", "Yue", ""], ["Guo", "Wenyi", ""], ["Li", "Qingli", ""], ["Zhang", "Xiao-Ping", ""]]}, {"id": "2010.10197", "submitter": "Vincent Christlein", "authors": "Mathias Seuret, Anguelos Nicolaou, Dominique Stutzmann, Andreas Maier,\n  Vincent Christlein", "title": "ICFHR 2020 Competition on Image Retrieval for Historical Handwritten\n  Fragments", "comments": "ICFHR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This competition succeeds upon a line of competitions for writer and style\nanalysis of historical document images. In particular, we investigate the\nperformance of large-scale retrieval of historical document fragments in terms\nof style and writer identification. The analysis of historic fragments is a\ndifficult challenge commonly solved by trained humanists. In comparison to\nprevious competitions, we make the results more meaningful by addressing the\nissue of sample granularity and moving from writer to page fragment retrieval.\nThe two approaches, style and author identification, provide information on\nwhat kind of information each method makes better use of and indirectly\ncontribute to the interpretability of the participating method. Therefore, we\ncreated a large dataset consisting of more than 120 000 fragments. Although the\nmost teams submitted methods based on convolutional neural networks, the\nwinning entry achieves an mAP below 40%.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 11:12:35 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Seuret", "Mathias", ""], ["Nicolaou", "Anguelos", ""], ["Stutzmann", "Dominique", ""], ["Maier", "Andreas", ""], ["Christlein", "Vincent", ""]]}, {"id": "2010.10207", "submitter": "Tong Zheng", "authors": "Tong Zheng, Hirohisa Oda, Masahiro Oda, Shota Nakamura, Masaki Mori,\n  Hirotsugu Takabatake, Hiroshi Natori, Kensaku Mori", "title": "Micro CT Image-Assisted Cross Modality Super-Resolution of Clinical CT\n  Images Utilizing Synthesized Training Dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel, unsupervised super-resolution (SR) approach for\nperforming the SR of a clinical CT into the resolution level of a micro CT\n($\\mu$CT). The precise non-invasive diagnosis of lung cancer typically utilizes\nclinical CT data. Due to the resolution limitations of clinical CT (about $0.5\n\\times 0.5 \\times 0.5$ mm$^3$), it is difficult to obtain enough pathological\ninformation such as the invasion area at alveoli level. On the other hand,\n$\\mu$CT scanning allows the acquisition of volumes of lung specimens with much\nhigher resolution ($50 \\times 50 \\times 50 \\mu {\\rm m}^3$ or higher). Thus,\nsuper-resolution of clinical CT volume may be helpful for diagnosis of lung\ncancer. Typical SR methods require aligned pairs of low-resolution (LR) and\nhigh-resolution (HR) images for training. Unfortunately, obtaining paired\nclinical CT and $\\mu$CT volumes of human lung tissues is infeasible.\nUnsupervised SR methods are required that do not need paired LR and HR images.\nIn this paper, we create corresponding clinical CT-$\\mu$CT pairs by simulating\nclinical CT images from $\\mu$CT images by modified CycleGAN. After this, we use\nsimulated clinical CT-$\\mu$CT image pairs to train an SR network based on\nSRGAN. Finally, we use the trained SR network to perform SR of the clinical CT\nimages. We compare our proposed method with another unsupervised SR method for\nclinical CT images named SR-CycleGAN. Experimental results demonstrate that the\nproposed method can successfully perform SR of clinical CT images of lung\ncancer patients with $\\mu$CT level resolution, and quantitatively and\nqualitatively outperformed conventional method (SR-CycleGAN), improving the\nSSIM (structure similarity) form 0.40 to 0.51.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 11:40:24 GMT"}], "update_date": "2020-11-22", "authors_parsed": [["Zheng", "Tong", ""], ["Oda", "Hirohisa", ""], ["Oda", "Masahiro", ""], ["Nakamura", "Shota", ""], ["Mori", "Masaki", ""], ["Takabatake", "Hirotsugu", ""], ["Natori", "Hiroshi", ""], ["Mori", "Kensaku", ""]]}, {"id": "2010.10241", "submitter": "Michal Valko", "authors": "Pierre H. Richemond, Jean-Bastien Grill, Florent Altch\\'e, Corentin\n  Tallec, Florian Strub, Andrew Brock, Samuel Smith, Soham De, Razvan Pascanu,\n  Bilal Piot, Michal Valko", "title": "BYOL works even without batch statistics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bootstrap Your Own Latent (BYOL) is a self-supervised learning approach for\nimage representation. From an augmented view of an image, BYOL trains an online\nnetwork to predict a target network representation of a different augmented\nview of the same image. Unlike contrastive methods, BYOL does not explicitly\nuse a repulsion term built from negative pairs in its training objective. Yet,\nit avoids collapse to a trivial, constant representation. Thus, it has recently\nbeen hypothesized that batch normalization (BN) is critical to prevent collapse\nin BYOL. Indeed, BN flows gradients across batch elements, and could leak\ninformation about negative views in the batch, which could act as an implicit\nnegative (contrastive) term. However, we experimentally show that replacing BN\nwith a batch-independent normalization scheme (namely, a combination of group\nnormalization and weight standardization) achieves performance comparable to\nvanilla BYOL ($73.9\\%$ vs. $74.3\\%$ top-1 accuracy under the linear evaluation\nprotocol on ImageNet with ResNet-$50$). Our finding disproves the hypothesis\nthat the use of batch statistics is a crucial ingredient for BYOL to learn\nuseful representations.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 13:05:05 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Richemond", "Pierre H.", ""], ["Grill", "Jean-Bastien", ""], ["Altch\u00e9", "Florent", ""], ["Tallec", "Corentin", ""], ["Strub", "Florian", ""], ["Brock", "Andrew", ""], ["Smith", "Samuel", ""], ["De", "Soham", ""], ["Pascanu", "Razvan", ""], ["Piot", "Bilal", ""], ["Valko", "Michal", ""]]}, {"id": "2010.10242", "submitter": "Thomas Cilloni", "authors": "Thomas Cilloni, Wei Wang, Charles Walter, Charles Fleming", "title": "Preventing Personal Data Theft in Images with Adversarial ML", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial recognition tools are becoming exceptionally accurate in identifying\npeople from images. However, this comes at the cost of privacy for users of\nonline services with photo management (e.g. social media platforms).\nParticularly troubling is the ability to leverage unsupervised learning to\nrecognize faces even when the user has not labeled their images. This is made\nsimpler by modern facial recognition tools, such as FaceNet, that use encoders\nto generate low dimensional embeddings that can be clustered to learn\npreviously unknown faces. In this paper, we propose a strategy to generate\nnon-invasive noise masks to apply to facial images for a newly introduced user,\nyielding adversarial examples and preventing the formation of identifiable\nclusters in the embedding space. We demonstrate the effectiveness of our method\nby showing that various classification and clustering methods cannot reliably\ncluster the adversarial examples we generate.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 13:05:51 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Cilloni", "Thomas", ""], ["Wang", "Wei", ""], ["Walter", "Charles", ""], ["Fleming", "Charles", ""]]}, {"id": "2010.10261", "submitter": "Zhao Zhong", "authors": "Yikang Zhang, Jian Zhang, Zhao Zhong", "title": "AutoBSS: An Efficient Algorithm for Block Stacking Style Search", "comments": "NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural network architecture design mostly focuses on the new convolutional\noperator or special topological structure of network block, little attention is\ndrawn to the configuration of stacking each block, called Block Stacking Style\n(BSS). Recent studies show that BSS may also have an unneglectable impact on\nnetworks, thus we design an efficient algorithm to search it automatically. The\nproposed method, AutoBSS, is a novel AutoML algorithm based on Bayesian\noptimization by iteratively refining and clustering Block Stacking Style Code\n(BSSC), which can find optimal BSS in a few trials without biased evaluation.\nOn ImageNet classification task, ResNet50/MobileNetV2/EfficientNet-B0 with our\nsearched BSS achieve 79.29%/74.5%/77.79%, which outperform the original\nbaselines by a large margin. More importantly, experimental results on model\ncompression, object detection and instance segmentation show the strong\ngeneralizability of the proposed AutoBSS, and further verify the unneglectable\nimpact of BSS on neural networks.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 13:32:10 GMT"}, {"version": "v2", "created": "Mon, 29 Mar 2021 05:08:52 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Zhang", "Yikang", ""], ["Zhang", "Jian", ""], ["Zhong", "Zhao", ""]]}, {"id": "2010.10266", "submitter": "A. Ben Hamza", "authors": "Hasib Zunair and A. Ben Hamza", "title": "Synthesis of COVID-19 Chest X-rays using Unpaired Image-to-Image\n  Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the lack of publicly available datasets of chest radiographs of\npositive patients with Coronavirus disease 2019 (COVID-19), we build the\nfirst-of-its-kind open dataset of synthetic COVID-19 chest X-ray images of high\nfidelity using an unsupervised domain adaptation approach by leveraging class\nconditioning and adversarial training. Our contributions are twofold. First, we\nshow considerable performance improvements on COVID-19 detection using various\ndeep learning architectures when employing synthetic images as additional\ntraining set. Second, we show how our image synthesis method can serve as a\ndata anonymization tool by achieving comparable detection performance when\ntrained only on synthetic data. In addition, the proposed data generation\nframework offers a viable solution to the COVID-19 detection in particular, and\nto medical image classification tasks in general. Our publicly available\nbenchmark dataset consists of 21,295 synthetic COVID-19 chest X-ray images. The\ninsights gleaned from this dataset can be used for preventive actions in the\nfight against the COVID-19 pandemic.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 13:37:40 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Zunair", "Hasib", ""], ["Hamza", "A. Ben", ""]]}, {"id": "2010.10270", "submitter": "Saeed Saadatnejad", "authors": "Smail Ait Bouhsain, Saeed Saadatnejad and Alexandre Alahi", "title": "Pedestrian Intention Prediction: A Multi-task Perspective", "comments": "Accepted and published in hEART2020 (the 9th Symposium of the\n  European Association for Research in Transportation):\n  http://www.heart-web.org/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to be globally deployed, autonomous cars must guarantee the safety\nof pedestrians. This is the reason why forecasting pedestrians' intentions\nsufficiently in advance is one of the most critical and challenging tasks for\nautonomous vehicles. This work tries to solve this problem by jointly\npredicting the intention and visual states of pedestrians. In terms of visual\nstates, whereas previous work focused on x-y coordinates, we will also predict\nthe size and indeed the whole bounding box of the pedestrian. The method is a\nrecurrent neural network in a multi-task learning approach. It has one head\nthat predicts the intention of the pedestrian for each one of its future\nposition and another one predicting the visual states of the pedestrian.\nExperiments on the JAAD dataset show the superiority of the performance of our\nmethod compared to previous works for intention prediction. Also, although its\nsimple architecture (more than 2 times faster), the performance of the bounding\nbox prediction is comparable to the ones yielded by much more complex\narchitectures. Our code is available online.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 13:42:31 GMT"}, {"version": "v2", "created": "Thu, 20 May 2021 11:14:35 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Bouhsain", "Smail Ait", ""], ["Saadatnejad", "Saeed", ""], ["Alahi", "Alexandre", ""]]}, {"id": "2010.10295", "submitter": "Dmitry Pozdnyakov", "authors": "Dmitry Pozdnyakov", "title": "Fisheye lens distortion correction", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new distortion correction algorithm for fisheye lens with equidistant\nmapping function is considered in the present study. The algorithm is much more\ndata lossless and accurate than such a classical approach like Brown-Conrady\nmodel\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 11:11:54 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Pozdnyakov", "Dmitry", ""]]}, {"id": "2010.10298", "submitter": "Jingyu Liu", "authors": "Jie Lian, Jingyu Liu, Yizhou Yu, Mengyuan Ding, Yaoci Lu, Yi Lu, Jie\n  Cai, Deshou Lin, Miao Zhang, Zhe Wang, Kai He, Yijie Yu", "title": "The Detection of Thoracic Abnormalities ChestX-Det10 Challenge Results", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The detection of thoracic abnormalities challenge is organized by the\nDeepwise AI Lab. The challenge is divided into two rounds. In this paper, we\npresent the results of 6 teams which reach the second round. The challenge\nadopts the ChestX-Det10 dateset proposed by the Deepwise AI Lab. ChestX-Det10\nis the first chest X-Ray dataset with instance-level annotations, including 10\ncategories of disease/abnormality of 3,543 images. The annotations are located\nat https://github.com/Deepwise-AILab/ChestX-Det10-Dataset. In the challenge, we\nrandomly split all data into 3001 images for training and 542 images for\ntesting.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 07:57:27 GMT"}, {"version": "v2", "created": "Thu, 22 Oct 2020 03:42:29 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Lian", "Jie", ""], ["Liu", "Jingyu", ""], ["Yu", "Yizhou", ""], ["Ding", "Mengyuan", ""], ["Lu", "Yaoci", ""], ["Lu", "Yi", ""], ["Cai", "Jie", ""], ["Lin", "Deshou", ""], ["Zhang", "Miao", ""], ["Wang", "Zhe", ""], ["He", "Kai", ""], ["Yu", "Yijie", ""]]}, {"id": "2010.10338", "submitter": "Sang Ho Lee", "authors": "Sangho Lee, Kiyoon Yoo, Nojun Kwak", "title": "Edge Bias in Federated Learning and its Solution by Buffered Knowledge\n  Distillation", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning (FL), which utilizes communication between the server\n(core) and local devices (edges) to indirectly learn from more data, is an\nemerging field in deep learning research. Recently, Knowledge\nDistillation-based FL methods with notable performance and high applicability\nhave been suggested. In this paper, we choose knowledge distillation-based FL\nmethod as our baseline and tackle a challenging problem that ensues from using\nthese methods. Especially, we focus on the problem incurred in the server model\nthat tries to mimic different datasets, each of which is unique to an\nindividual edge device. We dub the problem 'edge bias', which occurs when\nmultiple teacher models trained on different datasets are used individually to\ndistill knowledge. We introduce this nuisance that occurs in certain scenarios\nof FL, and to alleviate it, we propose a simple yet effective distillation\nscheme named 'buffered distillation'. In addition, we also experimentally show\nthat this scheme is effective in mitigating the straggler problem caused by\ndelayed edges.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 15:00:43 GMT"}, {"version": "v2", "created": "Thu, 22 Oct 2020 07:02:14 GMT"}, {"version": "v3", "created": "Tue, 9 Feb 2021 07:26:33 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Lee", "Sangho", ""], ["Yoo", "Kiyoon", ""], ["Kwak", "Nojun", ""]]}, {"id": "2010.10340", "submitter": "Zohaib Salahuddin", "authors": "Jaime Simarro, Zohaib Salahuddin, Ahmed Gouda, Anindo Saha", "title": "Leveraging SLIC Superpixel Segmentation and Cascaded Ensemble SVM for\n  Fully Automated Mass Detection In Mammograms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identification and segmentation of breast masses in mammograms face complex\nchallenges, owing to the highly variable nature of malignant densities with\nregards to their shape, contours, texture and orientation. Additionally,\nclassifiers typically suffer from high class imbalance in region candidates,\nwhere normal tissue regions vastly outnumber malignant masses. This paper\nproposes a rigorous segmentation method, supported by morphological enhancement\nusing grayscale linear filters. A novel cascaded ensemble of support vector\nmachines (SVM) is used to effectively tackle the class imbalance and provide\nsignificant predictions. For True Positive Rate (TPR) of 0.35, 0.69 and 0.82,\nthe system generates only 0.1, 0.5 and 1.0 False Positives/Image (FPI),\nrespectively.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 15:02:25 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Simarro", "Jaime", ""], ["Salahuddin", "Zohaib", ""], ["Gouda", "Ahmed", ""], ["Saha", "Anindo", ""]]}, {"id": "2010.10357", "submitter": "Radu Tudor Ionescu", "authors": "Nicolae-C\\u{a}t\\u{a}lin Ristea, Andrei Anghel, Radu Tudor Ionescu,\n  Yonina C. Eldar", "title": "Automotive Radar Interference Mitigation with Unfolded Robust PCA based\n  on Residual Overcomplete Auto-Encoder Blocks", "comments": "Accepted at the CVPR 2021 Embedded Vision Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In autonomous driving, radar systems play an important role in detecting\ntargets such as other vehicles on the road. Radars mounted on different cars\ncan interfere with each other, degrading the detection performance. Deep\nlearning methods for automotive radar interference mitigation can succesfully\nestimate the amplitude of targets, but fail to recover the phase of the\nrespective targets. In this paper, we propose an efficient and effective\ntechnique based on unfolded robust Principal Component Analysis (RPCA) that is\nable to estimate both amplitude and phase in the presence of interference. Our\ncontribution consists in introducing residual overcomplete auto-encoder\n(ROC-AE) blocks into the recurrent architecture of unfolded RPCA, which results\nin a deeper model that significantly outperforms unfolded RPCA as well as other\ndeep learning models.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 09:41:06 GMT"}, {"version": "v2", "created": "Sat, 17 Apr 2021 11:37:44 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Ristea", "Nicolae-C\u0103t\u0103lin", ""], ["Anghel", "Andrei", ""], ["Ionescu", "Radu Tudor", ""], ["Eldar", "Yonina C.", ""]]}, {"id": "2010.10368", "submitter": "Ali Akbari", "authors": "Ali Akbari, Muhammad Awais, Zhen-Hua Feng, Ammarah Farooq and Josef\n  Kittler", "title": "A Flatter Loss for Bias Mitigation in Cross-dataset Facial Age\n  Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The most existing studies in the facial age estimation assume training and\ntest images are captured under similar shooting conditions. However, this is\nrarely valid in real-world applications, where training and test sets usually\nhave different characteristics. In this paper, we advocate a cross-dataset\nprotocol for age estimation benchmarking. In order to improve the cross-dataset\nage estimation performance, we mitigate the inherent bias caused by the\nlearning algorithm itself. To this end, we propose a novel loss function that\nis more effective for neural network training. The relative smoothness of the\nproposed loss function is its advantage with regards to the optimisation\nprocess performed by stochastic gradient descent (SGD). Compared with existing\nloss functions, the lower gradient of the proposed loss function leads to the\nconvergence of SGD to a better optimum point, and consequently a better\ngeneralisation. The cross-dataset experimental results demonstrate the\nsuperiority of the proposed method over the state-of-the-art algorithms in\nterms of accuracy and generalisation capability.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 15:22:29 GMT"}, {"version": "v2", "created": "Mon, 26 Oct 2020 19:29:06 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Akbari", "Ali", ""], ["Awais", "Muhammad", ""], ["Feng", "Zhen-Hua", ""], ["Farooq", "Ammarah", ""], ["Kittler", "Josef", ""]]}, {"id": "2010.10373", "submitter": "Ekaterina Kondrateva", "authors": "Ruslan Aliev and Ekaterina Kondrateva and Maxim Sharaev and Oleg\n  Bronov and Alexey Marinets and Sergey Subbotin and Alexander Bernstein and\n  Evgeny Burnaev", "title": "Convolutional neural networks for automatic detection of Focal Cortical\n  Dysplasia", "comments": "MRI, Deep learning, CNN, computer vision, medical detection,\n  epilepsy, FCD, focal cortical dysplasia", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Focal cortical dysplasia (FCD) is one of the most common epileptogenic\nlesions associated with cortical development malformations. However, the\naccurate detection of the FCD relies on the radiologist professionalism, and in\nmany cases, the lesion could be missed. In this work, we solve the problem of\nautomatic identification of FCD on magnetic resonance images (MRI). For this\ntask, we improve recent methods of Deep Learning-based FCD detection and apply\nit for a dataset of 15 labeled FCD patients. The model results in the\nsuccessful detection of FCD on 11 out of 15 subjects.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 15:30:37 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Aliev", "Ruslan", ""], ["Kondrateva", "Ekaterina", ""], ["Sharaev", "Maxim", ""], ["Bronov", "Oleg", ""], ["Marinets", "Alexey", ""], ["Subbotin", "Sergey", ""], ["Bernstein", "Alexander", ""], ["Burnaev", "Evgeny", ""]]}, {"id": "2010.10451", "submitter": "Rafal Pytel", "authors": "Rafal Pytel, Osman Semih Kayhan, Jan C. van Gemert", "title": "Tilting at windmills: Data augmentation for deep pose estimation does\n  not help with occlusions", "comments": "ICPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Occlusion degrades the performance of human pose estimation. In this paper,\nwe introduce targeted keypoint and body part occlusion attacks. The effects of\nthe attacks are systematically analyzed on the best performing methods. In\naddition, we propose occlusion specific data augmentation techniques against\nkeypoint and part attacks. Our extensive experiments show that human pose\nestimation methods are not robust to occlusion and data augmentation does not\nsolve the occlusion problems.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 17:06:46 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Pytel", "Rafal", ""], ["Kayhan", "Osman Semih", ""], ["van Gemert", "Jan C.", ""]]}, {"id": "2010.10475", "submitter": "Bj{\\o}rn Magnus Mathisen", "authors": "Bj{\\o}rn Magnus Mathisen and Kerstin Bach and Espen Meidell and\n  H{\\aa}kon M{\\aa}l{\\o}y and Edvard Schreiner Sj{\\o}blom", "title": "FishNet: A Unified Embedding for Salmon Recognition", "comments": "ECAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying individual salmon can be very beneficial for the aquaculture\nindustry as it enables monitoring and analyzing fish behavior and welfare. For\naquaculture researchers identifying individual salmon is imperative to their\nresearch. The current methods of individual salmon tagging and tracking rely on\nphysical interaction with the fish. This process is inefficient and can cause\nphysical harm and stress for the salmon. In this paper we propose FishNet,\nbased on a deep learning technique that has been successfully used for\nidentifying humans, to identify salmon.We create a dataset of labeled fish\nimages and then test the performance of the FishNet architecture. Our\nexperiments show that this architecture learns a useful representation based on\nimages of salmon heads. Further, we show that good performance can be achieved\nwith relatively small neural network models: FishNet achieves a false positive\nrate of 1\\% and a true positive rate of 96\\%.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 17:35:01 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Mathisen", "Bj\u00f8rn Magnus", ""], ["Bach", "Kerstin", ""], ["Meidell", "Espen", ""], ["M\u00e5l\u00f8y", "H\u00e5kon", ""], ["Sj\u00f8blom", "Edvard Schreiner", ""]]}, {"id": "2010.10505", "submitter": "Chen-Hsuan Lin", "authors": "Chen-Hsuan Lin, Chaoyang Wang, Simon Lucey", "title": "SDF-SRN: Learning Signed Distance 3D Object Reconstruction from Static\n  Images", "comments": "Accepted to NeurIPS 2020. Project page & code:\n  https://chenhsuanlin.bitbucket.io/signed-distance-SRN/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dense 3D object reconstruction from a single image has recently witnessed\nremarkable advances, but supervising neural networks with ground-truth 3D\nshapes is impractical due to the laborious process of creating paired\nimage-shape datasets. Recent efforts have turned to learning 3D reconstruction\nwithout 3D supervision from RGB images with annotated 2D silhouettes,\ndramatically reducing the cost and effort of annotation. These techniques,\nhowever, remain impractical as they still require multi-view annotations of the\nsame object instance during training. As a result, most experimental efforts to\ndate have been limited to synthetic datasets. In this paper, we address this\nissue and propose SDF-SRN, an approach that requires only a single view of\nobjects at training time, offering greater utility for real-world scenarios.\nSDF-SRN learns implicit 3D shape representations to handle arbitrary shape\ntopologies that may exist in the datasets. To this end, we derive a novel\ndifferentiable rendering formulation for learning signed distance functions\n(SDF) from 2D silhouettes. Our method outperforms the state of the art under\nchallenging single-view supervision settings on both synthetic and real-world\ndatasets.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 17:59:47 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Lin", "Chen-Hsuan", ""], ["Wang", "Chaoyang", ""], ["Lucey", "Simon", ""]]}, {"id": "2010.10557", "submitter": "Nitin Agarwal", "authors": "Tomer Weiss, Ilkay Yildiz, Nitin Agarwal, Esra Ataer-Cansizoglu,\n  Jae-Woo Choi", "title": "Image-Driven Furniture Style for Interactive 3D Scene Modeling", "comments": "Accepted to Pacific Graphics 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Creating realistic styled spaces is a complex task, which involves design\nknow-how for what furniture pieces go well together. Interior style follows\nabstract rules involving color, geometry and other visual elements. Following\nsuch rules, users manually select similar-style items from large repositories\nof 3D furniture models, a process which is both laborious and time-consuming.\nWe propose a method for fast-tracking style-similarity tasks, by learning a\nfurniture's style-compatibility from interior scene images. Such images contain\nmore style information than images depicting single furniture. To understand\nstyle, we train a deep learning network on a classification task. Based on\nimage embeddings extracted from our network, we measure stylistic compatibility\nof furniture. We demonstrate our method with several 3D model\nstyle-compatibility results, and with an interactive system for modeling\nstyle-consistent scenes.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 18:19:28 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Weiss", "Tomer", ""], ["Yildiz", "Ilkay", ""], ["Agarwal", "Nitin", ""], ["Ataer-Cansizoglu", "Esra", ""], ["Choi", "Jae-Woo", ""]]}, {"id": "2010.10563", "submitter": "Pablo Messina", "authors": "Pablo Messina, Pablo Pino, Denis Parra, Alvaro Soto, Cecilia Besa,\n  Sergio Uribe, Marcelo and\\'ia, Cristian Tejos, Claudia Prieto and Daniel\n  Capurro", "title": "A Survey on Deep Learning and Explainability for Automatic Image-based\n  Medical Report Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Every year physicians face an increasing demand of image-based diagnosis from\npatients, a problem that can be addressed with recent artificial intelligence\nmethods. In this context, we survey works in the area of automatic report\ngeneration from medical images, with emphasis on methods using deep neural\nnetworks, with respect to: (1) Datasets, (2) Architecture Design, (3)\nExplainability and (4) Evaluation Metrics. Our survey identifies interesting\ndevelopments, but also remaining challenges. Among them, the current evaluation\nof generated reports is especially weak, since it mostly relies on traditional\nNatural Language Processing (NLP) metrics, which do not accurately capture\nmedical correctness.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 18:48:37 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Messina", "Pablo", ""], ["Pino", "Pablo", ""], ["Parra", "Denis", ""], ["Soto", "Alvaro", ""], ["Besa", "Cecilia", ""], ["Uribe", "Sergio", ""], ["and\u00eda", "Marcelo", ""], ["Tejos", "Cristian", ""], ["Prieto", "Claudia", ""], ["Capurro", "Daniel", ""]]}, {"id": "2010.10584", "submitter": "Surej Mouli PhD", "authors": "Ramaswamy Palaniappan, Surej Mouli, Evangelina Fringi, Howard Bowman\n  and Ian McLoughlin", "title": "Incandescent Bulb and LED Brake Lights:Novel Analysis of Reaction Times", "comments": "10 pages, 18 figures", "journal-ref": "For a revised version and its published version refer to IEEE\n  Access journal, 2021", "doi": "10.1109/ACCESS.2021.3058579", "report-no": null, "categories": "cs.HC cs.CV eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rear-end collision accounts for around 8% of all vehicle crashes in the UK,\nwith the failure to notice or react to a brake light signal being a major\ncontributory cause. Meanwhile traditional incandescent brake light bulbs on\nvehicles are increasingly being replaced by a profusion of designs featuring\nLEDs. In this paper, we investigate the efficacy of brake light design using a\nnovel approach to recording subject reaction times in a simulation setting\nusing physical brake light assemblies. The reaction times of 22 subjects were\nmeasured for ten pairs of LED and incandescent bulb brake lights. Three events\nwere investigated for each subject, namely the latency of brake light\nactivation to accelerator release (BrakeAcc), the latency of accelerator\nrelease to brake pedal depression (AccPdl), and the cumulative time from light\nactivation to brake pedal depression (BrakePdl). To our knowledge, this is the\nfirst study in which reaction times have been split into BrakeAcc and AccPdl.\nResults indicate that the two brake lights containing incandescent bulbs led to\nsignificantly slower reaction times compared to the tested eight LED lights.\nBrakeAcc results also show that experienced subjects were quicker to respond to\nthe activation of brake lights by releasing the accelerator pedal.\nInterestingly, the analysis also revealed that the type of brake light\ninfluenced the AccPdl time, although experienced subjects did not always act\nquicker than inexperienced subjects. Overall, the study found that different\ndesigns of brake light can significantly influence driver response times.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 19:41:52 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Palaniappan", "Ramaswamy", ""], ["Mouli", "Surej", ""], ["Fringi", "Evangelina", ""], ["Bowman", "Howard", ""], ["McLoughlin", "Ian", ""]]}, {"id": "2010.10590", "submitter": "Puru Malhotra", "authors": "Yugam Bajaj and Puru Malhotra", "title": "American Sign Language Identification Using Hand Trackpoint Analysis", "comments": "12 Pages, 6 Images", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sign Language helps people with Speaking and Hearing Disabilities communicate\nwith others efficiently. Sign Language identification is a challenging area in\nthe field of computer vision and recent developments have been able to achieve\nnear perfect results for the task, though some challenges are yet to be solved.\nIn this paper we propose a novel machine learning based pipeline for American\nSign Language identification using hand track points. We convert a hand gesture\ninto a series of hand track point coordinates that serve as an input to our\nsystem. In order to make the solution more efficient, we experimented with 28\ndifferent combinations of pre-processing techniques, each run on three\ndifferent machine learning algorithms namely k-Nearest Neighbours, Random\nForests and a Neural Network. Their performance was contrasted to determine the\nbest pre-processing scheme and algorithm pair. Our system achieved an Accuracy\nof 95.66% to identify American sign language gestures.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 19:59:16 GMT"}, {"version": "v2", "created": "Sat, 24 Oct 2020 21:11:27 GMT"}, {"version": "v3", "created": "Tue, 19 Jan 2021 21:11:21 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Bajaj", "Yugam", ""], ["Malhotra", "Puru", ""]]}, {"id": "2010.10593", "submitter": "Tristan Sylvain", "authors": "Tristan Sylvain, Francis Dutil, Tess Berthier, Lisa Di Jorio, Margaux\n  Luck, Devon Hjelm, Yoshua Bengio", "title": "Cross-Modal Information Maximization for Medical Imaging: CMIM", "comments": "ICASSP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In hospitals, data are siloed to specific information systems that make the\nsame information available under different modalities such as the different\nmedical imaging exams the patient undergoes (CT scans, MRI, PET, Ultrasound,\netc.) and their associated radiology reports. This offers unique opportunities\nto obtain and use at train-time those multiple views of the same information\nthat might not always be available at test-time.\n  In this paper, we propose an innovative framework that makes the most of\navailable data by learning good representations of a multi-modal input that are\nresilient to modality dropping at test-time, using recent advances in mutual\ninformation maximization. By maximizing cross-modal information at train time,\nwe are able to outperform several state-of-the-art baselines in two different\nsettings, medical image classification, and segmentation. In particular, our\nmethod is shown to have a strong impact on the inference-time performance of\nweaker modalities.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 20:05:35 GMT"}, {"version": "v2", "created": "Sat, 28 Nov 2020 17:08:34 GMT"}, {"version": "v3", "created": "Mon, 1 Feb 2021 21:10:37 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Sylvain", "Tristan", ""], ["Dutil", "Francis", ""], ["Berthier", "Tess", ""], ["Di Jorio", "Lisa", ""], ["Luck", "Margaux", ""], ["Hjelm", "Devon", ""], ["Bengio", "Yoshua", ""]]}, {"id": "2010.10612", "submitter": "Mohammad Hamghalam", "authors": "Mohammad Hamghalam, Baiying Lei, and Tianfu Wang", "title": "Convolutional 3D to 2D Patch Conversion for Pixel-wise Glioma\n  Segmentation in MRI Scans", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-46640-4_1", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structural magnetic resonance imaging (MRI) has been widely utilized for\nanalysis and diagnosis of brain diseases. Automatic segmentation of brain\ntumors is a challenging task for computer-aided diagnosis due to low-tissue\ncontrast in the tumor subregions. To overcome this, we devise a novel\npixel-wise segmentation framework through a convolutional 3D to 2D MR patch\nconversion model to predict class labels of the central pixel in the input\nsliding patches. Precisely, we first extract 3D patches from each modality to\ncalibrate slices through the squeeze and excitation (SE) block. Then, the\noutput of the SE block is fed directly into subsequent bottleneck layers to\nreduce the number of channels. Finally, the calibrated 2D slices are\nconcatenated to obtain multimodal features through a 2D convolutional neural\nnetwork (CNN) for prediction of the central pixel. In our architecture, both\nlocal inter-slice and global intra-slice features are jointly exploited to\npredict class label of the central voxel in a given patch through the 2D CNN\nclassifier. We implicitly apply all modalities through trainable parameters to\nassign weights to the contributions of each sequence for segmentation.\nExperimental results on the segmentation of brain tumors in multimodal MRI\nscans (BraTS'19) demonstrate that our proposed method can efficiently segment\nthe tumor regions.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 20:42:52 GMT"}], "update_date": "2020-11-22", "authors_parsed": [["Hamghalam", "Mohammad", ""], ["Lei", "Baiying", ""], ["Wang", "Tianfu", ""]]}, {"id": "2010.10631", "submitter": "Hemant Kumar Aggarwal", "authors": "Hemant Kumar Aggarwal, Aniket Pramanik, Mathews Jacob", "title": "ENSURE: A General Approach for Unsupervised Training of Deep Image\n  Reconstruction Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image reconstruction using deep learning algorithms offers improved\nreconstruction quality and lower reconstruction time than classical compressed\nsensing and model-based algorithms. Unfortunately, clean and fully sampled\nground-truth data to train the deep networks is often not available in several\napplications, restricting the applicability of the above methods. This work\nintroduces the ENsemble Stein's Unbiased Risk Estimate (ENSURE) framework as a\ngeneral approach to train deep image reconstruction algorithms without fully\nsampled and noise-free images. The proposed framework is the generalization of\nthe classical SURE and GSURE formulation to the setting where the images are\nsampled by different measurement operators, chosen randomly from a set. We show\nthat the ENSURE loss function, which only uses the measurement data, is an\nunbiased estimate for the true mean-square error. Our experiments show that the\nnetworks trained with this loss function can offer reconstructions comparable\nto the supervised setting. While we demonstrate this framework in the context\nof MR image recovery, the ENSURE framework is generally applicable to arbitrary\ninverse problems.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 21:18:33 GMT"}, {"version": "v2", "created": "Tue, 22 Dec 2020 16:31:08 GMT"}, {"version": "v3", "created": "Tue, 23 Mar 2021 18:06:00 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Aggarwal", "Hemant Kumar", ""], ["Pramanik", "Aniket", ""], ["Jacob", "Mathews", ""]]}, {"id": "2010.10637", "submitter": "Xiaofeng Liu", "authors": "Xiaofeng Liu, Linghao Jin, Xu Han, Jane You", "title": "Mutual Information Regularized Identity-aware Facial\n  ExpressionRecognition in Compressed Video", "comments": "Published in Pattern Recognition", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How to extract effective expression representations that invariant to the\nidentity-specific attributes is a long-lasting problem for facial expression\nrecognition (FER). Most of the previous methods process the RGB images of a\nsequence, while we argue that the off-the-shelf and valuable expression-related\nmuscle movement is already embedded in the compression format. In this paper,\nwe target to explore the inter-subject variations eliminated facial expression\nrepresentation in the compressed video domain. In the up to two orders of\nmagnitude compressed domain, we can explicitly infer the expression from the\nresidual frames and possibly extract identity factors from the I frame with a\npre-trained face recognition network. By enforcing the marginal independence of\nthem, the expression feature is expected to be purer for the expression and be\nrobust to identity shifts. Specifically, we propose a novel collaborative\nmin-min game for mutual information (MI) minimization in latent space. We do\nnot need the identity label or multiple expression samples from the same person\nfor identity elimination. Moreover, when the apex frame is annotated in the\ndataset, the complementary constraint can be further added to regularize the\nfeature-level game. In testing, only the compressed residual frames are\nrequired to achieve expression prediction. Our solution can achieve comparable\nor better performance than the recent decoded image-based methods on the\ntypical FER benchmarks with about 3 times faster inference.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 21:42:18 GMT"}, {"version": "v2", "created": "Sat, 5 Jun 2021 15:09:55 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Liu", "Xiaofeng", ""], ["Jin", "Linghao", ""], ["Han", "Xu", ""], ["You", "Jane", ""]]}, {"id": "2010.10648", "submitter": "Elman Mansimov", "authors": "Elman Mansimov, Mitchell Stern, Mia Chen, Orhan Firat, Jakob\n  Uszkoreit, Puneet Jain", "title": "Towards End-to-End In-Image Neural Machine Translation", "comments": "Accepted as an oral presentation at EMNLP, NLP Beyond Text workshop,\n  2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we offer a preliminary investigation into the task of in-image\nmachine translation: transforming an image containing text in one language into\nan image containing the same text in another language. We propose an end-to-end\nneural model for this task inspired by recent approaches to neural machine\ntranslation, and demonstrate promising initial results based purely on\npixel-level supervision. We then offer a quantitative and qualitative\nevaluation of our system outputs and discuss some common failure modes.\nFinally, we conclude with directions for future work.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 22:20:04 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Mansimov", "Elman", ""], ["Stern", "Mitchell", ""], ["Chen", "Mia", ""], ["Firat", "Orhan", ""], ["Uszkoreit", "Jakob", ""], ["Jain", "Puneet", ""]]}, {"id": "2010.10661", "submitter": "Rajeev Yasarla", "authors": "Rajeev Yasarla (Student Member, IEEE), Jeya Maria Jose Valanarasu\n  (Student Member, IEEE), and Vishal M. Patel (Senior Member, IEEE)", "title": "Exploring Overcomplete Representations for Single Image Deraining using\n  CNNs", "comments": null, "journal-ref": "IEEE Journal of Selected Topics in Signal Processing, 2020", "doi": "10.1109/JSTSP.2020.3039393", "report-no": "J-STSP-DLIVRC-00060-2020", "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Removal of rain streaks from a single image is an extremely challenging\nproblem since the rainy images often contain rain streaks of different size,\nshape, direction and density. Most recent methods for deraining use a deep\nnetwork following a generic \"encoder-decoder\" architecture which captures\nlow-level features across the initial layers and high-level features in the\ndeeper layers. For the task of deraining, the rain streaks which are to be\nremoved are relatively small and focusing much on global features is not an\nefficient way to solve the problem. To this end, we propose using an\novercomplete convolutional network architecture which gives special attention\nin learning local structures by restraining the receptive field of filters. We\ncombine it with U-Net so that it does not lose out on the global structures as\nwell while focusing more on low-level features, to compute the derained image.\nThe proposed network called, Over-and-Under Complete Deraining Network (OUCD),\nconsists of two branches: overcomplete branch which is confined to small\nreceptive field size in order to focus on the local structures and an\nundercomplete branch that has larger receptive fields to primarily focus on\nglobal structures. Extensive experiments on synthetic and real datasets\ndemonstrate that the proposed method achieves significant improvements over the\nrecent state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 22:55:02 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Yasarla", "Rajeev", "", "Student Member, IEEE"], ["Valanarasu", "Jeya Maria Jose", "", "Student Member, IEEE"], ["Patel", "Vishal M.", "", "Senior Member, IEEE"]]}, {"id": "2010.10681", "submitter": "Vishal Mandal", "authors": "Vishal Mandal, Abdul Rashid Mussah, Yaw Adu-Gyamfi", "title": "Deep Learning Frameworks for Pavement Distress Classification: A\n  Comparative Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic detection and classification of pavement distresses is critical in\ntimely maintaining and rehabilitating pavement surfaces. With the evolution of\ndeep learning and high performance computing, the feasibility of vision-based\npavement defect assessments has significantly improved. In this study, the\nauthors deploy state-of-the-art deep learning algorithms based on different\nnetwork backbones to detect and characterize pavement distresses. The influence\nof different backbone models such as CSPDarknet53, Hourglass-104 and\nEfficientNet were studied to evaluate their classification performance. The\nmodels were trained using 21,041 images captured across urban and rural streets\nof Japan, Czech Republic and India. Finally, the models were assessed based on\ntheir ability to predict and classify distresses, and tested using F1 score\nobtained from the statistical precision and recall values. The best performing\nmodel achieved an F1 score of 0.58 and 0.57 on two test datasets released by\nthe IEEE Global Road Damage Detection Challenge. The source code including the\ntrained models are made available at [1].\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 00:26:59 GMT"}, {"version": "v2", "created": "Sun, 29 Nov 2020 19:57:06 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Mandal", "Vishal", ""], ["Mussah", "Abdul Rashid", ""], ["Adu-Gyamfi", "Yaw", ""]]}, {"id": "2010.10695", "submitter": "Kuang-Yu Jeng", "authors": "Kuang-Yu Jeng, Yueh-Cheng Liu, Zhe Yu Liu, Jen-Wei Wang, Ya-Liang\n  Chang, Hung-Ting Su, and Winston H. Hsu", "title": "GDN: A Coarse-To-Fine (C2F) Representation for End-To-End 6-DoF Grasp\n  Detection", "comments": "Accepted to CoRL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We proposed an end-to-end grasp detection network, Grasp Detection Network\n(GDN), cooperated with a novel coarse-to-fine (C2F) grasp representation design\nto detect diverse and accurate 6-DoF grasps based on point clouds. Compared to\nprevious two-stage approaches which sample and evaluate multiple grasp\ncandidates, our architecture is at least 20 times faster. It is also 8% and 40%\nmore accurate in terms of the success rate in single object scenes and the\ncomplete rate in clutter scenes, respectively. Our method shows superior\nresults among settings with different number of views and input points.\nMoreover, we propose a new AP-based metric which considers both rotation and\ntransition errors, making it a more comprehensive evaluation tool for grasp\ndetection models.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 01:01:50 GMT"}, {"version": "v2", "created": "Thu, 29 Oct 2020 08:05:07 GMT"}, {"version": "v3", "created": "Thu, 5 Nov 2020 12:57:39 GMT"}, {"version": "v4", "created": "Wed, 11 Nov 2020 07:00:03 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Jeng", "Kuang-Yu", ""], ["Liu", "Yueh-Cheng", ""], ["Liu", "Zhe Yu", ""], ["Wang", "Jen-Wei", ""], ["Chang", "Ya-Liang", ""], ["Su", "Hung-Ting", ""], ["Hsu", "Winston H.", ""]]}, {"id": "2010.10700", "submitter": "Liang Peng", "authors": "Liang Peng, Dan Deng, and Deng Cai", "title": "Geometry-based Occlusion-Aware Unsupervised Stereo Matching for\n  Autonomous Driving", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, there are emerging many stereo matching methods for autonomous\ndriving based on unsupervised learning. Most of them take advantage of\nreconstruction losses to remove dependency on disparity groundtruth. Occlusion\nhandling is a challenging problem in stereo matching, especially for\nunsupervised methods. Previous unsupervised methods failed to take full\nadvantage of geometry properties in occlusion handling. In this paper, we\nintroduce an effective way to detect occlusion regions and propose a novel\nunsupervised training strategy to deal with occlusion that only uses the\npredicted left disparity map, by making use of its geometry features in an\niterative way. In the training process, we regard the predicted left disparity\nmap as pseudo groundtruth and infer occluded regions using geometry features.\nThe resulting occlusion mask is then used in either training, post-processing,\nor both of them as guidance. Experiments show that our method could deal with\nthe occlusion problem effectively and significantly outperforms the other\nunsupervised methods for stereo matching. Moreover, our occlusion-aware\nstrategies can be extended to the other stereo methods conveniently and improve\ntheir performances.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 01:22:55 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Peng", "Liang", ""], ["Deng", "Dan", ""], ["Cai", "Deng", ""]]}, {"id": "2010.10712", "submitter": "Fanhua Shang", "authors": "Hongying Liu, Zhenyu Zhou, Fanhua Shang, Xiaoyu Qi, Yuanyuan Liu,\n  Licheng Jiao", "title": "Boosting Gradient for White-Box Adversarial Attacks", "comments": "9 pages,6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) are playing key roles in various artificial\nintelligence applications such as image classification and object recognition.\nHowever, a growing number of studies have shown that there exist adversarial\nexamples in DNNs, which are almost imperceptibly different from original\nsamples, but can greatly change the network output. Existing white-box attack\nalgorithms can generate powerful adversarial examples. Nevertheless, most of\nthe algorithms concentrate on how to iteratively make the best use of gradients\nto improve adversarial performance. In contrast, in this paper, we focus on the\nproperties of the widely-used ReLU activation function, and discover that there\nexist two phenomena (i.e., wrong blocking and over transmission) misleading the\ncalculation of gradients in ReLU during the backpropagation. Both issues\nenlarge the difference between the predicted changes of the loss function from\ngradient and corresponding actual changes, and mislead the gradients which\nresults in larger perturbations. Therefore, we propose a universal adversarial\nexample generation method, called ADV-ReLU, to enhance the performance of\ngradient based white-box attack algorithms. During the backpropagation of the\nnetwork, our approach calculates the gradient of the loss function versus\nnetwork input, maps the values to scores, and selects a part of them to update\nthe misleading gradients. Comprehensive experimental results on \\emph{ImageNet}\ndemonstrate that our ADV-ReLU can be easily integrated into many\nstate-of-the-art gradient-based white-box attack algorithms, as well as\ntransferred to black-box attack attackers, to further decrease perturbations in\nthe ${\\ell _2}$-norm.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 02:13:26 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Liu", "Hongying", ""], ["Zhou", "Zhenyu", ""], ["Shang", "Fanhua", ""], ["Qi", "Xiaoyu", ""], ["Liu", "Yuanyuan", ""], ["Jiao", "Licheng", ""]]}, {"id": "2010.10716", "submitter": "Hui Zhu", "authors": "Hui Zhu, Xiaofang Zhao", "title": "TargetDrop: A Targeted Regularization Method for Convolutional Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dropout regularization has been widely used in deep learning but performs\nless effective for convolutional neural networks since the spatially correlated\nfeatures allow dropped information to still flow through the networks. Some\nstructured forms of dropout have been proposed to address this but prone to\nresult in over or under regularization as features are dropped randomly. In\nthis paper, we propose a targeted regularization method named TargetDrop which\nincorporates the attention mechanism to drop the discriminative feature units.\nSpecifically, it masks out the target regions of the feature maps corresponding\nto the target channels. Experimental results compared with the other methods or\napplied for different networks demonstrate the regularization effect of our\nmethod.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 02:26:05 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Zhu", "Hui", ""], ["Zhao", "Xiaofang", ""]]}, {"id": "2010.10717", "submitter": "Jakob Krzyston", "authors": "Jakob Krzyston, Rajib Bhattacharjea, Andrew Stark", "title": "High-Capacity Complex Convolutional Neural Networks For I/Q Modulation\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  I/Q modulation classification is a unique pattern recognition problem as the\ndata for each class varies in quality, quantified by signal to noise ratio\n(SNR), and has structure in the complex-plane. Previous work shows treating\nthese samples as complex-valued signals and computing complex-valued\nconvolutions within deep learning frameworks significantly increases the\nperformance over comparable shallow CNN architectures. In this work, we claim\nstate of the art performance by enabling high-capacity architectures containing\nresidual and/or dense connections to compute complex-valued convolutions, with\npeak classification accuracy of 92.4% on a benchmark classification problem,\nthe RadioML 2016.10a dataset. We show statistically significant improvements in\nall networks with complex convolutions for I/Q modulation classification.\nComplexity and inference speed analyses show models with complex convolutions\nsubstantially outperform architectures with a comparable number of parameters\nand comparable speed by over 10% in each case.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 02:26:24 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Krzyston", "Jakob", ""], ["Bhattacharjea", "Rajib", ""], ["Stark", "Andrew", ""]]}, {"id": "2010.10732", "submitter": "Yehui Tang", "authors": "Yehui Tang, Yunhe Wang, Yixing Xu, Dacheng Tao, Chunjing Xu, Chao Xu,\n  Chang Xu", "title": "SCOP: Scientific Control for Reliable Neural Network Pruning", "comments": "This paper is accepted by NeurIPS 2020. Key words: Filter Pruning,\n  Adversarial Pruning, Network Compression, CNN, Deep Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a reliable neural network pruning algorithm by setting up\na scientific control. Existing pruning methods have developed various\nhypotheses to approximate the importance of filters to the network and then\nexecute filter pruning accordingly. To increase the reliability of the results,\nwe prefer to have a more rigorous research design by including a scientific\ncontrol group as an essential part to minimize the effect of all factors except\nthe association between the filter and expected network output. Acting as a\ncontrol group, knockoff feature is generated to mimic the feature map produced\nby the network filter, but they are conditionally independent of the example\nlabel given the real feature map. We theoretically suggest that the knockoff\ncondition can be approximately preserved given the information propagation of\nnetwork layers. Besides the real feature map on an intermediate layer, the\ncorresponding knockoff feature is brought in as another auxiliary input signal\nfor the subsequent layers. Redundant filters can be discovered in the\nadversarial process of different features. Through experiments, we demonstrate\nthe superiority of the proposed algorithm over state-of-the-art methods. For\nexample, our method can reduce 57.8% parameters and 60.2% FLOPs of ResNet-101\nwith only 0.01% top-1 accuracy loss on ImageNet. The code is available at\nhttps://github.com/huawei-noah/Pruning/tree/master/SCOP_NeurIPS2020.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 03:02:01 GMT"}, {"version": "v2", "created": "Mon, 11 Jan 2021 03:06:52 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Tang", "Yehui", ""], ["Wang", "Yunhe", ""], ["Xu", "Yixing", ""], ["Tao", "Dacheng", ""], ["Xu", "Chunjing", ""], ["Xu", "Chao", ""], ["Xu", "Chang", ""]]}, {"id": "2010.10744", "submitter": "Ye He", "authors": "Ye He, Chao Zhu, Xu-Cheng Yin", "title": "Mutual-Supervised Feature Modulation Network for Occluded Pedestrian\n  Detection", "comments": "Accepted at ICPR2020, 8 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art pedestrian detectors have achieved significant progress on\nnon-occluded pedestrians, yet they are still struggling under heavy occlusions.\nThe recent occlusion handling strategy of popular two-stage approaches is to\nbuild a two-branch architecture with the help of additional visible body\nannotations. Nonetheless, these methods still have some weaknesses. Either the\ntwo branches are trained independently with only score-level fusion, which\ncannot guarantee the detectors to learn robust enough pedestrian features. Or\nthe attention mechanisms are exploited to only emphasize on the visible body\nfeatures. However, the visible body features of heavily occluded pedestrians\nare concentrated on a relatively small area, which will easily cause missing\ndetections. To address the above issues, we propose in this paper a novel\nMutual-Supervised Feature Modulation (MSFM) network, to better handle occluded\npedestrian detection. The key MSFM module in our network calculates the\nsimilarity loss of full body boxes and visible body boxes corresponding to the\nsame pedestrian so that the full-body detector could learn more complete and\nrobust pedestrian features with the assist of contextual features from the\noccluding parts. To facilitate the MSFM module, we also propose a novel\ntwo-branch architecture, consisting of a standard full body detection branch\nand an extra visible body classification branch. These two branches are trained\nin a mutual-supervised way with full body annotations and visible body\nannotations, respectively. To verify the effectiveness of our proposed method,\nextensive experiments are conducted on two challenging pedestrian datasets:\nCaltech and CityPersons, and our approach achieves superior performance\ncompared to other state-of-the-art methods on both datasets, especially in\nheavy occlusion case.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 03:42:22 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["He", "Ye", ""], ["Zhu", "Chao", ""], ["Yin", "Xu-Cheng", ""]]}, {"id": "2010.10748", "submitter": "Yuchen He", "authors": "Yuchen He", "title": "Underwater Image Color Correction by Complementary Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel approach for underwater image color\ncorrection based on a Tikhonov type optimization model in the CIELAB color\nspace. It presents a new variational interpretation of the complementary\nadaptation theory in psychophysics, which establishes the connection between\ncolorimetric notions and color constancy of the human visual system (HVS).\nUnderstood as a long-term adaptive process, our method effectively removes the\nunderwater color cast and yields a balanced color distribution. For\nvisualization purposes, we enhance the image contrast by properly rescaling\nboth lightness and chroma without trespassing the CIELAB gamut. The magnitude\nof the enhancement is hue-selective and image-based, thus our method is robust\nfor different underwater imaging environments. To improve the uniformity of\nCIELAB, we include an approximate hue-linearization as the pre-processing and\nan inverse transform of the Helmholtz-Kohlrausch effect as the post-processing.\nWe analyze and validate the proposed model by various numerical experiments.\nBased on image quality metrics designed for underwater conditions, we compare\nwith some state-of-art approaches to show that the proposed method has\nconsistently superior performances.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 03:59:22 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["He", "Yuchen", ""]]}, {"id": "2010.10754", "submitter": "Saurabh Bagchi", "authors": "Ran Xu, Chen-lin Zhang, Pengcheng Wang, Jayoung Lee, Subrata Mitra,\n  Somali Chaterji, Yin Li, Saurabh Bagchi", "title": "ApproxDet: Content and Contention-Aware Approximate Object Detection for\n  Mobiles", "comments": "Accepted to appear at the 18th ACM Conference on Embedded Networked\n  Sensor Systems (SenSys), 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advanced video analytic systems, including scene classification and object\ndetection, have seen widespread success in various domains such as smart cities\nand autonomous transportation. With an ever-growing number of powerful client\ndevices, there is incentive to move these heavy video analytics workloads from\nthe cloud to mobile devices to achieve low latency and real-time processing and\nto preserve user privacy. However, most video analytic systems are heavyweight\nand are trained offline with some pre-defined latency or accuracy requirements.\nThis makes them unable to adapt at runtime in the face of three types of\ndynamism -- the input video characteristics change, the amount of compute\nresources available on the node changes due to co-located applications, and the\nuser's latency-accuracy requirements change. In this paper we introduce\nApproxDet, an adaptive video object detection framework for mobile devices to\nmeet accuracy-latency requirements in the face of changing content and resource\ncontention scenarios. To achieve this, we introduce a multi-branch object\ndetection kernel (layered on Faster R-CNN), which incorporates a data-driven\nmodeling approach on the performance metrics, and a latency SLA-driven\nscheduler to pick the best execution branch at runtime. We couple this kernel\nwith approximable video object tracking algorithms to create an end-to-end\nvideo object detection system. We evaluate ApproxDet on a large benchmark video\ndataset and compare quantitatively to AdaScale and YOLOv3. We find that\nApproxDet is able to adapt to a wide variety of contention and content\ncharacteristics and outshines all baselines, e.g., it achieves 52% lower\nlatency and 11.1% higher accuracy over YOLOv3.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 04:11:05 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Xu", "Ran", ""], ["Zhang", "Chen-lin", ""], ["Wang", "Pengcheng", ""], ["Lee", "Jayoung", ""], ["Mitra", "Subrata", ""], ["Chaterji", "Somali", ""], ["Li", "Yin", ""], ["Bagchi", "Saurabh", ""]]}, {"id": "2010.10763", "submitter": "Hrithwik Shalu", "authors": "Joseph N Stember, Hrithwik Shalu", "title": "Reinforcement learning using Deep Q Networks and Q learning accurately\n  localizes brain tumors on MRI with very small training sets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose Supervised deep learning in radiology suffers from notorious inherent\nlimitations: 1) It requires large, hand-annotated data sets, 2) It is\nnon-generalizable, and 3) It lacks explainability and intuition. We have\nrecently proposed Reinforcement Learning to address all threes. However, we\napplied it to images with radiologist eye tracking points, which limits the\nstate-action space. Here we generalize the Deep-Q Learning to a gridworld-based\nenvironment, so that only the images and image masks are required.\n  Materials and Methods We trained a Deep Q network on 30 two-dimensional image\nslices from the BraTS brain tumor database. Each image contained one lesion. We\nthen tested the trained Deep Q network on a separate set of 30 testing set\nimages. For comparison, we also trained and tested a keypoint detection\nsupervised deep learning network for the same set of training / testing images.\n  Results Whereas the supervised approach quickly overfit the training data,\nand predicably performed poorly on the testing set (11\\% accuracy), the Deep-Q\nlearning approach showed progressive improved generalizability to the testing\nset over training time, reaching 70\\% accuracy.\n  Conclusion We have shown a proof-of-principle application of reinforcement\nlearning to radiological images, here using 2D contrast-enhanced MRI brain\nimages with the goal of localizing brain tumors. This represents a\ngeneralization of recent work to a gridworld setting, naturally suitable for\nanalyzing medical images.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 05:00:04 GMT"}, {"version": "v2", "created": "Fri, 19 Mar 2021 05:23:10 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Stember", "Joseph N", ""], ["Shalu", "Hrithwik", ""]]}, {"id": "2010.10771", "submitter": "Boris Ba\\v{c}i\\'c Dr.", "authors": "Boris Ba\\v{c}i\\'c and Jason Zhang", "title": "Towards Real-time Drowsiness Detection for Elderly Care", "comments": "This unpublished paper was accepted by the Conference on Innovative\n  Technologies in Intelligent Systems & Industrial Applications (CITISIA 2020)\n  [https://ieee-citisia.org] and uploaded to ArXiv.org preprint server. The\n  camera-ready copy with DOI should be available in IEEE Xplore sometimes after\n  the conference presentation and copyright transfer to IEEE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The primary focus of this paper is to produce a proof of concept for\nextracting drowsiness information from videos to help elderly living on their\nown. To quantify yawning, eyelid and head movement over time, we extracted 3000\nimages from captured videos for training and testing of deep learning models\nintegrated with OpenCV library. The achieved classification accuracy for eyelid\nand mouth open/close status were between 94.3%-97.2%. Visual inspection of head\nmovement from videos with generated 3D coordinate overlays, indicated clear\nspatiotemporal patterns in collected data (yaw, roll and pitch). Extraction\nmethodology of the drowsiness information as timeseries is applicable to other\ncontexts including support for prior work in privacy-preserving augmented\ncoaching, sport rehabilitation, and integration with big data platform in\nhealthcare.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 05:48:59 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Ba\u010di\u0107", "Boris", ""], ["Zhang", "Jason", ""]]}, {"id": "2010.10772", "submitter": "Jia-Wei Yan", "authors": "Jia-Wei Yan, Ci-Siang Lin, Fu-En Yang, Yu-Jhe Li, Yu-Chiang Frank Wang", "title": "Semantics-Guided Representation Learning with Applications to Visual\n  Synthesis", "comments": "ICPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning interpretable and interpolatable latent representations has been an\nemerging research direction, allowing researchers to understand and utilize the\nderived latent space for further applications such as visual synthesis or\nrecognition. While most existing approaches derive an interpolatable latent\nspace and induces smooth transition in image appearance, it is still not clear\nhow to observe desirable representations which would contain semantic\ninformation of interest. In this paper, we aim to learn meaningful\nrepresentations and simultaneously perform semantic-oriented and\nvisually-smooth interpolation. To this end, we propose an angular\ntriplet-neighbor loss (ATNL) that enables learning a latent representation\nwhose distribution matches the semantic information of interest. With the\nlatent space guided by ATNL, we further utilize spherical semantic\ninterpolation for generating semantic warping of images, allowing synthesis of\ndesirable visual data. Experiments on MNIST and CMU Multi-PIE datasets\nqualitatively and quantitatively verify the effectiveness of our method.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 05:51:17 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Yan", "Jia-Wei", ""], ["Lin", "Ci-Siang", ""], ["Yang", "Fu-En", ""], ["Li", "Yu-Jhe", ""], ["Wang", "Yu-Chiang Frank", ""]]}, {"id": "2010.10778", "submitter": "Xinneng Yang", "authors": "Xinneng Yang, Yan Wu, Junqiao Zhao, Feilin Liu", "title": "Dense Dual-Path Network for Real-time Semantic Segmentation", "comments": "Accepted by ACCV2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic segmentation has achieved remarkable results with high computational\ncost and a large number of parameters. However, real-world applications require\nefficient inference speed on embedded devices. Most previous works address the\nchallenge by reducing depth, width and layer capacity of network, which leads\nto poor performance. In this paper, we introduce a novel Dense Dual-Path\nNetwork (DDPNet) for real-time semantic segmentation under resource\nconstraints. We design a light-weight and powerful backbone with dense\nconnectivity to facilitate feature reuse throughout the whole network and the\nproposed Dual-Path module (DPM) to sufficiently aggregate multi-scale contexts.\nMeanwhile, a simple and effective framework is built with a skip architecture\nutilizing the high-resolution feature maps to refine the segmentation output\nand an upsampling module leveraging context information from the feature maps\nto refine the heatmaps. The proposed DDPNet shows an obvious advantage in\nbalancing accuracy and speed. Specifically, on Cityscapes test dataset, DDPNet\nachieves 75.3% mIoU with 52.6 FPS for an input of 1024 X 2048 resolution on a\nsingle GTX 1080Ti card. Compared with other state-of-the-art methods, DDPNet\nachieves a significant better accuracy with a comparable speed and fewer\nparameters.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 06:11:41 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Yang", "Xinneng", ""], ["Wu", "Yan", ""], ["Zhao", "Junqiao", ""], ["Liu", "Feilin", ""]]}, {"id": "2010.10781", "submitter": "Aydogan Ozcan", "authors": "Luzhe Huang, Yilin Luo, Yair Rivenson, Aydogan Ozcan", "title": "Recurrent neural network-based volumetric fluorescence microscopy", "comments": "17 pages, 7 figures", "journal-ref": "Light: Science & Applications (2021)", "doi": "10.1038/s41377-021-00506-9", "report-no": null, "categories": "physics.optics cs.CV cs.LG physics.bio-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Volumetric imaging of samples using fluorescence microscopy plays an\nimportant role in various fields including physical, medical and life sciences.\nHere we report a deep learning-based volumetric image inference framework that\nuses 2D images that are sparsely captured by a standard wide-field fluorescence\nmicroscope at arbitrary axial positions within the sample volume. Through a\nrecurrent convolutional neural network, which we term as Recurrent-MZ, 2D\nfluorescence information from a few axial planes within the sample is\nexplicitly incorporated to digitally reconstruct the sample volume over an\nextended depth-of-field. Using experiments on C. Elegans and nanobead samples,\nRecurrent-MZ is demonstrated to increase the depth-of-field of a 63x/1.4NA\nobjective lens by approximately 50-fold, also providing a 30-fold reduction in\nthe number of axial scans required to image the same sample volume. We further\nillustrated the generalization of this recurrent network for 3D imaging by\nshowing its resilience to varying imaging conditions, including e.g., different\nsequences of input images, covering various axial permutations and unknown\naxial positioning errors. Recurrent-MZ demonstrates the first application of\nrecurrent neural networks in microscopic image reconstruction and provides a\nflexible and rapid volumetric imaging framework, overcoming the limitations of\ncurrent 3D scanning microscopy tools.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 06:17:38 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Huang", "Luzhe", ""], ["Luo", "Yilin", ""], ["Rivenson", "Yair", ""], ["Ozcan", "Aydogan", ""]]}, {"id": "2010.10802", "submitter": "Itai Gat", "authors": "Itai Gat and Idan Schwartz and Alexander Schwing and Tamir Hazan", "title": "Removing Bias in Multi-modal Classifiers: Regularization by Maximizing\n  Functional Entropies", "comments": "NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Many recent datasets contain a variety of different data modalities, for\ninstance, image, question, and answer data in visual question answering (VQA).\nWhen training deep net classifiers on those multi-modal datasets, the\nmodalities get exploited at different scales, i.e., some modalities can more\neasily contribute to the classification results than others. This is suboptimal\nbecause the classifier is inherently biased towards a subset of the modalities.\nTo alleviate this shortcoming, we propose a novel regularization term based on\nthe functional entropy. Intuitively, this term encourages to balance the\ncontribution of each modality to the classification result. However,\nregularization with the functional entropy is challenging. To address this, we\ndevelop a method based on the log-Sobolev inequality, which bounds the\nfunctional entropy with the functional-Fisher-information. Intuitively, this\nmaximizes the amount of information that the modalities contribute. On the two\nchallenging multi-modal datasets VQA-CPv2 and SocialIQ, we obtain\nstate-of-the-art results while more uniformly exploiting the modalities. In\naddition, we demonstrate the efficacy of our method on Colored MNIST.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 07:40:33 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Gat", "Itai", ""], ["Schwartz", "Idan", ""], ["Schwing", "Alexander", ""], ["Hazan", "Tamir", ""]]}, {"id": "2010.10804", "submitter": "Zhongzheng Ren", "authors": "Zhongzheng Ren, Zhiding Yu, Xiaodong Yang, Ming-Yu Liu, Alexander G.\n  Schwing, Jan Kautz", "title": "UFO$^2$: A Unified Framework towards Omni-supervised Object Detection", "comments": "ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing work on object detection often relies on a single form of\nannotation: the model is trained using either accurate yet costly bounding\nboxes or cheaper but less expressive image-level tags. However, real-world\nannotations are often diverse in form, which challenges these existing works.\nIn this paper, we present UFO$^2$, a unified object detection framework that\ncan handle different forms of supervision simultaneously. Specifically, UFO$^2$\nincorporates strong supervision (e.g., boxes), various forms of partial\nsupervision (e.g., class tags, points, and scribbles), and unlabeled data.\nThrough rigorous evaluations, we demonstrate that each form of label can be\nutilized to either train a model from scratch or to further improve a\npre-trained model. We also use UFO$^2$ to investigate budget-aware\nomni-supervised learning, i.e., various annotation policies are studied under a\nfixed annotation budget: we show that competitive performance needs no strong\nlabels for all data. Finally, we demonstrate the generalization of UFO$^2$,\ndetecting more than 1,000 different objects without bounding box annotations.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 07:46:30 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Ren", "Zhongzheng", ""], ["Yu", "Zhiding", ""], ["Yang", "Xiaodong", ""], ["Liu", "Ming-Yu", ""], ["Schwing", "Alexander G.", ""], ["Kautz", "Jan", ""]]}, {"id": "2010.10842", "submitter": "Ren\\'e Schuster", "authors": "Ren\\'e Schuster, Christian Unger, Didier Stricker", "title": "MonoComb: A Sparse-to-Dense Combination Approach for Monocular Scene\n  Flow", "comments": "Accepted to ACM CSCS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contrary to the ongoing trend in automotive applications towards usage of\nmore diverse and more sensors, this work tries to solve the complex scene flow\nproblem under a monocular camera setup, i.e. using a single sensor. Towards\nthis end, we exploit the latest achievements in single image depth estimation,\noptical flow, and sparse-to-dense interpolation and propose a monocular\ncombination approach (MonoComb) to compute dense scene flow. MonoComb uses\noptical flow to relate reconstructed 3D positions over time and interpolates\noccluded areas. This way, existing monocular methods are outperformed in\ndynamic foreground regions which leads to the second best result among the\ncompetitors on the challenging KITTI 2015 scene flow benchmark.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 09:06:49 GMT"}, {"version": "v2", "created": "Thu, 12 Nov 2020 10:12:40 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Schuster", "Ren\u00e9", ""], ["Unger", "Christian", ""], ["Stricker", "Didier", ""]]}, {"id": "2010.10864", "submitter": "Lucas Smaira", "authors": "Lucas Smaira (DeepMind), Jo\\~ao Carreira (DeepMind), Eric Noland\n  (DeepMind), Ellen Clancy (DeepMind), Amy Wu (DeepMind), Andrew Zisserman\n  (DeepMind)", "title": "A Short Note on the Kinetics-700-2020 Human Action Dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe the 2020 edition of the DeepMind Kinetics human action dataset,\nwhich replenishes and extends the Kinetics-700 dataset. In this new version,\nthere are at least 700 video clips from different YouTube videos for each of\nthe 700 classes. This paper details the changes introduced for this new release\nof the dataset and includes a comprehensive set of statistics as well as\nbaseline results using the I3D network.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 09:47:09 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Smaira", "Lucas", "", "DeepMind"], ["Carreira", "Jo\u00e3o", "", "DeepMind"], ["Noland", "Eric", "", "DeepMind"], ["Clancy", "Ellen", "", "DeepMind"], ["Wu", "Amy", "", "DeepMind"], ["Zisserman", "Andrew", "", "DeepMind"]]}, {"id": "2010.10867", "submitter": "Florian Tschopp", "authors": "Felix Taubner, Florian Tschopp, Tonci Novkovic, Roland Siegwart, Fadri\n  Furrer", "title": "LCD -- Line Clustering and Description for Place Recognition", "comments": "Accepted for International Conference on 3D Vision (3DV) 2020", "journal-ref": "2020 International Conference on 3D Vision (3DV)", "doi": "10.1109/3DV50981.2020.00101", "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current research on visual place recognition mostly focuses on aggregating\nlocal visual features of an image into a single vector representation.\nTherefore, high-level information such as the geometric arrangement of the\nfeatures is typically lost. In this paper, we introduce a novel learning-based\napproach to place recognition, using RGB-D cameras and line clusters as visual\nand geometric features. We state the place recognition problem as a problem of\nrecognizing clusters of lines instead of individual patches, thus maintaining\nstructural information. In our work, line clusters are defined as lines that\nmake up individual objects, hence our place recognition approach can be\nunderstood as object recognition. 3D line segments are detected in RGB-D images\nusing state-of-the-art techniques. We present a neural network architecture\nbased on the attention mechanism for frame-wise line clustering. A similar\nneural network is used for the description of these clusters with a compact\nembedding of 128 floating point numbers, trained with triplet loss on training\ndata obtained from the InteriorNet dataset. We show experiments on a large\nnumber of indoor scenes and compare our method with the bag-of-words\nimage-retrieval approach using SIFT and SuperPoint features and the global\ndescriptor NetVLAD. Trained only on synthetic data, our approach generalizes\nwell to real-world data captured with Kinect sensors, while also providing\ninformation about the geometric arrangement of instances.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 09:52:47 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Taubner", "Felix", ""], ["Tschopp", "Florian", ""], ["Novkovic", "Tonci", ""], ["Siegwart", "Roland", ""], ["Furrer", "Fadri", ""]]}, {"id": "2010.10876", "submitter": "Roberto Bondesan", "authors": "Marc Finzi, Roberto Bondesan, Max Welling", "title": "Probabilistic Numeric Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continuous input signals like images and time series that are irregularly\nsampled or have missing values are challenging for existing deep learning\nmethods. Coherently defined feature representations must depend on the values\nin unobserved regions of the input. Drawing from the work in probabilistic\nnumerics, we propose Probabilistic Numeric Convolutional Neural Networks which\nrepresent features as Gaussian processes (GPs), providing a probabilistic\ndescription of discretization error. We then define a convolutional layer as\nthe evolution of a PDE defined on this GP, followed by a nonlinearity. This\napproach also naturally admits steerable equivariant convolutions under e.g.\nthe rotation group. In experiments we show that our approach yields a $3\\times$\nreduction of error from the previous state of the art on the SuperPixel-MNIST\ndataset and competitive performance on the medical time series dataset\nPhysioNet2012.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 10:08:21 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Finzi", "Marc", ""], ["Bondesan", "Roberto", ""], ["Welling", "Max", ""]]}, {"id": "2010.10888", "submitter": "Tobias Alt", "authors": "Tobias Alt, Joachim Weickert", "title": "Learning Integrodifferential Models for Image Denoising", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce an integrodifferential extension of the edge-enhancing\nanisotropic diffusion model for image denoising. By accumulating weighted\nstructural information on multiple scales, our model is the first to create\nanisotropy through multiscale integration. It follows the philosophy of\ncombining the advantages of model-based and data-driven approaches within\ncompact, insightful, and mathematically well-founded models with improved\nperformance. We explore trained results of scale-adaptive weighting and\ncontrast parameters to obtain an explicit modelling by smooth functions. This\nleads to a transparent model with only three parameters, without significantly\ndecreasing its denoising performance. Experiments demonstrate that it\noutperforms its diffusion-based predecessors. We show that both multiscale\ninformation and anisotropy are crucial for its success.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 10:50:29 GMT"}, {"version": "v2", "created": "Mon, 17 May 2021 09:44:13 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Alt", "Tobias", ""], ["Weickert", "Joachim", ""]]}, {"id": "2010.10897", "submitter": "Th\\'eo Estienne", "authors": "Th\\'eo Estienne, Maria Vakalopoulou, Enzo Battistella, Alexandre\n  Carr\\'e, Th\\'eophraste Henry, Marvin Lerousseau, Charlotte Robert, Nikos\n  Paragios and Eric Deutsch", "title": "Deep learning based registration using spatial gradients and noisy\n  segmentation labels", "comments": "6 pages, 3 figures. Updated version after review modifications.\n  Published to Segmentation, Classification, and Registration of Multi-modality\n  Medical Imaging Data. MICCAI 2020. Lecture Notes in Computer Science, vol\n  12587", "journal-ref": "In: Shusharina N., Heinrich M.P., Huang R. (eds) Segmentation,\n  Classification, and Registration of Multi-modality Medical Imaging Data.\n  MICCAI 2020. Lecture Notes in Computer Science, vol 12587. Springer, Cham", "doi": "10.1007/978-3-030-71827-5_11", "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Image registration is one of the most challenging problems in medical image\nanalysis. In the recent years, deep learning based approaches became quite\npopular, providing fast and performing registration strategies. In this short\npaper, we summarise our work presented on Learn2Reg challenge 2020. The main\ncontributions of our work rely on (i) a symmetric formulation, predicting the\ntransformations from source to target and from target to source simultaneously,\nenforcing the trained representations to be similar and (ii) integration of\nvariety of publicly available datasets used both for pretraining and for\naugmenting segmentation labels. Our method reports a mean dice of $0.64$ for\ntask 3 and $0.85$ for task 4 on the test sets, taking third place on the\nchallenge. Our code and models are publicly available at\nhttps://github.com/TheoEst/abdominal_registration and\n\\https://github.com/TheoEst/hippocampus_registration.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 11:08:45 GMT"}, {"version": "v2", "created": "Fri, 9 Apr 2021 08:42:32 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Estienne", "Th\u00e9o", ""], ["Vakalopoulou", "Maria", ""], ["Battistella", "Enzo", ""], ["Carr\u00e9", "Alexandre", ""], ["Henry", "Th\u00e9ophraste", ""], ["Lerousseau", "Marvin", ""], ["Robert", "Charlotte", ""], ["Paragios", "Nikos", ""], ["Deutsch", "Eric", ""]]}, {"id": "2010.10903", "submitter": "Jon\\'a\\v{s} Kulh\\'anek", "authors": "Jon\\'a\\v{s} Kulh\\'anek and Erik Derner and Robert Babu\\v{s}ka", "title": "Visual Navigation in Real-World Indoor Environments Using End-to-End\n  Deep Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual navigation is essential for many applications in robotics, from\nmanipulation, through mobile robotics to automated driving. Deep reinforcement\nlearning (DRL) provides an elegant map-free approach integrating image\nprocessing, localization, and planning in one module, which can be trained and\ntherefore optimized for a given environment. However, to date, DRL-based visual\nnavigation was validated exclusively in simulation, where the simulator\nprovides information that is not available in the real world, e.g., the robot's\nposition or image segmentation masks. This precludes the use of the learned\npolicy on a real robot. Therefore, we propose a novel approach that enables a\ndirect deployment of the trained policy on real robots. We have designed visual\nauxiliary tasks, a tailored reward scheme, and a new powerful simulator to\nfacilitate domain randomization. The policy is fine-tuned on images collected\nfrom real-world environments. We have evaluated the method on a mobile robot in\na real office environment. The training took ~30 hours on a single GPU. In 30\nnavigation experiments, the robot reached a 0.3-meter neighborhood of the goal\nin more than 86.7% of cases. This result makes the proposed method directly\napplicable to tasks like mobile manipulation.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 11:22:30 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Kulh\u00e1nek", "Jon\u00e1\u0161", ""], ["Derner", "Erik", ""], ["Babu\u0161ka", "Robert", ""]]}, {"id": "2010.10949", "submitter": "Xu Xuecheng", "authors": "Xuecheng Xu, Huan Yin, Zexi Chen, Yue Wang and Rong Xiong", "title": "DiSCO: Differentiable Scan Context with Orientation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Global localization is essential for robot navigation, of which the first\nstep is to retrieve a query from the map database. This problem is called place\nrecognition. In recent years, LiDAR scan based place recognition has drawn\nattention as it is robust against the appearance change. In this paper, we\npropose a LiDAR-based place recognition method, named Differentiable Scan\nContext with Orientation (DiSCO), which simultaneously finds the scan at a\nsimilar place and estimates their relative orientation. The orientation can\nfurther be used as the initial value for the down-stream local optimal metric\npose estimation, improving the pose estimation especially when a large\norientation between the current scan and retrieved scan exists. Our key idea is\nto transform the feature into the frequency domain. We utilize the magnitude of\nthe spectrum as the place signature, which is theoretically rotation-invariant.\nIn addition, based on the differentiable phase correlation, we can efficiently\nestimate the global optimal relative orientation using the spectrum. With such\nstructural constraints, the network can be learned in an end-to-end manner, and\nthe backbone is fully shared by the two tasks, achieving interpretability and\nlight weight. Finally, DiSCO is validated on three datasets with long-term\noutdoor conditions, showing better performance than the compared methods.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 12:38:21 GMT"}, {"version": "v2", "created": "Wed, 13 Jan 2021 06:57:45 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Xu", "Xuecheng", ""], ["Yin", "Huan", ""], ["Chen", "Zexi", ""], ["Wang", "Yue", ""], ["Xiong", "Rong", ""]]}, {"id": "2010.10952", "submitter": "Leon Lang", "authors": "Leon Lang, Maurice Weiler", "title": "A Wigner-Eckart Theorem for Group Equivariant Convolution Kernels", "comments": "100 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Group equivariant convolutional networks (GCNNs) endow classical\nconvolutional networks with additional symmetry priors, which can lead to a\nconsiderably improved performance. Recent advances in the theoretical\ndescription of GCNNs revealed that such models can generally be understood as\nperforming convolutions with G-steerable kernels, that is, kernels that satisfy\nan equivariance constraint themselves. While the G-steerability constraint has\nbeen derived, it has to date only been solved for specific use cases - a\ngeneral characterization of G-steerable kernel spaces is still missing. This\nwork provides such a characterization for the practically relevant case of G\nbeing any compact group. Our investigation is motivated by a striking analogy\nbetween the constraints underlying steerable kernels on the one hand and\nspherical tensor operators from quantum mechanics on the other hand. By\ngeneralizing the famous Wigner-Eckart theorem for spherical tensor operators,\nwe prove that steerable kernel spaces are fully understood and parameterized in\nterms of 1) generalized reduced matrix elements, 2) Clebsch-Gordan\ncoefficients, and 3) harmonic basis functions on homogeneous spaces.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 12:42:23 GMT"}, {"version": "v2", "created": "Thu, 22 Oct 2020 17:12:54 GMT"}, {"version": "v3", "created": "Tue, 15 Dec 2020 21:45:22 GMT"}, {"version": "v4", "created": "Thu, 21 Jan 2021 10:00:28 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Lang", "Leon", ""], ["Weiler", "Maurice", ""]]}, {"id": "2010.10957", "submitter": "Xiangyue Liu", "authors": "Kai Jiang (1), Xiangyue Liu (2), Zheng Ju (3), Xiang Luo (1)((1)\n  LinkDoc Technology, Beijing, China, (2) School of Software, Beihang\n  University, Beijing, China, (3) Huaxin consulting Co., Ltd, Hangzhou, China)", "title": "2nd Place Solution to Instance Segmentation of IJCAI 3D AI Challenge\n  2020", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compared with MS-COCO, the dataset for the competition has a larger\nproportion of large objects which area is greater than 96x96 pixels. As getting\nfine boundaries is vitally important for large object segmentation, Mask R-CNN\nwith PointRend is selected as the base segmentation framework to output\nhigh-quality object boundaries. Besides, a better engine that integrates\nResNeSt, FPN and DCNv2, and a range of effective tricks that including\nmulti-scale training and test time augmentation are applied to improve\nsegmentation performance. Our best performance is an ensemble of four models\n(three PointRend-based models and SOLOv2), which won the 2nd place in\nIJCAI-PRICAI 3D AI Challenge 2020: Instance Segmentation.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 12:53:01 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Jiang", "Kai", ""], ["Liu", "Xiangyue", ""], ["Ju", "Zheng", ""], ["Luo", "Xiang", ""]]}, {"id": "2010.10959", "submitter": "Francois Darmon", "authors": "Fran\\c{c}ois Darmon and Mathieu Aubry and Pascal Monasse", "title": "Learning to Guide Local Feature Matches", "comments": "Accepted to 3DV 2020", "journal-ref": null, "doi": "10.1109/3DV50981.2020.00123", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We tackle the problem of finding accurate and robust keypoint correspondences\nbetween images. We propose a learning-based approach to guide local feature\nmatches via a learned approximate image matching. Our approach can boost the\nresults of SIFT to a level similar to state-of-the-art deep descriptors, such\nas Superpoint, ContextDesc, or D2-Net and can improve performance for these\ndescriptors. We introduce and study different levels of supervision to learn\ncoarse correspondences. In particular, we show that weak supervision from\nepipolar geometry leads to performances higher than the stronger but more\nbiased point level supervision and is a clear improvement over weak image level\nsupervision. We demonstrate the benefits of our approach in a variety of\nconditions by evaluating our guided keypoint correspondences for localization\nof internet images on the YFCC100M dataset and indoor images on theSUN3D\ndataset, for robust localization on the Aachen day-night benchmark and for 3D\nreconstruction in challenging conditions using the LTLL historical image data.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 12:53:36 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Darmon", "Fran\u00e7ois", ""], ["Aubry", "Mathieu", ""], ["Monasse", "Pascal", ""]]}, {"id": "2010.10968", "submitter": "Huu Le", "authors": "Huu Le, Christopher Zach, Edward Rosten and Oliver J. Woodford", "title": "Progressive Batching for Efficient Non-linear Least Squares", "comments": "Accepted to ACCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-linear least squares solvers are used across a broad range of offline and\nreal-time model fitting problems. Most improvements of the basic Gauss-Newton\nalgorithm tackle convergence guarantees or leverage the sparsity of the\nunderlying problem structure for computational speedup. With the success of\ndeep learning methods leveraging large datasets, stochastic optimization\nmethods received recently a lot of attention. Our work borrows ideas from both\nstochastic machine learning and statistics, and we present an approach for\nnon-linear least-squares that guarantees convergence while at the same time\nsignificantly reduces the required amount of computation. Empirical results\nshow that our proposed method achieves competitive convergence rates compared\nto traditional second-order approaches on common computer vision problems, such\nas image alignment and essential matrix estimation, with very large numbers of\nresiduals.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 13:00:04 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Le", "Huu", ""], ["Zach", "Christopher", ""], ["Rosten", "Edward", ""], ["Woodford", "Oliver J.", ""]]}, {"id": "2010.10979", "submitter": "Koichiro Niinuma", "authors": "Koichiro Niinuma, Itir Onal Ertugrul, Jeffrey F Cohn, L\\'aszl\\'o A\n  Jeni", "title": "Synthetic Expressions are Better Than Real for Learning to Detect Facial\n  Actions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Critical obstacles in training classifiers to detect facial actions are the\nlimited sizes of annotated video databases and the relatively low frequencies\nof occurrence of many actions. To address these problems, we propose an\napproach that makes use of facial expression generation. Our approach\nreconstructs the 3D shape of the face from each video frame, aligns the 3D mesh\nto a canonical view, and then trains a GAN-based network to synthesize novel\nimages with facial action units of interest. To evaluate this approach, a deep\nneural network was trained on two separate datasets: One network was trained on\nvideo of synthesized facial expressions generated from FERA17; the other\nnetwork was trained on unaltered video from the same database. Both networks\nused the same train and validation partitions and were tested on the test\npartition of actual video from FERA17. The network trained on synthesized\nfacial expressions outperformed the one trained on actual facial expressions\nand surpassed current state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 13:11:45 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Niinuma", "Koichiro", ""], ["Ertugrul", "Itir Onal", ""], ["Cohn", "Jeffrey F", ""], ["Jeni", "L\u00e1szl\u00f3 A", ""]]}, {"id": "2010.11008", "submitter": "Camila Gonzalez", "authors": "Camila Gonzalez, Georgios Sakas and Anirban Mukhopadhyay", "title": "What is Wrong with Continual Learning in Medical Image Segmentation?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Continual learning protocols are attracting increasing attention from the\nmedical imaging community. In a continual setup, data from different sources\narrives sequentially and each batch is only available for a limited period.\nGiven the inherent privacy risks associated with medical data, this setup\nreflects the reality of deployment for deep learning diagnostic radiology\nsystems. Many techniques exist to learn continuously for classification tasks,\nand several have been adapted to semantic segmentation. Yet most have at least\none of the following flaws: a) they rely too heavily on domain identity\ninformation during inference, or b) data as seen in early training stages does\nnot profit from training with later data. In this work, we propose an\nevaluation framework that addresses both concerns, and introduce a fair\nmulti-model benchmark. We show that the benchmark outperforms two popular\ncontinual learning methods for the task of T2-weighted MR prostate\nsegmentation.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 13:48:37 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Gonzalez", "Camila", ""], ["Sakas", "Georgios", ""], ["Mukhopadhyay", "Anirban", ""]]}, {"id": "2010.11029", "submitter": "Derek Hoiem", "authors": "Derek Hoiem, Tanmay Gupta, Zhizhong Li, Michal M. Shlapentokh-Rothman", "title": "Learning Curves for Analysis of Deep Networks", "comments": "Improved text and figure organization, additional experiments on\n  optimization", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning curves model a classifier's test error as a function of the number\nof training samples. Prior works show that learning curves can be used to\nselect model parameters and extrapolate performance. We investigate how to use\nlearning curves to evaluate design choices, such as pretraining, architecture,\nand data augmentation. We propose a method to robustly estimate learning\ncurves, abstract their parameters into error and data-reliance, and evaluate\nthe effectiveness of different parameterizations. Our experiments exemplify use\nof learning curves for analysis and yield several interesting observations.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 14:20:05 GMT"}, {"version": "v2", "created": "Mon, 5 Apr 2021 17:01:02 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Hoiem", "Derek", ""], ["Gupta", "Tanmay", ""], ["Li", "Zhizhong", ""], ["Shlapentokh-Rothman", "Michal M.", ""]]}, {"id": "2010.11081", "submitter": "Haley Abramson", "authors": "Haley G. Abramson, Dan M. Popescu, Rebecca Yu, Changxin Lai, Julie K.\n  Shade, Katherine C. Wu, Mauro Maggioni, Natalia A. Trayanova", "title": "Anatomically-Informed Deep Learning on Contrast-Enhanced Cardiac MRI for\n  Scar Segmentation and Clinical Feature Extraction", "comments": "Haley G. Abramson and Dan M. Popescu contributed equally to this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visualizing disease-induced scarring and fibrosis in the heart on cardiac\nmagnetic resonance (CMR) imaging with contrast enhancement (LGE) is paramount\nin characterizing disease progression and quantifying pathophysiological\nsubstrates of arrhythmias. However, segmentation and scar/fibrosis\nidentification from LGE-CMR is an intensive manual process prone to large\ninter-observer variability. Here, we present a novel fully-automated\nanatomically-informed deep learning solution for left ventricle (LV) and\nscar/fibrosis segmentation and clinical feature extraction from LGE-CMR. The\ntechnology involves three cascading convolutional neural networks that segment\nmyocardium and scar/fibrosis from raw LGE-CMR images and constrain these\nsegmentations within anatomical guidelines, thus facilitating seamless\nderivation of clinically-significant parameters. In addition to available\nLGE-CMR images, training used \"LGE-like\" synthetically enhanced cine scans.\nResults show excellent agreement with those of trained experts in terms of\nsegmentation (balanced accuracy of $96\\%$ and $75\\%$ for LV and scar\nsegmentation), clinical features ($2\\%$ difference in mean scar-to-LV wall\nvolume fraction), and anatomical fidelity. Our segmentation technology is\nextendable to other computer vision medical applications and to problems\nrequiring guidelines adherence of predicted outputs.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 15:43:08 GMT"}, {"version": "v2", "created": "Fri, 8 Jan 2021 21:04:30 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Abramson", "Haley G.", ""], ["Popescu", "Dan M.", ""], ["Yu", "Rebecca", ""], ["Lai", "Changxin", ""], ["Shade", "Julie K.", ""], ["Wu", "Katherine C.", ""], ["Maggioni", "Mauro", ""], ["Trayanova", "Natalia A.", ""]]}, {"id": "2010.11083", "submitter": "Chen Tang", "authors": "Chen Tang, Wenyu Sun, Zhuqing Yuan, Yongpan Liu", "title": "Adaptive Pixel-wise Structured Sparse Network for Efficient CNNs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To accelerate deep CNN models, this paper proposes a novel spatially adaptive\nframework that can dynamically generate pixel-wise sparsity according to the\ninput image. The sparse scheme is pixel-wise refined, regional adaptive under a\nunified importance map, which makes it friendly to hardware implementation. A\nsparse controlling method is further presented to enable online adjustment for\napplications with different precision/latency requirements. The sparse model is\napplicable to a wide range of vision tasks. Experimental results show that this\nmethod efficiently improve the computing efficiency for both image\nclassification using ResNet-18 and super resolution using SRResNet. On image\nclassification task, our method can save 30%-70% MACs with a slightly drop in\ntop-1 and top-5 accuracy. On super resolution task, our method can reduce more\nthan 90% MACs while only causing around 0.1 dB and 0.01 decreasing in PSNR and\nSSIM. Hardware validation is also included.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 15:47:13 GMT"}, {"version": "v2", "created": "Thu, 18 Mar 2021 14:56:47 GMT"}, {"version": "v3", "created": "Sat, 20 Mar 2021 08:46:42 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Tang", "Chen", ""], ["Sun", "Wenyu", ""], ["Yuan", "Zhuqing", ""], ["Liu", "Yongpan", ""]]}, {"id": "2010.11087", "submitter": "Micha{\\l} Stypu{\\l}kowski", "authors": "Micha{\\l} Stypu{\\l}kowski, Kacper Kania, Maciej Zamorski, Maciej\n  Zi\\k{e}ba, Tomasz Trzci\\'nski, Jan Chorowski", "title": "Representing Point Clouds with Generative Conditional Invertible Flow\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a simple yet effective method to represent point\nclouds as sets of samples drawn from a cloud-specific probability distribution.\nThis interpretation matches intrinsic characteristics of point clouds: the\nnumber of points and their ordering within a cloud is not important as all\npoints are drawn from the proximity of the object boundary. We postulate to\nrepresent each cloud as a parameterized probability distribution defined by a\ngenerative neural network. Once trained, such a model provides a natural\nframework for point cloud manipulation operations, such as aligning a new cloud\ninto a default spatial orientation. To exploit similarities between same-class\nobjects and to improve model performance, we turn to weight sharing: networks\nthat model densities of points belonging to objects in the same family share\nall parameters with the exception of a small, object-specific embedding vector.\nWe show that these embedding vectors capture semantic relationships between\nobjects. Our method leverages generative invertible flow networks to learn\nembeddings as well as to generate point clouds. Thanks to this formulation and\ncontrary to similar approaches, we are able to train our model in an end-to-end\nfashion. As a result, our model offers competitive or superior quantitative\nresults on benchmark datasets, while enabling unprecedented capabilities to\nperform cloud manipulation tasks, such as point cloud registration and\nregeneration, by a generative network.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 18:30:47 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Stypu\u0142kowski", "Micha\u0142", ""], ["Kania", "Kacper", ""], ["Zamorski", "Maciej", ""], ["Zi\u0119ba", "Maciej", ""], ["Trzci\u0144ski", "Tomasz", ""], ["Chorowski", "Jan", ""]]}, {"id": "2010.11106", "submitter": "Weikai Tan", "authors": "Weikai Tan, Dedong Zhang, Lingfei Ma, Ying Li, Lanying Wang, and\n  Jonathan Li", "title": "UAV LiDAR Point Cloud Segmentation of A Stack Interchange with Deep\n  Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stack interchanges are essential components of transportation systems. Mobile\nlaser scanning (MLS) systems have been widely used in road infrastructure\nmapping, but accurate mapping of complicated multi-layer stack interchanges are\nstill challenging. This study examined the point clouds collected by a new\nUnmanned Aerial Vehicle (UAV) Light Detection and Ranging (LiDAR) system to\nperform the semantic segmentation task of a stack interchange. An end-to-end\nsupervised 3D deep learning framework was proposed to classify the point\nclouds. The proposed method has proven to capture 3D features in complicated\ninterchange scenarios with stacked convolution and the result achieved over 93%\nclassification accuracy. In addition, the new low-cost semi-solid-state LiDAR\nsensor Livox Mid-40 featuring a incommensurable rosette scanning pattern has\ndemonstrated its potential in high-definition urban mapping.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 16:15:41 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Tan", "Weikai", ""], ["Zhang", "Dedong", ""], ["Ma", "Lingfei", ""], ["Li", "Ying", ""], ["Wang", "Lanying", ""], ["Li", "Jonathan", ""]]}, {"id": "2010.11113", "submitter": "Christian Bartz", "authors": "Christian Bartz, Joseph Bethge, Haojin Yang, Christoph Meinel", "title": "One Model to Reconstruct Them All: A Novel Way to Use the Stochastic\n  Noise in StyleGAN", "comments": "Code and Models are available at\n  https://github.com/Bartzi/one-model-to-reconstruct-them-all", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Networks (GANs) have achieved state-of-the-art\nperformance for several image generation and manipulation tasks. Different\nworks have improved the limited understanding of the latent space of GANs by\nembedding images into specific GAN architectures to reconstruct the original\nimages. We present a novel StyleGAN-based autoencoder architecture, which can\nreconstruct images with very high quality across several data domains. We\ndemonstrate a previously unknown grade of generalizablility by training the\nencoder and decoder independently and on different datasets. Furthermore, we\nprovide new insights about the significance and capabilities of noise inputs of\nthe well-known StyleGAN architecture. Our proposed architecture can handle up\nto 40 images per second on a single GPU, which is approximately 28x faster than\nprevious approaches. Finally, our model also shows promising results, when\ncompared to the state-of-the-art on the image denoising task, although it was\nnot explicitly designed for this task.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 16:24:07 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Bartz", "Christian", ""], ["Bethge", "Joseph", ""], ["Yang", "Haojin", ""], ["Meinel", "Christoph", ""]]}, {"id": "2010.11126", "submitter": "Jonas Bialopetravi\\v{c}ius", "authors": "J. Bialopetravi\\v{c}ius, D. Narbutis", "title": "Study of star clusters in the M83 galaxy with a convolutional neural\n  network", "comments": "14 pages, 9 figures, 1 table", "journal-ref": null, "doi": "10.3847/1538-3881/abbf53", "report-no": null, "categories": "astro-ph.GA cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a study of evolutionary and structural parameters of star cluster\ncandidates in the spiral galaxy M83. For this we use a convolutional neural\nnetwork trained on mock clusters and capable of fast identification and\nlocalization of star clusters, as well as inference of their parameters from\nmulti-band images. We use this pipeline to detect 3,380 cluster candidates in\nHubble Space Telescope observations. The sample of cluster candidates shows an\nage gradient across the galaxy's spiral arms, which is in good agreement with\npredictions of the density wave theory and other studies. As measured from the\ndust lanes of the spiral arms, the younger population of cluster candidates\npeaks at the distance of $\\sim$0.4 kpc while the older candidates are more\ndispersed, but shifted towards $\\gtrsim$0.7 kpc in the leading part of the\nspiral arms. We find high extinction cluster candidates positioned in the\ntrailing part of the spiral arms, close to the dust lanes. We also find a large\nnumber of dense older clusters near the center of the galaxy and a slight\nincrease of the typical cluster size further from the center.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 16:35:09 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Bialopetravi\u010dius", "J.", ""], ["Narbutis", "D.", ""]]}, {"id": "2010.11158", "submitter": "Radu Tudor Ionescu", "authors": "Antonio Barbalau, Adrian Cosma, Radu Tudor Ionescu, Marius Popescu", "title": "Black-Box Ripper: Copying black-box models using generative evolutionary\n  algorithms", "comments": "Accepted as Oral at NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the task of replicating the functionality of black-box neural\nmodels, for which we only know the output class probabilities provided for a\nset of input images. We assume back-propagation through the black-box model is\nnot possible and its training images are not available, e.g. the model could be\nexposed only through an API. In this context, we present a teacher-student\nframework that can distill the black-box (teacher) model into a student model\nwith minimal accuracy loss. To generate useful data samples for training the\nstudent, our framework (i) learns to generate images on a proxy data set (with\nimages and classes different from those used to train the black-box) and (ii)\napplies an evolutionary strategy to make sure that each generated data sample\nexhibits a high response for a specific class when given as input to the black\nbox. Our framework is compared with several baseline and state-of-the-art\nmethods on three benchmark data sets. The empirical evidence indicates that our\nmodel is superior to the considered baselines. Although our method does not\nback-propagate through the black-box network, it generally surpasses\nstate-of-the-art methods that regard the teacher as a glass-box model. Our code\nis available at: https://github.com/antoniobarbalau/black-box-ripper.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 17:25:23 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Barbalau", "Antonio", ""], ["Cosma", "Adrian", ""], ["Ionescu", "Radu Tudor", ""], ["Popescu", "Marius", ""]]}, {"id": "2010.11159", "submitter": "Yi Fang", "authors": "Hao Huang, Lingjing Wang, Xiang Li, Yi Fang", "title": "3D Meta Point Signature: Learning to Learn 3D Point Signature for 3D\n  Dense Shape Correspondence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point signature, a representation describing the structural neighborhood of a\npoint in 3D shapes, can be applied to establish correspondences between points\nin 3D shapes. Conventional methods apply a weight-sharing network, e.g., any\nkind of graph neural networks, across all neighborhoods to directly generate\npoint signatures and gain the generalization ability by extensive training over\na large amount of training samples from scratch. However, these methods lack\nthe flexibility in rapidly adapting to unseen neighborhood structures and thus\ngeneralizes poorly on new point sets. In this paper, we propose a novel\nmeta-learning based 3D point signature model, named 3Dmetapointsignature (MEPS)\nnetwork, that is capable of learning robust point signatures in 3D shapes. By\nregarding each point signature learning process as a task, our method obtains\nan optimized model over the best performance on the distribution of all tasks,\ngenerating reliable signatures for new tasks, i.e., signatures of unseen point\nneighborhoods. Specifically, the MEPS consists of two modules: a base signature\nlearner and a meta signature learner. During training, the base-learner is\ntrained to perform specific signature learning tasks. In the meantime, the\nmeta-learner is trained to update the base-learner with optimal parameters.\nDuring testing, the meta-learner that is learned with the distribution of all\ntasks can adaptively change parameters of the base-learner, accommodating to\nunseen local neighborhoods. We evaluate the MEPS model on two datasets, e.g.,\nFAUST and TOSCA, for dense 3Dshape correspondence. Experimental results\ndemonstrate that our method not only gains significant improvements over the\nbaseline model and achieves state-of-the-art results, but also is capable of\nhandling unseen 3D shapes.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 17:27:39 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Huang", "Hao", ""], ["Wang", "Lingjing", ""], ["Li", "Xiang", ""], ["Fang", "Yi", ""]]}, {"id": "2010.11162", "submitter": "Sandipan Banerjee", "authors": "Ajjen Joshi, Survi Kyal, Sandipan Banerjee, Taniya Mishra", "title": "In-the-wild Drowsiness Detection from Facial Expressions", "comments": "Paper from HSIM Workshop at IEEE Intelligent Vehicles Symposium 2020\n  (IV2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Driving in a state of drowsiness is a major cause of road accidents,\nresulting in tremendous damage to life and property. Developing robust,\nautomatic, real-time systems that can infer drowsiness states of drivers has\nthe potential of making life-saving impact. However, developing drowsiness\ndetection systems that work well in real-world scenarios is challenging because\nof the difficulties associated with collecting high-volume realistic drowsy\ndata and modeling the complex temporal dynamics of evolving drowsy states. In\nthis paper, we propose a data collection protocol that involves outfitting\nvehicles of overnight shift workers with camera kits that record their faces\nwhile driving. We develop a drowsiness annotation guideline to enable humans to\nlabel the collected videos into 4 levels of drowsiness: `alert', `slightly\ndrowsy', `moderately drowsy' and `extremely drowsy'. We experiment with\ndifferent convolutional and temporal neural network architectures to predict\ndrowsiness states from pose, expression and emotion-based representation of the\ninput video of the driver's face. Our best performing model achieves a macro\nROC-AUC of 0.78, compared to 0.72 for a baseline model.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 17:28:56 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Joshi", "Ajjen", ""], ["Kyal", "Survi", ""], ["Banerjee", "Sandipan", ""], ["Mishra", "Taniya", ""]]}, {"id": "2010.11188", "submitter": "Thao Ha", "authors": "Ha Thi Phuong Thao, Balamurali B.T., Dorien Herremans and Gemma Roig", "title": "AttendAffectNet: Self-Attention based Networks for Predicting Affective\n  Responses from Movies", "comments": "8 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CV eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose different variants of the self-attention based\nnetwork for emotion prediction from movies, which we call AttendAffectNet. We\ntake both audio and video into account and incorporate the relation among\nmultiple modalities by applying self-attention mechanism in a novel manner into\nthe extracted features for emotion prediction. We compare it to the typically\ntemporal integration of the self-attention based model, which in our case,\nallows to capture the relation of temporal representations of the movie while\nconsidering the sequential dependencies of emotion responses. We demonstrate\nthe effectiveness of our proposed architectures on the extended COGNIMUSE\ndataset [1], [2] and the MediaEval 2016 Emotional Impact of Movies Task [3],\nwhich consist of movies with emotion annotations. Our results show that\napplying the self-attention mechanism on the different audio-visual features,\nrather than in the time domain, is more effective for emotion prediction. Our\napproach is also proven to outperform many state-ofthe-art models for emotion\nprediction. The code to reproduce our results with the models' implementation\nis available at: https://github.com/ivyha010/AttendAffectNet.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 05:13:24 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Thao", "Ha Thi Phuong", ""], ["T.", "Balamurali B.", ""], ["Herremans", "Dorien", ""], ["Roig", "Gemma", ""]]}, {"id": "2010.11248", "submitter": "Yuki Kawana", "authors": "Yuki Kawana, Yusuke Mukuta, Tatsuya Harada", "title": "Neural Star Domain as Primitive Representation", "comments": "34th Conference on Neural Information Processing Systems (NeurIPS\n  2020), Vancouver, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reconstructing 3D objects from 2D images is a fundamental task in computer\nvision. Accurate structured reconstruction by parsimonious and semantic\nprimitive representation further broadens its application. When reconstructing\na target shape with multiple primitives, it is preferable that one can\ninstantly access the union of basic properties of the shape such as collective\nvolume and surface, treating the primitives as if they are one single shape.\nThis becomes possible by primitive representation with unified implicit and\nexplicit representations. However, primitive representations in current\napproaches do not satisfy all of the above requirements at the same time. To\nsolve this problem, we propose a novel primitive representation named neural\nstar domain (NSD) that learns primitive shapes in the star domain. We show that\nNSD is a universal approximator of the star domain and is not only parsimonious\nand semantic but also an implicit and explicit shape representation. We\ndemonstrate that our approach outperforms existing methods in image\nreconstruction tasks, semantic capabilities, and speed and quality of sampling\nhigh-resolution meshes.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 19:05:16 GMT"}, {"version": "v2", "created": "Thu, 12 Nov 2020 14:22:27 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Kawana", "Yuki", ""], ["Mukuta", "Yusuke", ""], ["Harada", "Tatsuya", ""]]}, {"id": "2010.11289", "submitter": "Wolfgang Kratsch", "authors": "Wolfgang Kratsch, Fabian K\\\"onig, Maximilian R\\\"oglinger", "title": "Shedding Light on Blind Spots: Developing a Reference Architecture to\n  Leverage Video Data for Process Mining", "comments": "Submitted to Information Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Process mining is one of the most active research streams in business process\nmanagement. In recent years, numerous methods have been proposed for analyzing\nstructured process data. Yet, in many cases, it is only the digitized parts of\nprocesses that are directly captured from process-aware information systems,\nand manual activities often result in blind spots. While the use of video\ncameras to observe these activities could help to fill this gap, a standardized\napproach to extracting event logs from unstructured video data remains lacking.\nHere, we propose a reference architecture to bridge the gap between computer\nvision and process mining. Various evaluation activities (i.e., competing\nartifact analysis, prototyping, and real-world application) ensured that the\nproposed reference architecture allows flexible, use-case-driven, and\ncontext-specific instantiations. Our results also show that an exemplary\nsoftware prototype instantiation of the proposed reference architecture is\ncapable of automatically extracting most of the process-relevant events from\nunstructured video data.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 20:01:52 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Kratsch", "Wolfgang", ""], ["K\u00f6nig", "Fabian", ""], ["R\u00f6glinger", "Maximilian", ""]]}, {"id": "2010.11290", "submitter": "Huy Vu", "authors": "Huy Vu, Gene Cheung and Yonina C. Eldar", "title": "Unrolling of Deep Graph Total Variation for Image Denoising", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While deep learning (DL) architectures like convolutional neural networks\n(CNNs) have enabled effective solutions in image denoising, in general their\nimplementations overly rely on training data, lack interpretability, and\nrequire tuning of a large parameter set. In this paper, we combine classical\ngraph signal filtering with deep feature learning into a competitive hybrid\ndesign -- one that utilizes interpretable analytical low-pass graph filters and\nemploys 80% fewer network parameters than state-of-the-art DL denoising scheme\nDnCNN. Specifically, to construct a suitable similarity graph for graph\nspectral filtering, we first adopt a CNN to learn feature representations per\npixel, and then compute feature distances to establish edge weights. Given a\nconstructed graph, we next formulate a convex optimization problem for\ndenoising using a graph total variation (GTV) prior. Via a $l_1$ graph\nLaplacian reformulation, we interpret its solution in an iterative procedure as\na graph low-pass filter and derive its frequency response. For fast filter\nimplementation, we realize this response using a Lanczos approximation.\nExperimental results show that in the case of statistical mistmatch, our\nalgorithm outperformed DnCNN by up to 3dB in PSNR.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 20:04:22 GMT"}, {"version": "v2", "created": "Wed, 24 Mar 2021 02:04:56 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Vu", "Huy", ""], ["Cheung", "Gene", ""], ["Eldar", "Yonina C.", ""]]}, {"id": "2010.11297", "submitter": "Halima Bouzidi", "authors": "Halima Bouzidi, Hamza Ouarnoughi, Smail Niar and Abdessamad Ait El\n  Cadi", "title": "Performance Prediction for Convolutional Neural Networks in Edge Devices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Running Convolutional Neural Network (CNN) based applications on edge devices\nnear the source of data can meet the latency and privacy challenges. However\ndue to their reduced computing resources and their energy constraints, these\nedge devices can hardly satisfy CNN needs in processing and data storage. For\nthese platforms, choosing the CNN with the best trade-off between accuracy and\nexecution time while respecting Hardware constraints is crucial. In this paper,\nwe present and compare five (5) of the widely used Machine Learning based\nmethods for execution time prediction of CNNs on two (2) edge GPU platforms.\nFor these 5 methods, we also explore the time needed for their training and\ntuning their corresponding hyperparameters. Finally, we compare times to run\nthe prediction models on different platforms. The utilization of these methods\nwill highly facilitate design space exploration by providing quickly the best\nCNN on a target edge GPU. Experimental results show that eXtreme Gradient\nBoosting (XGBoost) provides a less than 14.73% average prediction error even\nfor unexplored and unseen CNN models' architectures. Random Forest (RF) depicts\ncomparable accuracy but needs more effort and time to be trained. The other 3\napproaches (OLS, MLP and SVR) are less accurate for CNN performances\nestimation.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 20:21:25 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Bouzidi", "Halima", ""], ["Ouarnoughi", "Hamza", ""], ["Niar", "Smail", ""], ["Cadi", "Abdessamad Ait El", ""]]}, {"id": "2010.11339", "submitter": "Andrea Tagliasacchi Dr", "authors": "Soroosh Yazdani and Andrea Tagliasacchi", "title": "Voronoi Convolutional Neural Networks", "comments": "Technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this technical report, we investigate extending convolutional neural\nnetworks to the setting where functions are not sampled in a grid pattern. We\nshow that by treating the samples as the average of a function within a cell,\nwe can find a natural equivalent of most layers used in CNN. We also present an\nalgorithm for running inference for these models exactly using standard convex\ngeometry algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 22:42:19 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Yazdani", "Soroosh", ""], ["Tagliasacchi", "Andrea", ""]]}, {"id": "2010.11352", "submitter": "Alessandro Lameiras Koerich", "authors": "Mohammad Esmaeilpour, Patrick Cardinal, Alessandro Lameiras Koerich", "title": "Class-Conditional Defense GAN Against End-to-End Speech Attacks", "comments": "5 pages", "journal-ref": "46th IEEE International Conference on Acoustics, Speech, & Signal\n  Processing (ICASSP), 2021", "doi": null, "report-no": null, "categories": "cs.SD cs.CR cs.CV cs.LG eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a novel defense approach against end-to-end\nadversarial attacks developed to fool advanced speech-to-text systems such as\nDeepSpeech and Lingvo. Unlike conventional defense approaches, the proposed\napproach does not directly employ low-level transformations such as\nautoencoding a given input signal aiming at removing potential adversarial\nperturbation. Instead of that, we find an optimal input vector for a class\nconditional generative adversarial network through minimizing the relative\nchordal distance adjustment between a given test input and the generator\nnetwork. Then, we reconstruct the 1D signal from the synthesized spectrogram\nand the original phase information derived from the given input signal. Hence,\nthis reconstruction does not add any extra noise to the signal and according to\nour experimental results, our defense-GAN considerably outperforms conventional\ndefense algorithms both in terms of word error rate and sentence level\nrecognition accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 00:02:02 GMT"}, {"version": "v2", "created": "Sat, 20 Feb 2021 02:51:55 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Esmaeilpour", "Mohammad", ""], ["Cardinal", "Patrick", ""], ["Koerich", "Alessandro Lameiras", ""]]}, {"id": "2010.11353", "submitter": "Arash Raftari", "authors": "Ehsan Emad Marvasti, Arash Raftari, Amir Emad Marvasti, Yaser P.\n  Fallah", "title": "Bandwidth-Adaptive Feature Sharing for Cooperative LIDAR Object\n  Detection", "comments": "8 pages, 4 figures, 2 table, 2020 IEEE 3rd Connected and Automated\n  Vehicles Symposium: IEEE CAVS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Situational awareness as a necessity in the connected and autonomous vehicles\n(CAV) domain is the subject of a significant number of researches in recent\nyears. The driver's safety is directly dependent on the robustness,\nreliability, and scalability of such systems. Cooperative mechanisms have\nprovided a solution to improve situational awareness by utilizing high speed\nwireless vehicular networks. These mechanisms mitigate problems such as\nocclusion and sensor range limitation. However, the network capacity is a\nfactor determining the maximum amount of information being shared among\ncooperative entities. The notion of feature sharing, proposed in our previous\nwork, aims to address these challenges by maintaining a balance between\ncomputation and communication load. In this work, we propose a mechanism to add\nflexibility in adapting to communication channel capacity and a novel\ndecentralized shared data alignment method to further improve cooperative\nobject detection performance. The performance of the proposed framework is\nverified through experiments on Volony dataset. The results confirm that our\nproposed framework outperforms our previous cooperative object detection method\n(FS-COD) in terms of average precision.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 00:12:58 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Marvasti", "Ehsan Emad", ""], ["Raftari", "Arash", ""], ["Marvasti", "Amir Emad", ""], ["Fallah", "Yaser P.", ""]]}, {"id": "2010.11363", "submitter": "Gang-Xuan Lin", "authors": "Gang-Xuan Lin and Shih-Wei Hu and Chun-Shien Lu", "title": "QISTA-Net: DNN Architecture to Solve $\\ell_q$-norm Minimization Problem\n  and Image Compressed Sensing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we reformulate the non-convex $\\ell_q$-norm minimization\nproblem with $q\\in(0,1)$ into a 2-step problem, which consists of one convex\nand one non-convex subproblems, and propose a novel iterative algorithm called\nQISTA ($\\ell_q$-ISTA) to solve the $\\left(\\ell_q\\right)$-problem. By taking\nadvantage of deep learning in accelerating optimization algorithms, together\nwith the speedup strategy that using the momentum from all previous layers in\nthe network, we propose a learning-based method, called QISTA-Net-s, to solve\nthe sparse signal reconstruction problem. Extensive experimental comparisons\ndemonstrate that the QISTA-Net-s yield better reconstruction qualities than\nstate-of-the-art $\\ell_1$-norm optimization (plus learning) algorithms even if\nthe original sparse signal is noisy. On the other hand, based on the network\narchitecture associated with QISTA, with considering the use of convolution\nlayers, we proposed the QISTA-Net-n for solving the image CS problem, and the\nperformance of the reconstruction still outperforms most of the\nstate-of-the-art natural images reconstruction methods. QISTA-Net-n is designed\nin unfolding QISTA and adding the convolutional operator as the dictionary.\nThis makes QISTA-Net-s interpretable. We provide complete experimental results\nthat QISTA-Net-s and QISTA-Net-n contribute the better reconstruction\nperformance than the competing.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 01:00:45 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Lin", "Gang-Xuan", ""], ["Hu", "Shih-Wei", ""], ["Lu", "Chun-Shien", ""]]}, {"id": "2010.11369", "submitter": "Colin Samplawski", "authors": "Colin Samplawski, Jannik Wolff, Tassilo Klein, Moin Nabi", "title": "Learning Graph-Based Priors for Generalized Zero-Shot Learning", "comments": "Presented at AAAI 2020 Workshop on Deep Learning on Graphs:\n  Methodologies and Applications (DLGMA'20)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of zero-shot learning (ZSL) requires correctly predicting the label\nof samples from classes which were unseen at training time. This is achieved by\nleveraging side information about class labels, such as label attributes or\nword embeddings. Recently, attention has shifted to the more realistic task of\ngeneralized ZSL (GZSL) where test sets consist of seen and unseen samples.\nRecent approaches to GZSL have shown the value of generative models, which are\nused to generate samples from unseen classes. In this work, we incorporate an\nadditional source of side information in the form of a relation graph over\nlabels. We leverage this graph in order to learn a set of prior distributions,\nwhich encourage an aligned variational autoencoder (VAE) model to learn\nembeddings which respect the graph structure. Using this approach we are able\nto achieve improved performance on the CUB and SUN benchmarks over a strong\nbaseline.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 01:20:46 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Samplawski", "Colin", ""], ["Wolff", "Jannik", ""], ["Klein", "Tassilo", ""], ["Nabi", "Moin", ""]]}, {"id": "2010.11375", "submitter": "Po-Hsuan Cameron Chen", "authors": "Zaid Nabulsi, Andrew Sellergren, Shahar Jamshy, Charles Lau, Eddie\n  Santos, Atilla P. Kiraly, Wenxing Ye, Jie Yang, Sahar Kazemzadeh, Jin Yu,\n  Raju Kalidindi, Mozziyar Etemadi, Florencia Garcia Vicente, David Melnick,\n  Greg S. Corrado, Lily Peng, Krish Eswaran, Daniel Tse, Neeral Beladia, Yun\n  Liu, Po-Hsuan Cameron Chen, Shravya Shetty", "title": "Deep Learning for Distinguishing Normal versus Abnormal Chest\n  Radiographs and Generalization to Unseen Diseases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chest radiography (CXR) is the most widely-used thoracic clinical imaging\nmodality and is crucial for guiding the management of cardiothoracic\nconditions. The detection of specific CXR findings has been the main focus of\nseveral artificial intelligence (AI) systems. However, the wide range of\npossible CXR abnormalities makes it impractical to build specific systems to\ndetect every possible condition. In this work, we developed and evaluated an AI\nsystem to classify CXRs as normal or abnormal. For development, we used a\nde-identified dataset of 248,445 patients from a multi-city hospital network in\nIndia. To assess generalizability, we evaluated our system using 6\ninternational datasets from India, China, and the United States. Of these\ndatasets, 4 focused on diseases that the AI was not trained to detect: 2\ndatasets with tuberculosis and 2 datasets with coronavirus disease 2019. Our\nresults suggest that the AI system generalizes to new patient populations and\nabnormalities. In a simulated workflow where the AI system prioritized abnormal\ncases, the turnaround time for abnormal cases reduced by 7-28%. These results\nrepresent an important step towards evaluating whether AI can be safely used to\nflag cases in a general setting where previously unseen abnormalities exist.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 01:52:51 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Nabulsi", "Zaid", ""], ["Sellergren", "Andrew", ""], ["Jamshy", "Shahar", ""], ["Lau", "Charles", ""], ["Santos", "Eddie", ""], ["Kiraly", "Atilla P.", ""], ["Ye", "Wenxing", ""], ["Yang", "Jie", ""], ["Kazemzadeh", "Sahar", ""], ["Yu", "Jin", ""], ["Kalidindi", "Raju", ""], ["Etemadi", "Mozziyar", ""], ["Vicente", "Florencia Garcia", ""], ["Melnick", "David", ""], ["Corrado", "Greg S.", ""], ["Peng", "Lily", ""], ["Eswaran", "Krish", ""], ["Tse", "Daniel", ""], ["Beladia", "Neeral", ""], ["Liu", "Yun", ""], ["Chen", "Po-Hsuan Cameron", ""], ["Shetty", "Shravya", ""]]}, {"id": "2010.11378", "submitter": "Meng Jia", "authors": "Meng Jia and Matthew Kyan", "title": "Learning Occupancy Function from Point Clouds for Surface Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Implicit function based surface reconstruction has been studied for a long\ntime to recover 3D shapes from point clouds sampled from surfaces. Recently,\nSigned Distance Functions (SDFs) and Occupany Functions are adopted in\nlearning-based shape reconstruction methods as implicit 3D shape\nrepresentation. This paper proposes a novel method for learning occupancy\nfunctions from sparse point clouds and achieves better performance on\nchallenging surface reconstruction tasks. Unlike the previous methods, which\npredict point occupancy with fully-connected multi-layer networks, we adapt the\npoint cloud deep learning architecture, Point Convolution Neural Network\n(PCNN), to build our learning model. Specifically, we create a sampling\noperator and insert it into PCNN to continuously sample the feature space at\nthe points where occupancy states need to be predicted. This method natively\nobtains point cloud data's geometric nature, and it's invariant to point\npermutation. Our occupancy function learning can be easily fit into procedures\nof point cloud up-sampling and surface reconstruction. Our experiments show\nstate-of-the-art performance for reconstructing With ShapeNet dataset and\ndemonstrate this method's well-generalization by testing it with McGill 3D\ndataset \\cite{siddiqi2008retrieving}. Moreover, we find the learned occupancy\nfunction is relatively more rotation invariant than previous shape learning\nmethods.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 02:07:29 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Jia", "Meng", ""], ["Kyan", "Matthew", ""]]}, {"id": "2010.11398", "submitter": "Vaikkunth Mugunthan", "authors": "Vaikkunth Mugunthan, Vignesh Gokul, Lalana Kagal and Shlomo Dubnov", "title": "DPD-InfoGAN: Differentially Private Distributed InfoGAN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Networks (GANs) are deep learning architectures\ncapable of generating synthetic datasets. Despite producing high-quality\nsynthetic images, the default GAN has no control over the kinds of images it\ngenerates. The Information Maximizing GAN (InfoGAN) is a variant of the default\nGAN that introduces feature-control variables that are automatically learned by\nthe framework, hence providing greater control over the different kinds of\nimages produced. Due to the high model complexity of InfoGAN, the generative\ndistribution tends to be concentrated around the training data points. This is\na critical problem as the models may inadvertently expose the sensitive and\nprivate information present in the dataset. To address this problem, we propose\na differentially private version of InfoGAN (DP-InfoGAN). We also extend our\nframework to a distributed setting (DPD-InfoGAN) to allow clients to learn\ndifferent attributes present in other clients' datasets in a privacy-preserving\nmanner. In our experiments, we show that both DP-InfoGAN and DPD-InfoGAN can\nsynthesize high-quality images with flexible control over image attributes\nwhile preserving privacy.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 03:07:01 GMT"}, {"version": "v2", "created": "Sat, 24 Oct 2020 13:54:18 GMT"}, {"version": "v3", "created": "Mon, 22 Mar 2021 19:05:23 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Mugunthan", "Vaikkunth", ""], ["Gokul", "Vignesh", ""], ["Kagal", "Lalana", ""], ["Dubnov", "Shlomo", ""]]}, {"id": "2010.11418", "submitter": "Amauri Souza", "authors": "Diego Mesquita, Amauri H. Souza, Samuel Kaski", "title": "Rethinking pooling in graph neural networks", "comments": "Accepted to NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph pooling is a central component of a myriad of graph neural network\n(GNN) architectures. As an inheritance from traditional CNNs, most approaches\nformulate graph pooling as a cluster assignment problem, extending the idea of\nlocal patches in regular grids to graphs. Despite the wide adherence to this\ndesign choice, no work has rigorously evaluated its influence on the success of\nGNNs. In this paper, we build upon representative GNNs and introduce variants\nthat challenge the need for locality-preserving representations, either using\nrandomization or clustering on the complement graph. Strikingly, our\nexperiments demonstrate that using these variants does not result in any\ndecrease in performance. To understand this phenomenon, we study the interplay\nbetween convolutional layers and the subsequent pooling ones. We show that the\nconvolutions play a leading role in the learned representations. In contrast to\nthe common belief, local pooling is not responsible for the success of GNNs on\nrelevant and widely-used benchmarks.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 03:48:56 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Mesquita", "Diego", ""], ["Souza", "Amauri H.", ""], ["Kaski", "Samuel", ""]]}, {"id": "2010.11422", "submitter": "Ildoo Kim", "authors": "Ildoo Kim, Younghoon Kim, Sungwoong Kim", "title": "Learning Loss for Test-Time Augmentation", "comments": "Accepted at NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data augmentation has been actively studied for robust neural networks. Most\nof the recent data augmentation methods focus on augmenting datasets during the\ntraining phase. At the testing phase, simple transformations are still widely\nused for test-time augmentation. This paper proposes a novel instance-level\ntest-time augmentation that efficiently selects suitable transformations for a\ntest input. Our proposed method involves an auxiliary module to predict the\nloss of each possible transformation given the input. Then, the transformations\nhaving lower predicted losses are applied to the input. The network obtains the\nresults by averaging the prediction results of augmented inputs. Experimental\nresults on several image classification benchmarks show that the proposed\ninstance-aware test-time augmentation improves the model's robustness against\nvarious corruptions.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 03:56:34 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Kim", "Ildoo", ""], ["Kim", "Younghoon", ""], ["Kim", "Sungwoong", ""]]}, {"id": "2010.11423", "submitter": "Rodrigo Santa Cruz", "authors": "Rodrigo Santa Cruz, Leo Lebrat, Pierrick Bourgeat, Clinton Fookes,\n  Jurgen Fripp, Olivier Salvado", "title": "DeepCSR: A 3D Deep Learning Approach for Cortical Surface Reconstruction", "comments": "Accepted in 2021 IEEE Winter Conference on Applications of Computer\n  Vision (WACV)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The study of neurodegenerative diseases relies on the reconstruction and\nanalysis of the brain cortex from magnetic resonance imaging (MRI). Traditional\nframeworks for this task like FreeSurfer demand lengthy runtimes, while its\naccelerated variant FastSurfer still relies on a voxel-wise segmentation which\nis limited by its resolution to capture narrow continuous objects as cortical\nsurfaces. Having these limitations in mind, we propose DeepCSR, a 3D deep\nlearning framework for cortical surface reconstruction from MRI. Towards this\nend, we train a neural network model with hypercolumn features to predict\nimplicit surface representations for points in a brain template space. After\ntraining, the cortical surface at a desired level of detail is obtained by\nevaluating surface representations at specific coordinates, and subsequently\napplying a topology correction algorithm and an isosurface extraction method.\nThanks to the continuous nature of this approach and the efficacy of its\nhypercolumn features scheme, DeepCSR efficiently reconstructs cortical surfaces\nat high resolution capturing fine details in the cortical folding. Moreover,\nDeepCSR is as accurate, more precise, and faster than the widely used\nFreeSurfer toolbox and its deep learning powered variant FastSurfer on\nreconstructing cortical surfaces from MRI which should facilitate large-scale\nmedical studies and new healthcare applications.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 03:57:44 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Cruz", "Rodrigo Santa", ""], ["Lebrat", "Leo", ""], ["Bourgeat", "Pierrick", ""], ["Fookes", "Clinton", ""], ["Fripp", "Jurgen", ""], ["Salvado", "Olivier", ""]]}, {"id": "2010.11426", "submitter": "Xianzhi Du", "authors": "Xianzhi Du, Tsung-Yi Lin, Pengchong Jin, Yin Cui, Mingxing Tan, Quoc\n  Le, and Xiaodan Song", "title": "Efficient Scale-Permuted Backbone with Learned Resource Distribution", "comments": "ECCV2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, SpineNet has demonstrated promising results on object detection and\nimage classification over ResNet model. However, it is unclear if the\nimprovement adds up when combining scale-permuted backbone with advanced\nefficient operations and compound scaling. Furthermore, SpineNet is built with\na uniform resource distribution over operations. While this strategy seems to\nbe prevalent for scale-decreased models, it may not be an optimal design for\nscale-permuted models. In this work, we propose a simple technique to combine\nefficient operations and compound scaling with a previously learned\nscale-permuted architecture. We demonstrate the efficiency of scale-permuted\nmodel can be further improved by learning a resource distribution over the\nentire network. The resulting efficient scale-permuted models outperform\nstate-of-the-art EfficientNet-based models on object detection and achieve\ncompetitive performance on image classification and semantic segmentation. Code\nand models will be open-sourced soon.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 03:59:51 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Du", "Xianzhi", ""], ["Lin", "Tsung-Yi", ""], ["Jin", "Pengchong", ""], ["Cui", "Yin", ""], ["Tan", "Mingxing", ""], ["Le", "Quoc", ""], ["Song", "Xiaodan", ""]]}, {"id": "2010.11437", "submitter": "Jun Seo", "authors": "Jun Seo, Young-Hyun Park, Sung-Whan Yoon, Jaekyun Moon", "title": "Task-Adaptive Feature Transformer for Few-Shot Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Few-shot learning allows machines to classify novel classes using only a few\nlabeled samples. Recently, few-shot segmentation aiming at semantic\nsegmentation on low sample data has also seen great interest. In this paper, we\npropose a learnable module for few-shot segmentation, the task-adaptive feature\ntransformer (TAFT). TAFT linearly transforms task-specific high-level features\nto a set of task-agnostic features well-suited to the segmentation job. Using\nthis task-conditioned feature transformation, the model is shown to effectively\nutilize the semantic information in novel classes to generate tight\nsegmentation masks. The proposed TAFT module can be easily plugged into\nexisting semantic segmentation algorithms to achieve few-shot segmentation\ncapability with only a few added parameters. We combine TAFT with Deeplab V3+,\na well-known segmentation architecture; experiments on the PASCAL-$5^i$ dataset\nconfirm that this combination successfully adds few-shot learning capability to\nthe segmentation algorithm, achieving the state-of-the-art few-shot\nsegmentation performance in some key representative cases.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 04:35:37 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Seo", "Jun", ""], ["Park", "Young-Hyun", ""], ["Yoon", "Sung-Whan", ""], ["Moon", "Jaekyun", ""]]}, {"id": "2010.11438", "submitter": "Quan Liu", "authors": "Quan Liu, Isabella M. Gaeta, Bryan A. Millis, Matthew J. Tyska,\n  Yuankai Huo", "title": "GAN based Unsupervised Segmentation: Should We Match the Exact Number of\n  Objects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The unsupervised segmentation is an increasingly popular topic in biomedical\nimage analysis. The basic idea is to approach the supervised segmentation task\nas an unsupervised synthesis problem, where the intensity images can be\ntransferred to the annotation domain using cycle-consistent adversarial\nlearning. The previous studies have shown that the macro-level (global\ndistribution level) matching on the number of the objects (e.g., cells,\ntissues, protrusions etc.) between two domains resulted in better segmentation\nperformance. However, no prior studies have exploited whether the unsupervised\nsegmentation performance would be further improved when matching the exact\nnumber of objects at micro-level (mini-batch level). In this paper, we propose\na deep learning based unsupervised segmentation method for segmenting highly\noverlapped and dynamic sub-cellular microvilli. With this challenging task,\nboth micro-level and macro-level matching strategies were evaluated. To match\nthe number of objects at the micro-level, the novel fluorescence-based\nmicro-level matching approach was presented. From the experimental results, the\nmicro-level matching did not improve the segmentation performance, compared\nwith the simpler macro-level matching.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 04:36:41 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Liu", "Quan", ""], ["Gaeta", "Isabella M.", ""], ["Millis", "Bryan A.", ""], ["Tyska", "Matthew J.", ""], ["Huo", "Yuankai", ""]]}, {"id": "2010.11464", "submitter": "Haruhiro Fujita", "authors": "Haruhiro Fujita, Masatoshi Itagaki, Kenta Ichikawa, Yew Kwang Hooi,\n  Kazutaka Kawano and Ryo Yamamoto", "title": "Fine-tuned Pre-trained Mask R-CNN Models for Surface Object Detection", "comments": "12 page, 12 tables, 15 figures, to be published in one of\n  professional journals", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study evaluates road surface object detection tasks using four Mask\nR-CNN models as a pre-study of surface deterioration detection of stone-made\narchaeological objects. The models were pre-trained and fine-tuned by COCO\ndatasets and 15,188 segmented road surface annotation tags. The quality of the\nmodels were measured using Average Precisions and Average Recalls. Result\nindicates substantial number of counts of false negatives, i.e. left detection\nand unclassified detections. A modified confusion matrix model to avoid\nprioritizing IoU is tested and there are notable true positive increases in\nbounding box detection, but almost no changes in segmentation masks.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 06:09:59 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Fujita", "Haruhiro", ""], ["Itagaki", "Masatoshi", ""], ["Ichikawa", "Kenta", ""], ["Hooi", "Yew Kwang", ""], ["Kawano", "Kazutaka", ""], ["Yamamoto", "Ryo", ""]]}, {"id": "2010.11468", "submitter": "Yuchao Dai Dr.", "authors": "Xiang Guo, Bo Li, Yuchao Dai, Tongxin Zhang, Hui Deng", "title": "Novel View Synthesis from only a 6-DoF Camera Pose by Two-stage Networks", "comments": "Accepted by International Conference on Pattern Recognition (ICPR\n  2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Novel view synthesis is a challenging problem in computer vision and\nrobotics. Different from the existing works, which need the reference images or\n3D models of the scene to generate images under novel views, we propose a novel\nparadigm to this problem. That is, we synthesize the novel view from only a\n6-DoF camera pose directly. Although this setting is the most straightforward\nway, there are few works addressing it. While, our experiments demonstrate\nthat, with a concise CNN, we could get a meaningful parametric model that could\nreconstruct the correct scenery images only from the 6-DoF pose. To this end,\nwe propose a two-stage learning strategy, which consists of two consecutive\nCNNs: GenNet and RefineNet. GenNet generates a coarse image from a camera pose.\nRefineNet is a generative adversarial network that refines the coarse image. In\nthis way, we decouple the geometric relationship between mapping and texture\ndetail rendering. Extensive experiments conducted on the public datasets prove\nthe effectiveness of our method. We believe this paradigm is of high research\nand application value and could be an important direction in novel view\nsynthesis.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 06:23:40 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Guo", "Xiang", ""], ["Li", "Bo", ""], ["Dai", "Yuchao", ""], ["Zhang", "Tongxin", ""], ["Deng", "Hui", ""]]}, {"id": "2010.11472", "submitter": "Don Pathirage", "authors": "Golnaz Moallem (1), Don D. Pathirage (1), Joel Reznick (1), James\n  Gallagher (2), Hamed Sari-Sarraf (1) ((1) Applied Vision Lab Texas Tech\n  University (2) Texas Parks and Wildlife Department)", "title": "An explainable deep vision system for animal classification and\n  detection in trail-camera images with automatic post-deployment retraining", "comments": null, "journal-ref": "Knowledge-Based Systems, Volume 216, 15 March 2021, 106815", "doi": "10.1016/j.knosys.2021.106815", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces an automated vision system for animal detection in\ntrail-camera images taken from a field under the administration of the Texas\nParks and Wildlife Department. As traditional wildlife counting techniques are\nintrusive and labor intensive to conduct, trail-camera imaging is a\ncomparatively non-intrusive method for capturing wildlife activity. However,\ngiven the large volume of images produced from trail-cameras, manual analysis\nof the images remains time-consuming and inefficient. We implemented a\ntwo-stage deep convolutional neural network pipeline to find animal-containing\nimages in the first stage and then process these images to detect birds in the\nsecond stage. The animal classification system classifies animal images with\noverall 93% sensitivity and 96% specificity. The bird detection system achieves\nbetter than 93% sensitivity, 92% specificity, and 68% average\nIntersection-over-Union rate. The entire pipeline processes an image in less\nthan 0.5 seconds as opposed to an average 30 seconds for a human labeler. We\nalso addressed post-deployment issues related to data drift for the animal\nclassification system as image features vary with seasonal changes. This system\nutilizes an automatic retraining algorithm to detect data drift and update the\nsystem. We introduce a novel technique for detecting drifted images and\ntriggering the retraining procedure. Two statistical experiments are also\npresented to explain the prediction behavior of the animal classification\nsystem. These experiments investigate the cues that steers the system towards a\nparticular decision. Statistical hypothesis testing demonstrates that the\npresence of an animal in the input image significantly contributes to the\nsystem's decisions.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 06:29:55 GMT"}, {"version": "v2", "created": "Sat, 5 Dec 2020 01:42:12 GMT"}, {"version": "v3", "created": "Tue, 8 Dec 2020 02:17:00 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Moallem", "Golnaz", ""], ["Pathirage", "Don D.", ""], ["Reznick", "Joel", ""], ["Gallagher", "James", ""], ["Sari-Sarraf", "Hamed", ""]]}, {"id": "2010.11475", "submitter": "Konpat Preechakul", "authors": "Konpat Preechakul, Sira Sriswasdi, Boonserm Kijsirikul, Ekapol\n  Chuangsuwanich", "title": "High resolution weakly supervised localization architectures for medical\n  images", "comments": "submitted to ICASSP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In medical imaging, Class-Activation Map (CAM) serves as the main\nexplainability tool by pointing to the region of interest. Since the\nlocalization accuracy from CAM is constrained by the resolution of the model's\nfeature map, one may expect that segmentation models, which generally have\nlarge feature maps, would produce more accurate CAMs. However, we have found\nthat this is not the case due to task mismatch. While segmentation models are\ndeveloped for datasets with pixel-level annotation, only image-level annotation\nis available in most medical imaging datasets. Our experiments suggest that\nGlobal Average Pooling (GAP) and Group Normalization are the main culprits that\nworsen the localization accuracy of CAM. To address this issue, we propose\nPyramid Localization Network (PYLON), a model for high-accuracy\nweakly-supervised localization that achieved 0.62 average point localization\naccuracy on NIH's Chest X-Ray 14 dataset, compared to 0.45 for a traditional\nCAM model. Source code and extended results are available at\nhttps://github.com/cmb-chula/pylon.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 06:42:00 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Preechakul", "Konpat", ""], ["Sriswasdi", "Sira", ""], ["Kijsirikul", "Boonserm", ""], ["Chuangsuwanich", "Ekapol", ""]]}, {"id": "2010.11488", "submitter": "Cheng Lin", "authors": "Cheng Lin, Lingjie Liu, Changjian Li, Leif Kobbelt, Bin Wang, Shiqing\n  Xin, Wenping Wang", "title": "SEG-MAT: 3D Shape Segmentation Using Medial Axis Transform", "comments": "IEEE Transactions on Visualization and Computer Graphics (TVCG), to\n  appear", "journal-ref": null, "doi": "10.1109/TVCG.2020.3032566", "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Segmenting arbitrary 3D objects into constituent parts that are structurally\nmeaningful is a fundamental problem encountered in a wide range of computer\ngraphics applications. Existing methods for 3D shape segmentation suffer from\ncomplex geometry processing and heavy computation caused by using low-level\nfeatures and fragmented segmentation results due to the lack of global\nconsideration. We present an efficient method, called SEG-MAT, based on the\nmedial axis transform (MAT) of the input shape. Specifically, with the rich\ngeometrical and structural information encoded in the MAT, we are able to\ndevelop a simple and principled approach to effectively identify the various\ntypes of junctions between different parts of a 3D shape. Extensive evaluations\nand comparisons show that our method outperforms the state-of-the-art methods\nin terms of segmentation quality and is also one order of magnitude faster.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 07:15:23 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Lin", "Cheng", ""], ["Liu", "Lingjie", ""], ["Li", "Changjian", ""], ["Kobbelt", "Leif", ""], ["Wang", "Bin", ""], ["Xin", "Shiqing", ""], ["Wang", "Wenping", ""]]}, {"id": "2010.11504", "submitter": "Yi Fang", "authors": "Lingjing Wang, Yu Hao, Xiang Li, Yi Fang", "title": "3D Meta-Registration: Learning to Learn Registration of 3D Point Clouds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Deep learning-based point cloud registration models are often generalized\nfrom extensive training over a large volume of data to learn the ability to\npredict the desired geometric transformation to register 3D point clouds. In\nthis paper, we propose a meta-learning based 3D registration model, named 3D\nMeta-Registration, that is capable of rapidly adapting and well generalizing to\nnew 3D registration tasks for unseen 3D point clouds. Our 3D Meta-Registration\ngains a competitive advantage by training over a variety of 3D registration\ntasks, which leads to an optimized model for the best performance on the\ndistribution of registration tasks including potentially unseen tasks.\nSpecifically, the proposed 3D Meta-Registration model consists of two modules:\n3D registration learner and 3D registration meta-learner. During the training,\nthe 3D registration learner is trained to complete a specific registration task\naiming to determine the desired geometric transformation that aligns the source\npoint cloud with the target one. In the meantime, the 3D registration\nmeta-learner is trained to provide the optimal parameters to update the 3D\nregistration learner based on the learned task distribution. After training,\nthe 3D registration meta-learner, which is learned with the optimized coverage\nof distribution of 3D registration tasks, is able to dynamically update 3D\nregistration learners with desired parameters to rapidly adapt to new\nregistration tasks. We tested our model on synthesized dataset ModelNet and\nFlyingThings3D, as well as real-world dataset KITTI. Experimental results\ndemonstrate that 3D Meta-Registration achieves superior performance over other\nprevious techniques (e.g. FlowNet3D).\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 07:45:09 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Wang", "Lingjing", ""], ["Hao", "Yu", ""], ["Li", "Xiang", ""], ["Fang", "Yi", ""]]}, {"id": "2010.11510", "submitter": "Hao Zou", "authors": "Hao Zou, Jinhao Cui, Xin Kong, Chujuan Zhang, Yong Liu, Feng Wen and\n  Wanlong Li", "title": "F-Siamese Tracker: A Frustum-based Double Siamese Network for 3D Single\n  Object Tracking", "comments": "7pages, 5 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents F-Siamese Tracker, a novel approach for single object\ntracking prominently characterized by more robustly integrating 2D and 3D\ninformation to reduce redundant search space. A main challenge in 3D single\nobject tracking is how to reduce search space for generating appropriate 3D\ncandidates. Instead of solely relying on 3D proposals, firstly, our method\nleverages the Siamese network applied on RGB images to produce 2D region\nproposals which are then extruded into 3D viewing frustums. Besides, we perform\nan online accuracy validation on the 3D frustum to generate refined point cloud\nsearching space, which can be embedded directly into the existing 3D tracking\nbackbone. For efficiency, our approach gains better performance with fewer\ncandidates by reducing search space. In addition, benefited from introducing\nthe online accuracy validation, for occasional cases with strong occlusions or\nvery sparse points, our approach can still achieve high precision, even when\nthe 2D Siamese tracker loses the target. This approach allows us to set a new\nstate-of-the-art in 3D single object tracking by a significant margin on a\nsparse outdoor dataset (KITTI tracking). Moreover, experiments on 2D single\nobject tracking show that our framework boosts 2D tracking performance as well.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 08:01:17 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Zou", "Hao", ""], ["Cui", "Jinhao", ""], ["Kong", "Xin", ""], ["Zhang", "Chujuan", ""], ["Liu", "Yong", ""], ["Wen", "Feng", ""], ["Li", "Wanlong", ""]]}, {"id": "2010.11521", "submitter": "Kushal Shah", "authors": "Subrata Sarkar, Rati Sharma and Kushal Shah", "title": "Malaria detection from RBC images using shallow Convolutional Neural\n  Networks", "comments": "8 pages, 4 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The advent of Deep Learning models like VGG-16 and Resnet-50 has considerably\nrevolutionized the field of image classification, and by using these\nConvolutional Neural Networks (CNN) architectures, one can get a high\nclassification accuracy on a wide variety of image datasets. However, these\nDeep Learning models have a very high computational complexity and so incur a\nhigh computational cost of running these algorithms as well as make it hard to\ninterpret the results. In this paper, we present a shallow CNN architecture\nwhich gives the same classification accuracy as the VGG-16 and Resnet-50 models\nfor thin blood smear RBC slide images for detection of malaria, while\ndecreasing the computational run time by an order of magnitude. This can offer\na significant advantage for commercial deployment of these algorithms,\nespecially in poorer countries in Africa and some parts of the Indian\nsubcontinent, where the menace of malaria is quite severe.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 08:32:10 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Sarkar", "Subrata", ""], ["Sharma", "Rati", ""], ["Shah", "Kushal", ""]]}, {"id": "2010.11531", "submitter": "Manuel Kaufmann", "authors": "Manuel Kaufmann, Emre Aksan, Jie Song, Fabrizio Pece, Remo Ziegler,\n  Otmar Hilliges", "title": "Convolutional Autoencoders for Human Motion Infilling", "comments": "Accepted to 3DV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a convolutional autoencoder to address the problem\nof motion infilling for 3D human motion data. Given a start and end sequence,\nmotion infilling aims to complete the missing gap in between, such that the\nfilled in poses plausibly forecast the start sequence and naturally transition\ninto the end sequence. To this end, we propose a single, end-to-end trainable\nconvolutional autoencoder. We show that a single model can be used to create\nnatural transitions between different types of activities. Furthermore, our\nmethod is not only able to fill in entire missing frames, but it can also be\nused to complete gaps where partial poses are available (e.g. from end\neffectors), or to clean up other forms of noise (e.g. Gaussian). Also, the\nmodel can fill in an arbitrary number of gaps that potentially vary in length.\nIn addition, no further post-processing on the model's outputs is necessary\nsuch as smoothing or closing discontinuities at the end of the gap. At the\nheart of our approach lies the idea to cast motion infilling as an inpainting\nproblem and to train a convolutional de-noising autoencoder on image-like\nrepresentations of motion sequences. At training time, blocks of columns are\nremoved from such images and we ask the model to fill in the gaps. We\ndemonstrate the versatility of the approach via a number of complex motion\nsequences and report on thorough evaluations performed to better understand the\ncapabilities and limitations of the proposed approach.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 08:45:38 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Kaufmann", "Manuel", ""], ["Aksan", "Emre", ""], ["Song", "Jie", ""], ["Pece", "Fabrizio", ""], ["Ziegler", "Remo", ""], ["Hilliges", "Otmar", ""]]}, {"id": "2010.11535", "submitter": "Zifei Zhang", "authors": "Zifei Zhang, Kai Qiao, Jian Chen and Ningning Liang", "title": "Defense-guided Transferable Adversarial Attacks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Though deep neural networks perform challenging tasks excellently, they are\nsusceptible to adversarial examples, which mislead classifiers by applying\nhuman-imperceptible perturbations on clean inputs. Under the query-free\nblack-box scenario, adversarial examples are hard to transfer to unknown\nmodels, and several methods have been proposed with the low transferability. To\nsettle such issue, we design a max-min framework inspired by input\ntransformations, which are benificial to both the adversarial attack and\ndefense. Explicitly, we decrease loss values with inputs' affline\ntransformations as a defense in the minimum procedure, and then increase loss\nvalues with the momentum iterative algorithm as an attack in the maximum\nprocedure. To further promote transferability, we determine transformed values\nwith the max-min theory. Extensive experiments on Imagenet demonstrate that our\ndefense-guided transferable attacks achieve impressive increase on\ntransferability. Experimentally, we show that our ASR of adversarial attack\nreaches to 58.38% on average, which outperforms the state-of-the-art method by\n12.1% on the normally trained models and by 11.13% on the adversarially trained\nmodels. Additionally, we provide elucidative insights on the improvement of\ntransferability, and our method is expected to be a benchmark for assessing the\nrobustness of deep models.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 08:51:45 GMT"}, {"version": "v2", "created": "Wed, 4 Nov 2020 01:34:31 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Zhang", "Zifei", ""], ["Qiao", "Kai", ""], ["Chen", "Jian", ""], ["Liang", "Ningning", ""]]}, {"id": "2010.11547", "submitter": "Myungsung Kwak", "authors": "Dongyoung Kim, Myungsung Kwak, Eunji Won, Sejung Shin, Jeongyeon Nam", "title": "TLGAN: document Text Localization using Generative Adversarial Nets", "comments": "17 pages, three figures, 4 tables, methods for IEEE ICDAR RRC SROIE\n  task1 leader board", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Text localization from the digital image is the first step for the optical\ncharacter recognition task. Conventional image processing based text\nlocalization performs adequately for specific examples. Yet, a general text\nlocalization are only archived by recent deep-learning based modalities. Here\nwe present document Text Localization Generative Adversarial Nets (TLGAN) which\nare deep neural networks to perform the text localization from digital image.\nTLGAN is an versatile and easy-train text localization model requiring a small\namount of data. Training only ten labeled receipt images from Robust Reading\nChallenge on Scanned Receipts OCR and Information Extraction (SROIE), TLGAN\nachieved 99.83% precision and 99.64% recall for SROIE test data. Our TLGAN is a\npractical text localization solution requiring minimal effort for data labeling\nand model training and producing a state-of-art performance.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 09:19:13 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Kim", "Dongyoung", ""], ["Kwak", "Myungsung", ""], ["Won", "Eunji", ""], ["Shin", "Sejung", ""], ["Nam", "Jeongyeon", ""]]}, {"id": "2010.11550", "submitter": "Keyu Wen", "authors": "Keyu Wen, Xiaodong Gu, Qingrong Cheng", "title": "Learning Dual Semantic Relations with Graph Attention for Image-Text\n  Matching", "comments": "14pages, 9 figures. Accepted at: IEEE Transactions on Circuits and\n  Systems for Video Technology (Early Access Print) | |Codes Available at:\n  https://github.com/kywen1119/DSRAN", "journal-ref": null, "doi": "10.1109/TCSVT.2020.3030656", "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image-Text Matching is one major task in cross-modal information processing.\nThe main challenge is to learn the unified visual and textual representations.\nPrevious methods that perform well on this task primarily focus on not only the\nalignment between region features in images and the corresponding words in\nsentences, but also the alignment between relations of regions and relational\nwords. However, the lack of joint learning of regional features and global\nfeatures will cause the regional features to lose contact with the global\ncontext, leading to the mismatch with those non-object words which have global\nmeanings in some sentences. In this work, in order to alleviate this issue, it\nis necessary to enhance the relations between regions and the relations between\nregional and global concepts to obtain a more accurate visual representation so\nas to be better correlated to the corresponding text. Thus, a novel multi-level\nsemantic relations enhancement approach named Dual Semantic Relations Attention\nNetwork(DSRAN) is proposed which mainly consists of two modules, separate\nsemantic relations module and the joint semantic relations module. DSRAN\nperforms graph attention in both modules respectively for region-level\nrelations enhancement and regional-global relations enhancement at the same\ntime. With these two modules, different hierarchies of semantic relations are\nlearned simultaneously, thus promoting the image-text matching process by\nproviding more information for the final visual representation. Quantitative\nexperimental results have been performed on MS-COCO and Flickr30K and our\nmethod outperforms previous approaches by a large margin due to the\neffectiveness of the dual semantic relations learning scheme. Codes are\navailable at https://github.com/kywen1119/DSRAN.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 09:21:32 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Wen", "Keyu", ""], ["Gu", "Xiaodong", ""], ["Cheng", "Qingrong", ""]]}, {"id": "2010.11563", "submitter": "Amit Trivedi", "authors": "Amit Kumar Trivedi", "title": "Fingerprint Orientation Estimation: Challenges and Opportunities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is an exponential increase in portable electronic devices with\nbiometric security mechanisms, in particular fingerprint biometric. A person\nhas a limited number of fingerprints and it remains unchanged throughout his\nlifetime, once leaked to the adversary, it leaks for a lifetime. So, there is a\nneed to secure the biometric template itself. In this survey paper, we review\nthe different security models and fingerprint template protection techniques.\nThe research challenges in different fingerprint template protection techniques\nare also highlighted in respective sections of the paper. This survey provides\na comprehensive study of template protection techniques for fingerprint\nbiometric systems and highlights the challenges and future opportunities.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 09:41:18 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Trivedi", "Amit Kumar", ""]]}, {"id": "2010.11575", "submitter": "Yuanzhi Wang", "authors": "Tao Lu, Yuanzhi Wang, Yanduo Zhang, Yu Wang, Wei Liu, Zhongyuan Wang,\n  Junjun Jiang", "title": "Face Hallucination via Split-Attention in Split-Attention Network", "comments": "Accepted by ACM MM 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently, convolutional neural networks (CNNs) have been widely employed to\npromote the face hallucination due to the ability to predict high-frequency\ndetails from a large number of samples. However, most of them fail to take into\naccount the overall facial profile and fine texture details simultaneously,\nresulting in reduced naturalness and fidelity of the reconstructed face, and\nfurther impairing the performance of downstream tasks (e.g., face detection,\nfacial recognition). To tackle this issue, we propose a novel external-internal\nsplit attention group (ESAG), which encompasses two paths responsible for\nfacial structure information and facial texture details, respectively. By\nfusing the features from these two paths, the consistency of facial structure\nand the fidelity of facial details are strengthened at the same time. Then, we\npropose a split-attention in split-attention network (SISN) to reconstruct\nphotorealistic high-resolution facial images by cascading several ESAGs.\nExperimental results on face hallucination and face recognition unveil that the\nproposed method not only significantly improves the clarity of hallucinated\nfaces, but also encourages the subsequent face recognition performance\nsubstantially. Codes have been released at\nhttps://github.com/mdswyz/SISN-Face-Hallucination.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 10:09:31 GMT"}, {"version": "v2", "created": "Tue, 15 Dec 2020 13:05:41 GMT"}, {"version": "v3", "created": "Wed, 7 Jul 2021 10:08:19 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Lu", "Tao", ""], ["Wang", "Yuanzhi", ""], ["Zhang", "Yanduo", ""], ["Wang", "Yu", ""], ["Liu", "Wei", ""], ["Wang", "Zhongyuan", ""], ["Jiang", "Junjun", ""]]}, {"id": "2010.11594", "submitter": "Yuanhao Zhai", "authors": "Yuanhao Zhai, Le Wang, Wei Tang, Qilin Zhang, Junsong Yuan, Gang Hua", "title": "Two-Stream Consensus Network for Weakly-Supervised Temporal Action\n  Localization", "comments": "ECCV 2020 spotlight", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Weakly-supervised Temporal Action Localization (W-TAL) aims to classify and\nlocalize all action instances in an untrimmed video under only video-level\nsupervision. However, without frame-level annotations, it is challenging for\nW-TAL methods to identify false positive action proposals and generate action\nproposals with precise temporal boundaries. In this paper, we present a\nTwo-Stream Consensus Network (TSCN) to simultaneously address these challenges.\nThe proposed TSCN features an iterative refinement training method, where a\nframe-level pseudo ground truth is iteratively updated, and used to provide\nframe-level supervision for improved model training and false positive action\nproposal elimination. Furthermore, we propose a new attention normalization\nloss to encourage the predicted attention to act like a binary selection, and\npromote the precise localization of action instance boundaries. Experiments\nconducted on the THUMOS14 and ActivityNet datasets show that the proposed TSCN\noutperforms current state-of-the-art methods, and even achieves comparable\nresults with some recent fully-supervised methods.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 10:53:32 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Zhai", "Yuanhao", ""], ["Wang", "Le", ""], ["Tang", "Wei", ""], ["Zhang", "Qilin", ""], ["Yuan", "Junsong", ""], ["Hua", "Gang", ""]]}, {"id": "2010.11600", "submitter": "Junghoon Seo", "authors": "Junghoon Seo, Joon Suk Huh", "title": "On the Power of Deep but Naive Partial Label Learning", "comments": null, "journal-ref": "2021 IEEE ICASSP International Conference on Acoustics, Speech,\n  and Signal Processing", "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Partial label learning (PLL) is a class of weakly supervised learning where\neach training instance consists of a data and a set of candidate labels\ncontaining a unique ground truth label. To tackle this problem, a majority of\ncurrent state-of-the-art methods employs either label disambiguation or\naveraging strategies. So far, PLL methods without such techniques have been\nconsidered impractical. In this paper, we challenge this view by revealing the\nhidden power of the oldest and naivest PLL method when it is instantiated with\ndeep neural networks. Specifically, we show that, with deep neural networks,\nthe naive model can achieve competitive performances against the other\nstate-of-the-art methods, suggesting it as a strong baseline for PLL. We also\naddress the question of how and why such a naive model works well with deep\nneural networks. Our empirical results indicate that deep neural networks\ntrained on partially labeled examples generalize very well even in the\nover-parametrized regime and without label disambiguations or regularizations.\nWe point out that existing learning theories on PLL are vacuous in the\nover-parametrized regime. Hence they cannot explain why the deep naive method\nworks. We propose an alternative theory on how deep learning generalize in PLL\nproblems.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 11:02:56 GMT"}, {"version": "v2", "created": "Mon, 8 Feb 2021 07:32:23 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Seo", "Junghoon", ""], ["Huh", "Joon Suk", ""]]}, {"id": "2010.11619", "submitter": "Florin-Alexandru Vasluianu", "authors": "Florin-Alexandru Vasluianu and Andres Romero and Luc Van Gool and Radu\n  Timofte", "title": "Self-Supervised Shadow Removal", "comments": "10 pages, 4 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shadow removal is an important computer vision task aiming at the detection\nand successful removal of the shadow produced by an occluded light source and a\nphoto-realistic restoration of the image contents. Decades of re-search\nproduced a multitude of hand-crafted restoration techniques and, more recently,\nlearned solutions from shad-owed and shadow-free training image pairs. In this\nwork,we propose an unsupervised single image shadow removal solution via\nself-supervised learning by using a conditioned mask. In contrast to existing\nliterature, we do not require paired shadowed and shadow-free images, instead\nwe rely on self-supervision and jointly learn deep models to remove and add\nshadows to images. We validate our approach on the recently introduced ISTD and\nUSR datasets. We largely improve quantitatively and qualitatively over the\ncompared methods and set a new state-of-the-art performance in single image\nshadow removal.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 11:33:41 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Vasluianu", "Florin-Alexandru", ""], ["Romero", "Andres", ""], ["Van Gool", "Luc", ""], ["Timofte", "Radu", ""]]}, {"id": "2010.11649", "submitter": "Gagan Kanojia", "authors": "Gagan Kanojia and Shanmuganathan Raman", "title": "Learning to Sort Image Sequences via Accumulated Temporal Differences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a set of n images of a scene with dynamic objects captured with a\nstatic or a handheld camera. Let the temporal order in which these images are\ncaptured be unknown. There can be n! possibilities for the temporal order in\nwhich these images could have been captured. In this work, we tackle the\nproblem of temporally sequencing the unordered set of images of a dynamic scene\ncaptured with a hand-held camera. We propose a convolutional block which\ncaptures the spatial information through 2D convolution kernel and captures the\ntemporal information by utilizing the differences present among the feature\nmaps extracted from the input images. We evaluate the performance of the\nproposed approach on the dataset extracted from a standard action recognition\ndataset, UCF101. We show that the proposed approach outperforms the\nstate-of-the-art methods by a significant margin. We show that the network\ngeneralizes well by evaluating it on a dataset extracted from the DAVIS\ndataset, a dataset meant for video object segmentation, when the same network\nwas trained with a dataset extracted from UCF101, a dataset meant for action\nrecognition.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 12:34:05 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Kanojia", "Gagan", ""], ["Raman", "Shanmuganathan", ""]]}, {"id": "2010.11661", "submitter": "Jason McEwen", "authors": "Oliver J. Cobb, Christopher G. R. Wallis, Augustine N. Mavor-Parker,\n  Augustin Marignier, Matthew A. Price, Mayeul d'Avezac, Jason D. McEwen", "title": "Efficient Generalized Spherical CNNs", "comments": "20 pages, 4 figures, accepted by ICLR, code at\n  https://www.kagenova.com/products/fourpiAI/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV astro-ph.IM cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many problems across computer vision and the natural sciences require the\nanalysis of spherical data, for which representations may be learned\nefficiently by encoding equivariance to rotational symmetries. We present a\ngeneralized spherical CNN framework that encompasses various existing\napproaches and allows them to be leveraged alongside each other. The only\nexisting non-linear spherical CNN layer that is strictly equivariant has\ncomplexity $\\mathcal{O}(C^2L^5)$, where $C$ is a measure of representational\ncapacity and $L$ the spherical harmonic bandlimit. Such a high computational\ncost often prohibits the use of strictly equivariant spherical CNNs. We develop\ntwo new strictly equivariant layers with reduced complexity $\\mathcal{O}(CL^4)$\nand $\\mathcal{O}(CL^3 \\log L)$, making larger, more expressive models\ncomputationally feasible. Moreover, we adopt efficient sampling theory to\nachieve further computational savings. We show that these developments allow\nthe construction of more expressive hybrid models that achieve state-of-the-art\naccuracy and parameter efficiency on spherical benchmark problems.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 18:00:05 GMT"}, {"version": "v2", "created": "Fri, 23 Oct 2020 15:52:16 GMT"}, {"version": "v3", "created": "Mon, 8 Mar 2021 11:55:27 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Cobb", "Oliver J.", ""], ["Wallis", "Christopher G. R.", ""], ["Mavor-Parker", "Augustine N.", ""], ["Marignier", "Augustin", ""], ["Price", "Matthew A.", ""], ["d'Avezac", "Mayeul", ""], ["McEwen", "Jason D.", ""]]}, {"id": "2010.11671", "submitter": "Guoliang Liu Prof. Dr.", "authors": "Hejing Ling, Guoliang Liu, Guohui Tian", "title": "Motion Planning Combines Psychological Safety and Motion Prediction for\n  a Sense Motive Robot", "comments": "submitted to RAL/ICRA2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human safety is the most important demand for human robot interaction and\ncollaboration (HRIC), which not only refers to physical safety, but also\nincludes psychological safety. Although many robots with different\nconfigurations have entered our living and working environments, the human\nsafety problem is still an ongoing research problem in human-robot coexistence\nscenarios. This paper addresses the human safety issue by covering both the\nphysical safety and psychological safety aspects. First, we introduce an\nadaptive robot velocity control and step size adjustment method according to\nhuman facial expressions, such that the robot can adjust its movement to keep\nsafety when the human emotion is unusual. Second, we predict the human motion\nby detecting the suddenly changes of human head pose and gaze direction, such\nthat the robot can infer whether the human attention is distracted, predict the\nnext move of human and rebuild a repulsive force to avoid potential collision.\nFinally, we demonstrate our idea using a 7 DOF TIAGo robot in a dynamic HRIC\nenvironment, which shows that the robot becomes sense motive, and responds to\nhuman action and emotion changes quickly and efficiently.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 04:19:53 GMT"}, {"version": "v2", "created": "Fri, 23 Oct 2020 15:32:08 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Ling", "Hejing", ""], ["Liu", "Guoliang", ""], ["Tian", "Guohui", ""]]}, {"id": "2010.11679", "submitter": "Shudeng Wu", "authors": "Shudeng Wu, Tao Dai, Shu-Tao Xia", "title": "DPAttack: Diffused Patch Attacks against Universal Object Detection", "comments": "4 pages, 2 figures, CIKM Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, deep neural networks (DNNs) have been widely and successfully used\nin Object Detection, e.g. Faster RCNN, YOLO, CenterNet. However, recent studies\nhave shown that DNNs are vulnerable to adversarial attacks. Adversarial attacks\nagainst object detection can be divided into two categories, whole-pixel\nattacks and patch attacks. While these attacks add perturbations to a large\nnumber of pixels in images, we proposed a diffused patch attack\n(\\textbf{DPAttack}) to successfully fool object detectors by diffused patches\nof asteroid-shaped or grid-shape, which only change a small number of pixels.\nExperiments show that our DPAttack can successfully fool most object detectors\nwith diffused patches and we get the second place in the Alibaba Tianchi\ncompetition: Alibaba-Tsinghua Adversarial Challenge on Object Detection. Our\ncode can be obtained from https://github.com/Wu-Shudeng/DPAttack.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 04:48:24 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Wu", "Shudeng", ""], ["Dai", "Tao", ""], ["Xia", "Shu-Tao", ""]]}, {"id": "2010.11681", "submitter": "Sumanth Chennupati", "authors": "Sumanth Chennupati, Venkatraman Narayanan, Ganesh Sistu, Senthil\n  Yogamani and Samir A Rawashdeh", "title": "Learning Panoptic Segmentation from Instance Contours", "comments": "Accepted at ICRA 2021. Overview Video: https://youtu.be/wBtcxRhG3e0", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Panoptic Segmentation aims to provide an understanding of background (stuff)\nand instances of objects (things) at a pixel level. It combines the separate\ntasks of semantic segmentation (pixel level classification) and instance\nsegmentation to build a single unified scene understanding task. Typically,\npanoptic segmentation is derived by combining semantic and instance\nsegmentation tasks that are learned separately or jointly (multi-task\nnetworks). In general, instance segmentation networks are built by adding a\nforeground mask estimation layer on top of object detectors or using instance\nclustering methods that assign a pixel to an instance center. In this work, we\npresent a fully convolution neural network that learns instance segmentation\nfrom semantic segmentation and instance contours (boundaries of things).\nInstance contours along with semantic segmentation yield a boundary aware\nsemantic segmentation of things. Connected component labeling on these results\nproduces instance segmentation. We merge semantic and instance segmentation\nresults to output panoptic segmentation. We evaluate our proposed method on the\nCityScapes dataset to demonstrate qualitative and quantitative performances\nalong with several ablation studies. Our overview video can be accessed from\nurl:https://youtu.be/wBtcxRhG3e0.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 03:05:48 GMT"}, {"version": "v2", "created": "Tue, 6 Apr 2021 01:09:26 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Chennupati", "Sumanth", ""], ["Narayanan", "Venkatraman", ""], ["Sistu", "Ganesh", ""], ["Yogamani", "Senthil", ""], ["Rawashdeh", "Samir A", ""]]}, {"id": "2010.11682", "submitter": "Sumeet Menon", "authors": "Kushal Mehta, Arshita Jain, Jayalakshmi Mangalagiri, Sumeet Menon,\n  Phuong Nguyen, David R. Chapman", "title": "Lung Nodule Classification Using Biomarkers, Volumetric Radiomics and 3D\n  CNNs", "comments": "This paper has been submitted to the Journal of Digital Imaging (JDI\n  2020). The poster of this paper has received the 2nd prize for the Research\n  Poster Award. Link: https://siim.org/page/20m_p_lung_node_malignancy", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a hybrid algorithm to estimate lung nodule malignancy that\ncombines imaging biomarkers from Radiologist's annotation with image\nclassification of CT scans. Our algorithm employs a 3D Convolutional Neural\nNetwork (CNN) as well as a Random Forest in order to combine CT imagery with\nbiomarker annotation and volumetric radiomic features. We analyze and compare\nthe performance of the algorithm using only imagery, only biomarkers, combined\nimagery + biomarkers, combined imagery + volumetric radiomic features and\nfinally the combination of imagery + biomarkers + volumetric features in order\nto classify the suspicion level of nodule malignancy. The National Cancer\nInstitute (NCI) Lung Image Database Consortium (LIDC) IDRI dataset is used to\ntrain and evaluate the classification task. We show that the incorporation of\nsemi-supervised learning by means of K-Nearest-Neighbors (KNN) can increase the\navailable training sample size of the LIDC-IDRI thereby further improving the\naccuracy of malignancy estimation of most of the models tested although there\nis no significant improvement with the use of KNN semi-supervised learning if\nimage classification with CNNs and volumetric features are combined with\ndescriptive biomarkers. Unexpectedly, we also show that a model using image\nbiomarkers alone is more accurate than one that combines biomarkers with\nvolumetric radiomics, 3D CNNs, and semi-supervised learning. We discuss the\npossibility that this result may be influenced by cognitive bias in LIDC-IDRI\nbecause malignancy estimates were recorded by the same radiologist panel as\nbiomarkers, as well as future work to incorporate pathology information over a\nsubset of study participants.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 18:57:26 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Mehta", "Kushal", ""], ["Jain", "Arshita", ""], ["Mangalagiri", "Jayalakshmi", ""], ["Menon", "Sumeet", ""], ["Nguyen", "Phuong", ""], ["Chapman", "David R.", ""]]}, {"id": "2010.11684", "submitter": "Jiantao Wu", "authors": "Jiantao Wu and Lin Wang", "title": "Disentangling Action Sequences: Discovering Correlated Samples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Disentanglement is a highly desirable property of representation due to its\nsimilarity with human's understanding and reasoning. This improves\ninterpretability, enables the performance of down-stream tasks, and enables\ncontrollable generative models. However, this domain is challenged by the\nabstract notion and incomplete theories to support unsupervised disentanglement\nlearning. We demonstrate the data itself, such as the orientation of images,\nplays a crucial role in disentanglement and instead of the factors, and the\ndisentangled representations align the latent variables with the action\nsequences. We further introduce the concept of disentangling action sequences\nwhich facilitates the description of the behaviours of the existing\ndisentangling approaches. An analogy for this process is to discover the\ncommonality between the things and categorizing them. Furthermore, we analyze\nthe inductive biases on the data and find that the latent information\nthresholds are correlated with the significance of the actions. For the\nsupervised and unsupervised settings, we respectively introduce two methods to\nmeasure the thresholds. We further propose a novel framework, fractional\nvariational autoencoder (FVAE), to disentangle the action sequences with\ndifferent significance step-by-step. Experimental results on dSprites and 3D\nChairs show that FVAE improves the stability of disentanglement.\n", "versions": [{"version": "v1", "created": "Sat, 17 Oct 2020 07:37:50 GMT"}], "update_date": "2020-10-24", "authors_parsed": [["Wu", "Jiantao", ""], ["Wang", "Lin", ""]]}, {"id": "2010.11685", "submitter": "Zilong Wang", "authors": "Zilong Wang, Mingjie Zhan, Xuebo Liu, Ding Liang", "title": "DocStruct: A Multimodal Method to Extract Hierarchy Structure in\n  Document for General Form Understanding", "comments": "Accepted to EMNLP 2020 Findings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Form understanding depends on both textual contents and organizational\nstructure. Although modern OCR performs well, it is still challenging to\nrealize general form understanding because forms are commonly used and of\nvarious formats. The table detection and handcrafted features in previous works\ncannot apply to all forms because of their requirements on formats. Therefore,\nwe concentrate on the most elementary components, the key-value pairs, and\nadopt multimodal methods to extract features. We consider the form structure as\na tree-like or graph-like hierarchy of text fragments. The parent-child\nrelation corresponds to the key-value pairs in forms. We utilize the\nstate-of-the-art models and design targeted extraction modules to extract\nmultimodal features from semantic contents, layout information, and visual\nimages. A hybrid fusion method of concatenation and feature shifting is\ndesigned to fuse the heterogeneous features and provide an informative joint\nrepresentation. We adopt an asymmetric algorithm and negative sampling in our\nmodel as well. We validate our method on two benchmarks, MedForm and FUNSD, and\nextensive experiments demonstrate the effectiveness of our method.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 08:54:17 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Wang", "Zilong", ""], ["Zhan", "Mingjie", ""], ["Liu", "Xuebo", ""], ["Liang", "Ding", ""]]}, {"id": "2010.11686", "submitter": "Ren-Song Tsay", "authors": "Tsung-Ying Lu, Hsu-Hsun Chin, Hsin-I Wu, and Ren-Song Tsay", "title": "A Very Compact Embedded CNN Processor Design Based on Logarithmic\n  Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AR cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we propose a very compact embedded CNN processor design based\non a modified logarithmic computing method using very low bit-width\nrepresentation. Our high-quality CNN processor can easily fit into edge\ndevices. For Yolov2, our processing circuit takes only 0.15 mm2 using TSMC 40\nnm cell library. The key idea is to constrain the activation and weight values\nof all layers uniformly to be within the range [-1, 1] and produce low\nbit-width logarithmic representation. With the uniform representations, we\ndevise a unified, reusable CNN computing kernel and significantly reduce\ncomputing resources. The proposed approach has been extensively evaluated on\nmany popular image classification CNN models (AlexNet, VGG16, and ResNet-18/34)\nand object detection models (Yolov2). The hardware-implemented results show\nthat our design consumes only minimal computing and storage resources, yet\nattains very high accuracy. The design is thoroughly verified on FPGAs, and the\nSoC integration is underway with promising results. With extremely efficient\nresource and energy usage, our design is excellent for edge computing purposes.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 23:48:36 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Lu", "Tsung-Ying", ""], ["Chin", "Hsu-Hsun", ""], ["Wu", "Hsin-I", ""], ["Tsay", "Ren-Song", ""]]}, {"id": "2010.11687", "submitter": "Christopher Hahne", "authors": "Christopher Hahne and Amar Aggoun", "title": "PlenoptiCam v1.0: A light-field imaging framework", "comments": "final author version", "journal-ref": null, "doi": "10.1109/TIP.2021.3095671", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Light-field cameras play a vital role for rich 3-D information retrieval in\nnarrow range depth sensing applications. The key obstacle in composing\nlight-fields from exposures taken by a plenoptic camera is to computationally\ncalibrate, align and rearrange four-dimensional image data. Several attempts\nhave been proposed to enhance the overall image quality by tailoring pipelines\ndedicated to particular plenoptic cameras and improving the consistency across\nviewpoints at the expense of high computational loads. The framework presented\nherein advances prior outcomes thanks to its novel micro image scale-space\nanalysis for generic camera calibration independent of the lens specifications\nand its parallax-invariant, cost-effective viewpoint color equalization from\noptimal transport theory. Artifacts from the sensor and micro lens grid are\ncompensated in an innovative way to enable superior quality in sub-aperture\nimage extraction, computational refocusing and Scheimpflug rendering with\nsub-sampling capabilities. Benchmark comparisons using established image\nmetrics suggest that our proposed pipeline outperforms state-of-the-art tool\nchains in the majority of cases. Results from a Wasserstein distance further\nshow that our color transfer outdoes the existing transport methods. Our\nalgorithms are released under an open-source license, offer cross-platform\ncompatibility with few dependencies and different user interfaces. This makes\nthe reproduction of results and experimentation with plenoptic camera\ntechnology convenient for peer researchers, developers, photographers, data\nscientists and others working in this field.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 09:23:18 GMT"}, {"version": "v2", "created": "Sun, 7 Mar 2021 22:55:07 GMT"}, {"version": "v3", "created": "Sat, 1 May 2021 07:45:38 GMT"}, {"version": "v4", "created": "Thu, 27 May 2021 22:23:00 GMT"}, {"version": "v5", "created": "Sun, 25 Jul 2021 17:38:22 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Hahne", "Christopher", ""], ["Aggoun", "Amar", ""]]}, {"id": "2010.11689", "submitter": "Fariborz Taherkhani", "authors": "Moktari Mostofa, Fariborz Taherkhani, Jeremy Dawson, Nasser M.\n  Nasrabadi", "title": "Cross-Spectral Iris Matching Using Conditional Coupled GAN", "comments": "International Joint Conference on Biometrics (IJCB-2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Cross-spectral iris recognition is emerging as a promising biometric approach\nto authenticating the identity of individuals. However, matching iris images\nacquired at different spectral bands shows significant performance degradation\nwhen compared to single-band near-infrared (NIR) matching due to the spectral\ngap between iris images obtained in the NIR and visual-light (VIS) spectra.\nAlthough researchers have recently focused on deep-learning-based approaches to\nrecover invariant representative features for more accurate recognition\nperformance, the existing methods cannot achieve the expected accuracy required\nfor commercial applications. Hence, in this paper, we propose a conditional\ncoupled generative adversarial network (CpGAN) architecture for cross-spectral\niris recognition by projecting the VIS and NIR iris images into a\nlow-dimensional embedding domain to explore the hidden relationship between\nthem. The conditional CpGAN framework consists of a pair of GAN-based networks,\none responsible for retrieving images in the visible domain and other\nresponsible for retrieving images in the NIR domain. Both networks try to map\nthe data into a common embedding subspace to ensure maximum pair-wise\nsimilarity between the feature vectors from the two iris modalities of the same\nsubject. To prove the usefulness of our proposed approach, extensive\nexperimental results obtained on the PolyU dataset are compared to existing\nstate-of-the-art cross-spectral recognition methods.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 19:13:24 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Mostofa", "Moktari", ""], ["Taherkhani", "Fariborz", ""], ["Dawson", "Jeremy", ""], ["Nasrabadi", "Nasser M.", ""]]}, {"id": "2010.11691", "submitter": "Jan Cejka", "authors": "Jan \\v{C}ejka, Fotis Liarokapis", "title": "Tackling problems of marker-based augmented reality under water", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-37191-3_11", "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Underwater sites are a harsh environment for augmented reality applications.\nObstacles that must be battled include poor visibility conditions, difficult\nnavigation, and hard manipulation with devices under water. This chapter\nfocuses on the problem of localizing a device under water using markers. It\ndiscusses various filters that enhance and improve images recorded under water,\nand their impact on marker-based tracking. It presents various combinations of\n10 image improving algorithms and 4 marker detecting algorithms, and tests\ntheir performance in real situations. All solutions are designed to run\nreal-time on mobile devices to provide a solid basis for augmented reality.\nUsability of this solution is evaluated on locations in Mediterranean Sea. It\nis shown that image improving algorithms with carefully chosen parameters can\nreduce the problems with visibility under water and improve the detection of\nmarkers. The best results are obtained with marker detecting algorithms that\nare specifically designed for underwater environments.\n", "versions": [{"version": "v1", "created": "Sun, 11 Oct 2020 09:54:13 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["\u010cejka", "Jan", ""], ["Liarokapis", "Fotis", ""]]}, {"id": "2010.11692", "submitter": "Mihir Rao", "authors": "Mihir Rao, Michelle Zhu, Tianyang Wang", "title": "Conversion and Implementation of State-of-the-Art Deep Learning\n  Algorithms for the Classification of Diabetic Retinopathy", "comments": "Pre-print version (in-review)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diabetic retinopathy (DR) is a retinal microvascular condition that emerges\nin diabetic patients. DR will continue to be a leading cause of blindness\nworldwide, with a predicted 191.0 million globally diagnosed patients in 2030.\nMicroaneurysms, hemorrhages, exudates, and cotton wool spots are common signs\nof DR. However, they can be small and hard for human eyes to detect. Early\ndetection of DR is crucial for effective clinical treatment. Existing methods\nto classify images require much time for feature extraction and selection, and\nare limited in their performance. Convolutional Neural Networks (CNNs), as an\nemerging deep learning (DL) method, have proven their potential in image\nclassification tasks. In this paper, comprehensive experimental studies of\nimplementing state-of-the-art CNNs for the detection and classification of DR\nare conducted in order to determine the top performing classifiers for the\ntask. Five CNN classifiers, namely Inception-V3, VGG19, VGG16, ResNet50, and\nInceptionResNetV2, are evaluated through experiments. They categorize medical\nimages into five different classes based on DR severity. Data augmentation and\ntransfer learning techniques are applied since annotated medical images are\nlimited and imbalanced. Experimental results indicate that the ResNet50\nclassifier has top performance for binary classification and that the\nInceptionResNetV2 classifier has top performance for multi-class DR\nclassification.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 20:42:14 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Rao", "Mihir", ""], ["Zhu", "Michelle", ""], ["Wang", "Tianyang", ""]]}, {"id": "2010.11694", "submitter": "Feng Wang", "authors": "Feng Wang, Huaping Liu, Di Guo, Fuchun Sun", "title": "Unsupervised Representation Learning by InvariancePropagation", "comments": "Accepted to NeurIPS 2020 (spotlight presentation)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised learning methods based on contrastive learning have drawn\nincreasing attention and achieved promising results. Most of them aim to learn\nrepresentations invariant to instance-level variations, which are provided by\ndifferent views of the same instance. In this paper, we propose Invariance\nPropagation to focus on learning representations invariant to category-level\nvariations, which are provided by different instances from the same category.\nOur method recursively discovers semantically consistent samples residing in\nthe same high-density regions in representation space. We demonstrate a hard\nsampling strategy to concentrate on maximizing the agreement between the anchor\nsample and its hard positive samples, which provide more intra-class variations\nto help capture more abstract invariance. As a result, with a ResNet-50 as the\nbackbone, our method achieves 71.3% top-1 accuracy on ImageNet linear\nclassification and 78.2% top-5 accuracy fine-tuning on only 1% labels,\nsurpassing previous results. We also achieve state-of-the-art performance on\nother downstream tasks, including linear classification on Places205 and Pascal\nVOC, and transfer learning on small scale datasets.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 13:00:33 GMT"}, {"version": "v2", "created": "Tue, 3 Nov 2020 07:14:44 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Wang", "Feng", ""], ["Liu", "Huaping", ""], ["Guo", "Di", ""], ["Sun", "Fuchun", ""]]}, {"id": "2010.11695", "submitter": "Ju Xu", "authors": "Ju Xu, Mengzhang Li, Zhanxing Zhu", "title": "Automatic Data Augmentation for 3D Medical Image Segmentation", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Data augmentation is an effective and universal technique for improving\ngeneralization performance of deep neural networks. It could enrich diversity\nof training samples that is essential in medical image segmentation tasks\nbecause 1) the scale of medical image dataset is typically smaller, which may\nincrease the risk of overfitting; 2) the shape and modality of different\nobjects such as organs or tumors are unique, thus requiring customized data\naugmentation policy. However, most data augmentation implementations are\nhand-crafted and suboptimal in medical image processing. To fully exploit the\npotential of data augmentation, we propose an efficient algorithm to\nautomatically search for the optimal augmentation strategies. We formulate the\ncoupled optimization w.r.t. network weights and augmentation parameters into a\ndifferentiable form by means of stochastic relaxation. This formulation allows\nus to apply alternative gradient-based methods to solve it, i.e. stochastic\nnatural gradient method with adaptive step-size. To the best of our knowledge,\nit is the first time that differentiable automatic data augmentation is\nemployed in medical image segmentation tasks. Our numerical experiments\ndemonstrate that the proposed approach significantly outperforms existing\nbuild-in data augmentation of state-of-the-art models.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 12:51:17 GMT"}, {"version": "v2", "created": "Sun, 27 Dec 2020 10:56:02 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Xu", "Ju", ""], ["Li", "Mengzhang", ""], ["Zhu", "Zhanxing", ""]]}, {"id": "2010.11696", "submitter": "Christoph Heindl", "authors": "Christoph Heindl, Lukas Brunner, Sebastian Zambal, Josef Scharinger", "title": "BlendTorch: A Real-Time, Adaptive Domain Randomization Library", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Solving complex computer vision tasks by deep learning techniques relies on\nlarge amounts of (supervised) image data, typically unavailable in industrial\nenvironments. The lack of training data starts to impede the successful\ntransfer of state-of-the-art methods in computer vision to industrial\napplications. We introduce BlendTorch, an adaptive Domain Randomization (DR)\nlibrary, to help creating infinite streams of synthetic training data.\nBlendTorch generates data by massively randomizing low-fidelity simulations and\ntakes care of distributing artificial training data for model learning in\nreal-time. We show that models trained with BlendTorch repeatedly perform\nbetter in an industrial object detection task than those trained on real or\nphoto-realistic datasets.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 16:13:39 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Heindl", "Christoph", ""], ["Brunner", "Lukas", ""], ["Zambal", "Sebastian", ""], ["Scharinger", "Josef", ""]]}, {"id": "2010.11697", "submitter": "Federico Milani", "authors": "Federico Milani and Piero Fraternali", "title": "A Data Set and a Convolutional Model for Iconography Classification in\n  Paintings", "comments": "Published at ACM Journal on Computing and Cultural Heritage (JOCCH)\n  https://doi.org/10.1145/3458885", "journal-ref": "Journal on Computing and Cultural Heritage (JOCCH), 2021, 14.4:\n  1-18", "doi": "10.1145/3458885", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Iconography in art is the discipline that studies the visual content of\nartworks to determine their motifs and themes andto characterize the way these\nare represented. It is a subject of active research for a variety of purposes,\nincluding the interpretation of meaning, the investigation of the origin and\ndiffusion in time and space of representations, and the study of influences\nacross artists and art works. With the proliferation of digital archives of art\nimages, the possibility arises of applying Computer Vision techniques to the\nanalysis of art images at an unprecedented scale, which may support iconography\nresearch and education. In this paper we introduce a novel paintings data set\nfor iconography classification and present the quantitativeand qualitative\nresults of applying a Convolutional Neural Network (CNN) classifier to the\nrecognition of the iconography of artworks. The proposed classifier achieves\ngood performances (71.17% Precision, 70.89% Recall, 70.25% F1-Score and 72.73%\nAverage Precision) in the task of identifying saints in Christian religious\npaintings, a task made difficult by the presence of classes with very similar\nvisual features. Qualitative analysis of the results shows that the CNN focuses\non the traditional iconic motifs that characterize the representation of each\nsaint and exploits such hints to attain correct identification. The ultimate\ngoal of our work is to enable the automatic extraction, decomposition, and\ncomparison of iconography elements to support iconographic studies and\nautomatic art work annotation.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 12:40:46 GMT"}, {"version": "v2", "created": "Wed, 11 Nov 2020 10:32:20 GMT"}, {"version": "v3", "created": "Mon, 26 Jul 2021 12:27:36 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Milani", "Federico", ""], ["Fraternali", "Piero", ""]]}, {"id": "2010.11698", "submitter": "Michael Girard", "authors": "Haris Cheong, Sripad Krishna Devalla, Thanadet Chuangsuwanich, Tin A.\n  Tun, Xiaofei Wang, Tin Aung, Leopold Schmetterer, Martin L. Buist, Craig\n  Boote, Alexandre H. Thi\\'ery, and Micha\\\"el J. A. Girard", "title": "OCT-GAN: Single Step Shadow and Noise Removal from Optical Coherence\n  Tomography Images of the Human Optic Nerve Head", "comments": "20 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Speckle noise and retinal shadows within OCT B-scans occlude important edges,\nfine textures and deep tissues, preventing accurate and robust diagnosis by\nalgorithms and clinicians. We developed a single process that successfully\nremoved both noise and retinal shadows from unseen single-frame B-scans within\n10.4ms. Mean average gradient magnitude (AGM) for the proposed algorithm was\n57.2% higher than current state-of-the-art, while mean peak signal to noise\nratio (PSNR), contrast to noise ratio (CNR), and structural similarity index\nmetric (SSIM) increased by 11.1%, 154% and 187% respectively compared to\nsingle-frame B-scans. Mean intralayer contrast (ILC) improvement for the\nretinal nerve fiber layer (RNFL), photoreceptor layer (PR) and retinal pigment\nepithelium (RPE) layers decreased from 0.362 \\pm 0.133 to 0.142 \\pm 0.102,\n0.449 \\pm 0.116 to 0.0904 \\pm 0.0769, 0.381 \\pm 0.100 to 0.0590 \\pm 0.0451\nrespectively. The proposed algorithm reduces the necessity for long image\nacquisition times, minimizes expensive hardware requirements and reduces motion\nartifacts in OCT images.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 08:32:32 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Cheong", "Haris", ""], ["Devalla", "Sripad Krishna", ""], ["Chuangsuwanich", "Thanadet", ""], ["Tun", "Tin A.", ""], ["Wang", "Xiaofei", ""], ["Aung", "Tin", ""], ["Schmetterer", "Leopold", ""], ["Buist", "Martin L.", ""], ["Boote", "Craig", ""], ["Thi\u00e9ry", "Alexandre H.", ""], ["Girard", "Micha\u00ebl J. A.", ""]]}, {"id": "2010.11699", "submitter": "Anthony Bourached", "authors": "Anthony Bourached, Ryan-Rhys Griffiths, Robert Gray, Ashwani Jha,\n  Parashkev Nachev", "title": "Generative Model-Enhanced Human Motion Prediction", "comments": "8 pages + 5 pages supplementary materials, under review at ICLR", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of predicting human motion is complicated by the natural\nheterogeneity and compositionality of actions, necessitating robustness to\ndistributional shifts as far as out-of-distribution (OoD). Here we formulate a\nnew OoD benchmark based on the Human3.6M and CMU motion capture datasets, and\nintroduce a hybrid framework for hardening discriminative architectures to OoD\nfailure by augmenting them with a generative model. When applied to current\nstate-of-the-art discriminative models, we show that the proposed approach\nimproves OoD robustness without sacrificing in-distribution performance, and\ncan theoretically facilitate model interpretability. We suggest human motion\npredictors ought to be constructed with OoD challenges in mind, and provide an\nextensible general framework for hardening diverse discriminative architectures\nto extreme distributional shift. The code is available at\nhttps://github.com/bouracha/OoDMotion.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 17:04:34 GMT"}, {"version": "v2", "created": "Tue, 24 Nov 2020 13:25:21 GMT"}, {"version": "v3", "created": "Wed, 25 Nov 2020 10:16:28 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Bourached", "Anthony", ""], ["Griffiths", "Ryan-Rhys", ""], ["Gray", "Robert", ""], ["Jha", "Ashwani", ""], ["Nachev", "Parashkev", ""]]}, {"id": "2010.11700", "submitter": "Fadi Boutros", "authors": "Fadi Boutros, Naser Damer, Kiran Raja, Raghavendra Ramachandra,\n  Florian Kirchbuchner and Arjan Kuijper", "title": "On Benchmarking Iris Recognition within a Head-mounted Display for AR/VR\n  Application", "comments": "Accepted at International Join Conference on Biometrics (IJCB 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Augmented and virtual reality is being deployed in different fields of\napplications. Such applications might involve accessing or processing critical\nand sensitive information, which requires strict and continuous access control.\nGiven that Head-Mounted Displays (HMD) developed for such applications commonly\ncontains internal cameras for gaze tracking purposes, we evaluate the\nsuitability of such setup for verifying the users through iris recognition. In\nthis work, we first evaluate a set of iris recognition algorithms suitable for\nHMD devices by investigating three well-established handcrafted feature\nextraction approaches, and to complement it, we also present the analysis using\nfour deep learning models. While taking into consideration the minimalistic\nhardware requirements of stand-alone HMD, we employ and adapt a recently\ndeveloped miniature segmentation model (EyeMMS) for segmenting the iris.\nFurther, to account for non-ideal and non-collaborative capture of iris, we\ndefine a new iris quality metric that we termed as Iris Mask Ratio (IMR) to\nquantify the iris recognition performance. Motivated by the performance of iris\nrecognition, we also propose the continuous authentication of users in a\nnon-collaborative capture setting in HMD. Through the experiments on a publicly\navailable OpenEDS dataset, we show that performance with EER = 5% can be\nachieved using deep learning methods in a general setting, along with high\naccuracy for continuous user authentication.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 17:05:11 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Boutros", "Fadi", ""], ["Damer", "Naser", ""], ["Raja", "Kiran", ""], ["Ramachandra", "Raghavendra", ""], ["Kirchbuchner", "Florian", ""], ["Kuijper", "Arjan", ""]]}, {"id": "2010.11701", "submitter": "Philipp Sadler", "authors": "Philipp Sadler", "title": "Spatial Attention as an Interface for Image Captioning Models", "comments": "A thesis submitted in fulfillment of the requirements for the degree\n  Master of Science in Cognitive Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The internal workings of modern deep learning models stay often unclear to an\nexternal observer, although spatial attention mechanisms are involved. The idea\nof this work is to translate these spatial attentions into natural language to\nprovide a simpler access to the model's function. Thus, I took a neural image\ncaptioning model and measured the reactions to external modification in its\nspatial attention for three different interface methods: a fixation over the\nwhole generation process, a fixation for the first time-steps and an addition\nto the generator's attention. The experimental results for bounding box based\nspatial attention vectors have shown that the captioning model reacts to method\ndependent changes in up to 52.65% and includes in 9.00% of the cases object\ncategories, which were otherwise unmentioned. Afterwards, I established such a\nlink to a hierarchical co-attention network for visual question answering by\nextraction of its word, phrase and question level spatial attentions. Here,\ngenerated captions for the word level included details of the question-answer\npairs in up to 55.20% of the cases. This work indicates that spatial attention\nseen as an external interface for image caption generators is an useful method\nto access visual functions in natural language.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 16:04:08 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Sadler", "Philipp", ""]]}, {"id": "2010.11702", "submitter": "Jianhao Jiao", "authors": "Jianhao Jiao, Peng Yun, Lei Tai, Ming Liu", "title": "MLOD: Awareness of Extrinsic Perturbation in Multi-LiDAR 3D Object\n  Detection for Autonomous Driving", "comments": "8 pages, 6 figures", "journal-ref": "IROS 2020", "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extrinsic perturbation always exists in multiple sensors. In this paper, we\nfocus on the extrinsic uncertainty in multi-LiDAR systems for 3D object\ndetection. We first analyze the influence of extrinsic perturbation on\ngeometric tasks with two basic examples. To minimize the detrimental effect of\nextrinsic perturbation, we propagate an uncertainty prior on each point of\ninput point clouds, and use this information to boost an approach for 3D\ngeometric tasks. Then we extend our findings to propose a multi-LiDAR 3D object\ndetector called MLOD. MLOD is a two-stage network where the multi-LiDAR\ninformation is fused through various schemes in stage one, and the extrinsic\nperturbation is handled in stage two. We conduct extensive experiments on a\nreal-world dataset, and demonstrate both the accuracy and robustness\nimprovement of MLOD. The code, data and supplementary materials are available\nat: https://ram-lab.com/file/site/mlod\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 06:11:22 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Jiao", "Jianhao", ""], ["Yun", "Peng", ""], ["Tai", "Lei", ""], ["Liu", "Ming", ""]]}, {"id": "2010.11703", "submitter": "Shan An", "authors": "Shan An, Haogang Zhu, Dong Wei, Konstantinos A. Tsintotas", "title": "Fast and Incremental Loop Closure Detection with Deep Features and\n  Proximity Graphs", "comments": "submitted to Transactions on Robotics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, methods concerning the place recognition task have been\nextensively examined from the robotics community within the scope of\nsimultaneous localization and mapping applications. In this article, an\nappearance-based loop closure detection pipeline is proposed, entitled \"FILD++\"\n(Fast and Incremental Loop closure Detection). When the incoming camera\nobservation arrives, global and local visual features are extracted through two\npasses of a single convolutional neural network. Subsequently, a modified\nhierarchical-navigable small-world graph incrementally generates a visual\ndatabase that represents the robot's traversed path based on global features.\nGiven the query sensor measurement, similar locations from the trajectory are\nretrieved using these representations, while an image-to-image pairing is\nfurther evaluated thanks to the spatial information provided by the local\nfeatures. Exhaustive experiments on several publicly-available datasets exhibit\nthe system's high performance and low execution time compared to other\ncontemporary state-of-the-art pipelines.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 02:42:47 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["An", "Shan", ""], ["Zhu", "Haogang", ""], ["Wei", "Dong", ""], ["Tsintotas", "Konstantinos A.", ""]]}, {"id": "2010.11704", "submitter": "Neil Sachdeva", "authors": "Neil Sachdeva, Misha Klopukh, Rachel St. Clair, William Hahn", "title": "Using Conditional Generative Adversarial Networks to Reduce the Effects\n  of Latency in Robotic Telesurgery", "comments": "6 pages with 5 figures and 1 table. J Robotic Surg (2020)", "journal-ref": null, "doi": "10.1007/s11701-020-01149-5", "report-no": null, "categories": "cs.CV cs.AI cs.RO eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The introduction of surgical robots brought about advancements in surgical\nprocedures. The applications of remote telesurgery range from building medical\nclinics in underprivileged areas, to placing robots abroad in military\nhot-spots where accessibility and diversity of medical experience may be\nlimited. Poor wireless connectivity may result in a prolonged delay, referred\nto as latency, between a surgeon's input and action a robot takes. In surgery,\nany micro-delay can injure a patient severely and in some cases, result in\nfatality. One was to increase safety is to mitigate the effects of latency\nusing deep learning aided computer vision. While the current surgical robots\nuse calibrated sensors to measure the position of the arms and tools, in this\nwork we present a purely optical approach that provides a measurement of the\ntool position in relation to the patient's tissues. This research aimed to\nproduce a neural network that allowed a robot to detect its own mechanical\nmanipulator arms. A conditional generative adversarial networks (cGAN) was\ntrained on 1107 frames of mock gastrointestinal robotic surgery data from the\n2015 EndoVis Instrument Challenge and corresponding hand-drawn labels for each\nframe. When run on new testing data, the network generated near-perfect labels\nof the input images which were visually consistent with the hand-drawn labels\nand was able to do this in 299 milliseconds. These accurately generated labels\ncan then be used as simplified identifiers for the robot to track its own\ncontrolled tools. These results show potential for conditional GANs as a\nreaction mechanism such that the robot can detect when its arms move outside\nthe operating area within a patient. This system allows for more accurate\nmonitoring of the position of surgical instruments in relation to the patient's\ntissue, increasing safety measures that are integral to successful telesurgery\nsystems.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 13:40:44 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Sachdeva", "Neil", ""], ["Klopukh", "Misha", ""], ["Clair", "Rachel St.", ""], ["Hahn", "William", ""]]}, {"id": "2010.11714", "submitter": "Yukuan Yang", "authors": "Yukuan Yang, Fangyun Wei, Miaojing Shi, Guoqi Li", "title": "Restoring Negative Information in Few-Shot Object Detection", "comments": "To appear in NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Few-shot learning has recently emerged as a new challenge in the deep\nlearning field: unlike conventional methods that train the deep neural networks\n(DNNs) with a large number of labeled data, it asks for the generalization of\nDNNs on new classes with few annotated samples. Recent advances in few-shot\nlearning mainly focus on image classification while in this paper we focus on\nobject detection. The initial explorations in few-shot object detection tend to\nsimulate a classification scenario by using the positive proposals in images\nwith respect to certain object class while discarding the negative proposals of\nthat class. Negatives, especially hard negatives, however, are essential to the\nembedding space learning in few-shot object detection. In this paper, we\nrestore the negative information in few-shot object detection by introducing a\nnew negative- and positive-representative based metric learning framework and a\nnew inference scheme with negative and positive representatives. We build our\nwork on a recent few-shot pipeline RepMet with several new modules to encode\nnegative information for both training and testing. Extensive experiments on\nImageNet-LOC and PASCAL VOC show our method substantially improves the\nstate-of-the-art few-shot object detection solutions. Our code is available at\nhttps://github.com/yang-yk/NP-RepMet.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 13:39:48 GMT"}, {"version": "v2", "created": "Mon, 26 Oct 2020 03:33:08 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Yang", "Yukuan", ""], ["Wei", "Fangyun", ""], ["Shi", "Miaojing", ""], ["Li", "Guoqi", ""]]}, {"id": "2010.11724", "submitter": "Yunchao Wei", "authors": "Yunchao Wei, Shuai Zheng, Ming-Ming Cheng, Hang Zhao, Liwei Wang,\n  Errui Ding, Yi Yang, Antonio Torralba, Ting Liu, Guolei Sun, Wenguan Wang,\n  Luc Van Gool, Wonho Bae, Junhyug Noh, Jinhwan Seo, Gunhee Kim, Hao Zhao, Ming\n  Lu, Anbang Yao, Yiwen Guo, Yurong Chen, Li Zhang, Chuangchuang Tan, Tao Ruan,\n  Guanghua Gu, Shikui Wei, Yao Zhao, Mariia Dobko, Ostap Viniavskyi, Oles\n  Dobosevych, Zhendong Wang, Zhenyuan Chen, Chen Gong, Huanqing Yan, Jun He", "title": "LID 2020: The Learning from Imperfect Data Challenge Results", "comments": "Summary of the 2nd Learning from Imperfect Data Workshop in\n  conjunction with CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning from imperfect data becomes an issue in many industrial applications\nafter the research community has made profound progress in supervised learning\nfrom perfectly annotated datasets. The purpose of the Learning from Imperfect\nData (LID) workshop is to inspire and facilitate the research in developing\nnovel approaches that would harness the imperfect data and improve the\ndata-efficiency during training. A massive amount of user-generated data\nnowadays available on multiple internet services. How to leverage those and\nimprove the machine learning models is a high impact problem. We organize the\nchallenges in conjunction with the workshop. The goal of these challenges is to\nfind the state-of-the-art approaches in the weakly supervised learning setting\nfor object detection, semantic segmentation, and scene parsing. There are three\ntracks in the challenge, i.e., weakly supervised semantic segmentation (Track\n1), weakly supervised scene parsing (Track 2), and weakly supervised object\nlocalization (Track 3). In Track 1, based on ILSVRC DET, we provide pixel-level\nannotations of 15K images from 200 categories for evaluation. In Track 2, we\nprovide point-based annotations for the training set of ADE20K. In Track 3,\nbased on ILSVRC CLS-LOC, we provide pixel-level annotations of 44,271 images\nfor evaluation. Besides, we further introduce a new evaluation metric proposed\nby \\cite{zhang2020rethinking}, i.e., IoU curve, to measure the quality of the\ngenerated object localization maps. This technical report summarizes the\nhighlights from the challenge. The challenge submission server and the\nleaderboard will continue to open for the researchers who are interested in it.\nMore details regarding the challenge and the benchmarks are available at\nhttps://lidchallenge.github.io\n", "versions": [{"version": "v1", "created": "Sat, 17 Oct 2020 13:06:12 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Wei", "Yunchao", ""], ["Zheng", "Shuai", ""], ["Cheng", "Ming-Ming", ""], ["Zhao", "Hang", ""], ["Wang", "Liwei", ""], ["Ding", "Errui", ""], ["Yang", "Yi", ""], ["Torralba", "Antonio", ""], ["Liu", "Ting", ""], ["Sun", "Guolei", ""], ["Wang", "Wenguan", ""], ["Van Gool", "Luc", ""], ["Bae", "Wonho", ""], ["Noh", "Junhyug", ""], ["Seo", "Jinhwan", ""], ["Kim", "Gunhee", ""], ["Zhao", "Hao", ""], ["Lu", "Ming", ""], ["Yao", "Anbang", ""], ["Guo", "Yiwen", ""], ["Chen", "Yurong", ""], ["Zhang", "Li", ""], ["Tan", "Chuangchuang", ""], ["Ruan", "Tao", ""], ["Gu", "Guanghua", ""], ["Wei", "Shikui", ""], ["Zhao", "Yao", ""], ["Dobko", "Mariia", ""], ["Viniavskyi", "Ostap", ""], ["Dobosevych", "Oles", ""], ["Wang", "Zhendong", ""], ["Chen", "Zhenyuan", ""], ["Gong", "Chen", ""], ["Yan", "Huanqing", ""], ["He", "Jun", ""]]}, {"id": "2010.11725", "submitter": "Haoyue Dai", "authors": "Haoyue Dai", "title": "What do CNN neurons learn: Visualization & Clustering", "comments": "9 pages, 10 figures, tech report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In recent years convolutional neural networks (CNN) have shown striking\nprogress in various tasks. However, despite the high performance, the training\nand prediction process remains to be a black box, leaving it a mystery to\nextract what neurons learn in CNN. In this paper, we address the problem of\ninterpreting a CNN from the aspects of the input image's focus and preference,\nand the neurons' domination, activation and contribution to a concrete final\nprediction. Specifically, we use two techniques - visualization and clustering\n- to tackle the problems above. Visualization means the method of gradient\ndescent on image pixel, and in clustering section two algorithms are proposed\nto cluster respectively over image categories and network neurons. Experiments\nand quantitative analyses have demonstrated the effectiveness of the two\nmethods in explaining the question: what do neurons learn.\n", "versions": [{"version": "v1", "created": "Sun, 18 Oct 2020 05:29:22 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Dai", "Haoyue", ""]]}, {"id": "2010.11727", "submitter": "Huichen Yang", "authors": "Huichen Yang, William H. Hsu", "title": "Vision-Based Layout Detection from Scientific Literature using Recurrent\n  Convolutional Neural Networks", "comments": "8 pages", "journal-ref": "25th International Conference on Pattern Recognition (ICPR2020)", "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach for adapting convolutional neural networks for object\nrecognition and classification to scientific literature layout detection\n(SLLD), a shared subtask of several information extraction problems. Scientific\npublications contain multiple types of information sought by researchers in\nvarious disciplines, organized into an abstract, bibliography, and sections\ndocumenting related work, experimental methods, and results; however, there is\nno effective way to extract this information due to their diverse layout. In\nthis paper, we present a novel approach to developing an end-to-end learning\nframework to segment and classify major regions of a scientific document. We\nconsider scientific document layout analysis as an object detection task over\ndigital images, without any additional text features that need to be added into\nthe network during the training process. Our technical objective is to\nimplement transfer learning via fine-tuning of pre-trained networks and thereby\ndemonstrate that this deep learning architecture is suitable for tasks that\nlack very large document corpora for training ab initio. As part of the\nexperimental test bed for empirical evaluation of this approach, we created a\nmerged multi-corpus data set for scientific publication layout detection tasks.\nOur results show good improvement with fine-tuning of a pre-trained base\nnetwork using this merged data set, compared to the baseline convolutional\nneural network architecture.\n", "versions": [{"version": "v1", "created": "Sun, 18 Oct 2020 23:50:28 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Yang", "Huichen", ""], ["Hsu", "William H.", ""]]}, {"id": "2010.11732", "submitter": "Paulo Renato Conceicao Mendes", "authors": "Paulo R C Mendes, Antonio J G Busson, S\\'ergio Colcher, Daniel\n  Schwabe, \\'Alan L V Guedes, Carlos Laufer", "title": "A Cluster-Matching-Based Method for Video Face Recognition", "comments": "13 pages", "journal-ref": null, "doi": "10.1145/3428658.3430967", "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face recognition systems are present in many modern solutions and thousands\nof applications in our daily lives. However, current solutions are not easily\nscalable, especially when it comes to the addition of new targeted people. We\npropose a cluster-matching-based approach for face recognition in video. In our\napproach, we use unsupervised learning to cluster the faces present in both the\ndataset and targeted videos selected for face recognition. Moreover, we design\na cluster matching heuristic to associate clusters in both sets that is also\ncapable of identifying when a face belongs to a non-registered person. Our\nmethod has achieved a recall of 99.435% and a precision of 99.131% in the task\nof video face recognition. Besides performing face recognition, it can also be\nused to determine the video segments where each person is present.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 00:44:54 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Mendes", "Paulo R C", ""], ["Busson", "Antonio J G", ""], ["Colcher", "S\u00e9rgio", ""], ["Schwabe", "Daniel", ""], ["Guedes", "\u00c1lan L V", ""], ["Laufer", "Carlos", ""]]}, {"id": "2010.11734", "submitter": "Menghan Hu", "authors": "Yunlu Wang, Cheng Yang, Menghan Hu, Jian Zhang, Qingli Li, Guangtao\n  Zhai, Xiao-Ping Zhang", "title": "Identification of deep breath while moving forward based on multiple\n  body regions and graph signal analysis", "comments": "5 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an unobtrusive solution that can automatically identify\ndeep breath when a person is walking past the global depth camera. Existing\nnon-contact breath assessments achieve satisfactory results under restricted\nconditions when human body stays relatively still. When someone moves forward,\nthe breath signals detected by depth camera are hidden within signals of trunk\ndisplacement and deformation, and the signal length is short due to the short\nstay time, posing great challenges for us to establish models. To overcome\nthese challenges, multiple region of interests (ROIs) based signal extraction\nand selection method is proposed to automatically obtain the signal informative\nto breath from depth video. Subsequently, graph signal analysis (GSA) is\nadopted as a spatial-temporal filter to wipe the components unrelated to\nbreath. Finally, a classifier for identifying deep breath is established based\non the selected breath-informative signal. In validation experiments, the\nproposed approach outperforms the comparative methods with the accuracy,\nprecision, recall and F1 of 75.5%, 76.2%, 75.0% and 75.2%, respectively. This\nsystem can be extended to public places to provide timely and ubiquitous help\nfor those who may have or are going through physical or mental trouble.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 08:26:50 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Wang", "Yunlu", ""], ["Yang", "Cheng", ""], ["Hu", "Menghan", ""], ["Zhang", "Jian", ""], ["Li", "Qingli", ""], ["Zhai", "Guangtao", ""], ["Zhang", "Xiao-Ping", ""]]}, {"id": "2010.11735", "submitter": "Yahao Shi", "authors": "Yahao Shi, Xinyu Cao and Bin Zhou", "title": "Self-Supervised Learning of Part Mobility from Point Cloud Sequence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Part mobility analysis is a significant aspect required to achieve a\nfunctional understanding of 3D objects. It would be natural to obtain part\nmobility from the continuous part motion of 3D objects. In this study, we\nintroduce a self-supervised method for segmenting motion parts and predicting\ntheir motion attributes from a point cloud sequence representing a dynamic\nobject. To sufficiently utilize spatiotemporal information from the point cloud\nsequence, we generate trajectories by using correlations among successive\nframes of the sequence instead of directly processing the point clouds. We\npropose a novel neural network architecture called PointRNN to learn feature\nrepresentations of trajectories along with their part rigid motions. We\nevaluate our method on various tasks including motion part segmentation, motion\naxis prediction and motion range estimation. The results demonstrate that our\nmethod outperforms previous techniques on both synthetic and real datasets.\nMoreover, our method has the ability to generalize to new and unseen objects.\nIt is important to emphasize that it is not required to know any prior shape\nstructure, prior shape category information, or shape orientation. To the best\nof our knowledge, this is the first study on deep learning to extract part\nmobility from point cloud sequence of a dynamic object.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 11:29:46 GMT"}, {"version": "v2", "created": "Tue, 2 Mar 2021 09:34:11 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Shi", "Yahao", ""], ["Cao", "Xinyu", ""], ["Zhou", "Bin", ""]]}, {"id": "2010.11742", "submitter": "Jiancheng Yang", "authors": "Jiancheng Yang, Yangzhou Jiang, Xiaoyang Huang, Bingbing Ni, Chenglong\n  Zhao", "title": "Learning Black-Box Attackers with Transferable Priors and Query Feedback", "comments": "NeurIPS 2020. Code is available at\n  https://github.com/TrustworthyDL/LeBA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper addresses the challenging black-box adversarial attack problem,\nwhere only classification confidence of a victim model is available. Inspired\nby consistency of visual saliency between different vision models, a surrogate\nmodel is expected to improve the attack performance via transferability. By\ncombining transferability-based and query-based black-box attack, we propose a\nsurprisingly simple baseline approach (named SimBA++) using the surrogate\nmodel, which significantly outperforms several state-of-the-art methods.\nMoreover, to efficiently utilize the query feedback, we update the surrogate\nmodel in a novel learning scheme, named High-Order Gradient Approximation\n(HOGA). By constructing a high-order gradient computation graph, we update the\nsurrogate model to approximate the victim model in both forward and backward\npass. The SimBA++ and HOGA result in Learnable Black-Box Attack (LeBA), which\nsurpasses previous state of the art by considerable margins: the proposed LeBA\nsignificantly reduces queries, while keeping higher attack success rates close\nto 100% in extensive ImageNet experiments, including attacking vision\nbenchmarks and defensive models. Code is open source at\nhttps://github.com/TrustworthyDL/LeBA.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 05:43:11 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Yang", "Jiancheng", ""], ["Jiang", "Yangzhou", ""], ["Huang", "Xiaoyang", ""], ["Ni", "Bingbing", ""], ["Zhao", "Chenglong", ""]]}, {"id": "2010.11757", "submitter": "Chun-Fu (Richard) Chen", "authors": "Chun-Fu Chen, Rameswar Panda, Kandan Ramakrishnan, Rogerio Feris, John\n  Cohn, Aude Oliva, Quanfu Fan", "title": "Deep Analysis of CNN-based Spatio-temporal Representations for Action\n  Recognition", "comments": "CVPR 2021 camera-ready version. Codes and models are available on\n  https://github.com/IBM/action-recognition-pytorch", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, a number of approaches based on 2D or 3D convolutional\nneural networks (CNN) have emerged for video action recognition, achieving\nstate-of-the-art results on several large-scale benchmark datasets. In this\npaper, we carry out in-depth comparative analysis to better understand the\ndifferences between these approaches and the progress made by them. To this\nend, we develop an unified framework for both 2D-CNN and 3D-CNN action models,\nwhich enables us to remove bells and whistles and provides a common ground for\nfair comparison. We then conduct an effort towards a large-scale analysis\ninvolving over 300 action recognition models. Our comprehensive analysis\nreveals that a) a significant leap is made in efficiency for action\nrecognition, but not in accuracy; b) 2D-CNN and 3D-CNN models behave similarly\nin terms of spatio-temporal representation abilities and transferability. Our\ncodes are available at https://github.com/IBM/action-recognition-pytorch.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 14:26:09 GMT"}, {"version": "v2", "created": "Fri, 23 Oct 2020 00:51:53 GMT"}, {"version": "v3", "created": "Mon, 18 Jan 2021 14:50:20 GMT"}, {"version": "v4", "created": "Mon, 29 Mar 2021 14:33:42 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Chen", "Chun-Fu", ""], ["Panda", "Rameswar", ""], ["Ramakrishnan", "Kandan", ""], ["Feris", "Rogerio", ""], ["Cohn", "John", ""], ["Oliva", "Aude", ""], ["Fan", "Quanfu", ""]]}, {"id": "2010.11780", "submitter": "Tristan Hascoet", "authors": "Hascoet Tristan, Yihao Zhang, Persch Andreas, Ryoichi Takashima,\n  Tetsuya Takiguchi, Yasuo Ariki", "title": "FasterRCNN Monitoring of Road Damages: Competition and Deployment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Maintaining aging infrastructure is a challenge currently faced by local and\nnational administrators all around the world. An important prerequisite for\nefficient infrastructure maintenance is to continuously monitor (i.e., quantify\nthe level of safety and reliability) the state of very large structures.\nMeanwhile, computer vision has made impressive strides in recent years, mainly\ndue to successful applications of deep learning models. These novel progresses\nare allowing the automation of vision tasks, which were previously impossible\nto automate, offering promising possibilities to assist administrators in\noptimizing their infrastructure maintenance operations. In this context, the\nIEEE 2020 global Road Damage Detection (RDD) Challenge is giving an opportunity\nfor deep learning and computer vision researchers to get involved and help\naccurately track pavement damages on road networks. This paper proposes two\ncontributions to that topic: In a first part, we detail our solution to the RDD\nChallenge. In a second part, we present our efforts in deploying our model on a\nlocal road network, explaining the proposed methodology and encountered\nchallenges.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 14:56:00 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Tristan", "Hascoet", ""], ["Zhang", "Yihao", ""], ["Andreas", "Persch", ""], ["Takashima", "Ryoichi", ""], ["Takiguchi", "Tetsuya", ""], ["Ariki", "Yasuo", ""]]}, {"id": "2010.11800", "submitter": "Zhengxia Zou", "authors": "Zhengxia Zou", "title": "Castle in the Sky: Dynamic Sky Replacement and Harmonization in Videos", "comments": "project website: https://jiupinjia.github.io/skyar/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a vision-based method for video sky replacement and\nharmonization, which can automatically generate realistic and dramatic sky\nbackgrounds in videos with controllable styles. Different from previous sky\nediting methods that either focus on static photos or require inertial\nmeasurement units integrated in smartphones on shooting videos, our method is\npurely vision-based, without any requirements on the capturing devices, and can\nbe well applied to either online or offline processing scenarios. Our method\nruns in real-time and is free of user interactions. We decompose this artistic\ncreation process into a couple of proxy tasks including sky matting, motion\nestimation, and image blending. Experiments are conducted on videos diversely\ncaptured in the wild by handheld smartphones and dash cameras, and show high\nfidelity and good generalization of our method in both visual quality and\nlighting/motion dynamics. Our code and animated results are available at\n\\url{https://jiupinjia.github.io/skyar/}.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 15:27:31 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Zou", "Zhengxia", ""]]}, {"id": "2010.11828", "submitter": "Haotao Wang", "authors": "Haotao Wang, Tianlong Chen, Shupeng Gui, Ting-Kuei Hu, Ji Liu and\n  Zhangyang Wang", "title": "Once-for-All Adversarial Training: In-Situ Tradeoff between Robustness\n  and Accuracy for Free", "comments": "NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial training and its many variants substantially improve deep network\nrobustness, yet at the cost of compromising standard accuracy. Moreover, the\ntraining process is heavy and hence it becomes impractical to thoroughly\nexplore the trade-off between accuracy and robustness. This paper asks this new\nquestion: how to quickly calibrate a trained model in-situ, to examine the\nachievable trade-offs between its standard and robust accuracies, without\n(re-)training it many times? Our proposed framework, Once-for-all Adversarial\nTraining (OAT), is built on an innovative model-conditional training framework,\nwith a controlling hyper-parameter as the input. The trained model could be\nadjusted among different standard and robust accuracies \"for free\" at testing\ntime. As an important knob, we exploit dual batch normalization to separate\nstandard and adversarial feature statistics, so that they can be learned in one\nmodel without degrading performance. We further extend OAT to a Once-for-all\nAdversarial Training and Slimming (OATS) framework, that allows for the joint\ntrade-off among accuracy, robustness and runtime efficiency. Experiments show\nthat, without any re-training nor ensembling, OAT/OATS achieve similar or even\nsuperior performance compared to dedicatedly trained models at various\nconfigurations. Our codes and pretrained models are available at:\nhttps://github.com/VITA-Group/Once-for-All-Adversarial-Training.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 16:06:34 GMT"}, {"version": "v2", "created": "Tue, 10 Nov 2020 08:18:58 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Wang", "Haotao", ""], ["Chen", "Tianlong", ""], ["Gui", "Shupeng", ""], ["Hu", "Ting-Kuei", ""], ["Liu", "Ji", ""], ["Wang", "Zhangyang", ""]]}, {"id": "2010.11838", "submitter": "Chenyang Lei", "authors": "Chenyang Lei, Yazhou Xing, Qifeng Chen", "title": "Blind Video Temporal Consistency via Deep Video Prior", "comments": "NeurIPS 2020; github link: github.com/ChenyangLEI/deep-video-prior", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Applying image processing algorithms independently to each video frame often\nleads to temporal inconsistency in the resulting video. To address this issue,\nwe present a novel and general approach for blind video temporal consistency.\nOur method is only trained on a pair of original and processed videos directly\ninstead of a large dataset. Unlike most previous methods that enforce temporal\nconsistency with optical flow, we show that temporal consistency can be\nachieved by training a convolutional network on a video with the Deep Video\nPrior. Moreover, a carefully designed iteratively reweighted training strategy\nis proposed to address the challenging multimodal inconsistency problem. We\ndemonstrate the effectiveness of our approach on 7 computer vision tasks on\nvideos. Extensive quantitative and perceptual experiments show that our\napproach obtains superior performance than state-of-the-art methods on blind\nvideo temporal consistency. Our source codes are publicly available at\ngithub.com/ChenyangLEI/deep-video-prior.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 16:19:20 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Lei", "Chenyang", ""], ["Xing", "Yazhou", ""], ["Chen", "Qifeng", ""]]}, {"id": "2010.11844", "submitter": "Ipek Ganiyusufoglu", "authors": "Ipek Ganiyusufoglu, L. Minh Ng\\^o, Nedko Savov, Sezer Karaoglu, Theo\n  Gevers", "title": "Spatio-temporal Features for Generalized Detection of Deepfake Videos", "comments": "Submitted to Computer Vision and Image Understanding (CVIU)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For deepfake detection, video-level detectors have not been explored as\nextensively as image-level detectors, which do not exploit temporal data. In\nthis paper, we empirically show that existing approaches on image and sequence\nclassifiers generalize poorly to new manipulation techniques. To this end, we\npropose spatio-temporal features, modeled by 3D CNNs, to extend the\ngeneralization capabilities to detect new sorts of deepfake videos. We show\nthat spatial features learn distinct deepfake-method-specific attributes, while\nspatio-temporal features capture shared attributes between deepfake methods. We\nprovide an in-depth analysis of how the sequential and spatio-temporal video\nencoders are utilizing temporal information using DFDC dataset\narXiv:2006.07397. Thus, we unravel that our approach captures local\nspatio-temporal relations and inconsistencies in the deepfake videos while\nexisting sequence encoders are indifferent to it. Through large scale\nexperiments conducted on the FaceForensics++ arXiv:1901.08971 and Deeper\nForensics arXiv:2001.03024 datasets, we show that our approach outperforms\nexisting methods in terms of generalization capabilities.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 16:28:50 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Ganiyusufoglu", "Ipek", ""], ["Ng\u00f4", "L. Minh", ""], ["Savov", "Nedko", ""], ["Karaoglu", "Sezer", ""], ["Gevers", "Theo", ""]]}, {"id": "2010.11884", "submitter": "James Ren Hou Lee Mr", "authors": "James Ren Hou Lee, Alexander Wong", "title": "AEGIS: A real-time multimodal augmented reality computer vision based\n  system to assist facial expression recognition for individuals with autism\n  spectrum disorder", "comments": "4 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to interpret social cues comes naturally for most people, but for\nthose living with Autism Spectrum Disorder (ASD), some experience a deficiency\nin this area. This paper presents the development of a multimodal augmented\nreality (AR) system which combines the use of computer vision and deep\nconvolutional neural networks (CNN) in order to assist individuals with the\ndetection and interpretation of facial expressions in social settings. The\nproposed system, which we call AEGIS (Augmented-reality Expression Guided\nInterpretation System), is an assistive technology deployable on a variety of\nuser devices including tablets, smartphones, video conference systems, or\nsmartglasses, showcasing its extreme flexibility and wide range of use cases,\nto allow integration into daily life with ease. Given a streaming video camera\nsource, each real-world frame is passed into AEGIS, processed for facial\nbounding boxes, and then fed into our novel deep convolutional time windowed\nneural network (TimeConvNet). We leverage both spatial and temporal information\nin order to provide an accurate expression prediction, which is then converted\ninto its corresponding visualization and drawn on top of the original video\nframe. The system runs in real-time, requires minimal set up and is simple to\nuse. With the use of AEGIS, we can assist individuals living with ASD to learn\nto better identify expressions and thus improve their social experiences.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 17:20:38 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Lee", "James Ren Hou", ""], ["Wong", "Alexander", ""]]}, {"id": "2010.11886", "submitter": "K L Bhanu Moorthy", "authors": "K L Bhanu Moorthy, Moneish Kumar, Ramanathan Subramaniam, Vineet\n  Gandhi", "title": "GAZED- Gaze-guided Cinematic Editing of Wide-Angle Monocular Video\n  Recordings", "comments": "10 pages", "journal-ref": "In Proceedings of the 2020 CHI Conference on Human Factors in\n  Computing Systems (CHI '20). Association for Computing Machinery, New York,\n  NY, USA, 1-11", "doi": "10.1145/3313831.3376544", "report-no": null, "categories": "cs.CV cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present GAZED- eye GAZe-guided EDiting for videos captured by a solitary,\nstatic, wide-angle and high-resolution camera. Eye-gaze has been effectively\nemployed in computational applications as a cue to capture interesting scene\ncontent; we employ gaze as a proxy to select shots for inclusion in the edited\nvideo. Given the original video, scene content and user eye-gaze tracks are\ncombined to generate an edited video comprising cinematically valid actor shots\nand shot transitions to generate an aesthetic and vivid representation of the\noriginal narrative. We model cinematic video editing as an energy minimization\nproblem over shot selection, whose constraints capture cinematographic editing\nconventions. Gazed scene locations primarily determine the shots constituting\nthe edited video. Effectiveness of GAZED against multiple competing methods is\ndemonstrated via a psychophysical study involving 12 users and twelve\nperformance videos.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 17:27:03 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Moorthy", "K L Bhanu", ""], ["Kumar", "Moneish", ""], ["Subramaniam", "Ramanathan", ""], ["Gandhi", "Vineet", ""]]}, {"id": "2010.11929", "submitter": "Alexey Dosovitskiy", "authors": "Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk\n  Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias\n  Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit and Neil Houlsby", "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at\n  Scale", "comments": "Fine-tuning code and pre-trained models are available at\n  https://github.com/google-research/vision_transformer. ICLR camera-ready\n  version with 2 small modifications: 1) Added a discussion of CLS vs GAP\n  classifier in the appendix, 2) Fixed an error in exaFLOPs computation in\n  Figure 5 and Table 6 (relative performance of models is basically not\n  affected)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While the Transformer architecture has become the de-facto standard for\nnatural language processing tasks, its applications to computer vision remain\nlimited. In vision, attention is either applied in conjunction with\nconvolutional networks, or used to replace certain components of convolutional\nnetworks while keeping their overall structure in place. We show that this\nreliance on CNNs is not necessary and a pure transformer applied directly to\nsequences of image patches can perform very well on image classification tasks.\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\nTransformer (ViT) attains excellent results compared to state-of-the-art\nconvolutional networks while requiring substantially fewer computational\nresources to train.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 17:55:59 GMT"}, {"version": "v2", "created": "Thu, 3 Jun 2021 13:08:56 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Dosovitskiy", "Alexey", ""], ["Beyer", "Lucas", ""], ["Kolesnikov", "Alexander", ""], ["Weissenborn", "Dirk", ""], ["Zhai", "Xiaohua", ""], ["Unterthiner", "Thomas", ""], ["Dehghani", "Mostafa", ""], ["Minderer", "Matthias", ""], ["Heigold", "Georg", ""], ["Gelly", "Sylvain", ""], ["Uszkoreit", "Jakob", ""], ["Houlsby", "Neil", ""]]}, {"id": "2010.11943", "submitter": "Esther Robb", "authors": "Esther Robb and Wen-Sheng Chu and Abhishek Kumar and Jia-Bin Huang", "title": "Few-Shot Adaptation of Generative Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Networks (GANs) have shown remarkable performance in\nimage synthesis tasks, but typically require a large number of training samples\nto achieve high-quality synthesis. This paper proposes a simple and effective\nmethod, Few-Shot GAN (FSGAN), for adapting GANs in few-shot settings (less than\n100 images). FSGAN repurposes component analysis techniques and learns to adapt\nthe singular values of the pre-trained weights while freezing the corresponding\nsingular vectors. This provides a highly expressive parameter space for\nadaptation while constraining changes to the pretrained weights. We validate\nour method in a challenging few-shot setting of 5-100 images in the target\ndomain. We show that our method has significant visual quality gains compared\nwith existing GAN adaptation methods. We report qualitative and quantitative\nresults showing the effectiveness of our method. We additionally highlight a\nproblem for few-shot synthesis in the standard quantitative metric used by\ndata-efficient image synthesis works. Code and additional results are available\nat http://e-271.github.io/few-shot-gan.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 17:59:29 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Robb", "Esther", ""], ["Chu", "Wen-Sheng", ""], ["Kumar", "Abhishek", ""], ["Huang", "Jia-Bin", ""]]}, {"id": "2010.11971", "submitter": "Yaochen Xie", "authors": "Yaochen Xie, Zhengyang Wang, Shuiwang Ji", "title": "Noise2Same: Optimizing A Self-Supervised Bound for Image Denoising", "comments": "15 pages, NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-supervised frameworks that learn denoising models with merely individual\nnoisy images have shown strong capability and promising performance in various\nimage denoising tasks. Existing self-supervised denoising frameworks are mostly\nbuilt upon the same theoretical foundation, where the denoising models are\nrequired to be J-invariant. However, our analyses indicate that the current\ntheory and the J-invariance may lead to denoising models with reduced\nperformance. In this work, we introduce Noise2Same, a novel self-supervised\ndenoising framework. In Noise2Same, a new self-supervised loss is proposed by\nderiving a self-supervised upper bound of the typical supervised loss. In\nparticular, Noise2Same requires neither J-invariance nor extra information\nabout the noise model and can be used in a wider range of denoising\napplications. We analyze our proposed Noise2Same both theoretically and\nexperimentally. The experimental results show that our Noise2Same remarkably\noutperforms previous self-supervised denoising methods in terms of denoising\nperformance and training efficiency. Our code is available at\nhttps://github.com/divelab/Noise2Same.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 18:12:26 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Xie", "Yaochen", ""], ["Wang", "Zhengyang", ""], ["Ji", "Shuiwang", ""]]}, {"id": "2010.11978", "submitter": "Md. Abu Bakr Siddique", "authors": "Md. Abu Bakr Siddique, Shadman Sakib, Mohammad Mahmudur Rahman Khan,\n  Abyaz Kader Tanzeem, Madiha Chowdhury, Nowrin Yasmin", "title": "Deep Convolutional Neural Networks Model-based Brain Tumor Detection in\n  Brain MRI Images", "comments": "4th International conference on I-SMAC (IoT in Social, Mobile,\n  Analytics and Cloud) (I-SMAC 2020), IEEE, 7-9 October 2020, TamilNadu, INDIA", "journal-ref": "2020 Fourth International Conference on I-SMAC (IoT in Social,\n  Mobile, Analytics and Cloud) (I-SMAC)", "doi": "10.1109/I-SMAC49090.2020.9243461", "report-no": null, "categories": "eess.IV cs.CV cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Diagnosing Brain Tumor with the aid of Magnetic Resonance Imaging (MRI) has\ngained enormous prominence over the years, primarily in the field of medical\nscience. Detection and/or partitioning of brain tumors solely with the aid of\nMR imaging is achieved at the cost of immense time and effort and demands a lot\nof expertise from engaged personnel. This substantiates the necessity of\nfabricating an autonomous model brain tumor diagnosis. Our work involves\nimplementing a deep convolutional neural network (DCNN) for diagnosing brain\ntumors from MR images. The dataset used in this paper consists of 253 brain MR\nimages where 155 images are reported to have tumors. Our model can single out\nthe MR images with tumors with an overall accuracy of 96%. The model\noutperformed the existing conventional methods for the diagnosis of brain tumor\nin the test dataset (Precision = 0.93, Sensitivity = 1.00, and F1-score =\n0.97). Moreover, the proposed model's average precision-recall score is 0.93,\nCohen's Kappa 0.91, and AUC 0.95. Therefore, the proposed model can help\nclinical experts verify whether the patient has a brain tumor and,\nconsequently, accelerate the treatment procedure.\n", "versions": [{"version": "v1", "created": "Sat, 3 Oct 2020 07:42:17 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Siddique", "Md. Abu Bakr", ""], ["Sakib", "Shadman", ""], ["Khan", "Mohammad Mahmudur Rahman", ""], ["Tanzeem", "Abyaz Kader", ""], ["Chowdhury", "Madiha", ""], ["Yasmin", "Nowrin", ""]]}, {"id": "2010.11985", "submitter": "Jianing Yang", "authors": "Jianing Yang, Yongxin Wang, Ruitao Yi, Yuying Zhu, Azaan Rehman, Amir\n  Zadeh, Soujanya Poria, Louis-Philippe Morency", "title": "MTAG: Modal-Temporal Attention Graph for Unaligned Human Multimodal\n  Language Sequences", "comments": "NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human communication is multimodal in nature; it is through multiple\nmodalities such as language, voice, and facial expressions, that opinions and\nemotions are expressed. Data in this domain exhibits complex multi-relational\nand temporal interactions. Learning from this data is a fundamentally\nchallenging research problem. In this paper, we propose Modal-Temporal\nAttention Graph (MTAG). MTAG is an interpretable graph-based neural model that\nprovides a suitable framework for analyzing multimodal sequential data. We\nfirst introduce a procedure to convert unaligned multimodal sequence data into\na graph with heterogeneous nodes and edges that captures the rich interactions\nacross modalities and through time. Then, a novel graph fusion operation,\ncalled MTAG fusion, along with a dynamic pruning and read-out technique, is\ndesigned to efficiently process this modal-temporal graph and capture various\ninteractions. By learning to focus only on the important interactions within\nthe graph, MTAG achieves state-of-the-art performance on multimodal sentiment\nanalysis and emotion recognition benchmarks, while utilizing significantly\nfewer model parameters.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 18:58:50 GMT"}, {"version": "v2", "created": "Wed, 28 Apr 2021 18:44:01 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Yang", "Jianing", ""], ["Wang", "Yongxin", ""], ["Yi", "Ruitao", ""], ["Zhu", "Yuying", ""], ["Rehman", "Azaan", ""], ["Zadeh", "Amir", ""], ["Poria", "Soujanya", ""], ["Morency", "Louis-Philippe", ""]]}, {"id": "2010.11993", "submitter": "Baladitya Yellapragada", "authors": "Baladitya Yellapragada, Sascha Hornhauer, Kiersten Snyder, Stella Yu,\n  Glenn Yiu", "title": "Unsupervised deep learning for grading of age-related macular\n  degeneration using retinal fundus images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV q-bio.QM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many diseases are classified based on human-defined rubrics that are prone to\nbias. Supervised neural networks can automate the grading of retinal fundus\nimages, but require labor-intensive annotations and are restricted to the\nspecific trained task. Here, we employed an unsupervised network with\nNon-Parametric Instance Discrimination (NPID) to grade age-related macular\ndegeneration (AMD) severity using fundus photographs from the Age-Related Eye\nDisease Study (AREDS). Our unsupervised algorithm demonstrated versatility\nacross different AMD classification schemes without retraining, and achieved\nunbalanced accuracies comparable to supervised networks and human\nophthalmologists in classifying advanced or referable AMD, or on the 4-step AMD\nseverity scale. Exploring the networks behavior revealed disease-related fundus\nfeatures that drove predictions and unveiled the susceptibility of more\ngranular human-defined AMD severity schemes to misclassification by both\nophthalmologists and neural networks. Importantly, unsupervised learning\nenabled unbiased, data-driven discovery of AMD features such as geographic\natrophy, as well as other ocular phenotypes of the choroid, vitreous, and lens,\nsuch as visually-impairing cataracts, that were not pre-defined by human\nlabels.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 19:13:28 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Yellapragada", "Baladitya", ""], ["Hornhauer", "Sascha", ""], ["Snyder", "Kiersten", ""], ["Yu", "Stella", ""], ["Yiu", "Glenn", ""]]}, {"id": "2010.11995", "submitter": "Soraia Musse", "authors": "Rodolfo M. Favaretto, Roberto R. Santos, Marcio Ballotin, Paulo Knob,\n  Soraia R. Musse, Felipe Vilanova, Angelo B. Costa", "title": "Investigating Cultural Aspects in the Fundamental Diagram using\n  Convolutional Neural Networks and Simulation", "comments": "Computer Animation and Virtual Worlds, 2019", "journal-ref": null, "doi": "10.1002/cav.1899", "report-no": null, "categories": "cs.OH cs.CV cs.GR physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a study regarding group behavior in a controlled\nexperiment focused on differences in an important attribute that vary across\ncultures -- the personal spaces -- in two Countries: Brazil and Germany. In\norder to coherently compare Germany and Brazil evolutions with same population\napplying same task, we performed the pedestrian Fundamental Diagram experiment\nin Brazil, as performed in Germany. We use CNNs to detect and track people in\nvideo sequences. With this data, we use Voronoi Diagrams to find out the\nneighbor relation among people and then compute the walking distances to find\nout the personal spaces. Based on personal spaces analyses, we found out that\npeople behavior is more similar, in terms of their behaviours, in high dense\npopulations and vary more in low and medium densities. So, we focused our study\non cultural differences between the two Countries in low and medium densities.\nResults indicate that personal space analyses can be a relevant feature in\norder to understand cultural aspects in video sequences. In addition to the\ncultural differences, we also investigate the personality model in crowds,\nusing OCEAN. We also proposed a way to simulate the FD experiment from other\ncountries using the OCEAN psychological traits model as input. The simulated\ncountries were consistent with the literature.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2020 14:44:04 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Favaretto", "Rodolfo M.", ""], ["Santos", "Roberto R.", ""], ["Ballotin", "Marcio", ""], ["Knob", "Paulo", ""], ["Musse", "Soraia R.", ""], ["Vilanova", "Felipe", ""], ["Costa", "Angelo B.", ""]]}, {"id": "2010.11997", "submitter": "Zhanwen Chen", "authors": "Zhanwen Chen, Shiyao Li, Roxanne Rashedi, Xiaoman Zi, Morgan\n  Elrod-Erickson, Bryan Hollis, Angela Maliakal, Xinyu Shen, Simeng Zhao,\n  Maithilee Kunda", "title": "Characterizing Datasets for Social Visual Question Answering, and the\n  New TinySocial Dataset", "comments": "To appear in the Joint IEEE International Conference on Development\n  and Learning and on Epigenetic Robotics (ICDL), 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CL cs.CV cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern social intelligence includes the ability to watch videos and answer\nquestions about social and theory-of-mind-related content, e.g., for a scene in\nHarry Potter, \"Is the father really upset about the boys flying the car?\"\nSocial visual question answering (social VQA) is emerging as a valuable\nmethodology for studying social reasoning in both humans (e.g., children with\nautism) and AI agents. However, this problem space spans enormous variations in\nboth videos and questions. We discuss methods for creating and characterizing\nsocial VQA datasets, including 1) crowdsourcing versus in-house authoring,\nincluding sample comparisons of two new datasets that we created\n(TinySocial-Crowd and TinySocial-InHouse) and the previously existing Social-IQ\ndataset; 2) a new rubric for characterizing the difficulty and content of a\ngiven video; and 3) a new rubric for characterizing question types. We close by\ndescribing how having well-characterized social VQA datasets will enhance the\nexplainability of AI agents and can also inform assessments and educational\ninterventions for people.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 03:20:23 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Chen", "Zhanwen", ""], ["Li", "Shiyao", ""], ["Rashedi", "Roxanne", ""], ["Zi", "Xiaoman", ""], ["Elrod-Erickson", "Morgan", ""], ["Hollis", "Bryan", ""], ["Maliakal", "Angela", ""], ["Shen", "Xinyu", ""], ["Zhao", "Simeng", ""], ["Kunda", "Maithilee", ""]]}, {"id": "2010.12011", "submitter": "Johannes Stegmaier", "authors": "Dennis B\\\"ahr, Dennis Eschweiler, Anuk Bhattacharyya, Daniel\n  Moreno-Andr\\'es, Wolfram Antonin and Johannes Stegmaier", "title": "CellCycleGAN: Spatiotemporal Microscopy Image Synthesis of Cell\n  Populations using Statistical Shape Models and Conditional GANs", "comments": "5 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG q-bio.CB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic analysis of spatio-temporal microscopy images is inevitable for\nstate-of-the-art research in the life sciences. Recent developments in deep\nlearning provide powerful tools for automatic analyses of such image data, but\nheavily depend on the amount and quality of provided training data to perform\nwell. To this end, we developed a new method for realistic generation of\nsynthetic 2D+t microscopy image data of fluorescently labeled cellular nuclei.\nThe method combines spatiotemporal statistical shape models of different cell\ncycle stages with a conditional GAN to generate time series of cell populations\nand provides instance-level control of cell cycle stage and the fluorescence\nintensity of generated cells. We show the effect of the GAN conditioning and\ncreate a set of synthetic images that can be readily used for training and\nbenchmarking of cell segmentation and tracking approaches.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 20:02:41 GMT"}, {"version": "v2", "created": "Tue, 26 Jan 2021 19:10:28 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["B\u00e4hr", "Dennis", ""], ["Eschweiler", "Dennis", ""], ["Bhattacharyya", "Anuk", ""], ["Moreno-Andr\u00e9s", "Daniel", ""], ["Antonin", "Wolfram", ""], ["Stegmaier", "Johannes", ""]]}, {"id": "2010.12016", "submitter": "Matthew Leavitt", "authors": "Matthew L. Leavitt, Ari Morcos", "title": "Towards falsifiable interpretability research", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Methods for understanding the decisions of and mechanisms underlying deep\nneural networks (DNNs) typically rely on building intuition by emphasizing\nsensory or semantic features of individual examples. For instance, methods aim\nto visualize the components of an input which are \"important\" to a network's\ndecision, or to measure the semantic properties of single neurons. Here, we\nargue that interpretability research suffers from an over-reliance on\nintuition-based approaches that risk-and in some cases have caused-illusory\nprogress and misleading conclusions. We identify a set of limitations that we\nargue impede meaningful progress in interpretability research, and examine two\npopular classes of interpretability methods-saliency and single-neuron-based\napproaches-that serve as case studies for how overreliance on intuition and\nlack of falsifiability can undermine interpretability research. To address\nthese concerns, we propose a strategy to address these impediments in the form\nof a framework for strongly falsifiable interpretability research. We encourage\nresearchers to use their intuitions as a starting point to develop and test\nclear, falsifiable hypotheses, and hope that our framework yields robust,\nevidence-based interpretability methods that generate meaningful advances in\nour understanding of DNNs.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 22:03:41 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Leavitt", "Matthew L.", ""], ["Morcos", "Ari", ""]]}, {"id": "2010.12021", "submitter": "Baopu Li", "authors": "Baopu Li, Yanwen Fan, Zhihong Pan, Gang Zhang", "title": "AutoPruning for Deep Neural Network with Dynamic Channel Masking", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Modern deep neural network models are large and computationally intensive.\nOne typical solution to this issue is model pruning. However, most current\npruning algorithms depend on hand crafted rules or domain expertise. To\novercome this problem, we propose a learning based auto pruning algorithm for\ndeep neural network, which is inspired by recent automatic machine\nlearning(AutoML). A two objectives' problem that aims for the the weights and\nthe best channels for each layer is first formulated. An alternative\noptimization approach is then proposed to derive the optimal channel numbers\nand weights simultaneously. In the process of pruning, we utilize a searchable\nhyperparameter, remaining ratio, to denote the number of channels in each\nconvolution layer, and then a dynamic masking process is proposed to describe\nthe corresponding channel evolution. To control the trade-off between the\naccuracy of a model and the pruning ratio of floating point operations, a novel\nloss function is further introduced. Preliminary experimental results on\nbenchmark datasets demonstrate that our scheme achieves competitive results for\nneural network pruning.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 20:12:46 GMT"}, {"version": "v2", "created": "Mon, 2 Nov 2020 04:06:51 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Li", "Baopu", ""], ["Fan", "Yanwen", ""], ["Pan", "Zhihong", ""], ["Zhang", "Gang", ""]]}, {"id": "2010.12023", "submitter": "Dong Huang", "authors": "Zeyi Huang, Yang Zou, Vijayakumar Bhagavatula, Dong Huang", "title": "Comprehensive Attention Self-Distillation for Weakly-Supervised Object\n  Detection", "comments": "Neural Information Processing Systems (NeurIPS 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weakly Supervised Object Detection (WSOD) has emerged as an effective tool to\ntrain object detectors using only the image-level category labels. However,\nwithout object-level labels, WSOD detectors are prone to detect bounding boxes\non salient objects, clustered objects and discriminative object parts.\nMoreover, the image-level category labels do not enforce consistent object\ndetection across different transformations of the same images. To address the\nabove issues, we propose a Comprehensive Attention Self-Distillation (CASD)\ntraining approach for WSOD. To balance feature learning among all object\ninstances, CASD computes the comprehensive attention aggregated from multiple\ntransformations and feature layers of the same images. To enforce consistent\nspatial supervision on objects, CASD conducts self-distillation on the WSOD\nnetworks, such that the comprehensive attention is approximated simultaneously\nby multiple transformations and feature layers of the same images. CASD\nproduces new state-of-the-art WSOD results on standard benchmarks such as\nPASCAL VOC 2007/2012 and MS-COCO.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 20:13:32 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Huang", "Zeyi", ""], ["Zou", "Yang", ""], ["Bhagavatula", "Vijayakumar", ""], ["Huang", "Dong", ""]]}, {"id": "2010.12030", "submitter": "Goodarz Mehr", "authors": "Goodarz Mehr", "title": "Automating Abnormality Detection in Musculoskeletal Radiographs through\n  Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces MuRAD (Musculoskeletal Radiograph Abnormality Detection\ntool), a tool that can help radiologists automate the detection of\nabnormalities in musculoskeletal radiographs (bone X-rays). MuRAD utilizes a\nConvolutional Neural Network (CNN) that can accurately predict whether a bone\nX-ray is abnormal, and leverages Class Activation Map (CAM) to localize the\nabnormality in the image. MuRAD achieves an F1 score of 0.822 and a Cohen's\nkappa of 0.699, which is comparable to the performance of expert radiologists.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 01:48:56 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Mehr", "Goodarz", ""]]}, {"id": "2010.12035", "submitter": "Lucas Tabelini Torres", "authors": "Lucas Tabelini, Rodrigo Berriel, Thiago M. Paix\\~ao, Claudine Badue,\n  Alberto F. De Souza, Thiago Oliveira-Santos", "title": "Keep your Eyes on the Lane: Real-time Attention-guided Lane Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern lane detection methods have achieved remarkable performances in\ncomplex real-world scenarios, but many have issues maintaining real-time\nefficiency, which is important for autonomous vehicles. In this work, we\npropose LaneATT: an anchor-based deep lane detection model, which, akin to\nother generic deep object detectors, uses the anchors for the feature pooling\nstep. Since lanes follow a regular pattern and are highly correlated, we\nhypothesize that in some cases global information may be crucial to infer their\npositions, especially in conditions such as occlusion, missing lane markers,\nand others. Thus, this work proposes a novel anchor-based attention mechanism\nthat aggregates global information. The model was evaluated extensively on\nthree of the most widely used datasets in the literature. The results show that\nour method outperforms the current state-of-the-art methods showing both higher\nefficacy and efficiency. Moreover, an ablation study is performed along with a\ndiscussion on efficiency trade-off options that are useful in practice.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 20:25:08 GMT"}, {"version": "v2", "created": "Wed, 18 Nov 2020 01:09:01 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Tabelini", "Lucas", ""], ["Berriel", "Rodrigo", ""], ["Paix\u00e3o", "Thiago M.", ""], ["Badue", "Claudine", ""], ["De Souza", "Alberto F.", ""], ["Oliveira-Santos", "Thiago", ""]]}, {"id": "2010.12041", "submitter": "Tri Vu", "authors": "Tri Vu, Anthony DiSpirito III, Daiwei Li, Zixuan Zhang, Xiaoyi Zhu,\n  Maomao Chen, Laiming Jiang, Dong Zhang, Jianwen Luo, Yu Shrike Zhang, Qifa\n  Zhou, Roarke Horstmeyer, and Junjie Yao", "title": "Deep image prior for undersampling high-speed photoacoustic microscopy", "comments": null, "journal-ref": null, "doi": "10.1016/j.pacs.2021.100266", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Photoacoustic microscopy (PAM) is an emerging imaging method combining light\nand sound. However, limited by the laser's repetition rate, state-of-the-art\nhigh-speed PAM technology often sacrifices spatial sampling density (i.e.,\nundersampling) for increased imaging speed over a large field-of-view. Deep\nlearning (DL) methods have recently been used to improve sparsely sampled PAM\nimages; however, these methods often require time-consuming pre-training and\nlarge training dataset with ground truth. Here, we propose the use of deep\nimage prior (DIP) to improve the image quality of undersampled PAM images.\nUnlike other DL approaches, DIP requires neither pre-training nor fully-sampled\nground truth, enabling its flexible and fast implementation on various imaging\ntargets. Our results have demonstrated substantial improvement in PAM images\nwith as few as 1.4$\\%$ of the fully sampled pixels on high-speed PAM. Our\napproach outperforms interpolation, is competitive with pre-trained supervised\nDL method, and is readily translated to other high-speed, undersampling imaging\nmodalities.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 15:46:19 GMT"}, {"version": "v2", "created": "Wed, 7 Apr 2021 20:02:10 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Vu", "Tri", ""], ["DiSpirito", "Anthony", "III"], ["Li", "Daiwei", ""], ["Zhang", "Zixuan", ""], ["Zhu", "Xiaoyi", ""], ["Chen", "Maomao", ""], ["Jiang", "Laiming", ""], ["Zhang", "Dong", ""], ["Luo", "Jianwen", ""], ["Zhang", "Yu Shrike", ""], ["Zhou", "Qifa", ""], ["Horstmeyer", "Roarke", ""], ["Yao", "Junjie", ""]]}, {"id": "2010.12046", "submitter": "Jayaraman J. Thiagarajan", "authors": "Vivek Narayanaswamy, Jayaraman J. Thiagarajan, Andreas Spanias", "title": "Using Deep Image Priors to Generate Counterfactual Explanations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Through the use of carefully tailored convolutional neural network\narchitectures, a deep image prior (DIP) can be used to obtain pre-images from\nlatent representation encodings. Though DIP inversion has been known to be\nsuperior to conventional regularized inversion strategies such as total\nvariation, such an over-parameterized generator is able to effectively\nreconstruct even images that are not in the original data distribution. This\nlimitation makes it challenging to utilize such priors for tasks such as\ncounterfactual reasoning, wherein the goal is to generate small, interpretable\nchanges to an image that systematically leads to changes in the model\nprediction. To this end, we propose a novel regularization strategy based on an\nauxiliary loss estimator jointly trained with the predictor, which efficiently\nguides the prior to recover natural pre-images. Our empirical studies with a\nreal-world ISIC skin lesion detection problem clearly evidence the\neffectiveness of the proposed approach in synthesizing meaningful\ncounterfactuals. In comparison, we find that the standard DIP inversion often\nproposes visually imperceptible perturbations to irrelevant parts of the image,\nthus providing no additional insights into the model behavior.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 20:40:44 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Narayanaswamy", "Vivek", ""], ["Thiagarajan", "Jayaraman J.", ""], ["Spanias", "Andreas", ""]]}, {"id": "2010.12050", "submitter": "Chih-Hui Ho", "authors": "Chih-Hui Ho, Nuno Vasconcelos", "title": "Contrastive Learning with Adversarial Examples", "comments": "NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contrastive learning (CL) is a popular technique for self-supervised learning\n(SSL) of visual representations. It uses pairs of augmentations of unlabeled\ntraining examples to define a classification task for pretext learning of a\ndeep embedding. Despite extensive works in augmentation procedures, prior works\ndo not address the selection of challenging negative pairs, as images within a\nsampled batch are treated independently. This paper addresses the problem, by\nintroducing a new family of adversarial examples for constrastive learning and\nusing these examples to define a new adversarial training algorithm for SSL,\ndenoted as CLAE. When compared to standard CL, the use of adversarial examples\ncreates more challenging positive pairs and adversarial training produces\nharder negative pairs by accounting for all images in a batch during the\noptimization. CLAE is compatible with many CL methods in the literature.\nExperiments show that it improves the performance of several existing CL\nbaselines on multiple datasets.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 20:45:10 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Ho", "Chih-Hui", ""], ["Vasconcelos", "Nuno", ""]]}, {"id": "2010.12061", "submitter": "Yu Chen", "authors": "Jiawei Yang, Yu Chen", "title": "Simple Neighborhood Representative Pre-processing Boosts Outlier\n  Detectors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Outlier detectors heavily rely on data distribution. All outlier detectors\nwill become ineffective, for example, when data has collective outliers or a\nlarge portion of outliers. To better handle this issue, we propose a\npre-processing technique called neighborhood representative. The neighborhood\nrepresentative first selects a subset of representative objects from data, then\nemploys outlier detectors to score the representatives. The non-representative\ndata objects share the same score with the representative object nearby. The\nproposed technique is essentially an add-on to most existing outlier detector\nas it can improve 16% accuracy (from 0.64 AUC to 0.74 AUC) on average evaluated\non six datasets with nine state-of-the-art outlier detectors. In datasets with\nfewer outliers, the proposed technique can still improve most of the tested\noutlier detectors.\n", "versions": [{"version": "v1", "created": "Sun, 11 Oct 2020 21:31:14 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Yang", "Jiawei", ""], ["Chen", "Yu", ""]]}, {"id": "2010.12065", "submitter": "Nabit Bajwa", "authors": "Nabit Bajwa, Kedar Bajwa, Atif Rana, M. Faique Shakeel, Kashif Haqqi\n  and Suleiman Ali Khan", "title": "A generalized deep learning model for multi-disease Chest X-Ray\n  diagnostics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the generalizability of deep convolutional neural network\n(CNN) on the task of disease classification from chest x-rays collected over\nmultiple sites. We systematically train the model using datasets from three\nindependent sites with different patient populations: National Institute of\nHealth (NIH), Stanford University Medical Centre (CheXpert), and Shifa\nInternational Hospital (SIH). We formulate a sequential training approach and\ndemonstrate that the model produces generalized prediction performance using\nheld out test sets from the three sites. Our model generalizes better when\ntrained on multiple datasets, with the CheXpert-Shifa-NET model performing\nsignificantly better (p-values < 0.05) than the models trained on individual\ndatasets for 3 out of the 4 distinct disease classes. The code for training the\nmodel will be made available open source at: www.github.com/link-to-code at the\ntime of publication.\n", "versions": [{"version": "v1", "created": "Sat, 17 Oct 2020 18:57:40 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Bajwa", "Nabit", ""], ["Bajwa", "Kedar", ""], ["Rana", "Atif", ""], ["Shakeel", "M. Faique", ""], ["Haqqi", "Kashif", ""], ["Khan", "Suleiman Ali", ""]]}, {"id": "2010.12078", "submitter": "Anindya Maiti", "authors": "Mohd Sabra, Anindya Maiti, Murtuza Jadliwala", "title": "Zoom on the Keystrokes: Exploiting Video Calls for Keystroke Inference\n  Attacks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Due to recent world events, video calls have become the new norm for both\npersonal and professional remote communication. However, if a participant in a\nvideo call is not careful, he/she can reveal his/her private information to\nothers in the call. In this paper, we design and evaluate an attack framework\nto infer one type of such private information from the video stream of a call\n-- keystrokes, i.e., text typed during the call. We evaluate our video-based\nkeystroke inference framework using different experimental settings and\nparameters, including different webcams, video resolutions, keyboards,\nclothing, and backgrounds. Our relatively high keystroke inference accuracies\nunder commonly occurring and realistic settings highlight the need for\nawareness and countermeasures against such attacks. Consequently, we also\npropose and evaluate effective mitigation techniques that can automatically\nprotect users when they type during a video call.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 21:38:17 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Sabra", "Mohd", ""], ["Maiti", "Anindya", ""], ["Jadliwala", "Murtuza", ""]]}, {"id": "2010.12083", "submitter": "Simon Stepputtis", "authors": "Simon Stepputtis, Joseph Campbell, Mariano Phielipp, Stefan Lee,\n  Chitta Baral, Heni Ben Amor", "title": "Language-Conditioned Imitation Learning for Robot Manipulation Tasks", "comments": "Accepted to the 34th Conference on Neural Information Processing\n  Systems (NeurIPS 2020), Vancouver, Canada as spotlight presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Imitation learning is a popular approach for teaching motor skills to robots.\nHowever, most approaches focus on extracting policy parameters from execution\ntraces alone (i.e., motion trajectories and perceptual data). No adequate\ncommunication channel exists between the human expert and the robot to describe\ncritical aspects of the task, such as the properties of the target object or\nthe intended shape of the motion. Motivated by insights into the human teaching\nprocess, we introduce a method for incorporating unstructured natural language\ninto imitation learning. At training time, the expert can provide\ndemonstrations along with verbal descriptions in order to describe the\nunderlying intent (e.g., \"go to the large green bowl\"). The training process\nthen interrelates these two modalities to encode the correlations between\nlanguage, perception, and motion. The resulting language-conditioned visuomotor\npolicies can be conditioned at runtime on new human commands and instructions,\nwhich allows for more fine-grained control over the trained policies while also\nreducing situational ambiguity. We demonstrate in a set of simulation\nexperiments how our approach can learn language-conditioned manipulation\npolicies for a seven-degree-of-freedom robot arm and compare the results to a\nvariety of alternative methods.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 21:49:08 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Stepputtis", "Simon", ""], ["Campbell", "Joseph", ""], ["Phielipp", "Mariano", ""], ["Lee", "Stefan", ""], ["Baral", "Chitta", ""], ["Amor", "Heni Ben", ""]]}, {"id": "2010.12084", "submitter": "Debasmit Das", "authors": "Debasmit Das, J.H. Moon, C. S. George Lee", "title": "Few-shot Image Recognition with Manifolds", "comments": "International Symposium on Visual Computing (ISVC), 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we extend the traditional few-shot learning (FSL) problem to\nthe situation when the source-domain data is not accessible but only high-level\ninformation in the form of class prototypes is available. This limited\ninformation setup for the FSL problem deserves much attention due to its\nimplication of privacy-preserving inaccessibility to the source-domain data but\nit has rarely been addressed before. Because of limited training data, we\npropose a non-parametric approach to this FSL problem by assuming that all the\nclass prototypes are structurally arranged on a manifold. Accordingly, we\nestimate the novel-class prototype locations by projecting the few-shot samples\nonto the average of the subspaces on which the surrounding classes lie. During\nclassification, we again exploit the structural arrangement of the categories\nby inducing a Markov chain on the graph constructed with the class prototypes.\nThis manifold distance obtained using the Markov chain is expected to produce\nbetter results compared to a traditional nearest-neighbor-based Euclidean\ndistance. To evaluate our proposed framework, we have tested it on two image\ndatasets - the large-scale ImageNet and the small-scale but fine-grained\nCUB-200. We have also studied parameter sensitivity to better understand our\nframework.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 21:57:27 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Das", "Debasmit", ""], ["Moon", "J. H.", ""], ["Lee", "C. S. George", ""]]}, {"id": "2010.12108", "submitter": "Teresa White", "authors": "Teresa White, Jesse Wheeler, Colton Lindstrom, Randall Christensen,\n  Kevin R. Moon", "title": "GPS-Denied Navigation Using SAR Images and Neural Networks", "comments": "5 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unmanned aerial vehicles (UAV) often rely on GPS for navigation. GPS signals,\nhowever, are very low in power and easily jammed or otherwise disrupted. This\npaper presents a method for determining the navigation errors present at the\nbeginning of a GPS-denied period utilizing data from a synthetic aperture radar\n(SAR) system. This is accomplished by comparing an online-generated SAR image\nwith a reference image obtained a priori. The distortions relative to the\nreference image are learned and exploited with a convolutional neural network\nto recover the initial navigational errors, which can be used to recover the\ntrue flight trajectory throughout the synthetic aperture. The proposed neural\nnetwork approach is able to learn to predict the initial errors on both\nsimulated and real SAR image data.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 23:25:43 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["White", "Teresa", ""], ["Wheeler", "Jesse", ""], ["Lindstrom", "Colton", ""], ["Christensen", "Randall", ""], ["Moon", "Kevin R.", ""]]}, {"id": "2010.12110", "submitter": "Matej Ulicny", "authors": "Matej Ulicny, Vladimir A. Krylov and Rozenn Dahyot", "title": "Tensor Reordering for CNN Compression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show how parameter redundancy in Convolutional Neural Network (CNN)\nfilters can be effectively reduced by pruning in spectral domain. Specifically,\nthe representation extracted via Discrete Cosine Transform (DCT) is more\nconducive for pruning than the original space. By relying on a combination of\nweight tensor reshaping and reordering we achieve high levels of layer\ncompression with just minor accuracy loss. Our approach is applied to compress\npretrained CNNs and we show that minor additional fine-tuning allows our method\nto recover the original model performance after a significant parameter\nreduction. We validate our approach on ResNet-50 and MobileNet-V2 architectures\nfor ImageNet classification task.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 23:45:34 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Ulicny", "Matej", ""], ["Krylov", "Vladimir A.", ""], ["Dahyot", "Rozenn", ""]]}, {"id": "2010.12126", "submitter": "Li Ren", "authors": "Li Ren, Kai Li, LiQiang Wang, Kien Hua", "title": "Beyond the Deep Metric Learning: Enhance the Cross-Modal Matching with\n  Adversarial Discriminative Domain Regularization", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matching information across image and text modalities is a fundamental\nchallenge for many applications that involve both vision and natural language\nprocessing. The objective is to find efficient similarity metrics to compare\nthe similarity between visual and textual information. Existing approaches\nmainly match the local visual objects and the sentence words in a shared space\nwith attention mechanisms. The matching performance is still limited because\nthe similarity computation is based on simple comparisons of the matching\nfeatures, ignoring the characteristics of their distribution in the data. In\nthis paper, we address this limitation with an efficient learning objective\nthat considers the discriminative feature distributions between the visual\nobjects and sentence words. Specifically, we propose a novel Adversarial\nDiscriminative Domain Regularization (ADDR) learning framework, beyond the\nparadigm metric learning objective, to construct a set of discriminative data\ndomains within each image-text pairs. Our approach can generally improve the\nlearning efficiency and the performance of existing metrics learning frameworks\nby regulating the distribution of the hidden space between the matching pairs.\nThe experimental results show that this new approach significantly improves the\noverall performance of several popular cross-modal matching techniques (SCAN,\nVSRN, BFAN) on the MS-COCO and Flickr30K benchmarks.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 01:48:37 GMT"}, {"version": "v2", "created": "Tue, 27 Oct 2020 23:42:21 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Ren", "Li", ""], ["Li", "Kai", ""], ["Wang", "LiQiang", ""], ["Hua", "Kien", ""]]}, {"id": "2010.12136", "submitter": "Bowen Li", "authors": "Bowen Li, Xiaojuan Qi, Philip H. S. Torr, Thomas Lukasiewicz", "title": "Lightweight Generative Adversarial Networks for Text-Guided Image\n  Manipulation", "comments": "NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel lightweight generative adversarial network for efficient\nimage manipulation using natural language descriptions. To achieve this, a new\nword-level discriminator is proposed, which provides the generator with\nfine-grained training feedback at word-level, to facilitate training a\nlightweight generator that has a small number of parameters, but can still\ncorrectly focus on specific visual attributes of an image, and then edit them\nwithout affecting other contents that are not described in the text.\nFurthermore, thanks to the explicit training signal related to each word, the\ndiscriminator can also be simplified to have a lightweight structure. Compared\nwith the state of the art, our method has a much smaller number of parameters,\nbut still achieves a competitive manipulation performance. Extensive\nexperimental results demonstrate that our method can better disentangle\ndifferent visual attributes, then correctly map them to corresponding semantic\nwords, and thus achieve a more accurate image modification using natural\nlanguage descriptions.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 02:43:02 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Li", "Bowen", ""], ["Qi", "Xiaojuan", ""], ["Torr", "Philip H. S.", ""], ["Lukasiewicz", "Thomas", ""]]}, {"id": "2010.12138", "submitter": "Chao Liang", "authors": "Chao Liang, Zhipeng Zhang, Yi Lu, Xue Zhou, Bing Li, Xiyong Ye and\n  Jianxiao Zou", "title": "Rethinking the competition between detection and ReID in Multi-Object\n  Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to balanced accuracy and speed, joint learning detection and ReID-based\none-shot models have drawn great attention in multi-object tracking(MOT).\nHowever, the differences between the above two tasks in the one-shot tracking\nparadigm are unconsciously overlooked, leading to inferior performance than the\ntwo-stage methods. In this paper, we dissect the reasoning process of the\naforementioned two tasks. Our analysis reveals that the competition of them\ninevitably hurts the learning of task-dependent representations, which further\nimpedes the tracking performance. To remedy this issue, we propose a novel\ncross-correlation network that can effectively impel the separate branches to\nlearn task-dependent representations. Furthermore, we introduce a scale-aware\nattention network that learns discriminative embeddings to improve the ReID\ncapability. We integrate the delicately designed networks into a one-shot\nonline MOT system, dubbed CSTrack. Without bells and whistles, our model\nachieves new state-of-the-art performances on MOT16 and MOT17. Our code is\nreleased at https://github.com/JudasDie/SOTS.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 02:44:59 GMT"}, {"version": "v2", "created": "Thu, 19 Nov 2020 03:50:42 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Liang", "Chao", ""], ["Zhang", "Zhipeng", ""], ["Lu", "Yi", ""], ["Zhou", "Xue", ""], ["Li", "Bing", ""], ["Ye", "Xiyong", ""], ["Zou", "Jianxiao", ""]]}, {"id": "2010.12141", "submitter": "Mahesh Kumar Krishna Reddy", "authors": "Mahesh Kumar Krishna Reddy, Mrigank Rochan, Yiwei Lu, Yang Wang", "title": "AdaCrowd: Unlabeled Scene Adaptation for Crowd Counting", "comments": "Accepted for publication in IEEE Transactions on Multimedia (TMM)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of image-based crowd counting. In particular, we\npropose a new problem called unlabeled scene-adaptive crowd counting. Given a\nnew target scene, we would like to have a crowd counting model specifically\nadapted to this particular scene based on the target data that capture some\ninformation about the new scene. In this paper, we propose to use one or more\nunlabeled images from the target scene to perform the adaptation. In comparison\nwith the existing problem setups (e.g. fully supervised), our proposed problem\nsetup is closer to the real-world applications of crowd counting systems. We\nintroduce a novel AdaCrowd framework to solve this problem. Our framework\nconsists of a crowd counting network and a guiding network. The guiding network\npredicts some parameters in the crowd counting network based on the unlabeled\nimages from a particular scene. This allows our model to adapt to different\ntarget scenes. The experimental results on several challenging benchmark\ndatasets demonstrate the effectiveness of our proposed approach compared with\nother alternative methods. Code is available at\nhttps://github.com/maheshkkumar/adacrowd.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 03:20:42 GMT"}, {"version": "v2", "created": "Tue, 23 Feb 2021 06:05:48 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Reddy", "Mahesh Kumar Krishna", ""], ["Rochan", "Mrigank", ""], ["Lu", "Yiwei", ""], ["Wang", "Yang", ""]]}, {"id": "2010.12176", "submitter": "Yuxi Li", "authors": "Yuxi Li, Ning Xu, Jinlong Peng, John See, Weiyao Lin", "title": "Delving into the Cyclic Mechanism in Semi-supervised Video Object\n  Segmentation", "comments": "13 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address several inadequacies of current video object\nsegmentation pipelines. Firstly, a cyclic mechanism is incorporated to the\nstandard semi-supervised process to produce more robust representations. By\nrelying on the accurate reference mask in the starting frame, we show that the\nerror propagation problem can be mitigated. Next, we introduce a simple\ngradient correction module, which extends the offline pipeline to an online\nmethod while maintaining the efficiency of the former. Finally we develop cycle\neffective receptive field (cycle-ERF) based on gradient correction to provide a\nnew perspective into analyzing object-specific regions of interests. We conduct\ncomprehensive experiments on challenging benchmarks of DAVIS17 and Youtube-VOS,\ndemonstrating that the cyclic mechanism is beneficial to segmentation quality.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 05:40:53 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Li", "Yuxi", ""], ["Xu", "Ning", ""], ["Peng", "Jinlong", ""], ["See", "John", ""], ["Lin", "Weiyao", ""]]}, {"id": "2010.12184", "submitter": "Taotao Jing", "authors": "Taotao Jing, Bingrong Xu, Jingjing Li, Zhengming Ding", "title": "Towards Fair Knowledge Transfer for Imbalanced Domain Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain adaptation (DA) becomes an up-and-coming technique to address the\ninsufficient or no annotation issue by exploiting external source knowledge.\nExisting DA algorithms mainly focus on practical knowledge transfer through\ndomain alignment. Unfortunately, they ignore the fairness issue when the\nauxiliary source is extremely imbalanced across different categories, which\nresults in severe under-presented knowledge adaptation of minority source set.\nTo this end, we propose a Towards Fair Knowledge Transfer (TFKT) framework to\nhandle the fairness challenge in imbalanced cross-domain learning.\nSpecifically, a novel cross-domain mixup generation is exploited to augment the\nminority source set with target information to enhance fairness. Moreover, dual\ndistinct classifiers and cross-domain prototype alignment are developed to seek\na more robust classifier boundary and mitigate the domain shift. Such three\nstrategies are formulated into a unified framework to address the fairness\nissue and domain shift challenge. Extensive experiments over two popular\nbenchmarks have verified the effectiveness of our proposed model by comparing\nto existing state-of-the-art DA models, and especially our model significantly\nimproves over 20% on two benchmarks in terms of the overall accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 06:29:09 GMT"}, {"version": "v2", "created": "Tue, 24 Nov 2020 05:24:31 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Jing", "Taotao", ""], ["Xu", "Bingrong", ""], ["Li", "Jingjing", ""], ["Ding", "Zhengming", ""]]}, {"id": "2010.12190", "submitter": "Kun Fang", "authors": "Kun Fang, Yingwen Wu, Tao Li, Xiaolin Huang and Jie Yang", "title": "Learn Robust Features via Orthogonal Multi-Path", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is now widely known that by adversarial attacks, clean images with\ninvisible perturbations can fool deep neural networks. To defend adversarial\nattacks, we design a block containing multiple paths to learn robust features\nand the parameters of these paths are required to be orthogonal with each\nother. The so-called Orthogonal Multi-Path (OMP) block could be posed in any\nlayer of a neural network. Via forward learning and backward correction, one\nOMP block makes the neural networks learn features that are appropriate for all\nthe paths and hence are expected to be robust. With careful design and thorough\nexperiments on e.g., the positions of imposing orthogonality constraint, and\nthe trade-off between the variety and accuracy, the robustness of the neural\nnetworks is significantly improved. For example, under white-box PGD attack\nwith $l_\\infty$ bound ${8}/{255}$ (this is a fierce attack that can make the\naccuracy of many vanilla neural networks drop to nearly $10\\%$ on CIFAR10),\nVGG16 with the proposed OMP block could keep over $50\\%$ accuracy. For\nblack-box attacks, neural networks equipped with an OMP block have accuracy\nover $80\\%$. The performance under both white-box and black-box attacks is much\nbetter than the existing state-of-the-art adversarial defenders.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 06:40:56 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Fang", "Kun", ""], ["Wu", "Yingwen", ""], ["Li", "Tao", ""], ["Huang", "Xiaolin", ""], ["Yang", "Jie", ""]]}, {"id": "2010.12199", "submitter": "Irwandi Hipiny", "authors": "Dayang Nur Zulhijah Awang Jesemi, Hamimah Ujir, Irwandi Hipiny, Sarah\n  Flora Samson Juan", "title": "The Analysis of Facial Feature Deformation using Optical Flow Algorithm", "comments": "9 pages", "journal-ref": "IJEECS, Vol. 15, No. 2, pp. 769-777 (2019)", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial features deformed according to the intended facial expression.\nSpecific facial features are associated with specific facial expression, i.e.\nhappy means the deformation of mouth. This paper presents the study of facial\nfeature deformation for each facial expression by using an optical flow\nalgorithm and segmented into three different regions of interest. The\ndeformation of facial features shows the relation between facial the and facial\nexpression. Based on the experiments, the deformations of eye and mouth are\nsignificant in all expressions except happy. For happy expression, cheeks and\nmouths are the significant regions. This work also suggests that different\nfacial features' intensity varies in the way that they contribute to the\nrecognition of the different facial expression intensity. The maximum magnitude\nacross all expressions is shown by the mouth for surprise expression which is\n9x10-4. While the minimum magnitude is shown by the mouth for angry expression\nwhich is 0.4x10-4.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 07:14:02 GMT"}, {"version": "v2", "created": "Fri, 13 Nov 2020 04:14:33 GMT"}], "update_date": "2020-11-16", "authors_parsed": [["Jesemi", "Dayang Nur Zulhijah Awang", ""], ["Ujir", "Hamimah", ""], ["Hipiny", "Irwandi", ""], ["Juan", "Sarah Flora Samson", ""]]}, {"id": "2010.12216", "submitter": "Hang Zhu", "authors": "Hang Zhu and Zihao Wang", "title": "Feature matching in Ultrasound images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature matching is an important technique to identify a single object in\ndifferent images. It helps machines to construct recognition of a specific\nobject from multiple perspectives. For years, feature matching has been\ncommonly used in various computer vision applications, like traffic\nsurveillance, self-driving, and other systems. With the arise of Computer-Aided\nDiagnosis(CAD), the need for feature matching techniques also emerges in the\nmedical imaging field. In this paper, we present a deep learning-based method\nspecially for ultrasound images. It will be examined against existing methods\nthat have outstanding results on regular images. As the ultrasound images are\ndifferent from regular images in many fields like texture, noise type, and\ndimension, traditional methods will be evaluated and optimized to be applied to\nultrasound images.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 07:43:27 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Zhu", "Hang", ""], ["Wang", "Zihao", ""]]}, {"id": "2010.12219", "submitter": "Xinghao Ding", "authors": "Liyan Sun, Jianxiong Wu, Xinghao Ding, Yue Huang, Guisheng Wang and\n  Yizhou Yu", "title": "A Teacher-Student Framework for Semi-supervised Medical Image\n  Segmentation From Mixed Supervision", "comments": "10 pages, 11 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Standard segmentation of medical images based on full-supervised\nconvolutional networks demands accurate dense annotations. Such learning\nframework is built on laborious manual annotation with restrict demands for\nexpertise, leading to insufficient high-quality labels. To overcome such\nlimitation and exploit massive weakly labeled data, we relaxed the rigid\nlabeling requirement and developed a semi-supervised learning framework based\non a teacher-student fashion for organ and lesion segmentation with partial\ndense-labeled supervision and supplementary loose bounding-box supervision\nwhich are easier to acquire. Observing the geometrical relation of an organ and\nits inner lesions in most cases, we propose a hierarchical organ-to-lesion\n(O2L) attention module in a teacher segmentor to produce pseudo-labels. Then a\nstudent segmentor is trained with combinations of manual-labeled and\npseudo-labeled annotations. We further proposed a localization branch realized\nvia an aggregation of high-level features in a deep decoder to predict\nlocations of organ and lesion, which enriches student segmentor with precise\nlocalization information. We validated each design in our model on LiTS\nchallenge datasets by ablation study and showed its state-of-the-art\nperformance compared with recent methods. We show our model is robust to the\nquality of bounding box and achieves comparable performance compared with\nfull-supervised learning methods.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 07:58:20 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Sun", "Liyan", ""], ["Wu", "Jianxiong", ""], ["Ding", "Xinghao", ""], ["Huang", "Yue", ""], ["Wang", "Guisheng", ""], ["Yu", "Yizhou", ""]]}, {"id": "2010.12221", "submitter": "Negar Heidari", "authors": "Negar Heidari, Alexandros Iosifidis", "title": "Temporal Attention-Augmented Graph Convolutional Network for Efficient\n  Skeleton-Based Human Action Recognition", "comments": "Accepted by the 2020 International Conference on Pattern Recognition\n  (ICPR 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph convolutional networks (GCNs) have been very successful in modeling\nnon-Euclidean data structures, like sequences of body skeletons forming actions\nmodeled as spatio-temporal graphs. Most GCN-based action recognition methods\nuse deep feed-forward networks with high computational complexity to process\nall skeletons in an action. This leads to a high number of floating point\noperations (ranging from 16G to 100G FLOPs) to process a single sample, making\ntheir adoption in restricted computation application scenarios infeasible. In\nthis paper, we propose a temporal attention module (TAM) for increasing the\nefficiency in skeleton-based action recognition by selecting the most\ninformative skeletons of an action at the early layers of the network. We\nincorporate the TAM in a light-weight GCN topology to further reduce the\noverall number of computations. Experimental results on two benchmark datasets\nshow that the proposed method outperforms with a large margin the baseline\nGCN-based method while having 2.9 times less number of computations. Moreover,\nit performs on par with the state-of-the-art with up to 9.6 times less number\nof computations.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 08:01:55 GMT"}, {"version": "v2", "created": "Tue, 3 Nov 2020 14:53:16 GMT"}, {"version": "v3", "created": "Thu, 22 Apr 2021 18:15:32 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Heidari", "Negar", ""], ["Iosifidis", "Alexandros", ""]]}, {"id": "2010.12230", "submitter": "Jingzhao Zhang", "authors": "Jingzhao Zhang, Aditya Menon, Andreas Veit, Srinadh Bhojanapalli,\n  Sanjiv Kumar, Suvrit Sra", "title": "Coping with Label Shift via Distributionally Robust Optimisation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The label shift problem refers to the supervised learning setting where the\ntrain and test label distributions do not match. Existing work addressing label\nshift usually assumes access to an \\emph{unlabelled} test sample. This sample\nmay be used to estimate the test label distribution, and to then train a\nsuitably re-weighted classifier. While approaches using this idea have proven\neffective, their scope is limited as it is not always feasible to access the\ntarget domain; further, they require repeated retraining if the model is to be\ndeployed in \\emph{multiple} test environments. Can one instead learn a\n\\emph{single} classifier that is robust to arbitrary label shifts from a broad\nfamily? In this paper, we answer this question by proposing a model that\nminimises an objective based on distributionally robust optimisation (DRO). We\nthen design and analyse a gradient descent-proximal mirror ascent algorithm\ntailored for large-scale problems to optimise the proposed objective. %, and\nestablish its convergence. Finally, through experiments on CIFAR-100 and\nImageNet, we show that our technique can significantly improve performance over\na number of baselines in settings where label shift is present.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 08:33:04 GMT"}, {"version": "v2", "created": "Tue, 15 Jun 2021 07:23:28 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Zhang", "Jingzhao", ""], ["Menon", "Aditya", ""], ["Veit", "Andreas", ""], ["Bhojanapalli", "Srinadh", ""], ["Kumar", "Sanjiv", ""], ["Sra", "Suvrit", ""]]}, {"id": "2010.12238", "submitter": "Huan Fu", "authors": "Huan Fu, Shunming Li, Rongfei Jia, Mingming Gong, Binqiang Zhao, and\n  Dacheng Tao", "title": "Hard Example Generation by Texture Synthesis for Cross-domain Shape\n  Similarity Learning", "comments": "Accepted to NeurlPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image-based 3D shape retrieval (IBSR) aims to find the corresponding 3D shape\nof a given 2D image from a large 3D shape database. The common routine is to\nmap 2D images and 3D shapes into an embedding space and define (or learn) a\nshape similarity measure. While metric learning with some adaptation techniques\nseems to be a natural solution to shape similarity learning, the performance is\noften unsatisfactory for fine-grained shape retrieval. In the paper, we\nidentify the source of the poor performance and propose a practical solution to\nthis problem. We find that the shape difference between a negative pair is\nentangled with the texture gap, making metric learning ineffective in pushing\naway negative pairs. To tackle this issue, we develop a geometry-focused\nmulti-view metric learning framework empowered by texture synthesis. The\nsynthesis of textures for 3D shape models creates hard triplets, which suppress\nthe adverse effects of rich texture in 2D images, thereby push the network to\nfocus more on discovering geometric characteristics. Our approach shows\nstate-of-the-art performance on a recently released large-scale 3D-FUTURE[1]\nrepository, as well as three widely studied benchmarks, including Pix3D[2],\nStanford Cars[3], and Comp Cars[4]. Codes will be made publicly available at:\nhttps://github.com/3D-FRONT-FUTURE/IBSR-texture\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 08:52:00 GMT"}, {"version": "v2", "created": "Tue, 27 Oct 2020 02:12:58 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Fu", "Huan", ""], ["Li", "Shunming", ""], ["Jia", "Rongfei", ""], ["Gong", "Mingming", ""], ["Zhao", "Binqiang", ""], ["Tao", "Dacheng", ""]]}, {"id": "2010.12239", "submitter": "I\\~nigo Alonso", "authors": "Inigo Alonso, Luis Riazuelo. Luis Montesano, Ana C. Murillo", "title": "Domain Adaptation in LiDAR Semantic Segmentation", "comments": "7 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  LiDAR semantic segmentation provides 3D semantic information about the\nenvironment, an essential cue for intelligent systems during their decision\nmaking processes. Deep neural networks are achieving state-of-the-art results\non large public benchmarks on this task. Unfortunately, finding models that\ngeneralize well or adapt to additional domains, where data distribution is\ndifferent, remains a major challenge. This work addresses the problem of\nunsupervised domain adaptation for LiDAR semantic segmentation models. Our\napproach combines novel ideas on top of the current state-of-the-art approaches\nand yields new state-of-the-art results. We propose simple but effective\nstrategies to reduce the domain shift by aligning the data distribution on the\ninput space. Besides, we propose a learning-based approach that aligns the\ndistribution of the semantic classes of the target domain to the source domain.\nThe presented ablation study shows how each part contributes to the final\nperformance. Our strategy is shown to outperform previous approaches for domain\nadaptation with comparisons run on three different domains.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 08:52:15 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Alonso", "Inigo", ""], ["Montesano", "Luis Riazuelo. Luis", ""], ["Murillo", "Ana C.", ""]]}, {"id": "2010.12249", "submitter": "Vahid Reza Khazaie", "authors": "Vahid Reza Khazaie and Nicky Bayat and Yalda Mohsenzadeh", "title": "Multi Scale Identity-Preserving Image-to-Image Translation Network for\n  Low-Resolution Face Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art deep neural network models have reached near perfect face\nrecognition accuracy rates on controlled high-resolution face images. However,\ntheir performance is drastically degraded when they are tested with very\nlow-resolution face images. This is particularly critical in surveillance\nsystems, where a low-resolution probe image is to be matched with\nhigh-resolution gallery images. super-resolution techniques aim at producing\nhigh-resolution face images from low-resolution counterparts. While they are\ncapable of reconstructing images that are visually appealing, the\nidentity-related information is not preserved. Here, we propose an\nidentity-preserving end-to-end image-to-image translation deep neural network\nwhich is capable of super-resolving very low-resolution faces to their\nhigh-resolution counterparts while preserving identity-related information. We\nachieved this by training a very deep convolutional encoder-decoder network\nwith a symmetric contracting path between corresponding layers. This network\nwas trained with a combination of a reconstruction and an identity-preserving\nloss, on multi-scale low-resolution conditions. Extensive quantitative\nevaluations of our proposed model demonstrated that it outperforms competing\nsuper-resolution and low-resolution face recognition methods on natural and\nartificial low-resolution face data sets and even unseen identities.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 09:21:06 GMT"}, {"version": "v2", "created": "Sat, 1 May 2021 17:10:12 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Khazaie", "Vahid Reza", ""], ["Bayat", "Nicky", ""], ["Mohsenzadeh", "Yalda", ""]]}, {"id": "2010.12260", "submitter": "Alejo Jesus Nevado-Holgado", "authors": "Yurika Sakai, Andrey Kormilitzin, Qiang Liu, Alejo Nevado-Holgado", "title": "Population Gradients improve performance across data-sets and\n  architectures in object classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The most successful methods such as ReLU transfer functions, batch\nnormalization, Xavier initialization, dropout, learning rate decay, or dynamic\noptimizers, have become standards in the field due, particularly, to their\nability to increase the performance of Neural Networks (NNs) significantly and\nin almost all situations. Here we present a new method to calculate the\ngradients while training NNs, and show that it significantly improves final\nperformance across architectures, data-sets, hyper-parameter values, training\nlength, and model sizes, including when it is being combined with other common\nperformance-improving methods (such as the ones mentioned above). Besides being\neffective in the wide array situations that we have tested, the increase in\nperformance (e.g. F1) it provides is as high or higher than this one of all the\nother widespread performance-improving methods that we have compared against.\nWe call our method Population Gradients (PG), and it consists on using a\npopulation of NNs to calculate a non-local estimation of the gradient, which is\ncloser to the theoretical exact gradient (i.e. this one obtainable only with an\ninfinitely big data-set) of the error function than the empirical gradient\n(i.e. this one obtained with the real finite data-set).\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 09:40:23 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Sakai", "Yurika", ""], ["Kormilitzin", "Andrey", ""], ["Liu", "Qiang", ""], ["Nevado-Holgado", "Alejo", ""]]}, {"id": "2010.12267", "submitter": "Xinsheng Wang", "authors": "Xinsheng Wang, Siyuan Feng, Jihua Zhu, Mark Hasegawa-Johnson, Odette\n  Scharenborg", "title": "Show and Speak: Directly Synthesize Spoken Description of Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new model, referred to as the show and speak (SAS)\nmodel that, for the first time, is able to directly synthesize spoken\ndescriptions of images, bypassing the need for any text or phonemes. The basic\nstructure of SAS is an encoder-decoder architecture that takes an image as\ninput and predicts the spectrogram of speech that describes this image. The\nfinal speech audio is obtained from the predicted spectrogram via WaveNet.\nExtensive experiments on the public benchmark database Flickr8k demonstrate\nthat the proposed SAS is able to synthesize natural spoken descriptions for\nimages, indicating that synthesizing spoken descriptions for images while\nbypassing text and phonemes is feasible.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 09:53:01 GMT"}, {"version": "v2", "created": "Tue, 17 Nov 2020 10:58:19 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Wang", "Xinsheng", ""], ["Feng", "Siyuan", ""], ["Zhu", "Jihua", ""], ["Hasegawa-Johnson", "Mark", ""], ["Scharenborg", "Odette", ""]]}, {"id": "2010.12307", "submitter": "Yufeng Zheng", "authors": "Yufeng Zheng, Seonwook Park, Xucong Zhang, Shalini De Mello, Otmar\n  Hilliges", "title": "Self-Learning Transformations for Improving Gaze and Head Redirection", "comments": "Accepted at NeurIPS 2020. Check our supplementary video at:\n  https://ait.ethz.ch/projects/2020/STED-gaze/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Many computer vision tasks rely on labeled data. Rapid progress in generative\nmodeling has led to the ability to synthesize photorealistic images. However,\ncontrolling specific aspects of the generation process such that the data can\nbe used for supervision of downstream tasks remains challenging. In this paper\nwe propose a novel generative model for images of faces, that is capable of\nproducing high-quality images under fine-grained control over eye gaze and head\norientation angles. This requires the disentangling of many appearance related\nfactors including gaze and head orientation but also lighting, hue etc. We\npropose a novel architecture which learns to discover, disentangle and encode\nthese extraneous variations in a self-learned manner. We further show that\nexplicitly disentangling task-irrelevant factors results in more accurate\nmodelling of gaze and head orientation. A novel evaluation scheme shows that\nour method improves upon the state-of-the-art in redirection accuracy and\ndisentanglement between gaze direction and head orientation changes.\nFurthermore, we show that in the presence of limited amounts of real-world\ntraining data, our method allows for improvements in the downstream task of\nsemi-supervised cross-dataset gaze estimation. Please check our project page\nat: https://ait.ethz.ch/projects/2020/STED-gaze/\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 11:18:37 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Zheng", "Yufeng", ""], ["Park", "Seonwook", ""], ["Zhang", "Xucong", ""], ["De Mello", "Shalini", ""], ["Hilliges", "Otmar", ""]]}, {"id": "2010.12316", "submitter": "Valentyn Melnychuk", "authors": "Valentyn Melnychuk, Evgeniy Faerman, Ilja Manakov and Thomas Seidl", "title": "Matching the Clinical Reality: Accurate OCT-Based Diagnosis From Few\n  Labels", "comments": "KDAH-CIKM-2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Unlabeled data is often abundant in the clinic, making machine learning\nmethods based on semi-supervised learning a good match for this setting.\nDespite this, they are currently receiving relatively little attention in\nmedical image analysis literature. Instead, most practitioners and researchers\nfocus on supervised or transfer learning approaches. The recently proposed\nMixMatch and FixMatch algorithms have demonstrated promising results in\nextracting useful representations while requiring very few labels. Motivated by\nthese recent successes, we apply MixMatch and FixMatch in an ophthalmological\ndiagnostic setting and investigate how they fare against standard transfer\nlearning. We find that both algorithms outperform the transfer learning\nbaseline on all fractions of labelled data. Furthermore, our experiments show\nthat exponential moving average (EMA) of model parameters, which is a component\nof both algorithms, is not needed for our classification problem, as disabling\nit leaves the outcome unchanged. Our code is available online:\nhttps://github.com/Valentyn1997/oct-diagn-semi-supervised\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 11:47:28 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Melnychuk", "Valentyn", ""], ["Faerman", "Evgeniy", ""], ["Manakov", "Ilja", ""], ["Seidl", "Thomas", ""]]}, {"id": "2010.12317", "submitter": "Moritz Einfalt", "authors": "Nikolas Klug, Moritz Einfalt, Stephan Brehm, Rainer Lienhart", "title": "Error Bounds of Projection Models in Weakly Supervised 3D Human Pose\n  Estimation", "comments": "Accepted at 3DV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The current state-of-the-art in monocular 3D human pose estimation is heavily\ninfluenced by weakly supervised methods. These allow 2D labels to be used to\nlearn effective 3D human pose recovery either directly from images or via\n2D-to-3D pose uplifting. In this paper we present a detailed analysis of the\nmost commonly used simplified projection models, which relate the estimated 3D\npose representation to 2D labels: normalized perspective and weak perspective\nprojections. Specifically, we derive theoretical lower bound errors for those\nprojection models under the commonly used mean per-joint position error\n(MPJPE). Additionally, we show how the normalized perspective projection can be\nreplaced to avoid this guaranteed minimal error. We evaluate the derived lower\nbounds on the most commonly used 3D human pose estimation benchmark datasets.\nOur results show that both projection models lead to an inherent minimal error\nbetween 19.3mm and 54.7mm, even after alignment in position and scale. This is\na considerable share when comparing with recent state-of-the-art results. Our\npaper thus establishes a theoretical baseline that shows the importance of\nsuitable projection models in weakly supervised 3D human pose estimation.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 11:48:13 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Klug", "Nikolas", ""], ["Einfalt", "Moritz", ""], ["Brehm", "Stephan", ""], ["Lienhart", "Rainer", ""]]}, {"id": "2010.12320", "submitter": "Feng Liu", "authors": "Feng Liu and Xiaoming Liu", "title": "Learning Implicit Functions for Topology-Varying Dense 3D Shape\n  Correspondence", "comments": "Accepted by NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of this paper is to learn dense 3D shape correspondence for\ntopology-varying objects in an unsupervised manner. Conventional implicit\nfunctions estimate the occupancy of a 3D point given a shape latent code.\nInstead, our novel implicit function produces a part embedding vector for each\n3D point, which is assumed to be similar to its densely corresponded point in\nanother 3D shape of the same object category. Furthermore, we implement dense\ncorrespondence through an inverse function mapping from the part embedding to a\ncorresponded 3D point. Both functions are jointly learned with several\neffective loss functions to realize our assumption, together with the encoder\ngenerating the shape latent code. During inference, if a user selects an\narbitrary point on the source shape, our algorithm can automatically generate a\nconfidence score indicating whether there is a correspondence on the target\nshape, as well as the corresponding semantic point if there is one. Such a\nmechanism inherently benefits man-made objects with different part\nconstitutions. The effectiveness of our approach is demonstrated through\nunsupervised 3D semantic correspondence and shape segmentation.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 11:52:06 GMT"}, {"version": "v2", "created": "Mon, 26 Oct 2020 01:22:55 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Liu", "Feng", ""], ["Liu", "Xiaoming", ""]]}, {"id": "2010.12335", "submitter": "Ryosuke Tsumura", "authors": "Ryosuke Tsumura, John W. Hardin, Keshav Bimbraw, Olushola S. Odusanya,\n  Yihao Zheng, Jeffrey C. Hill, Beatrice Hoffmann, Winston Soboyejo, Haichong\n  K. Zhang", "title": "Tele-operative Robotic Lung Ultrasound Scanning Platform for Triage of\n  COVID-19 Patients", "comments": "The demonstration video of our robotic platform can be watched below\n  the link <https://youtu.be/BbNCvESTYik>", "journal-ref": "IEEE Robotics and Automation Letters (2021)", "doi": "10.1109/LRA.2021.3068702", "report-no": null, "categories": "cs.RO cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Novel severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) has become\na pandemic of epic proportions and a global response to prepare health systems\nworldwide is of utmost importance. In addition to its cost-effectiveness in a\nresources-limited setting, lung ultrasound (LUS) has emerged as a rapid\nnoninvasive imaging tool for the diagnosis of COVID-19 infected patients.\nConcerns surrounding LUS include the disparity of infected patients and\nhealthcare providers, relatively small number of physicians and sonographers\ncapable of performing LUS, and most importantly, the requirement for\nsubstantial physical contact between the patient and operator, increasing the\nrisk of transmission. Mitigation of the spread of the virus is of paramount\nimportance. A 2-dimensional (2D) tele-operative robotic platform capable of\nperforming LUS in for COVID-19 infected patients may be of significant benefit.\nThe authors address the aforementioned issues surrounding the use of LUS in the\napplication of COVID- 19 infected patients. In addition, first time\napplication, feasibility and safety were validated in three healthy subjects,\nalong with 2D image optimization and comparison for overall accuracy.\nPreliminary results demonstrate that the proposed platform allows for\nsuccessful acquisition and application of LUS in humans.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 12:17:42 GMT"}, {"version": "v2", "created": "Fri, 30 Oct 2020 02:58:41 GMT"}, {"version": "v3", "created": "Thu, 12 Nov 2020 02:36:53 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Tsumura", "Ryosuke", ""], ["Hardin", "John W.", ""], ["Bimbraw", "Keshav", ""], ["Odusanya", "Olushola S.", ""], ["Zheng", "Yihao", ""], ["Hill", "Jeffrey C.", ""], ["Hoffmann", "Beatrice", ""], ["Soboyejo", "Winston", ""], ["Zhang", "Haichong K.", ""]]}, {"id": "2010.12337", "submitter": "Puhong Duan", "authors": "Puhong Duan and Pedram Ghamisi and Xudong Kang and Behnood Rasti and\n  Shutao Li and Richard Gloaguen", "title": "Fusion of Dual Spatial Information for Hyperspectral Image\n  Classification", "comments": "13 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IT eess.IV math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The inclusion of spatial information into spectral classifiers for\nfine-resolution hyperspectral imagery has led to significant improvements in\nterms of classification performance. The task of spectral-spatial hyperspectral\nimage classification has remained challenging because of high intraclass\nspectrum variability and low interclass spectral variability. This fact has\nmade the extraction of spatial information highly active. In this work, a novel\nhyperspectral image classification framework using the fusion of dual spatial\ninformation is proposed, in which the dual spatial information is built by both\nexploiting pre-processing feature extraction and post-processing spatial\noptimization. In the feature extraction stage, an adaptive texture smoothing\nmethod is proposed to construct the structural profile (SP), which makes it\npossible to precisely extract discriminative features from hyperspectral\nimages. The SP extraction method is used here for the first time in the remote\nsensing community. Then, the extracted SP is fed into a spectral classifier. In\nthe spatial optimization stage, a pixel-level classifier is used to obtain the\nclass probability followed by an extended random walker-based spatial\noptimization technique. Finally, a decision fusion rule is utilized to fuse the\nclass probabilities obtained by the two different stages. Experiments performed\non three data sets from different scenes illustrate that the proposed method\ncan outperform other state-of-the-art classification techniques. In addition,\nthe proposed feature extraction method, i.e., SP, can effectively improve the\ndiscrimination between different land covers.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 12:20:18 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Duan", "Puhong", ""], ["Ghamisi", "Pedram", ""], ["Kang", "Xudong", ""], ["Rasti", "Behnood", ""], ["Li", "Shutao", ""], ["Gloaguen", "Richard", ""]]}, {"id": "2010.12347", "submitter": "Yuma Kinoshita", "authors": "Yuma Kinoshita and Hitoshi Kiya", "title": "Checkerboard-Artifact-Free Image-Enhancement Network Considering Local\n  and Global Features", "comments": "to appear in APSIPA ASC 2020. arXiv admin note: text overlap with\n  arXiv:1905.02899", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel convolutional neural network (CNN) that\nnever causes checkerboard artifacts, for image enhancement. In research fields\nof image-to-image translation problems, it is well-known that images generated\nby usual CNNs are distorted by checkerboard artifacts which mainly caused in\nforward-propagation of upsampling layers. However, checkerboard artifacts in\nimage enhancement have never been discussed. In this paper, we point out that\napplying U-Net based CNNs to image enhancement causes checkerboard artifacts.\nIn contrast, the proposed network that contains fixed convolutional layers can\nperfectly prevent the artifacts. In addition, the proposed network\narchitecture, which can handle both local and global features, enables us to\nimprove the performance of image enhancement. Experimental results show that\nthe use of fixed convolutional layers can prevent checkerboard artifacts and\nthe proposed network outperforms state-of-the-art CNN-based image-enhancement\nmethods in terms of various objective quality metrics: PSNR, SSIM, and NIQE.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 01:28:23 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Kinoshita", "Yuma", ""], ["Kiya", "Hitoshi", ""]]}, {"id": "2010.12369", "submitter": "Dennis Eschweiler", "authors": "Dennis Eschweiler and Malte Rethwisch and Simon Koppers and Johannes\n  Stegmaier", "title": "Spherical Harmonics for Shape-Constrained 3D Cell Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent microscopy imaging techniques allow to precisely analyze cell\nmorphology in 3D image data. To process the vast amount of image data generated\nby current digitized imaging techniques, automated approaches are demanded more\nthan ever. Segmentation approaches used for morphological analyses, however,\nare often prone to produce unnaturally shaped predictions, which in conclusion\ncould lead to inaccurate experimental outcomes. In order to minimize further\nmanual interaction, shape priors help to constrain the predictions to the set\nof natural variations. In this paper, we show how spherical harmonics can be\nused as an alternative way to inherently constrain the predictions of neural\nnetworks for the segmentation of cells in 3D microscopy image data. Benefits\nand limitations of the spherical harmonic representation are analyzed and final\nresults are compared to other state-of-the-art approaches on two different data\nsets.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 12:58:26 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Eschweiler", "Dennis", ""], ["Rethwisch", "Malte", ""], ["Koppers", "Simon", ""], ["Stegmaier", "Johannes", ""]]}, {"id": "2010.12390", "submitter": "Alexey Sidnev", "authors": "Alexey Sidnev, Ekaterina Krasikova, Maxim Kazakov", "title": "Efficient grouping for keypoint detection", "comments": "ICPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The success of deep neural networks in the traditional keypoint detection\ntask encourages researchers to solve new problems and collect more complex\ndatasets. The size of the DeepFashion2 dataset poses a new challenge on the\nkeypoint detection task, as it comprises 13 clothing categories that span a\nwide range of keypoints (294 in total). The direct prediction of all keypoints\nleads to huge memory consumption, slow training, and a slow inference time.\nThis paper studies the keypoint grouping approach and how it affects the\nperformance of the CenterNet architecture. We propose a simple and efficient\nautomatic grouping technique with a powerful post-processing method and apply\nit to the DeepFashion2 fashion landmark task and the MS COCO pose estimation\ntask. This reduces memory consumption and processing time during inference by\nup to 19% and 30% respectively, and during the training stage by 28% and 26%\nrespectively, without compromising accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 13:25:45 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Sidnev", "Alexey", ""], ["Krasikova", "Ekaterina", ""], ["Kazakov", "Maxim", ""]]}, {"id": "2010.12391", "submitter": "Priscille de Dumast", "authors": "Priscille de Dumast, Hamza Kebiri, Chirine Atat, Vincent Dunet,\n  M\\'eriam Koob, Meritxell Bach Cuadra", "title": "Segmentation of the cortical plate in fetal brain MRI with a topological\n  loss", "comments": "4 pages, 4 figures. This work has been submitted to the IEEE for\n  possible publication. Copyright may be transferred without notice, after\n  which this version may no longer be accessible", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The fetal cortical plate undergoes drastic morphological changes throughout\nearly in utero development that can be observed using magnetic resonance (MR)\nimaging. An accurate MR image segmentation, and more importantly a\ntopologically correct delineation of the cortical gray matter, is a key\nbaseline to perform further quantitative analysis of brain development. In this\npaper, we propose for the first time the integration of a topological\nconstraint, as an additional loss function, to enhance the morphological\nconsistency of a deep learning-based segmentation of the fetal cortical plate.\nWe quantitatively evaluate our method on 18 fetal brain atlases ranging from 21\nto 38 weeks of gestation, showing the significant benefits of our method\nthrough all gestational ages as compared to a baseline method. Furthermore,\nqualitative evaluation by three different experts on 130 randomly selected\nslices from 26 clinical MRIs evidences the out-performance of our method\nindependently of the MR reconstruction quality.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 13:25:45 GMT"}, {"version": "v2", "created": "Tue, 24 Nov 2020 12:38:57 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["de Dumast", "Priscille", ""], ["Kebiri", "Hamza", ""], ["Atat", "Chirine", ""], ["Dunet", "Vincent", ""], ["Koob", "M\u00e9riam", ""], ["Cuadra", "Meritxell Bach", ""]]}, {"id": "2010.12394", "submitter": "Fan Lu", "authors": "Fan Lu and Guang Chen and Yinlong Liu and Zhongnan Qu and Alois Knoll", "title": "RSKDD-Net: Random Sample-based Keypoint Detector and Descriptor", "comments": "Accepted to NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Keypoint detector and descriptor are two main components of point cloud\nregistration. Previous learning-based keypoint detectors rely on saliency\nestimation for each point or farthest point sample (FPS) for candidate points\nselection, which are inefficient and not applicable in large scale scenes. This\npaper proposes Random Sample-based Keypoint Detector and Descriptor Network\n(RSKDD-Net) for large scale point cloud registration. The key idea is using\nrandom sampling to efficiently select candidate points and using a\nlearning-based method to jointly generate keypoints and descriptors. To tackle\nthe information loss of random sampling, we exploit a novel random dilation\ncluster strategy to enlarge the receptive field of each sampled point and an\nattention mechanism to aggregate the positions and features of neighbor points.\nFurthermore, we propose a matching loss to train the descriptor in a weakly\nsupervised manner. Extensive experiments on two large scale outdoor LiDAR\ndatasets show that the proposed RSKDD-Net achieves state-of-the-art performance\nwith more than 15 times faster than existing methods. Our code is available at\nhttps://github.com/ispc-lab/RSKDD-Net.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 13:29:29 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Lu", "Fan", ""], ["Chen", "Guang", ""], ["Liu", "Yinlong", ""], ["Qu", "Zhongnan", ""], ["Knoll", "Alois", ""]]}, {"id": "2010.12416", "submitter": "Shuai Shao", "authors": "Shuai Shao and Rui Xu and Yan-Jiang Wang and Weifeng Liu and Bao-Di\n  Liu", "title": "SAHDL: Sparse Attention Hypergraph Regularized Dictionary Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, the attention mechanism contributes significantly to\nhypergraph based neural networks. However, these methods update the attention\nweights with the network propagating. That is to say, this type of attention\nmechanism is only suitable for deep learning-based methods while not applicable\nto the traditional machine learning approaches. In this paper, we propose a\nhypergraph based sparse attention mechanism to tackle this issue and embed it\ninto dictionary learning. More specifically, we first construct a sparse\nattention hypergraph, asset attention weights to samples by employing the\n$\\ell_1$-norm sparse regularization to mine the high-order relationship among\nsample features. Then, we introduce the hypergraph Laplacian operator to\npreserve the local structure for subspace transformation in dictionary\nlearning. Besides, we incorporate the discriminative information into the\nhypergraph as the guidance to aggregate samples. Unlike previous works, our\nmethod updates attention weights independently, does not rely on the deep\nnetwork. We demonstrate the efficacy of our approach on four benchmark\ndatasets.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 14:07:00 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Shao", "Shuai", ""], ["Xu", "Rui", ""], ["Wang", "Yan-Jiang", ""], ["Liu", "Weifeng", ""], ["Liu", "Bao-Di", ""]]}, {"id": "2010.12417", "submitter": "Shuai Shao", "authors": "Shuai Shao and Mengke Wang and Rui Xu and Yan-Jiang Wang and Bao-Di\n  Liu", "title": "DLDL: Dynamic Label Dictionary Learning via Hypergraph Regularization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For classification tasks, dictionary learning based methods have attracted\nlots of attention in recent years. One popular way to achieve this purpose is\nto introduce label information to generate a discriminative dictionary to\nrepresent samples. However, compared with traditional dictionary learning, this\ncategory of methods only achieves significant improvements in supervised\nlearning, and has little positive influence on semi-supervised or unsupervised\nlearning. To tackle this issue, we propose a Dynamic Label Dictionary Learning\n(DLDL) algorithm to generate the soft label matrix for unlabeled data.\nSpecifically, we employ hypergraph manifold regularization to keep the\nrelations among original data, transformed data, and soft labels consistent. We\ndemonstrate the efficiency of the proposed DLDL approach on two remote sensing\ndatasets.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 14:07:07 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Shao", "Shuai", ""], ["Wang", "Mengke", ""], ["Xu", "Rui", ""], ["Wang", "Yan-Jiang", ""], ["Liu", "Bao-Di", ""]]}, {"id": "2010.12422", "submitter": "Yue Cao", "authors": "Yali Peng, Yue Cao, Shigang Liu, Jian Yang, and Wangmeng Zuo", "title": "Progressive Training of Multi-level Wavelet Residual Networks for Image\n  Denoising", "comments": "13 pages, 7 figures code: https://github.com/happycaoyue/PT-MWRN", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have witnessed the great success of deep convolutional neural\nnetworks (CNNs) in image denoising. Albeit deeper network and larger model\ncapacity generally benefit performance, it remains a challenging practical\nissue to train a very deep image denoising network. Using multilevel\nwavelet-CNN (MWCNN) as an example, we empirically find that the denoising\nperformance cannot be significantly improved by either increasing wavelet\ndecomposition levels or increasing convolution layers within each level. To\ncope with this issue, this paper presents a multi-level wavelet residual\nnetwork (MWRN) architecture as well as a progressive training (PTMWRN) scheme\nto improve image denoising performance. In contrast to MWCNN, our MWRN\nintroduces several residual blocks after each level of discrete wavelet\ntransform (DWT) and before inverse discrete wavelet transform (IDWT). For\neasing the training difficulty, scale-specific loss is applied to each level of\nMWRN by requiring the intermediate output to approximate the corresponding\nwavelet subbands of ground-truth clean image. To ensure the effectiveness of\nscale-specific loss, we also take the wavelet subbands of noisy image as the\ninput to each scale of the encoder. Furthermore, progressive training scheme is\nadopted for better learning of MWRN by beigining with training the lowest level\nof MWRN and progressively training the upper levels to bring more fine details\nto denoising results. Experiments on both synthetic and real-world noisy images\nshow that our PT-MWRN performs favorably against the state-of-the-art denoising\nmethods in terms both quantitative metrics and visual quality.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 14:14:00 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Peng", "Yali", ""], ["Cao", "Yue", ""], ["Liu", "Shigang", ""], ["Yang", "Jian", ""], ["Zuo", "Wangmeng", ""]]}, {"id": "2010.12427", "submitter": "Shiqi Yang", "authors": "Shiqi Yang, Yaxing Wang, Joost van de Weijer, Luis Herranz and\n  Shangling Jui", "title": "Unsupervised Domain Adaptation without Source Data by Casting a BAIT", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised domain adaptation (UDA) aims to transfer the knowledge learned\nfrom a labeled source domain to an unlabeled target domain. Existing UDA\nmethods require access to source data during adaptation, which may not be\nfeasible in some real-world applications. In this paper, we address the\nsource-free unsupervised domain adaptation (SFUDA) problem, where only the\nsource model is available during the adaptation. We propose a method named BAIT\nto address SFUDA. Specifically, given only the source model, with the source\nclassifier head fixed, we introduce a new learnable classifier. When adapting\nto the target domain, class prototypes of the new added classifier will act as\na bait. They will first approach the target features which deviate from\nprototypes of the source classifier due to domain shift. Then those target\nfeatures are pulled towards the corresponding prototypes of the source\nclassifier, thus achieving feature alignment with the source classifier in the\nabsence of source data. Experimental results show that the proposed method\nachieves state-of-the-art performance on several benchmark datasets compared\nwith existing UDA and SFUDA methods.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 14:18:42 GMT"}, {"version": "v2", "created": "Wed, 28 Oct 2020 14:07:09 GMT"}, {"version": "v3", "created": "Thu, 19 Nov 2020 16:48:38 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Yang", "Shiqi", ""], ["Wang", "Yaxing", ""], ["van de Weijer", "Joost", ""], ["Herranz", "Luis", ""], ["Jui", "Shangling", ""]]}, {"id": "2010.12435", "submitter": "Xuehai He", "authors": "Xuehai He, Zhuo Cai, Wenlan Wei, Yichen Zhang, Luntian Mou, Eric Xing,\n  Pengtao Xie", "title": "Pathological Visual Question Answering", "comments": "arXiv admin note: text overlap with arXiv:2003.10286", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Is it possible to develop an \"AI Pathologist\" to pass the board-certified\nexamination of the American Board of Pathology (ABP)? To build such a system,\nthree challenges need to be addressed. First, we need to create a visual\nquestion answering (VQA) dataset where the AI agent is presented with a\npathology image together with a question and is asked to give the correct\nanswer. Due to privacy concerns, pathology images are usually not publicly\navailable. Besides, only well-trained pathologists can understand pathology\nimages, but they barely have time to help create datasets for AI research. The\nsecond challenge is: since it is difficult to hire highly experienced\npathologists to create pathology visual questions and answers, the resulting\npathology VQA dataset may contain errors. Training pathology VQA models using\nthese noisy or even erroneous data will lead to problematic models that cannot\ngeneralize well on unseen images. The third challenge is: the medical concepts\nand knowledge covered in pathology question-answer (QA) pairs are very diverse\nwhile the number of QA pairs available for modeling training is limited. How to\nlearn effective representations of diverse medical concepts based on limited\ndata is technically demanding. In this paper, we aim to address these three\nchallenges. To our best knowledge, our work represents the first one addressing\nthe pathology VQA problem. To deal with the issue that a publicly available\npathology VQA dataset is lacking, we create PathVQA dataset. To address the\nsecond challenge, we propose a learning-by-ignoring approach. To address the\nthird challenge, we propose to use cross-modal self-supervised learning. We\nperform experiments on our created PathVQA dataset and the results demonstrate\nthe effectiveness of our proposed learning-by-ignoring method and cross-modal\nself-supervised learning methods.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 00:36:55 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["He", "Xuehai", ""], ["Cai", "Zhuo", ""], ["Wei", "Wenlan", ""], ["Zhang", "Yichen", ""], ["Mou", "Luntian", ""], ["Xing", "Eric", ""], ["Xie", "Pengtao", ""]]}, {"id": "2010.12436", "submitter": "Christian Sormann", "authors": "Christian Sormann (1), Patrick Kn\\\"obelreiter (1), Andreas Kuhn (2),\n  Mattia Rossi (2), Thomas Pock (1), Friedrich Fraundorfer (1) ((1) Graz\n  University of Technology, (2) Sony Europe B.V.)", "title": "BP-MVSNet: Belief-Propagation-Layers for Multi-View-Stereo", "comments": "accepted at 3DV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose BP-MVSNet, a convolutional neural network\n(CNN)-based Multi-View-Stereo (MVS) method that uses a differentiable\nConditional Random Field (CRF) layer for regularization. To this end, we\npropose to extend the BP layer and add what is necessary to successfully use it\nin the MVS setting. We therefore show how we can calculate a normalization\nbased on the expected 3D error, which we can then use to normalize the label\njumps in the CRF. This is required to make the BP layer invariant to different\nscales in the MVS setting. In order to also enable fractional label jumps, we\npropose a differentiable interpolation step, which we embed into the\ncomputation of the pairwise term. These extensions allow us to integrate the BP\nlayer into a multi-scale MVS network, where we continuously improve a rough\ninitial estimate until we get high quality depth maps as a result. We evaluate\nthe proposed BP-MVSNet in an ablation study and conduct extensive experiments\non the DTU, Tanks and Temples and ETH3D data sets. The experiments show that we\ncan significantly outperform the baseline and achieve state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 14:30:07 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Sormann", "Christian", ""], ["Kn\u00f6belreiter", "Patrick", ""], ["Kuhn", "Andreas", ""], ["Rossi", "Mattia", ""], ["Pock", "Thomas", ""], ["Fraundorfer", "Friedrich", ""]]}, {"id": "2010.12440", "submitter": "Xiaofeng Liu", "authors": "Xiaofeng Liu, Yuzhuo Han, Song Bai, Yi Ge, Tianxing Wang, Xu Han, Site\n  Li, Jane You, Ju Lu", "title": "Importance-Aware Semantic Segmentation in Self-Driving with Discrete\n  Wasserstein Training", "comments": "Published in Thirty-Fourth AAAI Conference on Artificial Intelligence\n  (AAAI) 2020. arXiv admin note: text overlap with arXiv:2008.04751", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic segmentation (SS) is an important perception manner for self-driving\ncars and robotics, which classifies each pixel into a pre-determined class. The\nwidely-used cross entropy (CE) loss-based deep networks has achieved\nsignificant progress w.r.t. the mean Intersection-over Union (mIoU). However,\nthe cross entropy loss can not take the different importance of each class in\nan self-driving system into account. For example, pedestrians in the image\nshould be much more important than the surrounding buildings when make a\ndecisions in the driving, so their segmentation results are expected to be as\naccurate as possible. In this paper, we propose to incorporate the\nimportance-aware inter-class correlation in a Wasserstein training framework by\nconfiguring its ground distance matrix. The ground distance matrix can be\npre-defined following a priori in a specific task, and the previous\nimportance-ignored methods can be the particular cases. From an optimization\nperspective, we also extend our ground metric to a linear, convex or concave\nincreasing function $w.r.t.$ pre-defined ground distance. We evaluate our\nmethod on CamVid and Cityscapes datasets with different backbones (SegNet,\nENet, FCN and Deeplab) in a plug and play fashion. In our extenssive\nexperiments, Wasserstein loss demonstrates superior segmentation performance on\nthe predefined critical classes for safe-driving.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 20:43:47 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Liu", "Xiaofeng", ""], ["Han", "Yuzhuo", ""], ["Bai", "Song", ""], ["Ge", "Yi", ""], ["Wang", "Tianxing", ""], ["Han", "Xu", ""], ["Li", "Site", ""], ["You", "Jane", ""], ["Lu", "Ju", ""]]}, {"id": "2010.12446", "submitter": "Renee Miller", "authors": "Eric Kerfoot, Carlos Escudero King, Tefvik Ismail, David Nordsletten,\n  Renee Miller", "title": "Estimation of Cardiac Valve Annuli Motion with Deep Learning", "comments": "10 pages, STACOM abstract", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Valve annuli motion and morphology, measured from non-invasive imaging, can\nbe used to gain a better understanding of healthy and pathological heart\nfunction. Measurements such as long-axis strain as well as peak strain rates\nprovide markers of systolic function. Likewise, early and late-diastolic\nfilling velocities are used as indicators of diastolic function. Quantifying\nglobal strains, however, requires a fast and precise method of tracking\nlong-axis motion throughout the cardiac cycle. Valve landmarks such as the\ninsertion of leaflets into the myocardial wall provide features that can be\ntracked to measure global long-axis motion. Feature tracking methods require\ninitialisation, which can be time-consuming in studies with large cohorts.\nTherefore, this study developed and trained a neural network to identify ten\nfeatures from unlabeled long-axis MR images: six mitral valve points from three\nlong-axis views, two aortic valve points and two tricuspid valve points. This\nstudy used manual annotations of valve landmarks in standard 2-, 3- and\n4-chamber long-axis images collected in clinical scans to train the network.\nThe accuracy in the identification of these ten features, in pixel distance,\nwas compared with the accuracy of two commonly used feature tracking methods as\nwell as the inter-observer variability of manual annotations. Clinical\nmeasures, such as valve landmark strain and motion between end-diastole and\nend-systole, are also presented to illustrate the utility and robustness of the\nmethod.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 14:39:47 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Kerfoot", "Eric", ""], ["King", "Carlos Escudero", ""], ["Ismail", "Tefvik", ""], ["Nordsletten", "David", ""], ["Miller", "Renee", ""]]}, {"id": "2010.12447", "submitter": "Bharat Lal Bhatnagar", "authors": "Bharat Lal Bhatnagar, Cristian Sminchisescu, Christian Theobalt,\n  Gerard Pons-Moll", "title": "LoopReg: Self-supervised Learning of Implicit Surface Correspondences,\n  Pose and Shape for 3D Human Mesh Registration", "comments": "NeurIPS'20 (Oral)", "journal-ref": "NeurIPS 2020", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of fitting 3D human models to 3D scans of dressed\nhumans. Classical methods optimize both the data-to-model correspondences and\nthe human model parameters (pose and shape), but are reliable only when\ninitialized close to the solution. Some methods initialize the optimization\nbased on fully supervised correspondence predictors, which is not\ndifferentiable end-to-end, and can only process a single scan at a time. Our\nmain contribution is LoopReg, an end-to-end learning framework to register a\ncorpus of scans to a common 3D human model. The key idea is to create a\nself-supervised loop. A backward map, parameterized by a Neural Network,\npredicts the correspondence from every scan point to the surface of the human\nmodel. A forward map, parameterized by a human model, transforms the\ncorresponding points back to the scan based on the model parameters (pose and\nshape), thus closing the loop. Formulating this closed loop is not\nstraightforward because it is not trivial to force the output of the NN to be\non the surface of the human model - outside this surface the human model is not\neven defined. To this end, we propose two key innovations. First, we define the\ncanonical surface implicitly as the zero level set of a distance field in R3,\nwhich in contrast to morecommon UV parameterizations, does not require cutting\nthe surface, does not have discontinuities, and does not induce distortion.\nSecond, we diffuse the human model to the 3D domain R3. This allows to map the\nNN predictions forward,even when they slightly deviate from the zero level set.\nResults demonstrate that we can train LoopRegmainly self-supervised - following\na supervised warm-start, the model becomes increasingly more accurate as\nadditional unlabelled raw scans are processed. Our code and pre-trained models\ncan be downloaded for research.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 14:39:50 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Bhatnagar", "Bharat Lal", ""], ["Sminchisescu", "Cristian", ""], ["Theobalt", "Christian", ""], ["Pons-Moll", "Gerard", ""]]}, {"id": "2010.12455", "submitter": "Antonio Loquercio", "authors": "Francesco Milano, Antonio Loquercio, Antoni Rosinol, Davide\n  Scaramuzza, Luca Carlone", "title": "Primal-Dual Mesh Convolutional Neural Networks", "comments": "Accepted to the 34th Conference on Neural Information Processing\n  Systems (NeurIPS 2020), Vancouver, Canada. Code available at:\n  https://github.com/MIT-SPARK/PD-MeshNet", "journal-ref": "34th Conference on Neural Information Processing Systems (NeurIPS\n  2020)", "doi": null, "report-no": null, "categories": "cs.CV cs.CG cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent works in geometric deep learning have introduced neural networks that\nallow performing inference tasks on three-dimensional geometric data by\ndefining convolution, and sometimes pooling, operations on triangle meshes.\nThese methods, however, either consider the input mesh as a graph, and do not\nexploit specific geometric properties of meshes for feature aggregation and\ndownsampling, or are specialized for meshes, but rely on a rigid definition of\nconvolution that does not properly capture the local topology of the mesh. We\npropose a method that combines the advantages of both types of approaches,\nwhile addressing their limitations: we extend a primal-dual framework drawn\nfrom the graph-neural-network literature to triangle meshes, and define\nconvolutions on two types of graphs constructed from an input mesh. Our method\ntakes features for both edges and faces of a 3D mesh as input and dynamically\naggregates them using an attention mechanism. At the same time, we introduce a\npooling operation with a precise geometric interpretation, that allows handling\nvariations in the mesh connectivity by clustering mesh faces in a task-driven\nfashion. We provide theoretical insights of our approach using tools from the\nmesh-simplification literature. In addition, we validate experimentally our\nmethod in the tasks of shape classification and shape segmentation, where we\nobtain comparable or superior performance to the state of the art.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 14:49:02 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Milano", "Francesco", ""], ["Loquercio", "Antonio", ""], ["Rosinol", "Antoni", ""], ["Scaramuzza", "Davide", ""], ["Carlone", "Luca", ""]]}, {"id": "2010.12488", "submitter": "Jianren Wang", "authors": "Jianren Wang, Yujie Lu, Hang Zhao", "title": "CLOUD: Contrastive Learning of Unsupervised Dynamics", "comments": null, "journal-ref": "CORL 2020", "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Developing agents that can perform complex control tasks from high\ndimensional observations such as pixels is challenging due to difficulties in\nlearning dynamics efficiently. In this work, we propose to learn forward and\ninverse dynamics in a fully unsupervised manner via contrastive estimation.\nSpecifically, we train a forward dynamics model and an inverse dynamics model\nin the feature space of states and actions with data collected from random\nexploration. Unlike most existing deterministic models, our energy-based model\ntakes into account the stochastic nature of agent-environment interactions. We\ndemonstrate the efficacy of our approach across a variety of tasks including\ngoal-directed planning and imitation from observations. Project videos and code\nare at https://jianrenw.github.io/cloud/.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 15:42:57 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Wang", "Jianren", ""], ["Lu", "Yujie", ""], ["Zhao", "Hang", ""]]}, {"id": "2010.12496", "submitter": "Chaoning Zhang", "authors": "Chaoning Zhang, Philipp Benz, Dawit Mureja Argaw, Seokju Lee, Junsik\n  Kim, Francois Rameau, Jean-Charles Bazin, In So Kweon", "title": "ResNet or DenseNet? Introducing Dense Shortcuts to ResNet", "comments": "Accepted to WACV2021 first round", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  ResNet or DenseNet? Nowadays, most deep learning based approaches are\nimplemented with seminal backbone networks, among them the two arguably most\nfamous ones are ResNet and DenseNet. Despite their competitive performance and\noverwhelming popularity, inherent drawbacks exist for both of them. For ResNet,\nthe identity shortcut that stabilizes training also limits its representation\ncapacity, while DenseNet has a higher capacity with multi-layer feature\nconcatenation. However, the dense concatenation causes a new problem of\nrequiring high GPU memory and more training time. Partially due to this, it is\nnot a trivial choice between ResNet and DenseNet. This paper provides a unified\nperspective of dense summation to analyze them, which facilitates a better\nunderstanding of their core difference. We further propose dense weighted\nnormalized shortcuts as a solution to the dilemma between them. Our proposed\ndense shortcut inherits the design philosophy of simple design in ResNet and\nDenseNet. On several benchmark datasets, the experimental results show that the\nproposed DSNet achieves significantly better results than ResNet, and achieves\ncomparable performance as DenseNet but requiring fewer computation resources.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 16:00:15 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Zhang", "Chaoning", ""], ["Benz", "Philipp", ""], ["Argaw", "Dawit Mureja", ""], ["Lee", "Seokju", ""], ["Kim", "Junsik", ""], ["Rameau", "Francois", ""], ["Bazin", "Jean-Charles", ""], ["Kweon", "In So", ""]]}, {"id": "2010.12552", "submitter": "Saeed Khaki", "authors": "Saeed Khaki, Hieu Pham, Ye Han, Wade Kent and Lizhi Wang", "title": "High-Throughput Image-Based Plant Stand Count Estimation Using\n  Convolutional Neural Networks", "comments": "15 pages, 3 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The future landscape of modern farming and plant breeding is rapidly changing\ndue to the complex needs of our society. The explosion of collectable data has\nstarted a revolution in agriculture to the point where innovation must occur.\nTo a commercial organization, the accurate and efficient collection of\ninformation is necessary to ensure that optimal decisions are made at key\npoints of the breeding cycle. However, due to the shear size of a breeding\nprogram and current resource limitations, the ability to collect precise data\non individual plants is not possible. In particular, efficient phenotyping of\ncrops to record its color, shape, chemical properties, disease susceptibility,\netc. is severely limited due to labor requirements and, oftentimes, expert\ndomain knowledge. In this paper, we propose a deep learning based approach,\nnamed DeepStand, for image-based corn stand counting at early phenological\nstages. The proposed method adopts a truncated VGG-16 network as a backbone\nfeature extractor and merges multiple feature maps with different scales to\nmake the network robust against scale variation. Our extensive computational\nexperiments suggest that our proposed method can successfully count corn stands\nand out-perform other state-of-the-art methods. It is the goal of our work to\nbe used by the larger agricultural community as a way to enable high-throughput\nphenotyping without the use of extensive time and labor requirements.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 17:28:29 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Khaki", "Saeed", ""], ["Pham", "Hieu", ""], ["Han", "Ye", ""], ["Kent", "Wade", ""], ["Wang", "Lizhi", ""]]}, {"id": "2010.12573", "submitter": "Hong Zhang", "authors": "Qichuan Geng, Hong Zhang, Na Jiang, Xiaojuan Qi, Liangjun Zhang, Zhong\n  Zhou", "title": "Object-aware Feature Aggregation for Video Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an Object-aware Feature Aggregation (OFA) module for video object\ndetection (VID). Our approach is motivated by the intriguing property that\nvideo-level object-aware knowledge can be employed as a powerful semantic prior\nto help object recognition. As a consequence, augmenting features with such\nprior knowledge can effectively improve the classification and localization\nperformance. To make features get access to more content about the whole video,\nwe first capture the object-aware knowledge of proposals and incorporate such\nknowledge with the well-established pair-wise contexts. With extensive\nexperimental results on the ImageNet VID dataset, our approach demonstrates the\neffectiveness of object-aware knowledge with the superior performance of 83.93%\nand 86.09% mAP with ResNet-101 and ResNeXt-101, respectively. When further\nequipped with Sequence DIoU NMS, we obtain the best-reported mAP of 85.07% and\n86.88% upon the paper submitted. The code to reproduce our results will be\nreleased after acceptance.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 17:56:25 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Geng", "Qichuan", ""], ["Zhang", "Hong", ""], ["Jiang", "Na", ""], ["Qi", "Xiaojuan", ""], ["Zhang", "Liangjun", ""], ["Zhou", "Zhong", ""]]}, {"id": "2010.12575", "submitter": "Ponkrshnan Thiagarajan", "authors": "Pushkar Khairnar, Ponkrshnan Thiagarajan, and Susanta Ghosh", "title": "A modified Bayesian Convolutional Neural Network for Breast\n  Histopathology Image Classification and Uncertainty Quantification", "comments": "To be submitted to IEEE for peer review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural network (CNN) based classification models have been\nsuccessfully used on histopathological images for the detection of diseases.\nDespite its success, CNN may yield erroneous or overfitted results when the\ndata is not sufficiently large or is biased. To overcome these limitations of\nCNN and to provide uncertainty quantification Bayesian CNN is recently\nproposed. However, we show that Bayesian-CNN still suffers from inaccuracies,\nespecially in negative predictions. In the present work, we extend the\nBayesian-CNN to improve accuracy and the rate of convergence. The proposed\nmodel is called modified Bayesian-CNN. The novelty of the proposed model lies\nin an adaptive activation function that contains a learnable parameter for each\nof the neurons. This adaptive activation function dynamically changes the loss\nfunction thereby providing faster convergence and better accuracy. The\nuncertainties associated with the predictions are obtained since the model\nlearns a probability distribution on the network parameters. It reduces\noverfitting through an ensemble averaging over networks, which in turn improves\naccuracy on the unknown data. The proposed model demonstrates significant\nimprovement by nearly eliminating overfitting and remarkably reducing (about\n38%) the number of false-negative predictions. We found that the proposed model\npredicts higher uncertainty for images having features of both the classes. The\nuncertainty in the predictions of individual images can be used to decide when\nfurther human-expert intervention is needed. These findings have the potential\nto advance the state-of-the-art machine learning based automatic classification\nfor histopathological images.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 23:20:26 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Khairnar", "Pushkar", ""], ["Thiagarajan", "Ponkrshnan", ""], ["Ghosh", "Susanta", ""]]}, {"id": "2010.12576", "submitter": "Gabriele Scrivanti", "authors": "Gabriele Scrivanti, Luca Calatroni, Serena Morigi, Lindsay Nicholson,\n  Alin Achim", "title": "Non-convex Super-resolution of OCT images via sparse representation", "comments": "4 pages, 2 figures, 1 table, 1 algorithm, submitted to ISBI2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a non-convex variational model for the super-resolution of Optical\nCoherence Tomography (OCT) images of the murine eye, by enforcing sparsity with\nrespect to suitable dictionaries learnt from high-resolution OCT data. The\nstatistical characteristics of OCT images motivate the use of {\\alpha}-stable\ndistributions for learning dictionaries, by considering the non-Gaussian case,\n{\\alpha}=1. The sparsity-promoting cost function relies on a non-convex penalty\n- Cauchy-based or Minimax Concave Penalty (MCP) - which makes the problem\nparticularly challenging. We propose an efficient algorithm for minimizing the\nfunction based on the forward-backward splitting strategy which guarantees at\neach iteration the existence and uniqueness of the proximal point. Comparisons\nwith standard convex L1-based reconstructions show the better performance of\nnon-convex models, especially in view of further OCT image analysis\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 08:44:11 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Scrivanti", "Gabriele", ""], ["Calatroni", "Luca", ""], ["Morigi", "Serena", ""], ["Nicholson", "Lindsay", ""], ["Achim", "Alin", ""]]}, {"id": "2010.12606", "submitter": "Roland Zimmermann", "authors": "Judy Borowski, Roland S. Zimmermann, Judith Schepers, Robert Geirhos,\n  Thomas S. A. Wallis, Matthias Bethge, Wieland Brendel", "title": "Exemplary Natural Images Explain CNN Activations Better than\n  State-of-the-Art Feature Visualization", "comments": "Published at ICLR 2021. Joint first and last authors. Code is\n  available at https://bethgelab.github.io/testing_visualizations/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature visualizations such as synthetic maximally activating images are a\nwidely used explanation method to better understand the information processing\nof convolutional neural networks (CNNs). At the same time, there are concerns\nthat these visualizations might not accurately represent CNNs' inner workings.\nHere, we measure how much extremely activating images help humans to predict\nCNN activations. Using a well-controlled psychophysical paradigm, we compare\nthe informativeness of synthetic images by Olah et al. (2017) with a simple\nbaseline visualization, namely exemplary natural images that also strongly\nactivate a specific feature map. Given either synthetic or natural reference\nimages, human participants choose which of two query images leads to strong\npositive activation. The experiments are designed to maximize participants'\nperformance, and are the first to probe intermediate instead of final layer\nrepresentations. We find that synthetic images indeed provide helpful\ninformation about feature map activations ($82\\pm4\\%$ accuracy; chance would be\n$50\\%$). However, natural images - originally intended as a baseline -\noutperform synthetic images by a wide margin ($92\\pm2\\%$). Additionally,\nparticipants are faster and more confident for natural images, whereas\nsubjective impressions about the interpretability of the feature visualizations\nare mixed. The higher informativeness of natural images holds across most\nlayers, for both expert and lay participants as well as for hand- and\nrandomly-picked feature visualizations. Even if only a single reference image\nis given, synthetic images provide less information than natural images\n($65\\pm5\\%$ vs. $73\\pm4\\%$). In summary, synthetic images from a popular\nfeature visualization method are significantly less informative for assessing\nCNN activations than natural images. We argue that visualization methods should\nimprove over this baseline.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 18:31:13 GMT"}, {"version": "v2", "created": "Tue, 12 Jan 2021 10:00:47 GMT"}, {"version": "v3", "created": "Sun, 2 May 2021 19:27:06 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Borowski", "Judy", ""], ["Zimmermann", "Roland S.", ""], ["Schepers", "Judith", ""], ["Geirhos", "Robert", ""], ["Wallis", "Thomas S. A.", ""], ["Bethge", "Matthias", ""], ["Brendel", "Wieland", ""]]}, {"id": "2010.12622", "submitter": "Rahul Ragesh", "authors": "Arunava Chakraborty, Rahul Ragesh, Mahir Shah, Nipun Kwatra", "title": "S2cGAN: Semi-Supervised Training of Conditional GANs with Fewer Labels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative adversarial networks (GANs) have been remarkably successful in\nlearning complex high dimensional real word distributions and generating\nrealistic samples. However, they provide limited control over the generation\nprocess. Conditional GANs (cGANs) provide a mechanism to control the generation\nprocess by conditioning the output on a user defined input. Although training\nGANs requires only unsupervised data, training cGANs requires labelled data\nwhich can be very expensive to obtain. We propose a framework for\nsemi-supervised training of cGANs which utilizes sparse labels to learn the\nconditional mapping, and at the same time leverages a large amount of\nunsupervised data to learn the unconditional distribution. We demonstrate\neffectiveness of our method on multiple datasets and different conditional\ntasks.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 19:13:44 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Chakraborty", "Arunava", ""], ["Ragesh", "Rahul", ""], ["Shah", "Mahir", ""], ["Kwatra", "Nipun", ""]]}, {"id": "2010.12631", "submitter": "Cunjian Chen", "authors": "Cunjian Chen and Arun Ross", "title": "Attention-Guided Network for Iris Presentation Attack Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNNs) are being increasingly used to address\nthe problem of iris presentation attack detection. In this work, we propose\nattention-guided iris presentation attack detection (AG-PAD) to augment CNNs\nwith attention mechanisms. Two types of attention modules are independently\nappended on top of the last convolutional layer of the backbone network.\nSpecifically, the channel attention module is used to model the inter-channel\nrelationship between features, while the position attention module is used to\nmodel inter-spatial relationship between features. An element-wise sum is\nemployed to fuse these two attention modules. Further, a novel hierarchical\nattention mechanism is introduced. Experiments involving both a JHU-APL\nproprietary dataset and the benchmark LivDet-Iris-2017 dataset suggest that the\nproposed method achieves promising results. To the best of our knowledge, this\nis the first work that exploits the use of attention mechanisms in iris\npresentation attack detection.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 19:23:51 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Chen", "Cunjian", ""], ["Ross", "Arun", ""]]}, {"id": "2010.12639", "submitter": "Jesse Thomason", "authors": "Shurjo Banerjee, Jesse Thomason, Jason J. Corso", "title": "The RobotSlang Benchmark: Dialog-guided Robot Localization and\n  Navigation", "comments": "Conference on Robot Learning 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous robot systems for applications from search and rescue to assistive\nguidance should be able to engage in natural language dialog with people. To\nstudy such cooperative communication, we introduce Robot Simultaneous\nLocalization and Mapping with Natural Language (RobotSlang), a benchmark of 169\nnatural language dialogs between a human Driver controlling a robot and a human\nCommander providing guidance towards navigation goals. In each trial, the pair\nfirst cooperates to localize the robot on a global map visible to the\nCommander, then the Driver follows Commander instructions to move the robot to\na sequence of target objects. We introduce a Localization from Dialog History\n(LDH) and a Navigation from Dialog History (NDH) task where a learned agent is\ngiven dialog and visual observations from the robot platform as input and must\nlocalize in the global map or navigate towards the next target object,\nrespectively. RobotSlang is comprised of nearly 5k utterances and over 1k\nminutes of robot camera and control streams. We present an initial model for\nthe NDH task, and show that an agent trained in simulation can follow the\nRobotSlang dialog-based navigation instructions for controlling a physical\nrobot platform. Code and data are available at https://umrobotslang.github.io/.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 19:58:17 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Banerjee", "Shurjo", ""], ["Thomason", "Jesse", ""], ["Corso", "Jason J.", ""]]}, {"id": "2010.12669", "submitter": "Partha Roy Dr.", "authors": "Prasun Roy and Saumik Bhattacharya and Partha Pratim Roy and Umapada\n  Pal", "title": "Position and Rotation Invariant Sign Language Recognition from 3D Point\n  Cloud Data with Recurrent Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sign language is a gesture based symbolic communication medium among speech\nand hearing impaired people. It also serves as a communication bridge between\nnon-impaired population and impaired population. Unfortunately, in most\nsituations a non-impaired person is not well conversant in such symbolic\nlanguages which restricts natural information flow between these two categories\nof population. Therefore, an automated translation mechanism can be greatly\nuseful that can seamlessly translate sign language into natural language. In\nthis paper, we attempt to perform recognition on 30 basic Indian sign gestures.\nGestures are represented as temporal sequences of 3D depth maps each consisting\nof 3D coordinates of 20 body joints. A recurrent neural network (RNN) is\nemployed as classifier. To improve performance of the classifier, we use\ngeometric transformation for alignment correction of depth frames. In our\nexperiments the model achieves 84.81% accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 21:07:40 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Roy", "Prasun", ""], ["Bhattacharya", "Saumik", ""], ["Roy", "Partha Pratim", ""], ["Pal", "Umapada", ""]]}, {"id": "2010.12670", "submitter": "Djamila Aouada", "authors": "Alexandre Saint, Anis Kacem, Kseniya Cherenkova, Djamila Aouada", "title": "3DBooSTeR: 3D Body Shape and Texture Recovery", "comments": null, "journal-ref": "SHARP Workshop, European Conference on Computer Vision (ECCV),\n  2020", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose 3DBooSTeR, a novel method to recover a textured 3D body mesh from\na textured partial 3D scan. With the advent of virtual and augmented reality,\nthere is a demand for creating realistic and high-fidelity digital 3D human\nrepresentations. However, 3D scanning systems can only capture the 3D human\nbody shape up to some level of defects due to its complexity, including\nocclusion between body parts, varying levels of details, shape deformations and\nthe articulated skeleton. Textured 3D mesh completion is thus important to\nenhance 3D acquisitions. The proposed approach decouples the shape and texture\ncompletion into two sequential tasks. The shape is recovered by an\nencoder-decoder network deforming a template body mesh. The texture is\nsubsequently obtained by projecting the partial texture onto the template mesh\nbefore inpainting the corresponding texture map with a novel approach. The\napproach is validated on the 3DBodyTex.v2 dataset.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 21:07:59 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Saint", "Alexandre", ""], ["Kacem", "Anis", ""], ["Cherenkova", "Kseniya", ""], ["Aouada", "Djamila", ""]]}, {"id": "2010.12682", "submitter": "Mehmet Ayg\\\"un", "authors": "Mehmet Ayg\\\"un, Zorah L\\\"ahner, Daniel Cremers", "title": "Unsupervised Dense Shape Correspondence using Heat Kernels", "comments": "In International Conference on 3D Vision (3DV), 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose an unsupervised method for learning dense\ncorrespondences between shapes using a recent deep functional map framework.\nInstead of depending on ground-truth correspondences or the computationally\nexpensive geodesic distances, we use heat kernels. These can be computed\nquickly during training as the supervisor signal. Moreover, we propose a\ncurriculum learning strategy using different heat diffusion times which provide\ndifferent levels of difficulty during optimization without any sampling\nmechanism or hard example mining. We present the results of our method on\ndifferent benchmarks which have various challenges like partiality, topological\nnoise and different connectivity.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 21:54:10 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Ayg\u00fcn", "Mehmet", ""], ["L\u00e4hner", "Zorah", ""], ["Cremers", "Daniel", ""]]}, {"id": "2010.12697", "submitter": "Bilal Alsallakh", "authors": "Vivek Miglani and Narine Kokhlikyan and Bilal Alsallakh and Miguel\n  Martin and Orion Reblitz-Richardson", "title": "Investigating Saturation Effects in Integrated Gradients", "comments": "Presented at ICML Workshop on Human Interpretability in Machine\n  Learning (WHI 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Integrated Gradients has become a popular method for post-hoc model\ninterpretability. De-spite its popularity, the composition and relative impact\nof different regions of the integral path are not well understood. We explore\nthese effects and find that gradients in saturated regions of this path, where\nmodel output changes minimally, contribute disproportionately to the computed\nattribution. We propose a variant of IntegratedGradients which primarily\ncaptures gradients in unsaturated regions and evaluate this method on ImageNet\nclassification networks. We find that this attribution technique shows higher\nmodel faithfulness and lower sensitivity to noise com-pared with standard\nIntegrated Gradients. A note-book illustrating our computations and results is\navailable at https://github.com/vivekmig/captum-1/tree/ExpandedIG.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 22:48:02 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Miglani", "Vivek", ""], ["Kokhlikyan", "Narine", ""], ["Alsallakh", "Bilal", ""], ["Martin", "Miguel", ""], ["Reblitz-Richardson", "Orion", ""]]}, {"id": "2010.12737", "submitter": "Ji Hyun Nam", "authors": "Ji Hyun Nam, Eric Brandt, Sebastian Bauer, Xiaochun Liu, Eftychios\n  Sifakis, Andreas Velten", "title": "Real-time Non-line-of-Sight imaging of dynamic scenes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-Line-of-Sight (NLOS) imaging aims at recovering the 3D geometry of\nobjects that are hidden from the direct line of sight. In the past, this method\nhas suffered from the weak available multibounce signal limiting scene size,\ncapture speed, and reconstruction quality. While algorithms capable of\nreconstructing scenes at several frames per second have been demonstrated,\nreal-time NLOS video has only been demonstrated for retro-reflective objects\nwhere the NLOS signal strength is enhanced by 4 orders of magnitude or more.\nFurthermore, it has also been noted that the signal-to-noise ratio of\nreconstructions in NLOS methods drops quickly with distance and past\nreconstructions, therefore, have been limited to small scenes with depths of\nfew meters. Actual models of noise and resolution in the scene have been\nsimplistic, ignoring many of the complexities of the problem. We show that SPAD\n(Single-Photon Avalanche Diode) array detectors with a total of just 28 pixels\ncombined with a specifically extended Phasor Field reconstruction algorithm can\nreconstruct live real-time videos of non-retro-reflective NLOS scenes. We\nprovide an analysis of the Signal-to-Noise-Ratio (SNR) of our reconstructions\nand show that for our method it is possible to reconstruct the scene such that\nSNR, motion blur, angular resolution, and depth resolution are all independent\nof scene size suggesting that reconstruction of very large scenes may be\npossible. In the future, the light efficiency for NLOS imaging systems can be\nimproved further by adding more pixels to the sensor array.\n", "versions": [{"version": "v1", "created": "Sat, 24 Oct 2020 01:40:06 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Nam", "Ji Hyun", ""], ["Brandt", "Eric", ""], ["Bauer", "Sebastian", ""], ["Liu", "Xiaochun", ""], ["Sifakis", "Eftychios", ""], ["Velten", "Andreas", ""]]}, {"id": "2010.12764", "submitter": "Rodolfo Corona", "authors": "Rodolfo Corona, Daniel Fried, Coline Devin, Dan Klein, Trevor Darrell", "title": "Modular Networks for Compositional Instruction Following", "comments": "Published in NAACL-2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Standard architectures used in instruction following often struggle on novel\ncompositions of subgoals (e.g. navigating to landmarks or picking up objects)\nobserved during training. We propose a modular architecture for following\nnatural language instructions that describe sequences of diverse subgoals. In\nour approach, subgoal modules each carry out natural language instructions for\na specific subgoal type. A sequence of modules to execute is chosen by learning\nto segment the instructions and predicting a subgoal type for each segment.\nWhen compared to standard, non-modular sequence-to-sequence approaches on\nALFRED, a challenging instruction following benchmark, we find that\nmodularization improves generalization to novel subgoal compositions, as well\nas to environments unseen in training.\n", "versions": [{"version": "v1", "created": "Sat, 24 Oct 2020 03:48:45 GMT"}, {"version": "v2", "created": "Tue, 13 Apr 2021 05:34:01 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Corona", "Rodolfo", ""], ["Fried", "Daniel", ""], ["Devin", "Coline", ""], ["Klein", "Dan", ""], ["Darrell", "Trevor", ""]]}, {"id": "2010.12796", "submitter": "Xiaqing Ding", "authors": "Xiaqing Ding, Yue Wang, Li Tang, Yanmei Jiao and Rong Xiong", "title": "Improving the generalization of network based relative pose regression:\n  dimension reduction as a regularizer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual localization occupies an important position in many areas such as\nAugmented Reality, robotics and 3D reconstruction. The state-of-the-art visual\nlocalization methods perform pose estimation using geometry based solver within\nthe RANSAC framework. However, these methods require accurate pixel-level\nmatching at high image resolution, which is hard to satisfy under significant\nchanges from appearance, dynamics or perspective of view. End-to-end learning\nbased regression networks provide a solution to circumvent the requirement for\nprecise pixel-level correspondences, but demonstrate poor performance towards\ncross-scene generalization. In this paper, we explicitly add a learnable\nmatching layer within the network to isolate the pose regression solver from\nthe absolute image feature values, and apply dimension regularization on both\nthe correlation feature channel and the image scale to further improve\nperformance towards generalization and large viewpoint change. We implement\nthis dimension regularization strategy within a two-layer pyramid based\nframework to regress the localization results from coarse to fine. In addition,\nthe depth information is fused for absolute translational scale recovery.\nThrough experiments on real world RGBD datasets we validate the effectiveness\nof our design in terms of improving both generalization performance and\nrobustness towards viewpoint change, and also show the potential of regression\nbased visual localization networks towards challenging occasions that are\ndifficult for geometry based visual localization methods.\n", "versions": [{"version": "v1", "created": "Sat, 24 Oct 2020 06:20:46 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Ding", "Xiaqing", ""], ["Wang", "Yue", ""], ["Tang", "Li", ""], ["Jiao", "Yanmei", ""], ["Xiong", "Rong", ""]]}, {"id": "2010.12807", "submitter": "Weitong Hua", "authors": "Weitong Hua, Zhongxiang Zhou, Jun Wu, Huang Huang, Yue Wang, Rong\n  Xiong", "title": "REDE: End-to-end Object 6D Pose Robust Estimation Using Differentiable\n  Outliers Elimination", "comments": null, "journal-ref": null, "doi": "10.1109/LRA.2021.3062304", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object 6D pose estimation is a fundamental task in many applications.\nConventional methods solve the task by detecting and matching the keypoints,\nthen estimating the pose. Recent efforts bringing deep learning into the\nproblem mainly overcome the vulnerability of conventional methods to\nenvironmental variation due to the hand-crafted feature design. However, these\nmethods cannot achieve end-to-end learning and good interpretability at the\nsame time. In this paper, we propose REDE, a novel end-to-end object pose\nestimator using RGB-D data, which utilizes network for keypoint regression, and\na differentiable geometric pose estimator for pose error back-propagation.\nBesides, to achieve better robustness when outlier keypoint prediction occurs,\nwe further propose a differentiable outliers elimination method that regresses\nthe candidate result and the confidence simultaneously. Via confidence weighted\naggregation of multiple candidates, we can reduce the effect from the outliers\nin the final estimation. Finally, following the conventional method, we apply a\nlearnable refinement process to further improve the estimation. The\nexperimental results on three benchmark datasets show that REDE slightly\noutperforms the state-of-the-art approaches and is more robust to object\nocclusion.\n", "versions": [{"version": "v1", "created": "Sat, 24 Oct 2020 06:45:39 GMT"}, {"version": "v2", "created": "Tue, 12 Jan 2021 06:06:50 GMT"}, {"version": "v3", "created": "Wed, 24 Feb 2021 13:38:26 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Hua", "Weitong", ""], ["Zhou", "Zhongxiang", ""], ["Wu", "Jun", ""], ["Huang", "Huang", ""], ["Wang", "Yue", ""], ["Xiong", "Rong", ""]]}, {"id": "2010.12831", "submitter": "Liunian Harold Li", "authors": "Liunian Harold Li, Haoxuan You, Zhecan Wang, Alireza Zareian, Shih-Fu\n  Chang, Kai-Wei Chang", "title": "Unsupervised Vision-and-Language Pre-training Without Parallel Images\n  and Captions", "comments": "NAACL 2021 Camera Ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pre-trained contextual vision-and-language (V&L) models have achieved\nimpressive performance on various benchmarks. However, existing models require\na large amount of parallel image-caption data for pre-training. Such data are\ncostly to collect and require cumbersome curation. Inspired by unsupervised\nmachine translation, we investigate if a strong V&L representation model can be\nlearned through unsupervised pre-training without image-caption corpora. In\nparticular, we propose to conduct ``mask-and-predict'' pre-training on\ntext-only and image-only corpora and introduce the object tags detected by an\nobject recognition model as anchor points to bridge two modalities. We find\nthat such a simple approach achieves performance close to a model pre-trained\nwith aligned data, on four English V&L benchmarks. Our work challenges the\nwidely held notion that aligned data is necessary for V&L pre-training, while\nsignificantly reducing the amount of supervision needed for V&L models.\n", "versions": [{"version": "v1", "created": "Sat, 24 Oct 2020 08:17:54 GMT"}, {"version": "v2", "created": "Sun, 11 Apr 2021 23:54:25 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Li", "Liunian Harold", ""], ["You", "Haoxuan", ""], ["Wang", "Zhecan", ""], ["Zareian", "Alireza", ""], ["Chang", "Shih-Fu", ""], ["Chang", "Kai-Wei", ""]]}, {"id": "2010.12852", "submitter": "Radhika Dua", "authors": "Radhika Dua, Sai Srinivas Kancheti and Vineeth N Balasubramanian", "title": "Beyond VQA: Generating Multi-word Answer and Rationale to Visual\n  Questions", "comments": "MULA Workshop, CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Visual Question Answering is a multi-modal task that aims to measure\nhigh-level visual understanding. Contemporary VQA models are restrictive in the\nsense that answers are obtained via classification over a limited vocabulary\n(in the case of open-ended VQA), or via classification over a set of\nmultiple-choice-type answers. In this work, we present a completely generative\nformulation where a multi-word answer is generated for a visual query. To take\nthis a step forward, we introduce a new task: ViQAR (Visual Question Answering\nand Reasoning), wherein a model must generate the complete answer and a\nrationale that seeks to justify the generated answer. We propose an end-to-end\narchitecture to solve this task and describe how to evaluate it. We show that\nour model generates strong answers and rationales through qualitative and\nquantitative evaluation, as well as through a human Turing Test.\n", "versions": [{"version": "v1", "created": "Sat, 24 Oct 2020 09:44:50 GMT"}, {"version": "v2", "created": "Thu, 17 Jun 2021 09:44:12 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Dua", "Radhika", ""], ["Kancheti", "Sai Srinivas", ""], ["Balasubramanian", "Vineeth N", ""]]}, {"id": "2010.12880", "submitter": "Simindokht Jahangard", "authors": "Mehdi Bonyani, Simindokht Jahangard, Morteza Daneshmand", "title": "Persian Handwritten Digit, Character and Word Recognition Using Deep\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Digit, letter and word recognition for a particular script has various\napplications in todays commercial contexts. Nevertheless, only a limited number\nof relevant studies have dealt with Persian scripts. In this paper, deep neural\nnetworks are utilized through various DensNet architectures, as well as the\nXception, are adopted, modified and further boosted through data augmentation\nand test time augmentation, in order to come up with an optical character\nrecognition accounting for the particularities of the Persian language and the\ncorresponding handwritings. Taking advantage of dividing the databases to\ntraining, validation and test sets, as well as k-fold cross validation, the\ncomparison of the proposed method with various state-of-the-art alternatives is\nperformed on the basis of the HODA and Sadri databases, which offer the most\ncomprehensive collection of samples in terms of the various handwriting styles\npossessed by different human beings, as well as different forms each letter may\ntake, which depend on its position within a word. On the HODA database, we\nachieve recognition rates of 99.72% and 89.99% for digits and characters, being\n99.72%, 98.32% and 98.82% for digits, characters and words from the Sadri\ndatabase, respectively.\n", "versions": [{"version": "v1", "created": "Sat, 24 Oct 2020 11:42:28 GMT"}, {"version": "v2", "created": "Sat, 14 Nov 2020 06:20:53 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Bonyani", "Mehdi", ""], ["Jahangard", "Simindokht", ""], ["Daneshmand", "Morteza", ""]]}, {"id": "2010.12888", "submitter": "Sungho Suh", "authors": "Sungho Suh and Paul Lukowicz and Yong Oh Lee", "title": "Discriminative feature generation for classification of imbalanced data", "comments": "Submitted to Pattern Recognition, Elsevier", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The data imbalance problem is a frequent bottleneck in the classification\nperformance of neural networks. In this paper, we propose a novel supervised\ndiscriminative feature generation (DFG) method for a minority class dataset.\nDFG is based on the modified structure of a generative adversarial network\nconsisting of four independent networks: generator, discriminator, feature\nextractor, and classifier. To augment the selected discriminative features of\nthe minority class data by adopting an attention mechanism, the generator for\nthe class-imbalanced target task is trained, and the feature extractor and\nclassifier are regularized using the pre-trained features from a large source\ndata. The experimental results show that the DFG generator enhances the\naugmentation of the label-preserved and diverse features, and the\nclassification results are significantly improved on the target task. The\nfeature generation model can contribute greatly to the development of data\naugmentation methods through discriminative feature generation and supervised\nattention methods.\n", "versions": [{"version": "v1", "created": "Sat, 24 Oct 2020 12:19:05 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Suh", "Sungho", ""], ["Lukowicz", "Paul", ""], ["Lee", "Yong Oh", ""]]}, {"id": "2010.12913", "submitter": "Shafin Rahman", "authors": "Shafin Rahman, Sejuti Rahman, Omar Shahid, Md. Tahmeed Abdullah,\n  Jubair Ahmed Sourov", "title": "Classifying Eye-Tracking Data Using Saliency Maps", "comments": "Accepted in: International Conference on Pattern Recognition (ICPR)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A plethora of research in the literature shows how human eye fixation pattern\nvaries depending on different factors, including genetics, age, social\nfunctioning, cognitive functioning, and so on. Analysis of these variations in\nvisual attention has already elicited two potential research avenues: 1)\ndetermining the physiological or psychological state of the subject and 2)\npredicting the tasks associated with the act of viewing from the recorded\neye-fixation data. To this end, this paper proposes a visual saliency based\nnovel feature extraction method for automatic and quantitative classification\nof eye-tracking data, which is applicable to both of the research directions.\nInstead of directly extracting features from the fixation data, this method\nemploys several well-known computational models of visual attention to predict\neye fixation locations as saliency maps. Comparing the saliency amplitudes,\nsimilarity and dissimilarity of saliency maps with the corresponding eye\nfixations maps gives an extra dimension of information which is effectively\nutilized to generate discriminative features to classify the eye-tracking data.\nExtensive experimentation using Saliency4ASD, Age Prediction, and Visual\nPerceptual Task dataset show that our saliency-based feature can achieve\nsuperior performance, outperforming the previous state-of-the-art methods by a\nconsiderable margin. Moreover, unlike the existing application-specific\nsolutions, our method demonstrates performance improvement across three\ndistinct problems from the real-life domain: Autism Spectrum Disorder\nscreening, toddler age prediction, and human visual perceptual task\nclassification, providing a general paradigm that utilizes the\nextra-information inherent in saliency maps for a more accurate classification.\n", "versions": [{"version": "v1", "created": "Sat, 24 Oct 2020 15:18:07 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Rahman", "Shafin", ""], ["Rahman", "Sejuti", ""], ["Shahid", "Omar", ""], ["Abdullah", "Md. Tahmeed", ""], ["Sourov", "Jubair Ahmed", ""]]}, {"id": "2010.12917", "submitter": "Zan-Xia Jin", "authors": "Zan-Xia Jin, Heran Wu, Chun Yang, Fang Zhou, Jingyan Qin, Lei Xiao and\n  Xu-Cheng Yin", "title": "RUArt: A Novel Text-Centered Solution for Text-Based Visual Question\n  Answering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text-based visual question answering (VQA) requires to read and understand\ntext in an image to correctly answer a given question. However, most current\nmethods simply add optical character recognition (OCR) tokens extracted from\nthe image into the VQA model without considering contextual information of OCR\ntokens and mining the relationships between OCR tokens and scene objects. In\nthis paper, we propose a novel text-centered method called RUArt (Reading,\nUnderstanding and Answering the Related Text) for text-based VQA. Taking an\nimage and a question as input, RUArt first reads the image and obtains text and\nscene objects. Then, it understands the question, OCRed text and objects in the\ncontext of the scene, and further mines the relationships among them. Finally,\nit answers the related text for the given question through text semantic\nmatching and reasoning. We evaluate our RUArt on two text-based VQA benchmarks\n(ST-VQA and TextVQA) and conduct extensive ablation studies for exploring the\nreasons behind RUArt's effectiveness. Experimental results demonstrate that our\nmethod can effectively explore the contextual information of the text and mine\nthe stable relationships between the text and objects.\n", "versions": [{"version": "v1", "created": "Sat, 24 Oct 2020 15:37:09 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Jin", "Zan-Xia", ""], ["Wu", "Heran", ""], ["Yang", "Chun", ""], ["Zhou", "Fang", ""], ["Qin", "Jingyan", ""], ["Xiao", "Lei", ""], ["Yin", "Xu-Cheng", ""]]}, {"id": "2010.12921", "submitter": "Quanming Yao", "authors": "Wei He and Quanming Yao and Chao Li and Naoto Yokoya and Qibin Zhao\n  and Hongyan Zhang and Liangpei Zhang", "title": "Non-local Meets Global: An Iterative Paradigm for Hyperspectral Image\n  Restoration", "comments": "Accepted to TPAMI", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-local low-rank tensor approximation has been developed as a\nstate-of-the-art method for hyperspectral image (HSI) restoration, which\nincludes the tasks of denoising, compressed HSI reconstruction and inpainting.\nUnfortunately, while its restoration performance benefits from more spectral\nbands, its runtime also substantially increases. In this paper, we claim that\nthe HSI lies in a global spectral low-rank subspace, and the spectral subspaces\nof each full band patch group should lie in this global low-rank subspace. This\nmotivates us to propose a unified paradigm combining the spatial and spectral\nproperties for HSI restoration. The proposed paradigm enjoys performance\nsuperiority from the non-local spatial denoising and light computation\ncomplexity from the low-rank orthogonal basis exploration. An efficient\nalternating minimization algorithm with rank adaptation is developed. It is\ndone by first solving a fidelity term-related problem for the update of a\nlatent input image, and then learning a low-dimensional orthogonal basis and\nthe related reduced image from the latent input image. Subsequently, non-local\nlow-rank denoising is developed to refine the reduced image and orthogonal\nbasis iteratively. Finally, the experiments on HSI denoising, compressed\nreconstruction, and inpainting tasks, with both simulated and real datasets,\ndemonstrate its superiority with respect to state-of-the-art HSI restoration\nmethods.\n", "versions": [{"version": "v1", "created": "Sat, 24 Oct 2020 15:53:56 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["He", "Wei", ""], ["Yao", "Quanming", ""], ["Li", "Chao", ""], ["Yokoya", "Naoto", ""], ["Zhao", "Qibin", ""], ["Zhang", "Hongyan", ""], ["Zhang", "Liangpei", ""]]}, {"id": "2010.12932", "submitter": "Christine Allen-Blanchette", "authors": "Christine Allen-Blanchette, Sushant Veer, Anirudha Majumdar, Naomi\n  Ehrich Leonard", "title": "LagNetViP: A Lagrangian Neural Network for Video Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The dominant paradigms for video prediction rely on opaque transition models\nwhere neither the equations of motion nor the underlying physical quantities of\nthe system are easily inferred. The equations of motion, as defined by Newton's\nsecond law, describe the time evolution of a physical system state and can\ntherefore be applied toward the determination of future system states. In this\npaper, we introduce a video prediction model where the equations of motion are\nexplicitly constructed from learned representations of the underlying physical\nquantities. To achieve this, we simultaneously learn a low-dimensional state\nrepresentation and system Lagrangian. The kinetic and potential energy terms of\nthe Lagrangian are distinctly modelled and the low-dimensional equations of\nmotion are explicitly constructed using the Euler-Lagrange equations. We\ndemonstrate the efficacy of this approach for video prediction on image\nsequences rendered in modified OpenAI gym Pendulum-v0 and Acrobot environments.\n", "versions": [{"version": "v1", "created": "Sat, 24 Oct 2020 16:50:14 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Allen-Blanchette", "Christine", ""], ["Veer", "Sushant", ""], ["Majumdar", "Anirudha", ""], ["Leonard", "Naomi Ehrich", ""]]}, {"id": "2010.12949", "submitter": "Daniel McDuff", "authors": "Daniel McDuff, Javier Hernandez, Erroll Wood, Xin Liu, Tadas\n  Baltrusaitis", "title": "Advancing Non-Contact Vital Sign Measurement using Synthetic Avatars", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-contact physiological measurement has the potential to provide low-cost,\nnon-invasive health monitoring. However, machine vision approaches are often\nlimited by the availability and diversity of annotated video datasets resulting\nin poor generalization to complex real-life conditions. To address these\nchallenges, this work proposes the use of synthetic avatars that display facial\nblood flow changes and allow for systematic generation of samples under a wide\nvariety of conditions. Our results show that training on both simulated and\nreal video data can lead to performance gains under challenging conditions. We\nshow state-of-the-art performance on three large benchmark datasets and\nimproved robustness to skin type and motion.\n", "versions": [{"version": "v1", "created": "Sat, 24 Oct 2020 18:31:57 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["McDuff", "Daniel", ""], ["Hernandez", "Javier", ""], ["Wood", "Erroll", ""], ["Liu", "Xin", ""], ["Baltrusaitis", "Tadas", ""]]}, {"id": "2010.12967", "submitter": "Dor Amran", "authors": "Dor Amran, Maayan Frid-Adar, Nimrod Sagie, Jannette Nassar, Asher\n  Kabakovitch, Hayit Greenspan", "title": "Automated triage of COVID-19 from various lung abnormalities using chest\n  CT features", "comments": "This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The outbreak of COVID-19 has lead to a global effort to decelerate the\npandemic spread. For this purpose chest computed-tomography (CT) based\nscreening and diagnosis of COVID-19 suspected patients is utilized, either as a\nsupport or replacement to reverse transcription-polymerase chain reaction\n(RT-PCR) test. In this paper, we propose a fully automated AI based system that\ntakes as input chest CT scans and triages COVID-19 cases. More specifically, we\nproduce multiple descriptive features, including lung and infections\nstatistics, texture, shape and location, to train a machine learning based\nclassifier that distinguishes between COVID-19 and other lung abnormalities\n(including community acquired pneumonia). We evaluated our system on a dataset\nof 2191 CT cases and demonstrated a robust solution with 90.8% sensitivity at\n85.4% specificity with 94.0% ROC-AUC. In addition, we present an elaborated\nfeature analysis and ablation study to explore the importance of each feature.\n", "versions": [{"version": "v1", "created": "Sat, 24 Oct 2020 19:44:48 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Amran", "Dor", ""], ["Frid-Adar", "Maayan", ""], ["Sagie", "Nimrod", ""], ["Nassar", "Jannette", ""], ["Kabakovitch", "Asher", ""], ["Greenspan", "Hayit", ""]]}, {"id": "2010.12968", "submitter": "Zijian Kuang", "authors": "Zijian Kuang and Xinran Tie", "title": "Improved Actor Relation Graph based Group Activity Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video understanding is to recognize and classify different actions or\nactivities appearing in the video. A lot of previous work, such as video\ncaptioning, has shown promising performance in producing general video\nunderstanding. However, it is still challenging to generate a fine-grained\ndescription of human actions and their interactions using state-of-the-art\nvideo captioning techniques. The detailed description of human actions and\ngroup activities is essential information, which can be used in real-time CCTV\nvideo surveillance, health care, sports video analysis, etc. This study\nproposes a video understanding method that mainly focused on group activity\nrecognition by learning the pair-wise actor appearance similarity and actor\npositions. We propose to use Normalized cross-correlation (NCC) and the sum of\nabsolute differences (SAD) to calculate the pair-wise appearance similarity and\nbuild the actor relationship graph to allow the graph convolution network to\nlearn how to classify group activities. We also propose to use MobileNet as the\nbackbone to extract features from each video frame. A visualization model is\nfurther introduced to visualize each input video frame with predicted bounding\nboxes on each human object and predict individual action and collective\nactivity.\n", "versions": [{"version": "v1", "created": "Sat, 24 Oct 2020 19:46:49 GMT"}, {"version": "v2", "created": "Tue, 29 Dec 2020 16:56:54 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Kuang", "Zijian", ""], ["Tie", "Xinran", ""]]}, {"id": "2010.12970", "submitter": "Sreyas Mohan", "authors": "Sreyas Mohan, Ramon Manzorro, Joshua L. Vincent, Binh Tang, Dev\n  Yashpal Sheth, Eero P. Simoncelli, David S. Matteson, Peter A. Crozier,\n  Carlos Fernandez-Granda", "title": "Deep Denoising For Scientific Discovery: A Case Study In Electron\n  Microscopy", "comments": "The dataset and the code used to train and evaluate and our models\n  are available at\n  https://sreyas-mohan.github.io/electron-microscopy-denoising/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Denoising is a fundamental challenge in scientific imaging. Deep\nconvolutional neural networks (CNNs) provide the current state of the art in\ndenoising natural images, where they produce impressive results. However, their\npotential has barely been explored in the context of scientific imaging.\nDenoising CNNs are typically trained on real natural images artificially\ncorrupted with simulated noise. In contrast, in scientific applications,\nnoiseless ground-truth images are usually not available. To address this issue,\nwe propose a simulation-based denoising (SBD) framework, in which CNNs are\ntrained on simulated images. We test the framework on data obtained from\ntransmission electron microscopy (TEM), an imaging technique with widespread\napplications in material science, biology, and medicine. SBD outperforms\nexisting techniques by a wide margin on a simulated benchmark dataset, as well\nas on real data. Apart from the denoised images, SBD generates likelihood maps\nto visualize the agreement between the structure of the denoised image and the\nobserved data. Our results reveal shortcomings of state-of-the-art denoising\narchitectures, such as their small field-of-view: substantially increasing the\nfield-of-view of the CNNs allows them to exploit non-local periodic patterns in\nthe data, which is crucial at high noise levels. In addition, we analyze the\ngeneralization capability of SBD, demonstrating that the trained networks are\nrobust to variations of imaging parameters and of the underlying signal\nstructure. Finally, we release the first publicly available benchmark dataset\nof TEM images, containing 18,000 examples.\n", "versions": [{"version": "v1", "created": "Sat, 24 Oct 2020 19:59:28 GMT"}, {"version": "v2", "created": "Tue, 13 Jul 2021 15:12:33 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Mohan", "Sreyas", ""], ["Manzorro", "Ramon", ""], ["Vincent", "Joshua L.", ""], ["Tang", "Binh", ""], ["Sheth", "Dev Yashpal", ""], ["Simoncelli", "Eero P.", ""], ["Matteson", "David S.", ""], ["Crozier", "Peter A.", ""], ["Fernandez-Granda", "Carlos", ""]]}, {"id": "2010.12976", "submitter": "Linh K\\\"astner", "authors": "Linh K\\\"astner, Samim Ahmadi, Florian Jonietz, Mathias Ziegler, Peter\n  Jung, Giuseppe Caire and Jens Lambrecht", "title": "Classification of Spot-welded Joints in Laser Thermography Data using\n  Convolutional Neural Networks", "comments": "9 pages,11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spot welding is a crucial process step in various industries. However,\nclassification of spot welding quality is still a tedious process due to the\ncomplexity and sensitivity of the test material, which drain conventional\napproaches to its limits. In this paper, we propose an approach for quality\ninspection of spot weldings using images from laser thermography data.We\npropose data preparation approaches based on the underlying physics of spot\nwelded joints, heated with pulsed laser thermography by analyzing the intensity\nover time and derive dedicated data filters to generate training datasets.\nSubsequently, we utilize convolutional neural networks to classify weld quality\nand compare the performance of different models against each other. We achieve\ncompetitive results in terms of classifying the different welding quality\nclasses compared to traditional approaches, reaching an accuracy of more than\n95 percent. Finally, we explore the effect of different augmentation methods.\n", "versions": [{"version": "v1", "created": "Sat, 24 Oct 2020 20:38:12 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["K\u00e4stner", "Linh", ""], ["Ahmadi", "Samim", ""], ["Jonietz", "Florian", ""], ["Ziegler", "Mathias", ""], ["Jung", "Peter", ""], ["Caire", "Giuseppe", ""], ["Lambrecht", "Jens", ""]]}, {"id": "2010.13017", "submitter": "Jiangning Zhang", "authors": "Jiangning Zhang, Xianfang Zeng, Chao Xu, Jun Chen, Yong Liu, Yunliang\n  Jiang", "title": "APB2FaceV2: Real-Time Audio-Guided Multi-Face Reenactment", "comments": "ICASSP'21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Audio-guided face reenactment aims to generate a photorealistic face that has\nmatched facial expression with the input audio. However, current methods can\nonly reenact a special person once the model is trained or need extra\noperations such as 3D rendering and image post-fusion on the premise of\ngenerating vivid faces. To solve the above challenge, we propose a novel\n\\emph{R}eal-time \\emph{A}udio-guided \\emph{M}ulti-face reenactment approach\nnamed \\emph{APB2FaceV2}, which can reenact different target faces among\nmultiple persons with corresponding reference face and drive audio signal as\ninputs. Enabling the model to be trained end-to-end and have a faster speed, we\ndesign a novel module named Adaptive Convolution (AdaConv) to infuse audio\ninformation into the network, as well as adopt a lightweight network as our\nbackbone so that the network can run in real time on CPU and GPU. Comparison\nexperiments prove the superiority of our approach than existing\nstate-of-the-art methods, and further experiments demonstrate that our method\nis efficient and flexible for practical applications\nhttps://github.com/zhangzjn/APB2FaceV2\n", "versions": [{"version": "v1", "created": "Sun, 25 Oct 2020 02:30:09 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Zhang", "Jiangning", ""], ["Zeng", "Xianfang", ""], ["Xu", "Chao", ""], ["Chen", "Jun", ""], ["Liu", "Yong", ""], ["Jiang", "Yunliang", ""]]}, {"id": "2010.13046", "submitter": "Louise Gillian Bautista", "authors": "Louise Gillian C. Bautista and Prospero C. Naval Jr", "title": "CLRGaze: Contrastive Learning of Representations for Eye Movement\n  Signals", "comments": "Accepted to 29th European Signal Processing Conference (EUSIPCO 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Eye movements are intricate and dynamic biosignals that contain a wealth of\ncognitive information about the subject. However, these are ambiguous signals\nand therefore require meticulous feature engineering to be used by machine\nlearning algorithms. We instead propose to learn feature vectors of eye\nmovements in a self-supervised manner. We adopt a contrastive learning approach\nand propose a set of data transformations that encourage a deep neural network\nto discern salient and granular gaze patterns. This paper presents a novel\nexperiment utilizing six eye-tracking data sets despite different data\nspecifications and experimental conditions. We assess the learned features on\nbiometric tasks with only a linear classifier, achieving 84.6% accuracy on a\nmixed dataset, and up to 97.3% accuracy on a single dataset. Our work advances\nthe state of machine learning for eye movements and provides insights into a\ngeneral representation learning method not only for eye movements but also for\nsimilar biosignals.\n", "versions": [{"version": "v1", "created": "Sun, 25 Oct 2020 06:12:06 GMT"}, {"version": "v2", "created": "Sun, 30 May 2021 14:14:23 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Bautista", "Louise Gillian C.", ""], ["Naval", "Prospero C.", "Jr"]]}, {"id": "2010.13054", "submitter": "Johan Boetker", "authors": "Johan P. Boetker", "title": "Applying convolutional neural networks to extremely sparse image\n  datasets using an image subdivision approach", "comments": "10 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Purpose: The aim of this work is to demonstrate that convolutional neural\nnetworks (CNN) can be applied to extremely sparse image libraries by\nsubdivision of the original image datasets. Methods: Image datasets from a\nconventional digital camera was created and scanning electron microscopy (SEM)\nmeasurements were obtained from the literature. The image datasets were\nsubdivided and CNN models were trained on parts of the subdivided datasets.\nResults: The CNN models were capable of analyzing extremely sparse image\ndatasets by utilizing the proposed method of image subdivision. It was\nfurthermore possible to provide a direct assessment of the various regions\nwhere a given API or appearance was predominant.\n", "versions": [{"version": "v1", "created": "Sun, 25 Oct 2020 07:43:20 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Boetker", "Johan P.", ""]]}, {"id": "2010.13070", "submitter": "Asaf Shabtai", "authors": "Shahar Hoory and Tzvika Shapira and Asaf Shabtai and Yuval Elovici", "title": "Dynamic Adversarial Patch for Evading Object Detection Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent research shows that neural networks models used for computer vision\n(e.g., YOLO and Fast R-CNN) are vulnerable to adversarial evasion attacks. Most\nof the existing real-world adversarial attacks against object detectors use an\nadversarial patch which is attached to the target object (e.g., a carefully\ncrafted sticker placed on a stop sign). This method may not be robust to\nchanges in the camera's location relative to the target object; in addition, it\nmay not work well when applied to nonplanar objects such as cars. In this\nstudy, we present an innovative attack method against object detectors applied\nin a real-world setup that addresses some of the limitations of existing\nattacks. Our method uses dynamic adversarial patches which are placed at\nmultiple predetermined locations on a target object. An adversarial learning\nalgorithm is applied in order to generate the patches used. The dynamic attack\nis implemented by switching between optimized patches dynamically, according to\nthe camera's position (i.e., the object detection system's position). In order\nto demonstrate our attack in a real-world setup, we implemented the patches by\nattaching flat screens to the target object; the screens are used to present\nthe patches and switch between them, depending on the current camera location.\nThus, the attack is dynamic and adjusts itself to the situation to achieve\noptimal results. We evaluated our dynamic patch approach by attacking the\nYOLOv2 object detector with a car as the target object and succeeded in\nmisleading it in up to 90% of the video frames when filming the car from a wide\nviewing angle range. We improved the attack by generating patches that consider\nthe semantic distance between the target object and its classification. We also\nexamined the attack's transferability among different car models and were able\nto mislead the detector 71% of the time.\n", "versions": [{"version": "v1", "created": "Sun, 25 Oct 2020 08:55:40 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Hoory", "Shahar", ""], ["Shapira", "Tzvika", ""], ["Shabtai", "Asaf", ""], ["Elovici", "Yuval", ""]]}, {"id": "2010.13073", "submitter": "Chamira Edussooriya", "authors": "Sahan Hemachandra, Ranga Rodrigo, Chamira Edussooriya", "title": "Fast and Accurate Light Field Saliency Detection through Feature\n  Extraction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Light field saliency detection---important due to utility in many vision\ntasks---still lack speed and can improve in accuracy. Due to the formulation of\nthe saliency detection problem in light fields as a segmentation task or a\n\"memorizing\" tasks, existing approaches consume unnecessarily large amounts of\ncomputational resources for (training and) testing leading to execution times\nis several seconds. We solve this by aggressively reducing the large\nlight-field images to a much smaller three-channel feature map appropriate for\nsaliency detection using an RGB image saliency detector. We achieve this by\nintroducing a novel convolutional neural network based features extraction and\nencoding module. Our saliency detector takes $0.4$ s to process a light field\nof size $9\\times9\\times512\\times375$ in a CPU and is significantly faster than\nexisting systems, with better or comparable accuracy. Our work shows that\nextracting features from light fields through aggressive size reduction and the\nattention results in a faster and accurate light-field saliency detector.\n", "versions": [{"version": "v1", "created": "Sun, 25 Oct 2020 09:15:08 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Hemachandra", "Sahan", ""], ["Rodrigo", "Ranga", ""], ["Edussooriya", "Chamira", ""]]}, {"id": "2010.13082", "submitter": "Parvez Ahmad", "authors": "Parvez Ahmad, Saqib Qamar, Linlin Shen, Adnan Saeed", "title": "Context Aware 3D UNet for Brain Tumor Segmentation", "comments": "Accepted for MICCAI 2020 Brain Lesions (BrainLes) Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep convolutional neural network (CNN) achieves remarkable performance for\nmedical image analysis. UNet is the primary source in the performance of 3D CNN\narchitectures for medical imaging tasks, including brain tumor segmentation.\nThe skip connection in the UNet architecture concatenates features from both\nencoder and decoder paths to extract multi-contextual information from image\ndata. The multi-scaled features play an essential role in brain tumor\nsegmentation. However, the limited use of features can degrade the performance\nof the UNet approach for segmentation. In this paper, we propose a modified\nUNet architecture for brain tumor segmentation. In the proposed architecture,\nwe used densely connected blocks in both encoder and decoder paths to extract\nmulti-contextual information from the concept of feature reusability. In\naddition, residual-inception blocks (RIB) are used to extract the local and\nglobal information by merging features of different kernel sizes. We validate\nthe proposed architecture on the multi-modal brain tumor segmentation challenge\n(BRATS) 2020 testing dataset. The dice (DSC) scores of the whole tumor (WT),\ntumor core (TC), and enhancing tumor (ET) are 89.12%, 84.74%, and 79.12%,\nrespectively.\n", "versions": [{"version": "v1", "created": "Sun, 25 Oct 2020 10:32:25 GMT"}, {"version": "v2", "created": "Fri, 27 Nov 2020 13:57:26 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Ahmad", "Parvez", ""], ["Qamar", "Saqib", ""], ["Shen", "Linlin", ""], ["Saeed", "Adnan", ""]]}, {"id": "2010.13085", "submitter": "Mingyang Qian", "authors": "Mingyang Qian, Yi Fu, Xiao Tan, Yingying Li, Jinqing Qi, Huchuan Lu,\n  Shilei Wen, Errui Ding", "title": "Coherent Loss: A Generic Framework for Stable Video Segmentation", "comments": "10 pages, 6 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video segmentation approaches are of great importance for numerous vision\ntasks especially in video manipulation for entertainment. Due to the challenges\nassociated with acquiring high-quality per-frame segmentation annotations and\nlarge video datasets with different environments at scale, learning approaches\nshows overall higher accuracy on test dataset but lack strict temporal\nconstraints to self-correct jittering artifacts in most practical applications.\nWe investigate how this jittering artifact degrades the visual quality of video\nsegmentation results and proposed a metric of temporal stability to numerically\nevaluate it. In particular, we propose a Coherent Loss with a generic framework\nto enhance the performance of a neural network against jittering artifacts,\nwhich combines with high accuracy and high consistency. Equipped with our\nmethod, existing video object/semantic segmentation approaches achieve a\nsignificant improvement in term of more satisfactory visual quality on video\nhuman dataset, which we provide for further research in this field, and also on\nDAVIS and Cityscape.\n", "versions": [{"version": "v1", "created": "Sun, 25 Oct 2020 10:48:28 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Qian", "Mingyang", ""], ["Fu", "Yi", ""], ["Tan", "Xiao", ""], ["Li", "Yingying", ""], ["Qi", "Jinqing", ""], ["Lu", "Huchuan", ""], ["Wen", "Shilei", ""], ["Ding", "Errui", ""]]}, {"id": "2010.13106", "submitter": "Yao Wei", "authors": "Yao Wei, Shunping Ji", "title": "Scribble-based Weakly Supervised Deep Learning for Road Surface\n  Extraction from Remote Sensing Images", "comments": "12 pages, 8 figures, submitted to IEEE Transactions on Geoscience and\n  Remote Sensing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Road surface extraction from remote sensing images using deep learning\nmethods has achieved good performance, while most of the existing methods are\nbased on fully supervised learning, which requires a large amount of training\ndata with laborious per-pixel annotation. In this paper, we propose a\nscribble-based weakly supervised road surface extraction method named\nScRoadExtractor, which learns from easily accessible scribbles such as\ncenterlines instead of densely annotated road surface ground-truths. To\npropagate semantic information from sparse scribbles to unlabeled pixels, we\nintroduce a road label propagation algorithm which considers both the\nbuffer-based properties of road networks and the color and spatial information\nof super-pixels. The proposal masks generated from the road label propagation\nalgorithm are utilized to train a dual-branch encoder-decoder network we\ndesigned, which consists of a semantic segmentation branch and an auxiliary\nboundary detection branch. We perform experiments on three diverse road\ndatasets that are comprised of highresolution remote sensing satellite and\naerial images across the world. The results demonstrate that ScRoadExtractor\nexceed the classic scribble-supervised segmentation method by 20% for the\nintersection over union (IoU) indicator and outperform the state-of-the-art\nscribble-based weakly supervised methods at least 4%.\n", "versions": [{"version": "v1", "created": "Sun, 25 Oct 2020 12:40:30 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Wei", "Yao", ""], ["Ji", "Shunping", ""]]}, {"id": "2010.13114", "submitter": "Saurabh Kumar", "authors": "Ayush Bhardwaj, Sakshee Pimpale, Saurabh Kumar, Biplab Banerjee", "title": "Empowering Knowledge Distillation via Open Set Recognition for Robust 3D\n  Point Cloud Classification", "comments": "Preprint. Under consideration at Pattern Recognition Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-world scenarios pose several challenges to deep learning based computer\nvision techniques despite their tremendous success in research. Deeper models\nprovide better performance, but are challenging to deploy and knowledge\ndistillation allows us to train smaller models with minimal loss in\nperformance. The model also has to deal with open set samples from classes\noutside the ones it was trained on and should be able to identify them as\nunknown samples while classifying the known ones correctly. Finally, most\nexisting image recognition research focuses only on using two-dimensional\nsnapshots of the real world three-dimensional objects. In this work, we aim to\nbridge these three research fields, which have been developed independently\nuntil now, despite being deeply interrelated. We propose a joint Knowledge\nDistillation and Open Set recognition training methodology for\nthree-dimensional object recognition. We demonstrate the effectiveness of the\nproposed method via various experiments on how it allows us to obtain a much\nsmaller model, which takes a minimal hit in performance while being capable of\nopen set recognition for 3D point cloud data.\n", "versions": [{"version": "v1", "created": "Sun, 25 Oct 2020 13:26:48 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Bhardwaj", "Ayush", ""], ["Pimpale", "Sakshee", ""], ["Kumar", "Saurabh", ""], ["Banerjee", "Biplab", ""]]}, {"id": "2010.13118", "submitter": "Julian Lienen", "authors": "Julian Lienen, Eyke H\\\"ullermeier, Ralph Ewerth, Nils Nommensen", "title": "Monocular Depth Estimation via Listwise Ranking using the Plackett-Luce\n  Model", "comments": "15 pages, 5 figures, 7 tables, IEEE/CVF Conference on Computer Vision\n  and Pattern Recognition (CVPR) 2021", "journal-ref": "Proceedings of the IEEE/CVF Conference on Computer Vision and\n  Pattern Recognition (CVPR), 2021, pp. 14595-14604", "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In many real-world applications, the relative depth of objects in an image is\ncrucial for scene understanding. Recent approaches mainly tackle the problem of\ndepth prediction in monocular images by treating the problem as a regression\ntask. Yet, being interested in an order relation in the first place, ranking\nmethods suggest themselves as a natural alternative to regression, and indeed,\nranking approaches leveraging pairwise comparisons as training information\n(\"object A is closer to the camera than B\") have shown promising performance on\nthis problem. In this paper, we elaborate on the use of so-called listwise\nranking as a generalization of the pairwise approach. Our method is based on\nthe Plackett-Luce (PL) model, a probability distribution on rankings, which we\ncombine with a state-of-the-art neural network architecture and a simple\nsampling strategy to reduce training complexity. Moreover, taking advantage of\nthe representation of PL as a random utility model, the proposed predictor\noffers a natural way to recover (shift-invariant) metric depth information from\nranking-only data provided at training time. An empirical evaluation on several\nbenchmark datasets in a \"zero-shot\" setting demonstrates the effectiveness of\nour approach compared to existing ranking and regression methods.\n", "versions": [{"version": "v1", "created": "Sun, 25 Oct 2020 13:40:10 GMT"}, {"version": "v2", "created": "Wed, 28 Oct 2020 09:37:54 GMT"}, {"version": "v3", "created": "Sat, 31 Oct 2020 15:14:37 GMT"}, {"version": "v4", "created": "Wed, 7 Jul 2021 07:43:54 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Lienen", "Julian", ""], ["H\u00fcllermeier", "Eyke", ""], ["Ewerth", "Ralph", ""], ["Nommensen", "Nils", ""]]}, {"id": "2010.13136", "submitter": "Riccardo Marin", "authors": "Riccardo Marin, Marie-Julie Rakotosaona, Simone Melzi, Maks Ovsjanikov", "title": "Correspondence Learning via Linearly-invariant Embedding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CG cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a fully differentiable pipeline for estimating\naccurate dense correspondences between 3D point clouds. The proposed pipeline\nis an extension and a generalization of the functional maps framework. However,\ninstead of using the Laplace-Beltrami eigenfunctions as done in virtually all\nprevious works in this domain, we demonstrate that learning the basis from data\ncan both improve robustness and lead to better accuracy in challenging\nsettings. We interpret the basis as a learned embedding into a higher\ndimensional space. Following the functional map paradigm the optimal\ntransformation in this embedding space must be linear and we propose a separate\narchitecture aimed at estimating the transformation by learning optimal\ndescriptor functions. This leads to the first end-to-end trainable functional\nmap-based correspondence approach in which both the basis and the descriptors\nare learned from data. Interestingly, we also observe that learning a\n\\emph{canonical} embedding leads to worse results, suggesting that leaving an\nextra linear degree of freedom to the embedding network gives it more\nrobustness, thereby also shedding light onto the success of previous methods.\nFinally, we demonstrate that our approach achieves state-of-the-art results in\nchallenging non-rigid 3D point cloud correspondence applications.\n", "versions": [{"version": "v1", "created": "Sun, 25 Oct 2020 15:31:53 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Marin", "Riccardo", ""], ["Rakotosaona", "Marie-Julie", ""], ["Melzi", "Simone", ""], ["Ovsjanikov", "Maks", ""]]}, {"id": "2010.13160", "submitter": "Woojeong Kim", "authors": "Woojeong Kim, Suhyun Kim, Mincheol Park, Geonseok Jeon", "title": "Neuron Merging: Compensating for Pruned Neurons", "comments": "NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network pruning is widely used to lighten and accelerate neural network\nmodels. Structured network pruning discards the whole neuron or filter, leading\nto accuracy loss. In this work, we propose a novel concept of neuron merging\napplicable to both fully connected layers and convolution layers, which\ncompensates for the information loss due to the pruned neurons/filters. Neuron\nmerging starts with decomposing the original weights into two matrices/tensors.\nOne of them becomes the new weights for the current layer, and the other is\nwhat we name a scaling matrix, guiding the combination of neurons. If the\nactivation function is ReLU, the scaling matrix can be absorbed into the next\nlayer under certain conditions, compensating for the removed neurons. We also\npropose a data-free and inexpensive method to decompose the weights by\nutilizing the cosine similarity between neurons. Compared to the pruned model\nwith the same topology, our merged model better preserves the output feature\nmap of the original model; thus, it maintains the accuracy after pruning\nwithout fine-tuning. We demonstrate the effectiveness of our approach over\nnetwork pruning for various model architectures and datasets. As an example,\nfor VGG-16 on CIFAR-10, we achieve an accuracy of 93.16% while reducing 64% of\ntotal parameters, without any fine-tuning. The code can be found here:\nhttps://github.com/friendshipkim/neuron-merging\n", "versions": [{"version": "v1", "created": "Sun, 25 Oct 2020 16:50:26 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Kim", "Woojeong", ""], ["Kim", "Suhyun", ""], ["Park", "Mincheol", ""], ["Jeon", "Geonseok", ""]]}, {"id": "2010.13172", "submitter": "J\\\"org Sander", "authors": "J\\\"org Sander, Bob D. de Vos and Ivana I\\v{s}gum", "title": "Unsupervised Super-Resolution: Creating High-Resolution Medical Images\n  from Low-Resolution Anisotropic Examples", "comments": "accepted at the SPIE Medical Imaging 2021 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although high resolution isotropic 3D medical images are desired in clinical\npractice, their acquisition is not always feasible. Instead, lower resolution\nimages are upsampled to higher resolution using conventional interpolation\nmethods. Sophisticated learning-based super-resolution approaches are\nfrequently unavailable in clinical setting, because such methods require\ntraining with high-resolution isotropic examples. To address this issue, we\npropose a learning-based super-resolution approach that can be trained using\nsolely anisotropic images, i.e. without high-resolution ground truth data. The\nmethod exploits the latent space, generated by autoencoders trained on\nanisotropic images, to increase spatial resolution in low-resolution images.\nThe method was trained and evaluated using 100 publicly available cardiac cine\nMR scans from the Automated Cardiac Diagnosis Challenge (ACDC). The\nquantitative results show that the proposed method performs better than\nconventional interpolation methods. Furthermore, the qualitative results\nindicate that especially finer cardiac structures are synthesized with high\nquality. The method has the potential to be applied to other anatomies and\nmodalities and can be easily applied to any 3D anisotropic medical image\ndataset.\n", "versions": [{"version": "v1", "created": "Sun, 25 Oct 2020 17:41:54 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Sander", "J\u00f6rg", ""], ["de Vos", "Bob D.", ""], ["I\u0161gum", "Ivana", ""]]}, {"id": "2010.13175", "submitter": "Yihong Sun", "authors": "Yihong Sun, Adam Kortylewski, Alan Yuille", "title": "Weakly-Supervised Amodal Instance Segmentation with Compositional Priors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Amodal segmentation in biological vision refers to the perception of the\nentire object when only a fraction is visible. This ability of seeing through\noccluders and reasoning about occlusion is innate to biological vision but not\nadequately modeled in current machine vision approaches. A key challenge is\nthat ground-truth supervisions of amodal object segmentation are inherently\ndifficult to obtain. In this paper, we present a neural network architecture\nthat is capable of amodal perception, when weakly supervised with standard\n(modal) bounding box annotations. Our model extends compositional convolutional\nneural networks (CompositionalNets), which have been shown to be robust to\npartial occlusion by explicitly representing objects as a composition of parts.\nIn particular, we extend CompositionalNets to perform three new vision tasks\nfrom bounding box supervision only: 1) Learning compositional shape priors of\nobjects in varying 3D poses from modal bounding box supervision; 2) Predicting\ninstance segmentation by integrating the compositional shape priors into the\npart-voting mechanism in the CompositionalNets; 3) Predicting amodal completion\nfor both the bounding box and the instance segmentation mask by implementing\ncompositional feature alignment in CompositionalNets. Our extensive experiments\nshow that our proposed model can segment amodal masks robustly, with much\nimproved mask prediction qualities compared to state-of-the-art segmentation\napproaches.\n", "versions": [{"version": "v1", "created": "Sun, 25 Oct 2020 18:01:26 GMT"}, {"version": "v2", "created": "Thu, 7 Jan 2021 18:20:38 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Sun", "Yihong", ""], ["Kortylewski", "Adam", ""], ["Yuille", "Alan", ""]]}, {"id": "2010.13187", "submitter": "Akash Srivastava", "authors": "Akash Srivastava, Yamini Bansal, Yukun Ding, Cole Hurwitz, Kai Xu,\n  Bernhard Egger, Prasanna Sattigeri, Josh Tenenbaum, David D. Cox, Dan\n  Gutfreund", "title": "Improving the Reconstruction of Disentangled Representation Learners via\n  Multi-Stage Modelling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current autoencoder-based disentangled representation learning methods\nachieve disentanglement by penalizing the (aggregate) posterior to encourage\nstatistical independence of the latent factors. This approach introduces a\ntrade-off between disentangled representation learning and reconstruction\nquality since the model does not have enough capacity to learn correlated\nlatent variables that capture detail information present in most image data. To\novercome this trade-off, we present a novel multi-stage modelling approach\nwhere the disentangled factors are first learned using a preexisting\ndisentangled representation learning method (such as $\\beta$-TCVAE); then, the\nlow-quality reconstruction is improved with another deep generative model that\nis trained to model the missing correlated latent variables, adding detail\ninformation while maintaining conditioning on the previously learned\ndisentangled factors. Taken together, our multi-stage modelling approach\nresults in a single, coherent probabilistic model that is theoretically\njustified by the principal of D-separation and can be realized with a variety\nof model classes including likelihood-based models such as variational\nautoencoders, implicit models such as generative adversarial networks, and\ntractable models like normalizing flows or mixtures of Gaussians. We\ndemonstrate that our multi-stage model has much higher reconstruction quality\nthan current state-of-the-art methods with equivalent disentanglement\nperformance across multiple standard benchmarks.\n", "versions": [{"version": "v1", "created": "Sun, 25 Oct 2020 18:51:15 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Srivastava", "Akash", ""], ["Bansal", "Yamini", ""], ["Ding", "Yukun", ""], ["Hurwitz", "Cole", ""], ["Xu", "Kai", ""], ["Egger", "Bernhard", ""], ["Sattigeri", "Prasanna", ""], ["Tenenbaum", "Josh", ""], ["Cox", "David D.", ""], ["Gutfreund", "Dan", ""]]}, {"id": "2010.13197", "submitter": "Sriram Krishna", "authors": "Sriram Krishna, Nishant Sinha", "title": "Gestop : Customizable Gesture Control of Computer Systems", "comments": "5 pages, 5 figures, to appear in the proceedings of the 8th ACM IKDD\n  CODS and 26th COMAD (CODS-COMAD '21)", "journal-ref": null, "doi": "10.1145/3430984.3430993", "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The established way of interfacing with most computer systems is a mouse and\nkeyboard. Hand gestures are an intuitive and effective touchless way to\ninteract with computer systems. However, hand gesture based systems have seen\nlow adoption among end-users primarily due to numerous technical hurdles in\ndetecting in-air gestures accurately. This paper presents Gestop, a framework\ndeveloped to bridge this gap. The framework learns to detect gestures from\ndemonstrations, is customizable by end-users and enables users to interact in\nreal-time with computers having only RGB cameras, using gestures.\n", "versions": [{"version": "v1", "created": "Sun, 25 Oct 2020 19:13:01 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Krishna", "Sriram", ""], ["Sinha", "Nishant", ""]]}, {"id": "2010.13214", "submitter": "Ruangrawee Kitichotkul", "authors": "Ruangrawee Kitichotkul, Christopher A. Metzler, Frank Ong, Gordon\n  Wetzstein", "title": "SUREMap: Predicting Uncertainty in CNN-based Image Reconstruction Using\n  Stein's Unbiased Risk Estimate", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNN) have emerged as a powerful tool for\nsolving computational imaging reconstruction problems. However, CNNs are\ngenerally difficult-to-understand black-boxes. Accordingly, it is challenging\nto know when they will work and, more importantly, when they will fail. This\nlimitation is a major barrier to their use in safety-critical applications like\nmedical imaging: Is that blob in the reconstruction an artifact or a tumor?\n  In this work we use Stein's unbiased risk estimate (SURE) to develop\nper-pixel confidence intervals, in the form of heatmaps, for compressive\nsensing reconstruction using the approximate message passing (AMP) framework\nwith CNN-based denoisers. These heatmaps tell end-users how much to trust an\nimage formed by a CNN, which could greatly improve the utility of CNNs in\nvarious computational imaging applications.\n", "versions": [{"version": "v1", "created": "Sun, 25 Oct 2020 20:29:41 GMT"}, {"version": "v2", "created": "Tue, 20 Apr 2021 03:01:14 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Kitichotkul", "Ruangrawee", ""], ["Metzler", "Christopher A.", ""], ["Ong", "Frank", ""], ["Wetzstein", "Gordon", ""]]}, {"id": "2010.13231", "submitter": "Luis Leiva", "authors": "Luis A. Leiva and Moises Diaz and Miguel A. Ferrer and R\\'ejean\n  Plamondon", "title": "Human or Machine? It Is Not What You Write, But How You Write It", "comments": null, "journal-ref": "Proceedings of the 25th Intl. Conf. on Pattern Recognition (ICPR),\n  2020", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online fraud often involves identity theft. Since most security measures are\nweak or can be spoofed, we investigate a more nuanced and less explored avenue:\nbehavioral biometrics via handwriting movements. This kind of data can be used\nto verify whether a user is operating a device or a computer application, so it\nis important to distinguish between human and machine-generated movements\nreliably. For this purpose, we study handwritten symbols (isolated characters,\ndigits, gestures, and signatures) produced by humans and machines, and compare\nand contrast several deep learning models. We find that if symbols are\npresented as static images, they can fool state-of-the-art classifiers (near\n75% accuracy in the best case) but can be distinguished with remarkable\naccuracy if they are presented as temporal sequences (95% accuracy in the\naverage case). We conclude that an accurate detection of fake movements has\nmore to do with how users write, rather than what they write. Our work has\nimplications for computerized systems that need to authenticate or verify\nlegitimate human users, and provides an additional layer of security to keep\nattackers at bay.\n", "versions": [{"version": "v1", "created": "Sun, 25 Oct 2020 22:01:48 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Leiva", "Luis A.", ""], ["Diaz", "Moises", ""], ["Ferrer", "Miguel A.", ""], ["Plamondon", "R\u00e9jean", ""]]}, {"id": "2010.13232", "submitter": "Mehmet Ozan Unal", "authors": "Mehmet Ozan Unal, Metin Ertas, Isa Yildirim", "title": "Self-Supervised Training For Low Dose CT Reconstruction", "comments": null, "journal-ref": "2021 IEEE 18th International Symposium on Biomedical Imaging\n  (ISBI)", "doi": "10.1109/ISBI48211.2021.9433944", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ionizing radiation has been the biggest concern in CT imaging. To reduce the\ndose level without compromising the image quality, low-dose CT reconstruction\nhas been offered with the availability of compressed sensing based\nreconstruction methods. Recently, data-driven methods got attention with the\nrise of deep learning, the availability of high computational power, and big\ndatasets. Deep learning based methods have also been used in low-dose CT\nreconstruction problem in different manners. Usually, the success of these\nmethods depends on labeled data. However, recent studies showed that training\ncan be achieved successfully with noisy datasets. In this study, we defined a\ntraining scheme to use low-dose sinograms as their own training targets. We\napplied the self-supervision principle in the projection domain where the noise\nis element-wise independent which is a requirement for self-supervised training\nmethods. Using the self-supervised training, the filtering part of the FBP\nmethod and the parameters of a denoiser neural network are optimized. We\ndemonstrate that our method outperforms both conventional and compressed\nsensing based iterative reconstruction methods qualitatively and quantitatively\nin the reconstruction of analytic CT phantoms and real-world CT images in\nlow-dose CT reconstruction task.\n", "versions": [{"version": "v1", "created": "Sun, 25 Oct 2020 22:02:14 GMT"}, {"version": "v2", "created": "Sat, 17 Apr 2021 18:58:01 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Unal", "Mehmet Ozan", ""], ["Ertas", "Metin", ""], ["Yildirim", "Isa", ""]]}, {"id": "2010.13242", "submitter": "Sheng Li", "authors": "Xiaodong Jiang, Ronghang Zhu, Pengsheng Ji, Sheng Li", "title": "Co-embedding of Nodes and Edges with Graph Neural Networks", "comments": "This manuscript has been accepted by the IEEE Transactions on Pattern\n  Analysis and Machine Intelligence", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph, as an important data representation, is ubiquitous in many real world\napplications ranging from social network analysis to biology. How to correctly\nand effectively learn and extract information from graph is essential for a\nlarge number of machine learning tasks. Graph embedding is a way to transform\nand encode the data structure in high dimensional and non-Euclidean feature\nspace to a low dimensional and structural space, which is easily exploited by\nother machine learning algorithms. We have witnessed a huge surge of such\nembedding methods, from statistical approaches to recent deep learning methods\nsuch as the graph convolutional networks (GCN). Deep learning approaches\nusually outperform the traditional methods in most graph learning benchmarks by\nbuilding an end-to-end learning framework to optimize the loss function\ndirectly. However, most of the existing GCN methods can only perform\nconvolution operations with node features, while ignoring the handy information\nin edge features, such as relations in knowledge graphs. To address this\nproblem, we present CensNet, Convolution with Edge-Node Switching graph neural\nnetwork, for learning tasks in graph-structured data with both node and edge\nfeatures. CensNet is a general graph embedding framework, which embeds both\nnodes and edges to a latent feature space. By using line graph of the original\nundirected graph, the role of nodes and edges are switched, and two novel graph\nconvolution operations are proposed for feature propagation. Experimental\nresults on real-world academic citation networks and quantum chemistry graphs\nshow that our approach achieves or matches the state-of-the-art performance in\nfour graph learning tasks, including semi-supervised node classification,\nmulti-task graph classification, graph regression, and link prediction.\n", "versions": [{"version": "v1", "created": "Sun, 25 Oct 2020 22:39:31 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Jiang", "Xiaodong", ""], ["Zhu", "Ronghang", ""], ["Ji", "Pengsheng", ""], ["Li", "Sheng", ""]]}, {"id": "2010.13244", "submitter": "Akshay Agarwal", "authors": "Mehak Gupta, Vishal Singh, Akshay Agarwal, Mayank Vatsa, and Richa\n  Singh", "title": "Generalized Iris Presentation Attack Detection Algorithm under\n  Cross-Database Settings", "comments": "ICPR 2020, 8 pages, 7 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Presentation attacks are posing major challenges to most of the biometric\nmodalities. Iris recognition, which is considered as one of the most accurate\nbiometric modality for person identification, has also been shown to be\nvulnerable to advanced presentation attacks such as 3D contact lenses and\ntextured lens. While in the literature, several presentation attack detection\n(PAD) algorithms are presented; a significant limitation is the\ngeneralizability against an unseen database, unseen sensor, and different\nimaging environment. To address this challenge, we propose a generalized deep\nlearning-based PAD network, MVANet, which utilizes multiple representation\nlayers. It is inspired by the simplicity and success of hybrid algorithm or\nfusion of multiple detection networks. The computational complexity is an\nessential factor in training deep neural networks; therefore, to reduce the\ncomputational complexity while learning multiple feature representation layers,\na fixed base model has been used. The performance of the proposed network is\ndemonstrated on multiple databases such as IIITD-WVU MUIPA and IIITD-CLI\ndatabases under cross-database training-testing settings, to assess the\ngeneralizability of the proposed algorithm.\n", "versions": [{"version": "v1", "created": "Sun, 25 Oct 2020 22:42:27 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Gupta", "Mehak", ""], ["Singh", "Vishal", ""], ["Agarwal", "Akshay", ""], ["Vatsa", "Mayank", ""], ["Singh", "Richa", ""]]}, {"id": "2010.13246", "submitter": "Akshay Agarwal", "authors": "Nilay Sanghvi, Sushant Kumar Singh, Akshay Agarwal, Mayank Vatsa, and\n  Richa Singh", "title": "MixNet for Generalized Face Presentation Attack Detection", "comments": "ICPR 2020, 8 pages, 6 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The non-intrusive nature and high accuracy of face recognition algorithms\nhave led to their successful deployment across multiple applications ranging\nfrom border access to mobile unlocking and digital payments. However, their\nvulnerability against sophisticated and cost-effective presentation attack\nmediums raises essential questions regarding its reliability. In the\nliterature, several presentation attack detection algorithms are presented;\nhowever, they are still far behind from reality. The major problem with\nexisting work is the generalizability against multiple attacks both in the seen\nand unseen setting. The algorithms which are useful for one kind of attack\n(such as print) perform unsatisfactorily for another type of attack (such as\nsilicone masks). In this research, we have proposed a deep learning-based\nnetwork termed as \\textit{MixNet} to detect presentation attacks in\ncross-database and unseen attack settings. The proposed algorithm utilizes\nstate-of-the-art convolutional neural network architectures and learns the\nfeature mapping for each attack category. Experiments are performed using\nmultiple challenging face presentation attack databases such as SMAD and Spoof\nIn the Wild (SiW-M) databases. Extensive experiments and comparison with\nexisting state of the art algorithms show the effectiveness of the proposed\nalgorithm.\n", "versions": [{"version": "v1", "created": "Sun, 25 Oct 2020 23:01:13 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Sanghvi", "Nilay", ""], ["Singh", "Sushant Kumar", ""], ["Agarwal", "Akshay", ""], ["Vatsa", "Mayank", ""], ["Singh", "Richa", ""]]}, {"id": "2010.13247", "submitter": "Akshay Agarwal", "authors": "Saheb Chhabra, Akshay Agarwal, Richa Singh, and Mayank Vatsa", "title": "Attack Agnostic Adversarial Defense via Visual Imperceptible Bound", "comments": "ICPR 2020, 8 pages, 5 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The high susceptibility of deep learning algorithms against structured and\nunstructured perturbations has motivated the development of efficient\nadversarial defense algorithms. However, the lack of generalizability of\nexisting defense algorithms and the high variability in the performance of the\nattack algorithms for different databases raises several questions on the\neffectiveness of the defense algorithms. In this research, we aim to design a\ndefense model that is robust within a certain bound against both seen and\nunseen adversarial attacks. This bound is related to the visual appearance of\nan image, and we termed it as \\textit{Visual Imperceptible Bound (VIB)}. To\ncompute this bound, we propose a novel method that uses the database\ncharacteristics. The VIB is further used to measure the effectiveness of attack\nalgorithms. The performance of the proposed defense model is evaluated on the\nMNIST, CIFAR-10, and Tiny ImageNet databases on multiple attacks that include\nC\\&W ($l_2$) and DeepFool. The proposed defense model is not only able to\nincrease the robustness against several attacks but also retain or improve the\nclassification accuracy on an original clean test set. The proposed algorithm\nis attack agnostic, i.e. it does not require any knowledge of the attack\nalgorithm.\n", "versions": [{"version": "v1", "created": "Sun, 25 Oct 2020 23:14:26 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Chhabra", "Saheb", ""], ["Agarwal", "Akshay", ""], ["Singh", "Richa", ""], ["Vatsa", "Mayank", ""]]}, {"id": "2010.13271", "submitter": "Naimul Mefraz Khan", "authors": "Gayathiri Murugamoorthy and Naimul Khan", "title": "Interpreting Uncertainty in Model Predictions For COVID-19 Diagnosis", "comments": "Submitted to ISBI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  COVID-19, due to its accelerated spread has brought in the need to use\nassistive tools for faster diagnosis in addition to typical lab swab testing.\nChest X-Rays for COVID cases tend to show changes in the lungs such as ground\nglass opacities and peripheral consolidations which can be detected by deep\nneural networks. However, traditional convolutional networks use point estimate\nfor predictions, lacking in capture of uncertainty, which makes them less\nreliable for adoption. There have been several works so far in predicting COVID\npositive cases with chest X-Rays. However, not much has been explored on\nquantifying the uncertainty of these predictions, interpreting uncertainty, and\ndecomposing this to model or data uncertainty. To address these needs, we\ndevelop a visualization framework to address interpretability of uncertainty\nand its components, with uncertainty in predictions computed with a Bayesian\nConvolutional Neural Network. This framework aims to understand the\ncontribution of individual features in the Chest-X-Ray images to predictive\nuncertainty. Providing this as an assistive tool can help the radiologist\nunderstand why the model came up with a prediction and whether the regions of\ninterest captured by the model for the specific prediction are of significance\nin diagnosis. We demonstrate the usefulness of the tool in chest x-ray\ninterpretation through several test cases from a benchmark dataset.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 01:27:29 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Murugamoorthy", "Gayathiri", ""], ["Khan", "Naimul", ""]]}, {"id": "2010.13294", "submitter": "Raviteja Tirumalapudi Mr", "authors": "Tirumalapudi Raviteja, Rajay Vedaraj .I.S", "title": "Global Image Segmentation Process using Machine Learning algorithm &\n  Convolution Neural Network method for Self- Driving Vehicles", "comments": "11 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In autonomous Vehicles technology Image segmentation was a major problem in\nvisual perception. This image segmentation process is mainly used in medical\napplications. Here we adopted an image segmentation process to visual\nperception tasks for predicting the agents on the surrounding environment,\nidentifying the road boundaries and tracking the line markings. Main objective\nof the paper is to divide the input images using the image segmentation process\nand Convolution Neural Network method for efficient results of visual\nperception. For Sampling assume a local city data-set samples and validation\nprocess done in Jupyter Notebook using Python language. We proposed this image\nsegmentation method planning to standard and further the development of\nstate-of-the art methods for visual inspection system understanding. The\nexperimental results achieves 73% mean IOU. Our method also achieves 90 FPS\ninference speed and using a NVDIA GeForce GTX 1050 GPU.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 02:51:00 GMT"}, {"version": "v2", "created": "Wed, 4 Nov 2020 14:35:12 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Raviteja", "Tirumalapudi", ""], ["S", "Rajay Vedaraj . I.", ""]]}, {"id": "2010.13302", "submitter": "Zhe Zhang", "authors": "Zhe Zhang, Chunyu Wang, Weichao Qiu, Wenhu Qin, Wenjun Zeng", "title": "AdaFuse: Adaptive Multiview Fusion for Accurate Human Pose Estimation in\n  the Wild", "comments": "Accepted by IJCV", "journal-ref": null, "doi": "10.1007/s11263-020-01398-9", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Occlusion is probably the biggest challenge for human pose estimation in the\nwild. Typical solutions often rely on intrusive sensors such as IMUs to detect\noccluded joints. To make the task truly unconstrained, we present AdaFuse, an\nadaptive multiview fusion method, which can enhance the features in occluded\nviews by leveraging those in visible views. The core of AdaFuse is to determine\nthe point-point correspondence between two views which we solve effectively by\nexploring the sparsity of the heatmap representation. We also learn an adaptive\nfusion weight for each camera view to reflect its feature quality in order to\nreduce the chance that good features are undesirably corrupted by ``bad''\nviews. The fusion model is trained end-to-end with the pose estimation network,\nand can be directly applied to new camera configurations without additional\nadaptation. We extensively evaluate the approach on three public datasets\nincluding Human3.6M, Total Capture and CMU Panoptic. It outperforms the\nstate-of-the-arts on all of them. We also create a large scale synthetic\ndataset Occlusion-Person, which allows us to perform numerical evaluation on\nthe occluded joints, as it provides occlusion labels for every joint in the\nimages. The dataset and code are released at\nhttps://github.com/zhezh/adafuse-3d-human-pose.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 03:19:46 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Zhang", "Zhe", ""], ["Wang", "Chunyu", ""], ["Qiu", "Weichao", ""], ["Qin", "Wenhu", ""], ["Zeng", "Wenjun", ""]]}, {"id": "2010.13308", "submitter": "Jun Zhuang", "authors": "Jun Zhuang, Dali Wang", "title": "Geometrically Matched Multi-source Microscopic Image Synthesis Using\n  Bidirectional Adversarial Networks", "comments": "Published in MICAD 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Microscopic images from multiple modalities can produce plentiful\nexperimental information. In practice, biological or physical constraints under\na given observation period may prevent researchers from acquiring enough\nmicroscopic scanning. Recent studies demonstrate that image synthesis is one of\nthe popular approaches to release such constraints. Nonetheless, most existing\nsynthesis approaches only translate images from the source domain to the target\ndomain without solid geometric associations. To embrace this challenge, we\npropose an innovative model architecture, BANIS, to synthesize diversified\nmicroscopic images from multi-source domains with distinct geometric features.\nThe experimental outcomes indicate that BANIS successfully synthesizes\nfavorable image pairs on C. elegans microscopy embryonic images. To the best of\nour knowledge, BANIS is the first application to synthesize microscopic images\nthat associate distinct spatial geometric features from multi-source domains.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 03:30:59 GMT"}, {"version": "v2", "created": "Sat, 27 Mar 2021 04:13:20 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Zhuang", "Jun", ""], ["Wang", "Dali", ""]]}, {"id": "2010.13313", "submitter": "Liu Qing", "authors": "Ziwen Xu, Beiji Zou, Qing Liu", "title": "A Dark and Bright Channel Prior Guided Deep Network for Retinal Image\n  Quality Assessment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Retinal image quality assessment is an essential task in the diagnosis of\nretinal diseases. Recently, there are emerging deep models to grade quality of\nretinal images. Current state-of-the-arts either directly transfer\nclassification networks originally designed for natural images to quality\nclassification of retinal images or introduce extra image quality priors via\nmultiple CNN branches or independent CNNs. This paper proposes a dark and\nbright channel prior guided deep network for retinal image quality assessment\ncalled GuidedNet. Specifically, the dark and bright channel priors are embedded\ninto the start layer of network to improve the discriminate ability of deep\nfeatures. In addition, we re-annotate a new retinal image quality dataset\ncalled RIQA-RFMiD for further validation. Experimental results on a public\nretinal image quality dataset Eye-Quality and our re-annotated dataset\nRIQA-RFMiD demonstrate the effectiveness of the proposed GuidedNet.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 03:53:08 GMT"}, {"version": "v2", "created": "Tue, 20 Apr 2021 14:50:47 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Xu", "Ziwen", ""], ["Zou", "Beiji", ""], ["Liu", "Qing", ""]]}, {"id": "2010.13317", "submitter": "Isaac Gerg", "authors": "Isaac D. Gerg and Vishal Monga", "title": "Structural Prior Driven Regularized Deep Learning for Sonar Image\n  Classification", "comments": "To appear in TGRS, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has been recently shown to improve performance in the domain of\nsynthetic aperture sonar (SAS) image classification. Given the constant\nresolution with range of a SAS, it is no surprise that deep learning techniques\nperform so well. Despite deep learning's recent success, there are still\ncompelling open challenges in reducing the high false alarm rate and enabling\nsuccess when training imagery is limited, which is a practical challenge that\ndistinguishes the SAS classification problem from standard image classification\nset-ups where training imagery may be abundant. We address these challenges by\nexploiting prior knowledge that humans use to grasp the scene. These include\nunconscious elimination of the image speckle and localization of objects in the\nscene. We introduce a new deep learning architecture which incorporates these\npriors with the goal of improving automatic target recognition (ATR) from SAS\nimagery. Our proposal -- called SPDRDL, Structural Prior Driven Regularized\nDeep Learning -- incorporates the previously mentioned priors in a multi-task\nconvolutional neural network (CNN) and requires no additional training data\nwhen compared to traditional SAS ATR methods. Two structural priors are\nenforced via regularization terms in the learning of the network: (1)\nstructural similarity prior -- enhanced imagery (often through despeckling)\naids human interpretation and is semantically similar to the original imagery\nand (2) structural scene context priors -- learned features ideally encapsulate\ntarget centering information; hence learning may be enhanced via a\nregularization that encourages fidelity against known ground truth target\nshifts (relative target position from scene center). Experiments on a\nchallenging real-world dataset reveal that SPDRDL outperforms state-of-the-art\ndeep learning and other competing methods for SAS image classification.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 04:00:46 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Gerg", "Isaac D.", ""], ["Monga", "Vishal", ""]]}, {"id": "2010.13320", "submitter": "Tristan Sylvain", "authors": "Tristan Sylvain, Linda Petrini, R Devon Hjelm", "title": "Zero-Shot Learning from scratch (ZFS): leveraging local compositional\n  representations", "comments": "ICML 2019 Workshop on Understanding and Improving General-ization in\n  Deep Learning, Long Beach, California, 2019 Spotlight presentation. arXiv\n  admin note: text overlap with arXiv:1912.12179", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zero-shot classification is a generalization task where no instance from the\ntarget classes is seen during training. To allow for test-time transfer, each\nclass is annotated with semantic information, commonly in the form of\nattributes or text descriptions. While classical zero-shot learning does not\nexplicitly forbid using information from other datasets, the approaches that\nachieve the best absolute performance on image benchmarks rely on features\nextracted from encoders pretrained on Imagenet. This approach relies on\nhyper-optimized Imagenet-relevant parameters from the supervised classification\nsetting, entangling important questions about the suitability of those\nparameters and how they were learned with more fundamental questions about\nrepresentation learning and generalization. To remove these distractors, we\npropose a more challenging setting: Zero-Shot Learning from scratch (ZFS),\nwhich explicitly forbids the use of encoders fine-tuned on other datasets. Our\nanalysis on this setting highlights the importance of local information, and\ncompositional representations.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 23:11:18 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Sylvain", "Tristan", ""], ["Petrini", "Linda", ""], ["Hjelm", "R Devon", ""]]}, {"id": "2010.13321", "submitter": "Ting Liu", "authors": "Ting Liu, Jennifer J. Sun, Long Zhao, Jiaping Zhao, Liangzhe Yuan,\n  Yuxiao Wang, Liang-Chieh Chen, Florian Schroff, Hartwig Adam", "title": "View-Invariant, Occlusion-Robust Probabilistic Embedding for Human Pose", "comments": "Code is available at\n  https://github.com/google-research/google-research/tree/master/poem . Video\n  synchronization results are available at\n  https://drive.google.com/corp/drive/folders/1nhPuEcX4Lhe6iK3nv84cvSCov2eJ52Xy.\n  arXiv admin note: text overlap with arXiv:1912.01001", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recognition of human poses and activities is crucial for autonomous systems\nto interact smoothly with people. However, cameras generally capture human\nposes in 2D as images and videos, which can have significant appearance\nvariations across viewpoints. To address this, we explore recognizing\nsimilarity in 3D human body poses from 2D information, which has not been\nwell-studied in existing works. Here, we propose an approach to learning a\ncompact view-invariant embedding space from 2D body joint keypoints, without\nexplicitly predicting 3D poses. Input ambiguities of 2D poses from projection\nand occlusion are difficult to represent through a deterministic mapping, and\ntherefore we use probabilistic embeddings. In order to enable our embeddings to\nwork with partially visible input keypoints, we further investigate different\nkeypoint occlusion augmentation strategies during training. Experimental\nresults show that our embedding model achieves higher accuracy when retrieving\nsimilar poses across different camera views, in comparison with 3D pose\nestimation models. We further show that keypoint occlusion augmentation during\ntraining significantly improves retrieval performance on partial 2D input\nposes. Results on action recognition and video alignment demonstrate that our\nembeddings, without any additional training, achieves competitive performance\nrelative to other models specifically trained for each task.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 17:58:35 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Liu", "Ting", ""], ["Sun", "Jennifer J.", ""], ["Zhao", "Long", ""], ["Zhao", "Jiaping", ""], ["Yuan", "Liangzhe", ""], ["Wang", "Yuxiao", ""], ["Chen", "Liang-Chieh", ""], ["Schroff", "Florian", ""], ["Adam", "Hartwig", ""]]}, {"id": "2010.13336", "submitter": "Hojjat Salehinejad", "authors": "Hojjat Salehinejad, Edward Ho, Hui-Ming Lin, Priscila Crivellaro,\n  Oleksandra Samorodova, Monica Tafur Arciniegas, Zamir Merali, Suradech\n  Suthiphosuwan, Aditya Bharatha, Kristen Yeom, Muhammad Mamdani, Jefferson\n  Wilson, Errol Colak", "title": "Deep Sequential Learning for Cervical Spine Fracture Detection on\n  Computed Tomography Imaging", "comments": "This paper is accepted for presentation at the IEEE International\n  Symposium on Biomedical Imaging (ISBI) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fractures of the cervical spine are a medical emergency and may lead to\npermanent paralysis and even death. Accurate diagnosis in patients with\nsuspected fractures by computed tomography (CT) is critical to patient\nmanagement. In this paper, we propose a deep convolutional neural network\n(DCNN) with a bidirectional long-short term memory (BLSTM) layer for the\nautomated detection of cervical spine fractures in CT axial images. We used an\nannotated dataset of 3,666 CT scans (729 positive and 2,937 negative cases) to\ntrain and validate the model. The validation results show a classification\naccuracy of 70.92% and 79.18% on the balanced (104 positive and 104 negative\ncases) and imbalanced (104 positive and 419 negative cases) test datasets,\nrespectively.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 04:36:29 GMT"}, {"version": "v2", "created": "Tue, 27 Oct 2020 01:10:11 GMT"}, {"version": "v3", "created": "Fri, 30 Oct 2020 21:18:53 GMT"}, {"version": "v4", "created": "Fri, 5 Feb 2021 17:33:06 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["Salehinejad", "Hojjat", ""], ["Ho", "Edward", ""], ["Lin", "Hui-Ming", ""], ["Crivellaro", "Priscila", ""], ["Samorodova", "Oleksandra", ""], ["Arciniegas", "Monica Tafur", ""], ["Merali", "Zamir", ""], ["Suthiphosuwan", "Suradech", ""], ["Bharatha", "Aditya", ""], ["Yeom", "Kristen", ""], ["Mamdani", "Muhammad", ""], ["Wilson", "Jefferson", ""], ["Colak", "Errol", ""]]}, {"id": "2010.13337", "submitter": "Ziyu Jiang", "authors": "Ziyu Jiang, Tianlong Chen, Ting Chen, Zhangyang Wang", "title": "Robust Pre-Training by Adversarial Contrastive Learning", "comments": "Neurips 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work has shown that, when integrated with adversarial training,\nself-supervised pre-training can lead to state-of-the-art robustness In this\nwork, we improve robustness-aware self-supervised pre-training by learning\nrepresentations that are consistent under both data augmentations and\nadversarial perturbations. Our approach leverages a recent contrastive learning\nframework, which learns representations by maximizing feature consistency under\ndifferently augmented views. This fits particularly well with the goal of\nadversarial robustness, as one cause of adversarial fragility is the lack of\nfeature invariance, i.e., small input perturbations can result in undesirable\nlarge changes in features or even predicted labels. We explore various options\nto formulate the contrastive task, and demonstrate that by injecting\nadversarial perturbations, contrastive pre-training can lead to models that are\nboth label-efficient and robust. We empirically evaluate the proposed\nAdversarial Contrastive Learning (ACL) and show it can consistently outperform\nexisting methods. For example on the CIFAR-10 dataset, ACL outperforms the\nprevious state-of-the-art unsupervised robust pre-training approach by 2.99% on\nrobust accuracy and 2.14% on standard accuracy. We further demonstrate that ACL\npre-training can improve semi-supervised adversarial training, even when only a\nfew labeled examples are available. Our codes and pre-trained models have been\nreleased at: https://github.com/VITA-Group/Adversarial-Contrastive-Learning.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 04:44:43 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Jiang", "Ziyu", ""], ["Chen", "Tianlong", ""], ["Chen", "Ting", ""], ["Wang", "Zhangyang", ""]]}, {"id": "2010.13338", "submitter": "Songyan Zhang", "authors": "Songyan Zhang, Zhicheng Wang, Qiang Wang, Jinshuo Zhang, Gang Wei,\n  Xiaowen Chu", "title": "EDNet: Efficient Disparity Estimation with Cost Volume Combination and\n  Attention-based Spatial Residual", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing state-of-the-art disparity estimation works mostly leverage the 4D\nconcatenation volume and construct a very deep 3D convolution neural network\n(CNN) for disparity regression, which is inefficient due to the high memory\nconsumption and slow inference speed. In this paper, we propose a network named\nEDNet for efficient disparity estimation. Firstly, we construct a combined\nvolume which incorporates contextual information from the squeezed\nconcatenation volume and feature similarity measurement from the correlation\nvolume. The combined volume can be next aggregated by 2D convolutions which are\nfaster and require less memory than 3D convolutions. Secondly, we propose an\nattention-based spatial residual module to generate attention-aware residual\nfeatures. The attention mechanism is applied to provide intuitive spatial\nevidence about inaccurate regions with the help of error maps at multiple\nscales and thus improve the residual learning efficiency. Extensive experiments\non the Scene Flow and KITTI datasets show that EDNet outperforms the previous\n3D CNN based works and achieves state-of-the-art performance with significantly\nfaster speed and less memory consumption.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 04:49:44 GMT"}, {"version": "v2", "created": "Sun, 8 Nov 2020 09:00:19 GMT"}, {"version": "v3", "created": "Sat, 5 Dec 2020 02:55:49 GMT"}, {"version": "v4", "created": "Thu, 4 Mar 2021 05:30:42 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Zhang", "Songyan", ""], ["Wang", "Zhicheng", ""], ["Wang", "Qiang", ""], ["Zhang", "Jinshuo", ""], ["Wei", "Gang", ""], ["Chu", "Xiaowen", ""]]}, {"id": "2010.13343", "submitter": "S. Shailja", "authors": "S. Shailja, Jiaxiang Jiang, B.S. Manjunath", "title": "Semi supervised segmentation and graph-based tracking of 3D nuclei in\n  time-lapse microscopy", "comments": "To be submitted to ISBI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel weakly supervised method to improve the boundary of the 3D\nsegmented nuclei utilizing an over-segmented image. This is motivated by the\nobservation that current state-of-the-art deep learning methods do not result\nin accurate boundaries when the training data is weakly annotated. Towards\nthis, a 3D U-Net is trained to get the centroid of the nuclei and integrated\nwith a simple linear iterative clustering (SLIC) supervoxel algorithm that\nprovides better adherence to cluster boundaries. To track these segmented\nnuclei, our algorithm utilizes the relative nuclei location depicting the\nprocesses of nuclei division and apoptosis. The proposed algorithmic pipeline\nachieves better segmentation performance compared to the state-of-the-art\nmethod in Cell Tracking Challenge (CTC) 2019 and comparable performance to\nstate-of-the-art methods in IEEE ISBI CTC2020 while utilizing very few\npixel-wise annotated data. Detailed experimental results are provided, and the\nsource code is available on GitHub.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 05:09:44 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Shailja", "S.", ""], ["Jiang", "Jiaxiang", ""], ["Manjunath", "B. S.", ""]]}, {"id": "2010.13355", "submitter": "Guibin Chen", "authors": "Guibin Chen, Bosheng Wang, Xiaoliang Wang, Huanjun Deng, Bing Wang,\n  Shuo Zhang", "title": "PSF-LO: Parameterized Semantic Features Based Lidar Odometry", "comments": "Accepted in International Conference on Robotics and Automation\n  (ICRA) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lidar odometry (LO) is a key technology in numerous reliable and accurate\nlocalization and mapping systems of autonomous driving. The state-of-the-art LO\nmethods generally leverage geometric information to perform point cloud\nregistration. Furthermore, obtaining point cloud semantic information which can\ndescribe the environment more abundantly will help for the registration. We\npresent a novel semantic lidar odometry method based on self-designed\nparameterized semantic features (PSFs) to achieve low-drift ego-motion\nestimation for autonomous vehicle in realtime. We first use a convolutional\nneural network-based algorithm to obtain point-wise semantics from the input\nlaser point cloud, and then use semantic labels to separate the road, building,\ntraffic sign and pole-like point cloud and fit them separately to obtain\ncorresponding PSFs. A fast PSF-based matching enable us to refine geometric\nfeatures (GeFs) registration, reducing the impact of blurred submap surface on\nthe accuracy of GeFs matching. Besides, we design an efficient method to\naccurately recognize and remove the dynamic objects while retaining static ones\nin the semantic point cloud, which are beneficial to further improve the\naccuracy of LO. We evaluated our method, namely PSF-LO, on the public dataset\nKITTI Odometry Benchmark and ranked #1 among semantic lidar methods with an\naverage translation error of 0.82% in the test dataset at the time of writing.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 05:48:26 GMT"}, {"version": "v2", "created": "Sat, 31 Oct 2020 13:05:23 GMT"}, {"version": "v3", "created": "Thu, 25 Mar 2021 04:01:17 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Chen", "Guibin", ""], ["Wang", "Bosheng", ""], ["Wang", "Xiaoliang", ""], ["Deng", "Huanjun", ""], ["Wang", "Bing", ""], ["Zhang", "Shuo", ""]]}, {"id": "2010.13357", "submitter": "Haibo Su", "authors": "Haibo Su, Peng Wang, Lingqiao Liu, Hui Li, Zhen Li, Yanning Zhang", "title": "Where to Look and How to Describe: Fashion Image Retrieval with an\n  Attentional Heterogeneous Bilinear Network", "comments": "11 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fashion products typically feature in compositions of a variety of styles at\ndifferent clothing parts. In order to distinguish images of different fashion\nproducts, we need to extract both appearance (i.e., \"how to describe\") and\nlocalization (i.e.,\"where to look\") information, and their interactions. To\nthis end, we propose a biologically inspired framework for image-based fashion\nproduct retrieval, which mimics the hypothesized twostream visual processing\nsystem of human brain. The proposed attentional heterogeneous bilinear network\n(AHBN) consists of two branches: a deep CNN branch to extract fine-grained\nappearance attributes and a fully convolutional branch to extract landmark\nlocalization information. A joint channel-wise attention mechanism is further\napplied to the extracted heterogeneous features to focus on important channels,\nfollowed by a compact bilinear pooling layer to model the interaction of the\ntwo streams. Our proposed framework achieves satisfactory performance on three\nimage-based fashion product retrieval benchmarks.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 06:01:09 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Su", "Haibo", ""], ["Wang", "Peng", ""], ["Liu", "Lingqiao", ""], ["Li", "Hui", ""], ["Li", "Zhen", ""], ["Zhang", "Yanning", ""]]}, {"id": "2010.13365", "submitter": "Philipp Benz", "authors": "Philipp Benz, Chaoning Zhang, Adil Karjauv, In So Kweon", "title": "Robustness May Be at Odds with Fairness: An Empirical Study on\n  Class-wise Accuracy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, convolutional neural networks (CNNs) have made significant\nadvancement, however, they are widely known to be vulnerable to adversarial\nattacks. Adversarial training is the most widely used technique for improving\nadversarial robustness to strong white-box attacks. Prior works have been\nevaluating and improving the model average robustness without per-class\nevaluation. The average evaluation alone might provide a false sense of\nrobustness. For example, the attacker can focus on attacking the vulnerable\nclass, which can be dangerous, especially, when the vulnerable class is a\ncritical one, such as \"human\" in autonomous driving. In this preregistration\nsubmission, we propose an empirical study on the class-wise accuracy and\nrobustness of adversarially trained models. Given that the CIFAR10 training\ndataset has an equal number of samples for each class, interestingly,\npreliminary results on it with Resnet18 show that there exists inter-class\ndiscrepancy for accuracy and robustness on standard models, for instance, \"cat\"\nis more vulnerable than other classes. Moreover, adversarial training increases\ninter-class discrepancy. Our work aims to investigate the following questions:\n(a) is the phenomenon of inter-class discrepancy universal for other\nclassification benchmark datasets on other seminal model architectures with\nvarious optimization hyper-parameters? (b) If so, what can be possible\nexplanations for the inter-class discrepancy? (c) Can the techniques proposed\nin the long tail classification be readily extended to adversarial training for\naddressing the inter-class discrepancy?\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 06:32:32 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Benz", "Philipp", ""], ["Zhang", "Chaoning", ""], ["Karjauv", "Adil", ""], ["Kweon", "In So", ""]]}, {"id": "2010.13372", "submitter": "Marco Domenico Cirillo", "authors": "Marco Domenico Cirillo and David Abramian and Anders Eklund", "title": "What is the best data augmentation for 3D brain tumor segmentation?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Training segmentation networks requires large annotated datasets, which in\nmedical imaging can be hard to obtain. Despite this fact, data augmentation has\nin our opinion not been fully explored for brain tumor segmentation. In this\nproject we apply different types of data augmentation (flipping, rotation,\nscaling, brightness adjustment, elastic deformation) when training a standard\n3D U-Net, and demonstrate that augmentation significantly improves the\nnetwork's performance in many cases. Our conclusion is that brightness\naugmentation and elastic deformation work best, and that combinations of\ndifferent augmentation techniques do not provide further improvement compared\nto only using one augmentation technique. Our code is available at\nhttps://github.com/mdciri/3D-augmentation-techniques.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 07:00:16 GMT"}, {"version": "v2", "created": "Sat, 16 Jan 2021 15:28:23 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Cirillo", "Marco Domenico", ""], ["Abramian", "David", ""], ["Eklund", "Anders", ""]]}, {"id": "2010.13386", "submitter": "Daizong Liu", "authors": "Daizong Liu, Hongting Zhang, Pan Zhou", "title": "Video-based Facial Expression Recognition using Graph Convolutional\n  Networks", "comments": "Accepted by ICPR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial expression recognition (FER), aiming to classify the expression\npresent in the facial image or video, has attracted a lot of research interests\nin the field of artificial intelligence and multimedia. In terms of video based\nFER task, it is sensible to capture the dynamic expression variation among the\nframes to recognize facial expression. However, existing methods directly\nutilize CNN-RNN or 3D CNN to extract the spatial-temporal features from\ndifferent facial units, instead of concentrating on a certain region during\nexpression variation capturing, which leads to limited performance in FER. In\nour paper, we introduce a Graph Convolutional Network (GCN) layer into a common\nCNN-RNN based model for video-based FER. First, the GCN layer is utilized to\nlearn more significant facial expression features which concentrate on certain\nregions after sharing information between extracted CNN features of nodes.\nThen, a LSTM layer is applied to learn long-term dependencies among the GCN\nlearned features to model the variation. In addition, a weight assignment\nmechanism is also designed to weight the output of different nodes for final\nclassification by characterizing the expression intensities in each frame. To\nthe best of our knowledge, it is the first time to use GCN in FER task. We\nevaluate our method on three widely-used datasets, CK+, Oulu-CASIA and MMI, and\nalso one challenging wild dataset AFEW8.0, and the experimental results\ndemonstrate that our method has superior performance to existing methods.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 07:31:51 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Liu", "Daizong", ""], ["Zhang", "Hongting", ""], ["Zhou", "Pan", ""]]}, {"id": "2010.13412", "submitter": "Chongyi Li", "authors": "Chongyi Li, Chunle Guo, Qiming Ai, Shangchen Zhou, Chen Change Loy", "title": "Flexible Piecewise Curves Estimation for Photo Enhancement", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new method, called FlexiCurve, for photo enhancement.\nUnlike most existing methods that perform image-to-image mapping, which\nrequires expensive pixel-wise reconstruction, FlexiCurve takes an input image\nand estimates global curves to adjust the image. The adjustment curves are\nspecially designed for performing piecewise mapping, taking nonlinear\nadjustment and differentiability into account. To cope with challenging and\ndiverse illumination properties in real-world images, FlexiCurve is formulated\nas a multi-task framework to produce diverse estimations and the associated\nconfidence maps. These estimations are adaptively fused to improve local\nenhancements of different regions. Thanks to the image-to-curve formulation,\nfor an image with a size of 512*512*3, FlexiCurve only needs a lightweight\nnetwork (150K trainable parameters) and it has a fast inference speed (83FPS on\na single NVIDIA 2080Ti GPU). The proposed method improves efficiency without\ncompromising the enhancement quality and losing details in the original image.\nThe method is also appealing as it is not limited to paired training data, thus\nit can flexibly learn rich enhancement styles from unpaired data. Extensive\nexperiments demonstrate that our method achieves state-of-the-art performance\non photo enhancement quantitively and qualitatively.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 08:16:25 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Li", "Chongyi", ""], ["Guo", "Chunle", ""], ["Ai", "Qiming", ""], ["Zhou", "Shangchen", ""], ["Loy", "Chen Change", ""]]}, {"id": "2010.13418", "submitter": "Lipei Zhang", "authors": "Aozhi Liu, Lipei Zhang, Yaqi Mei, Sitong Lian, Maokun Han, Wen Cheng,\n  Yuyu Liu, Zifeng Cai, Zhaohua Zhu, Baoqiang Han, Jing Xiao", "title": "Residual Recurrent CRNN for End-to-End Optical Music Recognition on\n  Monophonic Scores", "comments": "5 pages, 4 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optical Music Recognition is a field that attempts to extract digital\ninformation from images of either the printed music scores or the handwritten\nmusic scores. One of the challenges of the Optical Music Recognition task is to\ntranscript the symbols of the camera-captured images into digital music\nnotations. Previous end-to-end model, based on deep learning, was developed as\na Convolutional Recurrent Neural Network. However, it does not explore\nsufficient contextual information from full scales and there is still a large\nroom for improvement. In this paper, we propose an innovative end-to-end\nframework that combines a block of Residual Recurrent Convolutional Neural\nNetwork with a recurrent Encoder-Decoder network to map a sequence of\nmonophonic music symbols corresponding to the notations present in the image.\nThe Residual Recurrent Convolutional block can improve the ability of the model\nto enrich the context information while the number of parameter will not be\nincreasing. The experiment results were benchmarked against a publicly\navailable dataset called CAMERA-PRIMUS. We evaluate the performances of our\nmodel on both the images with ideal conditions and that with non-ideal\nconditions. The experiments show that our approach surpass the state-of-the-art\nend-to-end method using Convolutional Recurrent Neural Network.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 08:39:37 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Liu", "Aozhi", ""], ["Zhang", "Lipei", ""], ["Mei", "Yaqi", ""], ["Lian", "Sitong", ""], ["Han", "Maokun", ""], ["Cheng", "Wen", ""], ["Liu", "Yuyu", ""], ["Cai", "Zifeng", ""], ["Zhu", "Zhaohua", ""], ["Han", "Baoqiang", ""], ["Xiao", "Jing", ""]]}, {"id": "2010.13422", "submitter": "Tao Deng", "authors": "Wenbo Liu, Fei Yan, Kuan Tang, Jiyong Zhang, Tao Deng", "title": "Lane detection in complex scenes based on end-to-end neural network", "comments": "Accepted by 2020 China Automation Congress (CAC)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The lane detection is a key problem to solve the division of derivable areas\nin unmanned driving, and the detection accuracy of lane lines plays an\nimportant role in the decision-making of vehicle driving. Scenes faced by\nvehicles in daily driving are relatively complex. Bright light, insufficient\nlight, and crowded vehicles will bring varying degrees of difficulty to lane\ndetection. So we combine the advantages of spatial convolution in spatial\ninformation processing and the efficiency of ERFNet in semantic segmentation,\npropose an end-to-end network to lane detection in a variety of complex scenes.\nAnd we design the information exchange block by combining spatial convolution\nand dilated convolution, which plays a great role in understanding detailed\ninformation. Finally, our network was tested on the CULane database and its\nF1-measure with IOU threshold of 0.5 can reach 71.9%.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 08:46:35 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Liu", "Wenbo", ""], ["Yan", "Fei", ""], ["Tang", "Kuan", ""], ["Zhang", "Jiyong", ""], ["Deng", "Tao", ""]]}, {"id": "2010.13424", "submitter": "Tae-Young Chung", "authors": "Tae-young Chung, Heansung Lee, Myeong Ah Cho, Suhwan Cho, Sangyoun Lee", "title": "Multi-object tracking with self-supervised associating network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-Object Tracking (MOT) is the task that has a lot of potential for\ndevelopment, and there are still many problems to be solved. In the traditional\ntracking by detection paradigm, There has been a lot of work on feature based\nobject re-identification methods. However, this method has a lack of training\ndata problem. For labeling multi-object tracking dataset, every detection in a\nvideo sequence need its location and IDs. Since assigning consecutive IDs to\neach detection in every sequence is a very labor-intensive task, current\nmulti-object tracking dataset is not sufficient enough to train\nre-identification network. So in this paper, we propose a novel self-supervised\nlearning method using a lot of short videos which has no human labeling, and\nimprove the tracking performance through the re-identification network trained\nin the self-supervised manner to solve the lack of training data problem.\nDespite the re-identification network is trained in a self-supervised manner,\nit achieves the state-of-the-art performance of MOTA 62.0\\% and IDF1 62.6\\% on\nthe MOT17 test benchmark. Furthermore, the performance is improved as much as\nlearned with a large amount of data, it shows the potential of self-supervised\nmethod.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 08:48:23 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Chung", "Tae-young", ""], ["Lee", "Heansung", ""], ["Cho", "Myeong Ah", ""], ["Cho", "Suhwan", ""], ["Lee", "Sangyoun", ""]]}, {"id": "2010.13433", "submitter": "Kai Yao", "authors": "Kai Yao, Alberto Ortiz, Francisco Bonnin-Pascual", "title": "A Weakly-Supervised Semantic Segmentation Approach based on the Centroid\n  Loss: Application to Quality Control and Inspection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is generally accepted that one of the critical parts of current vision\nalgorithms based on deep learning and convolutional neural networks is the\nannotation of a sufficient number of images to achieve competitive performance.\nThis is particularly difficult for semantic segmentation tasks since the\nannotation must be ideally generated at the pixel level. Weakly-supervised\nsemantic segmentation aims at reducing this cost by employing simpler\nannotations that, hence, are easier, cheaper and quicker to produce. In this\npaper, we propose and assess a new weakly-supervised semantic segmentation\napproach making use of a novel loss function whose goal is to counteract the\neffects of weak annotations. To this end, this loss function comprises several\nterms based on partial cross-entropy losses, being one of them the Centroid\nLoss. This term induces a clustering of the image pixels in the object classes\nunder consideration, whose aim is to improve the training of the segmentation\nnetwork by guiding the optimization. The performance of the approach is\nevaluated against datasets from two different industry-related case studies:\nwhile one involves the detection of instances of a number of different object\nclasses in the context of a quality control application, the other stems from\nthe visual inspection domain and deals with the localization of images areas\nwhose pixels correspond to scene surface points affected by a specific sort of\ndefect. The detection results that are reported for both cases show that,\ndespite the differences among them and the particular challenges, the use of\nweak annotations do not prevent from achieving a competitive performance level\nfor both.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 09:08:21 GMT"}, {"version": "v2", "created": "Wed, 3 Mar 2021 15:50:04 GMT"}, {"version": "v3", "created": "Thu, 4 Mar 2021 14:41:06 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Yao", "Kai", ""], ["Ortiz", "Alberto", ""], ["Bonnin-Pascual", "Francisco", ""]]}, {"id": "2010.13439", "submitter": "Marco Rosano", "authors": "Marco Rosano, Antonino Furnari, Luigi Gulino, Giovanni Maria Farinella", "title": "On Embodied Visual Navigation in Real Environments Through Habitat", "comments": "Published in International Conference on Pattern Recognition (ICPR),\n  2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual navigation models based on deep learning can learn effective policies\nwhen trained on large amounts of visual observations through reinforcement\nlearning. Unfortunately, collecting the required experience in the real world\nrequires the deployment of a robotic platform, which is expensive and\ntime-consuming. To deal with this limitation, several simulation platforms have\nbeen proposed in order to train visual navigation policies on virtual\nenvironments efficiently. Despite the advantages they offer, simulators present\na limited realism in terms of appearance and physical dynamics, leading to\nnavigation policies that do not generalize in the real world.\n  In this paper, we propose a tool based on the Habitat simulator which\nexploits real world images of the environment, together with sensor and\nactuator noise models, to produce more realistic navigation episodes. We\nperform a range of experiments to assess the ability of such policies to\ngeneralize using virtual and real-world images, as well as observations\ntransformed with unsupervised domain adaptation approaches. We also assess the\nimpact of sensor and actuation noise on the navigation performance and\ninvestigate whether it allows to learn more robust navigation policies. We show\nthat our tool can effectively help to train and evaluate navigation policies on\nreal-world observations without running navigation pisodes in the real world.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 09:19:07 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Rosano", "Marco", ""], ["Furnari", "Antonino", ""], ["Gulino", "Luigi", ""], ["Farinella", "Giovanni Maria", ""]]}, {"id": "2010.13454", "submitter": "Kumar Abhishek", "authors": "Kumar Abhishek, Ghassan Hamarneh", "title": "Matthews Correlation Coefficient Loss for Deep Convolutional Networks:\n  Application to Skin Lesion Segmentation", "comments": "6 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The segmentation of skin lesions is a crucial task in clinical decision\nsupport systems for the computer aided diagnosis of skin lesions. Although deep\nlearning-based approaches have improved segmentation performance, these models\nare often susceptible to class imbalance in the data, particularly, the\nfraction of the image occupied by the background healthy skin. Despite\nvariations of the popular Dice loss function being proposed to tackle the class\nimbalance problem, the Dice loss formulation does not penalize\nmisclassifications of the background pixels. We propose a novel metric-based\nloss function using the Matthews correlation coefficient, a metric that has\nbeen shown to be efficient in scenarios with skewed class distributions, and\nuse it to optimize deep segmentation models. Evaluations on three skin lesion\nimage datasets: the ISBI ISIC 2017 Skin Lesion Segmentation Challenge dataset,\nthe DermoFit Image Library, and the PH2 dataset, show that models trained using\nthe proposed loss function outperform those trained using Dice loss by 11.25%,\n4.87%, and 0.76% respectively in the mean Jaccard index. The code is available\nat https://github.com/kakumarabhishek/MCC-Loss.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 09:50:25 GMT"}, {"version": "v2", "created": "Sun, 21 Feb 2021 01:06:00 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Abhishek", "Kumar", ""], ["Hamarneh", "Ghassan", ""]]}, {"id": "2010.13460", "submitter": "Iulian Emil Tampu", "authors": "Iulian Emil Tampu and Neda Haj-Hosseini and Anders Eklund", "title": "Does anatomical contextual information improve 3D U-Net based brain\n  tumor segmentation?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Effective, robust and automatic tools for brain tumor segmentation are needed\nfor extraction of information useful in treatment planning. In recent years,\nconvolutional neural networks have shown state-of-the-art performance in the\nidentification of tumor regions in magnetic resonance (MR) images.\nContext-aware artificial intelligence is an emerging concept for the\ndevelopment of deep learning applications for computer-aided medical image\nanalysis. A large portion of the current research is devoted to the development\nof new network architectures to improve segmentation accuracy by using\ncontext-aware mechanisms. In this work it is investigated if the addition of\ncontextual information from the brain anatomy in the form of white matter (WM),\ngray matter (GM) and cerebrospinal fluid (CSF) masks and probability maps\nimproves U-Net based brain tumor segmentation. The BraTS2020 dataset was used\nto train and test two standard 3D U-Net (nnU-Net) models that, in addition to\nthe conventional MR image modalities, used the contextual information as extra\nchannels. For comparison, a baseline model that only used the conventional MR\nimage modalities was also trained. Median (mean) Dice scores of 89.3 (79.9),\n89.5 (83.0) and 90.16 (80.9) were obtained on the test dataset for the baseline\nmodel, the contextual model using binary masks and the model using probability\nmaps, respectively. Results show that there is no statistically significant\ndifference when comparing Dice scores between the baseline model and the\ncontextual information models on the test dataset (p>0.05). In conclusion,\nthere is no significant improvement in segmentation performance when using\ncontextual information in the form of either WM, GM and CFS masks or\nprobability maps as extra channels.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 09:57:58 GMT"}, {"version": "v2", "created": "Tue, 27 Apr 2021 14:34:15 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Tampu", "Iulian Emil", ""], ["Haj-Hosseini", "Neda", ""], ["Eklund", "Anders", ""]]}, {"id": "2010.13484", "submitter": "Zhipeng Ding", "authors": "Zhipeng Ding, Marc Niethammer", "title": "VoteNet++: Registration Refinement for Multi-Atlas Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-atlas segmentation (MAS) is a popular image segmentation technique for\nmedical images. In this work, we improve the performance of MAS by correcting\nregistration errors before label fusion. Specifically, we use a volumetric\ndisplacement field to refine registrations based on image anatomical appearance\nand predicted labels. We show the influence of the initial spatial alignment as\nwell as the beneficial effect of using label information for MAS performance.\nExperiments demonstrate that the proposed refinement approach improves MAS\nperformance on a 3D magnetic resonance dataset of the knee.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 11:16:59 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Ding", "Zhipeng", ""], ["Niethammer", "Marc", ""]]}, {"id": "2010.13499", "submitter": "Tom Eelbode", "authors": "Tom Eelbode, Jeroen Bertels, Maxim Berman, Dirk Vandermeulen, Frederik\n  Maes, Raf Bisschops, Matthew B. Blaschko", "title": "Optimization for Medical Image Segmentation: Theory and Practice when\n  evaluating with Dice Score or Jaccard Index", "comments": "15 pages, 14 figures, accepted for publication in IEEE Transactions\n  on Medical Imaging (2020)", "journal-ref": null, "doi": "10.1109/TMI.2020.3002417", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many medical imaging and classical computer vision tasks, the Dice score\nand Jaccard index are used to evaluate the segmentation performance. Despite\nthe existence and great empirical success of metric-sensitive losses, i.e.\nrelaxations of these metrics such as soft Dice, soft Jaccard and\nLovasz-Softmax, many researchers still use per-pixel losses, such as (weighted)\ncross-entropy to train CNNs for segmentation. Therefore, the target metric is\nin many cases not directly optimized. We investigate from a theoretical\nperspective, the relation within the group of metric-sensitive loss functions\nand question the existence of an optimal weighting scheme for weighted\ncross-entropy to optimize the Dice score and Jaccard index at test time. We\nfind that the Dice score and Jaccard index approximate each other relatively\nand absolutely, but we find no such approximation for a weighted Hamming\nsimilarity. For the Tversky loss, the approximation gets monotonically worse\nwhen deviating from the trivial weight setting where soft Tversky equals soft\nDice. We verify these results empirically in an extensive validation on six\nmedical segmentation tasks and can confirm that metric-sensitive losses are\nsuperior to cross-entropy based loss functions in case of evaluation with Dice\nScore or Jaccard Index. This further holds in a multi-class setting, and across\ndifferent object sizes and foreground/background ratios. These results\nencourage a wider adoption of metric-sensitive loss functions for medical\nsegmentation tasks where the performance measure of interest is the Dice score\nor Jaccard index.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 11:45:55 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Eelbode", "Tom", ""], ["Bertels", "Jeroen", ""], ["Berman", "Maxim", ""], ["Vandermeulen", "Dirk", ""], ["Maes", "Frederik", ""], ["Bisschops", "Raf", ""], ["Blaschko", "Matthew B.", ""]]}, {"id": "2010.13500", "submitter": "Zhi Yuan Wu", "authors": "Zhiyuan Wu, Hong Qi, Yu Jiang, Minghao Zhao, Chupeng Cui, Zongmin Yang\n  and Xinhui Xue", "title": "Activation Map Adaptation for Effective Knowledge Distillation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model compression becomes a recent trend due to the requirement of deploying\nneural networks on embedded and mobile devices. Hence, both accuracy and\nefficiency are of critical importance. To explore a balance between them, a\nknowledge distillation strategy is proposed for general visual representation\nlearning. It utilizes our well-designed activation map adaptive module to\nreplace some blocks of the teacher network, exploring the most appropriate\nsupervisory features adaptively during the training process. Using the\nteacher's hidden layer output to prompt the student network to train so as to\ntransfer effective semantic information.To verify the effectiveness of our\nstrategy, this paper applied our method to cifar-10 dataset. Results\ndemonstrate that the method can boost the accuracy of the student network by\n0.6% with 6.5% loss reduction, and significantly improve its training speed.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 11:55:13 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Wu", "Zhiyuan", ""], ["Qi", "Hong", ""], ["Jiang", "Yu", ""], ["Zhao", "Minghao", ""], ["Cui", "Chupeng", ""], ["Yang", "Zongmin", ""], ["Xue", "Xinhui", ""]]}, {"id": "2010.13501", "submitter": "Xuelian Cheng", "authors": "Xuelian Cheng, Yiran Zhong, Mehrtash Harandi, Yuchao Dai, Xiaojun\n  Chang, Tom Drummond, Hongdong Li, Zongyuan Ge", "title": "Hierarchical Neural Architecture Search for Deep Stereo Matching", "comments": "Accepted at NeurIPS 2020; Xuelian Cheng and Yiran Zhong made equal\n  contribution", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To reduce the human efforts in neural network design, Neural Architecture\nSearch (NAS) has been applied with remarkable success to various high-level\nvision tasks such as classification and semantic segmentation. The underlying\nidea for the NAS algorithm is straightforward, namely, to enable the network\nthe ability to choose among a set of operations (e.g., convolution with\ndifferent filter sizes), one is able to find an optimal architecture that is\nbetter adapted to the problem at hand. However, so far the success of NAS has\nnot been enjoyed by low-level geometric vision tasks such as stereo matching.\nThis is partly due to the fact that state-of-the-art deep stereo matching\nnetworks, designed by humans, are already sheer in size. Directly applying the\nNAS to such massive structures is computationally prohibitive based on the\ncurrently available mainstream computing resources. In this paper, we propose\nthe first end-to-end hierarchical NAS framework for deep stereo matching by\nincorporating task-specific human knowledge into the neural architecture search\nframework. Specifically, following the gold standard pipeline for deep stereo\nmatching (i.e., feature extraction -- feature volume construction and dense\nmatching), we optimize the architectures of the entire pipeline jointly.\nExtensive experiments show that our searched network outperforms all\nstate-of-the-art deep stereo matching architectures and is ranked at the top 1\naccuracy on KITTI stereo 2012, 2015 and Middlebury benchmarks, as well as the\ntop 1 on SceneFlow dataset with a substantial improvement on the size of the\nnetwork and the speed of inference. The code is available at\nhttps://github.com/XuelianCheng/LEAStereo.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 11:57:37 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Cheng", "Xuelian", ""], ["Zhong", "Yiran", ""], ["Harandi", "Mehrtash", ""], ["Dai", "Yuchao", ""], ["Chang", "Xiaojun", ""], ["Drummond", "Tom", ""], ["Li", "Hongdong", ""], ["Ge", "Zongyuan", ""]]}, {"id": "2010.13508", "submitter": "Anis Kacem", "authors": "Alexandre Saint, Anis Kacem, Kseniya Cherenkova, Konstantinos\n  Papadopoulos, Julian Chibane, Gerard Pons-Moll, Gleb Gusev, David Fofi,\n  Djamila Aouada, and Bjorn Ottersten", "title": "SHARP 2020: The 1st Shape Recovery from Partial Textured 3D Scans\n  Challenge Results", "comments": "SHARP workshop, ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The SHApe Recovery from Partial textured 3D scans challenge, SHARP 2020, is\nthe first edition of a challenge fostering and benchmarking methods for\nrecovering complete textured 3D scans from raw incomplete data. SHARP 2020 is\norganised as a workshop in conjunction with ECCV 2020. There are two\ncomplementary challenges, the first one on 3D human scans, and the second one\non generic objects. Challenge 1 is further split into two tracks, focusing,\nfirst, on large body and clothing regions, and, second, on fine body details. A\nnovel evaluation metric is proposed to quantify jointly the shape\nreconstruction, the texture reconstruction and the amount of completed data.\nAdditionally, two unique datasets of 3D scans are proposed, to provide raw\nground-truth data for the benchmarks. The datasets are released to the\nscientific community. Moreover, an accompanying custom library of software\nroutines is also released to the scientific community. It allows for processing\n3D scans, generating partial data and performing the evaluation. Results of the\ncompetition, analysed in comparison to baselines, show the validity of the\nproposed evaluation metrics, and highlight the challenging aspects of the task\nand of the datasets. Details on the SHARP 2020 challenge can be found at\nhttps://cvi2.uni.lu/sharp2020/.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 12:05:56 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Saint", "Alexandre", ""], ["Kacem", "Anis", ""], ["Cherenkova", "Kseniya", ""], ["Papadopoulos", "Konstantinos", ""], ["Chibane", "Julian", ""], ["Pons-Moll", "Gerard", ""], ["Gusev", "Gleb", ""], ["Fofi", "David", ""], ["Aouada", "Djamila", ""], ["Ottersten", "Bjorn", ""]]}, {"id": "2010.13527", "submitter": "Markus Marks", "authors": "Benjamin Estermann, Markus Marks, Mehmet Fatih Yanik", "title": "Robust Disentanglement of a Few Factors at a Time", "comments": "The first two authors contributed equally. Code is available at this\n  url https://github.com/besterma/robust_disentanglement", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Disentanglement is at the forefront of unsupervised learning, as disentangled\nrepresentations of data improve generalization, interpretability, and\nperformance in downstream tasks. Current unsupervised approaches remain\ninapplicable for real-world datasets since they are highly variable in their\nperformance and fail to reach levels of disentanglement of (semi-)supervised\napproaches. We introduce population-based training (PBT) for improving\nconsistency in training variational autoencoders (VAEs) and demonstrate the\nvalidity of this approach in a supervised setting (PBT-VAE). We then use\nUnsupervised Disentanglement Ranking (UDR) as an unsupervised heuristic to\nscore models in our PBT-VAE training and show how models trained this way tend\nto consistently disentangle only a subset of the generative factors. Building\non top of this observation we introduce the recursive rPU-VAE approach. We\ntrain the model until convergence, remove the learned factors from the dataset\nand reiterate. In doing so, we can label subsets of the dataset with the\nlearned factors and consecutively use these labels to train one model that\nfully disentangles the whole dataset. With this approach, we show striking\nimprovement in state-of-the-art unsupervised disentanglement performance and\nrobustness across multiple datasets and metrics.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 12:34:23 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Estermann", "Benjamin", ""], ["Marks", "Markus", ""], ["Yanik", "Mehmet Fatih", ""]]}, {"id": "2010.13547", "submitter": "Hao Tang", "authors": "Hao Tang, Zhiao Huang, Jiayuan Gu, Bao-Liang Lu, Hao Su", "title": "Towards Scale-Invariant Graph-related Problem Solving by Iterative\n  Homogeneous Graph Neural Networks", "comments": "To appear at NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current graph neural networks (GNNs) lack generalizability with respect to\nscales (graph sizes, graph diameters, edge weights, etc..) when solving many\ngraph analysis problems. Taking the perspective of synthesizing graph theory\nprograms, we propose several extensions to address the issue. First, inspired\nby the dependency of the iteration number of common graph theory algorithms on\ngraph size, we learn to terminate the message passing process in GNNs\nadaptively according to the computation progress. Second, inspired by the fact\nthat many graph theory algorithms are homogeneous with respect to graph\nweights, we introduce homogeneous transformation layers that are universal\nhomogeneous function approximators, to convert ordinary GNNs to be homogeneous.\nExperimentally, we show that our GNN can be trained from small-scale graphs but\ngeneralize well to large-scale graphs for a number of basic graph theory\nproblems. It also shows generalizability for applications of multi-body\nphysical simulation and image-based navigation problems.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 12:57:28 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Tang", "Hao", ""], ["Huang", "Zhiao", ""], ["Gu", "Jiayuan", ""], ["Lu", "Bao-Liang", ""], ["Su", "Hao", ""]]}, {"id": "2010.13572", "submitter": "Alireza M. Javid", "authors": "Alireza M. Javid, Sandipan Das, Mikael Skoglund, and Saikat Chatterjee", "title": "A ReLU Dense Layer to Improve the Performance of Neural Networks", "comments": "Submitted to ICASSP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose ReDense as a simple and low complexity way to improve the\nperformance of trained neural networks. We use a combination of random weights\nand rectified linear unit (ReLU) activation function to add a ReLU dense\n(ReDense) layer to the trained neural network such that it can achieve a lower\ntraining loss. The lossless flow property (LFP) of ReLU is the key to achieve\nthe lower training loss while keeping the generalization error small. ReDense\ndoes not suffer from vanishing gradient problem in the training due to having a\nshallow structure. We experimentally show that ReDense can improve the training\nand testing performance of various neural network architectures with different\noptimization loss and activation functions. Finally, we test ReDense on some of\nthe state-of-the-art architectures and show the performance improvement on\nbenchmark datasets.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 11:56:01 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Javid", "Alireza M.", ""], ["Das", "Sandipan", ""], ["Skoglund", "Mikael", ""], ["Chatterjee", "Saikat", ""]]}, {"id": "2010.13626", "submitter": "Junaid Ahmed Ghauri", "authors": "Junaid Ahmed Ghauri, Sherzod Hakimov and Ralph Ewerth", "title": "Classification of Important Segments in Educational Videos using\n  Multimodal Features", "comments": "Proceedings of the CIKM 2020 Workshops, October 19 to 20, Galway,\n  Ireland", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Videos are a commonly-used type of content in learning during Web search.\nMany e-learning platforms provide quality content, but sometimes educational\nvideos are long and cover many topics. Humans are good in extracting important\nsections from videos, but it remains a significant challenge for computers. In\nthis paper, we address the problem of assigning importance scores to video\nsegments, that is how much information they contain with respect to the overall\ntopic of an educational video. We present an annotation tool and a new dataset\nof annotated educational videos collected from popular online learning\nplatforms. Moreover, we propose a multimodal neural architecture that utilizes\nstate-of-the-art audio, visual and textual features. Our experiments\ninvestigate the impact of visual and temporal information, as well as the\ncombination of multimodal features on importance prediction.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 14:40:23 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Ghauri", "Junaid Ahmed", ""], ["Hakimov", "Sherzod", ""], ["Ewerth", "Ralph", ""]]}, {"id": "2010.13636", "submitter": "Muli Yang", "authors": "Yuehua Zhu, Muli Yang, Cheng Deng, and Wei Liu", "title": "Fewer is More: A Deep Graph Metric Learning Perspective Using Fewer\n  Proxies", "comments": "Accepted in NeurIPS 2020 as a spotlight paper. Code can be found at\n  https://github.com/YuehuaZhu/ProxyGML", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep metric learning plays a key role in various machine learning tasks. Most\nof the previous works have been confined to sampling from a mini-batch, which\ncannot precisely characterize the global geometry of the embedding space.\nAlthough researchers have developed proxy- and classification-based methods to\ntackle the sampling issue, those methods inevitably incur a redundant\ncomputational cost. In this paper, we propose a novel Proxy-based deep Graph\nMetric Learning (ProxyGML) approach from the perspective of graph\nclassification, which uses fewer proxies yet achieves better comprehensive\nperformance. Specifically, multiple global proxies are leveraged to\ncollectively approximate the original data points for each class. To\nefficiently capture local neighbor relationships, a small number of such\nproxies are adaptively selected to construct similarity subgraphs between these\nproxies and each data point. Further, we design a novel reverse label\npropagation algorithm, by which the neighbor relationships are adjusted\naccording to ground-truth labels, so that a discriminative metric space can be\nlearned during the process of subgraph classification. Extensive experiments\ncarried out on widely-used CUB-200-2011, Cars196, and Stanford Online Products\ndatasets demonstrate the superiority of the proposed ProxyGML over the\nstate-of-the-art methods in terms of both effectiveness and efficiency. The\nsource code is publicly available at https://github.com/YuehuaZhu/ProxyGML.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 14:52:42 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Zhu", "Yuehua", ""], ["Yang", "Muli", ""], ["Deng", "Cheng", ""], ["Liu", "Wei", ""]]}, {"id": "2010.13662", "submitter": "Shun-Cheng Wu", "authors": "Shun-Cheng Wu, Keisuke Tateno, Nassir Navab and Federico Tombari", "title": "SCFusion: Real-time Incremental Scene Reconstruction with Semantic\n  Completion", "comments": null, "journal-ref": "International Conference on 3D Vision (2020), 801-810", "doi": "10.1109/3DV50981.2020.00090", "report-no": null, "categories": "cs.CV cs.NE cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-time scene reconstruction from depth data inevitably suffers from\nocclusion, thus leading to incomplete 3D models. Partial reconstructions, in\nturn, limit the performance of algorithms that leverage them for applications\nin the context of, e.g., augmented reality, robotic navigation, and 3D mapping.\nMost methods address this issue by predicting the missing geometry as an\noffline optimization, thus being incompatible with real-time applications. We\npropose a framework that ameliorates this issue by performing scene\nreconstruction and semantic scene completion jointly in an incremental and\nreal-time manner, based on an input sequence of depth maps. Our framework\nrelies on a novel neural architecture designed to process occupancy maps and\nleverages voxel states to accurately and efficiently fuse semantic completion\nwith the 3D global model. We evaluate the proposed approach quantitatively and\nqualitatively, demonstrating that our method can obtain accurate 3D semantic\nscene completion in real-time.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 15:31:52 GMT"}, {"version": "v2", "created": "Tue, 2 Mar 2021 11:40:28 GMT"}, {"version": "v3", "created": "Wed, 31 Mar 2021 08:03:44 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Wu", "Shun-Cheng", ""], ["Tateno", "Keisuke", ""], ["Navab", "Nassir", ""], ["Tombari", "Federico", ""]]}, {"id": "2010.13676", "submitter": "Radu P Horaud", "authors": "Zhiqi Kang, Mostafa Sadeghi and Radu Horaud", "title": "Face Frontalization Based on Robustly Fitting a Deformable Shape Model\n  to 3D Landmarks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face frontalization consists of synthesizing a frontally-viewed face from an\narbitrarily-viewed one. The main contribution of this paper is a robust face\nalignment method that enables pixel-to-pixel warping. The method simultaneously\nestimates the rigid transformation (scale, rotation, and translation) and the\nnon-rigid deformation between two 3D point sets: a set of 3D landmarks\nextracted from an arbitrary-viewed face, and a set of 3D landmarks\nparameterized by a frontally-viewed deformable face model. An important merit\nof the proposed method is its ability to deal both with noise (small\nperturbations) and with outliers (large errors). We propose to model inliers\nand outliers with the generalized Student's t-probability distribution\nfunction, a heavy-tailed distribution that is immune to non-Gaussian errors in\nthe data. We describe in detail the associated expectation-maximization (EM)\nalgorithm that alternates between the estimation of (i) the rigid parameters,\n(ii) the deformation parameters, and (iii) the Student-t distribution\nparameters. We also propose to use the zero-mean normalized cross-correlation,\nbetween a frontalized face and the corresponding ground-truth frontally-viewed\nface, to evaluate the performance of frontalization. To this end, we use a\ndataset that contains pairs of profile-viewed and frontally-viewed faces. This\nevaluation, based on direct image-to-image comparison, stands in contrast with\nindirect evaluation, based on analyzing the effect of frontalization on face\nrecognition.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 15:52:50 GMT"}, {"version": "v2", "created": "Wed, 10 Mar 2021 10:45:41 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Kang", "Zhiqi", ""], ["Sadeghi", "Mostafa", ""], ["Horaud", "Radu", ""]]}, {"id": "2010.13677", "submitter": "Wenqi Huang", "authors": "Wenqi Huang, Ziwen Ke, Zhuo-Xu Cui, Jing Cheng, Zhilang Qiu, Sen Jia,\n  Leslie Ying, Yanjie Zhu, Dong Liang", "title": "Deep Low-rank plus Sparse Network for Dynamic MR Imaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In dynamic magnetic resonance (MR) imaging, low-rank plus sparse (L+S)\ndecomposition, or robust principal component analysis (PCA), has achieved\nstunning performance. However, the selection of the parameters of L+S is\nempirical, and the acceleration rate is limited, which are common failings of\niterative compressed sensing MR imaging (CS-MRI) reconstruction methods. Many\ndeep learning approaches have been proposed to address these issues, but few of\nthem use a low-rank prior. In this paper, a model-based low-rank plus sparse\nnetwork, dubbed L+S-Net, is proposed for dynamic MR reconstruction. In\nparticular, we use an alternating linearized minimization method to solve the\noptimization problem with low-rank and sparse regularization. Learned soft\nsingular value thresholding is introduced to ensure the clear separation of the\nL component and S component. Then, the iterative steps are unrolled into a\nnetwork in which the regularization parameters are learnable. We prove that the\nproposed L+S-Net achieves global convergence under two standard assumptions.\nExperiments on retrospective and prospective cardiac cine datasets show that\nthe proposed model outperforms state-of-the-art CS and existing deep learning\nmethods and has great potential for extremely high acceleration factors (up to\n24x).\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 15:55:24 GMT"}, {"version": "v2", "created": "Fri, 7 May 2021 10:12:33 GMT"}, {"version": "v3", "created": "Tue, 20 Jul 2021 12:51:35 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Huang", "Wenqi", ""], ["Ke", "Ziwen", ""], ["Cui", "Zhuo-Xu", ""], ["Cheng", "Jing", ""], ["Qiu", "Zhilang", ""], ["Jia", "Sen", ""], ["Ying", "Leslie", ""], ["Zhu", "Yanjie", ""], ["Liang", "Dong", ""]]}, {"id": "2010.13701", "submitter": "Sara Casao", "authors": "Sara Casao, Abel Naya, Ana C. Murillo, Eduardo Montijano", "title": "Distributed Multi-Target Tracking in Camera Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most recent works on multi-target tracking with multiple cameras focus on\ncentralized systems. In contrast, this paper presents a multi-target tracking\napproach implemented in a distributed camera network. The advantages of\ndistributed systems lie in lighter communication management, greater robustness\nto failures and local decision making. On the other hand, data association and\ninformation fusion are more challenging than in a centralized setup, mostly due\nto the lack of global and complete information. The proposed algorithm boosts\nthe benefits of the Distributed-Consensus Kalman Filter with the support of a\nre-identification network and a distributed tracker manager module to\nfacilitate consistent information. These techniques complement each other and\nfacilitate the cross-camera data association in a simple and effective manner.\nWe evaluate the whole system with known public data sets under different\nconditions demonstrating the advantages of combining all the modules. In\naddition, we compare our algorithm to some existing centralized tracking\nmethods, outperforming their behavior in terms of accuracy and bandwidth usage.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 16:34:53 GMT"}, {"version": "v2", "created": "Mon, 15 Mar 2021 16:32:16 GMT"}, {"version": "v3", "created": "Fri, 16 Apr 2021 17:57:21 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Casao", "Sara", ""], ["Naya", "Abel", ""], ["Murillo", "Ana C.", ""], ["Montijano", "Eduardo", ""]]}, {"id": "2010.13714", "submitter": "Aitik Gupta", "authors": "Aitik Gupta, Aadit Agarwal", "title": "ActiveNet: A computer-vision based approach to determine lethargy", "comments": "Accepted at The ACM India Joint International Conference on Data\n  Science and Management of Data (CoDS-COMAD) 2021", "journal-ref": null, "doi": "10.1145/3430984.3430986", "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The outbreak of COVID-19 has forced everyone to stay indoors, fabricating a\nsignificant drop in physical activeness. Our work is constructed upon the idea\nto formulate a backbone mechanism, to detect levels of activeness in real-time,\nusing a single monocular image of a target person. The scope can be generalized\nunder many applications, be it in an interview, online classes, security\nsurveillance, et cetera. We propose a Computer Vision based multi-stage\napproach, wherein the pose of a person is first detected, encoded with a novel\napproach, and then assessed by a classical machine learning algorithm to\ndetermine the level of activeness. An alerting system is wrapped around the\napproach to provide a solution to inhibit lethargy by sending notification\nalerts to individuals involved.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 16:54:03 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Gupta", "Aitik", ""], ["Agarwal", "Aadit", ""]]}, {"id": "2010.13715", "submitter": "Pavan Madhusudana", "authors": "Pavan C. Madhusudana, Neil Birkbeck, Yilin Wang, Balu Adsumilli and\n  Alan C. Bovik", "title": "ST-GREED: Space-Time Generalized Entropic Differences for Frame Rate\n  Dependent Video Quality Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of conducting frame rate dependent video quality\nassessment (VQA) on videos of diverse frame rates, including high frame rate\n(HFR) videos. More generally, we study how perceptual quality is affected by\nframe rate, and how frame rate and compression combine to affect perceived\nquality. We devise an objective VQA model called Space-Time GeneRalized\nEntropic Difference (GREED) which analyzes the statistics of spatial and\ntemporal band-pass video coefficients. A generalized Gaussian distribution\n(GGD) is used to model band-pass responses, while entropy variations between\nreference and distorted videos under the GGD model are used to capture video\nquality variations arising from frame rate changes. The entropic differences\nare calculated across multiple temporal and spatial subbands, and merged using\na learned regressor. We show through extensive experiments that GREED achieves\nstate-of-the-art performance on the LIVE-YT-HFR Database when compared with\nexisting VQA models. The features used in GREED are highly generalizable and\nobtain competitive performance even on standard, non-HFR VQA databases. The\nimplementation of GREED has been made available online:\nhttps://github.com/pavancm/GREED\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 16:54:33 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Madhusudana", "Pavan C.", ""], ["Birkbeck", "Neil", ""], ["Wang", "Yilin", ""], ["Adsumilli", "Balu", ""], ["Bovik", "Alan C.", ""]]}, {"id": "2010.13750", "submitter": "Muhamad Risqi U. Saputra", "authors": "Zhuangzhuang Dai, Muhamad Risqi U. Saputra, Chris Xiaoxuan Lu, Niki\n  Trigoni, Andrew Markham", "title": "Demo Abstract: Indoor Positioning System in Visually-Degraded\n  Environments with Millimetre-Wave Radar and Inertial Sensors", "comments": "Appear as demo abstract at the ACM Conference on Embedded Networked\n  Sensor Systems (SenSys 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Positional estimation is of great importance in the public safety sector.\nEmergency responders such as fire fighters, medical rescue teams, and the\npolice will all benefit from a resilient positioning system to deliver safe and\neffective emergency services. Unfortunately, satellite navigation (e.g., GPS)\noffers limited coverage in indoor environments. It is also not possible to rely\non infrastructure based solutions. To this end, wearable sensor-aided\nnavigation techniques, such as those based on camera and Inertial Measurement\nUnits (IMU), have recently emerged recently as an accurate, infrastructure-free\nsolution. Together with an increase in the computational capabilities of mobile\ndevices, motion estimation can be performed in real-time. In this\ndemonstration, we present a real-time indoor positioning system which fuses\nmillimetre-wave (mmWave) radar and IMU data via deep sensor fusion. We employ\nmmWave radar rather than an RGB camera as it provides better robustness to\nvisual degradation (e.g., smoke, darkness, etc.) while at the same time\nrequiring lower computational resources to enable runtime computation. We\nimplemented the sensor system on a handheld device and a mobile computer\nrunning at 10 FPS to track a user inside an apartment. Good accuracy and\nresilience were exhibited even in poorly illuminated scenes.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 17:41:25 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Dai", "Zhuangzhuang", ""], ["Saputra", "Muhamad Risqi U.", ""], ["Lu", "Chris Xiaoxuan", ""], ["Trigoni", "Niki", ""], ["Markham", "Andrew", ""]]}, {"id": "2010.13753", "submitter": "Jes\\'us Ruiz-Santaquiteria", "authors": "Jesus Ruiz-Santaquiteria, Alberto Velasco-Mata, Noelia Vallez, Gloria\n  Bueno, Juan A. \\'Alvarez-Garc\\'ia, Oscar Deniz", "title": "Handgun detection using combined human pose and weapon appearance", "comments": "17 pages, 18 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Closed-circuit television (CCTV) systems are essential nowadays to prevent\nsecurity threats or dangerous situations, in which early detection is crucial.\nNovel deep learning-based methods have allowed to develop automatic weapon\ndetectors with promising results. However, these approaches are mainly based on\nvisual weapon appearance only. For handguns, body pose may be a useful cue,\nespecially in cases where the gun is barely visible. In this work, a novel\nmethod is proposed to combine, in a single architecture, both weapon appearance\nand human pose information. First, pose keypoints are estimated to extract hand\nregions and generate binary pose images, which are the model inputs. Then, each\ninput is processed in different subnetworks and combined to produce the handgun\nbounding box. Results obtained show that the combined model improves the\nhandgun detection state of the art, achieving from 4.23 to 18.9 AP points more\nthan the best previous approach.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 17:45:12 GMT"}, {"version": "v2", "created": "Mon, 18 Jan 2021 12:00:53 GMT"}, {"version": "v3", "created": "Sun, 31 Jan 2021 17:28:41 GMT"}, {"version": "v4", "created": "Fri, 23 Jul 2021 09:55:57 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Ruiz-Santaquiteria", "Jesus", ""], ["Velasco-Mata", "Alberto", ""], ["Vallez", "Noelia", ""], ["Bueno", "Gloria", ""], ["\u00c1lvarez-Garc\u00eda", "Juan A.", ""], ["Deniz", "Oscar", ""]]}, {"id": "2010.13757", "submitter": "Quang Le", "authors": "Quang H. Le, Kamal Youcef-Toumi, Dzmitry Tsetserukou, Ali Jahanian", "title": "GAN Mask R-CNN:Instance semantic segmentation benefits from\n  generativeadversarial networks", "comments": "12 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In designing instance segmentation ConvNets that reconstruct masks,\nsegmentation is often taken as its literal definition -- assigning label to\nevery pixel -- for defining the loss functions. That is, using losses that\ncompute the difference between pixels in the predicted (reconstructed) mask and\nthe ground truth mask -- a template matching mechanism. However, any such\ninstance segmentation ConvNet is a generator, so we can lay the problem of\npredicting masks as a GANs game framework: We can think the ground truth mask\nis drawn from the true distribution, and a ConvNet like Mask R-CNN is an\nimplicit model that infers the true distribution. Then, designing a\ndiscriminator in front of this generator will close the loop of GANs concept\nand more importantly obtains a loss that is trained not hand-designed. We show\nthis design outperforms the baseline when trying on, without extra settings,\nseveral different domains: cellphone recycling, autonomous driving, large-scale\nobject detection, and medical glands. Further, we observe in general GANs yield\nmasks that account for better boundaries, clutter, and small details.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 17:47:30 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Le", "Quang H.", ""], ["Youcef-Toumi", "Kamal", ""], ["Tsetserukou", "Dzmitry", ""], ["Jahanian", "Ali", ""]]}, {"id": "2010.13773", "submitter": "Dongdong Chen", "authors": "Xiaoyi Dong and Dongdong Chen and Jianmin Bao and Chuan Qin and Lu\n  Yuan and Weiming Zhang and Nenghai Yu and Dong Chen", "title": "GreedyFool: Distortion-Aware Sparse Adversarial Attack", "comments": "To appear in NeurIPS 2020, code:\n  https://github.com/LightDXY/GreedyFool", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern deep neural networks(DNNs) are vulnerable to adversarial samples.\nSparse adversarial samples are a special branch of adversarial samples that can\nfool the target model by only perturbing a few pixels. The existence of the\nsparse adversarial attack points out that DNNs are much more vulnerable than\npeople believed, which is also a new aspect for analyzing DNNs. However,\ncurrent sparse adversarial attack methods still have some shortcomings on both\nsparsity and invisibility. In this paper, we propose a novel two-stage\ndistortion-aware greedy-based method dubbed as \"GreedyFool\". Specifically, it\nfirst selects the most effective candidate positions to modify by considering\nboth the gradient(for adversary) and the distortion map(for invisibility), then\ndrops some less important points in the reduce stage. Experiments demonstrate\nthat compared with the start-of-the-art method, we only need to modify\n$3\\times$ fewer pixels under the same sparse perturbation setting. For target\nattack, the success rate of our method is 9.96\\% higher than the\nstart-of-the-art method under the same pixel budget. Code can be found at\nhttps://github.com/LightDXY/GreedyFool.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 17:59:07 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Dong", "Xiaoyi", ""], ["Chen", "Dongdong", ""], ["Bao", "Jianmin", ""], ["Qin", "Chuan", ""], ["Yuan", "Lu", ""], ["Zhang", "Weiming", ""], ["Yu", "Nenghai", ""], ["Chen", "Dong", ""]]}, {"id": "2010.13783", "submitter": "Haruhiro Fujita", "authors": "Haruhiro Fujita, Masatoshi Itagaki, Yew Kwang Hooi, Kenta Ichikawa,\n  Kazutaka Kawano, Ryo Yamamoto", "title": "Detector Algorithms of Bounding Box and Segmentation Mask of a Mask\n  R-CNN Model", "comments": "8 pages, 9 figures, 4 tables. arXiv admin note: text overlap with\n  arXiv:2010.11464", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detection performances on bounding box and segmentation mask outputs of Mask\nR-CNN models are evaluated. There are significant differences in detection\nperformances of bounding boxes and segmentation masks, where the former is\nconstantly superior to the latter. Harmonic values of precisions and recalls of\nlinear cracks, joints, fillings, and shadows are significantly lower in\nsegmentation masks than bounding boxes. Other classes showed similar harmonic\nvalues. Discussions are made on different performances of detection metrics of\nbounding boxes and segmentation masks focusing on detection algorithms of both\ndetectors.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 23:56:41 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Fujita", "Haruhiro", ""], ["Itagaki", "Masatoshi", ""], ["Hooi", "Yew Kwang", ""], ["Ichikawa", "Kenta", ""], ["Kawano", "Kazutaka", ""], ["Yamamoto", "Ryo", ""]]}, {"id": "2010.13821", "submitter": "Jason Yu", "authors": "Jason J. Yu, Konstantinos G. Derpanis, Marcus A. Brubaker", "title": "Wavelet Flow: Fast Training of High Resolution Normalizing Flows", "comments": "Manuscript appendix images compressed with JPEG to meet arXiv size\n  limits. Visit the project page for PNG versions:\n  https://yorkucvil.github.io/Wavelet-Flow", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Normalizing flows are a class of probabilistic generative models which allow\nfor both fast density computation and efficient sampling and are effective at\nmodelling complex distributions like images. A drawback among current methods\nis their significant training cost, sometimes requiring months of GPU training\ntime to achieve state-of-the-art results. This paper introduces Wavelet Flow, a\nmulti-scale, normalizing flow architecture based on wavelets. A Wavelet Flow\nhas an explicit representation of signal scale that inherently includes models\nof lower resolution signals and conditional generation of higher resolution\nsignals, i.e., super resolution. A major advantage of Wavelet Flow is the\nability to construct generative models for high resolution data (e.g., 1024 x\n1024 images) that are impractical with previous models. Furthermore, Wavelet\nFlow is competitive with previous normalizing flows in terms of bits per\ndimension on standard (low resolution) benchmarks while being up to 15x faster\nto train.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 18:13:43 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Yu", "Jason J.", ""], ["Derpanis", "Konstantinos G.", ""], ["Brubaker", "Marcus A.", ""]]}, {"id": "2010.13837", "submitter": "Mikhail Saramud V", "authors": "M V Kubrikov, I A Paulin, M V Saramud and A S Kubrikova", "title": "Application of sequential processing of computer vision methods for\n  solving the problem of detecting the edges of a honeycomb block", "comments": "10 pages, 13 figures, APITECH-II - 2020", "journal-ref": null, "doi": "10.1088/1742-6596/1679/4/042098", "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The article describes the application of the Hough transform to a honeycomb\nblock image. The problem of cutting a mold from a honeycomb block is described.\nA number of image transformations are considered to increase the efficiency of\nthe Hough algorithm. A method for obtaining a binary image using a simple\nthreshold, a method for obtaining a binary image using Otsu binarization, and\nthe Canny Edge Detection algorithm are considered. The method of binary\nskeleton (skeletonization) is considered, in which the skeleton is obtained\nusing 2 main morphological operations: Dilation and Erosion. As a result of a\nnumber of experiments, the optimal sequence of processing the original image\nwas revealed, which allows obtaining the coordinates of the maximum number of\nfaces. This result allows one to choose the optimal places for cutting a\nhoneycomb block, which will improve the quality of the resulting shapes.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 18:48:46 GMT"}], "update_date": "2020-12-30", "authors_parsed": [["Kubrikov", "M V", ""], ["Paulin", "I A", ""], ["Saramud", "M V", ""], ["Kubrikova", "A S", ""]]}, {"id": "2010.13841", "submitter": "Hannes Rost", "authors": "Leon L. Xu, Hannes L. R\\\"ost", "title": "Peak Detection On Data Independent Acquisition Mass Spectrometry Data\n  With Semisupervised Convolutional Transformers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Liquid Chromatography coupled to Mass Spectrometry (LC-MS) based methods are\ncommonly used for high-throughput, quantitative measurements of the proteome\n(i.e. the set of all proteins in a sample at a given time). Targeted LC-MS\nproduces data in the form of a two-dimensional time series spectrum, with the\nmass to charge ratio of analytes (m/z) on one axis, and the retention time from\nthe chromatography on the other. The elution of a peptide of interest produces\nhighly specific patterns across multiple fragment ion traces (extracted ion\nchromatograms, or XICs). In this paper, we formulate this peak detection\nproblem as a multivariate time series segmentation problem, and propose a novel\napproach based on the Transformer architecture. Here we augment Transformers,\nwhich are capable of capturing long distance dependencies with a global view,\nwith Convolutional Neural Networks (CNNs), which can capture local context\nimportant to the task at hand, in the form of Transformers with Convolutional\nSelf-Attention. We further train this model in a semisupervised manner by\nadapting state of the art semisupervised image classification techniques for\nmulti-channel time series data. Experiments on a representative LC-MS dataset\nare benchmarked using manual annotations to showcase the encouraging\nperformance of our method; it outperforms baseline neural network architectures\nand is competitive against the current state of the art in automated peak\ndetection.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 18:55:27 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Xu", "Leon L.", ""], ["R\u00f6st", "Hannes L.", ""]]}, {"id": "2010.13844", "submitter": "Tarik Ayaou", "authors": "Tarik Ayaou, Azeddine Beghdadi, Karim Afdel, Abdellah Amghar", "title": "Enhancing road signs segmentation using photometric invariants", "comments": "5 pages, Submitted to the Computing Conference 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Road signs detection and recognition in natural scenes is one of the most\nimportant tasksin the design of Intelligent Transport Systems (ITS). However,\nillumination changes remain a major problem. In this paper, an efficient\nap-proach of road signs segmentation based on photometric invariants is\nproposed. This method is based on color in-formation using a hybrid distance,\nby exploiting the chro-matic distance and the red and blue ratio, on l Theta\nPhi color space which is invariant to highlight, shading and shadow changes. A\ncomparative study is performed to demonstrate the robustness of this approach\nover the most frequently used methods for road sign segmentation. The\nexperimental results and the detailed analysis show the high performance of the\nalgorithm described in this paper.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 18:59:06 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Ayaou", "Tarik", ""], ["Beghdadi", "Azeddine", ""], ["Afdel", "Karim", ""], ["Amghar", "Abdellah", ""]]}, {"id": "2010.13850", "submitter": "Alexander Olson", "authors": "Alexander W Olson, Andreea Cucu, Tom Bock", "title": "Multi-Class Zero-Shot Learning for Artistic Material Recognition", "comments": "9 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zero-Shot Learning (ZSL) is an extreme form of transfer learning, where no\nlabelled examples of the data to be classified are provided during the training\nstage. Instead, ZSL uses additional information learned about the domain, and\nrelies upon transfer learning algorithms to infer knowledge about the missing\ninstances. ZSL approaches are an attractive solution for sparse datasets. Here\nwe outline a model to identify the materials with which a work of art was\ncreated, by learning the relationship between English descriptions of the\nsubject of a piece and its composite materials. After experimenting with a\nrange of hyper-parameters, we produce a model which is capable of correctly\nidentifying the materials used on pieces from an entirely distinct museum\ndataset. This model returned a classification accuracy of 48.42% on 5,000\nartworks taken from the Tate collection, which is distinct from the Rijksmuseum\nnetwork used to create and train our model.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 19:04:50 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Olson", "Alexander W", ""], ["Cucu", "Andreea", ""], ["Bock", "Tom", ""]]}, {"id": "2010.13864", "submitter": "Vivien Cabannes", "authors": "Vivien Cabannes and Thomas Kerdreux and Louis Thiry", "title": "Diptychs of human and machine perceptions", "comments": "7 pages, 36 images", "journal-ref": "creativity workshop NeurIPS 2020", "doi": null, "report-no": null, "categories": "cs.GR cs.AI cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose visual creations that put differences in algorithms and humans\n\\emph{perceptions} into perspective. We exploit saliency maps of neural\nnetworks and visual focus of humans to create diptychs that are\nreinterpretations of an original image according to both machine and human\nattentions. Using those diptychs as a qualitative evaluation of perception, we\ndiscuss some crucial issues of current \\textit{task-oriented} artificial\nintelligence.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 10:22:28 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Cabannes", "Vivien", ""], ["Kerdreux", "Thomas", ""], ["Thiry", "Louis", ""]]}, {"id": "2010.13868", "submitter": "Burhaneddin Yaman", "authors": "Burhaneddin Yaman, Seyed Amir Hossein Hosseini, Steen Moeller and\n  Mehmet Ak\\c{c}akaya", "title": "Improved Supervised Training of Physics-Guided Deep Learning Image\n  Reconstruction with Multi-Masking", "comments": null, "journal-ref": "Proceedings of IEEE ICASSP, 2021", "doi": "10.1109/ICASSP39728.2021.9413495", "report-no": null, "categories": "eess.IV cs.CV cs.LG eess.SP physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Physics-guided deep learning (PG-DL) via algorithm unrolling has received\nsignificant interest for improved image reconstruction, including MRI\napplications. These methods unroll an iterative optimization algorithm into a\nseries of regularizer and data consistency units. The unrolled networks are\ntypically trained end-to-end using a supervised approach. Current supervised\nPG-DL approaches use all of the available sub-sampled measurements in their\ndata consistency units. Thus, the network learns to fit the rest of the\nmeasurements. In this study, we propose to improve the performance and\nrobustness of supervised training by utilizing randomness by retrospectively\nselecting only a subset of all the available measurements for data consistency\nunits. The process is repeated multiple times using different random masks\nduring training for further enhancement. Results on knee MRI show that the\nproposed multi-mask supervised PG-DL enhances reconstruction performance\ncompared to conventional supervised PG-DL approaches.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 19:39:32 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Yaman", "Burhaneddin", ""], ["Hosseini", "Seyed Amir Hossein", ""], ["Moeller", "Steen", ""], ["Ak\u00e7akaya", "Mehmet", ""]]}, {"id": "2010.13914", "submitter": "Tomasz Danel", "authors": "Tomasz Danel, Marek \\'Smieja, {\\L}ukasz Struski, Przemys{\\l}aw Spurek,\n  {\\L}ukasz Maziarka", "title": "Processing of incomplete images by (graph) convolutional neural networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the problem of training neural networks from incomplete images\nwithout replacing missing values. For this purpose, we first represent an image\nas a graph, in which missing pixels are entirely ignored. The graph image\nrepresentation is processed using a spatial graph convolutional network (SGCN)\n-- a type of graph convolutional networks, which is a proper generalization of\nclassical CNNs operating on images. On one hand, our approach avoids the\nproblem of missing data imputation while, on the other hand, there is a natural\ncorrespondence between CNNs and SGCN. Experiments confirm that our approach\nperforms better than analogical CNNs with the imputation of missing values on\ntypical classification and reconstruction tasks.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 21:40:03 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Danel", "Tomasz", ""], ["\u015amieja", "Marek", ""], ["Struski", "\u0141ukasz", ""], ["Spurek", "Przemys\u0142aw", ""], ["Maziarka", "\u0141ukasz", ""]]}, {"id": "2010.13938", "submitter": "Julian Chibane", "authors": "Julian Chibane, Aymen Mir, Gerard Pons-Moll", "title": "Neural Unsigned Distance Fields for Implicit Function Learning", "comments": "Neural Information Processing Systems (NeurIPS) 2020", "journal-ref": "Neural Information Processing Systems (NeurIPS) 2020", "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we target a learnable output representation that allows\ncontinuous, high resolution outputs of arbitrary shape. Recent works represent\n3D surfaces implicitly with a Neural Network, thereby breaking previous\nbarriers in resolution, and ability to represent diverse topologies. However,\nneural implicit representations are limited to closed surfaces, which divide\nthe space into inside and outside. Many real world objects such as walls of a\nscene scanned by a sensor, clothing, or a car with inner structures are not\nclosed. This constitutes a significant barrier, in terms of data pre-processing\n(objects need to be artificially closed creating artifacts), and the ability to\noutput open surfaces. In this work, we propose Neural Distance Fields (NDF), a\nneural network based model which predicts the unsigned distance field for\narbitrary 3D shapes given sparse point clouds. NDF represent surfaces at high\nresolutions as prior implicit models, but do not require closed surface data,\nand significantly broaden the class of representable shapes in the output. NDF\nallow to extract the surface as very dense point clouds and as meshes. We also\nshow that NDF allow for surface normal calculation and can be rendered using a\nslight modification of sphere tracing. We find NDF can be used for multi-target\nregression (multiple outputs for one input) with techniques that have been\nexclusively used for rendering in graphics. Experiments on ShapeNet show that\nNDF, while simple, is the state-of-the art, and allows to reconstruct shapes\nwith inner structures, such as the chairs inside a bus. Notably, we show that\nNDF are not restricted to 3D shapes, and can approximate more general open\nsurfaces such as curves, manifolds, and functions. Code is available for\nresearch at https://virtualhumans.mpi-inf.mpg.de/ndf/.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 22:49:45 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Chibane", "Julian", ""], ["Mir", "Aymen", ""], ["Pons-Moll", "Gerard", ""]]}, {"id": "2010.13954", "submitter": "Jianfeng Wu", "authors": "Gang Wang, Qunxi Dong, Jianfeng Wu, Yi Su, Kewei Chen, Qingtang Su,\n  Xiaofeng Zhang, Jinguang Hao, Tao Yao, Li Liu, Caiming Zhang, Richard J\n  Caselli, Eric M Reiman, Yalin Wang", "title": "Developing Univariate Neurodegeneration Biomarkers with Low-Rank and\n  Sparse Subspace Decomposition", "comments": "Accepted by Medical Image Analysis", "journal-ref": null, "doi": "10.1016/j.media.2020.101877", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cognitive decline due to Alzheimer's disease (AD) is closely associated with\nbrain structure alterations captured by structural magnetic resonance imaging\n(sMRI). It supports the validity to develop sMRI-based univariate\nneurodegeneration biomarkers (UNB). However, existing UNB work either fails to\nmodel large group variances or does not capture AD dementia (ADD) induced\nchanges. We propose a novel low-rank and sparse subspace decomposition method\ncapable of stably quantifying the morphological changes induced by ADD.\nSpecifically, we propose a numerically efficient rank minimization mechanism to\nextract group common structure and impose regularization constraints to encode\nthe original 3D morphometry connectivity. Further, we generate\nregions-of-interest (ROI) with group difference study between common subspaces\nof $A\\beta+$ AD and $A\\beta-$ cognitively unimpaired (CU) groups. A univariate\nmorphometry index (UMI) is constructed from these ROIs by summarizing\nindividual morphological characteristics weighted by normalized difference\nbetween $A\\beta+$ AD and $A\\beta-$ CU groups. We use hippocampal surface radial\ndistance feature to compute the UMIs and validate our work in the Alzheimer's\nDisease Neuroimaging Initiative (ADNI) cohort. With hippocampal UMIs, the\nestimated minimum sample sizes needed to detect a 25$\\%$ reduction in the mean\nannual change with 80$\\%$ power and two-tailed $P=0.05$ are 116, 279 and 387\nfor the longitudinal $A\\beta+$ AD, $A\\beta+$ mild cognitive impairment (MCI)\nand $A\\beta+$ CU groups, respectively. Additionally, for MCI patients, UMIs\nwell correlate with hazard ratio of conversion to AD ($4.3$, $95\\%$\nCI=$2.3-8.2$) within 18 months. Our experimental results outperform traditional\nhippocampal volume measures and suggest the application of UMI as a potential\nUNB.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 23:42:43 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Wang", "Gang", ""], ["Dong", "Qunxi", ""], ["Wu", "Jianfeng", ""], ["Su", "Yi", ""], ["Chen", "Kewei", ""], ["Su", "Qingtang", ""], ["Zhang", "Xiaofeng", ""], ["Hao", "Jinguang", ""], ["Yao", "Tao", ""], ["Liu", "Li", ""], ["Zhang", "Caiming", ""], ["Caselli", "Richard J", ""], ["Reiman", "Eric M", ""], ["Wang", "Yalin", ""]]}, {"id": "2010.13957", "submitter": "Kate Rakelly", "authors": "Tony Z. Zhao, Anusha Nagabandi, Kate Rakelly, Chelsea Finn, Sergey\n  Levine", "title": "MELD: Meta-Reinforcement Learning from Images via Latent State Models", "comments": "Accepted to CoRL 2020. Supplementary material at\n  https://sites.google.com/view/meld-lsm/home . 16 pages, 19 figures. V2: add\n  funding acknowledgements, reduce file size", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Meta-reinforcement learning algorithms can enable autonomous agents, such as\nrobots, to quickly acquire new behaviors by leveraging prior experience in a\nset of related training tasks. However, the onerous data requirements of\nmeta-training compounded with the challenge of learning from sensory inputs\nsuch as images have made meta-RL challenging to apply to real robotic systems.\nLatent state models, which learn compact state representations from a sequence\nof observations, can accelerate representation learning from visual inputs. In\nthis paper, we leverage the perspective of meta-learning as task inference to\nshow that latent state models can \\emph{also} perform meta-learning given an\nappropriately defined observation space. Building on this insight, we develop\nmeta-RL with latent dynamics (MELD), an algorithm for meta-RL from images that\nperforms inference in a latent state model to quickly acquire new skills given\nobservations and rewards. MELD outperforms prior meta-RL methods on several\nsimulated image-based robotic control problems, and enables a real WidowX\nrobotic arm to insert an Ethernet cable into new locations given a sparse task\ncompletion signal after only $8$ hours of real world meta-training. To our\nknowledge, MELD is the first meta-RL algorithm trained in a real-world robotic\ncontrol setting from images.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 23:50:30 GMT"}, {"version": "v2", "created": "Mon, 11 Jan 2021 16:09:19 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Zhao", "Tony Z.", ""], ["Nagabandi", "Anusha", ""], ["Rakelly", "Kate", ""], ["Finn", "Chelsea", ""], ["Levine", "Sergey", ""]]}, {"id": "2010.13967", "submitter": "Carlo Russo", "authors": "Carlo Russo, Sidong Liu, Antonio Di Ieva", "title": "Impact of Spherical Coordinates Transformation Pre-processing in Deep\n  Convolution Neural Networks for Brain Tumor Segmentation and Survival\n  Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pre-processing and Data Augmentation play an important role in Deep\nConvolutional Neural Networks (DCNN). Whereby several methods aim for\nstandardization and augmentation of the dataset, we here propose a novel method\naimed to feed DCNN with spherical space transformed input data that could\nbetter facilitate feature learning compared to standard Cartesian space images\nand volumes. In this work, the spherical coordinates transformation has been\napplied as a preprocessing method that, used in conjunction with normal MRI\nvolumes, improves the accuracy of brain tumor segmentation and patient overall\nsurvival (OS) prediction on Brain Tumor Segmentation (BraTS) Challenge 2020\ndataset. The LesionEncoder framework has been then applied to automatically\nextract features from DCNN models, achieving 0.586 accuracy of OS prediction on\nthe validation data set, which is one of the best results according to BraTS\n2020 leaderboard.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 00:33:03 GMT"}, {"version": "v2", "created": "Mon, 23 Nov 2020 00:56:25 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Russo", "Carlo", ""], ["Liu", "Sidong", ""], ["Di Ieva", "Antonio", ""]]}, {"id": "2010.13974", "submitter": "Changhoon Kim", "authors": "Changhoon Kim, Yi Ren, Yezhou Yang", "title": "Decentralized Attribution of Generative Models", "comments": "16 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Growing applications of generative models have led to new threats such as\nmalicious personation and digital copyright infringement. One solution to these\nthreats is model attribution, i.e., the identification of user-end models where\nthe contents under question are generated from. Existing studies showed\nempirical feasibility of attribution through a centralized classifier trained\non all user-end models. However, this approach is not scalable in reality as\nthe number of models ever grows. Neither does it provide an attributability\nguarantee. To this end, this paper studies decentralized attribution, which\nrelies on binary classifiers associated with each user-end model. Each binary\nclassifier is parameterized by a user-specific key and distinguishes its\nassociated model distribution from the authentic data distribution. We develop\nsufficient conditions of the keys that guarantee an attributability lower\nbound. Our method is validated on MNIST, CelebA, and FFHQ datasets. We also\nexamine the trade-off between generation quality and robustness of attribution\nagainst adversarial post-processes.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 01:03:45 GMT"}, {"version": "v2", "created": "Sun, 13 Dec 2020 07:02:58 GMT"}, {"version": "v3", "created": "Mon, 29 Mar 2021 03:29:41 GMT"}, {"version": "v4", "created": "Wed, 28 Apr 2021 13:04:51 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Kim", "Changhoon", ""], ["Ren", "Yi", ""], ["Yang", "Yezhou", ""]]}, {"id": "2010.14014", "submitter": "Yu Shen", "authors": "Yu Shen, Sijie Zhu, Taojiannan Yang, Chen Chen", "title": "Cross-directional Feature Fusion Network for Building Damage Assessment\n  from Satellite Imagery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fast and effective responses are required when a natural disaster (e.g.,\nearthquake, hurricane, etc.) strikes. Building damage assessment from satellite\nimagery is critical before an effective response is conducted. High-resolution\nsatellite images provide rich information with pre- and post-disaster scenes\nfor analysis. However, most existing works simply use pre- and post-disaster\nimages as input without considering their correlations. In this paper, we\npropose a novel cross-directional fusion strategy to better explore the\ncorrelations between pre- and post-disaster images. Moreover, the data\naugmentation method CutMix is exploited to tackle the challenge of hard\nclasses. The proposed method achieves state-of-the-art performance on a\nlarge-scale building damage assessment dataset -- xBD.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 02:49:52 GMT"}, {"version": "v2", "created": "Wed, 18 Nov 2020 19:07:27 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Shen", "Yu", ""], ["Zhu", "Sijie", ""], ["Yang", "Taojiannan", ""], ["Chen", "Chen", ""]]}, {"id": "2010.14036", "submitter": "Yu Sun", "authors": "Yu Sun and Qian Bao and Wu Liu and Wenpeng Gao and Yili Fu and Chuang\n  Gan and Tao Mei", "title": "Synthetic Training for Monocular Human Mesh Recovery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recovering 3D human mesh from monocular images is a popular topic in computer\nvision and has a wide range of applications. This paper aims to estimate 3D\nmesh of multiple body parts (e.g., body, hands) with large-scale differences\nfrom a single RGB image. Existing methods are mostly based on iterative\noptimization, which is very time-consuming. We propose to train a single-shot\nmodel to achieve this goal. The main challenge is lacking training data that\nhave complete 3D annotations of all body parts in 2D images. To solve this\nproblem, we design a multi-branch framework to disentangle the regression of\ndifferent body properties, enabling us to separate each component's training in\na synthetic training manner using unpaired data available. Besides, to\nstrengthen the generalization ability, most existing methods have used\nin-the-wild 2D pose datasets to supervise the estimated 3D pose via 3D-to-2D\nprojection. However, we observe that the commonly used weak-perspective model\nperforms poorly in dealing with the external foreshortening effect of camera\nprojection. Therefore, we propose a depth-to-scale (D2S) projection to\nincorporate the depth difference into the projection function to derive\nper-joint scale variants for more proper supervision. The proposed method\noutperforms previous methods on the CMU Panoptic Studio dataset according to\nthe evaluation results and achieves comparable results on the Human3.6M body\nand STB hand benchmarks. More impressively, the performance in close shot\nimages gets significantly improved using the proposed D2S projection for weak\nsupervision, while maintains obvious superiority in computational efficiency.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 03:31:35 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Sun", "Yu", ""], ["Bao", "Qian", ""], ["Liu", "Wu", ""], ["Gao", "Wenpeng", ""], ["Fu", "Yili", ""], ["Gan", "Chuang", ""], ["Mei", "Tao", ""]]}, {"id": "2010.14076", "submitter": "Luanxuan Hou", "authors": "Luanxuan Hou, Jie Cao, Yuan Zhao, Haifeng Shen, Jian Tang, Ran He", "title": "$P^2$ Net: Augmented Parallel-Pyramid Net for Attention Guided Pose\n  Estimation", "comments": "Accepted by ICPR2020. arXiv admin note: text overlap with\n  arXiv:2003.07516", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an augmented Parallel-Pyramid Net ($P^2~Net$) with feature\nrefinement by dilated bottleneck and attention module. During data\npreprocessing, we proposed a differentiable auto data augmentation ($DA^2$)\nmethod. We formulate the problem of searching data augmentaion policy in a\ndifferentiable form, so that the optimal policy setting can be easily updated\nby back propagation during training. $DA^2$ improves the training efficiency. A\nparallel-pyramid structure is followed to compensate the information loss\nintroduced by the network. We innovate two fusion structures, i.e. Parallel\nFusion and Progressive Fusion, to process pyramid features from backbone\nnetwork. Both fusion structures leverage the advantages of spatial information\naffluence at high resolution and semantic comprehension at low resolution\neffectively. We propose a refinement stage for the pyramid features to further\nboost the accuracy of our network. By introducing dilated bottleneck and\nattention module, we increase the receptive field for the features with limited\ncomplexity and tune the importance to different feature channels. To further\nrefine the feature maps after completion of feature extraction stage, an\nAttention Module ($AM$) is defined to extract weighted features from different\nscale feature maps generated by the parallel-pyramid structure. Compared with\nthe traditional up-sampling refining, $AM$ can better capture the relationship\nbetween channels. Experiments corroborate the effectiveness of our proposed\nmethod. Notably, our method achieves the best performance on the challenging\nMSCOCO and MPII datasets.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 02:10:12 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Hou", "Luanxuan", ""], ["Cao", "Jie", ""], ["Zhao", "Yuan", ""], ["Shen", "Haifeng", ""], ["Tang", "Jian", ""], ["He", "Ran", ""]]}, {"id": "2010.14091", "submitter": "Jianjia Zhang", "authors": "Jianjia Zhang", "title": "Triple-view Convolutional Neural Networks for COVID-19 Diagnosis with\n  Chest X-ray", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Coronavirus Disease 2019 (COVID-19) is affecting increasingly large\nnumber of people worldwide, posing significant stress to the health care\nsystems. Early and accurate diagnosis of COVID-19 is critical in screening of\ninfected patients and breaking the person-to-person transmission. Chest X-ray\n(CXR) based computer-aided diagnosis of COVID-19 using deep learning becomes a\npromising solution to this end. However, the diverse and various radiographic\nfeatures of COVID-19 make it challenging, especially when considering each CXR\nscan typically only generates one single image. Data scarcity is another issue\nsince collecting large-scale medical CXR data set could be difficult at\npresent. Therefore, how to extract more informative and relevant features from\nthe limited samples available becomes essential. To address these issues,\nunlike traditional methods processing each CXR image from a single view, this\npaper proposes triple-view convolutional neural networks for COVID-19 diagnosis\nwith CXR images. Specifically, the proposed networks extract individual\nfeatures from three views of each CXR image, i.e., the left lung view, the\nright lung view and the overall view, in three streams and then integrate them\nfor joint diagnosis. The proposed network structure respects the anatomical\nstructure of human lungs and is well aligned with clinical diagnosis of\nCOVID-19 in practice. In addition, the labeling of the views does not require\nexperts' domain knowledge, which is needed by many existing methods. The\nexperimental results show that the proposed method achieves state-of-the-art\nperformance, especially in the more challenging three class classification\ntask, and admits wide generality and high flexibility.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 06:15:32 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Zhang", "Jianjia", ""]]}, {"id": "2010.14095", "submitter": "Aisha Urooj", "authors": "Aisha Urooj Khan, Amir Mazaheri, Niels da Vitoria Lobo, Mubarak Shah", "title": "MMFT-BERT: Multimodal Fusion Transformer with BERT Encodings for Visual\n  Question Answering", "comments": "Accepted at Findings of EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present MMFT-BERT(MultiModal Fusion Transformer with BERT encodings), to\nsolve Visual Question Answering (VQA) ensuring individual and combined\nprocessing of multiple input modalities. Our approach benefits from processing\nmultimodal data (video and text) adopting the BERT encodings individually and\nusing a novel transformer-based fusion method to fuse them together. Our method\ndecomposes the different sources of modalities, into different BERT instances\nwith similar architectures, but variable weights. This achieves SOTA results on\nthe TVQA dataset. Additionally, we provide TVQA-Visual, an isolated diagnostic\nsubset of TVQA, which strictly requires the knowledge of visual (V) modality\nbased on a human annotator's judgment. This set of questions helps us to study\nthe model's behavior and the challenges TVQA poses to prevent the achievement\nof super human performance. Extensive experiments show the effectiveness and\nsuperiority of our method.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 06:34:14 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Khan", "Aisha Urooj", ""], ["Mazaheri", "Amir", ""], ["Lobo", "Niels da Vitoria", ""], ["Shah", "Mubarak", ""]]}, {"id": "2010.14100", "submitter": "Wei Zhang", "authors": "W. Zhang, H. Liu, P. Li and L. Han", "title": "A Multi-task Two-stream Spatiotemporal Convolutional Neural Network for\n  Convective Storm Nowcasting", "comments": "14 pages, 7 figures, 3 tables", "journal-ref": "2020 IEEE International Conference on Big Data (Big Data), 2020,\n  pp. 3953-3960", "doi": "10.1109/BigData50022.2020.9377890", "report-no": null, "categories": "cs.CV physics.ao-ph physics.geo-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of convective storm nowcasting is local prediction of severe and\nimminent convective storms. Here, we consider the convective storm nowcasting\nproblem from the perspective of machine learning. First, we use a pixel-wise\nsampling method to construct spatiotemporal features for nowcasting, and\nflexibly adjust the proportions of positive and negative samples in the\ntraining set to mitigate class-imbalance issues. Second, we employ a concise\ntwo-stream convolutional neural network to extract spatial and temporal cues\nfor nowcasting. This simplifies the network structure, reduces the training\ntime requirement, and improves classification accuracy. The two-stream network\nused both radar and satellite data. In the resulting two-stream, fused\nconvolutional neural network, some of the parameters are entered into a\nsingle-stream convolutional neural network, but it can learn the features of\nmany data. Further, considering the relevance of classification and regression\ntasks, we develop a multi-task learning strategy that predicts the labels used\nin such tasks. We integrate two-stream multi-task learning into a single\nconvolutional neural network. Given the compact architecture, this network is\nmore efficient and easier to optimize than existing recurrent neural networks.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 07:11:53 GMT"}, {"version": "v2", "created": "Thu, 1 Jul 2021 13:29:18 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Zhang", "W.", ""], ["Liu", "H.", ""], ["Li", "P.", ""], ["Han", "L.", ""]]}, {"id": "2010.14104", "submitter": "Bj\\\"orn Bebensee", "authors": "Bj\\\"orn Bebensee, Byoung-Tak Zhang", "title": "Co-attentional Transformers for Story-Based Video Understanding", "comments": "10 pages, 2 figures, submitted to ICASSP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by recent trends in vision and language learning, we explore\napplications of attention mechanisms for visio-lingual fusion within an\napplication to story-based video understanding. Like other video-based QA\ntasks, video story understanding requires agents to grasp complex temporal\ndependencies. However, as it focuses on the narrative aspect of video it also\nrequires understanding of the interactions between different characters, as\nwell as their actions and their motivations. We propose a novel co-attentional\ntransformer model to better capture long-term dependencies seen in visual\nstories such as dramas and measure its performance on the video question\nanswering task. We evaluate our approach on the recently introduced DramaQA\ndataset which features character-centered video story understanding questions.\nOur model outperforms the baseline model by 8 percentage points overall, at\nleast 4.95 and up to 12.8 percentage points on all difficulty levels and\nmanages to beat the winner of the DramaQA challenge.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 07:17:09 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Bebensee", "Bj\u00f6rn", ""], ["Zhang", "Byoung-Tak", ""]]}, {"id": "2010.14105", "submitter": "Hongwei Li", "authors": "Hongwei Li, Rameshwara G. N. Prasad, Anjany Sekuboyina, Chen Niu,\n  Siwei Bai, Werner Hemmert, and Bjoern Menze", "title": "Micro-CT Synthesis and Inner Ear Super Resolution via Generative\n  Adversarial Networks and Bayesian Inference", "comments": "final version in ISBI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing medical image super-resolution methods rely on pairs of low- and\nhigh- resolution images to learn a mapping in a fully supervised manner.\nHowever, such image pairs are often not available in clinical practice. In this\npaper, we address super-resolution problem in a real-world scenario using\nunpaired data and synthesize linearly \\textbf{eight times} higher resolved\nMicro-CT images of temporal bone structure, which is embedded in the inner ear.\nWe explore cycle-consistency generative adversarial networks for\nsuper-resolution task and equip the translation approach with Bayesian\ninference. We further introduce \\emph{Hu Moment distance} the evaluation metric\nto quantify the shape of the temporal bone. We evaluate our method on a public\ninner ear CT dataset and have seen both visual and quantitative improvement\nover state-of-the-art deep-learning-based methods. In addition, we perform a\nmulti-rater visual evaluation experiment and find that trained experts\nconsistently rate the proposed method the highest quality scores among all\nmethods. Furthermore, we are able to quantify uncertainty in the unpaired\ntranslation task and the uncertainty map can provide structural information of\nthe temporal bone.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 07:18:34 GMT"}, {"version": "v2", "created": "Thu, 4 Feb 2021 23:25:19 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["Li", "Hongwei", ""], ["Prasad", "Rameshwara G. N.", ""], ["Sekuboyina", "Anjany", ""], ["Niu", "Chen", ""], ["Bai", "Siwei", ""], ["Hemmert", "Werner", ""], ["Menze", "Bjoern", ""]]}, {"id": "2010.14119", "submitter": "Elsa Hu", "authors": "Meiqi Hu, Chen Wu, Liangpei Zhang, and Bo Du", "title": "Hyperspectral Anomaly Change Detection Based on Auto-encoder", "comments": "11 pages,9 figures,3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the hyperspectral imaging technology, hyperspectral data provides\nabundant spectral information and plays a more important role in geological\nsurvey, vegetation analysis and military reconnaissance. Different from normal\nchange detection, hyperspectral anomaly change detection (HACD) helps to find\nthose small but important anomaly changes between multi-temporal hyperspectral\nimages (HSI). In previous works, most classical methods use linear regression\nto establish the mapping relationship between two HSIs and then detect the\nanomalies from the residual image. However, the real spectral differences\nbetween multi-temporal HSIs are likely to be quite complex and of nonlinearity,\nleading to the limited performance of these linear predictors. In this paper,\nwe propose an original HACD algorithm based on auto-encoder (ACDA) to give a\nnonlinear solution. The proposed ACDA can construct an effective predictor\nmodel when facing complex imaging conditions. In the ACDA model, two systematic\nauto-encoder (AE) networks are deployed to construct two predictors from two\ndirections. The predictor is used to model the spectral variation of the\nbackground to obtain the predicted image under another imaging condition. Then\nmean square error (MSE) between the predictive image and corresponding expected\nimage is computed to obtain the loss map, where the spectral differences of the\nunchanged pixels are highly suppressed and anomaly changes are highlighted.\nUltimately, we take the minimum of the two loss maps of two directions as the\nfinal anomaly change intensity map. The experiments results on public\n\"Viareggio 2013\" datasets demonstrate the efficiency and superiority over\ntraditional methods.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 08:07:08 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Hu", "Meiqi", ""], ["Wu", "Chen", ""], ["Zhang", "Liangpei", ""], ["Du", "Bo", ""]]}, {"id": "2010.14129", "submitter": "Yang Yu", "authors": "Yang Yu, Rongrong Ni and Yao Zhao", "title": "Mining Generalized Features for Detecting AI-Manipulated Fake Faces", "comments": "14 pages, 9 figures. This work has been submitted to the IEEE for\n  possible publication. Copyright may be transferred without notice, after\n  which this version may no longer be accessible", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, AI-manipulated face techniques have developed rapidly and\nconstantly, which has raised new security issues in society. Although existing\ndetection methods consider different categories of fake faces, the performance\non detecting the fake faces with \"unseen\" manipulation techniques is still poor\ndue to the distribution bias among cross-manipulation techniques. To solve this\nproblem, we propose a novel framework that focuses on mining intrinsic features\nand further eliminating the distribution bias to improve the generalization\nability. Firstly, we focus on mining the intrinsic clues in the channel\ndifference image (CDI) and spectrum image (SI) from the camera imaging process\nand the indispensable step in AI manipulation process. Then, we introduce the\nOctave Convolution (OctConv) and an attention-based fusion module to\neffectively and adaptively mine intrinsic features from CDI and SI. Finally, we\ndesign an alignment module to eliminate the bias of manipulation techniques to\nobtain a more generalized detection framework. We evaluate the proposed\nframework on four categories of fake faces datasets with the most popular and\nstate-of-the-art manipulation techniques, and achieve very competitive\nperformances. To further verify the generalization ability of the proposed\nframework, we conduct experiments on cross-manipulation techniques, and the\nresults show the advantages of our method.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 08:41:16 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Yu", "Yang", ""], ["Ni", "Rongrong", ""], ["Zhao", "Yao", ""]]}, {"id": "2010.14205", "submitter": "Lina Felsner", "authors": "Lina Felsner, Tobias W\\\"urfl, Christopher Syben, Philipp Roser,\n  Alexander Preuhs, Andreas Maier, and Christian Riess", "title": "Reconstruction of Voxels with Position- and Angle-Dependent Weightings", "comments": "This paper was originally published at the 6th International\n  Conference on Image Formation in X-Ray Computed Tomography (CTmeeting 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The reconstruction problem of voxels with individual weightings can be\nmodeled a position- and angle- dependent function in the forward-projection.\nThis changes the system matrix and prohibits to use standard filtered\nbackprojection. In this work we first formulate this reconstruction problem in\nterms of a system matrix and weighting part. We compute the pseudoinverse and\nshow that the solution is rank-deficient and hence very ill posed. This is a\nfundamental limitation for reconstruction. We then derive an iterative solution\nand experimentally show its uperiority to any closed-form solution.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 11:29:47 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Felsner", "Lina", ""], ["W\u00fcrfl", "Tobias", ""], ["Syben", "Christopher", ""], ["Roser", "Philipp", ""], ["Preuhs", "Alexander", ""], ["Maier", "Andreas", ""], ["Riess", "Christian", ""]]}, {"id": "2010.14261", "submitter": "Hao Ma", "authors": "Hao Ma, Jingbin Liu, Keke Liu, Hongyu Qiu, Dong Xu, Zemin Wang,\n  Xiaodong Gong, Sheng Yang (State Key Laboratory of Information Engineering in\n  Survering, Mapping and Remote Sensing, Wuhan University)", "title": "A Simple and Efficient Registration of 3D Point Cloud and Image Data for\n  Indoor Mobile Mapping System", "comments": null, "journal-ref": null, "doi": "10.1364/JOSAA.414042", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Registration of 3D LiDAR point clouds with optical images is critical in the\ncombination of multi-source data. Geometric misalignment originally exists in\nthe pose data between LiDAR point clouds and optical images. To improve the\naccuracy of the initial pose and the applicability of the integration of 3D\npoints and image data, we develop a simple but efficient registration method.\nWe firstly extract point features from LiDAR point clouds and images: point\nfeatures is extracted from single-frame LiDAR and point features from images\nusing classical Canny method. Cost map is subsequently built based on Canny\nimage edge detection. The optimization direction is guided by the cost map\nwhere low cost represents the the desired direction, and loss function is also\nconsidered to improve the robustness of the the purposed method. Experiments\nshow pleasant results.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 13:01:54 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Ma", "Hao", "", "State Key Laboratory of Information Engineering in\n  Survering, Mapping and Remote Sensing, Wuhan University"], ["Liu", "Jingbin", "", "State Key Laboratory of Information Engineering in\n  Survering, Mapping and Remote Sensing, Wuhan University"], ["Liu", "Keke", "", "State Key Laboratory of Information Engineering in\n  Survering, Mapping and Remote Sensing, Wuhan University"], ["Qiu", "Hongyu", "", "State Key Laboratory of Information Engineering in\n  Survering, Mapping and Remote Sensing, Wuhan University"], ["Xu", "Dong", "", "State Key Laboratory of Information Engineering in\n  Survering, Mapping and Remote Sensing, Wuhan University"], ["Wang", "Zemin", "", "State Key Laboratory of Information Engineering in\n  Survering, Mapping and Remote Sensing, Wuhan University"], ["Gong", "Xiaodong", "", "State Key Laboratory of Information Engineering in\n  Survering, Mapping and Remote Sensing, Wuhan University"], ["Yang", "Sheng", "", "State Key Laboratory of Information Engineering in\n  Survering, Mapping and Remote Sensing, Wuhan University"]]}, {"id": "2010.14266", "submitter": "Song-Lu Chen", "authors": "Song-Lu Chen, Shu Tian, Jia-Wei Ma, Qi Liu, Chun Yang, Feng Chen and\n  Xu-Cheng Yin", "title": "End-to-end trainable network for degraded license plate detection via\n  vehicle-plate relation mining", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  License plate detection is the first and essential step of the license plate\nrecognition system and is still challenging in real applications, such as\non-road scenarios. In particular, small-sized and oblique license plates,\nmainly caused by the distant and mobile camera, are difficult to detect. In\nthis work, we propose a novel and applicable method for degraded license plate\ndetection via vehicle-plate relation mining, which localizes the license plate\nin a coarse-to-fine scheme. First, we propose to estimate the local region\naround the license plate by using the relationships between the vehicle and the\nlicense plate, which can greatly reduce the search area and precisely detect\nvery small-sized license plates. Second, we propose to predict the\nquadrilateral bounding box in the local region by regressing the four corners\nof the license plate to robustly detect oblique license plates. Moreover, the\nwhole network can be trained in an end-to-end manner. Extensive experiments\nverify the effectiveness of our proposed method for small-sized and oblique\nlicense plates. Codes are available at\nhttps://github.com/chensonglu/LPD-end-to-end.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 13:05:31 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Chen", "Song-Lu", ""], ["Tian", "Shu", ""], ["Ma", "Jia-Wei", ""], ["Liu", "Qi", ""], ["Yang", "Chun", ""], ["Chen", "Feng", ""], ["Yin", "Xu-Cheng", ""]]}, {"id": "2010.14270", "submitter": "Hao Ma", "authors": "Hao Ma, Jingbin Liu, Zhirong Hu, Hongyu Qiu, Dong Xu, Zemin Wang,\n  Xiaodong Gong, Sheng Yang", "title": "A Method of Generating Measurable Panoramic Image for Indoor Mobile\n  Measurement System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper designs a technique route to generate high-quality panoramic image\nwith depth information, which involves two critical research hotspots: fusion\nof LiDAR and image data and image stitching. For the fusion of 3D points and\nimage data, since a sparse depth map can be firstly generated by projecting\nLiDAR point onto the RGB image plane based on our reliable calibrated and\nsynchronized sensors, we adopt a parameter self-adaptive framework to produce\n2D dense depth map. For image stitching, optimal seamline for the overlapping\narea is searched using a graph-cuts-based method to alleviate the geometric\ninfluence and image blending based on the pyramid multi-band is utilized to\neliminate the photometric effects near the stitching line. Since each pixel is\nassociated with a depth value, we design this depth value as a radius in the\nspherical projection which can further project the panoramic image to the world\ncoordinate and consequently produces a high-quality measurable panoramic image.\nThe purposed method is tested on the data from our data collection platform and\npresents a satisfactory application prospects.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 13:12:02 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Ma", "Hao", ""], ["Liu", "Jingbin", ""], ["Hu", "Zhirong", ""], ["Qiu", "Hongyu", ""], ["Xu", "Dong", ""], ["Wang", "Zemin", ""], ["Gong", "Xiaodong", ""], ["Yang", "Sheng", ""]]}, {"id": "2010.14291", "submitter": "Quanyu Liao", "authors": "Quanyu Liao, Xin Wang, Bin Kong, Siwei Lyu, Youbing Yin, Qi Song and\n  Xi Wu", "title": "Fast Local Attack: Generating Local Adversarial Examples for Object\n  Detectors", "comments": "Published in: 2020 International Joint Conference on Neural Networks\n  (IJCNN)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The deep neural network is vulnerable to adversarial examples. Adding\nimperceptible adversarial perturbations to images is enough to make them fail.\nMost existing research focuses on attacking image classifiers or anchor-based\nobject detectors, but they generate globally perturbation on the whole image,\nwhich is unnecessary. In our work, we leverage higher-level semantic\ninformation to generate high aggressive local perturbations for anchor-free\nobject detectors. As a result, it is less computationally intensive and\nachieves a higher black-box attack as well as transferring attack performance.\nThe adversarial examples generated by our method are not only capable of\nattacking anchor-free object detectors, but also able to be transferred to\nattack anchor-based object detector.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 13:49:36 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Liao", "Quanyu", ""], ["Wang", "Xin", ""], ["Kong", "Bin", ""], ["Lyu", "Siwei", ""], ["Yin", "Youbing", ""], ["Song", "Qi", ""], ["Wu", "Xi", ""]]}, {"id": "2010.14294", "submitter": "Jianhao Jiao", "authors": "Jianhao Jiao, Haoyang Ye, Yilong Zhu, Ming Liu", "title": "Robust Odometry and Mapping for Multi-LiDAR Systems with Online\n  Extrinsic Calibration", "comments": "20 pages, 22 figures, accepted by IEEE Transcation on Robotics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Combining multiple LiDARs enables a robot to maximize its perceptual\nawareness of environments and obtain sufficient measurements, which is\npromising for simultaneous localization and mapping (SLAM). This paper proposes\na system to achieve robust and simultaneous extrinsic calibration, odometry,\nand mapping for multiple LiDARs. Our approach starts with measurement\npreprocessing to extract edge and planar features from raw measurements. After\na motion and extrinsic initialization procedure, a sliding window-based\nmulti-LiDAR odometry runs onboard to estimate poses with online calibration\nrefinement and convergence identification. We further develop a mapping\nalgorithm to construct a global map and optimize poses with sufficient features\ntogether with a method to model and reduce data uncertainty. We validate our\napproach's performance with extensive experiments on ten sequences (4.60km\ntotal length) for the calibration and SLAM and compare them against the\nstate-of-the-art. We demonstrate that the proposed work is a complete, robust,\nand extensible system for various multi-LiDAR setups. The source code,\ndatasets, and demonstrations are available at\nhttps://ram-lab.com/file/site/m-loam.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 13:51:26 GMT"}, {"version": "v2", "created": "Wed, 5 May 2021 15:32:33 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Jiao", "Jianhao", ""], ["Ye", "Haoyang", ""], ["Zhu", "Yilong", ""], ["Liu", "Ming", ""]]}, {"id": "2010.14296", "submitter": "Agnese Chiatti", "authors": "Agnese Chiatti, Enrico Motta, Enrico Daga, Gianluca Bardaro", "title": "Fit to Measure: Reasoning about Sizes for Robust Object Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Service robots can help with many of our daily tasks, especially in those\ncases where it is inconvenient or unsafe for us to intervene: e.g., under\nextreme weather conditions or when social distance needs to be maintained.\nHowever, before we can successfully delegate complex tasks to robots, we need\nto enhance their ability to make sense of dynamic, real world environments. In\nthis context, the first prerequisite to improving the Visual Intelligence of a\nrobot is building robust and reliable object recognition systems. While object\nrecognition solutions are traditionally based on Machine Learning methods,\naugmenting them with knowledge based reasoners has been shown to improve their\nperformance. In particular, based on our prior work on identifying the\nepistemic requirements of Visual Intelligence, we hypothesise that knowledge of\nthe typical size of objects could significantly improve the accuracy of an\nobject recognition system. To verify this hypothesis, in this paper we present\nan approach to integrating knowledge about object sizes in a ML based\narchitecture. Our experiments in a real world robotic scenario show that this\ncombined approach ensures a significant performance increase over state of the\nart Machine Learning methods.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 13:54:37 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Chiatti", "Agnese", ""], ["Motta", "Enrico", ""], ["Daga", "Enrico", ""], ["Bardaro", "Gianluca", ""]]}, {"id": "2010.14300", "submitter": "Manu Tom", "authors": "Manu Tom and Rajanie Prabha and Tianyu Wu and Emmanuel Baltsavias and\n  Laura Leal-Taixe and Konrad Schindler", "title": "Ice Monitoring in Swiss Lakes from Optical Satellites and Webcams using\n  Machine Learning", "comments": "Accepted for publication in MDPI Remote Sensing Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continuous observation of climate indicators, such as trends in lake\nfreezing, is important to understand the dynamics of the local and global\nclimate system. Consequently, lake ice has been included among the Essential\nClimate Variables (ECVs) of the Global Climate Observing System (GCOS), and\nthere is a need to set up operational monitoring capabilities. Multi-temporal\nsatellite images and publicly available webcam streams are among the viable\ndata sources to monitor lake ice. In this work we investigate machine\nlearning-based image analysis as a tool to determine the spatio-temporal extent\nof ice on Swiss Alpine lakes as well as the ice-on and ice-off dates, from both\nmultispectral optical satellite images (VIIRS and MODIS) and RGB webcam images.\nWe model lake ice monitoring as a pixel-wise semantic segmentation problem,\ni.e., each pixel on the lake surface is classified to obtain a spatially\nexplicit map of ice cover. We show experimentally that the proposed system\nproduces consistently good results when tested on data from multiple winters\nand lakes. Our satellite-based method obtains mean Intersection-over-Union\n(mIoU) scores >93%, for both sensors. It also generalises well across lakes and\nwinters with mIoU scores >78% and >80% respectively. On average, our webcam\napproach achieves mIoU values of 87% (approx.) and generalisation scores of 71%\n(approx.) and 69% (approx.) across different cameras and winters respectively.\nAdditionally, we put forward a new benchmark dataset of webcam images\n(Photi-LakeIce) which includes data from two winters and three cameras.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 14:02:00 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Tom", "Manu", ""], ["Prabha", "Rajanie", ""], ["Wu", "Tianyu", ""], ["Baltsavias", "Emmanuel", ""], ["Leal-Taixe", "Laura", ""], ["Schindler", "Konrad", ""]]}, {"id": "2010.14301", "submitter": "Peiyao Wang", "authors": "Peiyao Wang, Weixin Luo, Yanyu Xu, Haojie Li, Shugong Xu, Jianyu Yang,\n  Shenghua Gao", "title": "SIRI: Spatial Relation Induced Network For Spatial Description\n  Resolution", "comments": "Accepted at NeurIPS 2020; Peiyao Wang and Weixin Luo made equal\n  contribution", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatial Description Resolution, as a language-guided localization task, is\nproposed for target location in a panoramic street view, given corresponding\nlanguage descriptions. Explicitly characterizing an object-level relationship\nwhile distilling spatial relationships are currently absent but crucial to this\ntask. Mimicking humans, who sequentially traverse spatial relationship words\nand objects with a first-person view to locate their target, we propose a novel\nspatial relationship induced (SIRI) network. Specifically, visual features are\nfirstly correlated at an implicit object-level in a projected latent space;\nthen they are distilled by each spatial relationship word, resulting in each\ndifferently activated feature representing each spatial relationship. Further,\nwe introduce global position priors to fix the absence of positional\ninformation, which may result in global positional reasoning ambiguities. Both\nthe linguistic and visual features are concatenated to finalize the target\nlocalization. Experimental results on the Touchdown show that our method is\naround 24\\% better than the state-of-the-art method in terms of accuracy,\nmeasured by an 80-pixel radius. Our method also generalizes well on our\nproposed extended dataset collected using the same settings as Touchdown.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 14:04:05 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Wang", "Peiyao", ""], ["Luo", "Weixin", ""], ["Xu", "Yanyu", ""], ["Li", "Haojie", ""], ["Xu", "Shugong", ""], ["Yang", "Jianyu", ""], ["Gao", "Shenghua", ""]]}, {"id": "2010.14343", "submitter": "Hui Chen", "authors": "Hui Chen, Zhixiong Nan, Jingjing Jiang and Nanning Zheng", "title": "Learning to Infer Unseen Attribute-Object Compositions", "comments": "Submitted to IEEE Transactions on Pattern Analysis and Machine\n  Intelligence (TPAMI), currently under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The composition recognition of unseen attribute-object is critical to make\nmachines learn to decompose and compose complex concepts like people. Most of\nthe existing methods are limited to the composition recognition of\nsingle-attribute-object, and can hardly distinguish the compositions with\nsimilar appearances. In this paper, a graph-based model is proposed that can\nflexibly recognize both single- and multi-attribute-object compositions. The\nmodel maps the visual features of images and the attribute-object category\nlabels represented by word embedding vectors into a latent space. Then,\naccording to the constraints of the attribute-object semantic association,\ndistances are calculated between visual features and the corresponding label\nsemantic features in the latent space. During the inference, the composition\nthat is closest to the given image feature among all compositions is used as\nthe reasoning result. In addition, we build a large-scale Multi-Attribute\nDataset (MAD) with 116,099 images and 8,030 composition categories. Experiments\non MAD and two other single-attribute-object benchmark datasets demonstrate the\neffectiveness of our approach.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 14:57:35 GMT"}, {"version": "v2", "created": "Tue, 3 Nov 2020 09:32:41 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Chen", "Hui", ""], ["Nan", "Zhixiong", ""], ["Jiang", "Jingjing", ""], ["Zheng", "Nanning", ""]]}, {"id": "2010.14350", "submitter": "Yi Zhang", "authors": "Wenjun Xia, Zexin Lu, Yongqiang Huang, Yan Liu, Hu Chen, Jiliu Zhou,\n  Yi Zhang", "title": "CT Reconstruction with PDF: Parameter-Dependent Framework for Multiple\n  Scanning Geometries and Dose Levels", "comments": "4 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.med-ph cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Current mainstream of CT reconstruction methods based on deep learning\nusually needs to fix the scanning geometry and dose level, which will\nsignificantly aggravate the training cost and need more training data for\nclinical application. In this paper, we propose a parameter-dependent framework\n(PDF) which trains data with multiple scanning geometries and dose levels\nsimultaneously. In the proposed PDF, the geometry and dose level are\nparameterized and fed into two multi-layer perceptrons (MLPs). The MLPs are\nleveraged to modulate the feature maps of CT reconstruction network, which\ncondition the network outputs on different scanning geometries and dose levels.\nThe experiments show that our proposed method can obtain competing performance\nsimilar to the original network trained with specific geometry and dose level,\nwhich can efficiently save the extra training cost for multiple scanning\ngeometries and dose levels.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 15:05:32 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Xia", "Wenjun", ""], ["Lu", "Zexin", ""], ["Huang", "Yongqiang", ""], ["Liu", "Yan", ""], ["Chen", "Hu", ""], ["Zhou", "Jiliu", ""], ["Zhang", "Yi", ""]]}, {"id": "2010.14361", "submitter": "Yi Zhang", "authors": "Xiang Chen, Wenjun Xia, Yan Liu, Hu Chen, Jiliu Zhou, Yi Zhang", "title": "Fourth-Order Nonlocal Tensor Decomposition Model for Spectral Computed\n  Tomography", "comments": "4 pages, 3 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.med-ph cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Spectral computed tomography (CT) can reconstruct spectral images from\ndifferent energy bins using photon counting detectors (PCDs). However, due to\nthe limited photons and counting rate in the corresponding spectral fraction,\nthe reconstructed spectral images usually suffer from severe noise. In this\npaper, a fourth-order nonlocal tensor decomposition model for spectral CT image\nreconstruction (FONT-SIR) method is proposed. Similar patches are collected in\nboth spatial and spectral dimensions simultaneously to form the basic tensor\nunit. Additionally, principal component analysis (PCA) is applied to extract\nlatent features from the patches for a robust and efficient similarity measure.\nThen, low-rank and sparsity decomposition is performed on the produced\nfourth-order tensor unit, and the weighted nuclear norm and total variation\n(TV) norm are used to enforce the low-rank and sparsity constraints,\nrespectively. The alternating direction method of multipliers (ADMM) is adopted\nto optimize the objective function. The experimental results with our proposed\nFONT-SIR demonstrates a superior qualitative and quantitative performance for\nboth simulated and real data sets relative to several state-of-the-art methods,\nin terms of noise suppression and detail preservation.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 15:14:36 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Chen", "Xiang", ""], ["Xia", "Wenjun", ""], ["Liu", "Yan", ""], ["Chen", "Hu", ""], ["Zhou", "Jiliu", ""], ["Zhang", "Yi", ""]]}, {"id": "2010.14397", "submitter": "Arbish Akram", "authors": "Arbish Akram, Nazar Khan", "title": "Pixel-based Facial Expression Synthesis", "comments": "ICPR 2020, 7 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial expression synthesis has achieved remarkable advances with the advent\nof Generative Adversarial Networks (GANs). However, GAN-based approaches mostly\ngenerate photo-realistic results as long as the testing data distribution is\nclose to the training data distribution. The quality of GAN results\nsignificantly degrades when testing images are from a slightly different\ndistribution. Moreover, recent work has shown that facial expressions can be\nsynthesized by changing localized face regions. In this work, we propose a\npixel-based facial expression synthesis method in which each output pixel\nobserves only one input pixel. The proposed method achieves good generalization\ncapability by leveraging only a few hundred training images. Experimental\nresults demonstrate that the proposed method performs comparably well against\nstate-of-the-art GANs on in-dataset images and significantly better on\nout-of-dataset images. In addition, the proposed model is two orders of\nmagnitude smaller which makes it suitable for deployment on\nresource-constrained devices.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 16:00:45 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Akram", "Arbish", ""], ["Khan", "Nazar", ""]]}, {"id": "2010.14407", "submitter": "Andrea Dittadi", "authors": "Andrea Dittadi, Frederik Tr\\\"auble, Francesco Locatello, Manuel\n  W\\\"uthrich, Vaibhav Agrawal, Ole Winther, Stefan Bauer, Bernhard Sch\\\"olkopf", "title": "On the Transfer of Disentangled Representations in Realistic Settings", "comments": "Published at ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning meaningful representations that disentangle the underlying structure\nof the data generating process is considered to be of key importance in machine\nlearning. While disentangled representations were found to be useful for\ndiverse tasks such as abstract reasoning and fair classification, their\nscalability and real-world impact remain questionable. We introduce a new\nhigh-resolution dataset with 1M simulated images and over 1,800 annotated\nreal-world images of the same setup. In contrast to previous work, this new\ndataset exhibits correlations, a complex underlying structure, and allows to\nevaluate transfer to unseen simulated and real-world settings where the encoder\ni) remains in distribution or ii) is out of distribution. We propose new\narchitectures in order to scale disentangled representation learning to\nrealistic high-resolution settings and conduct a large-scale empirical study of\ndisentangled representations on this dataset. We observe that disentanglement\nis a good predictor for out-of-distribution (OOD) task performance.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 16:15:24 GMT"}, {"version": "v2", "created": "Thu, 11 Mar 2021 11:43:10 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Dittadi", "Andrea", ""], ["Tr\u00e4uble", "Frederik", ""], ["Locatello", "Francesco", ""], ["W\u00fcthrich", "Manuel", ""], ["Agrawal", "Vaibhav", ""], ["Winther", "Ole", ""], ["Bauer", "Stefan", ""], ["Sch\u00f6lkopf", "Bernhard", ""]]}, {"id": "2010.14411", "submitter": "Siddhant Bansal", "authors": "Siddhant Bansal, Praveen Krishnan, C.V. Jawahar", "title": "Improving Word Recognition using Multiple Hypotheses and Deep Embeddings", "comments": "8 pages, 6 figures, Accepted in International Conference on Pattern\n  Recognition (ICPR) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a novel scheme for improving the word recognition accuracy using\nword image embeddings. We use a trained text recognizer, which can predict\nmultiple text hypothesis for a given word image. Our fusion scheme improves the\nrecognition process by utilizing the word image and text embeddings obtained\nfrom a trained word image embedding network. We propose EmbedNet, which is\ntrained using a triplet loss for learning a suitable embedding space where the\nembedding of the word image lies closer to the embedding of the corresponding\ntext transcription. The updated embedding space thus helps in choosing the\ncorrect prediction with higher confidence. To further improve the accuracy, we\npropose a plug-and-play module called Confidence based Accuracy Booster (CAB).\nThe CAB module takes in the confidence scores obtained from the text recognizer\nand Euclidean distances between the embeddings to generate an updated distance\nvector. The updated distance vector has lower distance values for the correct\nwords and higher distance values for the incorrect words. We rigorously\nevaluate our proposed method systematically on a collection of books in the\nHindi language. Our method achieves an absolute improvement of around 10\npercent in terms of word recognition accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 16:21:23 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Bansal", "Siddhant", ""], ["Krishnan", "Praveen", ""], ["Jawahar", "C. V.", ""]]}, {"id": "2010.14438", "submitter": "Mert Kilickaya", "authors": "Mert Kilickaya and Arnold W.M. Smeulders", "title": "Structured Visual Search via Composition-aware Learning", "comments": "Accepted at WACV'21 (Algorithms Track)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies visual search using structured queries. The structure is\nin the form of a 2D composition that encodes the position and the category of\nthe objects. The transformation of the position and the category of the objects\nleads to a continuous-valued relationship between visual compositions, which\ncarries highly beneficial information, although not leveraged by previous\ntechniques. To that end, in this work, our goal is to leverage these continuous\nrelationships by using the notion of symmetry in equivariance. Our model output\nis trained to change symmetrically with respect to the input transformations,\nleading to a sensitive feature space. Doing so leads to a highly efficient\nsearch technique, as our approach learns from fewer data using a smaller\nfeature space. Experiments on two large-scale benchmarks of MS-COCO and\nHICO-DET demonstrates that our approach leads to a considerable gain in the\nperformance against competing techniques.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 16:52:03 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Kilickaya", "Mert", ""], ["Smeulders", "Arnold W. M.", ""]]}, {"id": "2010.14440", "submitter": "Jannis Horn", "authors": "Jannis Horn, Yi Zhao, Nils Wandel, Magdalena Landl, Andrea Schnepf,\n  and Sven Behnke", "title": "Robust Skeletonization for Plant Root Structure Reconstruction from MRI", "comments": "Accepted final version. In 25th International Conference on Pattern\n  Recognition (ICPR2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structural reconstruction of plant roots from MRI is challenging, because of\nlow resolution and low signal-to-noise ratio of the 3D measurements which may\nlead to disconnectivities and wrongly connected roots. We propose a two-stage\napproach for this task. The first stage is based on semantic root vs. soil\nsegmentation and finds lowest-cost paths from any root voxel to the shoot. The\nsecond stage takes the largest fully connected component generated in the first\nstage and uses 3D skeletonization to extract a graph structure. We evaluate our\nmethod on 22 MRI scans and compare to human expert reconstructions.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 16:54:40 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Horn", "Jannis", ""], ["Zhao", "Yi", ""], ["Wandel", "Nils", ""], ["Landl", "Magdalena", ""], ["Schnepf", "Andrea", ""], ["Behnke", "Sven", ""]]}, {"id": "2010.14462", "submitter": "He Sun", "authors": "He Sun, Katherine L. Bouman", "title": "Deep Probabilistic Imaging: Uncertainty Quantification and Multi-modal\n  Solution Characterization for Computational Imaging", "comments": "This paper has been accepted to AAAI 2021. Keywords: Computational\n  Imaging, Normalizing Flow, Uncertainty Quantification, Interferometry, MRI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG astro-ph.IM cs.CV eess.IV eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computational image reconstruction algorithms generally produce a single\nimage without any measure of uncertainty or confidence. Regularized Maximum\nLikelihood (RML) and feed-forward deep learning approaches for inverse problems\ntypically focus on recovering a point estimate. This is a serious limitation\nwhen working with underdetermined imaging systems, where it is conceivable that\nmultiple image modes would be consistent with the measured data. Characterizing\nthe space of probable images that explain the observational data is therefore\ncrucial. In this paper, we propose a variational deep probabilistic imaging\napproach to quantify reconstruction uncertainty. Deep Probabilistic Imaging\n(DPI) employs an untrained deep generative model to estimate a posterior\ndistribution of an unobserved image. This approach does not require any\ntraining data; instead, it optimizes the weights of a neural network to\ngenerate image samples that fit a particular measurement dataset. Once the\nnetwork weights have been learned, the posterior distribution can be\nefficiently sampled. We demonstrate this approach in the context of\ninterferometric radio imaging, which is used for black hole imaging with the\nEvent Horizon Telescope, and compressed sensing Magnetic Resonance Imaging\n(MRI).\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 17:23:09 GMT"}, {"version": "v2", "created": "Thu, 17 Dec 2020 06:13:58 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Sun", "He", ""], ["Bouman", "Katherine L.", ""]]}, {"id": "2010.14476", "submitter": "Maruf Ahmed Dhali", "authors": "Mladen Popovi\\'c, Maruf A. Dhali, Lambert Schomaker", "title": "Artificial intelligence based writer identification generates new\n  evidence for the unknown scribes of the Dead Sea Scrolls exemplified by the\n  Great Isaiah Scroll (1QIsaa)", "comments": "23 pages, 15 pages of supplementary materials, submitted to PLOS ONE\n  on 19 October 2019", "journal-ref": "PLoS ONE 2021", "doi": "10.1371/journal.pone.0249769", "report-no": "PLoS ONE 16(4): e0249769", "categories": "cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Dead Sea Scrolls are tangible evidence of the Bible's ancient scribal\nculture. Palaeography - the study of ancient handwriting - can provide access\nto this scribal culture. However, one of the problems of traditional\npalaeography is to determine writer identity when the writing style is near\nuniform. This is exemplified by the Great Isaiah Scroll (1QIsaa). To this end,\nwe used pattern recognition and artificial intelligence techniques to innovate\nthe palaeography of the scrolls regarding writer identification and to pioneer\nthe microlevel of individual scribes to open access to the Bible's ancient\nscribal culture. Although many scholars believe that 1QIsaa was written by one\nscribe, we report new evidence for a breaking point in the series of columns in\nthis scroll. Without prior assumption of writer identity, based on point clouds\nof the reduced-dimensionality feature-space, we found that columns from the\nfirst and second halves of the manuscript ended up in two distinct zones of\nsuch scatter plots, notably for a range of digital palaeography tools, each\naddressing very different featural aspects of the script samples. In a\nsecondary, independent, analysis, now assuming writer difference and using yet\nanother independent feature method and several different types of statistical\ntesting, a switching point was found in the column series. A clear phase\ntransition is apparent around column 27. Given the statistically significant\ndifferences between the two halves, a tertiary, post-hoc analysis was\nperformed. Demonstrating that two main scribes were responsible for the Great\nIsaiah Scroll, this study sheds new light on the Bible's ancient scribal\nculture by providing new, tangible evidence that ancient biblical texts were\nnot copied by a single scribe only but that multiple scribes could closely\ncollaborate on one particular manuscript.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 17:36:18 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Popovi\u0107", "Mladen", ""], ["Dhali", "Maruf A.", ""], ["Schomaker", "Lambert", ""]]}, {"id": "2010.14501", "submitter": "Aashaka Shah", "authors": "Aashaka Shah, Chao-Yuan Wu, Jayashree Mohan, Vijay Chidambaram,\n  Philipp Kr\\\"ahenb\\\"uhl", "title": "Memory Optimization for Deep Networks", "comments": "18 pages, ICLR'21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning is slowly, but steadily, hitting a memory bottleneck. While the\ntensor computation in top-of-the-line GPUs increased by 32x over the last five\nyears, the total available memory only grew by 2.5x. This prevents researchers\nfrom exploring larger architectures, as training large networks requires more\nmemory for storing intermediate outputs. In this paper, we present MONeT, an\nautomatic framework that minimizes both the memory footprint and computational\noverhead of deep networks. MONeT jointly optimizes the checkpointing schedule\nand the implementation of various operators. MONeT is able to outperform all\nprior hand-tuned operations as well as automated checkpointing. MONeT reduces\nthe overall memory requirement by 3x for various PyTorch models, with a 9-16%\noverhead in computation. For the same computation cost, MONeT requires 1.2-1.8x\nless memory than current state-of-the-art automated checkpointing frameworks.\nOur code is available at https://github.com/utsaslab/MONeT.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 17:57:34 GMT"}, {"version": "v2", "created": "Thu, 29 Oct 2020 17:21:28 GMT"}, {"version": "v3", "created": "Sat, 3 Apr 2021 00:32:47 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Shah", "Aashaka", ""], ["Wu", "Chao-Yuan", ""], ["Mohan", "Jayashree", ""], ["Chidambaram", "Vijay", ""], ["Kr\u00e4henb\u00fchl", "Philipp", ""]]}, {"id": "2010.14535", "submitter": "Dr. Suryansh Kumar", "authors": "Rhea Sanjay Sukthanker, Zhiwu Huang, Suryansh Kumar, Erik Goron\n  Endsjo, Yan Wu, Luc Van Gool", "title": "Neural Architecture Search of SPD Manifold Networks", "comments": "This paper is accepted for publication at IJCAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a new neural architecture search (NAS) problem of\nSymmetric Positive Definite (SPD) manifold networks, aiming to automate the\ndesign of SPD neural architectures. To address this problem, we first introduce\na geometrically rich and diverse SPD neural architecture search space for an\nefficient SPD cell design. Further, we model our new NAS problem with a\none-shot training process of a single supernet. Based on the supernet modeling,\nwe exploit a differentiable NAS algorithm on our relaxed continuous search\nspace for SPD neural architecture search. Statistical evaluation of our method\non drone, action, and emotion recognition tasks mostly provides better results\nthan the state-of-the-art SPD networks and traditional NAS algorithms.\nEmpirical results show that our algorithm excels in discovering better\nperforming SPD network design and provides models that are more than three\ntimes lighter than searched by the state-of-the-art NAS algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 18:08:57 GMT"}, {"version": "v2", "created": "Sun, 17 Jan 2021 00:06:46 GMT"}, {"version": "v3", "created": "Wed, 9 Jun 2021 21:06:51 GMT"}, {"version": "v4", "created": "Sun, 13 Jun 2021 21:24:32 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Sukthanker", "Rhea Sanjay", ""], ["Huang", "Zhiwu", ""], ["Kumar", "Suryansh", ""], ["Endsjo", "Erik Goron", ""], ["Wu", "Yan", ""], ["Van Gool", "Luc", ""]]}, {"id": "2010.14541", "submitter": "Octavio Arriaga", "authors": "Octavio Arriaga, Matias Valdenegro-Toro, Mohandass Muthuraja, Sushma\n  Devaramani, Frank Kirchner", "title": "Perception for Autonomous Systems (PAZ)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce the Perception for Autonomous Systems (PAZ)\nsoftware library. PAZ is a hierarchical perception library that allow users to\nmanipulate multiple levels of abstraction in accordance to their requirements\nor skill level. More specifically, PAZ is divided into three hierarchical\nlevels which we refer to as pipelines, processors, and backends. These\nabstractions allows users to compose functions in a hierarchical modular scheme\nthat can be applied for preprocessing, data-augmentation, prediction and\npostprocessing of inputs and outputs of machine learning (ML) models. PAZ uses\nthese abstractions to build reusable training and prediction pipelines for\nmultiple robot perception tasks such as: 2D keypoint estimation, 2D object\ndetection, 3D keypoint discovery, 6D pose estimation, emotion classification,\nface recognition, instance segmentation, and attention mechanisms.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 18:22:01 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Arriaga", "Octavio", ""], ["Valdenegro-Toro", "Matias", ""], ["Muthuraja", "Mohandass", ""], ["Devaramani", "Sushma", ""], ["Kirchner", "Frank", ""]]}, {"id": "2010.14543", "submitter": "Yue Wu", "authors": "Shangda Li, Devendra Singh Chaplot, Yao-Hung Hubert Tsai, Yue Wu,\n  Louis-Philippe Morency, Ruslan Salakhutdinov", "title": "Unsupervised Domain Adaptation for Visual Navigation", "comments": "Deep Reinforcement Learning Workshop at NeurIPS 2020. Camera Ready\n  Version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Advances in visual navigation methods have led to intelligent embodied\nnavigation agents capable of learning meaningful representations from raw RGB\nimages and perform a wide variety of tasks involving structural and semantic\nreasoning. However, most learning-based navigation policies are trained and\ntested in simulation environments. In order for these policies to be\npractically useful, they need to be transferred to the real-world. In this\npaper, we propose an unsupervised domain adaptation method for visual\nnavigation. Our method translates the images in the target domain to the source\ndomain such that the translation is consistent with the representations learned\nby the navigation policy. The proposed method outperforms several baselines\nacross two different navigation tasks in simulation. We further show that our\nmethod can be used to transfer the navigation policies learned in simulation to\nthe real world.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 18:22:43 GMT"}, {"version": "v2", "created": "Thu, 12 Nov 2020 17:41:51 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Li", "Shangda", ""], ["Chaplot", "Devendra Singh", ""], ["Tsai", "Yao-Hung Hubert", ""], ["Wu", "Yue", ""], ["Morency", "Louis-Philippe", ""], ["Salakhutdinov", "Ruslan", ""]]}, {"id": "2010.14551", "submitter": "Iro Laina", "authors": "Iro Laina, Ruth C. Fong, Andrea Vedaldi", "title": "Quantifying Learnability and Describability of Visual Concepts Emerging\n  in Representation Learning", "comments": "NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing impact of black box models, and particularly of unsupervised\nones, comes with an increasing interest in tools to understand and interpret\nthem. In this paper, we consider in particular how to characterise visual\ngroupings discovered automatically by deep neural networks, starting with\nstate-of-the-art clustering methods. In some cases, clusters readily correspond\nto an existing labelled dataset. However, often they do not, yet they still\nmaintain an \"intuitive interpretability\". We introduce two concepts, visual\nlearnability and describability, that can be used to quantify the\ninterpretability of arbitrary image groupings, including unsupervised ones. The\nidea is to measure (1) how well humans can learn to reproduce a grouping by\nmeasuring their ability to generalise from a small set of visual examples\n(learnability) and (2) whether the set of visual examples can be replaced by a\nsuccinct, textual description (describability). By assessing human annotators\nas classifiers, we remove the subjective quality of existing evaluation\nmetrics. For better scalability, we finally propose a class-level captioning\nsystem to generate descriptions for visual groupings automatically and compare\nit to human annotators using the describability metric.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 18:41:49 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Laina", "Iro", ""], ["Fong", "Ruth C.", ""], ["Vedaldi", "Andrea", ""]]}, {"id": "2010.14561", "submitter": "Zahra Mousavi Kouzehkanan", "authors": "Zahra Mousavi Kouzehkanan, Reshad Hosseini, Babak Nadjar Araabi", "title": "Contour Integration using Graph-Cut and Non-Classical Receptive Field", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Many edge and contour detection algorithms give a soft-value as an output and\nthe final binary map is commonly obtained by applying an optimal threshold. In\nthis paper, we propose a novel method to detect image contours from the\nextracted edge segments of other algorithms. Our method is based on an\nundirected graphical model with the edge segments set as the vertices. The\nproposed energy functions are inspired by the surround modulation in the\nprimary visual cortex that help suppressing texture noise. Our algorithm can\nimprove extracting the binary map, because it considers other important factors\nsuch as connectivity, smoothness, and length of the contour beside the\nsoft-values. Our quantitative and qualitative experimental results show the\nefficacy of the proposed method.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 19:07:13 GMT"}, {"version": "v2", "created": "Mon, 10 May 2021 21:07:53 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Kouzehkanan", "Zahra Mousavi", ""], ["Hosseini", "Reshad", ""], ["Araabi", "Babak Nadjar", ""]]}, {"id": "2010.14589", "submitter": "Chun-Hao Yang", "authors": "Chun-Hao Yang, Baba C. Vemuri", "title": "Nested Grassmanns for Dimensionality Reduction with Applications to\n  Shape Analysis", "comments": "12 pages, 5 figures. To appear in the 27th international conference\n  on Information Processing in Medical Imaging (IPMI)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Grassmann manifolds have been widely used to represent the geometry of\nfeature spaces in a variety of problems in medical imaging and computer vision\nincluding but not limited to shape analysis, action recognition, subspace\nclustering and motion segmentation. For these problems, the features usually\nlie in a very high-dimensional Grassmann manifold and hence an appropriate\ndimensionality reduction technique is called for in order to curtail the\ncomputational burden. To this end, the Principal Geodesic Analysis (PGA), a\nnonlinear extension of the well known principal component analysis, is\napplicable as a general tool to many Riemannian manifolds. In this paper, we\npropose a novel framework for dimensionality reduction of data in Riemannian\nhomogeneous spaces and then focus on the Grassman manifold which is an example\nof a homogeneous space. Our framework explicitly exploits the geometry of the\nhomogeneous space yielding reduced dimensional nested sub-manifolds that need\nnot be geodesic submanifolds and thus are more expressive. Specifically, we\nproject points in a Grassmann manifold to an embedded lower dimensional\nGrassmann manifold. A salient feature of our method is that it leads to higher\nexpressed variance compared to PGA which we demonstrate via synthetic and real\ndata experiments.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 20:09:12 GMT"}, {"version": "v2", "created": "Wed, 7 Apr 2021 01:48:33 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Yang", "Chun-Hao", ""], ["Vemuri", "Baba C.", ""]]}, {"id": "2010.14599", "submitter": "Xi Mo", "authors": "Xi Mo, Usman Sajid, Guanghui Wang", "title": "Stereo Frustums: A Siamese Pipeline for 3D Object Detection", "comments": "Accepted by Journal of Intelligent & Robotic Systems (JIRS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper proposes a light-weighted stereo frustums matching module for 3D\nobjection detection. The proposed framework takes advantage of a\nhigh-performance 2D detector and a point cloud segmentation network to regress\n3D bounding boxes for autonomous driving vehicles. Instead of performing\ntraditional stereo matching to compute disparities, the module directly takes\nthe 2D proposals from both the left and the right views as input. Based on the\nepipolar constraints recovered from the well-calibrated stereo cameras, we\npropose four matching algorithms to search for the best match for each proposal\nbetween the stereo image pairs. Each matching pair proposes a segmentation of\nthe scene which is then fed into a 3D bounding box regression network. Results\nof extensive experiments on KITTI dataset demonstrate that the proposed Siamese\npipeline outperforms the state-of-the-art stereo-based 3D bounding box\nregression methods.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 20:46:17 GMT"}, {"version": "v2", "created": "Sun, 8 Nov 2020 15:16:07 GMT"}], "update_date": "2020-11-22", "authors_parsed": [["Mo", "Xi", ""], ["Sajid", "Usman", ""], ["Wang", "Guanghui", ""]]}, {"id": "2010.14607", "submitter": "Peyman Tahghighi", "authors": "Peyman Tahghighi, Abbas Koochari, Masoume Jalali", "title": "Deformable Convolutional LSTM for Human Body Emotion Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  People represent their emotions in a myriad of ways. Among the most important\nones is whole body expressions which have many applications in different fields\nsuch as human-computer interaction (HCI). One of the most important challenges\nin human emotion recognition is that people express the same feeling in various\nways using their face and their body. Recently many methods have tried to\novercome these challenges using Deep Neural Networks (DNNs). However, most of\nthese methods were based on images or on facial expressions only and did not\nconsider deformation that may happen in the images such as scaling and rotation\nwhich can adversely affect the recognition accuracy. In this work, motivated by\nrecent researches on deformable convolutions, we incorporate the deformable\nbehavior into the core of convolutional long short-term memory (ConvLSTM) to\nimprove robustness to these deformations in the image and, consequently,\nimprove its accuracy on the emotion recognition task from videos of arbitrary\nlength. We did experiments on the GEMEP dataset and achieved state-of-the-art\naccuracy of 98.8% on the task of whole human body emotion recognition on the\nvalidation set.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 21:01:09 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Tahghighi", "Peyman", ""], ["Koochari", "Abbas", ""], ["Jalali", "Masoume", ""]]}, {"id": "2010.14701", "submitter": "Samuel McCandlish", "authors": "Tom Henighan, Jared Kaplan, Mor Katz, Mark Chen, Christopher Hesse,\n  Jacob Jackson, Heewoo Jun, Tom B. Brown, Prafulla Dhariwal, Scott Gray, Chris\n  Hallacy, Benjamin Mann, Alec Radford, Aditya Ramesh, Nick Ryder, Daniel M.\n  Ziegler, John Schulman, Dario Amodei, Sam McCandlish", "title": "Scaling Laws for Autoregressive Generative Modeling", "comments": "20+17 pages, 33 figures; added appendix with additional language\n  results", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We identify empirical scaling laws for the cross-entropy loss in four\ndomains: generative image modeling, video modeling, multimodal\nimage$\\leftrightarrow$text models, and mathematical problem solving. In all\ncases autoregressive Transformers smoothly improve in performance as model size\nand compute budgets increase, following a power-law plus constant scaling law.\nThe optimal model size also depends on the compute budget through a power-law,\nwith exponents that are nearly universal across all data domains.\n  The cross-entropy loss has an information theoretic interpretation as\n$S($True$) + D_{\\mathrm{KL}}($True$||$Model$)$, and the empirical scaling laws\nsuggest a prediction for both the true data distribution's entropy and the KL\ndivergence between the true and model distributions. With this interpretation,\nbillion-parameter Transformers are nearly perfect models of the YFCC100M image\ndistribution downsampled to an $8\\times 8$ resolution, and we can forecast the\nmodel size needed to achieve any given reducible loss (ie $D_{\\mathrm{KL}}$) in\nnats/image for other resolutions.\n  We find a number of additional scaling laws in specific domains: (a) we\nidentify a scaling relation for the mutual information between captions and\nimages in multimodal models, and show how to answer the question \"Is a picture\nworth a thousand words?\"; (b) in the case of mathematical problem solving, we\nidentify scaling laws for model performance when extrapolating beyond the\ntraining distribution; (c) we finetune generative image models for ImageNet\nclassification and find smooth scaling of the classification loss and error\nrate, even as the generative loss levels off. Taken together, these results\nstrengthen the case that scaling laws have important implications for neural\nnetwork performance, including on downstream tasks.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2020 02:17:24 GMT"}, {"version": "v2", "created": "Fri, 6 Nov 2020 04:16:36 GMT"}], "update_date": "2020-11-09", "authors_parsed": [["Henighan", "Tom", ""], ["Kaplan", "Jared", ""], ["Katz", "Mor", ""], ["Chen", "Mark", ""], ["Hesse", "Christopher", ""], ["Jackson", "Jacob", ""], ["Jun", "Heewoo", ""], ["Brown", "Tom B.", ""], ["Dhariwal", "Prafulla", ""], ["Gray", "Scott", ""], ["Hallacy", "Chris", ""], ["Mann", "Benjamin", ""], ["Radford", "Alec", ""], ["Ramesh", "Aditya", ""], ["Ryder", "Nick", ""], ["Ziegler", "Daniel M.", ""], ["Schulman", "John", ""], ["Amodei", "Dario", ""], ["McCandlish", "Sam", ""]]}, {"id": "2010.14705", "submitter": "Md Taufeeq Uddin", "authors": "Md Taufeeq Uddin, Shaun Canavan", "title": "Quantified Facial Temporal-Expressiveness Dynamics for Affect Analysis", "comments": "25th International Conference on Pattern Recognition (ICPR2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The quantification of visual affect data (e.g. face images) is essential to\nbuild and monitor automated affect modeling systems efficiently. Considering\nthis, this work proposes quantified facial Temporal-expressiveness Dynamics\n(TED) to quantify the expressiveness of human faces. The proposed algorithm\nleverages multimodal facial features by incorporating static and dynamic\ninformation to enable accurate measurements of facial expressiveness. We show\nthat TED can be used for high-level tasks such as summarization of unstructured\nvisual data, and expectation from and interpretation of automated affect\nrecognition models. To evaluate the positive impact of using TED, a case study\nwas conducted on spontaneous pain using the UNBC-McMaster spontaneous shoulder\npain dataset. Experimental results show the efficacy of using TED for\nquantified affect analysis.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2020 02:22:22 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Uddin", "Md Taufeeq", ""], ["Canavan", "Shaun", ""]]}, {"id": "2010.14713", "submitter": "Ajinkya Tejankar", "authors": "Soroush Abbasi Koohpayegani, Ajinkya Tejankar, and Hamed Pirsiavash", "title": "CompRess: Self-Supervised Learning by Compressing Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Self-supervised learning aims to learn good representations with unlabeled\ndata. Recent works have shown that larger models benefit more from\nself-supervised learning than smaller models. As a result, the gap between\nsupervised and self-supervised learning has been greatly reduced for larger\nmodels. In this work, instead of designing a new pseudo task for\nself-supervised learning, we develop a model compression method to compress an\nalready learned, deep self-supervised model (teacher) to a smaller one\n(student). We train the student model so that it mimics the relative similarity\nbetween the data points in the teacher's embedding space. For AlexNet, our\nmethod outperforms all previous methods including the fully supervised model on\nImageNet linear evaluation (59.0% compared to 56.5%) and on nearest neighbor\nevaluation (50.7% compared to 41.4%). To the best of our knowledge, this is the\nfirst time a self-supervised AlexNet has outperformed supervised one on\nImageNet classification. Our code is available here:\nhttps://github.com/UMBCvision/CompRess\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2020 02:49:18 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Koohpayegani", "Soroush Abbasi", ""], ["Tejankar", "Ajinkya", ""], ["Pirsiavash", "Hamed", ""]]}, {"id": "2010.14714", "submitter": "Yu Zhao", "authors": "Yu Zhao, Chung-Kuei Lee", "title": "Differentiable Channel Pruning Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose the differentiable channel pruning search (DCPS) of\nconvolutional neural networks. Unlike traditional channel pruning algorithms\nwhich require users to manually set prune ratio for each convolutional layer,\nDCPS search the optimal combination of prune ratio that automatically. Inspired\nby the differentiable architecture search (DARTS), we draws lessons from the\ncontinuous relaxation and leverages the gradient information to balance the\nmetrics and performance. However, directly applying the DARTS scheme will cause\nchannel mismatching problem and huge memory consumption. Therefore, we\nintroduce a novel weight sharing technique which can elegantly eliminate the\nshape mismatching problem with negligible additional resource. We test the\nproposed algorithm on image classification task and it achieves the\nstate-of-the-art pruning results for image classification on CIFAR-10,\nCIFAR-100 and ImageNet. DCPS is further utilized for semantic segmentation on\nPASCAL VOC 2012 for two purposes. The first is to demonstrate that\ntask-specific channel pruning achieves better performance against transferring\nslim models, and the second is to prove the memory efficiency of DCPS as the\ntask demand more memory budget than classification. Results of the experiments\nvalidate the effectiveness and wide applicability of DCPS.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2020 02:49:32 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Zhao", "Yu", ""], ["Lee", "Chung-Kuei", ""]]}, {"id": "2010.14731", "submitter": "Abdullah-Al-Zubaer Imran", "authors": "Ayaan Haque, Abdullah-Al-Zubaer Imran, Adam Wang, Demetri Terzopoulos", "title": "MultiMix: Sparingly Supervised, Extreme Multitask Learning From Medical\n  Images", "comments": "Accepted to IEEE International Symposium on Biomedical Imaging (ISBI)\n  2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semi-supervised learning via learning from limited quantities of labeled data\nhas been investigated as an alternative to supervised counterparts. Maximizing\nknowledge gains from copious unlabeled data benefit semi-supervised learning\nsettings. Moreover, learning multiple tasks within the same model further\nimproves model generalizability. We propose a novel multitask learning model,\nnamely MultiMix, which jointly learns disease classification and anatomical\nsegmentation in a sparingly supervised manner, while preserving explainability\nthrough bridge saliency between the two tasks. Our extensive experimentation\nwith varied quantities of labeled data in the training sets justify the\neffectiveness of our multitasking model for the classification of pneumonia and\nsegmentation of lungs from chest X-ray images. Moreover, both in-domain and\ncross-domain evaluations across the tasks further showcase the potential of our\nmodel to adapt to challenging generalization scenarios.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2020 03:47:29 GMT"}, {"version": "v2", "created": "Thu, 1 Apr 2021 06:21:28 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Haque", "Ayaan", ""], ["Imran", "Abdullah-Al-Zubaer", ""], ["Wang", "Adam", ""], ["Terzopoulos", "Demetri", ""]]}, {"id": "2010.14742", "submitter": "Hochul Hwang", "authors": "Hochul Hwang, Cheongjae Jang, Geonwoo Park, Junghyun Cho, Ig-Jae Kim", "title": "ElderSim: A Synthetic Data Generation Platform for Human Action\n  Recognition in Eldercare Applications", "comments": "18 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To train deep learning models for vision-based action recognition of elders'\ndaily activities, we need large-scale activity datasets acquired under various\ndaily living environments and conditions. However, most public datasets used in\nhuman action recognition either differ from or have limited coverage of elders'\nactivities in many aspects, making it challenging to recognize elders' daily\nactivities well by only utilizing existing datasets. Recently, such limitations\nof available datasets have actively been compensated by generating synthetic\ndata from realistic simulation environments and using those data to train deep\nlearning models. In this paper, based on these ideas we develop ElderSim, an\naction simulation platform that can generate synthetic data on elders' daily\nactivities. For 55 kinds of frequent daily activities of the elders, ElderSim\ngenerates realistic motions of synthetic characters with various adjustable\ndata-generating options, and provides different output modalities including RGB\nvideos, two- and three-dimensional skeleton trajectories. We then generate KIST\nSynADL, a large-scale synthetic dataset of elders' activities of daily living,\nfrom ElderSim and use the data in addition to real datasets to train three\nstate-of the-art human action recognition models. From the experiments\nfollowing several newly proposed scenarios that assume different real and\nsynthetic dataset configurations for training, we observe a noticeable\nperformance improvement by augmenting our synthetic data. We also offer\nguidance with insights for the effective utilization of synthetic data to help\nrecognize elders' daily activities.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2020 04:27:28 GMT"}], "update_date": "2020-11-22", "authors_parsed": [["Hwang", "Hochul", ""], ["Jang", "Cheongjae", ""], ["Park", "Geonwoo", ""], ["Cho", "Junghyun", ""], ["Kim", "Ig-Jae", ""]]}, {"id": "2010.14782", "submitter": "Xin Ding", "authors": "Xin Ding, Qiong Zhang, William J. Welch", "title": "Classification Beats Regression: Counting of Cells from Greyscale\n  Microscopic Images based on Annotation-free Training Samples", "comments": null, "journal-ref": "The CAAI International Conference on Artificial Intelligence\n  (CICAI 2021)", "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern methods often formulate the counting of cells from microscopic images\nas a regression problem and more or less rely on expensive, manually annotated\ntraining images (e.g., dot annotations indicating the centroids of cells or\nsegmentation masks identifying the contours of cells). This work proposes a\nsupervised learning framework based on classification-oriented convolutional\nneural networks (CNNs) to count cells from greyscale microscopic images without\nusing annotated training images. In this framework, we formulate the cell\ncounting task as an image classification problem, where the cell counts are\ntaken as class labels. This formulation has its limitation when some cell\ncounts in the test stage do not appear in the training data. Moreover, the\nordinal relation among cell counts is not utilized. To deal with these\nlimitations, we propose a simple but effective data augmentation (DA) method to\nsynthesize images for the unseen cell counts. We also introduce an ensemble\nmethod, which can not only moderate the influence of unseen cell counts but\nalso utilize the ordinal information to improve the prediction accuracy. This\nframework outperforms many modern cell counting methods and won the data\nanalysis competition (Case Study 1: Counting Cells From Microscopic Images\nhttps://ssc.ca/en/case-study/case-study-1-counting-cells-microscopic-images) of\nthe 47th Annual Meeting of the Statistical Society of Canada (SSC). Our code is\navailable at https://github.com/anno2020/CellCount_TinyBBBC005.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2020 06:19:30 GMT"}, {"version": "v2", "created": "Thu, 29 Oct 2020 20:35:45 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Ding", "Xin", ""], ["Zhang", "Qiong", ""], ["Welch", "William J.", ""]]}, {"id": "2010.14793", "submitter": "Angira Sharma", "authors": "Angira Sharma, Naeemullah Khan, Ganesh Sundaramoorthi, Philip Torr", "title": "Class-Agnostic Segmentation Loss and Its Application to Salient Object\n  Detection and Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a novel loss function, called class-agnostic\nsegmentation (CAS) loss. With CAS loss the class descriptors are learned during\ntraining of the network. We don't require to define the label of a class\na-priori, rather the CAS loss clusters regions with similar appearance together\nin a weakly-supervised manner. Furthermore, we show that the CAS loss function\nis sparse, bounded, and robust to class-imbalance. We apply our CAS loss\nfunction with fully-convolutional ResNet101 and DeepLab-v3 architectures to the\nbinary segmentation problem of salient object detection. We investigate the\nperformance against the state-of-the-art methods in two settings of low and\nhigh-fidelity training data on seven salient object detection datasets. For\nlow-fidelity training data (incorrect class label) class-agnostic segmentation\nloss outperforms the state-of-the-art methods on salient object detection\ndatasets by staggering margins of around 50%. For high-fidelity training data\n(correct class labels) class-agnostic segmentation models perform as good as\nthe state-of-the-art approaches while beating the state-of-the-art methods on\nmost datasets. In order to show the utility of the loss function across\ndifferent domains we also test on general segmentation dataset, where\nclass-agnostic segmentation loss outperforms cross-entropy based loss by huge\nmargins on both region and edge metrics.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2020 07:11:15 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Sharma", "Angira", ""], ["Khan", "Naeemullah", ""], ["Sundaramoorthi", "Ganesh", ""], ["Torr", "Philip", ""]]}, {"id": "2010.14802", "submitter": "Zhitian Zhang", "authors": "Zhitian Zhang, Jimin Rhim, Taher Ahmadi, Kefan Yang, Angelica Lim, Mo\n  Chen", "title": "SFU-Store-Nav: A Multimodal Dataset for Indoor Human Navigation", "comments": "5 pages, paper submitted to Data In Brief Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article describes a dataset collected in a set of experiments that\ninvolves human participants and a robot. The set of experiments was conducted\nin the computing science robotics lab in Simon Fraser University, Burnaby, BC,\nCanada, and its aim is to gather data containing common gestures, movements,\nand other behaviours that may indicate humans' navigational intent relevant for\nautonomous robot navigation. The experiment simulates a shopping scenario where\nhuman participants come in to pick up items from his/her shopping list and\ninteract with a Pepper robot that is programmed to help the human participant.\nWe collected visual data and motion capture data from 108 human participants.\nThe visual data contains live recordings of the experiments and the motion\ncapture data contains the position and orientation of the human participants in\nworld coordinates. This dataset could be valuable for researchers in the\nrobotics, machine learning and computer vision community.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2020 08:00:35 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Zhang", "Zhitian", ""], ["Rhim", "Jimin", ""], ["Ahmadi", "Taher", ""], ["Yang", "Kefan", ""], ["Lim", "Angelica", ""], ["Chen", "Mo", ""]]}, {"id": "2010.14805", "submitter": "Keunwoo Choi Mr", "authors": "Qiuqiang Kong, Keunwoo Choi, Yuxuan Wang", "title": "Large-Scale MIDI-based Composer Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CV cs.MM eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Music classification is a task to classify a music piece into labels such as\ngenres or composers. We propose large-scale MIDI based composer classification\nsystems using GiantMIDI-Piano, a transcription-based dataset. We propose to use\npiano rolls, onset rolls, and velocity rolls as input representations and use\ndeep neural networks as classifiers. To our knowledge, we are the first to\ninvestigate the composer classification problem with up to 100 composers. By\nusing convolutional recurrent neural networks as models, our MIDI based\ncomposer classification system achieves a 10-composer and a 100-composer\nclassification accuracies of 0.648 and 0.385 (evaluated on 30-second clips) and\n0.739 and 0.489 (evaluated on music pieces), respectively. Our MIDI based\ncomposer system outperforms several audio-based baseline classification\nsystems, indicating the effectiveness of using compact MIDI representations for\ncomposer classification.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2020 08:07:55 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Kong", "Qiuqiang", ""], ["Choi", "Keunwoo", ""], ["Wang", "Yuxuan", ""]]}, {"id": "2010.14808", "submitter": "Jun Ma", "authors": "Jun Ma, Yao Zhang, Song Gu, Cheng Zhu, Cheng Ge, Yichi Zhang, Xingle\n  An, Congcong Wang, Qiyuan Wang, Xin Liu, Shucheng Cao, Qi Zhang, Shangqing\n  Liu, Yunpeng Wang, Yuhui Li, Jian He, Xiaoping Yang", "title": "AbdomenCT-1K: Is Abdominal Organ Segmentation A Solved Problem?", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  With the unprecedented developments in deep learning, automatic segmentation\nof main abdominal organs seems to be a solved problem as state-of-the-art\n(SOTA) methods have achieved comparable results with inter-rater variability on\nmany benchmark datasets. However, most of the existing abdominal datasets only\ncontain single-center, single-phase, single-vendor, or single-disease cases,\nand it is unclear whether the excellent performance can generalize on diverse\ndatasets. This paper presents a large and diverse abdominal CT organ\nsegmentation dataset, termed AbdomenCT-1K, with more than 1000 (1K) CT scans\nfrom 12 medical centers, including multi-phase, multi-vendor, and multi-disease\ncases. Furthermore, we conduct a large-scale study for liver, kidney, spleen,\nand pancreas segmentation and reveal the unsolved segmentation problems of the\nSOTA methods, such as the limited generalization ability on distinct medical\ncenters, phases, and unseen diseases. To advance the unsolved problems, we\nfurther build four organ segmentation benchmarks for fully supervised,\nsemi-supervised, weakly supervised, and continual learning, which are currently\nchallenging and active research topics. Accordingly, we develop a simple and\neffective method for each benchmark, which can be used as out-of-the-box\nmethods and strong baselines. We believe the AbdomenCT-1K dataset will promote\nfuture in-depth research towards clinical applicable abdominal organ\nsegmentation methods. The datasets, codes, and trained models are publicly\navailable at https://github.com/JunMa11/AbdomenCT-1K.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2020 08:15:27 GMT"}, {"version": "v2", "created": "Wed, 21 Jul 2021 03:36:11 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Ma", "Jun", ""], ["Zhang", "Yao", ""], ["Gu", "Song", ""], ["Zhu", "Cheng", ""], ["Ge", "Cheng", ""], ["Zhang", "Yichi", ""], ["An", "Xingle", ""], ["Wang", "Congcong", ""], ["Wang", "Qiyuan", ""], ["Liu", "Xin", ""], ["Cao", "Shucheng", ""], ["Zhang", "Qi", ""], ["Liu", "Shangqing", ""], ["Wang", "Yunpeng", ""], ["Li", "Yuhui", ""], ["He", "Jian", ""], ["Yang", "Xiaoping", ""]]}, {"id": "2010.14810", "submitter": "Quan Kong", "authors": "Quan Kong, Wenpeng Wei, Ziwei Deng, Tomoaki Yoshinaga, Tomokazu\n  Murakami", "title": "Cycle-Contrast for Self-Supervised Video Representation Learning", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present Cycle-Contrastive Learning (CCL), a novel self-supervised method\nfor learning video representation. Following a nature that there is a belong\nand inclusion relation of video and its frames, CCL is designed to find\ncorrespondences across frames and videos considering the contrastive\nrepresentation in their domains respectively. It is different from recent\napproaches that merely learn correspondences across frames or clips. In our\nmethod, the frame and video representations are learned from a single network\nbased on an R3D architecture, with a shared non-linear transformation for\nembedding both frame and video features before the cycle-contrastive loss. We\ndemonstrate that the video representation learned by CCL can be transferred\nwell to downstream tasks of video understanding, outperforming previous methods\nin nearest neighbour retrieval and action recognition tasks on UCF101, HMDB51\nand MMAct.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2020 08:27:58 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Kong", "Quan", ""], ["Wei", "Wenpeng", ""], ["Deng", "Ziwei", ""], ["Yoshinaga", "Tomoaki", ""], ["Murakami", "Tomokazu", ""]]}, {"id": "2010.14819", "submitter": "Kai Han", "authors": "Kai Han, Yunhe Wang, Qiulin Zhang, Wei Zhang, Chunjing Xu, Tong Zhang", "title": "Model Rubik's Cube: Twisting Resolution, Depth and Width for TinyNets", "comments": "NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To obtain excellent deep neural architectures, a series of techniques are\ncarefully designed in EfficientNets. The giant formula for simultaneously\nenlarging the resolution, depth and width provides us a Rubik's cube for neural\nnetworks. So that we can find networks with high efficiency and excellent\nperformance by twisting the three dimensions. This paper aims to explore the\ntwisting rules for obtaining deep neural networks with minimum model sizes and\ncomputational costs. Different from the network enlarging, we observe that\nresolution and depth are more important than width for tiny networks.\nTherefore, the original method, i.e., the compound scaling in EfficientNet is\nno longer suitable. To this end, we summarize a tiny formula for downsizing\nneural architectures through a series of smaller models derived from the\nEfficientNet-B0 with the FLOPs constraint. Experimental results on the ImageNet\nbenchmark illustrate that our TinyNet performs much better than the smaller\nversion of EfficientNets using the inversed giant formula. For instance, our\nTinyNet-E achieves a 59.9% Top-1 accuracy with only 24M FLOPs, which is about\n1.9% higher than that of the previous best MobileNetV3 with similar\ncomputational cost. Code will be available at\nhttps://github.com/huawei-noah/ghostnet/tree/master/tinynet_pytorch, and\nhttps://gitee.com/mindspore/mindspore/tree/master/model_zoo/research/cv/tinynet.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2020 08:49:45 GMT"}, {"version": "v2", "created": "Thu, 24 Dec 2020 09:21:04 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["Han", "Kai", ""], ["Wang", "Yunhe", ""], ["Zhang", "Qiulin", ""], ["Zhang", "Wei", ""], ["Xu", "Chunjing", ""], ["Zhang", "Tong", ""]]}, {"id": "2010.14831", "submitter": "Stan Z Li", "authors": "Stan Z. Li, Zelin Zang, Lirong Wu", "title": "Deep Manifold Transformation for Nonlinear Dimensionality Reduction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Manifold learning-based encoders have been playing important roles in\nnonlinear dimensionality reduction (NLDR) for data exploration. However,\nexisting methods can often fail to preserve geometric, topological and/or\ndistributional structures of data. In this paper, we propose a deep manifold\nlearning framework, called deep manifold transformation (DMT) for unsupervised\nNLDR and embedding learning. DMT enhances deep neural networks by using\ncross-layer local geometry-preserving (LGP) constraints. The LGP constraints\nconstitute the loss for deep manifold learning and serve as geometric\nregularizers for NLDR network training. Extensive experiments on synthetic and\nreal-world data demonstrate that DMT networks outperform existing leading\nmanifold-based NLDR methods in terms of preserving the structures of data.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2020 09:09:41 GMT"}, {"version": "v2", "created": "Tue, 1 Dec 2020 09:26:44 GMT"}, {"version": "v3", "created": "Mon, 3 May 2021 15:24:11 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Li", "Stan Z.", ""], ["Zang", "Zelin", ""], ["Wu", "Lirong", ""]]}, {"id": "2010.14850", "submitter": "Meiling Fang", "authors": "Meiling Fang, Naser Damer, Florian Kirchbuchner, Arjan Kuijper", "title": "Micro Stripes Analyses for Iris Presentation Attack Detection", "comments": "Accepted at International Join Conference on Biometrics (IJCB 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Iris recognition systems are vulnerable to the presentation attacks, such as\ntextured contact lenses or printed images. In this paper, we propose a\nlightweight framework to detect iris presentation attacks by extracting\nmultiple micro-stripes of expanded normalized iris textures. In this procedure,\na standard iris segmentation is modified. For our presentation attack detection\nnetwork to better model the classification problem, the segmented area is\nprocessed to provide lower dimensional input segments and a higher number of\nlearning samples. Our proposed Micro Stripes Analyses (MSA) solution samples\nthe segmented areas as individual stripes. Then, the majority vote makes the\nfinal classification decision of those micro-stripes. Experiments are\ndemonstrated on five databases, where two databases (IIITD-WVU and Notre Dame)\nare from the LivDet-2017 Iris competition. An in-depth experimental evaluation\nof this framework reveals a superior performance compared with state-of-the-art\nalgorithms. Moreover, our solution minimizes the confusion between textured\n(attack) and soft (bona fide) contact lens presentations.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2020 09:55:35 GMT"}, {"version": "v2", "created": "Tue, 3 Nov 2020 11:56:53 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Fang", "Meiling", ""], ["Damer", "Naser", ""], ["Kirchbuchner", "Florian", ""], ["Kuijper", "Arjan", ""]]}, {"id": "2010.14851", "submitter": "Jianyuan Wang", "authors": "Jianyuan Wang, Yiran Zhong, Yuchao Dai, Kaihao Zhang, Pan Ji, Hongdong\n  Li", "title": "Displacement-Invariant Matching Cost Learning for Accurate Optical Flow\n  Estimation", "comments": "NeurIPS 2020, 9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning matching costs has been shown to be critical to the success of the\nstate-of-the-art deep stereo matching methods, in which 3D convolutions are\napplied on a 4D feature volume to learn a 3D cost volume. However, this\nmechanism has never been employed for the optical flow task. This is mainly due\nto the significantly increased search dimension in the case of optical flow\ncomputation, ie, a straightforward extension would require dense 4D\nconvolutions in order to process a 5D feature volume, which is computationally\nprohibitive. This paper proposes a novel solution that is able to bypass the\nrequirement of building a 5D feature volume while still allowing the network to\nlearn suitable matching costs from data. Our key innovation is to decouple the\nconnection between 2D displacements and learn the matching costs at each 2D\ndisplacement hypothesis independently, ie, displacement-invariant cost\nlearning. Specifically, we apply the same 2D convolution-based matching net\nindependently on each 2D displacement hypothesis to learn a 4D cost volume.\nMoreover, we propose a displacement-aware projection layer to scale the learned\ncost volume, which reconsiders the correlation between different displacement\ncandidates and mitigates the multi-modal problem in the learned cost volume.\nThe cost volume is then projected to optical flow estimation through a 2D\nsoft-argmin layer. Extensive experiments show that our approach achieves\nstate-of-the-art accuracy on various datasets, and outperforms all published\noptical flow methods on the Sintel benchmark.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2020 09:57:00 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Wang", "Jianyuan", ""], ["Zhong", "Yiran", ""], ["Dai", "Yuchao", ""], ["Zhang", "Kaihao", ""], ["Ji", "Pan", ""], ["Li", "Hongdong", ""]]}, {"id": "2010.14881", "submitter": "Jan Egger", "authors": "Jan Egger, Christina Gsaxner, Antonio Pepe, Jianning Li", "title": "Medical Deep Learning -- A systematic Meta-Review", "comments": "54 pages, 7 figures, 4 tables, 151 references", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning had a remarkable impact in different scientific disciplines\nduring the last years. This was demonstrated in numerous tasks, where deep\nlearning algorithms were able to outperform the cutting-edge methods, like in\nimage processing and analysis. Moreover, deep learning delivered\nstate-of-the-art results in tasks like autonomous driving, outclassing previous\nattempts. There are even contexts where deep learning outperformed humans, like\nobject recognition and gaming. Another field in which this development is\nshowing a huge potential is the medical domain. With the collection of large\nquantities of patient records and data, and a trend towards personalized\ntreatments, there is a great need for automated and reliable processing and\nanalysis of health information. Patient data is not only collected in clinical\ncenters, like hospitals, but it relates also to data collected by general\npractitioners, mobile healthcare apps, or online websites, just to name a few.\nThis trend resulted in new, massive research efforts during the last years. In\nQ2/2020, the search engine PubMed returned already over 11.000 results for the\nsearch term $'$deep learning$'$, and around 90% of these publications are from\nthe last three years. Hence, a complete overview of the field of $'$medical\ndeep learning$'$ is almost impossible to obtain and getting a full overview of\nmedical sub-fields becomes increasingly more difficult. Nevertheless, several\nreview and survey articles about medical deep learning have been presented\nwithin the last years. They focus, in general, on specific medical scenarios,\nlike the analysis of medical images containing specific pathologies. With these\nsurveys as foundation, the aim of this contribution is to provide a very first\nhigh-level, systematic meta-review of medical deep learning surveys.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2020 11:01:40 GMT"}, {"version": "v2", "created": "Fri, 27 Nov 2020 08:34:34 GMT"}, {"version": "v3", "created": "Fri, 4 Dec 2020 15:09:32 GMT"}, {"version": "v4", "created": "Tue, 15 Dec 2020 11:03:01 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Egger", "Jan", ""], ["Gsaxner", "Christina", ""], ["Pepe", "Antonio", ""], ["Li", "Jianning", ""]]}, {"id": "2010.14919", "submitter": "Andreas B\\\"ar", "authors": "Atiye Sadat Hashemi, Andreas B\\\"ar, Saeed Mozaffari, and Tim\n  Fingscheidt", "title": "Transferable Universal Adversarial Perturbations Using Generative Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks tend to be vulnerable to adversarial perturbations,\nwhich by adding to a natural image can fool a respective model with high\nconfidence. Recently, the existence of image-agnostic perturbations, also known\nas universal adversarial perturbations (UAPs), were discovered. However,\nexisting UAPs still lack a sufficiently high fooling rate, when being applied\nto an unknown target model. In this paper, we propose a novel deep learning\ntechnique for generating more transferable UAPs. We utilize a perturbation\ngenerator and some given pretrained networks so-called source models to\ngenerate UAPs using the ImageNet dataset. Due to the similar feature\nrepresentation of various model architectures in the first layer, we propose a\nloss formulation that focuses on the adversarial energy only in the respective\nfirst layer of the source models. This supports the transferability of our\ngenerated UAPs to any other target model. We further empirically analyze our\ngenerated UAPs and demonstrate that these perturbations generalize very well\ntowards different target models. Surpassing the current state of the art in\nboth, fooling rate and model-transferability, we can show the superiority of\nour proposed approach. Using our generated non-targeted UAPs, we obtain an\naverage fooling rate of 93.36% on the source models (state of the art: 82.16%).\nGenerating our UAPs on the deep ResNet-152, we obtain about a 12% absolute\nfooling rate advantage vs. cutting-edge methods on VGG-16 and VGG-19 target\nmodels.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2020 12:31:59 GMT"}, {"version": "v2", "created": "Thu, 29 Oct 2020 15:19:41 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Hashemi", "Atiye Sadat", ""], ["B\u00e4r", "Andreas", ""], ["Mozaffari", "Saeed", ""], ["Fingscheidt", "Tim", ""]]}, {"id": "2010.14924", "submitter": "Jyri Maanp\\\"a\\\"a", "authors": "Jyri Maanp\\\"a\\\"a, Josef Taher, Petri Manninen, Leo Pakola, Iaroslav\n  Melekhov and Juha Hyypp\\\"a", "title": "Multimodal End-to-End Learning for Autonomous Steering in Adverse Road\n  and Weather Conditions", "comments": "8 pages, 8 figures, included in the conference proceedings of the\n  2020 25th International Conference on Pattern Recognition (ICPR), 2021", "journal-ref": "2020 25th International Conference on Pattern Recognition (ICPR),\n  2021, pp. 699-706", "doi": "10.1109/ICPR48806.2021.9413109", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous driving is challenging in adverse road and weather conditions in\nwhich there might not be lane lines, the road might be covered in snow and the\nvisibility might be poor. We extend the previous work on end-to-end learning\nfor autonomous steering to operate in these adverse real-life conditions with\nmultimodal data. We collected 28 hours of driving data in several road and\nweather conditions and trained convolutional neural networks to predict the car\nsteering wheel angle from front-facing color camera images and lidar range and\nreflectance data. We compared the CNN model performances based on the different\nmodalities and our results show that the lidar modality improves the\nperformances of different multimodal sensor-fusion models. We also performed\non-road tests with different models and they support this observation.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2020 12:38:41 GMT"}, {"version": "v2", "created": "Tue, 29 Jun 2021 11:45:19 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Maanp\u00e4\u00e4", "Jyri", ""], ["Taher", "Josef", ""], ["Manninen", "Petri", ""], ["Pakola", "Leo", ""], ["Melekhov", "Iaroslav", ""], ["Hyypp\u00e4", "Juha", ""]]}, {"id": "2010.14925", "submitter": "Jiancheng Yang", "authors": "Jiancheng Yang, Rui Shi, Bingbing Ni", "title": "MedMNIST Classification Decathlon: A Lightweight AutoML Benchmark for\n  Medical Image Analysis", "comments": "ISBI 2021. Code and dataset are available at\n  https://medmnist.github.io/", "journal-ref": null, "doi": "10.1109/ISBI48211.2021.9434062", "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present MedMNIST, a collection of 10 pre-processed medical open datasets.\nMedMNIST is standardized to perform classification tasks on lightweight 28x28\nimages, which requires no background knowledge. Covering the primary data\nmodalities in medical image analysis, it is diverse on data scale (from 100 to\n100,000) and tasks (binary/multi-class, ordinal regression and multi-label).\nMedMNIST could be used for educational purpose, rapid prototyping, multi-modal\nmachine learning or AutoML in medical image analysis. Moreover, MedMNIST\nClassification Decathlon is designed to benchmark AutoML algorithms on all 10\ndatasets; We have compared several baseline methods, including open-source or\ncommercial AutoML tools. The datasets, evaluation code and baseline methods for\nMedMNIST are publicly available at https://medmnist.github.io/.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2020 12:41:18 GMT"}, {"version": "v2", "created": "Sat, 14 Nov 2020 05:20:20 GMT"}, {"version": "v3", "created": "Wed, 27 Jan 2021 08:58:41 GMT"}, {"version": "v4", "created": "Thu, 20 May 2021 22:47:30 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Yang", "Jiancheng", ""], ["Shi", "Rui", ""], ["Ni", "Bingbing", ""]]}, {"id": "2010.14933", "submitter": "Matteo Ronchetti", "authors": "Matteo Ronchetti, Davide Bacciu", "title": "Generative Tomography Reconstruction", "comments": "Accepted as a poster for the NeurIPS 2020 Workshop on Deep Learning\n  and Inverse Problems", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an end-to-end differentiable architecture for tomography\nreconstruction that directly maps a noisy sinogram into a denoised\nreconstruction. Compared to existing approaches our end-to-end architecture\nproduces more accurate reconstructions while using less parameters and time. We\nalso propose a generative model that, given a noisy sinogram, can sample\nrealistic reconstructions. This generative model can be used as prior inside an\niterative process that, by taking into consideration the physical model, can\nreduce artifacts and errors in the reconstructions.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 18:22:37 GMT"}, {"version": "v2", "created": "Wed, 25 Nov 2020 22:05:56 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Ronchetti", "Matteo", ""], ["Bacciu", "Davide", ""]]}, {"id": "2010.14953", "submitter": "Stanislav Frolov", "authors": "Stanislav Frolov, Shailza Jolly, J\\\"orn Hees, Andreas Dengel", "title": "Leveraging Visual Question Answering to Improve Text-to-Image Synthesis", "comments": "Accepted to the LANTERN workshop at COLING 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Generating images from textual descriptions has recently attracted a lot of\ninterest. While current models can generate photo-realistic images of\nindividual objects such as birds and human faces, synthesising images with\nmultiple objects is still very difficult. In this paper, we propose an\neffective way to combine Text-to-Image (T2I) synthesis with Visual Question\nAnswering (VQA) to improve the image quality and image-text alignment of\ngenerated images by leveraging the VQA 2.0 dataset. We create additional\ntraining samples by concatenating question and answer (QA) pairs and employ a\nstandard VQA model to provide the T2I model with an auxiliary learning signal.\nWe encourage images generated from QA pairs to look realistic and additionally\nminimize an external VQA loss. Our method lowers the FID from 27.84 to 25.38\nand increases the R-prec. from 83.82% to 84.79% when compared to the baseline,\nwhich indicates that T2I synthesis can successfully be improved using a\nstandard VQA model.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2020 13:11:34 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Frolov", "Stanislav", ""], ["Jolly", "Shailza", ""], ["Hees", "J\u00f6rn", ""], ["Dengel", "Andreas", ""]]}, {"id": "2010.14974", "submitter": "Huanqian Yan", "authors": "Yusheng Zhao, Huanqian Yan, Xingxing Wei", "title": "Object Hider: Adversarial Patch Attack Against Object Detectors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have been widely used in many computer vision tasks.\nHowever, it is proved that they are susceptible to small, imperceptible\nperturbations added to the input. Inputs with elaborately designed\nperturbations that can fool deep learning models are called adversarial\nexamples, and they have drawn great concerns about the safety of deep neural\nnetworks. Object detection algorithms are designed to locate and classify\nobjects in images or videos and they are the core of many computer vision\ntasks, which have great research value and wide applications. In this paper, we\nfocus on adversarial attack on some state-of-the-art object detection models.\nAs a practical alternative, we use adversarial patches for the attack. Two\nadversarial patch generation algorithms have been proposed: the heatmap-based\nalgorithm and the consensus-based algorithm. The experiment results have shown\nthat the proposed methods are highly effective, transferable and generic.\nAdditionally, we have applied the proposed methods to competition \"Adversarial\nChallenge on Object Detection\" that is organized by Alibaba on the Tianchi\nplatform and won top 7 in 1701 teams. Code is available at:\nhttps://github.com/FenHua/DetDak\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2020 13:34:16 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Zhao", "Yusheng", ""], ["Yan", "Huanqian", ""], ["Wei", "Xingxing", ""]]}, {"id": "2010.14977", "submitter": "Boyo Chen", "authors": "Boyo Chen, Buo-Fu Chen, Yun-Nung Chen", "title": "Real-time Tropical Cyclone Intensity Estimation by Handling Temporally\n  Heterogeneous Satellite Data", "comments": "under review of AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analyzing big geophysical observational data collected by multiple advanced\nsensors on various satellite platforms promotes our understanding of the\ngeophysical system. For instance, convolutional neural networks (CNN) have\nachieved great success in estimating tropical cyclone (TC) intensity based on\nsatellite data with fixed temporal frequency (e.g., 3 h). However, to achieve\nmore timely (under 30 min) and accurate TC intensity estimates, a deep learning\nmodel is demanded to handle temporally-heterogeneous satellite observations.\nSpecifically, infrared (IR1) and water vapor (WV) images are available under\nevery 15 minutes, while passive microwave rain rate (PMW) is available for\nabout every 3 hours. Meanwhile, the visible (VIS) channel is severely affected\nby noise and sunlight intensity, making it difficult to be utilized. Therefore,\nwe propose a novel framework that combines generative adversarial network (GAN)\nwith CNN. The model utilizes all data, including VIS and PMW information,\nduring the training phase and eventually uses only the high-frequent IR1 and WV\ndata for providing intensity estimates during the predicting phase.\nExperimental results demonstrate that the hybrid GAN-CNN framework achieves\ncomparable precision to the state-of-the-art models, while possessing the\ncapability of increasing the maximum estimation frequency from 3 hours to less\nthan 15 minutes.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2020 13:40:07 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Chen", "Boyo", ""], ["Chen", "Buo-Fu", ""], ["Chen", "Yun-Nung", ""]]}, {"id": "2010.14982", "submitter": "Rui Dai", "authors": "Rui Dai, Srijan Das, Saurav Sharma, Luca Minciullo, Lorenzo Garattoni,\n  Francois Bremond, Gianpiero Francesca", "title": "Toyota Smarthome Untrimmed: Real-World Untrimmed Videos for Activity\n  Detection", "comments": "Toyota Smarthome Untrimmed dataset, project page:\n  https://project.inria.fr/toyotasmarthome", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This work aims at building a large scale dataset with daily-living activities\nperformed in a natural manner. Activities performed in a spontaneous manner\nlead to many real-world challenges that are often ignored by the vision\ncommunity. This includes low inter-class due to the presence of similar\nactivities and high intra-class variance, low camera framing, low resolution,\nlong-tail distribution of activities, and occlusions. To this end, we propose\nthe Toyota Smarthome Untrimmed (TSU) dataset, which provides spontaneous\nactivities with rich and dense annotations to address the detection of complex\nactivities in real-world scenarios.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2020 13:47:16 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Dai", "Rui", ""], ["Das", "Srijan", ""], ["Sharma", "Saurav", ""], ["Minciullo", "Luca", ""], ["Garattoni", "Lorenzo", ""], ["Bremond", "Francois", ""], ["Francesca", "Gianpiero", ""]]}, {"id": "2010.15021", "submitter": "Van Vung Pham", "authors": "Vung Pham, Chau Pham, and Tommy Dang", "title": "Road Damage Detection and Classification with Detectron2 and Faster\n  R-CNN", "comments": "Under review for Global Road Damage Detection Challenge 2020, A Track\n  in the IEEE Big Data 2020 Big Data Cup Challenge", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The road is vital for many aspects of life, and road maintenance is crucial\nfor human safety. One of the critical tasks to allow timely repair of road\ndamages is to quickly and efficiently detect and classify them. This work\ndetails the strategies and experiments evaluated for these tasks. Specifically,\nwe evaluate Detectron2's implementation of Faster R-CNN using different base\nmodels and configurations. We also experiment with these approaches using the\nGlobal Road Damage Detection Challenge 2020, A Track in the IEEE Big Data 2020\nBig Data Cup Challenge dataset. The results show that the X101-FPN base model\nfor Faster R-CNN with Detectron2's default configurations are efficient and\ngeneral enough to be transferable to different countries in this challenge.\nThis approach results in F1 scores of 51.0% and 51.4% for the test1 and test2\nsets of the challenge, respectively. Though the visualizations show good\nprediction results, the F1 scores are low. Therefore, we also evaluate the\nprediction results against the existing annotations and discover some\ndiscrepancies. Thus, we also suggest strategies to improve the labeling process\nfor this dataset.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2020 14:53:17 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Pham", "Vung", ""], ["Pham", "Chau", ""], ["Dang", "Tommy", ""]]}, {"id": "2010.15041", "submitter": "Shan You", "authors": "Xiu Su, Shan You, Tao Huang, Hongyan Xu, Fei Wang, Chen Qian,\n  Changshui Zhang, Chang Xu", "title": "Data Agnostic Filter Gating for Efficient Deep Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To deploy a well-trained CNN model on low-end computation edge devices, it is\nusually supposed to compress or prune the model under certain computation\nbudget (e.g., FLOPs). Current filter pruning methods mainly leverage feature\nmaps to generate important scores for filters and prune those with smaller\nscores, which ignores the variance of input batches to the difference in sparse\nstructure over filters. In this paper, we propose a data agnostic filter\npruning method that uses an auxiliary network named Dagger module to induce\npruning and takes pretrained weights as input to learn the importance of each\nfilter. In addition, to help prune filters with certain FLOPs constraints, we\nleverage an explicit FLOPs-aware regularization to directly promote pruning\nfilters toward target FLOPs. Extensive experimental results on CIFAR-10 and\nImageNet datasets indicate our superiority to other state-of-the-art filter\npruning methods. For example, our 50\\% FLOPs ResNet-50 can achieve 76.1\\% Top-1\naccuracy on ImageNet dataset, surpassing many other filter pruning methods.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2020 15:26:40 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Su", "Xiu", ""], ["You", "Shan", ""], ["Huang", "Tao", ""], ["Xu", "Hongyan", ""], ["Wang", "Fei", ""], ["Qian", "Chen", ""], ["Zhang", "Changshui", ""], ["Xu", "Chang", ""]]}, {"id": "2010.15052", "submitter": "Ryan Steed", "authors": "Ryan Steed and Aylin Caliskan", "title": "Image Representations Learned With Unsupervised Pre-Training Contain\n  Human-like Biases", "comments": "10 pages, 3 figures. Replaced example image completions of real\n  people with completions of artificial people", "journal-ref": null, "doi": "10.1145/3442188.3445932", "report-no": null, "categories": "cs.CY cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Recent advances in machine learning leverage massive datasets of unlabeled\nimages from the web to learn general-purpose image representations for tasks\nfrom image classification to face recognition. But do unsupervised computer\nvision models automatically learn implicit patterns and embed social biases\nthat could have harmful downstream effects? We develop a novel method for\nquantifying biased associations between representations of social concepts and\nattributes in images. We find that state-of-the-art unsupervised models trained\non ImageNet, a popular benchmark image dataset curated from internet images,\nautomatically learn racial, gender, and intersectional biases. We replicate 8\npreviously documented human biases from social psychology, from the innocuous,\nas with insects and flowers, to the potentially harmful, as with race and\ngender. Our results closely match three hypotheses about intersectional bias\nfrom social psychology. For the first time in unsupervised computer vision, we\nalso quantify implicit human biases about weight, disabilities, and several\nethnicities. When compared with statistical patterns in online image datasets,\nour findings suggest that machine learning models can automatically learn bias\nfrom the way people are stereotypically portrayed on the web.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2020 15:55:49 GMT"}, {"version": "v2", "created": "Tue, 19 Jan 2021 20:51:57 GMT"}, {"version": "v3", "created": "Wed, 27 Jan 2021 18:48:10 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Steed", "Ryan", ""], ["Caliskan", "Aylin", ""]]}, {"id": "2010.15054", "submitter": "Geondo Park", "authors": "Geondo Park, June Yong Yang, Sung Ju Hwang, Eunho Yang", "title": "Attribution Preservation in Network Compression for Reliable Network\n  Interpretation", "comments": "NeurIPS 2020. Code: https://github.com/GeondoPark/attribute-preserve", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks embedded in safety-sensitive applications such as\nself-driving cars and wearable health monitors rely on two important\ntechniques: input attribution for hindsight analysis and network compression to\nreduce its size for edge-computing. In this paper, we show that these seemingly\nunrelated techniques conflict with each other as network compression deforms\nthe produced attributions, which could lead to dire consequences for\nmission-critical applications. This phenomenon arises due to the fact that\nconventional network compression methods only preserve the predictions of the\nnetwork while ignoring the quality of the attributions. To combat the\nattribution inconsistency problem, we present a framework that can preserve the\nattributions while compressing a network. By employing the Weighted Collapsed\nAttribution Matching regularizer, we match the attribution maps of the network\nbeing compressed to its pre-compression former self. We demonstrate the\neffectiveness of our algorithm both quantitatively and qualitatively on diverse\ncompression methods.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2020 16:02:31 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Park", "Geondo", ""], ["Yang", "June Yong", ""], ["Hwang", "Sung Ju", ""], ["Yang", "Eunho", ""]]}, {"id": "2010.15075", "submitter": "Noushin Hajarolasvadi", "authors": "Noushin Hajarolasvadi, Miguel Arjona Ram\\'irez and Hasan Demirel", "title": "Generative Adversarial Networks in Human Emotion Synthesis:A Review", "comments": "46 pages, 28 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.HC eess.AS eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Synthesizing realistic data samples is of great value for both academic and\nindustrial communities. Deep generative models have become an emerging topic in\nvarious research areas like computer vision and signal processing. Affective\ncomputing, a topic of a broad interest in computer vision society, has been no\nexception and has benefited from generative models. In fact, affective\ncomputing observed a rapid derivation of generative models during the last two\ndecades. Applications of such models include but are not limited to emotion\nrecognition and classification, unimodal emotion synthesis, and cross-modal\nemotion synthesis. As a result, we conducted a review of recent advances in\nhuman emotion synthesis by studying available databases, advantages, and\ndisadvantages of the generative models along with the related training\nstrategies considering two principal human communication modalities, namely\naudio and video. In this context, facial expression synthesis, speech emotion\nsynthesis, and the audio-visual (cross-modal) emotion synthesis is reviewed\nextensively under different application scenarios. Gradually, we discuss open\nresearch problems to push the boundaries of this research area for future\nworks.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2020 16:45:36 GMT"}, {"version": "v2", "created": "Sat, 7 Nov 2020 11:05:36 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Hajarolasvadi", "Noushin", ""], ["Ram\u00edrez", "Miguel Arjona", ""], ["Demirel", "Hasan", ""]]}, {"id": "2010.15086", "submitter": "Hao-Chiang Shao", "authors": "Hao-Chiang Shao, Ya-Jen Cheng, Meng-Yun Duh, Chia-Wen Lin", "title": "Forgery Blind Inspection for Detecting Manipulations of Gel\n  Electrophoresis Images", "comments": "This version is an extension of Prof. Shao's previous conference\n  paper (IEEE GlobalSIP 2018): \"Unveiling Vestiges of Man-Made Modifications on\n  Molecular-Biological Experiment Images.\"\n  (https://doi.org/10.1109/GlobalSIP.2018.8646594)", "journal-ref": null, "doi": "10.1109/GlobalSIP.2018.8646594", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, falsified images have been found in papers involved in research\nmisconducts. However, although there have been many image forgery detection\nmethods, none of them was designed for molecular-biological experiment images.\nIn this paper, we proposed a fast blind inquiry method, named FBI$_{GEL}$, for\nintegrity of images obtained from two common sorts of molecular experiments,\ni.e., western blot (WB) and polymerase chain reaction (PCR). Based on an\noptimized pseudo-background capable of highlighting local residues, FBI$_{GEL}$\ncan reveal traceable vestiges suggesting inappropriate local modifications on\nWB/PCR images. Additionally, because the optimized pseudo-background is derived\naccording to a closed-form solution, FBI$_{GEL}$ is computationally efficient\nand thus suitable for large scale inquiry tasks for WB/PCR image integrity. We\napplied FBI$_{GEL}$ on several papers questioned by the public on\n\\textbf{PUBPEER}, and our results show that figures of those papers indeed\ncontain doubtful unnatural patterns.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2020 16:59:51 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Shao", "Hao-Chiang", ""], ["Cheng", "Ya-Jen", ""], ["Duh", "Meng-Yun", ""], ["Lin", "Chia-Wen", ""]]}, {"id": "2010.15157", "submitter": "Stefano Gasperini", "authors": "Stefano Gasperini, Mohammad-Ali Nikouei Mahani, Alvaro Marcos-Ramiro,\n  Nassir Navab, Federico Tombari", "title": "Panoster: End-to-end Panoptic Segmentation of LiDAR Point Clouds", "comments": "Preprint of IEEE RA-L article", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Panoptic segmentation has recently unified semantic and instance\nsegmentation, previously addressed separately, thus taking a step further\ntowards creating more comprehensive and efficient perception systems. In this\npaper, we present Panoster, a novel proposal-free panoptic segmentation method\nfor LiDAR point clouds. Unlike previous approaches relying on several steps to\ngroup pixels or points into objects, Panoster proposes a simplified framework\nincorporating a learning-based clustering solution to identify instances. At\ninference time, this acts as a class-agnostic segmentation, allowing Panoster\nto be fast, while outperforming prior methods in terms of accuracy. Without any\npost-processing, Panoster reached state-of-the-art results among published\napproaches on the challenging SemanticKITTI benchmark, and further increased\nits lead by exploiting heuristic techniques. Additionally, we showcase how our\nmethod can be flexibly and effectively applied on diverse existing semantic\narchitectures to deliver panoptic predictions.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2020 18:10:20 GMT"}, {"version": "v2", "created": "Fri, 12 Feb 2021 17:46:31 GMT"}], "update_date": "2021-02-15", "authors_parsed": [["Gasperini", "Stefano", ""], ["Mahani", "Mohammad-Ali Nikouei", ""], ["Marcos-Ramiro", "Alvaro", ""], ["Navab", "Nassir", ""], ["Tombari", "Federico", ""]]}, {"id": "2010.15158", "submitter": "Boyo Chen", "authors": "Boyo Chen, Buo-Fu Chen, Chun-Min Hsiao", "title": "CNN Profiler on Polar Coordinate Images for Tropical Cyclone Structure\n  Analysis", "comments": "Submitted to AAAI2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNN) have achieved great success in analyzing\ntropical cyclones (TC) with satellite images in several tasks, such as TC\nintensity estimation. In contrast, TC structure, which is conventionally\ndescribed by a few parameters estimated subjectively by meteorology\nspecialists, is still hard to be profiled objectively and routinely. This study\napplies CNN on satellite images to create the entire TC structure profiles,\ncovering all the structural parameters. By utilizing the meteorological domain\nknowledge to construct TC wind profiles based on historical structure\nparameters, we provide valuable labels for training in our newly released\nbenchmark dataset. With such a dataset, we hope to attract more attention to\nthis crucial issue among data scientists. Meanwhile, a baseline is established\nwith a specialized convolutional model operating on polar-coordinates. We\ndiscovered that it is more feasible and physically reasonable to extract\nstructural information on polar-coordinates, instead of Cartesian coordinates,\naccording to a TC's rotational and spiral natures. Experimental results on the\nreleased benchmark dataset verified the robustness of the proposed model and\ndemonstrated the potential for applying deep learning techniques for this\nbarely developed yet important topic.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2020 18:13:19 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Chen", "Boyo", ""], ["Chen", "Buo-Fu", ""], ["Hsiao", "Chun-Min", ""]]}, {"id": "2010.15209", "submitter": "Dario Oliveira", "authors": "Dario Augusto Borges Oliveira, Daniil Semin, Semen Zaytsev", "title": "Ground Roll Suppression using Convolutional Neural Networks", "comments": "EAGE 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Seismic data processing plays a major role in seismic exploration as it\nconditions much of the seismic interpretation performance. In this context,\ngenerating reliable post-stack seismic data depends also on disposing of an\nefficient pre-stack noise attenuation tool. Here we tackle ground roll noise,\none of the most challenging and common noises observed in pre-stack seismic\ndata. Since ground roll is characterized by relative low frequencies and high\namplitudes, most commonly used approaches for its suppression are based on\nfrequency-amplitude filters for ground roll characteristic bands. However, when\nsignal and noise share the same frequency ranges, these methods usually deliver\nalso signal suppression or residual noise. In this paper we take advantage of\nthe highly non-linear features of convolutional neural networks, and propose to\nuse different architectures to detect ground roll in shot gathers and\nultimately to suppress them using conditional generative adversarial networks.\nAdditionally, we propose metrics to evaluate ground roll suppression, and\nreport strong results compared to expert filtering. Finally, we discuss\ngeneralization of trained models for similar and different geologies to better\nunderstand the feasibility of our proposal in real applications.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2020 20:21:21 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Oliveira", "Dario Augusto Borges", ""], ["Semin", "Daniil", ""], ["Zaytsev", "Semen", ""]]}, {"id": "2010.15233", "submitter": "Zhenzhen Dai", "authors": "Zhenzhen Dai, Ivan Jambor, Pekka Taimen, Milan Pantelic, Mohamed\n  Elshaikh, Craig Rogers, Otto Ettala, Peter Bostr\\\"om, Hannu Aronen, Harri\n  Merisaari and Ning Wen", "title": "Accurate Prostate Cancer Detection and Segmentation on Biparametric MRI\n  using Non-local Mask R-CNN with Histopathological Ground Truth", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: We aimed to develop deep machine learning (DL) models to improve the\ndetection and segmentation of intraprostatic lesions (IL) on bp-MRI by using\nwhole amount prostatectomy specimen-based delineations. We also aimed to\ninvestigate whether transfer learning and self-training would improve results\nwith small amount labelled data.\n  Methods: 158 patients had suspicious lesions delineated on MRI based on\nbp-MRI, 64 patients had ILs delineated on MRI based on whole mount\nprostatectomy specimen sections, 40 patients were unlabelled. A non-local Mask\nR-CNN was proposed to improve the segmentation accuracy. Transfer learning was\ninvestigated by fine-tuning a model trained using MRI-based delineations with\nprostatectomy-based delineations. Two label selection strategies were\ninvestigated in self-training. The performance of models was evaluated by 3D\ndetection rate, dice similarity coefficient (DSC), 95 percentile Hausdrauff (95\nHD, mm) and true positive ratio (TPR).\n  Results: With prostatectomy-based delineations, the non-local Mask R-CNN with\nfine-tuning and self-training significantly improved all evaluation metrics.\nFor the model with the highest detection rate and DSC, 80.5% (33/41) of lesions\nin all Gleason Grade Groups (GGG) were detected with DSC of 0.548[0.165], 95 HD\nof 5.72[3.17] and TPR of 0.613[0.193]. Among them, 94.7% (18/19) of lesions\nwith GGG > 2 were detected with DSC of 0.604[0.135], 95 HD of 6.26[3.44] and\nTPR of 0.580[0.190].\n  Conclusion: DL models can achieve high prostate cancer detection and\nsegmentation accuracy on bp-MRI based on annotations from histologic images. To\nfurther improve the performance, more data with annotations of both MRI and\nwhole amount prostatectomy specimens are required.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2020 21:07:09 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Dai", "Zhenzhen", ""], ["Jambor", "Ivan", ""], ["Taimen", "Pekka", ""], ["Pantelic", "Milan", ""], ["Elshaikh", "Mohamed", ""], ["Rogers", "Craig", ""], ["Ettala", "Otto", ""], ["Bostr\u00f6m", "Peter", ""], ["Aronen", "Hannu", ""], ["Merisaari", "Harri", ""], ["Wen", "Ning", ""]]}, {"id": "2010.15250", "submitter": "Triet Chau", "authors": "Minh Triet Chau", "title": "Semantic video segmentation for autonomous driving", "comments": "This work was done around 2017. Some minor changes were added", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We aim to solve semantic video segmentation in autonomous driving, namely\nroad detection in real time video, using techniques discussed in (Shelhamer et\nal., 2016a). While fully convolutional network gives good result, we show that\nthe speed can be halved while preserving the accuracy. The test dataset being\nused is KITTI, which consists of real footage from Germany's streets.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2020 21:42:16 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Chau", "Minh Triet", ""]]}, {"id": "2010.15251", "submitter": "Marimuthu Kalimuthu", "authors": "Marimuthu Kalimuthu, Aditya Mogadala, Marius Mosbach, Dietrich Klakow", "title": "Fusion Models for Improved Visual Captioning", "comments": "Accepted at \"Multi-Modal Deep Learning: Challenges and Applications\"\n  (MMDLCA), International Conference on Pattern Recognition (ICPR)-2020,\n  Milano, Italia", "journal-ref": "Springer LNCS, volume 12666, 2021", "doi": "10.1007/978-3-030-68780-9_32", "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Visual captioning aims to generate textual descriptions given images or\nvideos. Traditionally, image captioning models are trained on human annotated\ndatasets such as Flickr30k and MS-COCO, which are limited in size and\ndiversity. This limitation hinders the generalization capabilities of these\nmodels while also rendering them liable to making mistakes. Language models\ncan, however, be trained on vast amounts of freely available unlabelled data\nand have recently emerged as successful language encoders and coherent text\ngenerators. Meanwhile, several unimodal and multimodal fusion techniques have\nbeen proven to work well for natural language generation and automatic speech\nrecognition. Building on these recent developments, and with the aim of\nimproving the quality of generated captions, the contribution of our work in\nthis paper is two-fold: First, we propose a generic multimodal model fusion\nframework for caption generation as well as emendation where we utilize\ndifferent fusion strategies to integrate a pretrained Auxiliary Language Model\n(AuxLM) within the traditional encoder-decoder visual captioning frameworks.\nNext, we employ the same fusion strategies to integrate a pretrained Masked\nLanguage Model (MLM), namely BERT, with a visual captioning model, viz. Show,\nAttend, and Tell, for emending both syntactic and semantic errors in captions.\nOur caption emendation experiments on three benchmark image captioning\ndatasets, viz. Flickr8k, Flickr30k, and MSCOCO, show improvements over the\nbaseline, indicating the usefulness of our proposed multimodal fusion\nstrategies. Further, we perform a preliminary qualitative analysis on the\nemended captions and identify error categories based on the type of\ncorrections.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2020 21:55:25 GMT"}, {"version": "v2", "created": "Sat, 5 Dec 2020 04:01:02 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Kalimuthu", "Marimuthu", ""], ["Mogadala", "Aditya", ""], ["Mosbach", "Marius", ""], ["Klakow", "Dietrich", ""]]}, {"id": "2010.15260", "submitter": "Xin Gao", "authors": "Xin Gao, Sundaresh Ram, and Jeffrey J. Rodriguez", "title": "Object sieving and morphological closing to reduce false detections in\n  wide-area aerial imagery", "comments": "5 Pages, Submitted to 2016 23rd International Conference of Image\n  Processing (ICIP), September 23-28, Phoenix, AZ, USA (Paper ID: 3218)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For object detection in wide-area aerial imagery, post-processing is usually\nneeded to reduce false detections. We propose a two-stage post-processing\nscheme which comprises an area-thresholding sieving process and a morphological\nclosing operation. We use two wide-area aerial videos to compare the\nperformance of five object detection algorithms in the absence and in the\npresence of our post-processing scheme. The automatic detection results are\ncompared with the ground-truth objects. Several metrics are used for\nperformance comparison.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2020 22:20:17 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Gao", "Xin", ""], ["Ram", "Sundaresh", ""], ["Rodriguez", "Jeffrey J.", ""]]}, {"id": "2010.15261", "submitter": "Marvin Eisenberger", "authors": "Marvin Eisenberger, Aysim Toker, Laura Leal-Taix\\'e, Daniel Cremers", "title": "Deep Shells: Unsupervised Shape Correspondence with Optimal Transport", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel unsupervised learning approach to 3D shape correspondence\nthat builds a multiscale matching pipeline into a deep neural network. This\napproach is based on smooth shells, the current state-of-the-art axiomatic\ncorrespondence method, which requires an a priori stochastic search over the\nspace of initial poses. Our goal is to replace this costly preprocessing step\nby directly learning good initializations from the input surfaces. To that end,\nwe systematically derive a fully differentiable, hierarchical matching pipeline\nfrom entropy regularized optimal transport. This allows us to combine it with a\nlocal feature extractor based on smooth, truncated spectral convolution\nfilters. Finally, we show that the proposed unsupervised method significantly\nimproves over the state-of-the-art on multiple datasets, even in comparison to\nthe most recent supervised methods. Moreover, we demonstrate compelling\ngeneralization results by applying our learned filters to examples that\nsignificantly deviate from the training set.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2020 22:24:07 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Eisenberger", "Marvin", ""], ["Toker", "Aysim", ""], ["Leal-Taix\u00e9", "Laura", ""], ["Cremers", "Daniel", ""]]}, {"id": "2010.15269", "submitter": "Anirudh Joshi", "authors": "Viswesh Krishna, Anirudh Joshi, Philip L. Bulterys, Eric Yang, Andrew\n  Y. Ng, Pranav Rajpurkar", "title": "GloFlow: Global Image Alignment for Creation of Whole Slide Images for\n  Pathology from Video", "comments": "Machine Learning for Health (ML4H) at NeurIPS 2020 - Extended\n  Abstract", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The application of deep learning to pathology assumes the existence of\ndigital whole slide images of pathology slides. However, slide digitization is\nbottlenecked by the high cost of precise motor stages in slide scanners that\nare needed for position information used for slide stitching. We propose\nGloFlow, a two-stage method for creating a whole slide image using optical\nflow-based image registration with global alignment using a computationally\ntractable graph-pruning approach. In the first stage, we train an optical flow\npredictor to predict pairwise translations between successive video frames to\napproximate a stitch. In the second stage, this approximate stitch is used to\ncreate a neighborhood graph to produce a corrected stitch. On a simulated\ndataset of video scans of WSIs, we find that our method outperforms known\napproaches to slide-stitching, and stitches WSIs resembling those produced by\nslide scanners.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2020 23:01:31 GMT"}, {"version": "v2", "created": "Thu, 12 Nov 2020 08:53:58 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Krishna", "Viswesh", ""], ["Joshi", "Anirudh", ""], ["Bulterys", "Philip L.", ""], ["Yang", "Eric", ""], ["Ng", "Andrew Y.", ""], ["Rajpurkar", "Pranav", ""]]}, {"id": "2010.15277", "submitter": "Marc Masana Castrillo", "authors": "Marc Masana, Xialei Liu, Bartlomiej Twardowski, Mikel Menta, Andrew D.\n  Bagdanov, Joost van de Weijer", "title": "Class-incremental learning: survey and performance evaluation on image\n  classification", "comments": "Preprint under review. Code publicly available at\n  https://github.com/mmasana/FACIL", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For future learning systems incremental learning is desirable, because it\nallows for: efficient resource usage by eliminating the need to retrain from\nscratch at the arrival of new data; reduced memory usage by preventing or\nlimiting the amount of data required to be stored -- also important when\nprivacy limitations are imposed; and learning that more closely resembles human\nlearning. The main challenge for incremental learning is catastrophic\nforgetting, which refers to the precipitous drop in performance on previously\nlearned tasks after learning a new one. Incremental learning of deep neural\nnetworks has seen explosive growth in recent years. Initial work focused on\ntask-incremental learning, where a task-ID is provided at inference time.\nRecently, we have seen a shift towards class-incremental learning where the\nlearner must discriminate at inference time between all classes seen in\nprevious tasks without recourse to a task-ID. In this paper, we provide a\ncomplete survey of existing class-incremental learning methods for image\nclassification, and in particular we perform an extensive experimental\nevaluation on thirteen class-incremental methods. We consider several new\nexperimental scenarios, including a comparison of class-incremental methods on\nmultiple large-scale image classification datasets, investigation into small\nand large domain shifts, and comparison of various network architectures.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2020 23:28:15 GMT"}, {"version": "v2", "created": "Thu, 6 May 2021 21:30:23 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Masana", "Marc", ""], ["Liu", "Xialei", ""], ["Twardowski", "Bartlomiej", ""], ["Menta", "Mikel", ""], ["Bagdanov", "Andrew D.", ""], ["van de Weijer", "Joost", ""]]}, {"id": "2010.15288", "submitter": "Masood Seyed Mortazavi", "authors": "Masood S. Mortazavi", "title": "Speech-Image Semantic Alignment Does Not Depend on Any Prior\n  Classification Tasks", "comments": null, "journal-ref": "Proceedings of INTERSPEECH 2020", "doi": "10.21437/Interspeech.2020", "report-no": null, "categories": "cs.LG cs.CV cs.IT cs.MM math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantically-aligned $(speech, image)$ datasets can be used to explore\n\"visually-grounded speech\". In a majority of existing investigations, features\nof an image signal are extracted using neural networks \"pre-trained\" on other\ntasks (e.g., classification on ImageNet). In still others, pre-trained networks\nare used to extract audio features prior to semantic embedding. Without\n\"transfer learning\" through pre-trained initialization or pre-trained feature\nextraction, previous results have tended to show low rates of recall in $speech\n\\rightarrow image$ and $image \\rightarrow speech$ queries.\n  Choosing appropriate neural architectures for encoders in the speech and\nimage branches and using large datasets, one can obtain competitive recall\nrates without any reliance on any pre-trained initialization or feature\nextraction: $(speech,image)$ semantic alignment and $speech \\rightarrow image$\nand $image \\rightarrow speech$ retrieval are canonical tasks worthy of\nindependent investigation of their own and allow one to explore other\nquestions---e.g., the size of the audio embedder can be reduced significantly\nwith little loss of recall rates in $speech \\rightarrow image$ and $image\n\\rightarrow speech$ queries.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 00:14:33 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Mortazavi", "Masood S.", ""]]}, {"id": "2010.15302", "submitter": "Yueru Chen", "authors": "Yueru Chen, Yiting Shao, Jing Wang, Ge Li, C.-C. Jay Kuo", "title": "Point Cloud Attribute Compression via Successive Subspace Graph\n  Transform", "comments": "Accepted by VCIP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by the recently proposed successive subspace learning (SSL)\nprinciples, we develop a successive subspace graph transform (SSGT) to address\npoint cloud attribute compression in this work. The octree geometry structure\nis utilized to partition the point cloud, where every node of the octree\nrepresents a point cloud subspace with a certain spatial size. We design a\nweighted graph with self-loop to describe the subspace and define a graph\nFourier transform based on the normalized graph Laplacian. The transforms are\napplied to large point clouds from the leaf nodes to the root node of the\noctree recursively, while the represented subspace is expanded from the\nsmallest one to the whole point cloud successively. It is shown by experimental\nresults that the proposed SSGT method offers better R-D performances than the\nprevious Region Adaptive Haar Transform (RAHT) method.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 01:40:54 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Chen", "Yueru", ""], ["Shao", "Yiting", ""], ["Wang", "Jing", ""], ["Li", "Ge", ""], ["Kuo", "C. -C. Jay", ""]]}, {"id": "2010.15303", "submitter": "Quang Tran", "authors": "Quang Tran and Jeffery R. Roesler", "title": "Automatic joint damage quantification using computer vision and deep\n  learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Joint raveled or spalled damage (henceforth called joint damage) can affect\nthe safety and long-term performance of concrete pavements. It is important to\nassess and quantify the joint damage over time to assist in building action\nplans for maintenance, predicting maintenance costs, and maximize the concrete\npavement service life. A framework for the accurate, autonomous, and rapid\nquantification of joint damage with a low-cost camera is proposed using a\ncomputer vision technique with a deep learning (DL) algorithm. The DL model is\nemployed to train 263 images of sawcuts with joint damage. The trained DL model\nis used for pixel-wise color-masking joint damage in a series of query 2D\nimages, which are used to reconstruct a 3D image using open-source structure\nfrom motion algorithm. Another damage quantification algorithm using a color\nthreshold is applied to detect and compute the surface area of the damage in\nthe 3D reconstructed image. The effectiveness of the framework was validated\nthrough inspecting joint damage at four transverse contraction joints in\nIllinois, USA, including three acceptable joints and one unacceptable joint by\nvisual inspection. The results show the framework achieves 76% recall and 10%\nerror.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 01:41:20 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Tran", "Quang", ""], ["Roesler", "Jeffery R.", ""]]}, {"id": "2010.15314", "submitter": "Drew Linsley", "authors": "Drew Linsley, Junkyung Kim, Alekh Ashok, and Thomas Serre", "title": "Recurrent neural circuits for contour detection", "comments": "Published in ICLR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a deep recurrent neural network architecture that approximates\nvisual cortical circuits. We show that this architecture, which we refer to as\nthe gamma-net, learns to solve contour detection tasks with better sample\nefficiency than state-of-the-art feedforward networks, while also exhibiting a\nclassic perceptual illusion, known as the orientation-tilt illusion. Correcting\nthis illusion significantly reduces gamma-net contour detection accuracy by\ndriving it to prefer low-level edges over high-level object boundary contours.\nOverall, our study suggests that the orientation-tilt illusion is a byproduct\nof neural circuits that help biological visual systems achieve robust and\nefficient contour detection, and that incorporating these circuits in\nartificial neural networks can improve computer vision.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 02:09:40 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Linsley", "Drew", ""], ["Kim", "Junkyung", ""], ["Ashok", "Alekh", ""], ["Serre", "Thomas", ""]]}, {"id": "2010.15315", "submitter": "Mingren Shen", "authors": "Nick Lawrence, Mingren Shen, Ruiqi Yin, Cloris Feng, Dane Morgan", "title": "Exploring Generative Adversarial Networks for Image-to-Image Translation\n  in STEM Simulation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cond-mat.mtrl-sci cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The use of accurate scanning transmission electron microscopy (STEM) image\nsimulation methods require large computation times that can make their use\ninfeasible for the simulation of many images. Other simulation methods based on\nlinear imaging models, such as the convolution method, are much faster but are\ntoo inaccurate to be used in application. In this paper, we explore deep\nlearning models that attempt to translate a STEM image produced by the\nconvolution method to a prediction of the high accuracy multislice image. We\nthen compare our results to those of regression methods. We find that using the\ndeep learning model Generative Adversarial Network (GAN) provides us with the\nbest results and performs at a similar accuracy level to previous regression\nmodels on the same dataset. Codes and data for this project can be found in\nthis GitHub repository, https://github.com/uw-cmg/GAN-STEM-Conv2MultiSlice.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 02:14:57 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Lawrence", "Nick", ""], ["Shen", "Mingren", ""], ["Yin", "Ruiqi", ""], ["Feng", "Cloris", ""], ["Morgan", "Dane", ""]]}, {"id": "2010.15336", "submitter": "Haoyuan Zhang", "authors": "Haoyuan Zhang, Yonghong Hou, Pichao Wang, Zihui Guo, Wanqing Li", "title": "SAR-NAS: Skeleton-based Action Recognition via Neural Architecture\n  Searching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a study of automatic design of neural network\narchitectures for skeleton-based action recognition. Specifically, we encode a\nskeleton-based action instance into a tensor and carefully define a set of\noperations to build two types of network cells: normal cells and reduction\ncells. The recently developed DARTS (Differentiable Architecture Search) is\nadopted to search for an effective network architecture that is built upon the\ntwo types of cells. All operations are 2D based in order to reduce the overall\ncomputation and search space. Experiments on the challenging NTU RGB+D and\nKinectics datasets have verified that most of the networks developed to date\nfor skeleton-based action recognition are likely not compact and efficient. The\nproposed method provides an approach to search for such a compact network that\nis able to achieve comparative or even better performance than the\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 03:24:15 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Zhang", "Haoyuan", ""], ["Hou", "Yonghong", ""], ["Wang", "Pichao", ""], ["Guo", "Zihui", ""], ["Li", "Wanqing", ""]]}, {"id": "2010.15343", "submitter": "Jasper Sebastiaan Wijnands", "authors": "Jasper S. Wijnands, Haifeng Zhao, Kerry A. Nice, Jason Thompson,\n  Katherine Scully, Jingqiu Guo, Mark Stevenson", "title": "Identifying safe intersection design through unsupervised feature\n  extraction from satellite imagery", "comments": "16 pages, 10 figures. Computer-Aided Civil and Infrastructure\n  Engineering (2020)", "journal-ref": null, "doi": "10.1111/mice.12623", "report-no": null, "categories": "cs.CV cs.LG cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The World Health Organization has listed the design of safer intersections as\na key intervention to reduce global road trauma. This article presents the\nfirst study to systematically analyze the design of all intersections in a\nlarge country, based on aerial imagery and deep learning. Approximately 900,000\nsatellite images were downloaded for all intersections in Australia and\ncustomized computer vision techniques emphasized the road infrastructure. A\ndeep autoencoder extracted high-level features, including the intersection's\ntype, size, shape, lane markings, and complexity, which were used to cluster\nsimilar designs. An Australian telematics data set linked infrastructure design\nto driving behaviors captured during 66 million kilometers of driving. This\nshowed more frequent hard acceleration events (per vehicle) at four- than\nthree-way intersections, relatively low hard deceleration frequencies at\nT-intersections, and consistently low average speeds on roundabouts. Overall,\ndomain-specific feature extraction enabled the identification of infrastructure\nimprovements that could result in safer driving behaviors, potentially reducing\nroad trauma.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 03:42:09 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Wijnands", "Jasper S.", ""], ["Zhao", "Haifeng", ""], ["Nice", "Kerry A.", ""], ["Thompson", "Jason", ""], ["Scully", "Katherine", ""], ["Guo", "Jingqiu", ""], ["Stevenson", "Mark", ""]]}, {"id": "2010.15344", "submitter": "Ziyuan Zhao", "authors": "Ziyuan Zhao, Kartik Chopra, Zeng Zeng, Xiaoli Li", "title": "Sea-Net: Squeeze-And-Excitation Attention Net For Diabetic Retinopathy\n  Grading", "comments": "Accepted to ICIP 2020", "journal-ref": "2020 IEEE International Conference on Image Processing (ICIP), pp.\n  2496-2500", "doi": "10.1109/ICIP40778.2020.9191345", "report-no": null, "categories": "cs.CV cs.AI eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diabetes is one of the most common disease in individuals. \\textit{Diabetic\nretinopathy} (DR) is a complication of diabetes, which could lead to blindness.\nAutomatic DR grading based on retinal images provides a great diagnostic and\nprognostic value for treatment planning. However, the subtle differences among\nseverity levels make it difficult to capture important features using\nconventional methods. To alleviate the problems, a new deep learning\narchitecture for robust DR grading is proposed, referred to as SEA-Net, in\nwhich, spatial attention and channel attention are alternatively carried out\nand boosted with each other, improving the classification performance. In\naddition, a hybrid loss function is proposed to further maximize the\ninter-class distance and reduce the intra-class variability. Experimental\nresults have shown the effectiveness of the proposed architecture.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 03:48:01 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Zhao", "Ziyuan", ""], ["Chopra", "Kartik", ""], ["Zeng", "Zeng", ""], ["Li", "Xiaoli", ""]]}, {"id": "2010.15352", "submitter": "Peng Xiao", "authors": "Peng Xiao, Zhongzhou Luo, Yuqing Deng, Gengyuan Wang, and Jin Yuan", "title": "An automated and multi-parametric algorithm for objective analysis of\n  meibography images", "comments": "20 pages, 6 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Meibography is a non-contact imaging technique used by ophthalmologists to\nassist in the evaluation and diagnosis of meibomian gland dysfunction (MGD).\nWhile artificial qualitative analysis of meibography images could lead to low\nrepeatability and efficiency and multi-parametric analysis is demanding to\noffer more comprehensive information in discovering subtle changes of meibomian\nglands during MGD progression, we developed an automated and multi-parametric\nalgorithm for objective and quantitative analysis of meibography images. The\nfull architecture of the algorithm can be divided into three steps: (1)\nsegmentation of the tarsal conjunctiva area as the region of interest (ROI);\n(2) segmentation and identification of glands within the ROI; and (3)\nquantitative multi-parametric analysis including newly defined gland diameter\ndeformation index (DI), gland tortuosity index (TI), and glands signal index\n(SI). To evaluate the performance of the automated algorithm, the similarity\nindex (k) and the segmentation error including the false positive rate (r_P)\nand the false negative rate (r_N) are calculated between the manually defined\nground truth and the automatic segmentations of both the ROI and meibomian\nglands of 15 typical meibography images. The feasibility of the algorithm is\ndemonstrated in analyzing typical meibograhy images.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 04:26:51 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Xiao", "Peng", ""], ["Luo", "Zhongzhou", ""], ["Deng", "Yuqing", ""], ["Wang", "Gengyuan", ""], ["Yuan", "Jin", ""]]}, {"id": "2010.15356", "submitter": "Haiyu Wu", "authors": "Fukang Tian, Haiyu Wu, and Bo Xu", "title": "Financial ticket intelligent recognition system based on deep learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facing the rapid growth in the issuance of financial tickets (or bills,\ninvoices etc.), traditional manual invoice reimbursement and financial\naccounting system are imposing an increasing burden on financial accountants\nand consuming excessive manpower. To solve this problem, we proposes an\niterative self-learning Framework of Financial Ticket intelligent Recognition\nSystem (FFTRS), which can support the fast iterative updating and extensibility\nof the algorithm model, which are the fundamental requirements for a practical\nfinancial accounting system. In addition, we designed a simple yet efficient\nFinancial Ticket Faster Detection network (FTFDNet) and an intelligent data\nwarehouse of financial ticket are designed to strengthen its efficiency and\nperformance. At present, the system can recognize 194 kinds of financial\ntickets and has an automatic iterative optimization mechanism, which means,\nwith the increase of application time, the types of tickets supported by the\nsystem will continue to increase, and the accuracy of recognition will continue\nto improve. Experimental results show that the average recognition accuracy of\nthe system is 97.07%, and the average running time for a single ticket is\n175.67ms. The practical value of the system has been tested in a commercial\napplication, which makes a beneficial attempt for the deep learning technology\nin financial accounting work.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 05:07:40 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Tian", "Fukang", ""], ["Wu", "Haiyu", ""], ["Xu", "Bo", ""]]}, {"id": "2010.15378", "submitter": "Byungju Kim", "authors": "Byungju Kim, Jaeyoung Lee, Kyungsu Kim, Sungjin Kim and Junmo Kim", "title": "Collaborative Method for Incremental Learning on Classification and\n  Generation", "comments": "published on ICIP", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although well-trained deep neural networks have shown remarkable performance\non numerous tasks, they rapidly forget what they have learned as soon as they\nbegin to learn with additional data with the previous data stop being provided.\nIn this paper, we introduce a novel algorithm, Incremental Class Learning with\nAttribute Sharing (ICLAS), for incremental class learning with deep neural\nnetworks. As one of its component, we also introduce a generative model,\nincGAN, which can generate images with increased variety compared with the\ntraining data. Under challenging environment of data deficiency, ICLAS\nincrementally trains classification and the generation networks. Since ICLAS\ntrains both networks, our algorithm can perform multiple times of incremental\nclass learning. The experiments on MNIST dataset demonstrate the advantages of\nour algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 06:34:53 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Kim", "Byungju", ""], ["Lee", "Jaeyoung", ""], ["Kim", "Kyungsu", ""], ["Kim", "Sungjin", ""], ["Kim", "Junmo", ""]]}, {"id": "2010.15413", "submitter": "Christopher Fifty", "authors": "Christopher Fifty, Ehsan Amid, Zhe Zhao, Tianhe Yu, Rohan Anil,\n  Chelsea Finn", "title": "Measuring and Harnessing Transference in Multi-Task Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-task learning can leverage information learned by one task to benefit\nthe training of other tasks. Despite this capacity, naive formulations often\ndegrade performance and in particular, identifying the tasks that would benefit\nfrom co-training remains a challenging design question. In this paper, we\nanalyze the dynamics of information transfer, or transference, across tasks\nthroughout training. Specifically, we develop a similarity measure that can\nquantify transference among tasks and use this quantity to both better\nunderstand the optimization dynamics of multi-task learning as well as improve\noverall learning performance. In the latter case, we propose two methods to\nleverage our transference metric. The first operates at a macro-level by\nselecting which tasks should train together while the second functions at a\nmicro-level by determining how to combine task gradients at each training step.\nWe find these methods can lead to significant improvement over prior work on\nthree supervised multi-task learning benchmarks and one multi-task\nreinforcement learning paradigm.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 08:25:43 GMT"}, {"version": "v2", "created": "Tue, 9 Feb 2021 06:55:50 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Fifty", "Christopher", ""], ["Amid", "Ehsan", ""], ["Zhao", "Zhe", ""], ["Yu", "Tianhe", ""], ["Anil", "Rohan", ""], ["Finn", "Chelsea", ""]]}, {"id": "2010.15417", "submitter": "Mundher Al-Shabi", "authors": "Mundher Al-Shabi, Kelvin Shak, Maxine Tan", "title": "ProCAN: Progressive Growing Channel Attentive Non-Local Network for Lung\n  Nodule Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Lung cancer classification in screening computed tomography (CT) scans is one\nof the most crucial tasks for early detection of this disease. Many lives can\nbe saved if we are able to accurately classify malignant/ cancerous lung\nnodules. Consequently, several deep learning based models have been proposed\nrecently to classify lung nodules as malignant or benign. Nevertheless, the\nlarge variation in the size and heterogeneous appearance of the nodules makes\nthis task an extremely challenging one. We propose a new Progressive Growing\nChannel Attentive Non-Local (ProCAN) network for lung nodule classification.\nThe proposed method addresses this challenge from three different aspects.\nFirst, we enrich the Non-Local network by adding channel-wise attention\ncapability to it. Second, we apply Curriculum Learning principles, whereby we\nfirst train our model on easy examples before hard/ difficult ones. Third, as\nthe classification task gets harder during the Curriculum learning, our model\nis progressively grown to increase its capability of handling the task at hand.\nWe examined our proposed method on two different public datasets and compared\nits performance with state-of-the-art methods in the literature. The results\nshow that the ProCAN model outperforms state-of-the-art methods and achieves an\nAUC of 98.05% and accuracy of 95.28% on the LIDC-IDRI dataset. Moreover, we\nconducted extensive ablation studies to analyze the contribution and effects of\neach new component of our proposed method.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 08:42:11 GMT"}, {"version": "v2", "created": "Sat, 14 Nov 2020 08:38:11 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Al-Shabi", "Mundher", ""], ["Shak", "Kelvin", ""], ["Tan", "Maxine", ""]]}, {"id": "2010.15440", "submitter": "Varun Sundar", "authors": "Salman S. Khan, Varun Sundar, Vivek Boominathan, Ashok Veeraraghavan,\n  and Kaushik Mitra", "title": "FlatNet: Towards Photorealistic Scene Reconstruction from Lensless\n  Measurements", "comments": "Accepted to IEEE Transactions on Pattern Analysis and Machine\n  Intelligence (TPAMI) 2020. Supplementary material attached. For project\n  website, see https://siddiquesalman.github.io/flatnet/", "journal-ref": null, "doi": "10.1109/TPAMI.2020.3033882", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lensless imaging has emerged as a potential solution towards realizing\nultra-miniature cameras by eschewing the bulky lens in a traditional camera.\nWithout a focusing lens, the lensless cameras rely on computational algorithms\nto recover the scenes from multiplexed measurements. However, the current\niterative-optimization-based reconstruction algorithms produce noisier and\nperceptually poorer images. In this work, we propose a non-iterative deep\nlearning based reconstruction approach that results in orders of magnitude\nimprovement in image quality for lensless reconstructions. Our approach, called\n$\\textit{FlatNet}$, lays down a framework for reconstructing high-quality\nphotorealistic images from mask-based lensless cameras, where the camera's\nforward model formulation is known. FlatNet consists of two stages: (1) an\ninversion stage that maps the measurement into a space of intermediate\nreconstruction by learning parameters within the forward model formulation, and\n(2) a perceptual enhancement stage that improves the perceptual quality of this\nintermediate reconstruction. These stages are trained together in an end-to-end\nmanner. We show high-quality reconstructions by performing extensive\nexperiments on real and challenging scenes using two different types of\nlensless prototypes: one which uses a separable forward model and another,\nwhich uses a more general non-separable cropped-convolution model. Our\nend-to-end approach is fast, produces photorealistic reconstructions, and is\neasy to adopt for other mask-based lensless cameras.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 09:20:22 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Khan", "Salman S.", ""], ["Sundar", "Varun", ""], ["Boominathan", "Vivek", ""], ["Veeraraghavan", "Ashok", ""], ["Mitra", "Kaushik", ""]]}, {"id": "2010.15464", "submitter": "Li Tao", "authors": "Li Tao, Xueting Wang, Toshihiko Yamasaki", "title": "Pretext-Contrastive Learning: Toward Good Practices in Self-supervised\n  Video Representation Leaning", "comments": "Under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, pretext-task based methods are proposed one after another in\nself-supervised video feature learning. Meanwhile, contrastive learning methods\nalso yield good performance. Usually, new methods can beat previous ones as\nclaimed that they could capture \"better\" temporal information. However, there\nexist setting differences among them and it is hard to conclude which is\nbetter. It would be much more convincing in comparison if these methods have\nreached as closer to their performance limits as possible. In this paper, we\nstart from one pretext-task baseline, exploring how far it can go by combining\nit with contrastive learning, data pre-processing, and data augmentation. A\nproper setting has been found from extensive experiments, with which huge\nimprovements over the baselines can be achieved, indicating a joint\noptimization framework can boost both pretext task and contrastive learning. We\ndenote the joint optimization framework as Pretext-Contrastive Learning (PCL).\nThe other two pretext task baselines are used to validate the effectiveness of\nPCL. And we can easily outperform current state-of-the-art methods in the same\ntraining manner, showing the effectiveness and the generality of our proposal.\nIt is convenient to treat PCL as a standard training strategy and apply it to\nmany other works in self-supervised video feature learning.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 10:20:35 GMT"}, {"version": "v2", "created": "Sun, 4 Apr 2021 14:42:00 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Tao", "Li", ""], ["Wang", "Xueting", ""], ["Yamasaki", "Toshihiko", ""]]}, {"id": "2010.15487", "submitter": "Arslan Ali", "authors": "Arslan Ali, Andrea Migliorati, Tiziano Bianchi, Enrico Magli", "title": "Beyond cross-entropy: learning highly separable feature distributions\n  for robust and accurate classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has shown outstanding performance in several applications\nincluding image classification. However, deep classifiers are known to be\nhighly vulnerable to adversarial attacks, in that a minor perturbation of the\ninput can easily lead to an error. Providing robustness to adversarial attacks\nis a very challenging task especially in problems involving a large number of\nclasses, as it typically comes at the expense of an accuracy decrease. In this\nwork, we propose the Gaussian class-conditional simplex (GCCS) loss: a novel\napproach for training deep robust multiclass classifiers that provides\nadversarial robustness while at the same time achieving or even surpassing the\nclassification accuracy of state-of-the-art methods. Differently from other\nframeworks, the proposed method learns a mapping of the input classes onto\ntarget distributions in a latent space such that the classes are linearly\nseparable. Instead of maximizing the likelihood of target labels for individual\nsamples, our objective function pushes the network to produce feature\ndistributions yielding high inter-class separation. The mean values of the\ndistributions are centered on the vertices of a simplex such that each class is\nat the same distance from every other class. We show that the regularization of\nthe latent space based on our approach yields excellent classification accuracy\nand inherently provides robustness to multiple adversarial attacks, both\ntargeted and untargeted, outperforming state-of-the-art approaches over\nchallenging datasets.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 11:15:17 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Ali", "Arslan", ""], ["Migliorati", "Andrea", ""], ["Bianchi", "Tiziano", ""], ["Magli", "Enrico", ""]]}, {"id": "2010.15491", "submitter": "Kenule Nwigbo", "authors": "Nwigbo Kenule Tuador, Duong Hung Pham, J\\'er\\^ome Michetti, Adrian\n  Basarab, Denis Kouam\\'e", "title": "A Novel Fast 3D Single Image Super-Resolution Algorithm", "comments": "5 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper introduces a novel computationally efficient method of solving the\n3D single image super-resolution (SR) problem, i.e., reconstruction of a\nhigh-resolution volume from its low-resolution counterpart. The main\ncontribution lies in the original way of handling simultaneously the associated\ndecimation and blurring operators, based on their underlying properties in the\nfrequency domain. In particular, the proposed decomposition technique of the 3D\ndecimation operator allows a straightforward implementation for Tikhonov\nregularization, and can be further used to take into consideration other\nregularization functions such as the total variation, enabling the\ncomputational cost of state-of-the-art algorithms to be considerably decreased.\nNumerical experiments carried out showed that the proposed approach outperforms\nexisting 3D SR methods.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 11:23:28 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Tuador", "Nwigbo Kenule", ""], ["Pham", "Duong Hung", ""], ["Michetti", "J\u00e9r\u00f4me", ""], ["Basarab", "Adrian", ""], ["Kouam\u00e9", "Denis", ""]]}, {"id": "2010.15507", "submitter": "Sherif Mohamed", "authors": "Sherif A.S. Mohamed, Jawad N. Yasin, Mohammad-hashem Haghbayan,\n  Antonio Miele, Jukka Heikkonen, Hannu Tenhunen, and Juha Plosila", "title": "Dynamic Resource-aware Corner Detection for Bio-inspired Vision Sensors", "comments": "Accepted to 25th international conference on pattern recognition\n  (ICPR 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Event-based cameras are vision devices that transmit only brightness changes\nwith low latency and ultra-low power consumption. Such characteristics make\nevent-based cameras attractive in the field of localization and object tracking\nin resource-constrained systems. Since the number of generated events in such\ncameras is huge, the selection and filtering of the incoming events are\nbeneficial from both increasing the accuracy of the features and reducing the\ncomputational load. In this paper, we present an algorithm to detect\nasynchronous corners from a stream of events in real-time on embedded systems.\nThe algorithm is called the Three Layer Filtering-Harris or TLF-Harris\nalgorithm. The algorithm is based on an events' filtering strategy whose\npurpose is 1) to increase the accuracy by deliberately eliminating some\nincoming events, i.e., noise, and 2) to improve the real-time performance of\nthe system, i.e., preserving a constant throughput in terms of input events per\nsecond, by discarding unnecessary events with a limited accuracy loss. An\napproximation of the Harris algorithm, in turn, is used to exploit its\nhigh-quality detection capability with a low-complexity implementation to\nenable seamless real-time performance on embedded computing platforms. The\nproposed algorithm is capable of selecting the best corner candidate among\nneighbors and achieves an average execution time savings of 59 % compared with\nthe conventional Harris score. Moreover, our approach outperforms the competing\nmethods, such as eFAST, eHarris, and FA-Harris, in terms of real-time\nperformance, and surpasses Arc* in terms of accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 12:01:33 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Mohamed", "Sherif A. S.", ""], ["Yasin", "Jawad N.", ""], ["Haghbayan", "Mohammad-hashem", ""], ["Miele", "Antonio", ""], ["Heikkonen", "Jukka", ""], ["Tenhunen", "Hannu", ""], ["Plosila", "Juha", ""]]}, {"id": "2010.15509", "submitter": "Jawad Yasin", "authors": "Jawad N. Yasin, Sherif A.S. Mohamed, Mohammad-hashem Haghbayan, Jukka\n  Heikkonen, Hannu Tenhunen, Muhammad Mehboob Yasin, Juha Plosila", "title": "Night vision obstacle detection and avoidance based on Bio-Inspired\n  Vision Sensors", "comments": "Accepted to IEEE SENSORS 2020", "journal-ref": null, "doi": "10.1109/SENSORS47125.2020.9278914", "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Moving towards autonomy, unmanned vehicles rely heavily on state-of-the-art\ncollision avoidance systems (CAS). However, the detection of obstacles\nespecially during night-time is still a challenging task since the lighting\nconditions are not sufficient for traditional cameras to function properly.\nTherefore, we exploit the powerful attributes of event-based cameras to perform\nobstacle detection in low lighting conditions. Event cameras trigger events\nasynchronously at high output temporal rate with high dynamic range of up to\n120 $dB$. The algorithm filters background activity noise and extracts objects\nusing robust Hough transform technique. The depth of each detected object is\ncomputed by triangulating 2D features extracted utilising LC-Harris. Finally,\nasynchronous adaptive collision avoidance (AACA) algorithm is applied for\neffective avoidance. Qualitative evaluation is compared using event-camera and\ntraditional camera.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 12:02:02 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Yasin", "Jawad N.", ""], ["Mohamed", "Sherif A. S.", ""], ["Haghbayan", "Mohammad-hashem", ""], ["Heikkonen", "Jukka", ""], ["Tenhunen", "Hannu", ""], ["Yasin", "Muhammad Mehboob", ""], ["Plosila", "Juha", ""]]}, {"id": "2010.15510", "submitter": "Sherif Mohamed", "authors": "Sherif A.S. Mohamed, Jawad N. Yasin, Mohammad-Hashem Haghbayan,\n  Antonio Miele, Jukka Heikkonen, Hannu Tenhunen, and Juha Plosila", "title": "Asynchronous Corner Tracking Algorithm based on Lifetime of Events for\n  DAVIS Cameras", "comments": "Accepted to 15th International Symposium on Visual Computing\n  (ISVC2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Event cameras, i.e., the Dynamic and Active-pixel Vision Sensor (DAVIS) ones,\ncapture the intensity changes in the scene and generates a stream of events in\nan asynchronous fashion. The output rate of such cameras can reach up to 10\nmillion events per second in high dynamic environments. DAVIS cameras use novel\nvision sensors that mimic human eyes. Their attractive attributes, such as high\noutput rate, High Dynamic Range (HDR), and high pixel bandwidth, make them an\nideal solution for applications that require high-frequency tracking. Moreover,\napplications that operate in challenging lighting scenarios can exploit the\nhigh HDR of event cameras, i.e., 140 dB compared to 60 dB of traditional\ncameras. In this paper, a novel asynchronous corner tracking method is proposed\nthat uses both events and intensity images captured by a DAVIS camera. The\nHarris algorithm is used to extract features, i.e., frame-corners from\nkeyframes, i.e., intensity images. Afterward, a matching algorithm is used to\nextract event-corners from the stream of events. Events are solely used to\nperform asynchronous tracking until the next keyframe is captured. Neighboring\nevents, within a window size of 5x5 pixels around the event-corner, are used to\ncalculate the velocity and direction of extracted event-corners by fitting the\n2D planar using a randomized Hough transform algorithm. Experimental evaluation\nshowed that our approach is able to update the location of the extracted\ncorners up to 100 times during the blind time of traditional cameras, i.e.,\nbetween two consecutive intensity images.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 12:02:40 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Mohamed", "Sherif A. S.", ""], ["Yasin", "Jawad N.", ""], ["Haghbayan", "Mohammad-Hashem", ""], ["Miele", "Antonio", ""], ["Heikkonen", "Jukka", ""], ["Tenhunen", "Hannu", ""], ["Plosila", "Juha", ""]]}, {"id": "2010.15526", "submitter": "Kelly Payette", "authors": "Kelly Payette, Priscille de Dumast, Hamza Kebiri, Ivan Ezhov, Johannes\n  C. Paetzold, Suprosanna Shit, Asim Iqbal, Romesa Khan, Raimund Kottke,\n  Patrice Grehten, Hui Ji, Levente Lanczi, Marianna Nagy, Monika Beresova, Thi\n  Dao Nguyen, Giancarlo Natalucci, Theofanis Karayannis, Bjoern Menze,\n  Meritxell Bach Cuadra, Andras Jakab", "title": "An automatic multi-tissue human fetal brain segmentation benchmark using\n  the Fetal Tissue Annotation Dataset", "comments": "This is a preprint of an article published in Nature Scientific Data.\n  The final authenticated version is available online at:\n  https://doi.org/10.1038/s41597-021-00946-3", "journal-ref": "Sci Data 8, 167 (2021)", "doi": "10.1038/s41597-021-00946-3", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is critical to quantitatively analyse the developing human fetal brain in\norder to fully understand neurodevelopment in both normal fetuses and those\nwith congenital disorders. To facilitate this analysis, automatic multi-tissue\nfetal brain segmentation algorithms are needed, which in turn requires open\ndatabases of segmented fetal brains. Here we introduce a publicly available\ndatabase of 50 manually segmented pathological and non-pathological fetal\nmagnetic resonance brain volume reconstructions across a range of gestational\nages (20 to 33 weeks) into 7 different tissue categories (external\ncerebrospinal fluid, grey matter, white matter, ventricles, cerebellum, deep\ngrey matter, brainstem/spinal cord). In addition, we quantitatively evaluate\nthe accuracy of several automatic multi-tissue segmentation algorithms of the\ndeveloping human fetal brain. Four research groups participated, submitting a\ntotal of 10 algorithms, demonstrating the benefits the database for the\ndevelopment of automatic algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 12:46:05 GMT"}, {"version": "v2", "created": "Tue, 16 Feb 2021 15:39:55 GMT"}, {"version": "v3", "created": "Thu, 29 Apr 2021 14:53:33 GMT"}, {"version": "v4", "created": "Wed, 7 Jul 2021 12:17:26 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Payette", "Kelly", ""], ["de Dumast", "Priscille", ""], ["Kebiri", "Hamza", ""], ["Ezhov", "Ivan", ""], ["Paetzold", "Johannes C.", ""], ["Shit", "Suprosanna", ""], ["Iqbal", "Asim", ""], ["Khan", "Romesa", ""], ["Kottke", "Raimund", ""], ["Grehten", "Patrice", ""], ["Ji", "Hui", ""], ["Lanczi", "Levente", ""], ["Nagy", "Marianna", ""], ["Beresova", "Monika", ""], ["Nguyen", "Thi Dao", ""], ["Natalucci", "Giancarlo", ""], ["Karayannis", "Theofanis", ""], ["Menze", "Bjoern", ""], ["Cuadra", "Meritxell Bach", ""], ["Jakab", "Andras", ""]]}, {"id": "2010.15528", "submitter": "Yesheng Zhang", "authors": "Yesheng Zhang, Xu Zhao, Dahong Qian", "title": "An End to End Network Architecture for Fundamental Matrix Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a novel end-to-end network architecture to estimate\nfundamental matrix directly from stereo images. To establish a complete working\npipeline, different deep neural networks in charge of finding correspondences\nin images, performing outlier rejection and calculating fundamental matrix, are\nintegrated into an end-to-end network architecture.\n  To well train the network and preserve geometry properties of fundamental\nmatrix, a new loss function is introduced. To evaluate the accuracy of\nestimated fundamental matrix more reasonably, we design a new evaluation metric\nwhich is highly consistent with visualization result. Experiments conducted on\nboth outdoor and indoor data-sets show that this network outperforms\ntraditional methods as well as previous deep learning based methods on various\nmetrics and achieves significant performance improvements.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 12:48:43 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Zhang", "Yesheng", ""], ["Zhao", "Xu", ""], ["Qian", "Dahong", ""]]}, {"id": "2010.15560", "submitter": "Jiahong Wei", "authors": "Jiahong Wei, Zhun Fan", "title": "Genetic U-Net: Automatically Designed Deep Networks for Retinal Vessel\n  Segmentation Using a Genetic Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, many methods based on hand-designed convolutional neural networks\n(CNNs) have achieved promising results in automatic retinal vessel\nsegmentation. However, these CNNs remain constrained in capturing retinal\nvessels in complex fundus images. To improve their segmentation performance,\nthese CNNs tend to have many parameters, which may lead to overfitting and high\ncomputational complexity. Moreover, the manual design of competitive CNNs is\ntime-consuming and requires extensive empirical knowledge. Herein, a novel\nautomated design method, called Genetic U-Net, is proposed to generate a\nU-shaped CNN that can achieve better retinal vessel segmentation but with fewer\narchitecture-based parameters, thereby addressing the above issues. First, we\ndevised a condensed but flexible search space based on a U-shaped\nencoder-decoder. Then, we used an improved genetic algorithm to identify\nbetter-performing architectures in the search space and investigated the\npossibility of finding a superior network architecture with fewer parameters.\nThe experimental results show that the architecture obtained using the proposed\nmethod offered a superior performance with less than 1% of the number of the\noriginal U-Net parameters in particular and with significantly fewer parameters\nthan other state-of-the-art models. Furthermore, through in-depth investigation\nof the experimental results, several effective operations and patterns of\nnetworks to generate superior retinal vessel segmentations were identified.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 13:31:36 GMT"}, {"version": "v2", "created": "Mon, 2 Nov 2020 08:24:07 GMT"}, {"version": "v3", "created": "Tue, 3 Nov 2020 06:52:55 GMT"}, {"version": "v4", "created": "Fri, 11 Jun 2021 09:58:57 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Wei", "Jiahong", ""], ["Fan", "Zhun", ""]]}, {"id": "2010.15603", "submitter": "Kai Wang", "authors": "Xiaojiang Peng, Kai Wang, Zhaoyang Zeng, Qing Li, Jianfei Yang and Yu\n  Qiao", "title": "Suppressing Mislabeled Data via Grouping and Self-Attention", "comments": "This is a novel noisy-robust learning method", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep networks achieve excellent results on large-scale clean data but degrade\nsignificantly when learning from noisy labels. To suppressing the impact of\nmislabeled data, this paper proposes a conceptually simple yet efficient\ntraining block, termed as Attentive Feature Mixup (AFM), which allows paying\nmore attention to clean samples and less to mislabeled ones via sample\ninteractions in small groups. Specifically, this plug-and-play AFM first\nleverages a \\textit{group-to-attend} module to construct groups and assign\nattention weights for group-wise samples, and then uses a \\textit{mixup} module\nwith the attention weights to interpolate massive noisy-suppressed samples. The\nAFM has several appealing benefits for noise-robust deep learning. (i) It does\nnot rely on any assumptions and extra clean subset. (ii) With massive\ninterpolations, the ratio of useless samples is reduced dramatically compared\nto the original noisy ratio. (iii) \\pxj{It jointly optimizes the interpolation\nweights with classifiers, suppressing the influence of mislabeled data via low\nattention weights. (iv) It partially inherits the vicinal risk minimization of\nmixup to alleviate over-fitting while improves it by sampling fewer\nfeature-target vectors around mislabeled data from the mixup vicinal\ndistribution.} Extensive experiments demonstrate that AFM yields\nstate-of-the-art results on two challenging real-world noisy datasets: Food101N\nand Clothing1M. The code will be available at\nhttps://github.com/kaiwang960112/AFM.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 13:54:16 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Peng", "Xiaojiang", ""], ["Wang", "Kai", ""], ["Zeng", "Zhaoyang", ""], ["Li", "Qing", ""], ["Yang", "Jianfei", ""], ["Qiao", "Yu", ""]]}, {"id": "2010.15614", "submitter": "Jiayi Ye", "authors": "Yilin Wang, Jiayi Ye", "title": "An Overview Of 3D Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point cloud 3D object detection has recently received major attention and\nbecomes an active research topic in 3D computer vision community. However,\nrecognizing 3D objects in LiDAR (Light Detection and Ranging) is still a\nchallenge due to the complexity of point clouds. Objects such as pedestrians,\ncyclists, or traffic cones are usually represented by quite sparse points,\nwhich makes the detection quite complex using only point cloud. In this\nproject, we propose a framework that uses both RGB and point cloud data to\nperform multiclass object recognition. We use existing 2D detection models to\nlocalize the region of interest (ROI) on the RGB image, followed by a pixel\nmapping strategy in the point cloud, and finally, lift the initial 2D bounding\nbox to 3D space. We use the recently released nuScenes dataset---a large-scale\ndataset contains many data formats---to training and evaluate our proposed\narchitecture.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 14:04:50 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Wang", "Yilin", ""], ["Ye", "Jiayi", ""]]}, {"id": "2010.15643", "submitter": "Xin Ma", "authors": "Xin Ma, Xiaoqiang Zhou, Huaibo Huang, Zhenhua Chai, Xiaolin Wei, Ran\n  He", "title": "Free-Form Image Inpainting via Contrastive Attention Network", "comments": "Accepted by ICPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most deep learning based image inpainting approaches adopt autoencoder or its\nvariants to fill missing regions in images. Encoders are usually utilized to\nlearn powerful representational spaces, which are important for dealing with\nsophisticated learning tasks. Specifically, in image inpainting tasks, masks\nwith any shapes can appear anywhere in images (i.e., free-form masks) which\nform complex patterns. It is difficult for encoders to capture such powerful\nrepresentations under this complex situation. To tackle this problem, we\npropose a self-supervised Siamese inference network to improve the robustness\nand generalization. It can encode contextual semantics from full resolution\nimages and obtain more discriminative representations. we further propose a\nmulti-scale decoder with a novel dual attention fusion module (DAF), which can\ncombine both the restored and known regions in a smooth way. This multi-scale\narchitecture is beneficial for decoding discriminative representations learned\nby encoders into images layer by layer. In this way, unknown regions will be\nfilled naturally from outside to inside. Qualitative and quantitative\nexperiments on multiple datasets, including facial and natural datasets (i.e.,\nCeleb-HQ, Pairs Street View, Places2 and ImageNet), demonstrate that our\nproposed method outperforms state-of-the-art methods in generating high-quality\ninpainting results.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 14:46:05 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Ma", "Xin", ""], ["Zhou", "Xiaoqiang", ""], ["Huang", "Huaibo", ""], ["Chai", "Zhenhua", ""], ["Wei", "Xiaolin", ""], ["He", "Ran", ""]]}, {"id": "2010.15647", "submitter": "Chenyu Liu", "authors": "Chenyu Liu, Wangbin Ding, Lei Li, Zhen Zhang, Chenhao Pei, Liqin\n  Huang, Xiahai Zhuang", "title": "Brain Tumor Segmentation Network Using Attention-based Fusion and\n  Spatial Relationship Constraint", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Delineating the brain tumor from magnetic resonance (MR) images is critical\nfor the treatment of gliomas. However, automatic delineation is challenging due\nto the complex appearance and ambiguous outlines of tumors. Considering that\nmulti-modal MR images can reflect different tumor biological properties, we\ndevelop a novel multi-modal tumor segmentation network (MMTSN) to robustly\nsegment brain tumors based on multi-modal MR images. The MMTSN is composed of\nthree sub-branches and a main branch. Specifically, the sub-branches are used\nto capture different tumor features from multi-modal images, while in the main\nbranch, we design a spatial-channel fusion block (SCFB) to effectively\naggregate multi-modal features. Additionally, inspired by the fact that the\nspatial relationship between sub-regions of tumor is relatively fixed, e.g.,\nthe enhancing tumor is always in the tumor core, we propose a spatial loss to\nconstrain the relationship between different sub-regions of tumor. We evaluate\nour method on the test set of multi-modal brain tumor segmentation challenge\n2020 (BraTs2020). The method achieves 0.8764, 0.8243 and 0.773 dice score for\nwhole tumor, tumor core and enhancing tumor, respectively.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 14:51:10 GMT"}, {"version": "v2", "created": "Sat, 31 Oct 2020 07:34:53 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Liu", "Chenyu", ""], ["Ding", "Wangbin", ""], ["Li", "Lei", ""], ["Zhang", "Zhen", ""], ["Pei", "Chenhao", ""], ["Huang", "Liqin", ""], ["Zhuang", "Xiahai", ""]]}, {"id": "2010.15675", "submitter": "Gnana Praveen Rajasekar", "authors": "Gnana Praveen R, Eric Granger, Patrick Cardinal", "title": "Deep DA for Ordinal Regression of Pain Intensity Estimation Using\n  Weakly-Labeled Videos", "comments": "Readers are requested to visit the article (arXiv:2008.06392) to\n  access this article", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic estimation of pain intensity from facial expressions in videos has\nan immense potential in health care applications. However, domain adaptation\n(DA) is needed to alleviate the problem of domain shifts that typically occurs\nbetween video data captured in source and target do-mains. Given the laborious\ntask of collecting and annotating videos, and the subjective bias due to\nambiguity among adjacent intensity levels, weakly-supervised learning (WSL)is\ngaining attention in such applications. Yet, most state-of-the-art WSL models\nare typically formulated as regression problems, and do not leverage the\nordinal relation between intensity levels, nor the temporal coherence of\nmultiple consecutive frames. This paper introduces a new deep learn-ing model\nfor weakly-supervised DA with ordinal regression(WSDA-OR), where videos in\ntarget domain have coarse la-bels provided on a periodic basis. The WSDA-OR\nmodel enforces ordinal relationships among the intensity levels as-signed to\nthe target sequences, and associates multiple relevant frames to sequence-level\nlabels (instead of a single frame). In particular, it learns discriminant and\ndomain-invariant feature representations by integrating multiple in-stance\nlearning with deep adversarial DA, where soft Gaussian labels are used to\nefficiently represent the weak ordinal sequence-level labels from the target\ndomain. The proposed approach was validated on the RECOLA video dataset as\nfully-labeled source domain, and UNBC-McMaster video data as weakly-labeled\ntarget domain. We have also validated WSDA-OR on BIOVID and Fatigue (private)\ndatasets for sequence level estimation. Experimental results indicate that our\napproach can provide a significant improvement over the state-of-the-art\nmodels, allowing to achieve a greater localization accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2020 03:20:34 GMT"}, {"version": "v2", "created": "Tue, 2 Feb 2021 12:08:40 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["R", "Gnana Praveen", ""], ["Granger", "Eric", ""], ["Cardinal", "Patrick", ""]]}, {"id": "2010.15682", "submitter": "Lennart Husvogt", "authors": "Lennart Husvogt (1 and 2), Stefan B. Ploner (1), Siyu Chen (2), Daniel\n  Stromer (1, 2), Julia Schottenhamml (1), A. Yasin Alibhai (3), Eric Moult\n  (2), Nadia K. Waheed (3), James G. Fujimoto (2) and Andreas Maier (1) ((1)\n  Friedrich-Alexander-Universit\\\"at Erlangen-N\\\"urnberg Germany, (2)\n  Massachusetts Institute of Technology USA, (3) Tufts School of Medicine USA)", "title": "Maximum a posteriori signal recovery for optical coherence tomography\n  angiography image generation and denoising", "comments": "14 pages, 4 figures, to be published in Biomedical Optics Express", "journal-ref": null, "doi": "10.1364/BOE.408903", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optical coherence tomography angiography (OCTA) is a novel and clinically\npromising imaging modality to image retinal and sub-retinal vasculature. Based\non repeated optical coherence tomography (OCT) scans, intensity changes are\nobserved over time and used to compute OCTA image data. OCTA data are prone to\nnoise and artifacts caused by variations in flow speed and patient movement. We\npropose a novel iterative maximum a posteriori signal recovery algorithm in\norder to generate OCTA volumes with reduced noise and increased image quality.\nThis algorithm is based on previous work on probabilistic OCTA signal models\nand maximum likelihood estimates. Reconstruction results using total variation\nminimization and wavelet shrinkage for regularization were compared against an\nOCTA ground truth volume, merged from six co-registered single OCTA volumes.\nThe results show a significant improvement in peak signal-to-noise ratio and\nstructural similarity. The presented algorithm brings together OCTA image\ngeneration and Bayesian statistics and can be developed into new OCTA image\ngeneration and denoising algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 15:28:22 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Husvogt", "Lennart", "", "1 and 2"], ["Ploner", "Stefan B.", ""], ["Chen", "Siyu", ""], ["Stromer", "Daniel", ""], ["Schottenhamml", "Julia", ""], ["Alibhai", "A. Yasin", ""], ["Moult", "Eric", ""], ["Waheed", "Nadia K.", ""], ["Fujimoto", "James G.", ""], ["Maier", "Andreas", ""]]}, {"id": "2010.15687", "submitter": "Isaac Gerg", "authors": "Isaac Gerg and Vishal Monga", "title": "Deep Autofocus for Synthetic Aperture Sonar", "comments": "Submitted for review", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Synthetic aperture sonar (SAS) requires precise positional and environmental\ninformation to produce well-focused output during the image reconstruction\nstep. However, errors in these measurements are commonly present resulting in\ndefocused imagery. To overcome these issues, an \\emph{autofocus} algorithm is\nemployed as a post-processing step after image reconstruction for the purpose\nof improving image quality using the image content itself. These algorithms are\nusually iterative and metric-based in that they seek to optimize an image\nsharpness metric. In this letter, we demonstrate the potential of machine\nlearning, specifically deep learning, to address the autofocus problem. We\nformulate the problem as a self-supervised, phase error estimation task using a\ndeep network we call Deep Autofocus. Our formulation has the advantages of\nbeing non-iterative (and thus fast) and not requiring ground truth\nfocused-defocused images pairs as often required by other deblurring deep\nlearning methods. We compare our technique against a set of common sharpness\nmetrics optimized using gradient descent over a real-world dataset. Our results\ndemonstrate Deep Autofocus can produce imagery that is perceptually as good as\nbenchmark iterative techniques but at a substantially lower computational cost.\nWe conclude that our proposed Deep Autofocus can provide a more favorable\ncost-quality trade-off than state-of-the-art alternatives with significant\npotential of future research.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 15:31:15 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Gerg", "Isaac", ""], ["Monga", "Vishal", ""]]}, {"id": "2010.15689", "submitter": "Feng Li", "authors": "Feng Li, Runmin Cong, Huihui Bai, Yifan He, Yao Zhao, and Ce Zhu", "title": "Learning Deep Interleaved Networks with Asymmetric Co-Attention for\n  Image Restoration", "comments": "15pages, 15figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, convolutional neural network (CNN) has demonstrated significant\nsuccess for image restoration (IR) tasks (e.g., image super-resolution, image\ndeblurring, rain streak removal, and dehazing). However, existing CNN based\nmodels are commonly implemented as a single-path stream to enrich feature\nrepresentations from low-quality (LQ) input space for final predictions, which\nfail to fully incorporate preceding low-level contexts into later high-level\nfeatures within networks, thereby producing inferior results. In this paper, we\npresent a deep interleaved network (DIN) that learns how information at\ndifferent states should be combined for high-quality (HQ) images\nreconstruction. The proposed DIN follows a multi-path and multi-branch pattern\nallowing multiple interconnected branches to interleave and fuse at different\nstates. In this way, the shallow information can guide deep representative\nfeatures prediction to enhance the feature expression ability. Furthermore, we\npropose asymmetric co-attention (AsyCA) which is attached at each interleaved\nnode to model the feature dependencies. Such AsyCA can not only adaptively\nemphasize the informative features from different states, but also improves the\ndiscriminative ability of networks. Our presented DIN can be trained end-to-end\nand applied to various IR tasks. Comprehensive evaluations on public benchmarks\nand real-world datasets demonstrate that the proposed DIN perform favorably\nagainst the state-of-the-art methods quantitatively and qualitatively.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 15:32:00 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Li", "Feng", ""], ["Cong", "Runmin", ""], ["Bai", "Huihui", ""], ["He", "Yifan", ""], ["Zhao", "Yao", ""], ["Zhu", "Ce", ""]]}, {"id": "2010.15703", "submitter": "Julieta Martinez", "authors": "Julieta Martinez, Jashan Shewakramani, Ting Wei Liu, Ioan Andrei\n  B\\^arsan, Wenyuan Zeng, Raquel Urtasun", "title": "Permute, Quantize, and Fine-tune: Efficient Compression of Neural\n  Networks", "comments": "CVPR 21 Oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compressing large neural networks is an important step for their deployment\nin resource-constrained computational platforms. In this context, vector\nquantization is an appealing framework that expresses multiple parameters using\na single code, and has recently achieved state-of-the-art network compression\non a range of core vision and natural language processing tasks. Key to the\nsuccess of vector quantization is deciding which parameter groups should be\ncompressed together. Previous work has relied on heuristics that group the\nspatial dimension of individual convolutional filters, but a general solution\nremains unaddressed. This is desirable for pointwise convolutions (which\ndominate modern architectures), linear layers (which have no notion of spatial\ndimension), and convolutions (when more than one filter is compressed to the\nsame codeword). In this paper we make the observation that the weights of two\nadjacent layers can be permuted while expressing the same function. We then\nestablish a connection to rate-distortion theory and search for permutations\nthat result in networks that are easier to compress. Finally, we rely on an\nannealed quantization algorithm to better compress the network and achieve\nhigher final accuracy. We show results on image classification, object\ndetection, and segmentation, reducing the gap with the uncompressed model by 40\nto 70% with respect to the current state of the art.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 15:47:26 GMT"}, {"version": "v2", "created": "Sun, 20 Dec 2020 01:43:59 GMT"}, {"version": "v3", "created": "Sat, 10 Apr 2021 22:27:02 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Martinez", "Julieta", ""], ["Shewakramani", "Jashan", ""], ["Liu", "Ting Wei", ""], ["B\u00e2rsan", "Ioan Andrei", ""], ["Zeng", "Wenyuan", ""], ["Urtasun", "Raquel", ""]]}, {"id": "2010.15740", "submitter": "Ahmad B Qasim", "authors": "Ahmad B Qasim, Arnd Pettirsch", "title": "Recurrent Neural Networks for video object detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  There is lots of scientific work about object detection in images. For many\napplications like for example autonomous driving the actual data on which\nclassification has to be done are videos. This work compares different methods,\nespecially those which use Recurrent Neural Networks to detect objects in\nvideos. We differ between feature-based methods, which feed feature maps of\ndifferent frames into the recurrent units, box-level methods, which feed\nbounding boxes with class probabilities into the recurrent units and methods\nwhich use flow networks. This study indicates common outcomes of the compared\nmethods like the benefit of including the temporal context into object\ndetection and states conclusions and guidelines for video object detection\nnetworks.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 16:40:10 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Qasim", "Ahmad B", ""], ["Pettirsch", "Arnd", ""]]}, {"id": "2010.15773", "submitter": "Akshay Agarwal", "authors": "Divyam Anshumaan, Akshay Agarwal, Mayank Vatsa, and Richa Singh", "title": "WaveTransform: Crafting Adversarial Examples via Input Decomposition", "comments": "ECCV Workshop Adversarial Robustness in the Real World 2020, 17\n  pages, 3 Tables, 6 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Frequency spectrum has played a significant role in learning unique and\ndiscriminating features for object recognition. Both low and high frequency\ninformation present in images have been extracted and learnt by a host of\nrepresentation learning techniques, including deep learning. Inspired by this\nobservation, we introduce a novel class of adversarial attacks, namely\n`WaveTransform', that creates adversarial noise corresponding to low-frequency\nand high-frequency subbands, separately (or in combination). The frequency\nsubbands are analyzed using wavelet decomposition; the subbands are corrupted\nand then used to construct an adversarial example. Experiments are performed\nusing multiple databases and CNN models to establish the effectiveness of the\nproposed WaveTransform attack and analyze the importance of a particular\nfrequency component. The robustness of the proposed attack is also evaluated\nthrough its transferability and resiliency against a recent adversarial defense\nalgorithm. Experiments show that the proposed attack is effective against the\ndefense algorithm and is also transferable across CNNs.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 17:16:59 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Anshumaan", "Divyam", ""], ["Agarwal", "Akshay", ""], ["Vatsa", "Mayank", ""], ["Singh", "Richa", ""]]}, {"id": "2010.15775", "submitter": "Vaishnavh Nagarajan", "authors": "Vaishnavh Nagarajan, Anders Andreassen, Behnam Neyshabur", "title": "Understanding the Failure Modes of Out-of-Distribution Generalization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Empirical studies suggest that machine learning models often rely on\nfeatures, such as the background, that may be spuriously correlated with the\nlabel only during training time, resulting in poor accuracy during test-time.\nIn this work, we identify the fundamental factors that give rise to this\nbehavior, by explaining why models fail this way {\\em even} in easy-to-learn\ntasks where one would expect these models to succeed. In particular, through a\ntheoretical study of gradient-descent-trained linear classifiers on some\neasy-to-learn tasks, we uncover two complementary failure modes. These modes\narise from how spurious correlations induce two kinds of skews in the data: one\ngeometric in nature, and another, statistical in nature. Finally, we construct\nnatural modifications of image classification datasets to understand when these\nfailure modes can arise in practice. We also design experiments to isolate the\ntwo failure modes when training modern neural networks on these datasets.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 17:19:03 GMT"}, {"version": "v2", "created": "Thu, 29 Apr 2021 04:08:02 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Nagarajan", "Vaishnavh", ""], ["Andreassen", "Anders", ""], ["Neyshabur", "Behnam", ""]]}, {"id": "2010.15821", "submitter": "Houwen Peng", "authors": "Houwen Peng, Hao Du, Hongyuan Yu, Qi Li, Jing Liao, Jianlong Fu", "title": "Cream of the Crop: Distilling Prioritized Paths For One-Shot Neural\n  Architecture Search", "comments": "NeurIPS 2020, code url: https://github.com/microsoft/Cream", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One-shot weight sharing methods have recently drawn great attention in neural\narchitecture search due to high efficiency and competitive performance.\nHowever, weight sharing across models has an inherent deficiency, i.e.,\ninsufficient training of subnetworks in hypernetworks. To alleviate this\nproblem, we present a simple yet effective architecture distillation method.\nThe central idea is that subnetworks can learn collaboratively and teach each\nother throughout the training process, aiming to boost the convergence of\nindividual models. We introduce the concept of prioritized path, which refers\nto the architecture candidates exhibiting superior performance during training.\nDistilling knowledge from the prioritized paths is able to boost the training\nof subnetworks. Since the prioritized paths are changed on the fly depending on\ntheir performance and complexity, the final obtained paths are the cream of the\ncrop. We directly select the most promising one from the prioritized paths as\nthe final architecture, without using other complex search methods, such as\nreinforcement learning or evolution algorithms. The experiments on ImageNet\nverify such path distillation method can improve the convergence ratio and\nperformance of the hypernetwork, as well as boosting the training of\nsubnetworks. The discovered architectures achieve superior performance compared\nto the recent MobileNetV3 and EfficientNet families under aligned settings.\nMoreover, the experiments on object detection and more challenging search space\nshow the generality and robustness of the proposed method. Code and models are\navailable at https://github.com/microsoft/cream.git.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 17:55:05 GMT"}, {"version": "v2", "created": "Tue, 24 Nov 2020 15:37:37 GMT"}, {"version": "v3", "created": "Mon, 12 Apr 2021 06:30:36 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Peng", "Houwen", ""], ["Du", "Hao", ""], ["Yu", "Hongyuan", ""], ["Li", "Qi", ""], ["Liao", "Jing", ""], ["Fu", "Jianlong", ""]]}, {"id": "2010.15823", "submitter": "Matias Valdenegro-Toro", "authors": "Mohandass Muthuraja and Octavio Arriaga and Paul Pl\\\"oger and Frank\n  Kirchner and Matias Valdenegro-Toro", "title": "Black-Box Optimization of Object Detector Scales", "comments": "17 pages, 7 figures, with appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detectors have improved considerably in the last years by using\nadvanced CNN architectures. However, many detector hyper-parameters are\ngenerally manually tuned, or they are used with values set by the detector\nauthors. Automatic Hyper-parameter optimization has not been explored in\nimproving CNN-based object detectors hyper-parameters. In this work, we propose\nthe use of Black-box optimization methods to tune the prior/default box scales\nin Faster R-CNN and SSD, using Bayesian Optimization, SMAC, and CMA-ES. We show\nthat by tuning the input image size and prior box anchor scale on Faster R-CNN\nmAP increases by 2% on PASCAL VOC 2007, and by 3% with SSD. On the COCO dataset\nwith SSD there are mAP improvement in the medium and large objects, but mAP\ndecreases by 1% in small objects. We also perform a regression analysis to find\nthe significant hyper-parameters to tune.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 17:55:54 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Muthuraja", "Mohandass", ""], ["Arriaga", "Octavio", ""], ["Pl\u00f6ger", "Paul", ""], ["Kirchner", "Frank", ""], ["Valdenegro-Toro", "Matias", ""]]}, {"id": "2010.15824", "submitter": "Dongdong Chen", "authors": "Jie Zhang and Dongdong Chen and Jing Liao and Weiming Zhang and Gang\n  Hua and Nenghai Yu", "title": "Passport-aware Normalization for Deep Model Protection", "comments": "Fix typo error. To Appear in NeurIPs 2020, code at\n  \"https://github.com/ZJZAC/Passport-aware-Normalization\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite tremendous success in many application scenarios, deep learning faces\nserious intellectual property (IP) infringement threats. Considering the cost\nof designing and training a good model, infringements will significantly\ninfringe the interests of the original model owner. Recently, many impressive\nworks have emerged for deep model IP protection. However, they either are\nvulnerable to ambiguity attacks, or require changes in the target network\nstructure by replacing its original normalization layers and hence cause\nsignificant performance drops. To this end, we propose a new passport-aware\nnormalization formulation, which is generally applicable to most existing\nnormalization layers and only needs to add another passport-aware branch for IP\nprotection. This new branch is jointly trained with the target model but\ndiscarded in the inference stage. Therefore it causes no structure change in\nthe target model. Only when the model IP is suspected to be stolen by someone,\nthe private passport-aware branch is added back for ownership verification.\nThrough extensive experiments, we verify its effectiveness in both image and 3D\npoint recognition models. It is demonstrated to be robust not only to common\nattack techniques like fine-tuning and model compression, but also to ambiguity\nattacks. By further combining it with trigger-set based methods, both black-box\nand white-box verification can be achieved for enhanced security of deep\nlearning models deployed in real systems. Code can be found at\nhttps://github.com/ZJZAC/Passport-aware-Normalization.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 17:57:12 GMT"}, {"version": "v2", "created": "Tue, 3 Nov 2020 16:19:59 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Zhang", "Jie", ""], ["Chen", "Dongdong", ""], ["Liao", "Jing", ""], ["Zhang", "Weiming", ""], ["Hua", "Gang", ""], ["Yu", "Nenghai", ""]]}, {"id": "2010.15831", "submitter": "Han Hu", "authors": "Cheng Chi and Fangyun Wei and Han Hu", "title": "RelationNet++: Bridging Visual Representations for Object Detection via\n  Transformer Decoder", "comments": "NeurIPS2020 Spotlight", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing object detection frameworks are usually built on a single format of\nobject/part representation, i.e., anchor/proposal rectangle boxes in RetinaNet\nand Faster R-CNN, center points in FCOS and RepPoints, and corner points in\nCornerNet. While these different representations usually drive the frameworks\nto perform well in different aspects, e.g., better classification or finer\nlocalization, it is in general difficult to combine these representations in a\nsingle framework to make good use of each strength, due to the heterogeneous or\nnon-grid feature extraction by different representations. This paper presents\nan attention-based decoder module similar as that in\nTransformer~\\cite{vaswani2017attention} to bridge other representations into a\ntypical object detector built on a single representation format, in an\nend-to-end fashion. The other representations act as a set of \\emph{key}\ninstances to strengthen the main \\emph{query} representation features in the\nvanilla detectors. Novel techniques are proposed towards efficient computation\nof the decoder module, including a \\emph{key sampling} approach and a\n\\emph{shared location embedding} approach. The proposed module is named\n\\emph{bridging visual representations} (BVR). It can perform in-place and we\ndemonstrate its broad effectiveness in bridging other representations into\nprevalent object detection frameworks, including RetinaNet, Faster R-CNN, FCOS\nand ATSS, where about $1.5\\sim3.0$ AP improvements are achieved. In particular,\nwe improve a state-of-the-art framework with a strong backbone by about $2.0$\nAP, reaching $52.7$ AP on COCO test-dev. The resulting network is named\nRelationNet++. The code will be available at\nhttps://github.com/microsoft/RelationNet2.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 17:59:45 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Chi", "Cheng", ""], ["Wei", "Fangyun", ""], ["Hu", "Han", ""]]}, {"id": "2010.15865", "submitter": "Danial Maleki", "authors": "Danial Maleki, Mehdi Afshari, Morteza Babaie, H.R. Tizhoosh", "title": "Ink Marker Segmentation in Histopathology Images Using Deep Learning", "comments": "Accepted for publication in the 15th International Symposium on\n  Visual Computing (ISVC 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the recent advancements in machine vision, digital pathology has\ngained significant attention. Histopathology images are distinctly rich in\nvisual information. The tissue glass slide images are utilized for disease\ndiagnosis. Researchers study many methods to process histopathology images and\nfacilitate fast and reliable diagnosis; therefore, the availability of\nhigh-quality slides becomes paramount. The quality of the images can be\nnegatively affected when the glass slides are ink-marked by pathologists to\ndelineate regions of interest. As an example, in one of the largest public\nhistopathology datasets, The Cancer Genome Atlas (TCGA), approximately $12\\%$\nof the digitized slides are affected by manual delineations through ink\nmarkings. To process these open-access slide images and other repositories for\nthe design and validation of new methods, an algorithm to detect the marked\nregions of the images is essential to avoid confusing tissue pixels with\nink-colored pixels for computer methods. In this study, we propose to segment\nthe ink-marked areas of pathology patches through a deep network. A dataset\nfrom $79$ whole slide images with $4,305$ patches was created and different\nnetworks were trained. Finally, the results showed an FPN model with the\nEffiecentNet-B3 as the backbone was found to be the superior configuration with\nan F1 score of $94.53\\%$.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 18:09:59 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Maleki", "Danial", ""], ["Afshari", "Mehdi", ""], ["Babaie", "Morteza", ""], ["Tizhoosh", "H. R.", ""]]}, {"id": "2010.15886", "submitter": "Yongwei Wang", "authors": "Yongwei Wang, Xin Ding, Li Ding, Rabab Ward, Z. Jane Wang", "title": "Perception Matters: Exploring Imperceptible and Transferable\n  Anti-forensics for GAN-generated Fake Face Imagery Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, generative adversarial networks (GANs) can generate photo-realistic\nfake facial images which are perceptually indistinguishable from real face\nphotos, promoting research on fake face detection. Though fake face forensics\ncan achieve high detection accuracy, their anti-forensic counterparts are less\ninvestigated. Here we explore more \\textit{imperceptible} and\n\\textit{transferable} anti-forensics for fake face imagery detection based on\nadversarial attacks. Since facial and background regions are often smooth, even\nsmall perturbation could cause noticeable perceptual impairment in fake face\nimages. Therefore it makes existing adversarial attacks ineffective as an\nanti-forensic method. Our perturbation analysis reveals the intuitive reason of\nthe perceptual degradation issue when directly applying existing attacks. We\nthen propose a novel adversarial attack method, better suitable for image\nanti-forensics, in the transformed color domain by considering visual\nperception. Simple yet effective, the proposed method can fool both deep\nlearning and non-deep learning based forensic detectors, achieving higher\nattack success rate and significantly improved visual quality. Specially, when\nadversaries consider imperceptibility as a constraint, the proposed\nanti-forensic method can improve the average attack success rate by around 30\\%\non fake face images over two baseline attacks. \\textit{More imperceptible} and\n\\textit{more transferable}, the proposed method raises new security concerns to\nfake face imagery detection. We have released our code for public use, and\nhopefully the proposed method can be further explored in related forensic\napplications as an anti-forensic benchmark.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 18:54:06 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Wang", "Yongwei", ""], ["Ding", "Xin", ""], ["Ding", "Li", ""], ["Ward", "Rabab", ""], ["Wang", "Z. Jane", ""]]}, {"id": "2010.15891", "submitter": "Nitin Kamra", "authors": "Nitin Kamra, Hao Zhu, Dweep Trivedi, Ming Zhang, Yan Liu", "title": "Multi-agent Trajectory Prediction with Fuzzy Query Attention", "comments": "NeurIPS 2020 Camera-ready version. Code:\n  https://github.com/nitinkamra1992/FQA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Trajectory prediction for scenes with multiple agents and entities is a\nchallenging problem in numerous domains such as traffic prediction, pedestrian\ntracking and path planning. We present a general architecture to address this\nchallenge which models the crucial inductive biases of motion, namely, inertia,\nrelative motion, intents and interactions. Specifically, we propose a\nrelational model to flexibly model interactions between agents in diverse\nenvironments. Since it is well-known that human decision making is fuzzy by\nnature, at the core of our model lies a novel attention mechanism which models\ninteractions by making continuous-valued (fuzzy) decisions and learning the\ncorresponding responses. Our architecture demonstrates significant performance\ngains over existing state-of-the-art predictive models in diverse domains such\nas human crowd trajectories, US freeway traffic, NBA sports data and physics\ndatasets. We also present ablations and augmentations to understand the\ndecision-making process and the source of gains in our model.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 19:12:12 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Kamra", "Nitin", ""], ["Zhu", "Hao", ""], ["Trivedi", "Dweep", ""], ["Zhang", "Ming", ""], ["Liu", "Yan", ""]]}, {"id": "2010.15904", "submitter": "Andre G Hochuli", "authors": "Andre G. Hochuli, Alceu S. Britto Jr, David A. Saji, Jose M. Saavedra,\n  Robert Sabourin, Luiz S. Oliveira", "title": "A Comprehensive Comparison of End-to-End Approaches for Handwritten\n  Digit String Recognition", "comments": null, "journal-ref": "Expert System with Applications (2020)", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the last decades, most approaches proposed for handwritten digit string\nrecognition (HDSR) have resorted to digit segmentation, which is dominated by\nheuristics, thereby imposing substantial constraints on the final performance.\nFew of them have been based on segmentation-free strategies where each pixel\ncolumn has a potential cut location. Recently, segmentation-free strategies has\nadded another perspective to the problem, leading to promising results.\nHowever, these strategies still show some limitations when dealing with a large\nnumber of touching digits. To bridge the resulting gap, in this paper, we\nhypothesize that a string of digits can be approached as a sequence of objects.\nWe thus evaluate different end-to-end approaches to solve the HDSR problem,\nparticularly in two verticals: those based on object-detection (e.g., Yolo and\nRetinaNet) and those based on sequence-to-sequence representation (CRNN). The\nmain contribution of this work lies in its provision of a comprehensive\ncomparison with a critical analysis of the above mentioned strategies on five\nbenchmarks commonly used to assess HDSR, including the challenging Touching\nPair dataset, NIST SD19, and two real-world datasets (CAR and CVL) proposed for\nthe ICFHR 2014 competition on HDSR. Our results show that the Yolo model\ncompares favorably against segmentation-free models with the advantage of\nhaving a shorter pipeline that minimizes the presence of heuristics-based\nmodels. It achieved a 97%, 96%, and 84% recognition rate on the NIST-SD19, CAR,\nand CVL datasets, respectively.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 19:38:08 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Hochuli", "Andre G.", ""], ["Britto", "Alceu S.", "Jr"], ["Saji", "David A.", ""], ["Saavedra", "Jose M.", ""], ["Sabourin", "Robert", ""], ["Oliveira", "Luiz S.", ""]]}, {"id": "2010.15937", "submitter": "Daniel Ohrenstein", "authors": "Daniel C. Ohrenstein, Patrick Brandao, Daniel Toth, Laurence Lovat,\n  Danail Stoyanov and Peter Mountney", "title": "Detecting small polyps using a Dynamic SSD-GAN", "comments": "Machine Learning for Health (ML4H) at NeurIPS 2020 - Extended\n  Abstract", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Endoscopic examinations are used to inspect the throat, stomach and bowel for\npolyps which could develop into cancer. Machine learning systems can be trained\nto process colonoscopy images and detect polyps. However, these systems tend to\nperform poorly on objects which appear visually small in the images. It is\nshown here that combining the single-shot detector as a region proposal network\nwith an adversarially-trained generator to upsample small region proposals can\nsignificantly improve the detection of visually-small polyps. The Dynamic\nSSD-GAN pipeline introduced in this paper achieved a 12% increase in\nsensitivity on visually-small polyps compared to a conventional FCN baseline.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 20:54:34 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Ohrenstein", "Daniel C.", ""], ["Brandao", "Patrick", ""], ["Toth", "Daniel", ""], ["Lovat", "Laurence", ""], ["Stoyanov", "Danail", ""], ["Mountney", "Peter", ""]]}, {"id": "2010.15942", "submitter": "Ruohan Zhang", "authors": "Ruohan Zhang, Sihang Guo, Bo Liu, Yifeng Zhu, Mary Hayhoe, Dana\n  Ballard, Peter Stone", "title": "Machine versus Human Attention in Deep Reinforcement Learning Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep reinforcement learning (RL) algorithms are powerful tools for solving\nvisuomotor decision tasks. However, the trained models are often difficult to\ninterpret, because they are represented as end-to-end deep neural networks. In\nthis paper, we shed light on the inner workings of such trained models by\nanalyzing the pixels that they attend to during task execution, and comparing\nthem with the pixels attended to by humans executing the same tasks. To this\nend, we investigate the following two questions that, to the best of our\nknowledge, have not been previously studied. 1) How similar are the visual\nfeatures learned by RL agents and humans when performing the same task? and, 2)\nHow do similarities and differences in these learned features explain RL\nagents' performance on these tasks? Specifically, we compare the saliency maps\nof RL agents against visual attention models of human experts when learning to\nplay Atari games. Further, we analyze how hyperparameters of the deep RL\nalgorithm affect the learned features and saliency maps of the trained agents.\nThe insights provided by our results have the potential to inform novel\nalgorithms for the purpose of closing the performance gap between human experts\nand deep RL agents.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 20:58:45 GMT"}, {"version": "v2", "created": "Wed, 10 Feb 2021 21:25:41 GMT"}], "update_date": "2021-02-12", "authors_parsed": [["Zhang", "Ruohan", ""], ["Guo", "Sihang", ""], ["Liu", "Bo", ""], ["Zhu", "Yifeng", ""], ["Hayhoe", "Mary", ""], ["Ballard", "Dana", ""], ["Stone", "Peter", ""]]}, {"id": "2010.15947", "submitter": "Shubhang Bhatnagar", "authors": "Shubhang Bhatnagar, Sachin Goyal, Darshan Tank, Amit Sethi", "title": "PAL : Pretext-based Active Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of pool-based active learning is to judiciously select a fixed-sized\nsubset of unlabeled samples from a pool to query an oracle for their labels, in\norder to maximize the accuracy of a supervised learner. However, the unsaid\nrequirement that the oracle should always assign correct labels is unreasonable\nfor most situations. We propose an active learning technique for deep neural\nnetworks that is more robust to mislabeling than the previously proposed\ntechniques. Previous techniques rely on the task network itself to estimate the\nnovelty of the unlabeled samples, but learning the task (generalization) and\nselecting samples (out-of-distribution detection) can be conflicting goals. We\nuse a separate network to score the unlabeled samples for selection. The\nscoring network relies on self-supervision for modeling the distribution of the\nlabeled samples to reduce the dependency on potentially noisy labels. To\ncounter the paucity of data, we also deploy another head on the scoring network\nfor regularization via multi-task learning and use an unusual self-balancing\nhybrid scoring function. Furthermore, we divide each query into sub-queries\nbefore labeling to ensure that the query has diverse samples. In addition to\nhaving a higher tolerance to mislabeling of samples by the oracle, the\nresultant technique also produces competitive accuracy in the absence of label\nnoise. The technique also handles the introduction of new classes on-the-fly\nwell by temporarily increasing the sampling rate of these classes.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 21:16:37 GMT"}, {"version": "v2", "created": "Thu, 26 Nov 2020 20:59:39 GMT"}, {"version": "v3", "created": "Sun, 28 Mar 2021 21:04:37 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Bhatnagar", "Shubhang", ""], ["Goyal", "Sachin", ""], ["Tank", "Darshan", ""], ["Sethi", "Amit", ""]]}, {"id": "2010.15974", "submitter": "Jose Oramas", "authors": "Roger Granda, Tinne Tuytelaars, Jose Oramas", "title": "Can the state of relevant neurons in a deep neural networks serve as\n  indicators for detecting adversarial attacks?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for adversarial attack detection based on the inspection\nof a sparse set of neurons. We follow the hypothesis that adversarial attacks\nintroduce imperceptible perturbations in the input and that these perturbations\nchange the state of neurons relevant for the concepts modelled by the attacked\nmodel. Therefore, monitoring the status of these neurons would enable the\ndetection of adversarial attacks. Focusing on the image classification task,\nour method identifies neurons that are relevant for the classes predicted by\nthe model. A deeper qualitative inspection of these sparse set of neurons\nindicates that their state changes in the presence of adversarial samples.\nMoreover, quantitative results from our empirical evaluation indicate that our\nmethod is capable of recognizing adversarial samples, produced by\nstate-of-the-art attack methods, with comparable accuracy to that of\nstate-of-the-art detectors.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 22:31:42 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Granda", "Roger", ""], ["Tuytelaars", "Tinne", ""], ["Oramas", "Jose", ""]]}, {"id": "2010.15987", "submitter": "Kadri Aditya Mohan", "authors": "K. Aditya Mohan, Alan D. Kaplan", "title": "AutoAtlas: Neural Network for 3D Unsupervised Partitioning and\n  Representation Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel neural network architecture called AutoAtlas for fully\nunsupervised partitioning and representation learning of 3D brain Magnetic\nResonance Imaging (MRI) volumes. AutoAtlas consists of two neural network\ncomponents: one that performs multi-label partitioning based on local texture\nin the volume and a second that compresses the information contained within\neach partition. We train both of these components simultaneously by optimizing\na loss function that is designed to promote accurate reconstruction of each\npartition, while encouraging spatially smooth and contiguous partitioning, and\ndiscouraging relatively small partitions. We show that the partitions adapt to\nthe subject specific structural variations of brain tissue while consistently\nappearing at similar spatial locations across subjects. AutoAtlas also produces\nvery low dimensional features that represent local texture of each partition.\nWe demonstrate prediction of metadata associated with each subject using the\nderived feature representations and compare the results to prediction using\nfeatures derived from FreeSurfer anatomical parcellation. Since our features\nare intrinsically linked to distinct partitions, we can then map values of\ninterest, such as partition-specific feature importance scores onto the brain\nfor visualization.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 23:07:31 GMT"}, {"version": "v2", "created": "Thu, 5 Nov 2020 20:48:35 GMT"}], "update_date": "2020-11-09", "authors_parsed": [["Mohan", "K. Aditya", ""], ["Kaplan", "Alan D.", ""]]}, {"id": "2010.16003", "submitter": "Seowoo Han", "authors": "Seo Woo Han, Doug Young Suh", "title": "PIINET: A 360-degree Panoramic Image Inpainting Network Using a Cube Map", "comments": null, "journal-ref": "Vol.66, No.1, 2021, pp.213-228", "doi": "10.32604/cmc.2020.012223", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Inpainting has been continuously studied in the field of computer vision. As\nartificial intelligence technology developed, deep learning technology was\nintroduced in inpainting research, helping to improve performance. Currently,\nthe input target of an inpainting algorithm using deep learning has been\nstudied from a single image to a video. However, deep learning-based inpainting\ntechnology for panoramic images has not been actively studied. We propose a\n360-degree panoramic image inpainting method using generative adversarial\nnetworks (GANs). The proposed network inputs a 360-degree equirectangular\nformat panoramic image converts it into a cube map format, which has relatively\nlittle distortion and uses it as a training network. Since the cube map format\nis used, the correlation of the six sides of the cube map should be considered.\nTherefore, all faces of the cube map are used as input for the whole\ndiscriminative network, and each face of the cube map is used as input for the\nslice discriminative network to determine the authenticity of the generated\nimage. The proposed network performed qualitatively better than existing\nsingle-image inpainting algorithms and baseline algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2020 00:42:45 GMT"}, {"version": "v2", "created": "Tue, 26 Jan 2021 05:20:20 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Han", "Seo Woo", ""], ["Suh", "Doug Young", ""]]}, {"id": "2010.16010", "submitter": "Yangyang Guo", "authors": "Yangyang Guo, Liqiang Nie, Zhiyong Cheng, Qi Tian", "title": "Loss-rescaling VQA: Revisiting Language Prior Problem from a\n  Class-imbalance View", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies have pointed out that many well-developed Visual Question\nAnswering (VQA) models are heavily affected by the language prior problem,\nwhich refers to making predictions based on the co-occurrence pattern between\ntextual questions and answers instead of reasoning visual contents. To tackle\nit, most existing methods focus on enhancing visual feature learning to reduce\nthis superficial textual shortcut influence on VQA model decisions. However,\nlimited effort has been devoted to providing an explicit interpretation for its\ninherent cause. It thus lacks a good guidance for the research community to\nmove forward in a purposeful way, resulting in model construction perplexity in\novercoming this non-trivial problem. In this paper, we propose to interpret the\nlanguage prior problem in VQA from a class-imbalance view. Concretely, we\ndesign a novel interpretation scheme whereby the loss of mis-predicted frequent\nand sparse answers of the same question type is distinctly exhibited during the\nlate training phase. It explicitly reveals why the VQA model tends to produce a\nfrequent yet obviously wrong answer, to a given question whose right answer is\nsparse in the training set. Based upon this observation, we further develop a\nnovel loss re-scaling approach to assign different weights to each answer based\non the training data statistics for computing the final loss. We apply our\napproach into three baselines and the experimental results on two VQA-CP\nbenchmark datasets evidently demonstrate its effectiveness. In addition, we\nalso justify the validity of the class imbalance interpretation scheme on other\ncomputer vision tasks, such as face recognition and image classification.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2020 00:57:17 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Guo", "Yangyang", ""], ["Nie", "Liqiang", ""], ["Cheng", "Zhiyong", ""], ["Tian", "Qi", ""]]}, {"id": "2010.16031", "submitter": "Wei Li", "authors": "Wei Li, Yuanjun Xiong, Shuo Yang, Siqi Deng, Wei Xia", "title": "SMOT: Single-Shot Multi Object Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present single-shot multi-object tracker (SMOT), a new tracking framework\nthat converts any single-shot detector (SSD) model into an online multiple\nobject tracker, which emphasizes simultaneously detecting and tracking of the\nobject paths. Contrary to the existing tracking by detection approaches which\nsuffer from errors made by the object detectors, SMOT adopts the recently\nproposed scheme of tracking by re-detection. We combine this scheme with SSD\ndetectors by proposing a novel tracking anchor assignment module. With this\ndesign SMOT is able to generate tracklets with a constant per-frame runtime. A\nlight-weighted linkage algorithm is then used for online tracklet linking. On\nthree benchmarks of object tracking: Hannah, Music Videos, and MOT17, the\nproposed SMOT achieves state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2020 02:46:54 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Li", "Wei", ""], ["Xiong", "Yuanjun", ""], ["Yang", "Shuo", ""], ["Deng", "Siqi", ""], ["Xia", "Wei", ""]]}, {"id": "2010.16039", "submitter": "Zhi Qiao", "authors": "Zhi Qiao, Austin Bae, Lucas M. Glass, Cao Xiao, and Jimeng Sun", "title": "FLANNEL: Focal Loss Based Neural Network Ensemble for COVID-19 Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To test the possibility of differentiating chest x-ray images of COVID-19\nagainst other pneumonia and healthy patients using deep neural networks. We\nconstruct the X-ray imaging data from two publicly available sources, which\ninclude 5508 chest x-ray images across 2874 patients with four classes: normal,\nbacterial pneumonia, non-COVID-19 viral pneumonia, and COVID-19. To identify\nCOVID-19, we propose a Focal Loss Based Neural Ensemble Network (FLANNEL), a\nflexible module to ensemble several convolutional neural network (CNN) models\nand fuse with a focal loss for accurate COVID-19 detection on class imbalance\ndata. FLANNEL consistently outperforms baseline models on COVID-19\nidentification task in all metrics. Compared with the best baseline, FLANNEL\nshows a higher macro-F1 score with 6% relative increase on Covid-19\nidentification task where it achieves 0.7833(0.07) in Precision, 0.8609(0.03)\nin Recall, and 0.8168(0.03) F1 score.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2020 03:17:31 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Qiao", "Zhi", ""], ["Bae", "Austin", ""], ["Glass", "Lucas M.", ""], ["Xiao", "Cao", ""], ["Sun", "Jimeng", ""]]}, {"id": "2010.16041", "submitter": "Arash Mohammadi", "authors": "Shahin Heidarian, Parnian Afshar, Nastaran Enshaei, Farnoosh\n  Naderkhani, Anastasia Oikonomou, S. Farokh Atashzar, Faranak Babaki Fard,\n  Kaveh Samimi, Konstantinos N. Plataniotis, Arash Mohammadi, and Moezedin\n  Javad Rafiee", "title": "COVID-FACT: A Fully-Automated Capsule Network-based Framework for\n  Identification of COVID-19 Cases from Chest CT scans", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The newly discovered Corona virus Disease 2019 (COVID-19) has been globally\nspreading and causing hundreds of thousands of deaths around the world as of\nits first emergence in late 2019. Computed tomography (CT) scans have shown\ndistinctive features and higher sensitivity compared to other diagnostic tests,\nin particular the current gold standard, i.e., the Reverse Transcription\nPolymerase Chain Reaction (RT-PCR) test. Current deep learning-based algorithms\nare mainly developed based on Convolutional Neural Networks (CNNs) to identify\nCOVID-19 pneumonia cases. CNNs, however, require extensive data augmentation\nand large datasets to identify detailed spatial relations between image\ninstances. Furthermore, existing algorithms utilizing CT scans, either extend\nslice-level predictions to patient-level ones using a simple thresholding\nmechanism or rely on a sophisticated infection segmentation to identify the\ndisease. In this paper, we propose a two-stage fully-automated CT-based\nframework for identification of COVID-19 positive cases referred to as the\n\"COVID-FACT\". COVID-FACT utilizes Capsule Networks, as its main building blocks\nand is, therefore, capable of capturing spatial information. In particular, to\nmake the proposed COVID-FACT independent from sophisticated segmentation of the\narea of infection, slices demonstrating infection are detected at the first\nstage and the second stage is responsible for classifying patients into COVID\nand non-COVID cases. COVID-FACT detects slices with infection, and identifies\npositive COVID-19 cases using an in-house CT scan dataset, containing COVID-19,\ncommunity acquired pneumonia, and normal cases. Based on our experiments,\nCOVID-FACT achieves an accuracy of 90.82%, a sensitivity of 94.55%, a\nspecificity of 86.04%, and an Area Under the Curve (AUC) of 0.98, while\ndepending on far less supervision and annotation, in comparison to its\ncounterparts.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2020 03:30:22 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Heidarian", "Shahin", ""], ["Afshar", "Parnian", ""], ["Enshaei", "Nastaran", ""], ["Naderkhani", "Farnoosh", ""], ["Oikonomou", "Anastasia", ""], ["Atashzar", "S. Farokh", ""], ["Fard", "Faranak Babaki", ""], ["Samimi", "Kaveh", ""], ["Plataniotis", "Konstantinos N.", ""], ["Mohammadi", "Arash", ""], ["Rafiee", "Moezedin Javad", ""]]}, {"id": "2010.16043", "submitter": "Arash Mohammadi", "authors": "Shahin Heidarian, Parnian Afshar, Arash Mohammadi, Moezedin Javad\n  Rafiee, Anastasia Oikonomou, Konstantinos N. Plataniotis, and Farnoosh\n  Naderkhani", "title": "CT-CAPS: Feature Extraction-based Automated Framework for COVID-19\n  Disease Identification from Chest CT Scans using Capsule Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The global outbreak of the novel corona virus (COVID-19) disease has\ndrastically impacted the world and led to one of the most challenging crisis\nacross the globe since World War II. The early diagnosis and isolation of\nCOVID-19 positive cases are considered as crucial steps towards preventing the\nspread of the disease and flattening the epidemic curve. Chest Computed\nTomography (CT) scan is a highly sensitive, rapid, and accurate diagnostic\ntechnique that can complement Reverse Transcription Polymerase Chain Reaction\n(RT-PCR) test. Recently, deep learning-based models, mostly based on\nConvolutional Neural Networks (CNN), have shown promising diagnostic results.\nCNNs, however, are incapable of capturing spatial relations between image\ninstances and require large datasets. Capsule Networks, on the other hand, can\ncapture spatial relations, require smaller datasets, and have considerably\nfewer parameters. In this paper, a Capsule network framework, referred to as\nthe \"CT-CAPS\", is presented to automatically extract distinctive features of\nchest CT scans. These features, which are extracted from the layer before the\nfinal capsule layer, are then leveraged to differentiate COVID-19 from\nNon-COVID cases. The experiments on our in-house dataset of 307 patients show\nthe state-of-the-art performance with the accuracy of 90.8%, sensitivity of\n94.5%, and specificity of 86.0%.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2020 03:35:29 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Heidarian", "Shahin", ""], ["Afshar", "Parnian", ""], ["Mohammadi", "Arash", ""], ["Rafiee", "Moezedin Javad", ""], ["Oikonomou", "Anastasia", ""], ["Plataniotis", "Konstantinos N.", ""], ["Naderkhani", "Farnoosh", ""]]}, {"id": "2010.16073", "submitter": "Zeeshan Ahmad", "authors": "Zeeshan Ahmad and Naimul khan", "title": "CNN based Multistage Gated Average Fusion (MGAF) for Human Action\n  Recognition Using Depth and Inertial Sensors", "comments": "arXiv admin note: text overlap with arXiv:1910.11482", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Convolutional Neural Network (CNN) provides leverage to extract and fuse\nfeatures from all layers of its architecture. However, extracting and fusing\nintermediate features from different layers of CNN structure is still\nuninvestigated for Human Action Recognition (HAR) using depth and inertial\nsensors. To get maximum benefit of accessing all the CNN's layers, in this\npaper, we propose novel Multistage Gated Average Fusion (MGAF) network which\nextracts and fuses features from all layers of CNN using our novel and\ncomputationally efficient Gated Average Fusion (GAF) network, a decisive\nintegral element of MGAF. At the input of the proposed MGAF, we transform the\ndepth and inertial sensor data into depth images called sequential front view\nimages (SFI) and signal images (SI) respectively. These SFI are formed from the\nfront view information generated by depth data. CNN is employed to extract\nfeature maps from both input modalities. GAF network fuses the extracted\nfeatures effectively while preserving the dimensionality of fused feature as\nwell. The proposed MGAF network has structural extensibility and can be\nunfolded to more than two modalities. Experiments on three publicly available\nmultimodal HAR datasets demonstrate that the proposed MGAF outperforms the\nprevious state of the art fusion methods for depth-inertial HAR in terms of\nrecognition accuracy while being computationally much more efficient. We\nincrease the accuracy by an average of 1.5 percent while reducing the\ncomputational cost by approximately 50 percent over the previous state of the\nart.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 11:49:13 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Ahmad", "Zeeshan", ""], ["khan", "Naimul", ""]]}, {"id": "2010.16074", "submitter": "Yingwei Li", "authors": "Yingwei Li, Zhuotun Zhu, Yuyin Zhou, Yingda Xia, Wei Shen, Elliot K.\n  Fishman, Alan L. Yuille", "title": "Volumetric Medical Image Segmentation: A 3D Deep Coarse-to-fine\n  Framework and Its Adversarial Examples", "comments": "arXiv admin note: substantial text overlap with arXiv:1712.00201", "journal-ref": null, "doi": "10.1007/978-3-030-13969-8_4", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although deep neural networks have been a dominant method for many 2D vision\ntasks, it is still challenging to apply them to 3D tasks, such as medical image\nsegmentation, due to the limited amount of annotated 3D data and limited\ncomputational resources. In this chapter, by rethinking the strategy to apply\n3D Convolutional Neural Networks to segment medical images, we propose a novel\n3D-based coarse-to-fine framework to efficiently tackle these challenges. The\nproposed 3D-based framework outperforms their 2D counterparts by a large margin\nsince it can leverage the rich spatial information along all three axes. We\nfurther analyze the threat of adversarial attacks on the proposed framework and\nshow how to defense against the attack. We conduct experiments on three\ndatasets, the NIH pancreas dataset, the JHMI pancreas dataset and the JHMI\npathological cyst dataset, where the first two and the last one contain healthy\nand pathological pancreases respectively, and achieve the current\nstate-of-the-art in terms of Dice-Sorensen Coefficient (DSC) on all of them.\nEspecially, on the NIH pancreas segmentation dataset, we outperform the\nprevious best by an average of over $2\\%$, and the worst case is improved by\n$7\\%$ to reach almost $70\\%$, which indicates the reliability of our framework\nin clinical applications.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 15:39:19 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Li", "Yingwei", ""], ["Zhu", "Zhuotun", ""], ["Zhou", "Yuyin", ""], ["Xia", "Yingda", ""], ["Shen", "Wei", ""], ["Fishman", "Elliot K.", ""], ["Yuille", "Alan L.", ""]]}, {"id": "2010.16078", "submitter": "Aradhya Mathur", "authors": "Aradhya Neeraj Mathur, Devansh Batra, Yaman Kumar, Rajiv Ratn Shah,\n  Roger Zimmermann", "title": "LIFI: Towards Linguistically Informed Frame Interpolation", "comments": "9 pages, 7 tables, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In this work, we explore a new problem of frame interpolation for speech\nvideos. Such content today forms the major form of online communication. We try\nto solve this problem by using several deep learning video generation\nalgorithms to generate the missing frames. We also provide examples where\ncomputer vision models despite showing high performance on conventional\nnon-linguistic metrics fail to accurately produce faithful interpolation of\nspeech. With this motivation, we provide a new set of linguistically-informed\nmetrics specifically targeted to the problem of speech videos interpolation. We\nalso release several datasets to test computer vision video generation models\nof their speech understanding.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2020 05:02:23 GMT"}, {"version": "v2", "created": "Mon, 9 Nov 2020 06:48:57 GMT"}, {"version": "v3", "created": "Wed, 11 Nov 2020 11:28:08 GMT"}, {"version": "v4", "created": "Thu, 19 Nov 2020 07:12:03 GMT"}, {"version": "v5", "created": "Wed, 2 Dec 2020 16:47:06 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Mathur", "Aradhya Neeraj", ""], ["Batra", "Devansh", ""], ["Kumar", "Yaman", ""], ["Shah", "Rajiv Ratn", ""], ["Zimmermann", "Roger", ""]]}, {"id": "2010.16085", "submitter": "Tejas Zodage", "authors": "Tejas Zodage, Rahul Chakwate, Vinit Sarode, Rangaprasad Arun\n  Srivatsan, and Howie Choset", "title": "Correspondence Matrices are Underrated", "comments": "Accepted in the conference, 3DV 2020. 8 pages + 2 supplementary pages\n  + 2 reference papers", "journal-ref": "International Conference on 3D Vision (2020) 603-612", "doi": "10.1109/3DV50981.2020.00070", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point-cloud registration (PCR) is an important task in various applications\nsuch as robotic manipulation, augmented and virtual reality, SLAM, etc. PCR is\nan optimization problem involving minimization over two different types of\ninterdependent variables: transformation parameters and point-to-point\ncorrespondences. Recent developments in deep-learning have produced\ncomputationally fast approaches for PCR. The loss functions that are optimized\nin these networks are based on the error in the transformation parameters. We\nhypothesize that these methods would perform significantly better if they\ncalculated their loss function using correspondence error instead of only using\nerror in transformation parameters. We define correspondence error as a metric\nbased on incorrectly matched point pairs. We provide a fundamental explanation\nfor why this is the case and test our hypothesis by modifying existing methods\nto use correspondence-based loss instead of transformation-based loss. These\nexperiments show that the modified networks converge faster and register more\naccurately even at larger misalignment when compared to the original networks.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2020 05:31:50 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Zodage", "Tejas", ""], ["Chakwate", "Rahul", ""], ["Sarode", "Vinit", ""], ["Srivatsan", "Rangaprasad Arun", ""], ["Choset", "Howie", ""]]}, {"id": "2010.16092", "submitter": "Debapriya Roy", "authors": "Debapriya Roy, Diganta Mukherjee and Bhabatosh Chanda", "title": "An Unsupervised Approach towards Varying Human Skin Tone Using\n  Generative Adversarial Networks", "comments": "Accepted in International Conference on Pattern Recognition 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the increasing popularity of augmented and virtual reality, retailers\nare now focusing more towards customer satisfaction to increase the amount of\nsales. Although augmented reality is not a new concept but it has gained much\nneeded attention over the past few years. Our present work is targeted towards\nthis direction which may be used to enhance user experience in various virtual\nand augmented reality based applications. We propose a model to change skin\ntone of a person. Given any input image of a person or a group of persons with\nsome value indicating the desired change of skin color towards fairness or\ndarkness, this method can change the skin tone of the persons in the image.\nThis is an unsupervised method and also unconstrained in terms of pose,\nillumination, number of persons in the image etc. The goal of this work is to\nreduce the time and effort which is generally required for changing the skin\ntone using existing applications (e.g., Photoshop) by professionals or novice.\nTo establish the efficacy of this method we have compared our result with that\nof some popular photo editor and also with the result of some existing\nbenchmark method related to human attribute manipulation. Rigorous experiments\non different datasets show the effectiveness of this method in terms of\nsynthesizing perceptually convincing outputs.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2020 06:27:03 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Roy", "Debapriya", ""], ["Mukherjee", "Diganta", ""], ["Chanda", "Bhabatosh", ""]]}, {"id": "2010.16108", "submitter": "Ahmed Bensaoud", "authors": "Ahmed Bensaoud, Nawaf Abudawaood, Jugal Kalita", "title": "Classifying Malware Images with Convolutional Neural Network Models", "comments": "12 pages, 11 Figures", "journal-ref": "Vol.22, No.6, PP.1022-1031, Nov. 2020", "doi": "10.6633/IJNS.202011_22(6).17", "report-no": null, "categories": "cs.CR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to increasing threats from malicious software (malware) in both number\nand complexity, researchers have developed approaches to automatic detection\nand classification of malware, instead of analyzing methods for malware files\nmanually in a time-consuming effort. At the same time, malware authors have\ndeveloped techniques to evade signature-based detection techniques used by\nantivirus companies. Most recently, deep learning is being used in malware\nclassification to solve this issue. In this paper, we use several convolutional\nneural network (CNN) models for static malware classification. In particular,\nwe use six deep learning models, three of which are past winners of the\nImageNet Large-Scale Visual Recognition Challenge. The other three models are\nCNN-SVM, GRU-SVM and MLP-SVM, which enhance neural models with support vector\nmachines (SVM). We perform experiments using the Malimg dataset, which has\nmalware images that were converted from Portable Executable malware binaries.\nThe dataset is divided into 25 malware families. Comparisons show that the\nInception V3 model achieves a test accuracy of 99.24%, which is better than the\naccuracy of 98.52% achieved by the current state-of-the-art system called the\nM-CNN model.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2020 07:39:30 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Bensaoud", "Ahmed", ""], ["Abudawaood", "Nawaf", ""], ["Kalita", "Jugal", ""]]}, {"id": "2010.16117", "submitter": "Stefan Thalhammer", "authors": "Stefan Thalhammer, Markus Leitner, Timothy Patten and Markus Vincze", "title": "PyraPose: Feature Pyramids for Fast and Accurate Object Pose Estimation\n  under Domain Shift", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object pose estimation enables robots to understand and interact with their\nenvironments. Training with synthetic data is necessary in order to adapt to\nnovel situations. Unfortunately, pose estimation under domain shift, i.e.,\ntraining on synthetic data and testing in the real world, is challenging. Deep\nlearning-based approaches currently perform best when using encoder-decoder\nnetworks but typically do not generalize to new scenarios with different scene\ncharacteristics. We argue that patch-based approaches, instead of\nencoder-decoder networks, are more suited for synthetic-to-real transfer\nbecause local to global object information is better represented. To that end,\nwe present a novel approach based on a specialized feature pyramid network to\ncompute multi-scale features for creating pose hypotheses on different feature\nmap resolutions in parallel. Our single-shot pose estimation approach is\nevaluated on multiple standard datasets and outperforms the state of the art by\nup to 35%. We also perform grasping experiments in the real world to\ndemonstrate the advantage of using synthetic data to generalize to novel\nenvironments.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2020 08:26:22 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Thalhammer", "Stefan", ""], ["Leitner", "Markus", ""], ["Patten", "Timothy", ""], ["Vincze", "Markus", ""]]}, {"id": "2010.16119", "submitter": "Yangxin Wu", "authors": "Yangxin Wu, Gengwei Zhang, Hang Xu, Xiaodan Liang, Liang Lin", "title": "Auto-Panoptic: Cooperative Multi-Component Architecture Search for\n  Panoptic Segmentation", "comments": "NeurIPS2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Panoptic segmentation is posed as a new popular test-bed for the\nstate-of-the-art holistic scene understanding methods with the requirement of\nsimultaneously segmenting both foreground things and background stuff. The\nstate-of-the-art panoptic segmentation network exhibits high structural\ncomplexity in different network components, i.e. backbone, proposal-based\nforeground branch, segmentation-based background branch, and feature fusion\nmodule across branches, which heavily relies on expert knowledge and tedious\ntrials. In this work, we propose an efficient, cooperative and highly automated\nframework to simultaneously search for all main components including backbone,\nsegmentation branches, and feature fusion module in a unified panoptic\nsegmentation pipeline based on the prevailing one-shot Network Architecture\nSearch (NAS) paradigm. Notably, we extend the common single-task NAS into the\nmulti-component scenario by taking the advantage of the newly proposed\nintra-modular search space and problem-oriented inter-modular search space,\nwhich helps us to obtain an optimal network architecture that not only performs\nwell in both instance segmentation and semantic segmentation tasks but also be\naware of the reciprocal relations between foreground things and background\nstuff classes. To relieve the vast computation burden incurred by applying NAS\nto complicated network architectures, we present a novel path-priority greedy\nsearch policy to find a robust, transferrable architecture with significantly\nreduced searching overhead. Our searched architecture, namely Auto-Panoptic,\nachieves the new state-of-the-art on the challenging COCO and ADE20K\nbenchmarks. Moreover, extensive experiments are conducted to demonstrate the\neffectiveness of path-priority policy and transferability of Auto-Panoptic\nacross different datasets. Codes and models are available at:\nhttps://github.com/Jacobew/AutoPanoptic.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2020 08:34:35 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Wu", "Yangxin", ""], ["Zhang", "Gengwei", ""], ["Xu", "Hang", ""], ["Liang", "Xiaodan", ""], ["Lin", "Liang", ""]]}, {"id": "2010.16141", "submitter": "Matthias Humt", "authors": "Matthias Humt, Jongseok Lee, Rudolph Triebel", "title": "Bayesian Optimization Meets Laplace Approximation for Robotic\n  Introspection", "comments": "Accepted to the IEEE/RSJ International Conference on Intelligent\n  Robots and Systems (IROS) 2020 Long-Term Autonomy Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In robotics, deep learning (DL) methods are used more and more widely, but\ntheir general inability to provide reliable confidence estimates will\nultimately lead to fragile and unreliable systems. This impedes the potential\ndeployments of DL methods for long-term autonomy. Therefore, in this paper we\nintroduce a scalable Laplace Approximation (LA) technique to make Deep Neural\nNetworks (DNNs) more introspective, i.e. to enable them to provide accurate\nassessments of their failure probability for unseen test data. In particular,\nwe propose a novel Bayesian Optimization (BO) algorithm to mitigate their\ntendency of under-fitting the true weight posterior, so that both the\ncalibration and the accuracy of the predictions can be simultaneously\noptimized. We demonstrate empirically that the proposed BO approach requires\nfewer iterations for this when compared to random search, and we show that the\nproposed framework can be scaled up to large datasets and architectures.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2020 09:28:10 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Humt", "Matthias", ""], ["Lee", "Jongseok", ""], ["Triebel", "Rudolph", ""]]}, {"id": "2010.16164", "submitter": "Sherzod Salokhiddinov Sirojiddinovich", "authors": "Sherzod Salokhiddinov, Seungkyu Lee", "title": "Small Noisy and Perspective Face Detection using Deformable Symmetric\n  Gabor Wavelet Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face detection and tracking in low resolution image is not a trivial task due\nto the limitation in the appearance features for face characterization.\nMoreover, facial expression gives additional distortion on this small and noisy\nface. In this paper, we propose deformable symmetric Gabor wavelet network face\nmodel for face detection in low resolution image. Our model optimizes the\nrotation, translation, dilation, perspective and partial deformation amount of\nthe face model with symmetry constraints. Symmetry constraints help our model\nto be more robust to noise and distortion. Experimental results on our low\nresolution face image dataset and videos show promising face detection and\ntracking results under various challenging conditions.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2020 10:07:34 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Salokhiddinov", "Sherzod", ""], ["Lee", "Seungkyu", ""]]}, {"id": "2010.16165", "submitter": "Guangli Li", "authors": "Guangli Li, Xiu Ma, Xueying Wang, Lei Liu, Jingling Xue and Xiaobing\n  Feng", "title": "Fusion-Catalyzed Pruning for Optimizing Deep Learning on Intelligent\n  Edge Devices", "comments": "Published in IEEE Transactions on Computer-Aided Design of Integrated\n  Circuits and Systems (TCAD)", "journal-ref": null, "doi": "10.1109/TCAD.2020.3013050", "report-no": null, "categories": "cs.NE cs.CV cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing computational cost of deep neural network models limits the\napplicability of intelligent applications on resource-constrained edge devices.\nWhile a number of neural network pruning methods have been proposed to compress\nthe models, prevailing approaches focus only on parametric operators (e.g.,\nconvolution), which may miss optimization opportunities. In this paper, we\npresent a novel fusion-catalyzed pruning approach, called FuPruner, which\nsimultaneously optimizes the parametric and non-parametric operators for\naccelerating neural networks. We introduce an aggressive fusion method to\nequivalently transform a model, which extends the optimization space of pruning\nand enables non-parametric operators to be pruned in a similar manner as\nparametric operators, and a dynamic filter pruning method is applied to\ndecrease the computational cost of models while retaining the accuracy\nrequirement. Moreover, FuPruner provides configurable optimization options for\ncontrolling fusion and pruning, allowing much more flexible\nperformance-accuracy trade-offs to be made. Evaluation with state-of-the-art\nresidual neural networks on five representative intelligent edge platforms,\nJetson TX2, Jetson Nano, Edge TPU, NCS, and NCS2, demonstrates the\neffectiveness of our approach, which can accelerate the inference of models on\nCIFAR-10 and ImageNet datasets.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2020 10:10:08 GMT"}, {"version": "v2", "created": "Sun, 29 Nov 2020 12:50:10 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Li", "Guangli", ""], ["Ma", "Xiu", ""], ["Wang", "Xueying", ""], ["Liu", "Lei", ""], ["Xue", "Jingling", ""], ["Feng", "Xiaobing", ""]]}, {"id": "2010.16188", "submitter": "Jizhizi Li", "authors": "Jizhizi Li, Jing Zhang, Stephen J. Maybank, Dacheng Tao", "title": "Bridge Composite and Real: Towards End-to-end Deep Image Matting", "comments": "Both the dataset and source code will be available at\n  https://github.com/JizhiziLi/animal-matting/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extracting accurate foregrounds from natural images benefits many downstream\napplications such as film production and augmented reality. However, the furry\ncharacteristics and various appearance of the foregrounds, e.g., animal and\nportrait, challenge existing matting methods, which usually require extra user\ninputs such as trimap or scribbles. To resolve these problems, we study the\ndistinct roles of semantics and details for image matting and decompose the\ntask into two parallel sub-tasks: high-level semantic segmentation and\nlow-level details matting. Specifically, we propose a novel Glance and Focus\nMatting network (GFM), which employs a shared encoder and two separate decoders\nto learn both tasks in a collaborative manner for end-to-end natural image\nmatting. Besides, due to the limitation of available natural images in the\nmatting task, previous methods typically adopt composite images for training\nand evaluation, which result in limited generalization ability on real-world\nimages. In this paper, we investigate the domain gap issue between composite\nimages and real-world images systematically by conducting comprehensive\nanalyses of various discrepancies between foreground and background images. We\nfind that a carefully designed composition route RSSN that aims to reduce the\ndiscrepancies can lead to a better model with remarkable generalization\nability. Furthermore, we provide a benchmark containing 2,000 high-resolution\nreal-world animal images and 10,000 portrait images along with their manually\nlabeled alpha mattes to serve as a test bed for evaluating matting model's\ngeneralization ability on real-world images. Comprehensive empirical studies\nhave demonstrated that GFM outperforms state-of-the-art methods and effectively\nreduces the generalization error. The code and the dataset will be released.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2020 10:57:13 GMT"}, {"version": "v2", "created": "Sun, 11 Jul 2021 09:00:53 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Li", "Jizhizi", ""], ["Zhang", "Jing", ""], ["Maybank", "Stephen J.", ""], ["Tao", "Dacheng", ""]]}, {"id": "2010.16198", "submitter": "Kibrom Berihu Girum", "authors": "Kibrom Berihu Girum, Youssef Skandarani, Raabid Hussain, Alexis Bozorg\n  Grayeli, Gilles Cr\\'ehange, Alain Lalande", "title": "Automatic Myocardial Infarction Evaluation from Delayed-Enhancement\n  Cardiac MRI using Deep Convolutional Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we propose a new deep learning framework for an automatic\nmyocardial infarction evaluation from clinical information and delayed\nenhancement-MRI (DE-MRI). The proposed framework addresses two tasks. The first\ntask is automatic detection of myocardial contours, the infarcted area, the\nno-reflow area, and the left ventricular cavity from a short-axis DE-MRI\nseries. It employs two segmentation neural networks. The first network is used\nto segment the anatomical structures such as the myocardium and left\nventricular cavity. The second network is used to segment the pathological\nareas such as myocardial infarction, myocardial no-reflow, and normal\nmyocardial region. The segmented myocardium region from the first network is\nfurther used to refine the second network's pathological segmentation results.\nThe second task is to automatically classify a given case into normal or\npathological from clinical information with or without DE-MRI. A cascaded\nsupport vector machine (SVM) is employed to classify a given case from its\nassociated clinical information. The segmented pathological areas from DE-MRI\nare also used for the classification task. We evaluated our method on the 2020\nEMIDEC MICCAI challenge dataset. It yielded an average Dice index of 0.93 and\n0.84, respectively, for the left ventricular cavity and the myocardium. The\nclassification from using only clinical information yielded 80% accuracy over\nfive-fold cross-validation. Using the DE-MRI, our method can classify the cases\nwith 93.3% accuracy. These experimental results reveal that the proposed method\ncan automatically evaluate the myocardial infarction.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2020 11:18:25 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Girum", "Kibrom Berihu", ""], ["Skandarani", "Youssef", ""], ["Hussain", "Raabid", ""], ["Grayeli", "Alexis Bozorg", ""], ["Cr\u00e9hange", "Gilles", ""], ["Lalande", "Alain", ""]]}, {"id": "2010.16211", "submitter": "Heng Yao", "authors": "Mian Zou, Heng Yao, Chuan Qin, and Xinpeng Zhang", "title": "Statistical Analysis of Signal-Dependent Noise: Application in Blind\n  Localization of Image Splicing Forgery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual noise is often regarded as a disturbance in image quality, whereas it\ncan also provide a crucial clue for image-based forensic tasks. Conventionally,\nnoise is assumed to comprise an additive Gaussian model to be estimated and\nthen used to reveal anomalies. However, for real sensor noise, it should be\nmodeled as signal-dependent noise (SDN). In this work, we apply SDN to splicing\nforgery localization tasks. Through statistical analysis of the SDN model, we\nassume that noise can be modeled as a Gaussian approximation for a certain\nbrightness and propose a likelihood model for a noise level function. By\nbuilding a maximum a posterior Markov random field (MAP-MRF) framework, we\nexploit the likelihood of noise to reveal the alien region of spliced objects,\nwith a probability combination refinement strategy. To ensure a completely\nblind detection, an iterative alternating method is adopted to estimate the MRF\nparameters. Experimental results demonstrate that our method is effective and\nprovides a comparative localization performance.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2020 11:53:53 GMT"}, {"version": "v2", "created": "Mon, 2 Nov 2020 09:34:16 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Zou", "Mian", ""], ["Yao", "Heng", ""], ["Qin", "Chuan", ""], ["Zhang", "Xinpeng", ""]]}, {"id": "2010.16219", "submitter": "Yong-Lu Li", "authors": "Yong-Lu Li, Xinpeng Liu, Xiaoqian Wu, Yizhuo Li, Cewu Lu", "title": "HOI Analysis: Integrating and Decomposing Human-Object Interaction", "comments": "Accepted in NeurIPS 2020. Code:\n  github.com/DirtyHarryLYL/HAKE-Action-Torch/tree/IDN-(Integrating-Decomposing-Network),\n  Project: https://github.com/DirtyHarryLYL/HAKE-Action-Torch", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human-Object Interaction (HOI) consists of human, object and implicit\ninteraction/verb. Different from previous methods that directly map pixels to\nHOI semantics, we propose a novel perspective for HOI learning in an analytical\nmanner. In analogy to Harmonic Analysis, whose goal is to study how to\nrepresent the signals with the superposition of basic waves, we propose the HOI\nAnalysis. We argue that coherent HOI can be decomposed into isolated human and\nobject. Meanwhile, isolated human and object can also be integrated into\ncoherent HOI again. Moreover, transformations between human-object pairs with\nthe same HOI can also be easier approached with integration and decomposition.\nAs a result, the implicit verb will be represented in the transformation\nfunction space. In light of this, we propose an Integration-Decomposition\nNetwork (IDN) to implement the above transformations and achieve\nstate-of-the-art performance on widely-used HOI detection benchmarks. Code is\navailable at\nhttps://github.com/DirtyHarryLYL/HAKE-Action-Torch/tree/IDN-(Integrating-Decomposing-Network).\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2020 12:22:38 GMT"}, {"version": "v2", "created": "Sat, 7 Nov 2020 11:39:56 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Li", "Yong-Lu", ""], ["Liu", "Xinpeng", ""], ["Wu", "Xiaoqian", ""], ["Li", "Yizhuo", ""], ["Lu", "Cewu", ""]]}, {"id": "2010.16262", "submitter": "Tim Bakker", "authors": "Tim Bakker, Herke van Hoof, Max Welling", "title": "Experimental design for MRI by greedy policy search", "comments": "Accepted to NeurIPS 2020 (spotlight), 15-12-2020: Fixed typos, Figure\n  9, and pseudocode", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In today's clinical practice, magnetic resonance imaging (MRI) is routinely\naccelerated through subsampling of the associated Fourier domain. Currently,\nthe construction of these subsampling strategies - known as experimental design\n- relies primarily on heuristics. We propose to learn experimental design\nstrategies for accelerated MRI with policy gradient methods. Unexpectedly, our\nexperiments show that a simple greedy approximation of the objective leads to\nsolutions nearly on-par with the more general non-greedy approach. We offer a\npartial explanation for this phenomenon rooted in greater variance in the\nnon-greedy objective's gradient estimates, and experimentally verify that this\nvariance hampers non-greedy models in adapting their policies to individual MR\nimages. We empirically show that this adaptivity is key to improving\nsubsampling designs.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2020 13:38:09 GMT"}, {"version": "v2", "created": "Tue, 15 Dec 2020 11:12:46 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Bakker", "Tim", ""], ["van Hoof", "Herke", ""], ["Welling", "Max", ""]]}, {"id": "2010.16267", "submitter": "Wentong Liao", "authors": "Hao Cheng, Wentong Liao, Xuejiao Tang, Michael Ying Yang, Monika\n  Sester, Bodo Rosenhahn", "title": "Exploring Dynamic Context for Multi-path Trajectory Prediction", "comments": "accpeted by ICRA 2021, code available", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To accurately predict future positions of different agents in traffic\nscenarios is crucial for safely deploying intelligent autonomous systems in the\nreal-world environment. However, it remains a challenge due to the behavior of\na target agent being affected by other agents dynamically and there being more\nthan one socially possible paths the agent could take. In this paper, we\npropose a novel framework, named Dynamic Context Encoder Network (DCENet). In\nour framework, first, the spatial context between agents is explored by using\nself-attention architectures. Then, the two-stream encoders are trained to\nlearn temporal context between steps by taking the respective observed\ntrajectories and the extracted dynamic spatial context as input. The\nspatial-temporal context is encoded into a latent space using a Conditional\nVariational Auto-Encoder (CVAE) module. Finally, a set of future trajectories\nfor each agent is predicted conditioned on the learned spatial-temporal context\nby sampling from the latent space, repeatedly. DCENet is evaluated on one of\nthe most popular challenging benchmarks for trajectory forecasting Trajnet and\nreports a new state-of-the-art performance. It also demonstrates superior\nperformance evaluated on the benchmark inD for mixed traffic at intersections.\nA series of ablation studies is conducted to validate the effectiveness of each\nproposed module. Our code is available at https://github.com/wtliao/DCENet.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2020 13:39:20 GMT"}, {"version": "v2", "created": "Mon, 2 Nov 2020 11:41:38 GMT"}, {"version": "v3", "created": "Wed, 24 Mar 2021 10:28:47 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Cheng", "Hao", ""], ["Liao", "Wentong", ""], ["Tang", "Xuejiao", ""], ["Yang", "Michael Ying", ""], ["Sester", "Monika", ""], ["Rosenhahn", "Bodo", ""]]}, {"id": "2010.16279", "submitter": "Shamit Lal", "authors": "Mihir Prabhudesai, Shamit Lal, Hsiao-Yu Fish Tung, Adam W. Harley,\n  Shubhankar Potdar, Katerina Fragkiadaki", "title": "3D Object Recognition By Corresponding and Quantizing Neural 3D Scene\n  Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a system that learns to detect objects and infer their 3D poses in\nRGB-D images. Many existing systems can identify objects and infer 3D poses,\nbut they heavily rely on human labels and 3D annotations. The challenge here is\nto achieve this without relying on strong supervision signals. To address this\nchallenge, we propose a model that maps RGB-D images to a set of 3D visual\nfeature maps in a differentiable fully-convolutional manner, supervised by\npredicting views. The 3D feature maps correspond to a featurization of the 3D\nworld scene depicted in the images. The object 3D feature representations are\ninvariant to camera viewpoint changes or zooms, which means feature matching\ncan identify similar objects under different camera viewpoints. We can compare\nthe 3D feature maps of two objects by searching alignment across scales and 3D\nrotations, and, as a result of the operation, we can estimate pose and scale\nchanges without the need for 3D pose annotations. We cluster object feature\nmaps into a set of 3D prototypes that represent familiar objects in canonical\nscales and orientations. We then parse images by inferring the prototype\nidentity and 3D pose for each detected object. We compare our method to\nnumerous baselines that do not learn 3D feature visual representations or do\nnot attempt to correspond features across scenes, and outperform them by a\nlarge margin in the tasks of object retrieval and object pose estimation.\nThanks to the 3D nature of the object-centric feature maps, the visual\nsimilarity cues are invariant to 3D pose changes or small scale changes, which\ngives our method an advantage over 2D and 1D methods.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2020 13:56:09 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Prabhudesai", "Mihir", ""], ["Lal", "Shamit", ""], ["Tung", "Hsiao-Yu Fish", ""], ["Harley", "Adam W.", ""], ["Potdar", "Shubhankar", ""], ["Fragkiadaki", "Katerina", ""]]}, {"id": "2010.16285", "submitter": "Marcel Sheeny", "authors": "Marcel Sheeny", "title": "All-Weather Object Recognition Using Radar and Infrared Sensing", "comments": "201 pages, PhD Thesis, Heriot-Watt University, October 2020. Includes\n  results from arXiv:1804.02576, arXiv:1912.03157,\n  https://www.mdpi.com/2076-3417/10/11/3861 and arXiv:2010.09076", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Autonomous cars are an emergent technology which has the capacity to change\nhuman lives. The current sensor systems which are most capable of perception\nare based on optical sensors. For example, deep neural networks show\noutstanding results in recognising objects when used to process data from\ncameras and Light Detection And Ranging (LiDAR) sensors. However these sensors\nperform poorly under adverse weather conditions such as rain, fog, and snow due\nto the sensor wavelengths. This thesis explores new sensing developments based\non long wave polarised infrared (IR) imagery and imaging radar to recognise\nobjects. First, we developed a methodology based on Stokes parameters using\npolarised infrared data to recognise vehicles using deep neural networks.\nSecond, we explored the potential of using only the power spectrum captured by\nlow-THz radar sensors to perform object recognition in a controlled scenario.\nThis latter work is based on a data-driven approach together with the\ndevelopment of a data augmentation method based on attenuation, range and\nspeckle noise. Last, we created a new large-scale dataset in the \"wild\" with\nmany different weather scenarios (sunny, overcast, night, fog, rain and snow)\nshowing radar robustness to detect vehicles in adverse weather. High resolution\nradar and polarised IR imagery, combined with a deep learning approach, are\nshown as a potential alternative to current automotive sensing systems based on\nvisible spectrum optical technology as they are more robust in severe weather\nand adverse light conditions.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2020 14:16:39 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Sheeny", "Marcel", ""]]}, {"id": "2010.16298", "submitter": "Elie Aljalbout", "authors": "Elie Aljalbout and Ji Chen and Konstantin Ritt and Maximilian Ulmer\n  and Sami Haddadin", "title": "Learning Vision-based Reactive Policies for Obstacle Avoidance", "comments": "Accepted for publication at CoRL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the problem of vision-based obstacle avoidance for\nrobotic manipulators. This topic poses challenges for both perception and\nmotion generation. While most work in the field aims at improving one of those\naspects, we provide a unified framework for approaching this problem. The main\ngoal of this framework is to connect perception and motion by identifying the\nrelationship between the visual input and the corresponding motion\nrepresentation. To this end, we propose a method for learning reactive obstacle\navoidance policies. We evaluate our method on goal-reaching tasks for single\nand multiple obstacles scenarios. We show the ability of the proposed method to\nefficiently learn stable obstacle avoidance strategies at a high success rate,\nwhile maintaining closed-loop responsiveness required for critical applications\nlike human-robot interaction.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2020 14:50:32 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Aljalbout", "Elie", ""], ["Chen", "Ji", ""], ["Ritt", "Konstantin", ""], ["Ulmer", "Maximilian", ""], ["Haddadin", "Sami", ""]]}, {"id": "2010.16307", "submitter": "Rayson Laroca", "authors": "Rayson Laroca, Alessander Cidral Boslooper, David Menotti", "title": "Automatic Counting and Identification of Train Wagons Based on Computer\n  Vision and Deep Learning", "comments": "An article about the proposed system has been published in the\n  October 2020 issue of Railway Gazette International, the leading business\n  journal for the worldwide rail industry", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present a robust and efficient solution for counting and\nidentifying train wagons using computer vision and deep learning. The proposed\nsolution is cost-effective and can easily replace solutions based on\nradiofrequency identification (RFID), which are known to have high installation\nand maintenance costs. According to our experiments, our two-stage methodology\nachieves impressive results on real-world scenarios, i.e., 100% accuracy in the\ncounting stage and 99.7% recognition rate in the identification one. Moreover,\nthe system is able to automatically reject some of the train wagons\nsuccessfully counted, as they have damaged identification codes. The results\nachieved were surprising considering that the proposed system requires low\nprocessing power (i.e., it can run in low-end setups) and that we used a\nrelatively small number of images to train our Convolutional Neural Network\n(CNN) for character recognition. The proposed method is registered, under\nnumber BR512020000808-9, with the National Institute of Industrial Property\n(Brazil).\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2020 14:56:54 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Laroca", "Rayson", ""], ["Boslooper", "Alessander Cidral", ""], ["Menotti", "David", ""]]}, {"id": "2010.16322", "submitter": "Francesco Salvetti", "authors": "Vittorio Mazzia, Francesco Salvetti, Diego Aghi and Marcello Chiaberge", "title": "DeepWay: a Deep Learning Waypoint Estimator for Global Path Generation", "comments": "Submitted to Computers and Electronics in Agriculture", "journal-ref": "Volume 184, May 2021, 106091", "doi": "10.1016/j.compag.2021.106091", "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Agriculture 3.0 and 4.0 have gradually introduced service robotics and\nautomation into several agricultural processes, mostly improving crops quality\nand seasonal yield. Row-based crops are the perfect settings to test and deploy\nsmart machines capable of monitoring and manage the harvest. In this context,\nglobal path generation is essential either for ground or aerial vehicles, and\nit is the starting point for every type of mission plan. Nevertheless, little\nattention has been currently given to this problem by the research community\nand global path generation automation is still far to be solved. In order to\ngenerate a viable path for an autonomous machine, the presented research\nproposes a feature learning fully convolutional model capable of estimating\nwaypoints given an occupancy grid map. In particular, we apply the proposed\ndata-driven methodology to the specific case of row-based crops with the\ngeneral objective to generate a global path able to cover the extension of the\ncrop completely. Extensive experimentation with a custom made synthetic dataset\nand real satellite-derived images of different scenarios have proved the\neffectiveness of our methodology and demonstrated the feasibility of an\nend-to-end and completely autonomous global path planner.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2020 15:27:42 GMT"}, {"version": "v2", "created": "Thu, 21 Jan 2021 17:01:14 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Mazzia", "Vittorio", ""], ["Salvetti", "Francesco", ""], ["Aghi", "Diego", ""], ["Chiaberge", "Marcello", ""]]}, {"id": "2010.16363", "submitter": "Gregory Yauney", "authors": "Gregory Yauney, Jack Hessel, David Mimno", "title": "Domain-Specific Lexical Grounding in Noisy Visual-Textual Documents", "comments": null, "journal-ref": "Published in EMNLP 2020", "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Images can give us insights into the contextual meanings of words, but\ncurrent image-text grounding approaches require detailed annotations. Such\ngranular annotation is rare, expensive, and unavailable in most domain-specific\ncontexts. In contrast, unlabeled multi-image, multi-sentence documents are\nabundant. Can lexical grounding be learned from such documents, even though\nthey have significant lexical and visual overlap? Working with a case study\ndataset of real estate listings, we demonstrate the challenge of distinguishing\nhighly correlated grounded terms, such as \"kitchen\" and \"bedroom\", and\nintroduce metrics to assess this document similarity. We present a simple\nunsupervised clustering-based method that increases precision and recall beyond\nobject detection and image tagging baselines when evaluated on labeled subsets\nof the dataset. The proposed method is particularly effective for local\ncontextual meanings of a word, for example associating \"granite\" with\ncountertops in the real estate dataset and with rocky landscapes in a Wikipedia\ndataset.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2020 16:39:49 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Yauney", "Gregory", ""], ["Hessel", "Jack", ""], ["Mimno", "David", ""]]}, {"id": "2010.16380", "submitter": "Naofumi Tomita", "authors": "Mengdan Zhu, Bing Ren, Ryland Richards, Matthew Suriawinata, Naofumi\n  Tomita, Saeed Hassanpour", "title": "Development and Evaluation of a Deep Neural Network for Histologic\n  Classification of Renal Cell Carcinoma on Biopsy and Surgical Resection\n  Slides", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Renal cell carcinoma (RCC) is the most common renal cancer in adults. The\nhistopathologic classification of RCC is essential for diagnosis, prognosis,\nand management of patients. Reorganization and classification of complex\nhistologic patterns of RCC on biopsy and surgical resection slides under a\nmicroscope remains a heavily specialized, error-prone, and time-consuming task\nfor pathologists. In this study, we developed a deep neural network model that\ncan accurately classify digitized surgical resection slides and biopsy slides\ninto five related classes: clear cell RCC, papillary RCC, chromophobe RCC,\nrenal oncocytoma, and normal. In addition to the whole-slide classification\npipeline, we visualized the identified indicative regions and features on\nslides for classification by reprocessing patch-level classification results to\nensure the explainability of our diagnostic model. We evaluated our model on\nindependent test sets of 78 surgical resection whole slides and 79 biopsy\nslides from our tertiary medical institution, and 69 randomly selected surgical\nresection slides from The Cancer Genome Atlas (TCGA) database. The average area\nunder the curve (AUC) of our classifier on the internal resection slides,\ninternal biopsy slides, and external TCGA slides is 0.98, 0.98 and 0.99,\nrespectively. Our results suggest that the high generalizability of our\napproach across different data sources and specimen types. More importantly,\nour model has the potential to assist pathologists by (1) automatically\npre-screening slides to reduce false-negative cases, (2) highlighting regions\nof importance on digitized slides to accelerate diagnosis, and (3) providing\nobjective and accurate diagnosis as the second opinion.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2020 17:20:49 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Zhu", "Mengdan", ""], ["Ren", "Bing", ""], ["Richards", "Ryland", ""], ["Suriawinata", "Matthew", ""], ["Tomita", "Naofumi", ""], ["Hassanpour", "Saeed", ""]]}, {"id": "2010.16396", "submitter": "Panagiotis Filntisis", "authors": "Panagiotis Paraskevas Filntisis and Niki Efthymiou and Gerasimos\n  Potamianos and Petros Maragos", "title": "Emotion Understanding in Videos Through Body, Context, and\n  Visual-Semantic Embedding Loss", "comments": "Winner of the First International Workshop on Bodily Expressed\n  Emotion Understanding Challenge (ECCVW-2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present our winning submission to the First International Workshop on\nBodily Expressed Emotion Understanding (BEEU) challenge. Based on recent\nliterature on the effect of context/environment on emotion, as well as visual\nrepresentations with semantic meaning using word embeddings, we extend the\nframework of Temporal Segment Network to accommodate these. Our method is\nverified on the validation set of the Body Language Dataset (BoLD) and achieves\n0.26235 Emotion Recognition Score on the test set, surpassing the previous best\nresult of 0.2530.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2020 17:48:42 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Filntisis", "Panagiotis Paraskevas", ""], ["Efthymiou", "Niki", ""], ["Potamianos", "Gerasimos", ""], ["Maragos", "Petros", ""]]}, {"id": "2010.16402", "submitter": "Simon Kornblith", "authors": "Simon Kornblith, Honglak Lee, Ting Chen, Mohammad Norouzi", "title": "What's in a Loss Function for Image Classification?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is common to use the softmax cross-entropy loss to train neural networks\non classification datasets where a single class label is assigned to each\nexample. However, it has been shown that modifying softmax cross-entropy with\nlabel smoothing or regularizers such as dropout can lead to higher performance.\nThis paper studies a variety of loss functions and output layer regularization\nstrategies on image classification tasks. We observe meaningful differences in\nmodel predictions, accuracy, calibration, and out-of-distribution robustness\nfor networks trained with different objectives. However, differences in hidden\nrepresentations of networks trained with different objectives are restricted to\nthe last few layers; representational similarity reveals no differences among\nnetwork layers that are not close to the output. We show that all objectives\nthat improve over vanilla softmax loss produce greater class separation in the\npenultimate layer of the network, which potentially accounts for improved\nperformance on the original task, but results in features that transfer worse\nto other tasks.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2020 17:50:31 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Kornblith", "Simon", ""], ["Lee", "Honglak", ""], ["Chen", "Ting", ""], ["Norouzi", "Mohammad", ""]]}, {"id": "2010.16404", "submitter": "Hanhan Li", "authors": "Hanhan Li, Ariel Gordon, Hang Zhao, Vincent Casser, Anelia Angelova", "title": "Unsupervised Monocular Depth Learning in Dynamic Scenes", "comments": "Accepted at 4th Conference on Robot Learning (CoRL 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for jointly training the estimation of depth, ego-motion,\nand a dense 3D translation field of objects relative to the scene, with\nmonocular photometric consistency being the sole source of supervision. We show\nthat this apparently heavily underdetermined problem can be regularized by\nimposing the following prior knowledge about 3D translation fields: they are\nsparse, since most of the scene is static, and they tend to be constant for\nrigid moving objects. We show that this regularization alone is sufficient to\ntrain monocular depth prediction models that exceed the accuracy achieved in\nprior work for dynamic scenes, including methods that require semantic input.\nCode is at\nhttps://github.com/google-research/google-research/tree/master/depth_and_motion_learning .\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2020 17:52:27 GMT"}, {"version": "v2", "created": "Sat, 7 Nov 2020 19:19:10 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Li", "Hanhan", ""], ["Gordon", "Ariel", ""], ["Zhao", "Hang", ""], ["Casser", "Vincent", ""], ["Angelova", "Anelia", ""]]}, {"id": "2010.16417", "submitter": "Dongdong Chen", "authors": "Zhentao Tan and Menglei Chai and Dongdong Chen and Jing Liao and Qi\n  Chu and Lu Yuan and Sergey Tulyakov and Nenghai Yu", "title": "MichiGAN: Multi-Input-Conditioned Hair Image Generation for Portrait\n  Editing", "comments": "Siggraph 2020, code is available at\n  https://github.com/tzt101/MichiGAN", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the recent success of face image generation with GANs, conditional\nhair editing remains challenging due to the under-explored complexity of its\ngeometry and appearance. In this paper, we present MichiGAN\n(Multi-Input-Conditioned Hair Image GAN), a novel conditional image generation\nmethod for interactive portrait hair manipulation. To provide user control over\nevery major hair visual factor, we explicitly disentangle hair into four\northogonal attributes, including shape, structure, appearance, and background.\nFor each of them, we design a corresponding condition module to represent,\nprocess, and convert user inputs, and modulate the image generation pipeline in\nways that respect the natures of different visual attributes. All these\ncondition modules are integrated with the backbone generator to form the final\nend-to-end network, which allows fully-conditioned hair generation from\nmultiple user inputs. Upon it, we also build an interactive portrait hair\nediting system that enables straightforward manipulation of hair by projecting\nintuitive and high-level user inputs such as painted masks, guiding strokes, or\nreference photos to well-defined condition representations. Through extensive\nexperiments and evaluations, we demonstrate the superiority of our method\nregarding both result quality and user controllability. The code is available\nat https://github.com/tzt101/MichiGAN.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2020 17:59:10 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Tan", "Zhentao", ""], ["Chai", "Menglei", ""], ["Chen", "Dongdong", ""], ["Liao", "Jing", ""], ["Chu", "Qi", ""], ["Yuan", "Lu", ""], ["Tulyakov", "Sergey", ""], ["Yu", "Nenghai", ""]]}]