[{"id": "1803.00004", "submitter": "Longquan Dai", "authors": "Longquan Dai, Mengke Yuan, Xiaopeng Zhang", "title": "Speeding Up the Bilateral Filter: A Joint Acceleration Way", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2016.2549701", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computational complexity of the brute-force implementation of the bilateral\nfilter (BF) depends on its filter kernel size. To achieve the constant-time BF\nwhose complexity is irrelevant to the kernel size, many techniques have been\nproposed, such as 2D box filtering, dimension promotion, and shiftability\nproperty. Although each of the above techniques suffers from accuracy and\nefficiency problems, previous algorithm designers were used to take only one of\nthem to assemble fast implementations due to the hardness of combining them\ntogether. Hence, no joint exploitation of these techniques has been proposed to\nconstruct a new cutting edge implementation that solves these problems. Jointly\nemploying five techniques: kernel truncation, best N -term approximation as\nwell as previous 2D box filtering, dimension promotion, and shiftability\nproperty, we propose a unified framework to transform BF with arbitrary spatial\nand range kernels into a set of 3D box filters that can be computed in linear\ntime. To the best of our knowledge, our algorithm is the first method that can\nintegrate all these acceleration techniques and, therefore, can draw upon one\nanother's strong point to overcome deficiencies. The strength of our method has\nbeen corroborated by several carefully designed experiments. In particular, the\nfiltering accuracy is significantly improved without sacrificing the efficiency\nat running time.\n", "versions": [{"version": "v1", "created": "Wed, 28 Feb 2018 07:14:59 GMT"}], "update_date": "2018-03-02", "authors_parsed": [["Dai", "Longquan", ""], ["Yuan", "Mengke", ""], ["Zhang", "Xiaopeng", ""]]}, {"id": "1803.00005", "submitter": "Longquan Dai", "authors": "Longquan Dai, Mengke Yuan, Zechao Li, Xiaopeng Zhang, Jinhui Tang", "title": "Hardware-Efficient Guided Image Filtering For Multi-Label Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Guided Filter (GF) is well-known for its linear complexity. However, when\nfiltering an image with an n-channel guidance, GF needs to invert an n x n\nmatrix for each pixel. To the best of our knowledge existing matrix inverse\nalgorithms are inefficient on current hardwares. This shortcoming limits\napplications of multichannel guidance in computation intensive system such as\nmulti-label system. We need a new GF-like filter that can perform fast\nmultichannel image guided filtering. Since the optimal linear complexity of GF\ncannot be minimized further, the only way thus is to bring all potentialities\nof current parallel computing hardwares into full play. In this paper we\npropose a hardware-efficient Guided Filter (HGF), which solves the efficiency\nproblem of multichannel guided image filtering and yields competent results\nwhen applying it to multi-label problems with synthesized polynomial\nmultichannel guidance. Specifically, in order to boost the filtering\nperformance, HGF takes a new matrix inverse algorithm which only involves two\nhardware-efficient operations: element-wise arithmetic calculations and box\nfiltering. In order to break the linear model restriction, HGF synthesizes a\npolynomial multichannel guidance to introduce nonlinearity. Benefiting from our\npolynomial guidance and hardware-efficient matrix inverse algorithm, HGF not\nonly is more sensitive to the underlying structure of guidance but also\nachieves the fastest computing speed. Due to these merits, HGF obtains\nstate-of-the-art results in terms of accuracy and efficiency in the computation\nintensive multi-label\n", "versions": [{"version": "v1", "created": "Wed, 28 Feb 2018 07:27:43 GMT"}], "update_date": "2018-03-02", "authors_parsed": [["Dai", "Longquan", ""], ["Yuan", "Mengke", ""], ["Li", "Zechao", ""], ["Zhang", "Xiaopeng", ""], ["Tang", "Jinhui", ""]]}, {"id": "1803.00031", "submitter": "Ravimal Bandara", "authors": "A.M.R.R. Bandara, L. Ranathunga, N.A. Abdullah", "title": "A Feature Clustering Approach Based on Histogram of Oriented Optical\n  Flow and Superpixels", "comments": null, "journal-ref": "2015 IEEE 10th International Conference on Industrial and\n  Information Systems (ICIIS), Peradeniya, 2015, pp. 480-484", "doi": "10.1109/ICIINFS.2015.7399059", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual feature clustering is one of the cost-effective approaches to segment\nobjects in videos. However, the assumptions made for developing the existing\nalgorithms prevent them from being used in situations like segmenting an\nunknown number of static and moving objects under heavy camera movements. This\npaper addresses the problem by introducing a clustering approach based on\nsuperpixels and short-term Histogram of Oriented Optical Flow (HOOF). Salient\nDither Pattern Feature (SDPF) is used as the visual feature to track the flow\nand Simple Linear Iterative Clustering (SLIC) is used for obtaining the\nsuperpixels. This new clustering approach is based on merging superpixels by\ncomparing short term local HOOF and a color cue to form high-level semantic\nsegments. The new approach was compared with one of the latest feature\nclustering approaches based on K-Means in eight-dimensional space and the\nresults revealed that the new approach is better by means of consistency,\ncompleteness, and spatial accuracy. Further, the new approach completely solved\nthe problem of not knowing the number of objects in a scene.\n", "versions": [{"version": "v1", "created": "Wed, 28 Feb 2018 19:09:18 GMT"}], "update_date": "2018-03-02", "authors_parsed": [["Bandara", "A. M. R. R.", ""], ["Ranathunga", "L.", ""], ["Abdullah", "N. A.", ""]]}, {"id": "1803.00036", "submitter": "Ravimal Bandara", "authors": "A. M. R. R. Bandara, P. W. G. R. M. P. B. Giragama", "title": "A Retinal Image Enhancement Technique for Blood Vessel Segmentation\n  Algorithm", "comments": null, "journal-ref": "2017 IEEE International Conference on Industrial and Information\n  Systems (ICIIS), Peradeniya, Sri Lanka, 2017, pp. 1-5", "doi": "10.1109/ICIINFS.2017.8300426", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The morphology of blood vessels in retinal fundus images is an important\nindicator of diseases like glaucoma, hypertension and diabetic retinopathy. The\naccuracy of retinal blood vessels segmentation affects the quality of retinal\nimage analysis which is used in diagnosis methods in modern ophthalmology.\nContrast enhancement is one of the crucial steps in any of retinal blood vessel\nsegmentation approaches. The reliability of the segmentation depends on the\nconsistency of the contrast over the image. This paper presents an assessment\nof the suitability of a recently invented spatially adaptive contrast\nenhancement technique for enhancing retinal fundus images for blood vessel\nsegmentation. The enhancement technique was integrated with a variant of Tyler\nCoye algorithm, which has been improved with Hough line transformation based\nvessel reconstruction method. The proposed approach was evaluated on two public\ndatasets STARE and DRIVE. The assessment was done by comparing the segmentation\nperformance with five widely used contrast enhancement techniques based on\nwavelet transform, contrast limited histogram equalization, local\nnormalization, linear un-sharp masking and contourlet transform. The results\nrevealed that the assessed enhancement technique is well suited for the\napplication and also outperforms all compared techniques.\n", "versions": [{"version": "v1", "created": "Wed, 28 Feb 2018 19:15:33 GMT"}], "update_date": "2018-03-02", "authors_parsed": [["Bandara", "A. M. R. R.", ""], ["Giragama", "P. W. G. R. M. P. B.", ""]]}, {"id": "1803.00037", "submitter": "Ravimal Bandara", "authors": "A. M. R. R. Bandara, L. Ranathunga, and N. A. Abdullah", "title": "Invariant properties of a locally salient dither pattern with a\n  spatial-chromatic histogram", "comments": null, "journal-ref": "2013 IEEE 8th International Conference on Industrial and\n  Information Systems, Peradeniya, 2013, pp. 304-308", "doi": "10.1109/ICIInfS.2013.6732000", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compacted Dither Pattern Code (CDPC) is a recently found feature which is\nsuccessful in irregular shapes based visual depiction. Locally salient dither\npattern feature is an attempt to expand the capability of CDPC for both regular\nand irregular shape based visual depiction. This paper presents an analysis of\nrotational and scale invariance property of locally salient dither pattern\nfeature with a two dimensional spatialchromatic histogram, which expands the\napplicability of the visual feature. Experiments were conducted to exhibit\nrotational and scale invariance of the feature. These experiments were\nconducted by combining linear Support Vector Machine (SVM) classifier to the\nnew feature. The experimental results revealed that the locally salient dither\npattern feature with the spatialchromatic histogram is rotationally and scale\ninvariant.\n", "versions": [{"version": "v1", "created": "Wed, 28 Feb 2018 19:18:34 GMT"}], "update_date": "2018-03-02", "authors_parsed": [["Bandara", "A. M. R. R.", ""], ["Ranathunga", "L.", ""], ["Abdullah", "N. A.", ""]]}, {"id": "1803.00039", "submitter": "Ravimal Bandara", "authors": "A. M. R. R. Bandara, K. A. S. H. Kulathilake, P. W. G. R. M. P. B.\n  Giragama", "title": "Super-Efficient Spatially Adaptive Contrast Enhancement Algorithm for\n  Superficial Vein Imaging", "comments": null, "journal-ref": "2017 IEEE International Conference on Industrial and Information\n  Systems (ICIIS), Peradeniya, Sri Lanka, 2017, pp. 1-6", "doi": "10.1109/ICIINFS.2017.8300427", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a super-efficient spatially adaptive contrast enhancement\nalgorithm for enhancing infrared (IR) radiation based superficial vein images\nin real-time. The super-efficiency permits the algorithm to run in\nconsumer-grade handheld devices, which ultimately reduces the cost of vein\nimaging equipment. The proposed method utilizes the response from the\nlow-frequency range of the IR image signal to adjust the boundaries of the\nreference dynamic range in a linear contrast stretching process with a tunable\ncontrast enhancement parameter, as opposed to traditional approaches which use\ncostly adaptive histogram equalization based methods. The algorithm has been\nimplemented and deployed in a consumer grade Android-based mobile device to\nevaluate the performance. The results revealed that the proposed algorithm can\nprocess IR images of veins in real-time on low-performance computers. It was\ncompared with several well-performed traditional methods and the results\nrevealed that the new algorithm stands out with several beneficial features,\nnamely, the fastest processing, the ability to enhance the desired details, the\nexcellent illumination normalization capability and the ability to enhance\ndetails where the traditional methods failed.\n", "versions": [{"version": "v1", "created": "Wed, 28 Feb 2018 19:21:50 GMT"}], "update_date": "2018-03-02", "authors_parsed": [["Bandara", "A. M. R. R.", ""], ["Kulathilake", "K. A. S. H.", ""], ["Giragama", "P. W. G. R. M. P. B.", ""]]}, {"id": "1803.00057", "submitter": "Boyang Li", "authors": "Pelin Dogan, Boyang Li, Leonid Sigal, Markus Gross", "title": "A Neural Multi-sequence Alignment TeCHnique (NeuMATCH)", "comments": "Accepted at CVPR 2018 (Spotlight). arXiv file includes the paper and\n  the supplemental material", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The alignment of heterogeneous sequential data (video to text) is an\nimportant and challenging problem. Standard techniques for this task, including\nDynamic Time Warping (DTW) and Conditional Random Fields (CRFs), suffer from\ninherent drawbacks. Mainly, the Markov assumption implies that, given the\nimmediate past, future alignment decisions are independent of further history.\nThe separation between similarity computation and alignment decision also\nprevents end-to-end training. In this paper, we propose an end-to-end neural\narchitecture where alignment actions are implemented as moving data between\nstacks of Long Short-term Memory (LSTM) blocks. This flexible architecture\nsupports a large variety of alignment tasks, including one-to-one, one-to-many,\nskipping unmatched elements, and (with extensions) non-monotonic alignment.\nExtensive experiments on semi-synthetic and real datasets show that our\nalgorithm outperforms state-of-the-art baselines.\n", "versions": [{"version": "v1", "created": "Mon, 19 Feb 2018 06:51:01 GMT"}, {"version": "v2", "created": "Mon, 9 Apr 2018 20:51:32 GMT"}], "update_date": "2018-04-11", "authors_parsed": [["Dogan", "Pelin", ""], ["Li", "Boyang", ""], ["Sigal", "Leonid", ""], ["Gross", "Markus", ""]]}, {"id": "1803.00058", "submitter": "Andreas Mang", "authors": "Andreas Mang and Amir Gholami and Christos Davatzikos and George Biros", "title": "PDE-constrained optimization in medical image analysis", "comments": null, "journal-ref": "Optimization and Engineering 19, 765-812, 2018", "doi": "10.1007/s11081-018-9390-9", "report-no": null, "categories": "math.OC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  PDE-constrained optimization problems find many applications in medical image\nanalysis, for example, neuroimaging, cardiovascular imaging, and oncological\nimaging. We review related literature and give examples on the formulation,\ndiscretization, and numerical solution of PDE-constrained optimization problems\nfor medical imaging. We discuss three examples. The first one is image\nregistration. The second one is data assimilation for brain tumor patients, and\nthe third one data assimilation in cardiovascular imaging. The image\nregistration problem is a classical task in medical image analysis and seeks to\nfind pointwise correspondences between two or more images. The data\nassimilation problems use a PDE-constrained formulation to link a biophysical\nmodel to patient-specific data obtained from medical images. The associated\noptimality systems turn out to be sets of nonlinear, multicomponent PDEs that\nare challenging to solve in an efficient way.\n  The ultimate goal of our work is the design of inversion methods that\nintegrate complementary data, and rigorously follow mathematical and physical\nprinciples, in an attempt to support clinical decision making. This requires\nreliable, high-fidelity algorithms with a short time-to-solution. This task is\ncomplicated by model and data uncertainties, and by the fact that\nPDE-constrained optimization problems are ill-posed in nature, and in general\nyield high-dimensional, severely ill-conditioned systems after discretization.\nThese features make regularization, effective preconditioners, and iterative\nsolvers that, in many cases, have to be implemented on distributed-memory\narchitectures to be practical, a prerequisite. We showcase state-of-the-art\ntechniques in scientific computing to tackle these challenges.\n", "versions": [{"version": "v1", "created": "Wed, 28 Feb 2018 20:04:23 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["Mang", "Andreas", ""], ["Gholami", "Amir", ""], ["Davatzikos", "Christos", ""], ["Biros", "George", ""]]}, {"id": "1803.00068", "submitter": "Kihyuk Sohn", "authors": "Luan Tran, Kihyuk Sohn, Xiang Yu, Xiaoming Liu, Manmohan Chandraker", "title": "Gotta Adapt 'Em All: Joint Pixel and Feature-Level Domain Adaptation for\n  Recognition in the Wild", "comments": "Accepted to CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent developments in deep domain adaptation have allowed knowledge transfer\nfrom a labeled source domain to an unlabeled target domain at the level of\nintermediate features or input pixels. We propose that advantages may be\nderived by combining them, in the form of different insights that lead to a\nnovel design and complementary properties that result in better performance. At\nthe feature level, inspired by insights from semi-supervised learning, we\npropose a classification-aware domain adversarial neural network that brings\ntarget examples into more classifiable regions of source domain. Next, we posit\nthat computer vision insights are more amenable to injection at the pixel\nlevel. In particular, we use 3D geometry and image synthesis based on a\ngeneralized appearance flow to preserve identity across pose transformations,\nwhile using an attribute-conditioned CycleGAN to translate a single source into\nmultiple target images that differ in lower-level properties such as lighting.\nBesides standard UDA benchmark, we validate on a novel and apt problem of car\nrecognition in unlabeled surveillance images using labeled images from the web,\nhandling explicitly specified, nameable factors of variation through\npixel-level and implicit, unspecified factors through feature-level adaptation.\n", "versions": [{"version": "v1", "created": "Wed, 28 Feb 2018 20:23:20 GMT"}, {"version": "v2", "created": "Tue, 28 May 2019 19:50:33 GMT"}], "update_date": "2019-05-30", "authors_parsed": [["Tran", "Luan", ""], ["Sohn", "Kihyuk", ""], ["Yu", "Xiang", ""], ["Liu", "Xiaoming", ""], ["Chandraker", "Manmohan", ""]]}, {"id": "1803.00085", "submitter": "Tai-Ling Yuan", "authors": "Tai-Ling Yuan, Zhe Zhu, Kun Xu, Cheng-Jun Li, Shi-Min Hu", "title": "Chinese Text in the Wild", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We introduce Chinese Text in the Wild, a very large dataset of Chinese text\nin street view images. While optical character recognition (OCR) in document\nimages is well studied and many commercial tools are available, detection and\nrecognition of text in natural images is still a challenging problem,\nespecially for more complicated character sets such as Chinese text. Lack of\ntraining data has always been a problem, especially for deep learning methods\nwhich require massive training data.\n  In this paper we provide details of a newly created dataset of Chinese text\nwith about 1 million Chinese characters annotated by experts in over 30\nthousand street view images. This is a challenging dataset with good diversity.\nIt contains planar text, raised text, text in cities, text in rural areas, text\nunder poor illumination, distant text, partially occluded text, etc. For each\ncharacter in the dataset, the annotation includes its underlying character, its\nbounding box, and 6 attributes. The attributes indicate whether it has complex\nbackground, whether it is raised, whether it is handwritten or printed, etc.\nThe large size and diversity of this dataset make it suitable for training\nrobust neural networks for various tasks, particularly detection and\nrecognition. We give baseline results using several state-of-the-art networks,\nincluding AlexNet, OverFeat, Google Inception and ResNet for character\nrecognition, and YOLOv2 for character detection in images. Overall Google\nInception has the best performance on recognition with 80.5% top-1 accuracy,\nwhile YOLOv2 achieves an mAP of 71.0% on detection. Dataset, source code and\ntrained models will all be publicly available on the website.\n", "versions": [{"version": "v1", "created": "Wed, 28 Feb 2018 21:03:58 GMT"}], "update_date": "2018-03-02", "authors_parsed": [["Yuan", "Tai-Ling", ""], ["Zhu", "Zhe", ""], ["Xu", "Kun", ""], ["Li", "Cheng-Jun", ""], ["Hu", "Shi-Min", ""]]}, {"id": "1803.00094", "submitter": "Quynh Nguyen", "authors": "Quynh Nguyen, Mahesh Chandra Mukkamala, Matthias Hein", "title": "Neural Networks Should Be Wide Enough to Learn Disconnected Decision\n  Regions", "comments": "Accepted at ICML 2018. Added discussion for non-pyramidal networks\n  and ReLU activation function", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the recent literature the important role of depth in deep learning has\nbeen emphasized. In this paper we argue that sufficient width of a feedforward\nnetwork is equally important by answering the simple question under which\nconditions the decision regions of a neural network are connected. It turns out\nthat for a class of activation functions including leaky ReLU, neural networks\nhaving a pyramidal structure, that is no layer has more hidden units than the\ninput dimension, produce necessarily connected decision regions. This implies\nthat a sufficiently wide hidden layer is necessary to guarantee that the\nnetwork can produce disconnected decision regions. We discuss the implications\nof this result for the construction of neural networks, in particular the\nrelation to the problem of adversarial manipulation of classifiers.\n", "versions": [{"version": "v1", "created": "Wed, 28 Feb 2018 21:28:28 GMT"}, {"version": "v2", "created": "Wed, 6 Jun 2018 10:47:26 GMT"}, {"version": "v3", "created": "Fri, 8 Jun 2018 09:14:57 GMT"}], "update_date": "2018-06-11", "authors_parsed": [["Nguyen", "Quynh", ""], ["Mukkamala", "Mahesh Chandra", ""], ["Hein", "Matthias", ""]]}, {"id": "1803.00127", "submitter": "Nitin J Sanket", "authors": "Huai-Jen Liang, Nitin J. Sanket, Cornelia Ferm\\\"uller, Yiannis\n  Aloimonos", "title": "SalientDSO: Bringing Attention to Direct Sparse Odometry", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although cluttered indoor scenes have a lot of useful high-level semantic\ninformation which can be used for mapping and localization, most Visual\nOdometry (VO) algorithms rely on the usage of geometric features such as\npoints, lines and planes. Lately, driven by this idea, the joint optimization\nof semantic labels and obtaining odometry has gained popularity in the robotics\ncommunity. The joint optimization is good for accurate results but is generally\nvery slow. At the same time, in the vision community, direct and sparse\napproaches for VO have stricken the right balance between speed and accuracy.\n  We merge the successes of these two communities and present a way to\nincorporate semantic information in the form of visual saliency to Direct\nSparse Odometry - a highly successful direct sparse VO algorithm. We also\npresent a framework to filter the visual saliency based on scene parsing. Our\nframework, SalientDSO, relies on the widely successful deep learning based\napproaches for visual saliency and scene parsing which drives the feature\nselection for obtaining highly-accurate and robust VO even in the presence of\nas few as 40 point features per frame. We provide extensive quantitative\nevaluation of SalientDSO on the ICL-NUIM and TUM monoVO datasets and show that\nwe outperform DSO and ORB-SLAM - two very popular state-of-the-art approaches\nin the literature. We also collect and publicly release a CVL-UMD dataset which\ncontains two indoor cluttered sequences on which we show qualitative\nevaluations. To our knowledge this is the first paper to use visual saliency\nand scene parsing to drive the feature selection in direct VO.\n", "versions": [{"version": "v1", "created": "Wed, 28 Feb 2018 23:02:47 GMT"}], "update_date": "2018-03-02", "authors_parsed": [["Liang", "Huai-Jen", ""], ["Sanket", "Nitin J.", ""], ["Ferm\u00fcller", "Cornelia", ""], ["Aloimonos", "Yiannis", ""]]}, {"id": "1803.00130", "submitter": "Dipan Pal", "authors": "Yutong Zheng, Dipan K. Pal and Marios Savvides", "title": "Ring loss: Convex Feature Normalization for Face Recognition", "comments": "Accepted at CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We motivate and present Ring loss, a simple and elegant feature normalization\napproach for deep networks designed to augment standard loss functions such as\nSoftmax. We argue that deep feature normalization is an important aspect of\nsupervised classification problems where we require the model to represent each\nclass in a multi-class problem equally well. The direct approach to feature\nnormalization through the hard normalization operation results in a non-convex\nformulation. Instead, Ring loss applies soft normalization, where it gradually\nlearns to constrain the norm to the scaled unit circle while preserving\nconvexity leading to more robust features. We apply Ring loss to large-scale\nface recognition problems and present results on LFW, the challenging protocols\nof IJB-A Janus, Janus CS3 (a superset of IJB-A Janus), Celebrity\nFrontal-Profile (CFP) and MegaFace with 1 million distractors. Ring loss\noutperforms strong baselines, matches state-of-the-art performance on IJB-A\nJanus and outperforms all other results on the challenging Janus CS3 thereby\nachieving state-of-the-art. We also outperform strong baselines in handling\nextremely low resolution face matching.\n", "versions": [{"version": "v1", "created": "Wed, 28 Feb 2018 23:13:07 GMT"}], "update_date": "2018-03-02", "authors_parsed": [["Zheng", "Yutong", ""], ["Pal", "Dipan K.", ""], ["Savvides", "Marios", ""]]}, {"id": "1803.00159", "submitter": "Gang Bai", "authors": "Chengfei Yao, Jie Zou, Yanan Luo, Tao Li, Gang Bai", "title": "A Class-Incremental Learning Method Based on One Class Support Vector\n  Machine", "comments": "6 pages, 4 figures, Under review as a conference paper at\n  International Conference on Pattern Recognition 2018", "journal-ref": null, "doi": "10.1088/1742-6596/1267/1/012007", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A method based on one class support vector machine (OCSVM) is proposed for\nclass incremental learning. Several OCSVM models divide the input space into\nseveral parts. Then, the 1VS1 classifiers are constructed for the confuse part\nby using the support vectors. During the class incremental learning process,\nthe OCSVM of the new class is trained at first. Then the support vectors of the\nold classes and the support vectors of the new class are reused to train 1VS1\nclassifiers for the confuse part. In order to bring more information to the\ncertain support vectors, the support vectors are at the boundary of the\ndistribution of samples as much as possible when the OCSVM is built. Compared\nwith the traditional methods, the proposed method retains the original model\nand thus reduces memory consumption and training time cost. Various experiments\non different datasets also verify the efficiency of the proposed method.\n", "versions": [{"version": "v1", "created": "Thu, 1 Mar 2018 01:50:42 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Yao", "Chengfei", ""], ["Zou", "Jie", ""], ["Luo", "Yanan", ""], ["Li", "Tao", ""], ["Bai", "Gang", ""]]}, {"id": "1803.00185", "submitter": "Yang Hu Dr.", "authors": "Tianyuan Chang, Guihua Wen, Yang Hu, JiaJiong Ma", "title": "Facial Expression Recognition Based on Complexity Perception\n  Classification Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Facial expression recognition (FER) has always been a challenging issue in\ncomputer vision. The different expressions of emotion and uncontrolled\nenvironmental factors lead to inconsistencies in the complexity of FER and\nvariability of between expression categories, which is often overlooked in most\nfacial expression recognition systems. In order to solve this problem\neffectively, we presented a simple and efficient CNN model to extract facial\nfeatures, and proposed a complexity perception classification (CPC) algorithm\nfor FER. The CPC algorithm divided the dataset into an easy classification\nsample subspace and a complex classification sample subspace by evaluating the\ncomplexity of facial features that are suitable for classification. The\nexperimental results of our proposed algorithm on Fer2013 and CK-plus datasets\ndemonstrated the algorithm's effectiveness and superiority over other\nstate-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Thu, 1 Mar 2018 03:05:50 GMT"}], "update_date": "2018-03-02", "authors_parsed": [["Chang", "Tianyuan", ""], ["Wen", "Guihua", ""], ["Hu", "Yang", ""], ["Ma", "JiaJiong", ""]]}, {"id": "1803.00197", "submitter": "Xingyu Chen", "authors": "Xingyu Chen, Junzhi Yu, and Zhengxing Wu", "title": "Temporally Identity-Aware SSD with Attentional LSTM", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Temporal object detection has attracted significant attention, but most\npopular detection methods cannot leverage rich temporal information in videos.\nVery recently, many algorithms have been developed for video detection task,\nyet very few approaches can achieve \\emph{real-time online} object detection in\nvideos. In this paper, based on attention mechanism and convolutional long\nshort-term memory (ConvLSTM), we propose a temporal single-shot detector (TSSD)\nfor real-world detection. Distinct from previous methods, we take aim at\ntemporally integrating pyramidal feature hierarchy using ConvLSTM, and design a\nnovel structure including a low-level temporal unit as well as a high-level one\n(LH-TU) for multi-scale feature maps. Moreover, we develop a creative temporal\nanalysis unit, namely, attentional ConvLSTM (AC-LSTM), in which a temporal\nattention mechanism is specially tailored for background suppression and scale\nsuppression while a ConvLSTM integrates attention-aware features across time.\nAn association loss and a multi-step training are designed for temporal\ncoherence. Besides, an online tubelet analysis (OTA) is exploited for\nidentification. Our framework is evaluated on ImageNet VID dataset and 2DMOT15\ndataset. Extensive comparisons on the detection and tracking capability\nvalidate the superiority of the proposed approach. Consequently, the developed\nTSSD-OTA achieves a fast speed and an overall competitive performance in terms\nof detection and tracking. Finally, a real-world maneuver is conducted for\nunderwater object grasping. The source code is publicly available at\nhttps://github.com/SeanChenxy/TSSD-OTA.\n", "versions": [{"version": "v1", "created": "Thu, 1 Mar 2018 03:48:07 GMT"}, {"version": "v2", "created": "Mon, 19 Mar 2018 13:52:07 GMT"}, {"version": "v3", "created": "Thu, 26 Apr 2018 14:01:17 GMT"}, {"version": "v4", "created": "Wed, 25 Mar 2020 08:14:32 GMT"}], "update_date": "2020-03-26", "authors_parsed": [["Chen", "Xingyu", ""], ["Yu", "Junzhi", ""], ["Wu", "Zhengxing", ""]]}, {"id": "1803.00219", "submitter": "Yang Hu Dr.", "authors": "Jiajiong Ma, Guihua Wen, Yang Hu, Tianyuan Chang, Haibin Zeng, Lijun\n  Jiang, Jianzeng Qin", "title": "Tongue image constitution recognition based on Complexity Perception\n  method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Background and Object: In China, body constitution is highly related to\nphysiological and pathological functions of human body and determines the\ntendency of the disease, which is of great importance for treatment in clinical\nmedicine. Tongue diagnosis, as a key part of Traditional Chinese Medicine\ninspection, is an important way to recognize the type of constitution.In order\nto deploy tongue image constitution recognition system on non-invasive mobile\ndevice to achieve fast, efficient and accurate constitution recognition, an\nefficient method is required to deal with the challenge of this kind of complex\nenvironment. Methods: In this work, we perform the tongue area detection,\ntongue area calibration and constitution classification using methods which are\nbased on deep convolutional neural network. Subject to the variation of\ninconstant environmental condition, the distribution of the picture is uneven,\nwhich has a bad effect on classification performance. To solve this problem, we\npropose a method based on the complexity of individual instances to divide\ndataset into two subsets and classify them separately, which is capable of\nimproving classification accuracy. To evaluate the performance of our proposed\nmethod, we conduct experiments on three sizes of tongue datasets, in which deep\nconvolutional neural network method and traditional digital image analysis\nmethod are respectively applied to extract features for tongue images. The\nproposed method is combined with the base classifier Softmax, SVM, and\nDecisionTree respectively. Results: As the experiments results shown, our\nproposed method improves the classification accuracy by 1.135% on average and\nachieves 59.99% constitution classification accuracy. Conclusions: Experimental\nresults on three datasets show that our proposed method can effectively improve\nthe classification accuracy of tongue constitution recognition.\n", "versions": [{"version": "v1", "created": "Thu, 1 Mar 2018 05:32:43 GMT"}], "update_date": "2018-03-02", "authors_parsed": [["Ma", "Jiajiong", ""], ["Wen", "Guihua", ""], ["Hu", "Yang", ""], ["Chang", "Tianyuan", ""], ["Zeng", "Haibin", ""], ["Jiang", "Lijun", ""], ["Qin", "Jianzeng", ""]]}, {"id": "1803.00227", "submitter": "Asit Mishra", "authors": "Asit Mishra and Debbie Marr", "title": "WRPN & Apprentice: Methods for Training and Inference using\n  Low-Precision Numerics", "comments": "Tech report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Today's high performance deep learning architectures involve large models\nwith numerous parameters. Low precision numerics has emerged as a popular\ntechnique to reduce both the compute and memory requirements of these large\nmodels. However, lowering precision often leads to accuracy degradation. We\ndescribe three schemes whereby one can both train and do efficient inference\nusing low precision numerics without hurting accuracy. Finally, we describe an\nefficient hardware accelerator that can take advantage of the proposed low\nprecision numerics.\n", "versions": [{"version": "v1", "created": "Thu, 1 Mar 2018 06:22:37 GMT"}], "update_date": "2018-03-02", "authors_parsed": [["Mishra", "Asit", ""], ["Marr", "Debbie", ""]]}, {"id": "1803.00232", "submitter": "Alexandre Thiery", "authors": "Sripad Krishna Devalla, Prajwal K. Renukanand, Bharathwaj K. Sreedhar,\n  Shamira Perera, Jean-Martial Mari, Khai Sing Chin, Tin A. Tun, Nicholas G.\n  Strouthidis, Tin Aung, Alexandre H. Thiery, Michael J. A. Girard", "title": "DRUNET: A Dilated-Residual U-Net Deep Learning Network to Digitally\n  Stain Optic Nerve Head Tissues in Optical Coherence Tomography Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given that the neural and connective tissues of the optic nerve head (ONH)\nexhibit complex morphological changes with the development and progression of\nglaucoma, their simultaneous isolation from optical coherence tomography (OCT)\nimages may be of great interest for the clinical diagnosis and management of\nthis pathology. A deep learning algorithm was designed and trained to digitally\nstain (i.e. highlight) 6 ONH tissue layers by capturing both the local (tissue\ntexture) and contextual information (spatial arrangement of tissues). The\noverall dice coefficient (mean of all tissues) was $0.91 \\pm 0.05$ when\nassessed against manual segmentations performed by an expert observer. We offer\nhere a robust segmentation framework that could be extended for the automated\nparametric study of the ONH tissues.\n", "versions": [{"version": "v1", "created": "Thu, 1 Mar 2018 06:37:30 GMT"}], "update_date": "2018-03-02", "authors_parsed": [["Devalla", "Sripad Krishna", ""], ["Renukanand", "Prajwal K.", ""], ["Sreedhar", "Bharathwaj K.", ""], ["Perera", "Shamira", ""], ["Mari", "Jean-Martial", ""], ["Chin", "Khai Sing", ""], ["Tun", "Tin A.", ""], ["Strouthidis", "Nicholas G.", ""], ["Aung", "Tin", ""], ["Thiery", "Alexandre H.", ""], ["Girard", "Michael J. A.", ""]]}, {"id": "1803.00233", "submitter": "Suryansh Kumar", "authors": "Suryansh Kumar, Anoop Cherian, Yuchao Dai, Hongdong Li", "title": "Scalable Dense Non-rigid Structure-from-Motion: A Grassmannian\n  Perspective", "comments": "10 pages, 7 figure, 4 tables. Accepted for publication in Conference\n  on Computer Vision and Pattern Recognition (CVPR), 2018, typos fixed and\n  acknowledgement added", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the task of dense non-rigid structure-from-motion\n(NRSfM) using multiple images. State-of-the-art methods to this problem are\noften hurdled by scalability, expensive computations, and noisy measurements.\nFurther, recent methods to NRSfM usually either assume a small number of sparse\nfeature points or ignore local non-linearities of shape deformations, and thus\ncannot reliably model complex non-rigid deformations. To address these issues,\nin this paper, we propose a new approach for dense NRSfM by modeling the\nproblem on a Grassmann manifold. Specifically, we assume the complex non-rigid\ndeformations lie on a union of local linear subspaces both spatially and\ntemporally. This naturally allows for a compact representation of the complex\nnon-rigid deformation over frames. We provide experimental results on several\nsynthetic and real benchmark datasets. The procured results clearly demonstrate\nthat our method, apart from being scalable and more accurate than\nstate-of-the-art methods, is also more robust to noise and generalizes to\nhighly non-linear deformations.\n", "versions": [{"version": "v1", "created": "Thu, 1 Mar 2018 07:08:39 GMT"}, {"version": "v2", "created": "Fri, 23 Mar 2018 22:19:20 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Kumar", "Suryansh", ""], ["Cherian", "Anoop", ""], ["Dai", "Yuchao", ""], ["Li", "Hongdong", ""]]}, {"id": "1803.00260", "submitter": "Daniel Barath", "authors": "Daniel Barath", "title": "Five-point Fundamental Matrix Estimation for Uncalibrated Cameras", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We aim at estimating the fundamental matrix in two views from five\ncorrespondences of rotation invariant features obtained by e.g.\\ the SIFT\ndetector. The proposed minimal solver first estimates a homography from three\ncorrespondences assuming that they are co-planar and exploiting their\nrotational components. Then the fundamental matrix is obtained from the\nhomography and two additional point pairs in general position. The proposed\napproach, combined with robust estimators like Graph-Cut RANSAC, is superior to\nother state-of-the-art algorithms both in terms of accuracy and number of\niterations required. This is validated on synthesized data and $561$ real image\npairs. Moreover, the tests show that requiring three points on a plane is not\ntoo restrictive in urban environment and locally optimized robust estimators\nlead to accurate estimates even if the points are not entirely co-planar. As a\npotential application, we show that using the proposed method makes two-view\nmulti-motion estimation more accurate.\n", "versions": [{"version": "v1", "created": "Thu, 1 Mar 2018 09:08:18 GMT"}], "update_date": "2018-03-02", "authors_parsed": [["Barath", "Daniel", ""]]}, {"id": "1803.00344", "submitter": "Gangeshwar Krishnamurthy", "authors": "Gangeshwar Krishnamurthy, Navonil Majumder, Soujanya Poria, Erik\n  Cambria", "title": "A Deep Learning Approach for Multimodal Deception Detection", "comments": "Accepted at the 19th International Conference on Computational\n  Linguistics and Intelligent Text Processing (CICLing), 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic deception detection is an important task that has gained momentum\nin computational linguistics due to its potential applications. In this paper,\nwe propose a simple yet tough to beat multi-modal neural model for deception\ndetection. By combining features from different modalities such as video,\naudio, and text along with Micro-Expression features, we show that detecting\ndeception in real life videos can be more accurate. Experimental results on a\ndataset of real-life deception videos show that our model outperforms existing\ntechniques for deception detection with an accuracy of 96.14% and ROC-AUC of\n0.9799.\n", "versions": [{"version": "v1", "created": "Thu, 1 Mar 2018 12:38:13 GMT"}], "update_date": "2018-03-21", "authors_parsed": [["Krishnamurthy", "Gangeshwar", ""], ["Majumder", "Navonil", ""], ["Poria", "Soujanya", ""], ["Cambria", "Erik", ""]]}, {"id": "1803.00380", "submitter": "Nantheera Anantrasirichai", "authors": "N. Anantrasirichai, F. Albino, P. Hill, D. Bull, J. Biggs", "title": "Detecting Volcano Deformation in InSAR using Deep learning", "comments": "Janet Watson Meeting 2018: A Data Explosion - The Geological Society", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Globally 800 million people live within 100 km of a volcano and currently\n1500 volcanoes are considered active, but half of these have no ground-based\nmonitoring. Alternatively, satellite radar (InSAR) can be employed to observe\nvolcanic ground deformation, which has shown a significant statistical link to\neruptions. Modern satellites provide large coverage with high resolution\nsignals, leading to huge amounts of data. The explosion in data has brought\nmajor challenges associated with timely dissemination of information and\ndistinguishing volcano deformation patterns from noise, which currently relies\non manual inspection. Moreover, volcano observatories still lack expertise to\nexploit satellite datasets, particularly in developing countries. This paper\npresents a novel approach to detect volcanic ground deformation automatically\nfrom wrapped-phase InSAR images. Convolutional neural networks (CNN) are\nemployed to detect unusual patterns within the radar data.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jan 2018 17:28:58 GMT"}], "update_date": "2018-03-02", "authors_parsed": [["Anantrasirichai", "N.", ""], ["Albino", "F.", ""], ["Hill", "P.", ""], ["Bull", "D.", ""], ["Biggs", "J.", ""]]}, {"id": "1803.00384", "submitter": "Mikael Vejdemo-Johansson", "authors": "Leo Carlsson, Gunnar Carlsson, Mikael Vejdemo-Johansson", "title": "Fibres of Failure: Classifying errors in predictive processes", "comments": "10 pages, submitted to ICML 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DM math.AT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe Fibres of Failure (FiFa), a method to classify failure modes of\npredictive processes using the Mapper algorithm from Topological Data Analysis.\n  Our method uses Mapper to build a graph model of input data stratified by\nprediction error.\n  Groupings found in high-error regions of the Mapper model then provide\ndistinct failure modes of the predictive process.\n  We demonstrate FiFa on misclassifications of MNIST images with added noise,\nand demonstrate two ways to use the failure mode classification: either to\nproduce a correction layer that adjusts predictions by similarity to the\nfailure modes; or to inspect members of the failure modes to illustrate and\ninvestigate what characterizes each failure mode.\n", "versions": [{"version": "v1", "created": "Fri, 9 Feb 2018 20:36:06 GMT"}], "update_date": "2018-03-02", "authors_parsed": [["Carlsson", "Leo", ""], ["Carlsson", "Gunnar", ""], ["Vejdemo-Johansson", "Mikael", ""]]}, {"id": "1803.00385", "submitter": "Matt Amodio", "authors": "Matthew Amodio, Smita Krishnaswamy", "title": "MAGAN: Aligning Biological Manifolds", "comments": null, "journal-ref": "Proceedings of the 35th International Conference on Machine\n  Learning, PMLR 80:215-223, 2018", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  It is increasingly common in many types of natural and physical systems\n(especially biological systems) to have different types of measurements\nperformed on the same underlying system. In such settings, it is important to\nalign the manifolds arising from each measurement in order to integrate such\ndata and gain an improved picture of the system. We tackle this problem using\ngenerative adversarial networks (GANs). Recently, GANs have been utilized to\ntry to find correspondences between sets of samples. However, these GANs are\nnot explicitly designed for proper alignment of manifolds. We present a new GAN\ncalled the Manifold-Aligning GAN (MAGAN) that aligns two manifolds such that\nrelated points in each measurement space are aligned together. We demonstrate\napplications of MAGAN in single-cell biology in integrating two different\nmeasurement types together. In our demonstrated examples, cells from the same\ntissue are measured with both genomic (single-cell RNA-sequencing) and\nproteomic (mass cytometry) technologies. We show that the MAGAN successfully\naligns them such that known correlations between measured markers are improved\ncompared to other recently proposed models.\n", "versions": [{"version": "v1", "created": "Sat, 10 Feb 2018 01:11:34 GMT"}], "update_date": "2019-02-27", "authors_parsed": [["Amodio", "Matthew", ""], ["Krishnaswamy", "Smita", ""]]}, {"id": "1803.00386", "submitter": "Ruqayya Awan", "authors": "Ruqayya Awan, Navid Alemi Koohbanani, Muhammad Shaban, Anna Lisowska\n  and Nasir Rajpoot", "title": "Context-Aware Learning using Transferable Features for Classification of\n  Breast Cancer Histology Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Convolutional neural networks (CNNs) have been recently used for a variety of\nhistology image analysis. However, availability of a large dataset is a major\nprerequisite for training a CNN which limits its use by the computational\npathology community. In previous studies, CNNs have demonstrated their\npotential in terms of feature generalizability and transferability accompanied\nwith better performance. Considering these traits of CNN, we propose a simple\nyet effective method which leverages the strengths of CNN combined with the\nadvantages of including contextual information, particularly designed for a\nsmall dataset. Our method consists of two main steps: first it uses the\nactivation features of CNN trained for a patch-based classification and then it\ntrains a separate classifier using features of overlapping patches to perform\nimage-based classification using the contextual information. The proposed\nframework outperformed the state-of-the-art method for breast cancer\nclassification.\n", "versions": [{"version": "v1", "created": "Mon, 12 Feb 2018 12:42:27 GMT"}, {"version": "v2", "created": "Tue, 6 Mar 2018 07:23:56 GMT"}], "update_date": "2018-03-07", "authors_parsed": [["Awan", "Ruqayya", ""], ["Koohbanani", "Navid Alemi", ""], ["Shaban", "Muhammad", ""], ["Lisowska", "Anna", ""], ["Rajpoot", "Nasir", ""]]}, {"id": "1803.00387", "submitter": "Xinxin Du", "authors": "Xinxin Du, Marcelo H. Ang Jr., Sertac Karaman, Daniela Rus", "title": "A General Pipeline for 3D Detection of Vehicles", "comments": "Accepted at ICRA 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous driving requires 3D perception of vehicles and other objects in\nthe in environment. Much of the current methods support 2D vehicle detection.\nThis paper proposes a flexible pipeline to adopt any 2D detection network and\nfuse it with a 3D point cloud to generate 3D information with minimum changes\nof the 2D detection networks. To identify the 3D box, an effective model\nfitting algorithm is developed based on generalised car models and score maps.\nA two-stage convolutional neural network (CNN) is proposed to refine the\ndetected 3D box. This pipeline is tested on the KITTI dataset using two\ndifferent 2D detection networks. The 3D detection results based on these two\nnetworks are similar, demonstrating the flexibility of the proposed pipeline.\nThe results rank second among the 3D detection algorithms, indicating its\ncompetencies in 3D detection.\n", "versions": [{"version": "v1", "created": "Mon, 12 Feb 2018 15:32:23 GMT"}], "update_date": "2018-03-02", "authors_parsed": [["Du", "Xinxin", ""], ["Ang", "Marcelo H.", "Jr."], ["Karaman", "Sertac", ""], ["Rus", "Daniela", ""]]}, {"id": "1803.00388", "submitter": "Ilker Cam", "authors": "Ilker Cam, F. Boray Tek", "title": "Learning Filter Scale and Orientation In CNNs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks have many hyperparameters such as the filter\nsize, number of filters, and pooling size, which require manual tuning. Though\ndeep stacked structures are able to create multi-scale and hierarchical\nrepresentations, manually fixed filter sizes limit the scale of representations\nthat can be learned in a single convolutional layer.\n  This paper introduces a new adaptive filter model that allows variable scale\nand orientation. The scale and orientation parameters of filters can be learned\nusing back propagation. Therefore, in a single convolution layer, we can create\nfilters of different scale and orientation that can adapt to small or large\nfeatures and objects. The proposed model uses a relatively large base size\n(grid) for filters. In the grid, a differentiable function acts as an envelope\nfor the filters. The envelope function guides effective filter scale and\nshape/orientation by masking the filter weights before the convolution.\nTherefore, only the weights in the envelope are updated during training.\n  In this work, we employed a multivariate (2D) Gaussian as the envelope\nfunction and showed that it can grow, shrink, or rotate by updating its\ncovariance matrix during back propagation training . We tested the new filter\nmodel on MNIST, MNIST-cluttered, and CIFAR-10 and compared the results with the\nnetworks that used conventional convolution layers. The results demonstrate\nthat the new model can effectively learn and produce filters of different\nscales and orientations in a single layer. Moreover, the experiments show that\nthe adaptive convolution layers perform equally; or better, especially when\ndata includes objects of varying scale and noisy backgrounds.\n", "versions": [{"version": "v1", "created": "Tue, 13 Feb 2018 19:45:21 GMT"}], "update_date": "2018-03-02", "authors_parsed": [["Cam", "Ilker", ""], ["Tek", "F. Boray", ""]]}, {"id": "1803.00389", "submitter": "Milad Niknejad", "authors": "Milad Niknejad and Mario A.T. Figueiredo", "title": "Poisson Image Denoising Using Best Linear Prediction: A Post-processing\n  Framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the problem of denoising images degraded by Poisson\nnoise. We propose a new patch-based approach based on best linear prediction to\nestimate the underlying clean image. A simplified prediction formula is derived\nfor Poisson observations, which requires the covariance matrix of the\nunderlying clean patch. We use the assumption that similar patches in a\nneighborhood share the same covariance matrix, and we use off-the-shelf Poisson\ndenoising methods in order to obtain an initial estimate of the covariance\nmatrices. Our method can be seen as a post-processing step for Poisson\ndenoising methods and the results show that it improves upon several Poisson\ndenoising methods by relevant margins.\n", "versions": [{"version": "v1", "created": "Thu, 1 Mar 2018 14:42:24 GMT"}], "update_date": "2018-03-02", "authors_parsed": [["Niknejad", "Milad", ""], ["Figueiredo", "Mario A. T.", ""]]}, {"id": "1803.00391", "submitter": "Hongjia Li", "authors": "Hongjia Li, Xiaolong Ma, Aditya Singh Rathore, Zhe Li, Qiyuan An, Chen\n  Song, Wenyao Xu and Yanzhi Wang", "title": "Image Dataset for Visual Objects Classification in 3D Printing", "comments": "It is not accepted and the work needed major reversion and\n  improvement", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rapid development in additive manufacturing (AM), also known as 3D\nprinting, has brought about potential risk and security issues along with\nsignificant benefits. In order to enhance the security level of the 3D printing\nprocess, the present research aims to detect and recognize illegal components\nusing deep learning. In this work, we collected a dataset of 61,340 2D images\n(28x28 for each image) of 10 classes including guns and other non-gun objects,\ncorresponding to the projection results of the original 3D models. To validate\nthe dataset, we train a convolutional neural network (CNN) model for gun\nclassification which can achieve 98.16% classification accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 15 Feb 2018 20:23:52 GMT"}, {"version": "v2", "created": "Thu, 22 Mar 2018 16:30:04 GMT"}], "update_date": "2018-03-23", "authors_parsed": [["Li", "Hongjia", ""], ["Ma", "Xiaolong", ""], ["Rathore", "Aditya Singh", ""], ["Li", "Zhe", ""], ["An", "Qiyuan", ""], ["Song", "Chen", ""], ["Xu", "Wenyao", ""], ["Wang", "Yanzhi", ""]]}, {"id": "1803.00395", "submitter": "Ni Chen", "authors": "Ao Zhou, Wei Wang, Ni Chen, Edmund Y. Lam, Byoungho Lee, Guohai Situ", "title": "Fast and robust misalignment correction of Fourier ptychographic\n  microscopy", "comments": null, "journal-ref": null, "doi": "10.1364/OE.26.023661", "report-no": null, "categories": "cs.CV physics.optics", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Fourier ptychographi cmicroscopy(FPM) is a newly developed computational\nimaging technique that can provide gigapixel images with both high resolution\n(HR) and wide field of view (FOV). However, the positional misalignment of the\nLED array induces a degradation of the reconstruction, especially in the\nregions away from the optical axis. In this paper, we propose a robust and fast\nmethod to correct the LED misalignment of FPM, termed as misalignment\ncorrection for FPM (mcFPM). Although different regions in the FOV have\ndifferent sensitivity to the LED misalignment, the experimental results show\nthat mcFPM is robust to eliminate the degradation in each region. Compared with\nthe state-of-the-art methods, mcFPM is much faster.\n", "versions": [{"version": "v1", "created": "Tue, 20 Feb 2018 03:32:29 GMT"}], "update_date": "2018-09-03", "authors_parsed": [["Zhou", "Ao", ""], ["Wang", "Wei", ""], ["Chen", "Ni", ""], ["Lam", "Edmund Y.", ""], ["Lee", "Byoungho", ""], ["Situ", "Guohai", ""]]}, {"id": "1803.00397", "submitter": "Evgeny Burnaev", "authors": "Alexey Trekin and German Novikov and Georgy Potapov and Vladimir\n  Ignatiev and Evgeny Burnaev", "title": "Satellite imagery analysis for operational damage assessment in\n  Emergency situations", "comments": "12 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When major disaster occurs the questions are raised how to estimate the\ndamage in time to support the decision making process and relief efforts by\nlocal authorities or humanitarian teams. In this paper we consider the use of\nMachine Learning and Computer Vision on remote sensing imagery to improve time\nefficiency of assessment of damaged buildings in disaster affected area. We\npropose a general workflow that can be useful in various disaster management\napplications, and demonstrate the use of the proposed workflow for the\nassessment of the damage caused by the wildfires in California in 2017.\n", "versions": [{"version": "v1", "created": "Mon, 19 Feb 2018 08:04:25 GMT"}], "update_date": "2018-03-02", "authors_parsed": [["Trekin", "Alexey", ""], ["Novikov", "German", ""], ["Potapov", "Georgy", ""], ["Ignatiev", "Vladimir", ""], ["Burnaev", "Evgeny", ""]]}, {"id": "1803.00398", "submitter": "Oleg Kupervasser", "authors": "Oleg Kupervasser, Vitalii Sarychev, Alexander Rubinstein, Roman Yavich", "title": "Robust positioning of drones for land use monitoring in strong terrain\n  relief using vision-based navigation", "comments": "7 pages, 7 figures", "journal-ref": "International Journal of GEOMATE, May, 2018 Vol.14, Issue 45,\n  pp.10-16; Proceeding of the 7 Conference on GEOMATE 2017, November 2017, Tsu\n  City, Japan", "doi": "10.21660/2018.45.7322", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For land use monitoring, the main problems are robust positioning in urban\ncanyons and strong terrain reliefs with the use of GPS system only. Indeed,\nsatellite signal reflection and shielding in urban canyons and strong terrain\nrelief results in problems with correct positioning. Using GNSS-RTK does not\nsolve the problem completely because in some complex situations the whole\nsatellite's system works incorrectly. We transform the weakness (urban canyons\nand strong terrain relief) to an advantage. It is a vision-based navigation\nusing a map of the terrain relief. We investigate and demonstrate the\neffectiveness of this technology in Chinese region Xiaoshan. The accuracy of\nthe vision-based navigation system corresponds to the expected for these\nconditions. . It was concluded that the maximum position error based on\nvision-based navigation is 20 m and the maximum angle Euler error based on\nvision-based navigation is 0.83 degree. In case of camera movement, the maximum\nposition error based on vision-based navigation is 30m and the maximum Euler\nangle error based on vision-based navigation is 2.2 degrees.\n", "versions": [{"version": "v1", "created": "Tue, 20 Feb 2018 06:51:09 GMT"}], "update_date": "2018-03-02", "authors_parsed": [["Kupervasser", "Oleg", ""], ["Sarychev", "Vitalii", ""], ["Rubinstein", "Alexander", ""], ["Yavich", "Roman", ""]]}, {"id": "1803.00399", "submitter": "Siming Yan", "authors": "Siming Yan, Feng Shi, Yuhua Chen, Damini Dey, Sang-Eun Lee, Hyuk-Jae\n  Chang, Debiao Li, Yibin Xie", "title": "Calcium Removal From Cardiac CT Images Using Deep Convolutional Neural\n  Network", "comments": "Accepted by ISBI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coronary calcium causes beam hardening and blooming artifacts on cardiac\ncomputed tomography angiography (CTA) images, which lead to overestimation of\nlumen stenosis and reduction of diagnostic specificity. To properly remove\ncoronary calcification and restore arterial lumen precisely, we propose a\nmachine learning-based method with a multi-step inpainting process. We\ndeveloped a new network configuration, Dense-Unet, to achieve optimal\nperformance with low computational cost. Results after the calcium removal\nprocess were validated by comparing with gold-standard X-ray angiography. Our\nresults demonstrated that removing coronary calcification from images with the\nproposed approach was feasible, and may potentially improve the diagnostic\naccuracy of CTA.\n", "versions": [{"version": "v1", "created": "Tue, 20 Feb 2018 23:10:34 GMT"}], "update_date": "2018-03-02", "authors_parsed": [["Yan", "Siming", ""], ["Shi", "Feng", ""], ["Chen", "Yuhua", ""], ["Dey", "Damini", ""], ["Lee", "Sang-Eun", ""], ["Chang", "Hyuk-Jae", ""], ["Li", "Debiao", ""], ["Xie", "Yibin", ""]]}, {"id": "1803.00401", "submitter": "Akshay Agarwal", "authors": "Gaurav Goswami, Nalini Ratha, Akshay Agarwal, Richa Singh, Mayank\n  Vatsa", "title": "Unravelling Robustness of Deep Learning based Face Recognition Against\n  Adversarial Attacks", "comments": "Accepted in AAAI 2018 (8 pages, 5 figures, 5 tables)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural network (DNN) architecture based models have high expressive\npower and learning capacity. However, they are essentially a black box method\nsince it is not easy to mathematically formulate the functions that are learned\nwithin its many layers of representation. Realizing this, many researchers have\nstarted to design methods to exploit the drawbacks of deep learning based\nalgorithms questioning their robustness and exposing their singularities. In\nthis paper, we attempt to unravel three aspects related to the robustness of\nDNNs for face recognition: (i) assessing the impact of deep architectures for\nface recognition in terms of vulnerabilities to attacks inspired by commonly\nobserved distortions in the real world that are well handled by shallow\nlearning methods along with learning based adversaries; (ii) detecting the\nsingularities by characterizing abnormal filter response behavior in the hidden\nlayers of deep networks; and (iii) making corrections to the processing\npipeline to alleviate the problem. Our experimental evaluation using multiple\nopen-source DNN-based face recognition networks, including OpenFace and\nVGG-Face, and two publicly available databases (MEDS and PaSC) demonstrates\nthat the performance of deep learning based face recognition algorithms can\nsuffer greatly in the presence of such distortions. The proposed method is also\ncompared with existing detection algorithms and the results show that it is\nable to detect the attacks with very high accuracy by suitably designing a\nclassifier using the response of the hidden layers in the network. Finally, we\npresent several effective countermeasures to mitigate the impact of adversarial\nattacks and improve the overall robustness of DNN-based face recognition.\n", "versions": [{"version": "v1", "created": "Thu, 22 Feb 2018 08:03:26 GMT"}], "update_date": "2018-03-02", "authors_parsed": [["Goswami", "Gaurav", ""], ["Ratha", "Nalini", ""], ["Agarwal", "Akshay", ""], ["Singh", "Richa", ""], ["Vatsa", "Mayank", ""]]}, {"id": "1803.00404", "submitter": "Ziang Yan", "authors": "Ziang Yan, Yiwen Guo, Changshui Zhang", "title": "Deep Defense: Training DNNs with Improved Adversarial Robustness", "comments": "Accepted by NeurIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the efficacy on a variety of computer vision tasks, deep neural\nnetworks (DNNs) are vulnerable to adversarial attacks, limiting their\napplications in security-critical systems. Recent works have shown the\npossibility of generating imperceptibly perturbed image inputs (a.k.a.,\nadversarial examples) to fool well-trained DNN classifiers into making\narbitrary predictions. To address this problem, we propose a training recipe\nnamed \"deep defense\". Our core idea is to integrate an adversarial\nperturbation-based regularizer into the classification objective, such that the\nobtained models learn to resist potential attacks, directly and precisely. The\nwhole optimization problem is solved just like training a recursive network.\nExperimental results demonstrate that our method outperforms training with\nadversarial/Parseval regularizations by large margins on various datasets\n(including MNIST, CIFAR-10 and ImageNet) and different DNN architectures. Code\nand models for reproducing our results are available at\nhttps://github.com/ZiangYan/deepdefense.pytorch\n", "versions": [{"version": "v1", "created": "Fri, 23 Feb 2018 05:02:59 GMT"}, {"version": "v2", "created": "Wed, 30 May 2018 01:59:20 GMT"}, {"version": "v3", "created": "Thu, 20 Dec 2018 01:53:51 GMT"}], "update_date": "2018-12-21", "authors_parsed": [["Yan", "Ziang", ""], ["Guo", "Yiwen", ""], ["Zhang", "Changshui", ""]]}, {"id": "1803.00406", "submitter": "Shadrokh Samavi", "authors": "Alireza Norouzi, Ali Emami, S.M.Reza Soroushmehr, Nader Karimi,\n  Shadrokh Samavi, Kayvan Najarian", "title": "Left ventricle segmentation By modelling uncertainty in prediction of\n  deep convolutional neural networks and adaptive thresholding inference", "comments": "5 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have shown great achievements in solving complex\nproblems. However, there are fundamental problems that limit their real world\napplications. Lack of measurable criteria for estimating uncertainty in the\nnetwork outputs is one of these problems. In this paper, we address this\nlimitation by introducing deformation to the network input and measuring the\nlevel of stability in the network's output. We calculate simple random\ntransformations to estimate the prediction uncertainty of deep convolutional\nneural networks. For a real use-case, we apply this method to left ventricle\nsegmentation in MRI cardiac images. We also propose an adaptive thresholding\nmethod to consider the deep neural network uncertainty. Experimental results\ndemonstrate state-of-the-art performance and highlight the capabilities of\nsimple methods in conjunction with deep neural networks.\n", "versions": [{"version": "v1", "created": "Fri, 23 Feb 2018 06:11:37 GMT"}], "update_date": "2018-03-02", "authors_parsed": [["Norouzi", "Alireza", ""], ["Emami", "Ali", ""], ["Soroushmehr", "S. M. Reza", ""], ["Karimi", "Nader", ""], ["Samavi", "Shadrokh", ""], ["Najarian", "Kayvan", ""]]}, {"id": "1803.00407", "submitter": "Marc Chaumont", "authors": "Mehdi Yedroudj and Frederic Comby and Marc Chaumont", "title": "Yedrouj-Net: An efficient CNN for spatial steganalysis", "comments": "ICASSP'2018, 15-20 April 2018, Calgary, Alberta, Canada, 5 pages", "journal-ref": "IEEE International Conference on Acoustics, Speech and Signal\n  Processing, ICASSP'2018, 15-20 April 2018, Calgary, Alberta, Canada, 5 pages", "doi": null, "report-no": null, "categories": "cs.CV cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For about 10 years, detecting the presence of a secret message hidden in an\nimage was performed with an Ensemble Classifier trained with Rich features. In\nrecent years, studies such as Xu et al. have indicated that well-designed\nconvolutional Neural Networks (CNN) can achieve comparable performance to the\ntwo-step machine learning approaches.\n  In this paper, we propose a CNN that outperforms the state-ofthe-art in terms\nof error probability. The proposition is in the continuity of what has been\nrecently proposed and it is a clever fusion of important bricks used in various\npapers. Among the essential parts of the CNN, one can cite the use of a\npre-processing filterbank and a Truncation activation function, five\nconvolutional layers with a Batch Normalization associated with a Scale Layer,\nas well as the use of a sufficiently sized fully connected section. An\naugmented database has also been used to improve the training of the CNN.\n  Our CNN was experimentally evaluated against S-UNIWARD and WOW embedding\nalgorithms and its performances were compared with those of three other\nmethods: an Ensemble Classifier plus a Rich Model, and two other CNN\nsteganalyzers.\n", "versions": [{"version": "v1", "created": "Mon, 26 Feb 2018 14:09:14 GMT"}], "update_date": "2018-03-02", "authors_parsed": [["Yedroudj", "Mehdi", ""], ["Comby", "Frederic", ""], ["Chaumont", "Marc", ""]]}, {"id": "1803.00425", "submitter": "Hichem Sahbi", "authors": "Anjan Dutta and Hichem Sahbi", "title": "Graph Kernels based on High Order Graphlet Parsing and Hashing", "comments": "arXiv admin note: substantial text overlap with arXiv:1702.00156", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph-based methods are known to be successful in many machine learning and\npattern classification tasks. These methods consider semi-structured data as\ngraphs where nodes correspond to primitives (parts, interest points, segments,\netc.) and edges characterize the relationships between these primitives.\nHowever, these non-vectorial graph data cannot be straightforwardly plugged\ninto off-the-shelf machine learning algorithms without a preliminary step of --\nexplicit/implicit -- graph vectorization and embedding. This embedding process\nshould be resilient to intra-class graph variations while being highly\ndiscriminant. In this paper, we propose a novel high-order stochastic graphlet\nembedding (SGE) that maps graphs into vector spaces. Our main contribution\nincludes a new stochastic search procedure that efficiently parses a given\ngraph and extracts/samples unlimitedly high-order graphlets. We consider these\ngraphlets, with increasing orders, to model local primitives as well as their\nincreasingly complex interactions. In order to build our graph representation,\nwe measure the distribution of these graphlets into a given graph, using\nparticular hash functions that efficiently assign sampled graphlets into\nisomorphic sets with a very low probability of collision. When combined with\nmaximum margin classifiers, these graphlet-based representations have positive\nimpact on the performance of pattern comparison and recognition as corroborated\nthrough extensive experiments using standard benchmark databases.\n", "versions": [{"version": "v1", "created": "Wed, 28 Feb 2018 12:37:41 GMT"}], "update_date": "2018-03-02", "authors_parsed": [["Dutta", "Anjan", ""], ["Sahbi", "Hichem", ""]]}, {"id": "1803.00443", "submitter": "Suraj Srinivas", "authors": "Suraj Srinivas, Francois Fleuret", "title": "Knowledge Transfer with Jacobian Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Classical distillation methods transfer representations from a \"teacher\"\nneural network to a \"student\" network by matching their output activations.\nRecent methods also match the Jacobians, or the gradient of output activations\nwith the input. However, this involves making some ad hoc decisions, in\nparticular, the choice of the loss function.\n  In this paper, we first establish an equivalence between Jacobian matching\nand distillation with input noise, from which we derive appropriate loss\nfunctions for Jacobian matching. We then rely on this analysis to apply\nJacobian matching to transfer learning by establishing equivalence of a recent\ntransfer learning procedure to distillation.\n  We then show experimentally on standard image datasets that Jacobian-based\npenalties improve distillation, robustness to noisy inputs, and transfer\nlearning.\n", "versions": [{"version": "v1", "created": "Thu, 1 Mar 2018 15:31:26 GMT"}], "update_date": "2018-03-02", "authors_parsed": [["Srinivas", "Suraj", ""], ["Fleuret", "Francois", ""]]}, {"id": "1803.00455", "submitter": "Gr\\'egory Rogez", "authors": "Gregory Rogez, Philippe Weinzaepfel and Cordelia Schmid", "title": "LCR-Net++: Multi-person 2D and 3D Pose Detection in Natural Images", "comments": "journal version of the CVPR 2017 paper, accepted to appear in IEEE\n  Trans. PAMI", "journal-ref": null, "doi": "10.1109/TPAMI.2019.2892985", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an end-to-end architecture for joint 2D and 3D human pose\nestimation in natural images. Key to our approach is the generation and scoring\nof a number of pose proposals per image, which allows us to predict 2D and 3D\nposes of multiple people simultaneously. Hence, our approach does not require\nan approximate localization of the humans for initialization. Our\nLocalization-Classification-Regression architecture, named LCR-Net, contains 3\nmain components: 1) the pose proposal generator that suggests candidate poses\nat different locations in the image; 2) a classifier that scores the different\npose proposals; and 3) a regressor that refines pose proposals both in 2D and\n3D. All three stages share the convolutional feature layers and are trained\njointly. The final pose estimation is obtained by integrating over neighboring\npose hypotheses, which is shown to improve over a standard non maximum\nsuppression algorithm. Our method recovers full-body 2D and 3D poses,\nhallucinating plausible body parts when the persons are partially occluded or\ntruncated by the image boundary. Our approach significantly outperforms the\nstate of the art in 3D pose estimation on Human3.6M, a controlled environment.\nMoreover, it shows promising results on real images for both single and\nmulti-person subsets of the MPII 2D pose benchmark and demonstrates satisfying\n3D pose results even for multi-person images.\n", "versions": [{"version": "v1", "created": "Thu, 1 Mar 2018 15:39:38 GMT"}, {"version": "v2", "created": "Sat, 13 Oct 2018 17:31:32 GMT"}, {"version": "v3", "created": "Sun, 13 Jan 2019 19:33:24 GMT"}], "update_date": "2019-01-15", "authors_parsed": [["Rogez", "Gregory", ""], ["Weinzaepfel", "Philippe", ""], ["Schmid", "Cordelia", ""]]}, {"id": "1803.00492", "submitter": "Andre Mastmeyer", "authors": "Andre Mastmeyer", "title": "Genaue modellbasierte Identifikation von gyn\\\"akologischen\n  Katheterpfaden f\\\"ur die MRT-bildgest\\\"utzte Brachytherapie", "comments": "German text: 5 pages, 3 figures, Management im Krankenhaus 11/2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.med-ph cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  German text, english abstract: Mortality in gynecologic cancers, including\ncervical, ovarian, vaginal and vulvar cancers, is more than 6% internationally\n[1]. In many countries external radiotherapy is supplemented by brachytherapy\nwith high locally administered doses as standard. The superior ability of\nmagnetic resonance imaging (MRI) to differentiate soft tissue has led to an\nincreasing use of this imaging technique in the intraoperative planning and\nimplementation of brachytherapy. A technical challenge associated with the use\nof MRI imaging for brachytherapy - in contrast to computed tomography (CT)\nimaging - is the dark-diffuse appearance and thus difficult identification of\nthe catheter paths in the resulting images. This problem is addressed by the\nprecise method described herein of tracing the catheters from the catheter tip.\nThe average identification time for a single catheter path was three seconds on\na standard PC. Segmentation time, accuracy and precision are promising\nindicators of the value of this method for the clinical application of\nimage-guided gynecological brachytherapy. After surgery (OP), the healthy\nsurrounding tissue of the tumor is usually irradiated. This reduces the risk of\nleaving behind residual cells that would likely cause a recurrence of the\ncancer or the formation of metastases - secondary tumors elsewhere in the body.\nIn the case of a tumor on the cervix or prostate, the operation is minimally\ninvasive, ie. the removal of the cancer and the irradiation are performed\ncost-effectively and risk-avoiding by keyhole surgery instead of open surgery.\n", "versions": [{"version": "v1", "created": "Tue, 27 Feb 2018 17:05:56 GMT"}, {"version": "v2", "created": "Sat, 10 Mar 2018 10:22:32 GMT"}], "update_date": "2018-03-13", "authors_parsed": [["Mastmeyer", "Andre", ""]]}, {"id": "1803.00500", "submitter": "Tom Lorimer", "authors": "Tom Lorimer, Karlis Kanders, Ruedi Stoop", "title": "Natural data structure extracted from neighborhood-similarity graphs", "comments": null, "journal-ref": null, "doi": "10.1016/j.chaos.2018.12.033", "report-no": null, "categories": "stat.ML cond-mat.dis-nn cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  'Big' high-dimensional data are commonly analyzed in low-dimensions, after\nperforming a dimensionality-reduction step that inherently distorts the data\nstructure. For the same purpose, clustering methods are also often used. These\nmethods also introduce a bias, either by starting from the assumption of a\nparticular geometric form of the clusters, or by using iterative schemes to\nenhance cluster contours, with uncontrollable consequences. The goal of data\nanalysis should, however, be to encode and detect structural data features at\nall scales and densities simultaneously, without assuming a parametric form of\ndata point distances, or modifying them. We propose a novel approach that\ndirectly encodes data point neighborhood similarities as a sparse graph. Our\nnon-iterative framework permits a transparent interpretation of data, without\naltering the original data dimension and metric. Several natural and synthetic\ndata applications demonstrate the efficacy of our novel approach.\n", "versions": [{"version": "v1", "created": "Thu, 15 Feb 2018 14:06:46 GMT"}], "update_date": "2019-02-20", "authors_parsed": [["Lorimer", "Tom", ""], ["Kanders", "Karlis", ""], ["Stoop", "Ruedi", ""]]}, {"id": "1803.00557", "submitter": "Jordi Pont-Tuset", "authors": "Sergi Caelles and Alberto Montes and Kevis-Kokitsi Maninis and Yuhua\n  Chen and Luc Van Gool and Federico Perazzi and Jordi Pont-Tuset", "title": "The 2018 DAVIS Challenge on Video Object Segmentation", "comments": "Challenge website: http://davischallenge.org/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the 2018 DAVIS Challenge on Video Object Segmentation, a public\ncompetition specifically designed for the task of video object segmentation. It\nbuilds upon the DAVIS 2017 dataset, which was presented in the previous edition\nof the DAVIS Challenge, and added 100 videos with multiple objects per sequence\nto the original DAVIS 2016 dataset. Motivated by the analysis of the results of\nthe 2017 edition, the main track of the competition will be the same than in\nthe previous edition (segmentation given the full mask of the objects in the\nfirst frame -- semi-supervised scenario). This edition, however, also adds an\ninteractive segmentation teaser track, where the participants will interact\nwith a web service simulating the input of a human that provides scribbles to\niteratively improve the result.\n", "versions": [{"version": "v1", "created": "Thu, 1 Mar 2018 18:56:36 GMT"}, {"version": "v2", "created": "Tue, 27 Mar 2018 10:02:26 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Caelles", "Sergi", ""], ["Montes", "Alberto", ""], ["Maninis", "Kevis-Kokitsi", ""], ["Chen", "Yuhua", ""], ["Van Gool", "Luc", ""], ["Perazzi", "Federico", ""], ["Pont-Tuset", "Jordi", ""]]}, {"id": "1803.00638", "submitter": "Lorenzo Putzu", "authors": "C. Di Ruberto, L. Putzu and G. Rodriguez", "title": "Fast and accurate computation of orthogonal moments for texture analysis", "comments": "29 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we describe a fast and stable algorithm for the computation of\nthe orthogonal moments of an image. Indeed, orthogonal moments are\ncharacterized by a high discriminative power, but some of their possible\nformulations are characterized by a large computational complexity, which\nlimits their real-time application. This paper describes in detail an approach\nbased on recurrence relations, and proposes an optimized Matlab implementation\nof the corresponding computational procedure, aiming to solve the above\nlimitations and put at the community's disposal an efficient and easy to use\nsoftware. In our experiments we evaluate the effectiveness of the recurrence\nformulation, as well as its performance for the reconstruction task, in\ncomparison to the closed form representation, often used in the literature. The\nresults show a sensible reduction in the computational complexity, together\nwith a greater accuracy in reconstruction. In order to assess and compare the\naccuracy of the computed moments in texture analysis, we perform classification\nexperiments on six well-known databases of texture images. Again, the\nrecurrence formulation performs better in classification than the closed form\nrepresentation. More importantly, if computed from the GLCM of the image using\nthe proposed stable procedure, the orthogonal moments outperform in some\nsituations some of the most diffused state-of-the-art descriptors for texture\nclassification.\n", "versions": [{"version": "v1", "created": "Thu, 1 Mar 2018 21:40:42 GMT"}, {"version": "v2", "created": "Wed, 2 May 2018 14:51:05 GMT"}], "update_date": "2018-05-03", "authors_parsed": [["Di Ruberto", "C.", ""], ["Putzu", "L.", ""], ["Rodriguez", "G.", ""]]}, {"id": "1803.00653", "submitter": "Alexey Dosovitskiy", "authors": "Nikolay Savinov, Alexey Dosovitskiy, Vladlen Koltun", "title": "Semi-parametric Topological Memory for Navigation", "comments": "Published at International Conference on Learning Representations\n  (ICLR) 2018. Project website at https://sites.google.com/view/SPTM", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new memory architecture for navigation in previously unseen\nenvironments, inspired by landmark-based navigation in animals. The proposed\nsemi-parametric topological memory (SPTM) consists of a (non-parametric) graph\nwith nodes corresponding to locations in the environment and a (parametric)\ndeep network capable of retrieving nodes from the graph based on observations.\nThe graph stores no metric information, only connectivity of locations\ncorresponding to the nodes. We use SPTM as a planning module in a navigation\nsystem. Given only 5 minutes of footage of a previously unseen maze, an\nSPTM-based navigation agent can build a topological map of the environment and\nuse it to confidently navigate towards goals. The average success rate of the\nSPTM agent in goal-directed navigation across test environments is higher than\nthe best-performing baseline by a factor of three. A video of the agent is\navailable at https://youtu.be/vRF7f4lhswo\n", "versions": [{"version": "v1", "created": "Thu, 1 Mar 2018 22:50:35 GMT"}], "update_date": "2018-03-05", "authors_parsed": [["Savinov", "Nikolay", ""], ["Dosovitskiy", "Alexey", ""], ["Koltun", "Vladlen", ""]]}, {"id": "1803.00663", "submitter": "Fei Gao", "authors": "Fei Gao, Teresa Wu, Jing Li, Bin Zheng, Lingxiang Ruan, Desheng Shang\n  and Bhavika Patel", "title": "SD-CNN: a Shallow-Deep CNN for Improved Breast Cancer Diagnosis", "comments": null, "journal-ref": "Computerized Medical Imaging and Graphics (2018) 70 53-62", "doi": "10.1016/j.compmedimag.2018.09.004", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Breast cancer is the second leading cause of cancer death among women\nworldwide. Nevertheless, it is also one of the most treatable malignances if\ndetected early. Screening for breast cancer with digital mammography (DM) has\nbeen widely used. However it demonstrates limited sensitivity for women with\ndense breasts. An emerging technology in the field is contrast-enhanced digital\nmammography (CEDM), which includes a low energy (LE) image similar to DM, and a\nrecombined image leveraging tumor neoangiogenesis similar to breast magnetic\nresonance imaging (MRI). CEDM has shown better diagnostic accuracy than DM.\nWhile promising, CEDM is not yet widely available across medical centers. In\nthis research, we propose a Shallow-Deep Convolutional Neural Network (SD-CNN)\nwhere a shallow CNN is developed to derive \"virtual\" recombined images from LE\nimages, and a deep CNN is employed to extract novel features from LE,\nrecombined or \"virtual\" recombined images for ensemble models to classify the\ncases as benign vs. cancer. To evaluate the validity of our approach, we first\ndevelop a deep-CNN using 49 CEDM cases collected from Mayo Clinic to prove the\ncontributions from recombined images for improved breast cancer diagnosis (0.86\nin accuracy using LE imaging vs. 0.90 in accuracy using both LE and recombined\nimaging). We then develop a shallow-CNN using the same 49 CEDM cases to learn\nthe nonlinear mapping from LE to recombined images. Next, we use 69 DM cases\ncollected from the hospital located at Zhejiang University, China to generate\n\"virtual\" recombined images. Using DM alone provides 0.91 in accuracy, whereas\nSD-CNN improves the diagnostic accuracy to 0.95.\n", "versions": [{"version": "v1", "created": "Thu, 1 Mar 2018 23:59:20 GMT"}, {"version": "v2", "created": "Fri, 26 Oct 2018 17:48:24 GMT"}], "update_date": "2018-10-29", "authors_parsed": [["Gao", "Fei", ""], ["Wu", "Teresa", ""], ["Li", "Jing", ""], ["Zheng", "Bin", ""], ["Ruan", "Lingxiang", ""], ["Shang", "Desheng", ""], ["Patel", "Bhavika", ""]]}, {"id": "1803.00676", "submitter": "Mengye Ren", "authors": "Mengye Ren, Eleni Triantafillou, Sachin Ravi, Jake Snell, Kevin\n  Swersky, Joshua B. Tenenbaum, Hugo Larochelle, Richard S. Zemel", "title": "Meta-Learning for Semi-Supervised Few-Shot Classification", "comments": "Published as a conference paper at ICLR 2018. 15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In few-shot classification, we are interested in learning algorithms that\ntrain a classifier from only a handful of labeled examples. Recent progress in\nfew-shot classification has featured meta-learning, in which a parameterized\nmodel for a learning algorithm is defined and trained on episodes representing\ndifferent classification problems, each with a small labeled training set and\nits corresponding test set. In this work, we advance this few-shot\nclassification paradigm towards a scenario where unlabeled examples are also\navailable within each episode. We consider two situations: one where all\nunlabeled examples are assumed to belong to the same set of classes as the\nlabeled examples of the episode, as well as the more challenging situation\nwhere examples from other distractor classes are also provided. To address this\nparadigm, we propose novel extensions of Prototypical Networks (Snell et al.,\n2017) that are augmented with the ability to use unlabeled examples when\nproducing prototypes. These models are trained in an end-to-end way on\nepisodes, to learn to leverage the unlabeled examples successfully. We evaluate\nthese methods on versions of the Omniglot and miniImageNet benchmarks, adapted\nto this new framework augmented with unlabeled examples. We also propose a new\nsplit of ImageNet, consisting of a large set of classes, with a hierarchical\nstructure. Our experiments confirm that our Prototypical Networks can learn to\nimprove their predictions due to unlabeled examples, much like a\nsemi-supervised algorithm would.\n", "versions": [{"version": "v1", "created": "Fri, 2 Mar 2018 01:07:49 GMT"}], "update_date": "2018-03-05", "authors_parsed": [["Ren", "Mengye", ""], ["Triantafillou", "Eleni", ""], ["Ravi", "Sachin", ""], ["Snell", "Jake", ""], ["Swersky", "Kevin", ""], ["Tenenbaum", "Joshua B.", ""], ["Larochelle", "Hugo", ""], ["Zemel", "Richard S.", ""]]}, {"id": "1803.00686", "submitter": "Gantugs Atarsaikhan", "authors": "Gantugs Atarsaikhan, Brian Kenji Iwana and Seiichi Uchida", "title": "Constrained Neural Style Transfer for Decorated Logo Generation", "comments": "Accepted by DAS2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Making decorated logos requires image editing skills, without sufficient\nskills, it could be a time-consuming task. While there are many on-line web\nservices to make new logos, they have limited designs and duplicates can be\nmade. We propose using neural style transfer with clip art and text for the\ncreation of new and genuine logos. We introduce a new loss function based on\ndistance transform of the input image, which allows the preservation of the\nsilhouettes of text and objects. The proposed method constrains style transfer\nonly around the designated area. We demonstrate the characteristics of proposed\nmethod. Finally, we show the results of logo generation with various input\nimages.\n", "versions": [{"version": "v1", "created": "Fri, 2 Mar 2018 02:47:33 GMT"}, {"version": "v2", "created": "Sat, 14 Jul 2018 03:42:19 GMT"}], "update_date": "2018-07-17", "authors_parsed": [["Atarsaikhan", "Gantugs", ""], ["Iwana", "Brian Kenji", ""], ["Uchida", "Seiichi", ""]]}, {"id": "1803.00694", "submitter": "Hoyeon Lee Mr.", "authors": "Hoyeon Lee, Jongha Lee, Hyeongseok Kim, Byungchul Cho and Seungryong\n  Cho", "title": "Deep-neural-network based sinogram synthesis for sparse-view CT image\n  reconstruction", "comments": null, "journal-ref": null, "doi": "10.1109/TRPMS.2018.2867611", "report-no": null, "categories": "physics.med-ph cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, a number of approaches to low-dose computed tomography (CT) have\nbeen developed and deployed in commercialized CT scanners. Tube current\nreduction is perhaps the most actively explored technology with advanced image\nreconstruction algorithms. Sparse data sampling is another viable option to the\nlow-dose CT, and sparse-view CT has been particularly of interest among the\nresearchers in CT community. Since analytic image reconstruction algorithms\nwould lead to severe image artifacts, various iterative algorithms have been\ndeveloped for reconstructing images from sparsely view-sampled projection data.\nHowever, iterative algorithms take much longer computation time than the\nanalytic algorithms, and images are usually prone to different types of image\nartifacts that heavily depend on the reconstruction parameters. Interpolation\nmethods have also been utilized to fill the missing data in the sinogram of\nsparse-view CT thus providing synthetically full data for analytic image\nreconstruction. In this work, we introduce a deep-neural-network-enabled\nsinogram synthesis method for sparse-view CT, and show its outperformance to\nthe existing interpolation methods and also to the iterative image\nreconstruction approach.\n", "versions": [{"version": "v1", "created": "Fri, 2 Mar 2018 03:38:47 GMT"}, {"version": "v2", "created": "Tue, 6 Mar 2018 02:22:59 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Lee", "Hoyeon", ""], ["Lee", "Jongha", ""], ["Kim", "Hyeongseok", ""], ["Cho", "Byungchul", ""], ["Cho", "Seungryong", ""]]}, {"id": "1803.00702", "submitter": "Emad Grais", "authors": "Emad M. Grais, Dominic Ward, and Mark D. Plumbley", "title": "Raw Multi-Channel Audio Source Separation using Multi-Resolution\n  Convolutional Auto-Encoders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Supervised multi-channel audio source separation requires extracting useful\nspectral, temporal, and spatial features from the mixed signals. The success of\nmany existing systems is therefore largely dependent on the choice of features\nused for training. In this work, we introduce a novel multi-channel,\nmulti-resolution convolutional auto-encoder neural network that works on raw\ntime-domain signals to determine appropriate multi-resolution features for\nseparating the singing-voice from stereo music. Our experimental results show\nthat the proposed method can achieve multi-channel audio source separation\nwithout the need for hand-crafted features or any pre- or post-processing.\n", "versions": [{"version": "v1", "created": "Fri, 2 Mar 2018 03:47:47 GMT"}], "update_date": "2018-03-05", "authors_parsed": [["Grais", "Emad M.", ""], ["Ward", "Dominic", ""], ["Plumbley", "Mark D.", ""]]}, {"id": "1803.00737", "submitter": "Anas Al-Oraiqat Dr.", "authors": "Anas M. Al-Oraiqat, E. A. Bashkov, V. Babkov, C. Titarenko", "title": "Fusion of multispectral satellite imagery using a cluster of graphics\n  processing unit", "comments": "7 pages, 5 Figures, 3 tables", "journal-ref": "International Geoinformatics Research and Development Journal,\n  2013", "doi": null, "report-no": null, "categories": "cs.CV cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper presents a parallel implementation of existing image fusion methods\non a graphical cluster. Parallel implementations of methods based on discrete\nwavelet transformation (Haars and Daubechies discrete wavelet transform) are\ndeveloped. Experiments were performed on a cluster using GPU and CPU and\nperformance gains were estimated for the use of the developed parallel\nimplementations to process satellite images from satellite Landsat 7. The\nimplementation on a graphic cluster provides performance improvement from 2 to\n18 times. The quality of the considered methods was evaluated by ERGAS and QNR\nmetrics. The results show performance gains and retaining of quality with the\ncluster of GPU compared to the results obtained by the authors and other\nresearchers for a CPU and single GPU.\n", "versions": [{"version": "v1", "created": "Fri, 2 Mar 2018 07:09:20 GMT"}], "update_date": "2018-03-05", "authors_parsed": [["Al-Oraiqat", "Anas M.", ""], ["Bashkov", "E. A.", ""], ["Babkov", "V.", ""], ["Titarenko", "C.", ""]]}, {"id": "1803.00758", "submitter": "Evgeny Burnaev", "authors": "Oleg Sudakov and Evgeny Burnaev and Dmitry Koroteev", "title": "Driving Digital Rock towards Machine Learning: predicting permeability\n  with Gradient Boosting and Deep Neural Networks", "comments": "21 pages, 7 figures", "journal-ref": null, "doi": "10.1016/j.cageo.2019.02.002", "report-no": null, "categories": "physics.geo-ph cs.CV physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a research study aimed at testing of applicability of machine\nlearning techniques for prediction of permeability of digitized rock samples.\nWe prepare a training set containing 3D images of sandstone samples imaged with\nX-ray microtomography and corresponding permeability values simulated with Pore\nNetwork approach. We also use Minkowski functionals and Deep Learning-based\ndescriptors of 3D images and 2D slices as input features for predictive model\ntraining and prediction. We compare predictive power of various feature sets\nand methods. The later include Gradient Boosting and various architectures of\nDeep Neural Networks (DNN). The results demonstrate applicability of machine\nlearning for image-based permeability prediction and open a new area of Digital\nRock research.\n", "versions": [{"version": "v1", "created": "Fri, 2 Mar 2018 08:41:58 GMT"}, {"version": "v2", "created": "Wed, 14 Mar 2018 19:21:22 GMT"}], "update_date": "2019-05-01", "authors_parsed": [["Sudakov", "Oleg", ""], ["Burnaev", "Evgeny", ""], ["Koroteev", "Dmitry", ""]]}, {"id": "1803.00764", "submitter": "Guillaume Noyel", "authors": "Guillaume Noyel (IPRI), Michel Jourlin (IPRI, LHC)", "title": "Aspl{\\\"u}nd's metric defined in the Logarithmic Image Processing (LIP)\n  framework for colour and multivariate images", "comments": null, "journal-ref": "Proceedings IEEE International Conference on Image Processing\n  (ICIP 2015, Sep 2015, Qu{\\'e}bec City, Canada). ISBN: 978-1-4799-8338-4\n  (CFP15CIP-USB), pp.3921 - 3925, 2015, http://www.icip2015.org/", "doi": "10.1109/ICIP.2015.7351540", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aspl{\\\"u}nd's metric, which is useful for pattern matching, consists in a\ndouble-sided probing, i.e. the over-graph and the sub-graph of a function are\nprobed jointly. It has previously been defined for grey-scale images using the\nLogarithmic Image Processing (LIP) framework. LIP is a non-linear model to\nperform operations between images while being consistent with the human visual\nsystem. Our contribution consists in extending the Aspl{\\\"u}nd's metric to\ncolour and multivariate images using the LIP framework. Aspl{\\\"u}nd's metric is\ninsensitive to lighting variations and we propose a colour variant which is\nrobust to noise.\n", "versions": [{"version": "v1", "created": "Fri, 2 Mar 2018 09:06:54 GMT"}], "update_date": "2018-06-24", "authors_parsed": [["Noyel", "Guillaume", "", "IPRI"], ["Jourlin", "Michel", "", "IPRI, LHC"]]}, {"id": "1803.00788", "submitter": "Andrew Calway Dr", "authors": "Pilailuck Panphattarasap and Andrew Calway", "title": "Automated Map Reading: Image Based Localisation in 2-D Maps Using Binary\n  Semantic Descriptors", "comments": "8 pages, submitted to IEEE/RSJ International Conference on\n  Intelligent Robots and Systems 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a novel approach to image based localisation in urban\nenvironments using semantic matching between images and a 2-D map. It contrasts\nwith the vast majority of existing approaches which use image to image database\nmatching. We use highly compact binary descriptors to represent semantic\nfeatures at locations, significantly increasing scalability compared with\nexisting methods and having the potential for greater invariance to variable\nimaging conditions. The approach is also more akin to human map reading, making\nit more suited to human-system interaction. The binary descriptors indicate the\npresence or not of semantic features relating to buildings and road junctions\nin discrete viewing directions. We use CNN classifiers to detect the features\nin images and match descriptor estimates with a database of location tagged\ndescriptors derived from the 2-D map. In isolation, the descriptors are not\nsufficiently discriminative, but when concatenated sequentially along a route,\ntheir combination becomes highly distinctive and allows localisation even when\nusing non-perfect classifiers. Performance is further improved by taking into\naccount left or right turns over a route. Experimental results obtained using\nGoogle StreetView and OpenStreetMap data show that the approach has\nconsiderable potential, achieving localisation accuracy of around 85% using\nroutes corresponding to approximately 200 meters.\n", "versions": [{"version": "v1", "created": "Fri, 2 Mar 2018 10:14:27 GMT"}], "update_date": "2018-03-05", "authors_parsed": [["Panphattarasap", "Pilailuck", ""], ["Calway", "Andrew", ""]]}, {"id": "1803.00805", "submitter": "Louis Lettry", "authors": "Louis Lettry, Kenneth Vanhoey, Luc van Gool", "title": "Unsupervised Deep Single-Image Intrinsic Decomposition using\n  Illumination-Varying Image Sequences", "comments": "To appear in Pacific Graphics 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning based Single Image Intrinsic Decomposition (SIID) methods\ndecompose a captured scene into its albedo and shading images by using the\nknowledge of a large set of known and realistic ground truth decompositions.\nCollecting and annotating such a dataset is an approach that cannot scale to\nsufficient variety and realism. We free ourselves from this limitation by\ntraining on unannotated images.\n  Our method leverages the observation that two images of the same scene but\nwith different lighting provide useful information on their intrinsic\nproperties: by definition, albedo is invariant to lighting conditions, and\ncross-combining the estimated albedo of a first image with the estimated\nshading of a second one should lead back to the second one's input image. We\ntranscribe this relationship into a siamese training scheme for a deep\nconvolutional neural network that decomposes a single image into albedo and\nshading. The siamese setting allows us to introduce a new loss function\nincluding such cross-combinations, and to train solely on (time-lapse) images,\ndiscarding the need for any ground truth annotations.\n  As a result, our method has the good properties of i) taking advantage of the\ntime-varying information of image sequences in the (pre-computed) training\nstep, ii) not requiring ground truth data to train on, and iii) being able to\ndecompose single images of unseen scenes at runtime. To demonstrate and\nevaluate our work, we additionally propose a new rendered dataset containing\nillumination-varying scenes and a set of quantitative metrics to evaluate SIID\nalgorithms. Despite its unsupervised nature, our results compete with state of\nthe art methods, including supervised and non data-driven methods.\n", "versions": [{"version": "v1", "created": "Fri, 2 Mar 2018 11:06:50 GMT"}, {"version": "v2", "created": "Mon, 3 Sep 2018 08:41:15 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Lettry", "Louis", ""], ["Vanhoey", "Kenneth", ""], ["van Gool", "Luc", ""]]}, {"id": "1803.00830", "submitter": "Ziliang Chen", "authors": "Ruijia Xu, Ziliang Chen, Wangmeng Zuo, Junjie Yan, Liang Lin", "title": "Deep Cocktail Network: Multi-source Unsupervised Domain Adaptation with\n  Category Shift", "comments": "Accepted for publication in Conference on Computer Vision and Pattern\n  Recognition(CVPR), 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised domain adaptation (UDA) conventionally assumes labeled source\nsamples coming from a single underlying source distribution. Whereas in\npractical scenario, labeled data are typically collected from diverse sources.\nThe multiple sources are different not only from the target but also from each\nother, thus, domain adaptater should not be modeled in the same way. Moreover,\nthose sources may not completely share their categories, which further brings a\nnew transfer challenge called category shift. In this paper, we propose a deep\ncocktail network (DCTN) to battle the domain and category shifts among multiple\nsources. Motivated by the theoretical results in \\cite{mansour2009domain}, the\ntarget distribution can be represented as the weighted combination of source\ndistributions, and, the multi-source unsupervised domain adaptation via DCTN is\nthen performed as two alternating steps: i) It deploys multi-way adversarial\nlearning to minimize the discrepancy between the target and each of the\nmultiple source domains, which also obtains the source-specific perplexity\nscores to denote the possibilities that a target sample belongs to different\nsource domains. ii) The multi-source category classifiers are integrated with\nthe perplexity scores to classify target sample, and the pseudo-labeled target\nsamples together with source samples are utilized to update the multi-source\ncategory classifier and the feature extractor. We evaluate DCTN in three domain\nadaptation benchmarks, which clearly demonstrate the superiority of our\nframework.\n", "versions": [{"version": "v1", "created": "Fri, 2 Mar 2018 12:58:51 GMT"}], "update_date": "2018-03-05", "authors_parsed": [["Xu", "Ruijia", ""], ["Chen", "Ziliang", ""], ["Zuo", "Wangmeng", ""], ["Yan", "Junjie", ""], ["Lin", "Liang", ""]]}, {"id": "1803.00839", "submitter": "Yu Rong", "authors": "Kaidi Cao and Yu Rong and Cheng Li and Xiaoou Tang and Chen Change Loy", "title": "Pose-Robust Face Recognition via Deep Residual Equivariant Mapping", "comments": "Accepted to CVPR2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face recognition achieves exceptional success thanks to the emergence of deep\nlearning. However, many contemporary face recognition models still perform\nrelatively poor in processing profile faces compared to frontal faces. A key\nreason is that the number of frontal and profile training faces are highly\nimbalanced - there are extensively more frontal training samples compared to\nprofile ones. In addition, it is intrinsically hard to learn a deep\nrepresentation that is geometrically invariant to large pose variations. In\nthis study, we hypothesize that there is an inherent mapping between frontal\nand profile faces, and consequently, their discrepancy in the deep\nrepresentation space can be bridged by an equivariant mapping. To exploit this\nmapping, we formulate a novel Deep Residual EquivAriant Mapping (DREAM) block,\nwhich is capable of adaptively adding residuals to the input deep\nrepresentation to transform a profile face representation to a canonical pose\nthat simplifies recognition. The DREAM block consistently enhances the\nperformance of profile face recognition for many strong deep networks,\nincluding ResNet models, without deliberately augmenting training data of\nprofile faces. The block is easy to use, light-weight, and can be implemented\nwith a negligible computational overhead.\n", "versions": [{"version": "v1", "created": "Fri, 2 Mar 2018 13:25:34 GMT"}], "update_date": "2018-03-05", "authors_parsed": [["Cao", "Kaidi", ""], ["Rong", "Yu", ""], ["Li", "Cheng", ""], ["Tang", "Xiaoou", ""], ["Loy", "Chen Change", ""]]}, {"id": "1803.00853", "submitter": "Przemys{\\l}aw Sadowski", "authors": "Przemys{\\l}aw Sadowski", "title": "Quantum distance-based classifier with constant size memory, distributed\n  knowledge and state recycling", "comments": "17 pages, 2 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.CV cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we examine recently proposed distance-based classification\nmethod designed for near-term quantum processing units with limited resources.\nWe further study possibilities to reduce the quantum resources without any\nefficiency decrease. We show that only a part of the information undergoes\ncoherent evolution and this fact allows us to introduce an algorithm with\nsignificantly reduced quantum memory size. Additionally, considering only\npartial information at a time, we propose a classification protocol with\ninformation distributed among a number of agents. Finally, we show that the\ninformation evolution during a measurement can lead to a better solution and\nthat accuracy of the algorithm can be improved by harnessing the state after\nthe final measurement.\n", "versions": [{"version": "v1", "created": "Fri, 2 Mar 2018 14:05:21 GMT"}], "update_date": "2018-03-05", "authors_parsed": [["Sadowski", "Przemys\u0142aw", ""]]}, {"id": "1803.00891", "submitter": "Dan Xu", "authors": "Dan Xu, Elisa Ricci, Wanli Ouyang, Xiaogang Wang, Nicu Sebe", "title": "Monocular Depth Estimation using Multi-Scale Continuous CRFs as\n  Sequential Deep Networks", "comments": "arXiv admin note: substantial text overlap with arXiv:1704.02157", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Depth cues have been proved very useful in various computer vision and\nrobotic tasks. This paper addresses the problem of monocular depth estimation\nfrom a single still image. Inspired by the effectiveness of recent works on\nmulti-scale convolutional neural networks (CNN), we propose a deep model which\nfuses complementary information derived from multiple CNN side outputs.\nDifferent from previous methods using concatenation or weighted average\nschemes, the integration is obtained by means of continuous Conditional Random\nFields (CRFs). In particular, we propose two different variations, one based on\na cascade of multiple CRFs, the other on a unified graphical model. By\ndesigning a novel CNN implementation of mean-field updates for continuous CRFs,\nwe show that both proposed models can be regarded as sequential deep networks\nand that training can be performed end-to-end. Through an extensive\nexperimental evaluation, we demonstrate the effectiveness of the proposed\napproach and establish new state of the art results for the monocular depth\nestimation task on three publicly available datasets, i.e. NYUD-V2, Make3D and\nKITTI.\n", "versions": [{"version": "v1", "created": "Thu, 1 Mar 2018 07:33:19 GMT"}], "update_date": "2018-03-05", "authors_parsed": [["Xu", "Dan", ""], ["Ricci", "Elisa", ""], ["Ouyang", "Wanli", ""], ["Wang", "Xiaogang", ""], ["Sebe", "Nicu", ""]]}, {"id": "1803.00907", "submitter": "Oggi Rudovic", "authors": "Adria Ruiz, Ognjen Rudovic, Xavier Binefa and Maja Pantic", "title": "Multi-Instance Dynamic Ordinal Random Fields for Weakly-supervised\n  Facial Behavior Analysis", "comments": "submitted TIP (June 2017). arXiv admin note: text overlap with\n  arXiv:1609.01465", "journal-ref": null, "doi": "10.1109/TIP.2018.2830189", "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a Multi-Instance-Learning (MIL) approach for weakly-supervised\nlearning problems, where a training set is formed by bags (sets of feature\nvectors or instances) and only labels at bag-level are provided. Specifically,\nwe consider the Multi-Instance Dynamic-Ordinal-Regression (MI-DOR) setting,\nwhere the instance labels are naturally represented as ordinal variables and\nbags are structured as temporal sequences. To this end, we propose\nMulti-Instance Dynamic Ordinal Random Fields (MI-DORF). In this framework, we\ntreat instance-labels as temporally-dependent latent variables in an Undirected\nGraphical Model. Different MIL assumptions are modelled via newly introduced\nhigh-order potentials relating bag and instance-labels within the energy\nfunction of the model. We also extend our framework to address the\nPartially-Observed MI-DOR problems, where a subset of instance labels are\navailable during training. We show on the tasks of weakly-supervised facial\nbehavior analysis, Facial Action Unit (DISFA dataset) and Pain (UNBC dataset)\nIntensity estimation, that the proposed framework outperforms alternative\nlearning approaches. Furthermore, we show that MIDORF can be employed to reduce\nthe data annotation efforts in this context by large-scale.\n", "versions": [{"version": "v1", "created": "Thu, 1 Mar 2018 04:13:47 GMT"}], "update_date": "2018-07-04", "authors_parsed": [["Ruiz", "Adria", ""], ["Rudovic", "Ognjen", ""], ["Binefa", "Xavier", ""], ["Pantic", "Maja", ""]]}, {"id": "1803.00940", "submitter": "Aaditya Prakash", "authors": "Aaditya Prakash, Nick Moran, Solomon Garber, Antonella DiLillo, James\n  Storer", "title": "Protecting JPEG Images Against Adversarial Attacks", "comments": "Accepted to IEEE Data Compression Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As deep neural networks (DNNs) have been integrated into critical systems,\nseveral methods to attack these systems have been developed. These adversarial\nattacks make imperceptible modifications to an image that fool DNN classifiers.\nWe present an adaptive JPEG encoder which defends against many of these\nattacks. Experimentally, we show that our method produces images with high\nvisual quality while greatly reducing the potency of state-of-the-art attacks.\nOur algorithm requires only a modest increase in encoding time, produces a\ncompressed image which can be decompressed by an off-the-shelf JPEG decoder,\nand classified by an unmodified classifier\n", "versions": [{"version": "v1", "created": "Fri, 2 Mar 2018 16:35:44 GMT"}], "update_date": "2018-03-05", "authors_parsed": [["Prakash", "Aaditya", ""], ["Moran", "Nick", ""], ["Garber", "Solomon", ""], ["DiLillo", "Antonella", ""], ["Storer", "James", ""]]}, {"id": "1803.00949", "submitter": "Mathieu Carpentier", "authors": "Mathieu Carpentier, Philippe Gigu\\`ere, Jonathan Gaudreault", "title": "Tree Species Identification from Bark Images Using Convolutional Neural\n  Networks", "comments": "2018 IEEE/RSJ International Conference on Intelligent Robots and\n  Systems (IROS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tree species identification using bark images is a challenging problem that\ncould prove useful for many forestry related tasks. However, while the recent\nprogress in deep learning showed impressive results on standard vision\nproblems, a lack of datasets prevented its use on tree bark species\nclassification. In this work, we present, and make publicly available, a novel\ndataset called BarkNet 1.0 containing more than 23,000 high-resolution bark\nimages from 23 different tree species over a wide range of tree diameters. With\nit, we demonstrate the feasibility of species recognition through bark images,\nusing deep learning. More specifically, we obtain an accuracy of 93.88% on\nsingle crop, and an accuracy of 97.81% using a majority voting approach on all\nof the images of a tree. We also empirically demonstrate that, for a fixed\nnumber of images, it is better to maximize the number of tree individuals in\nthe training database, thus directing future data collection efforts.\n", "versions": [{"version": "v1", "created": "Fri, 2 Mar 2018 17:01:17 GMT"}, {"version": "v2", "created": "Tue, 31 Jul 2018 14:13:13 GMT"}], "update_date": "2018-08-01", "authors_parsed": [["Carpentier", "Mathieu", ""], ["Gigu\u00e8re", "Philippe", ""], ["Gaudreault", "Jonathan", ""]]}, {"id": "1803.00951", "submitter": "\\'Alvaro Su\\'arez Hervella", "authors": "\\'Alvaro S. Hervella, Jos\\'e Rouco, Jorge Novo, and Marcos Ortega", "title": "Multimodal Registration of Retinal Images Using Domain-Specific\n  Landmarks and Vessel Enhancement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The analysis of different image modalities is frequently performed in\nophthalmology as it provides complementary information for the diagnosis and\nfollow-up of relevant diseases, like hypertension or diabetes. This work\npresents a hybrid method for the multimodal registration of color fundus\nretinography and fluorescein angiography. The proposed method combines a\nfeature-based approach, using domain-specific landmarks, with an\nintensity-based approach that employs a domain-adapted similarity metric. The\nmethodology is tested on a dataset of 59 image pairs containing both healthy\nand pathological cases. The results show a satisfactory performance of the\nproposed combined approach in this multimodal scenario, improving the\nregistration accuracy achieved by the feature-based and the intensity-based\napproaches.\n", "versions": [{"version": "v1", "created": "Fri, 2 Mar 2018 17:07:44 GMT"}, {"version": "v2", "created": "Mon, 2 Apr 2018 18:03:36 GMT"}], "update_date": "2018-04-04", "authors_parsed": [["Hervella", "\u00c1lvaro S.", ""], ["Rouco", "Jos\u00e9", ""], ["Novo", "Jorge", ""], ["Ortega", "Marcos", ""]]}, {"id": "1803.00974", "submitter": "Kun He", "authors": "Fatih Cakir, Kun He, Sarah Adel Bargal, Stan Sclaroff", "title": "Hashing with Mutual Information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Binary vector embeddings enable fast nearest neighbor retrieval in large\ndatabases of high-dimensional objects, and play an important role in many\npractical applications, such as image and video retrieval. We study the problem\nof learning binary vector embeddings under a supervised setting, also known as\nhashing. We propose a novel supervised hashing method based on optimizing an\ninformation-theoretic quantity: mutual information. We show that optimizing\nmutual information can reduce ambiguity in the induced neighborhood structure\nin the learned Hamming space, which is essential in obtaining high retrieval\nperformance. To this end, we optimize mutual information in deep neural\nnetworks with minibatch stochastic gradient descent, with a formulation that\nmaximally and efficiently utilizes available supervision. Experiments on four\nimage retrieval benchmarks, including ImageNet, confirm the effectiveness of\nour method in learning high-quality binary embeddings for nearest neighbor\nretrieval.\n", "versions": [{"version": "v1", "created": "Fri, 2 Mar 2018 18:12:04 GMT"}, {"version": "v2", "created": "Mon, 25 Jun 2018 03:51:33 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Cakir", "Fatih", ""], ["He", "Kun", ""], ["Bargal", "Sarah Adel", ""], ["Sclaroff", "Stan", ""]]}, {"id": "1803.01071", "submitter": "Soumyabrata Dev", "authors": "Soumyabrata Dev, Florian M. Savoy, Yee Hui Lee and Stefan Winkler", "title": "High-Dynamic-Range Imaging for Cloud Segmentation", "comments": "Published in Atmospheric Measurement Techniques (AMT), 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sky/cloud images obtained from ground-based sky-cameras are usually captured\nusing a fish-eye lens with a wide field of view. However, the sky exhibits a\nlarge dynamic range in terms of luminance, more than a conventional camera can\ncapture. It is thus difficult to capture the details of an entire scene with a\nregular camera in a single shot. In most cases, the circumsolar region is\nover-exposed, and the regions near the horizon are under-exposed. This renders\ncloud segmentation for such images difficult. In this paper, we propose\nHDRCloudSeg -- an effective method for cloud segmentation using\nHigh-Dynamic-Range (HDR) imaging based on multi-exposure fusion. We describe\nthe HDR image generation process and release a new database to the community\nfor benchmarking. Our proposed approach is the first using HDR radiance maps\nfor cloud segmentation and achieves very good results.\n", "versions": [{"version": "v1", "created": "Fri, 2 Mar 2018 23:30:06 GMT"}], "update_date": "2018-03-06", "authors_parsed": [["Dev", "Soumyabrata", ""], ["Savoy", "Florian M.", ""], ["Lee", "Yee Hui", ""], ["Winkler", "Stefan", ""]]}, {"id": "1803.01114", "submitter": "Xiaoliang Wang", "authors": "Xiaoliang Wang, Peng Cheng, Xinchuan Liu, Benedict Uzochukwu", "title": "Focal Loss Dense Detector for Vehicle Surveillance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has been widely recognized as a promising approach in different\ncomputer vision applications. Specifically, one-stage object detector and\ntwo-stage object detector are regarded as the most important two groups of\nConvolutional Neural Network based object detection methods. One-stage object\ndetector could usually outperform two-stage object detector in speed; However,\nit normally trails in detection accuracy, compared with two-stage object\ndetectors. In this study, focal loss based RetinaNet, which works as one-stage\nobject detector, is utilized to be able to well match the speed of regular\none-stage detectors and also defeat two-stage detectors in accuracy, for\nvehicle detection. State-of-the-art performance result has been showed on the\nDETRAC vehicle dataset.\n", "versions": [{"version": "v1", "created": "Sat, 3 Mar 2018 06:18:27 GMT"}], "update_date": "2018-03-06", "authors_parsed": [["Wang", "Xiaoliang", ""], ["Cheng", "Peng", ""], ["Liu", "Xinchuan", ""], ["Uzochukwu", "Benedict", ""]]}, {"id": "1803.01129", "submitter": "Matthias M\\\"uller", "authors": "Guohao Li, Matthias M\\\"uller, Vincent Casser, Neil Smith, Dominik L.\n  Michels, Bernard Ghanem", "title": "OIL: Observational Imitation Learning", "comments": "Accepted at RSS'19. First two authors contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work has explored the problem of autonomous navigation by imitating a\nteacher and learning an end-to-end policy, which directly predicts controls\nfrom raw images. However, these approaches tend to be sensitive to mistakes by\nthe teacher and do not scale well to other environments or vehicles. To this\nend, we propose Observational Imitation Learning (OIL), a novel imitation\nlearning variant that supports online training and automatic selection of\noptimal behavior by observing multiple imperfect teachers. We apply our\nproposed methodology to the challenging problems of autonomous driving and UAV\nracing. For both tasks, we utilize the Sim4CV simulator that enables the\ngeneration of large amounts of synthetic training data and also allows for\nonline learning and evaluation. We train a perception network to predict\nwaypoints from raw image data and use OIL to train another network to predict\ncontrols from these waypoints. Extensive experiments demonstrate that our\ntrained network outperforms its teachers, conventional imitation learning (IL)\nand reinforcement learning (RL) baselines and even humans in simulation. The\nproject website is available at https://sites.google.com/kaust.edu.sa/oil/ and\na video at https://youtu.be/_rhq8a0qgeg\n", "versions": [{"version": "v1", "created": "Sat, 3 Mar 2018 09:17:42 GMT"}, {"version": "v2", "created": "Tue, 27 Nov 2018 12:55:18 GMT"}, {"version": "v3", "created": "Thu, 23 May 2019 11:24:12 GMT"}], "update_date": "2019-05-24", "authors_parsed": [["Li", "Guohao", ""], ["M\u00fcller", "Matthias", ""], ["Casser", "Vincent", ""], ["Smith", "Neil", ""], ["Michels", "Dominik L.", ""], ["Ghanem", "Bernard", ""]]}, {"id": "1803.01159", "submitter": "Guodong Du", "authors": "Guodong Du, Liang Yuan, Kong Joo Shin, Shunsuke Managi", "title": "Enhancement of land-use change modeling using convolutional neural\n  networks and convolutional denoising autoencoders", "comments": "26 pages, 8 tables, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The neighborhood effect is a key driving factor for the land-use change (LUC)\nprocess. This study applies convolutional neural networks (CNN) to capture\nneighborhood characteristics from satellite images and to enhance the\nperformance of LUC modeling. We develop a hybrid CNN model (conv-net) to\npredict the LU transition probability by combining satellite images and\ngeographical features. A spatial weight layer is designed to incorporate the\ndistance-decay characteristics of neighborhood effect into conv-net. As an\nalternative model, we also develop a hybrid convolutional denoising autoencoder\nand multi-layer perceptron model (CDAE-net), which specifically learns latent\nrepresentations from satellite images and denoises the image data. Finally, a\nDINAMICA-based cellular automata (CA) model simulates the LU pattern. The\nresults show that the convolutional-based models improve the modeling\nperformances compared with a model that accepts only the geographical features.\nOverall, conv-net outperforms CDAE-net in terms of LUC predictive performance.\nNonetheless, CDAE-net performs better when the data are noisy.\n", "versions": [{"version": "v1", "created": "Sat, 3 Mar 2018 13:28:51 GMT"}], "update_date": "2018-03-06", "authors_parsed": [["Du", "Guodong", ""], ["Yuan", "Liang", ""], ["Shin", "Kong Joo", ""], ["Managi", "Shunsuke", ""]]}, {"id": "1803.01160", "submitter": "Radu Tudor Ionescu", "authors": "Sorina Smeureanu, Radu Tudor Ionescu", "title": "Real-Time Deep Learning Method for Abandoned Luggage Detection in Video", "comments": "Accepted at EUSIPCO 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent terrorist attacks in major cities around the world have brought many\ncasualties among innocent citizens. One potential threat is represented by\nabandoned luggage items (that could contain bombs or biological warfare) in\npublic areas. In this paper, we describe an approach for real-time automatic\ndetection of abandoned luggage in video captured by surveillance cameras. The\napproach is comprised of two stages: (i) static object detection based on\nbackground subtraction and motion estimation and (ii) abandoned luggage\nrecognition based on a cascade of convolutional neural networks (CNN). To train\nour neural networks we provide two types of examples: images collected from the\nInternet and realistic examples generated by imposing various suitcases and\nbags over the scene's background. We present empirical results demonstrating\nthat our approach yields better performance than a strong CNN baseline method.\n", "versions": [{"version": "v1", "created": "Sat, 3 Mar 2018 13:30:06 GMT"}, {"version": "v2", "created": "Tue, 22 May 2018 16:57:43 GMT"}, {"version": "v3", "created": "Fri, 15 Jun 2018 15:01:43 GMT"}], "update_date": "2018-06-18", "authors_parsed": [["Smeureanu", "Sorina", ""], ["Ionescu", "Radu Tudor", ""]]}, {"id": "1803.01164", "submitter": "Md Zahangir Alom", "authors": "Md Zahangir Alom, Tarek M. Taha, Christopher Yakopcic, Stefan\n  Westberg, Paheding Sidike, Mst Shamima Nasrin, Brian C Van Esesn, Abdul A S.\n  Awwal, and Vijayan K. Asari", "title": "The History Began from AlexNet: A Comprehensive Survey on Deep Learning\n  Approaches", "comments": "39 pages, 46 figures, 3 tables. arXiv admin note: text overlap with\n  arXiv:1408.3264, arXiv:1411.4046", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep learning has demonstrated tremendous success in variety of application\ndomains in the past few years. This new field of machine learning has been\ngrowing rapidly and applied in most of the application domains with some new\nmodalities of applications, which helps to open new opportunity. There are\ndifferent methods have been proposed on different category of learning\napproaches, which includes supervised, semi-supervised and un-supervised\nlearning. The experimental results show state-of-the-art performance of deep\nlearning over traditional machine learning approaches in the field of Image\nProcessing, Computer Vision, Speech Recognition, Machine Translation, Art,\nMedical imaging, Medical information processing, Robotics and control,\nBio-informatics, Natural Language Processing (NLP), Cyber security, and many\nmore. This report presents a brief survey on development of DL approaches,\nincluding Deep Neural Network (DNN), Convolutional Neural Network (CNN),\nRecurrent Neural Network (RNN) including Long Short Term Memory (LSTM) and\nGated Recurrent Units (GRU), Auto-Encoder (AE), Deep Belief Network (DBN),\nGenerative Adversarial Network (GAN), and Deep Reinforcement Learning (DRL). In\naddition, we have included recent development of proposed advanced variant DL\ntechniques based on the mentioned DL approaches. Furthermore, DL approaches\nhave explored and evaluated in different application domains are also included\nin this survey. We have also comprised recently developed frameworks, SDKs, and\nbenchmark datasets that are used for implementing and evaluating deep learning\napproaches. There are some surveys have published on Deep Learning in Neural\nNetworks [1, 38] and a survey on RL [234]. However, those papers have not\ndiscussed the individual advanced techniques for training large scale deep\nlearning models and the recently developed method of generative models [1].\n", "versions": [{"version": "v1", "created": "Sat, 3 Mar 2018 13:46:40 GMT"}, {"version": "v2", "created": "Wed, 12 Sep 2018 15:57:23 GMT"}], "update_date": "2018-09-13", "authors_parsed": [["Alom", "Md Zahangir", ""], ["Taha", "Tarek M.", ""], ["Yakopcic", "Christopher", ""], ["Westberg", "Stefan", ""], ["Sidike", "Paheding", ""], ["Nasrin", "Mst Shamima", ""], ["Van Esesn", "Brian C", ""], ["Awwal", "Abdul A S.", ""], ["Asari", "Vijayan K.", ""]]}, {"id": "1803.01199", "submitter": "Yuri G. Gordienko", "authors": "Sergii Stirenko, Yuriy Kochura, Oleg Alienin, Oleksandr Rokovyi, Peng\n  Gang, Wei Zeng, and Yuri Gordienko", "title": "Chest X-Ray Analysis of Tuberculosis by Deep Learning with Segmentation\n  and Augmentation", "comments": "6 pages, 11 figures, 1 table", "journal-ref": "2018 IEEE 38th International Conference on Electronics and\n  Nanotechnology (ELNANO), Kiev, 2018, pp. 422-428", "doi": "10.1109/ELNANO.2018.8477564", "report-no": null, "categories": "cs.LG cs.CV cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The results of chest X-ray (CXR) analysis of 2D images to get the\nstatistically reliable predictions (availability of tuberculosis) by\ncomputer-aided diagnosis (CADx) on the basis of deep learning are presented.\nThey demonstrate the efficiency of lung segmentation, lossless and lossy data\naugmentation for CADx of tuberculosis by deep convolutional neural network\n(CNN) applied to the small and not well-balanced dataset even. CNN demonstrates\nability to train (despite overfitting) on the pre-processed dataset obtained\nafter lung segmentation in contrast to the original not-segmented dataset.\nLossless data augmentation of the segmented dataset leads to the lowest\nvalidation loss (without overfitting) and nearly the same accuracy (within the\nlimits of standard deviation) in comparison to the original and other\npre-processed datasets after lossy data augmentation. The additional limited\nlossy data augmentation results in the lower validation loss, but with a\ndecrease of the validation accuracy. In conclusion, besides the more complex\ndeep CNNs and bigger datasets, the better progress of CADx for the small and\nnot well-balanced datasets even could be obtained by better segmentation, data\naugmentation, dataset stratification, and exclusion of non-evident outliers.\n", "versions": [{"version": "v1", "created": "Sat, 3 Mar 2018 16:42:19 GMT"}], "update_date": "2018-11-19", "authors_parsed": [["Stirenko", "Sergii", ""], ["Kochura", "Yuriy", ""], ["Alienin", "Oleg", ""], ["Rokovyi", "Oleksandr", ""], ["Gang", "Peng", ""], ["Zeng", "Wei", ""], ["Gordienko", "Yuri", ""]]}, {"id": "1803.01207", "submitter": "Alexey Shvets", "authors": "Alexey Shvets, Alexander Rakhlin, Alexandr A. Kalinin, and Vladimir\n  Iglovikov", "title": "Automatic Instrument Segmentation in Robot-Assisted Surgery Using Deep\n  Learning", "comments": "9 pages, 3 figures. arXiv admin note: substantial text overlap with\n  arXiv:1804.08024", "journal-ref": "2018 17th IEEE International Conference on Machine Learning and\n  Applications (ICMLA)", "doi": "10.1109/ICMLA.2018.00100", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Semantic segmentation of robotic instruments is an important problem for the\nrobot-assisted surgery. One of the main challenges is to correctly detect an\ninstrument's position for the tracking and pose estimation in the vicinity of\nsurgical scenes. Accurate pixel-wise instrument segmentation is needed to\naddress this challenge. In this paper we describe our winning solution for\nMICCAI 2017 Endoscopic Vision SubChallenge: Robotic Instrument Segmentation.\nOur approach demonstrates an improvement over the state-of-the-art results\nusing several novel deep neural network architectures. It addressed the binary\nsegmentation problem, where every pixel in an image is labeled as an instrument\nor background from the surgery video feed. In addition, we solve a multi-class\nsegmentation problem, where we distinguish different instruments or different\nparts of an instrument from the background. In this setting, our approach\noutperforms other methods in every task subcategory for automatic instrument\nsegmentation thereby providing state-of-the-art solution for this problem. The\nsource code for our solution is made publicly available at\nhttps://github.com/ternaus/robot-surgery-segmentation\n", "versions": [{"version": "v1", "created": "Sat, 3 Mar 2018 17:41:55 GMT"}, {"version": "v2", "created": "Tue, 19 Jun 2018 19:50:47 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Shvets", "Alexey", ""], ["Rakhlin", "Alexander", ""], ["Kalinin", "Alexandr A.", ""], ["Iglovikov", "Vladimir", ""]]}, {"id": "1803.01216", "submitter": "Matthias Rottmann", "authors": "Matthias Rottmann, Karsten Kahl and Hanno Gottschalk", "title": "Deep Bayesian Active Semi-Supervised Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many applications the process of generating label information is expensive\nand time consuming. We present a new method that combines active and\nsemi-supervised deep learning to achieve high generalization performance from a\ndeep convolutional neural network with as few known labels as possible. In a\nsetting where a small amount of labeled data as well as a large amount of\nunlabeled data is available, our method first learns the labeled data set. This\ninitialization is followed by an expectation maximization algorithm, where\nfurther training reduces classification entropy on the unlabeled data by\ntargeting a low entropy fit which is consistent with the labeled data. In\naddition the algorithm asks at a specified frequency an oracle for labels of\ndata with entropy above a certain entropy quantile. Using this active learning\ncomponent we obtain an agile labeling process that achieves high accuracy, but\nrequires only a small amount of known labels. For the MNIST dataset we report\nan error rate of 2.06% using only 300 labels and 1.06% for 1000 labels. These\nresults are obtained without employing any special network architecture or data\naugmentation.\n", "versions": [{"version": "v1", "created": "Sat, 3 Mar 2018 19:13:40 GMT"}], "update_date": "2018-03-06", "authors_parsed": [["Rottmann", "Matthias", ""], ["Kahl", "Karsten", ""], ["Gottschalk", "Hanno", ""]]}, {"id": "1803.01229", "submitter": "Maayan Frid-Adar", "authors": "Maayan Frid-Adar, Idit Diamant, Eyal Klang, Michal Amitai, Jacob\n  Goldberger, Hayit Greenspan", "title": "GAN-based Synthetic Medical Image Augmentation for increased CNN\n  Performance in Liver Lesion Classification", "comments": "Preprint submitted to Neurocomputing", "journal-ref": null, "doi": "10.1016/j.neucom.2018.09.013", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning methods, and in particular convolutional neural networks\n(CNNs), have led to an enormous breakthrough in a wide range of computer vision\ntasks, primarily by using large-scale annotated datasets. However, obtaining\nsuch datasets in the medical domain remains a challenge. In this paper, we\npresent methods for generating synthetic medical images using recently\npresented deep learning Generative Adversarial Networks (GANs). Furthermore, we\nshow that generated medical images can be used for synthetic data augmentation,\nand improve the performance of CNN for medical image classification. Our novel\nmethod is demonstrated on a limited dataset of computed tomography (CT) images\nof 182 liver lesions (53 cysts, 64 metastases and 65 hemangiomas). We first\nexploit GAN architectures for synthesizing high quality liver lesion ROIs. Then\nwe present a novel scheme for liver lesion classification using CNN. Finally,\nwe train the CNN using classic data augmentation and our synthetic data\naugmentation and compare performance. In addition, we explore the quality of\nour synthesized examples using visualization and expert assessment. The\nclassification performance using only classic data augmentation yielded 78.6%\nsensitivity and 88.4% specificity. By adding the synthetic data augmentation\nthe results increased to 85.7% sensitivity and 92.4% specificity. We believe\nthat this approach to synthetic data augmentation can generalize to other\nmedical classification applications and thus support radiologists' efforts to\nimprove diagnosis.\n", "versions": [{"version": "v1", "created": "Sat, 3 Mar 2018 20:20:38 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Frid-Adar", "Maayan", ""], ["Diamant", "Idit", ""], ["Klang", "Eyal", ""], ["Amitai", "Michal", ""], ["Goldberger", "Jacob", ""], ["Greenspan", "Hayit", ""]]}, {"id": "1803.01250", "submitter": "Evair Severo", "authors": "Evair Severo, Rayson Laroca, Cides S. Bezerra, Luiz A. Zanlorensi,\n  Daniel Weingaertner, Gladston Moreira and David Menotti", "title": "A Benchmark for Iris Location and a Deep Learning Detector Evaluation", "comments": "Accepted for presentation at the International Joint Conference on\n  Neural Networks (IJCNN) 2018", "journal-ref": null, "doi": "10.1109/IJCNN.2018.8489638", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The iris is considered as the biometric trait with the highest unique\nprobability. The iris location is an important task for biometrics systems,\naffecting directly the results obtained in specific applications such as iris\nrecognition, spoofing and contact lenses detection, among others. This work\ndefines the iris location problem as the delimitation of the smallest squared\nwindow that encompasses the iris region. In order to build a benchmark for iris\nlocation we annotate (iris squared bounding boxes) four databases from\ndifferent biometric applications and make them publicly available to the\ncommunity. Besides these 4 annotated databases, we include 2 others from the\nliterature. We perform experiments on these six databases, five obtained with\nnear infra-red sensors and one with visible light sensor. We compare the\nclassical and outstanding Daugman iris location approach with two window based\ndetectors: 1) a sliding window detector based on features from Histogram of\nOriented Gradients (HOG) and a linear Support Vector Machines (SVM) classifier;\n2) a deep learning based detector fine-tuned from YOLO object detector.\nExperimental results showed that the deep learning based detector outperforms\nthe other ones in terms of accuracy and runtime (GPUs version) and should be\nchosen whenever possible.\n", "versions": [{"version": "v1", "created": "Sat, 3 Mar 2018 22:08:30 GMT"}, {"version": "v2", "created": "Thu, 15 Mar 2018 16:58:26 GMT"}, {"version": "v3", "created": "Fri, 13 Apr 2018 02:57:24 GMT"}, {"version": "v4", "created": "Mon, 16 Apr 2018 05:15:50 GMT"}, {"version": "v5", "created": "Mon, 30 Apr 2018 05:02:39 GMT"}], "update_date": "2018-10-17", "authors_parsed": [["Severo", "Evair", ""], ["Laroca", "Rayson", ""], ["Bezerra", "Cides S.", ""], ["Zanlorensi", "Luiz A.", ""], ["Weingaertner", "Daniel", ""], ["Moreira", "Gladston", ""], ["Menotti", "David", ""]]}, {"id": "1803.01260", "submitter": "Samyak Datta", "authors": "Samyak Datta, Gaurav Sharma, C.V. Jawahar", "title": "Unsupervised Learning of Face Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach for unsupervised training of CNNs in order to learn\ndiscriminative face representations. We mine supervised training data by noting\nthat multiple faces in the same video frame must belong to different persons\nand the same face tracked across multiple frames must belong to the same\nperson. We obtain millions of face pairs from hundreds of videos without using\nany manual supervision. Although faces extracted from videos have a lower\nspatial resolution than those which are available as part of standard\nsupervised face datasets such as LFW and CASIA-WebFace, the former represent a\nmuch more realistic setting, e.g. in surveillance scenarios where most of the\nfaces detected are very small. We train our CNNs with the relatively low\nresolution faces extracted from video frames collected, and achieve a higher\nverification accuracy on the benchmark LFW dataset cf. hand-crafted features\nsuch as LBPs, and even surpasses the performance of state-of-the-art deep\nnetworks such as VGG-Face, when they are made to work with low resolution input\nimages.\n", "versions": [{"version": "v1", "created": "Sat, 3 Mar 2018 23:20:52 GMT"}], "update_date": "2018-03-06", "authors_parsed": [["Datta", "Samyak", ""], ["Sharma", "Gaurav", ""], ["Jawahar", "C. V.", ""]]}, {"id": "1803.01314", "submitter": "Se Young Chun", "authors": "Shakarim Soltanayev, Se Young Chun", "title": "Training Deep Learning Based Denoisers without Ground Truth Data", "comments": "12 pages, 10 figures, 7 tables, NeurIPS 2018, this is an extended\n  version of it", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently developed deep-learning-based denoisers often outperform\nstate-of-the-art conventional denoisers such as the BM3D. They are typically\ntrained to minimize the mean squared error (MSE) between the output image of a\ndeep neural network (DNN) and a ground truth image. Thus, it is important for\ndeep-learning-based denoisers to use high quality noiseless ground truth data\nfor high performance. However, it is often challenging or even infeasible to\nobtain noiseless images in some applications. Here, we propose a method based\non Stein's unbiased risk estimator (SURE) for training DNN denoisers based only\non the use of noisy images in the training data with Gaussian noise. We\ndemonstrate that our SURE-based method, without the use of ground truth data,\nis able to train DNN denoisers to yield performances close to those networks\ntrained with ground truth for both grayscale and color images. We also propose\na SURE-based refining method with a noisy test image for further performance\nimprovement. Our quick refining method outperformed conventional BM3D, deep\nimage prior, and often the networks trained with ground truth. Potential\nextension of our SURE-based methods to Poisson noise model was also\ninvestigated.\n", "versions": [{"version": "v1", "created": "Sun, 4 Mar 2018 07:55:40 GMT"}, {"version": "v2", "created": "Wed, 23 May 2018 08:23:32 GMT"}, {"version": "v3", "created": "Mon, 25 Mar 2019 06:57:36 GMT"}, {"version": "v4", "created": "Thu, 22 Apr 2021 02:48:50 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Soltanayev", "Shakarim", ""], ["Chun", "Se Young", ""]]}, {"id": "1803.01356", "submitter": "Se Young Chun", "authors": "Dongwon Park, Se Young Chun", "title": "Classification based Grasp Detection using Spatial Transformer Network", "comments": "6 pages, 10 figures, Under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robotic grasp detection task is still challenging, particularly for novel\nobjects. With the recent advance of deep learning, there have been several\nworks on detecting robotic grasp using neural networks. Typically, regression\nbased grasp detection methods have outperformed classification based detection\nmethods in computation complexity with excellent accuracy. However,\nclassification based robotic grasp detection still seems to have merits such as\nintermediate step observability and straightforward back propagation routine\nfor end-to-end training. In this work, we propose a novel classification based\nrobotic grasp detection method with multiple-stage spatial transformer networks\n(STN). Our proposed method was able to achieve state-of-the-art performance in\naccuracy with real- time computation. Additionally, unlike other regression\nbased grasp detection methods, our proposed method allows partial observation\nfor intermediate results such as grasp location and orientation for a number of\ngrasp configuration candidates.\n", "versions": [{"version": "v1", "created": "Sun, 4 Mar 2018 14:02:47 GMT"}], "update_date": "2018-03-06", "authors_parsed": [["Park", "Dongwon", ""], ["Chun", "Se Young", ""]]}, {"id": "1803.01413", "submitter": "Gedas Bertasius", "authors": "Gedas Bertasius, Aaron Chan, Jianbo Shi", "title": "Egocentric Basketball Motion Planning from a Single First-Person Image", "comments": "CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a model that uses a single first-person image to generate an\negocentric basketball motion sequence in the form of a 12D camera configuration\ntrajectory, which encodes a player's 3D location and 3D head orientation\nthroughout the sequence. To do this, we first introduce a future convolutional\nneural network (CNN) that predicts an initial sequence of 12D camera\nconfigurations, aiming to capture how real players move during a one-on-one\nbasketball game. We also introduce a goal verifier network, which is trained to\nverify that a given camera configuration is consistent with the final goals of\nreal one-on-one basketball players. Next, we propose an inverse synthesis\nprocedure to synthesize a refined sequence of 12D camera configurations that\n(1) sufficiently matches the initial configurations predicted by the future\nCNN, while (2) maximizing the output of the goal verifier network. Finally, by\nfollowing the trajectory resulting from the refined camera configuration\nsequence, we obtain the complete 12D motion sequence.\n  Our model generates realistic basketball motion sequences that capture the\ngoals of real players, outperforming standard deep learning approaches such as\nrecurrent neural networks (RNNs), long short-term memory networks (LSTMs), and\ngenerative adversarial networks (GANs).\n", "versions": [{"version": "v1", "created": "Sun, 4 Mar 2018 20:12:58 GMT"}], "update_date": "2018-03-06", "authors_parsed": [["Bertasius", "Gedas", ""], ["Chan", "Aaron", ""], ["Shi", "Jianbo", ""]]}, {"id": "1803.01417", "submitter": "Yuhua Chen", "authors": "Yuhua Chen, Feng Shi, Anthony G. Christodoulou, Zhengwei Zhou, Yibin\n  Xie, Debiao Li", "title": "Efficient and Accurate MRI Super-Resolution using a Generative\n  Adversarial Network and 3D Multi-Level Densely Connected Network", "comments": "10 pages, 2 figures, 2 tables. MICCAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-resolution (HR) magnetic resonance images (MRI) provide detailed\nanatomical information important for clinical application and quantitative\nimage analysis. However, HR MRI conventionally comes at the cost of longer scan\ntime, smaller spatial coverage, and lower signal-to-noise ratio (SNR). Recent\nstudies have shown that single image super-resolution (SISR), a technique to\nrecover HR details from one single low-resolution (LR) input image, could\nprovide high-quality image details with the help of advanced deep convolutional\nneural networks (CNN). However, deep neural networks consume memory heavily and\nrun slowly, especially in 3D settings. In this paper, we propose a novel 3D\nneural network design, namely a multi-level densely connected super-resolution\nnetwork (mDCSRN) with generative adversarial network (GAN)-guided training. The\nmDCSRN quickly trains and inferences and the GAN promotes realistic output\nhardly distinguishable from original HR images. Our results from experiments on\na dataset with 1,113 subjects show that our new architecture beats other\npopular deep learning methods in recovering 4x resolution-downgraded im-ages\nand runs 6x faster.\n", "versions": [{"version": "v1", "created": "Sun, 4 Mar 2018 20:45:06 GMT"}, {"version": "v2", "created": "Sat, 17 Mar 2018 18:42:41 GMT"}, {"version": "v3", "created": "Sat, 9 Jun 2018 06:14:37 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Chen", "Yuhua", ""], ["Shi", "Feng", ""], ["Christodoulou", "Anthony G.", ""], ["Zhou", "Zhengwei", ""], ["Xie", "Yibin", ""], ["Li", "Debiao", ""]]}, {"id": "1803.01449", "submitter": "Sohil Shah", "authors": "Sohil Atul Shah and Vladlen Koltun", "title": "Deep Continuous Clustering", "comments": "The code is available at http://github.com/shahsohil/DCC", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering high-dimensional datasets is hard because interpoint distances\nbecome less informative in high-dimensional spaces. We present a clustering\nalgorithm that performs nonlinear dimensionality reduction and clustering\njointly. The data is embedded into a lower-dimensional space by a deep\nautoencoder. The autoencoder is optimized as part of the clustering process.\nThe resulting network produces clustered data. The presented approach does not\nrely on prior knowledge of the number of ground-truth clusters. Joint nonlinear\ndimensionality reduction and clustering are formulated as optimization of a\nglobal continuous objective. We thus avoid discrete reconfigurations of the\nobjective that characterize prior clustering algorithms. Experiments on\ndatasets from multiple domains demonstrate that the presented algorithm\noutperforms state-of-the-art clustering schemes, including recent methods that\nuse deep networks.\n", "versions": [{"version": "v1", "created": "Mon, 5 Mar 2018 01:15:38 GMT"}], "update_date": "2018-03-06", "authors_parsed": [["Shah", "Sohil Atul", ""], ["Koltun", "Vladlen", ""]]}, {"id": "1803.01457", "submitter": "Yangyu Chen", "authors": "Yangyu Chen, Shuhui Wang, Weigang Zhang and Qingming Huang", "title": "Less Is More: Picking Informative Frames for Video Captioning", "comments": "10 pages, 7 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In video captioning task, the best practice has been achieved by\nattention-based models which associate salient visual components with sentences\nin the video. However, existing study follows a common procedure which includes\na frame-level appearance modeling and motion modeling on equal interval frame\nsampling, which may bring about redundant visual information, sensitivity to\ncontent noise and unnecessary computation cost.\n  We propose a plug-and-play PickNet to perform informative frame picking in\nvideo captioning. Based on a standard Encoder-Decoder framework, we develop a\nreinforcement-learning-based procedure to train the network sequentially, where\nthe reward of each frame picking action is designed by maximizing visual\ndiversity and minimizing textual discrepancy. If the candidate is rewarded, it\nwill be selected and the corresponding latent representation of Encoder-Decoder\nwill be updated for future trials. This procedure goes on until the end of the\nvideo sequence. Consequently, a compact frame subset can be selected to\nrepresent the visual information and perform video captioning without\nperformance degradation. Experiment results shows that our model can use 6-8\nframes to achieve competitive performance across popular benchmarks.\n", "versions": [{"version": "v1", "created": "Mon, 5 Mar 2018 01:57:49 GMT"}], "update_date": "2018-03-06", "authors_parsed": [["Chen", "Yangyu", ""], ["Wang", "Shuhui", ""], ["Zhang", "Weigang", ""], ["Huang", "Qingming", ""]]}, {"id": "1803.01485", "submitter": "Amir Rosenfeld", "authors": "Amir Rosenfeld, Markus D. Solbach, John K. Tsotsos", "title": "Totally Looks Like - How Humans Compare, Compared to Machines", "comments": "ACCV 2018. Project website:\n  https://sites.google.com/view/totally-looks-like-dataset", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Perceptual judgment of image similarity by humans relies on rich internal\nrepresentations ranging from low-level features to high-level concepts, scene\nproperties and even cultural associations. However, existing methods and\ndatasets attempting to explain perceived similarity use stimuli which arguably\ndo not cover the full breadth of factors that affect human similarity\njudgments, even those geared toward this goal. We introduce a new dataset\ndubbed Totally-Looks-Like (TLL) after a popular entertainment website, which\ncontains images paired by humans as being visually similar. The dataset\ncontains 6016 image-pairs from the wild, shedding light upon a rich and diverse\nset of criteria employed by human beings. We conduct experiments to try to\nreproduce the pairings via features extracted from state-of-the-art deep\nconvolutional neural networks, as well as additional human experiments to\nverify the consistency of the collected data. Though we create conditions to\nartificially make the matching task increasingly easier, we show that\nmachine-extracted representations perform very poorly in terms of reproducing\nthe matching selected by humans. We discuss and analyze these results,\nsuggesting future directions for improvement of learned image representations.\n", "versions": [{"version": "v1", "created": "Mon, 5 Mar 2018 03:43:20 GMT"}, {"version": "v2", "created": "Thu, 8 Mar 2018 20:07:19 GMT"}, {"version": "v3", "created": "Thu, 18 Oct 2018 18:31:00 GMT"}], "update_date": "2018-10-22", "authors_parsed": [["Rosenfeld", "Amir", ""], ["Solbach", "Markus D.", ""], ["Tsotsos", "John K.", ""]]}, {"id": "1803.01504", "submitter": "Dan Xu", "authors": "Dan Xu, Xavier Alameda-Pineda, Jingkuan Song, Elisa Ricci, Nicu Sebe", "title": "Cross-Paced Representation Learning with Partial Curricula for\n  Sketch-based Image Retrieval", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2018.2837381", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we address the problem of learning robust cross-domain\nrepresentations for sketch-based image retrieval (SBIR). While most SBIR\napproaches focus on extracting low- and mid-level descriptors for direct\nfeature matching, recent works have shown the benefit of learning coupled\nfeature representations to describe data from two related sources. However,\ncross-domain representation learning methods are typically cast into non-convex\nminimization problems that are difficult to optimize, leading to unsatisfactory\nperformance. Inspired by self-paced learning, a learning methodology designed\nto overcome convergence issues related to local optima by exploiting the\nsamples in a meaningful order (i.e. easy to hard), we introduce the cross-paced\npartial curriculum learning (CPPCL) framework. Compared with existing\nself-paced learning methods which only consider a single modality and cannot\ndeal with prior knowledge, CPPCL is specifically designed to assess the\nlearning pace by jointly handling data from dual sources and modality-specific\nprior information provided in the form of partial curricula. Additionally,\nthanks to the learned dictionaries, we demonstrate that the proposed CPPCL\nembeds robust coupled representations for SBIR. Our approach is extensively\nevaluated on four publicly available datasets (i.e. CUFS, Flickr15K, QueenMary\nSBIR and TU-Berlin Extension datasets), showing superior performance over\ncompeting SBIR methods.\n", "versions": [{"version": "v1", "created": "Mon, 5 Mar 2018 05:30:08 GMT"}], "update_date": "2018-08-01", "authors_parsed": [["Xu", "Dan", ""], ["Alameda-Pineda", "Xavier", ""], ["Song", "Jingkuan", ""], ["Ricci", "Elisa", ""], ["Sebe", "Nicu", ""]]}, {"id": "1803.01516", "submitter": "Kiyoshi Oguri", "authors": "Kiyoshi Oguri and Yuichiro Shibata", "title": "A new stereo formulation not using pixel and disparity models", "comments": "7 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce a new stereo formulation which does not use pixel and disparity\nmodels. Many problems in vision are treated as assigning each pixel a label.\nDisparities are labels for stereo. Such pixel-labeling problems are naturally\nrepresented in terms of energy minimization, where the energy function has two\nterms: one term penalizes solutions that inconsistent with the observed data,\nthe other term enforces spatial smoothness. Graph cuts are one of the effi-\ncient methods for solving energy minimization. However, exact minimization of\nmulti labeling problems can be performed by graph cuts only for the case with\nconvex smoothness terms. In pixel-disparity formulation, convex smoothness\nterms do not generate well reconstructed 3D results. Thus, truncated linear or\nquadratic smoothness terms, etc. are used, where approximate energy\nminimization is necessary. In this paper, we introduce a new site-labeling\nformulation, where the sites are not pixels but lines in 3D space, labels are\nnot disparities but depth numbers. For this formulation, visibility reasoning\nis naturally included in the energy function. In addition, this formulation\nallows us to use a small smoothness term, which does not affect the 3D results\nmuch. This makes the optimization step very simple, so we could develop an\napproximation method for graph cut itself (not for energy minimization) and a\nhigh performance GPU graph cut program. For Tsukuba stereo pair in Middlebury\ndata set, we got the result in 5ms using GTX1080GPU, 19ms using GTX660GPU.\n", "versions": [{"version": "v1", "created": "Mon, 5 Mar 2018 06:50:23 GMT"}], "update_date": "2018-03-06", "authors_parsed": [["Oguri", "Kiyoshi", ""], ["Shibata", "Yuichiro", ""]]}, {"id": "1803.01529", "submitter": "Hao Chen", "authors": "Hao Chen, Yali Wang, Guoyou Wang, Yu Qiao", "title": "LSTD: A Low-Shot Transfer Detector for Object Detection", "comments": "Accepted by AAAI2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in object detection are mainly driven by deep learning with\nlarge-scale detection benchmarks. However, the fully-annotated training set is\noften limited for a target detection task, which may deteriorate the\nperformance of deep detectors. To address this challenge, we propose a novel\nlow-shot transfer detector (LSTD) in this paper, where we leverage rich\nsource-domain knowledge to construct an effective target-domain detector with\nvery few training examples. The main contributions are described as follows.\nFirst, we design a flexible deep architecture of LSTD to alleviate transfer\ndifficulties in low-shot detection. This architecture can integrate the\nadvantages of both SSD and Faster RCNN in a unified deep framework. Second, we\nintroduce a novel regularized transfer learning framework for low-shot\ndetection, where the transfer knowledge (TK) and background depression (BD)\nregularizations are proposed to leverage object knowledge respectively from\nsource and target domains, in order to further enhance fine-tuning with a few\ntarget images. Finally, we examine our LSTD on a number of challenging low-shot\ndetection experiments, where LSTD outperforms other state-of-the-art\napproaches. The results demonstrate that LSTD is a preferable deep detector for\nlow-shot scenarios.\n", "versions": [{"version": "v1", "created": "Mon, 5 Mar 2018 07:30:58 GMT"}], "update_date": "2018-03-06", "authors_parsed": [["Chen", "Hao", ""], ["Wang", "Yali", ""], ["Wang", "Guoyou", ""], ["Qiao", "Yu", ""]]}, {"id": "1803.01532", "submitter": "Chang Liu", "authors": "Chang Liu, Xiaolin Wu, Xiao Shu", "title": "Learning-Based Dequantization For Image Restoration Against Extremely\n  Poor Illumination", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  All existing image enhancement methods, such as HDR tone mapping, cannot\nrecover A/D quantization losses due to insufficient or excessive lighting,\n(underflow and overflow problems). The loss of image details due to A/D\nquantization is complete and it cannot be recovered by traditional image\nprocessing methods, but the modern data-driven machine learning approach offers\na much needed cure to the problem. In this work we propose a novel approach to\nrestore and enhance images acquired in low and uneven lighting. First, the ill\nillumination is algorithmically compensated by emulating the effects of\nartificial supplementary lighting. Then a DCNN trained using only synthetic\ndata recovers the missing detail caused by quantization.\n", "versions": [{"version": "v1", "created": "Mon, 5 Mar 2018 07:39:29 GMT"}, {"version": "v2", "created": "Tue, 20 Mar 2018 18:29:28 GMT"}], "update_date": "2018-03-22", "authors_parsed": [["Liu", "Chang", ""], ["Wu", "Xiaolin", ""], ["Shu", "Xiao", ""]]}, {"id": "1803.01534", "submitter": "Shu Liu", "authors": "Shu Liu, Lu Qi, Haifang Qin, Jianping Shi, Jiaya Jia", "title": "Path Aggregation Network for Instance Segmentation", "comments": "Accepted to CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The way that information propagates in neural networks is of great\nimportance. In this paper, we propose Path Aggregation Network (PANet) aiming\nat boosting information flow in proposal-based instance segmentation framework.\nSpecifically, we enhance the entire feature hierarchy with accurate\nlocalization signals in lower layers by bottom-up path augmentation, which\nshortens the information path between lower layers and topmost feature. We\npresent adaptive feature pooling, which links feature grid and all feature\nlevels to make useful information in each feature level propagate directly to\nfollowing proposal subnetworks. A complementary branch capturing different\nviews for each proposal is created to further improve mask prediction. These\nimprovements are simple to implement, with subtle extra computational overhead.\nOur PANet reaches the 1st place in the COCO 2017 Challenge Instance\nSegmentation task and the 2nd place in Object Detection task without\nlarge-batch training. It is also state-of-the-art on MVD and Cityscapes. Code\nis available at https://github.com/ShuLiu1993/PANet\n", "versions": [{"version": "v1", "created": "Mon, 5 Mar 2018 07:46:36 GMT"}, {"version": "v2", "created": "Wed, 5 Sep 2018 02:06:15 GMT"}, {"version": "v3", "created": "Thu, 6 Sep 2018 05:50:15 GMT"}, {"version": "v4", "created": "Tue, 18 Sep 2018 04:26:54 GMT"}], "update_date": "2018-09-19", "authors_parsed": [["Liu", "Shu", ""], ["Qi", "Lu", ""], ["Qin", "Haifang", ""], ["Shi", "Jianping", ""], ["Jia", "Jiaya", ""]]}, {"id": "1803.01541", "submitter": "Xiang Wei", "authors": "Xiang Wei, Boqing Gong, Zixia Liu, Wei Lu, Liqiang Wang", "title": "Improving the Improved Training of Wasserstein GANs: A Consistency Term\n  and Its Dual Effect", "comments": "Accepted as a conference paper in International Conference on\n  Learning Representation(ICLR). Xiang Wei and Boqing Gong contributed equally\n  in this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite being impactful on a variety of problems and applications, the\ngenerative adversarial nets (GANs) are remarkably difficult to train. This\nissue is formally analyzed by \\cite{arjovsky2017towards}, who also propose an\nalternative direction to avoid the caveats in the minmax two-player training of\nGANs. The corresponding algorithm, called Wasserstein GAN (WGAN), hinges on the\n1-Lipschitz continuity of the discriminator. In this paper, we propose a novel\napproach to enforcing the Lipschitz continuity in the training procedure of\nWGANs. Our approach seamlessly connects WGAN with one of the recent\nsemi-supervised learning methods. As a result, it gives rise to not only better\nphoto-realistic samples than the previous methods but also state-of-the-art\nsemi-supervised learning results. In particular, our approach gives rise to the\ninception score of more than 5.0 with only 1,000 CIFAR-10 images and is the\nfirst that exceeds the accuracy of 90% on the CIFAR-10 dataset using only 4,000\nlabeled images, to the best of our knowledge.\n", "versions": [{"version": "v1", "created": "Mon, 5 Mar 2018 08:00:39 GMT"}], "update_date": "2018-03-06", "authors_parsed": [["Wei", "Xiang", ""], ["Gong", "Boqing", ""], ["Liu", "Zixia", ""], ["Lu", "Wei", ""], ["Wang", "Liqiang", ""]]}, {"id": "1803.01549", "submitter": "Tong Qin", "authors": "Tong Qin, Perliang Li, and Shaojie Shen", "title": "Relocalization, Global Optimization and Map Merging for Monocular\n  Visual-Inertial SLAM", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The monocular visual-inertial system (VINS), which consists one camera and\none low-cost inertial measurement unit (IMU), is a popular approach to achieve\naccurate 6-DOF state estimation. However, such locally accurate visual-inertial\nodometry is prone to drift and cannot provide absolute pose estimation.\nLeveraging history information to relocalize and correct drift has become a hot\ntopic. In this paper, we propose a monocular visual-inertial SLAM system, which\ncan relocalize camera and get the absolute pose in a previous-built map. Then\n4-DOF pose graph optimization is performed to correct drifts and achieve global\nconsistent. The 4-DOF contains x, y, z, and yaw angle, which is the actual\ndrifted direction in the visual-inertial system. Furthermore, the proposed\nsystem can reuse a map by saving and loading it in an efficient way. Current\nmap and previous map can be merged together by the global pose graph\noptimization. We validate the accuracy of our system on public datasets and\ncompare against other state-of-the-art algorithms. We also evaluate the map\nmerging ability of our system in the large-scale outdoor environment. The\nsource code of map reuse is integrated into our public code, VINS-Mono.\n", "versions": [{"version": "v1", "created": "Mon, 5 Mar 2018 08:13:42 GMT"}], "update_date": "2018-03-06", "authors_parsed": [["Qin", "Tong", ""], ["Li", "Perliang", ""], ["Shen", "Shaojie", ""]]}, {"id": "1803.01555", "submitter": "Yue Xi", "authors": "Yue Xi, Jiangbin Zheng, Xiangjian He, Wenjing Jia, Hanhui Li", "title": "Beyond Context: Exploring Semantic Similarity for Tiny Face Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tiny face detection aims to find faces with high degrees of variability in\nscale, resolution and occlusion in cluttered scenes. Due to the very little\ninformation available on tiny faces, it is not sufficient to detect them merely\nbased on the information presented inside the tiny bounding boxes or their\ncontext. In this paper, we propose to exploit the semantic similarity among all\npredicted targets in each image to boost current face detectors. To this end,\nwe present a novel framework to model semantic similarity as pairwise\nconstraints within the metric learning scheme, and then refine our predictions\nwith the semantic similarity by utilizing the graph cut techniques. Experiments\nconducted on three widely-used benchmark datasets have demonstrated the\nimprovement over the-state-of-the-arts gained by applying this idea.\n", "versions": [{"version": "v1", "created": "Mon, 5 Mar 2018 08:29:35 GMT"}], "update_date": "2018-03-06", "authors_parsed": [["Xi", "Yue", ""], ["Zheng", "Jiangbin", ""], ["He", "Xiangjian", ""], ["Jia", "Wenjing", ""], ["Li", "Hanhui", ""]]}, {"id": "1803.01562", "submitter": "Hossein Rajabzadeh", "authors": "Hossein Rajabzadeh, Mansoor Zolghadri Jahromi, Mohammad Sadegh Zare,\n  Mostafa Fakhrahmad", "title": "Local Distance Metric Learning for Nearest Neighbor Algorithm", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distance metric learning is a successful way to enhance the performance of\nthe nearest neighbor classifier. In most cases, however, the distribution of\ndata does not obey a regular form and may change in different parts of the\nfeature space. Regarding that, this paper proposes a novel local distance\nmetric learning method, namely Local Mahalanobis Distance Learning (LMDL), in\norder to enhance the performance of the nearest neighbor classifier. LMDL\nconsiders the neighborhood influence and learns multiple distance metrics for a\nreduced set of input samples. The reduced set is called as prototypes which try\nto preserve local discriminative information as much as possible. The proposed\nLMDL can be kernelized very easily, which is significantly desirable in the\ncase of highly nonlinear data. The quality as well as the efficiency of the\nproposed method assesses through a set of different experiments on various\ndatasets and the obtained results show that LDML as well as the kernelized\nversion is superior to the other related state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 5 Mar 2018 08:45:47 GMT"}, {"version": "v2", "created": "Thu, 15 Mar 2018 20:21:22 GMT"}], "update_date": "2018-03-19", "authors_parsed": [["Rajabzadeh", "Hossein", ""], ["Jahromi", "Mansoor Zolghadri", ""], ["Zare", "Mohammad Sadegh", ""], ["Fakhrahmad", "Mostafa", ""]]}, {"id": "1803.01577", "submitter": "Oliver Moolan-Feroze Dr", "authors": "Oliver Moolan-Feroze, Andrew Calway", "title": "Predicting Out-of-View Feature Points for Model-Based Camera Pose\n  Estimation", "comments": "Submitted to IROS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we present a novel framework that uses deep learning to predict\nobject feature points that are out-of-view in the input image. This system was\ndeveloped with the application of model-based tracking in mind, particularly in\nthe case of autonomous inspection robots, where only partial views of the\nobject are available. Out-of-view prediction is enabled by applying scaling to\nthe feature point labels during network training. This is combined with a\nrecurrent neural network architecture designed to provide the final prediction\nlayers with rich feature information from across the spatial extent of the\ninput image. To show the versatility of these out-of-view predictions, we\ndescribe how to integrate them in both a particle filter tracker and an\noptimisation based tracker. To evaluate our work we compared our framework with\none that predicts only points inside the image. We show that as the amount of\nthe object in view decreases, being able to predict outside the image bounds\nadds robustness to the final pose estimation.\n", "versions": [{"version": "v1", "created": "Mon, 5 Mar 2018 10:03:17 GMT"}], "update_date": "2018-03-06", "authors_parsed": [["Moolan-Feroze", "Oliver", ""], ["Calway", "Andrew", ""]]}, {"id": "1803.01595", "submitter": "Rada Deeb", "authors": "Rada Deeb, Damien Muselet, Mathieu Hebert, Alain Tremeau", "title": "Spectral reflectance estimation from one RGB image using\n  self-interreflections in a concave object", "comments": "submitted to Applied Optics", "journal-ref": null, "doi": "10.1364/AO.57.004918", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Light interreflections occurring in a concave object generate a color\ngradient which is characteristic of the object's spectral reflectance. In this\npaper, we use this property in order to estimate the spectral reflectance of\nmatte, uniformly colored, V-shaped surfaces from a single RGB image taken under\ndirectional lighting. First, simulations show that using one image of the\nconcave object is equivalent to, and can even outperform, the state of the art\napproaches based on three images taken under three lightings with different\ncolors. Experiments on real images of folded papers were performed under\nunmeasured direct sunlight. The results show that our interreflection-based\napproach outperforms existing approaches even when the latter are improved by a\ncalibration step. The mathematical solution for the interreflection equation\nand the effect of surface parameters on the performance of the method are also\ndiscussed in this paper.\n", "versions": [{"version": "v1", "created": "Mon, 5 Mar 2018 10:43:50 GMT"}], "update_date": "2018-07-04", "authors_parsed": [["Deeb", "Rada", ""], ["Muselet", "Damien", ""], ["Hebert", "Mathieu", ""], ["Tremeau", "Alain", ""]]}, {"id": "1803.01599", "submitter": "Jogendra Nath Kundu", "authors": "Jogendra Nath Kundu, Phani Krishna Uppala, Anuj Pahuja, R. Venkatesh\n  Babu", "title": "AdaDepth: Unsupervised Content Congruent Adaptation for Depth Estimation", "comments": "CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Supervised deep learning methods have shown promising results for the task of\nmonocular depth estimation; but acquiring ground truth is costly, and prone to\nnoise as well as inaccuracies. While synthetic datasets have been used to\ncircumvent above problems, the resultant models do not generalize well to\nnatural scenes due to the inherent domain shift. Recent adversarial approaches\nfor domain adaption have performed well in mitigating the differences between\nthe source and target domains. But these methods are mostly limited to a\nclassification setup and do not scale well for fully-convolutional\narchitectures. In this work, we propose AdaDepth - an unsupervised domain\nadaptation strategy for the pixel-wise regression task of monocular depth\nestimation. The proposed approach is devoid of above limitations through a)\nadversarial learning and b) explicit imposition of content consistency on the\nadapted target representation. Our unsupervised approach performs competitively\nwith other established approaches on depth estimation tasks and achieves\nstate-of-the-art results in a semi-supervised setting.\n", "versions": [{"version": "v1", "created": "Mon, 5 Mar 2018 10:55:58 GMT"}, {"version": "v2", "created": "Thu, 7 Jun 2018 11:06:01 GMT"}], "update_date": "2018-06-08", "authors_parsed": [["Kundu", "Jogendra Nath", ""], ["Uppala", "Phani Krishna", ""], ["Pahuja", "Anuj", ""], ["Babu", "R. Venkatesh", ""]]}, {"id": "1803.01669", "submitter": "Stanley Tuznik", "authors": "Stanley L. Tuznik, Peter J. Olver, Allen Tannenbaum", "title": "Affine Differential Invariants for Invariant Feature Point Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV math.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image feature points are detected as pixels which locally maximize a detector\nfunction, two commonly used examples of which are the (Euclidean) image\ngradient and the Harris-Stephens corner detector. A major limitation of these\nfeature detectors are that they are only Euclidean-invariant. In this work we\ndemonstrate the application of a 2D affine-invariant image feature point\ndetector based on differential invariants as derived through the equivariant\nmethod of moving frames. The fundamental equi-affine differential invariants\nfor 3D image volumes are also computed.\n", "versions": [{"version": "v1", "created": "Mon, 5 Mar 2018 14:14:13 GMT"}, {"version": "v2", "created": "Mon, 12 Mar 2018 17:02:18 GMT"}], "update_date": "2018-03-13", "authors_parsed": [["Tuznik", "Stanley L.", ""], ["Olver", "Peter J.", ""], ["Tannenbaum", "Allen", ""]]}, {"id": "1803.01687", "submitter": "Vandit Gajjar J", "authors": "Vandit Gajjar, Yash Khandhediya, Ayesha Gurnani, Viraj Mavani, Mehul\n  S. Raval", "title": "ViS-HuD: Using Visual Saliency to Improve Human Detection with\n  Convolutional Neural Networks", "comments": "9 Pages, 10 Figures, 2 Tables; Accepted to MBCC Workshop in\n  Conjunction with CVPR-2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper presents a technique to improve human detection in still images\nusing deep learning. Our novel method, ViS-HuD, computes visual saliency map\nfrom the image. Then the input image is multiplied by the map and product is\nfed to the Convolutional Neural Network (CNN) which detects humans in the\nimage. A visual saliency map is generated using ML-Net and human detection is\ncarried out using DetectNet. ML-Net is pre-trained on SALICON while, DetectNet\nis pre-trained on ImageNet database for visual saliency detection and image\nclassification respectively. The CNNs of ViS-HuD were trained on two\nchallenging databases - Penn Fudan and TUD-Brussels Benchmark. Experimental\nresults demonstrate that the proposed method achieves state-of-the-art\nperformance on Penn Fudan Dataset with 91.4% human detection accuracy and it\nachieves average miss-rate of 53% on the TUDBrussels benchmark.\n", "versions": [{"version": "v1", "created": "Wed, 21 Feb 2018 19:57:37 GMT"}, {"version": "v2", "created": "Tue, 6 Mar 2018 06:57:26 GMT"}, {"version": "v3", "created": "Wed, 18 Apr 2018 06:42:46 GMT"}], "update_date": "2018-04-19", "authors_parsed": [["Gajjar", "Vandit", ""], ["Khandhediya", "Yash", ""], ["Gurnani", "Ayesha", ""], ["Mavani", "Viraj", ""], ["Raval", "Mehul S.", ""]]}, {"id": "1803.01711", "submitter": "Lakshmanan Nataraj", "authors": "Arjuna Flenner, Lawrence Peterson, Jason Bunk, Tajuddin Manhar\n  Mohammed, Lakshmanan Nataraj, B.S. Manjunath", "title": "Resampling Forgery Detection Using Deep Learning and A-Contrario\n  Analysis", "comments": "arXiv admin note: text overlap with arXiv:1802.03154", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The amount of digital imagery recorded has recently grown exponentially, and\nwith the advancement of software, such as Photoshop or Gimp, it has become\neasier to manipulate images. However, most images on the internet have not been\nmanipulated and any automated manipulation detection algorithm must carefully\ncontrol the false alarm rate. In this paper we discuss a method to\nautomatically detect local resampling using deep learning while controlling the\nfalse alarm rate using a-contrario analysis. The automated procedure consists\nof three primary steps. First, resampling features are calculated for image\nblocks. A deep learning classifier is then used to generate a heatmap that\nindicates if the image block has been resampled. We expect some of these blocks\nto be falsely identified as resampled. We use a-contrario hypothesis testing to\nboth identify if the patterns of the manipulated blocks indicate if the image\nhas been tampered with and to localize the manipulation. We demonstrate that\nthis strategy is effective in indicating if an image has been manipulated and\nlocalizing the manipulations.\n", "versions": [{"version": "v1", "created": "Thu, 1 Mar 2018 21:59:04 GMT"}], "update_date": "2018-03-06", "authors_parsed": [["Flenner", "Arjuna", ""], ["Peterson", "Lawrence", ""], ["Bunk", "Jason", ""], ["Mohammed", "Tajuddin Manhar", ""], ["Nataraj", "Lakshmanan", ""], ["Manjunath", "B. S.", ""]]}, {"id": "1803.01780", "submitter": "Meng Ding", "authors": "Meng Ding and Guoliang Fan", "title": "A generalized parametric 3D shape representation for articulated pose\n  estimation", "comments": "6 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel parametric 3D shape representation, Generalized sum of\nGaussians (G-SoG), which is particularly suitable for pose estimation of\narticulated objects. Compared with the original sum-of-Gaussians (SoG), G-SoG\ncan handle both isotropic and anisotropic Gaussians, leading to a more flexible\nand adaptable shape representation yet with much fewer anisotropic Gaussians\ninvolved. An articulated shape template can be developed by embedding G-SoG in\na tree-structured skeleton model to represent an articulated object. We further\nderive a differentiable similarity function between G-SoG (the template) and\nSoG (observed data) that can be optimized analytically for efficient pose\nestimation. The experimental results on a standard human pose estimation\ndataset show the effectiveness and advantages of G-SoG over the original SoG as\nwell as the promise compared with the recent algorithms that use more\ncomplicated shape models.\n", "versions": [{"version": "v1", "created": "Mon, 5 Mar 2018 17:12:36 GMT"}], "update_date": "2018-03-06", "authors_parsed": [["Ding", "Meng", ""], ["Fan", "Guoliang", ""]]}, {"id": "1803.01837", "submitter": "Chen-Hsuan Lin", "authors": "Chen-Hsuan Lin, Ersin Yumer, Oliver Wang, Eli Shechtman, Simon Lucey", "title": "ST-GAN: Spatial Transformer Generative Adversarial Networks for Image\n  Compositing", "comments": "Accepted to CVPR 2018 (website & code:\n  https://chenhsuanlin.bitbucket.io/spatial-transformer-GAN/)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of finding realistic geometric corrections to a\nforeground object such that it appears natural when composited into a\nbackground image. To achieve this, we propose a novel Generative Adversarial\nNetwork (GAN) architecture that utilizes Spatial Transformer Networks (STNs) as\nthe generator, which we call Spatial Transformer GANs (ST-GANs). ST-GANs seek\nimage realism by operating in the geometric warp parameter space. In\nparticular, we exploit an iterative STN warping scheme and propose a sequential\ntraining strategy that achieves better results compared to naive training of a\nsingle generator. One of the key advantages of ST-GAN is its applicability to\nhigh-resolution images indirectly since the predicted warp parameters are\ntransferable between reference frames. We demonstrate our approach in two\napplications: (1) visualizing how indoor furniture (e.g. from product images)\nmight be perceived in a room, (2) hallucinating how accessories like glasses\nwould look when matched with real portraits.\n", "versions": [{"version": "v1", "created": "Mon, 5 Mar 2018 18:59:01 GMT"}], "update_date": "2018-03-06", "authors_parsed": [["Lin", "Chen-Hsuan", ""], ["Yumer", "Ersin", ""], ["Wang", "Oliver", ""], ["Shechtman", "Eli", ""], ["Lucey", "Simon", ""]]}, {"id": "1803.01906", "submitter": "Pengcheng Xi", "authors": "Pengcheng Xi, Chang Shu, Rafik Goubran", "title": "Abnormality Detection in Mammography using Deep Convolutional Neural\n  Networks", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Breast cancer is the most common cancer in women worldwide. The most common\nscreening technology is mammography. To reduce the cost and workload of\nradiologists, we propose a computer aided detection approach for classifying\nand localizing calcifications and masses in mammogram images. To improve on\nconventional approaches, we apply deep convolutional neural networks (CNN) for\nautomatic feature learning and classifier building. In computer-aided\nmammography, deep CNN classifiers cannot be trained directly on full mammogram\nimages because of the loss of image details from resizing at input layers.\nInstead, our classifiers are trained on labelled image patches and then adapted\nto work on full mammogram images for localizing the abnormalities.\nState-of-the-art deep convolutional neural networks are compared on their\nperformance of classifying the abnormalities. Experimental results indicate\nthat VGGNet receives the best overall accuracy at 92.53\\% in classifications.\nFor localizing abnormalities, ResNet is selected for computing class activation\nmaps because it is ready to be deployed without structural change or further\ntraining. Our approach demonstrates that deep convolutional neural network\nclassifiers have remarkable localization capabilities despite no supervision on\nthe location of abnormalities is provided.\n", "versions": [{"version": "v1", "created": "Mon, 5 Mar 2018 20:04:56 GMT"}], "update_date": "2018-03-07", "authors_parsed": [["Xi", "Pengcheng", ""], ["Shu", "Chang", ""], ["Goubran", "Rafik", ""]]}, {"id": "1803.01945", "submitter": "Dino Ienco", "authors": "P. Benedetti, D. Ienco, R. Gaetano, K. Os\\'e, R. Pensa and S. Dupuy", "title": "M3Fusion: A Deep Learning Architecture for Multi-{Scale/Modal/Temporal}\n  satellite data fusion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern Earth Observation systems provide sensing data at different temporal\nand spatial resolutions. Among optical sensors, today the Sentinel-2 program\nsupplies high-resolution temporal (every 5 days) and high spatial resolution\n(10m) images that can be useful to monitor land cover dynamics. On the other\nhand, Very High Spatial Resolution images (VHSR) are still an essential tool to\nfigure out land cover mapping characterized by fine spatial patterns.\nUnderstand how to efficiently leverage these complementary sources of\ninformation together to deal with land cover mapping is still challenging. With\nthe aim to tackle land cover mapping through the fusion of multi-temporal High\nSpatial Resolution and Very High Spatial Resolution satellite images, we\npropose an End-to-End Deep Learning framework, named M3Fusion, able to leverage\nsimultaneously the temporal knowledge contained in time series data as well as\nthe fine spatial information available in VHSR information. Experiments carried\nout on the Reunion Island study area asses the quality of our proposal\nconsidering both quantitative and qualitative aspects.\n", "versions": [{"version": "v1", "created": "Mon, 5 Mar 2018 21:59:42 GMT"}], "update_date": "2018-03-07", "authors_parsed": [["Benedetti", "P.", ""], ["Ienco", "D.", ""], ["Gaetano", "R.", ""], ["Os\u00e9", "K.", ""], ["Pensa", "R.", ""], ["Dupuy", "S.", ""]]}, {"id": "1803.01947", "submitter": "Lian Duan", "authors": "Lian Duan, Xi Qin, Yuanhao He, Xialin Sang, Jinda Pan, Tao Xu, Jing\n  Men, Rudolph E. Tanzi, Airong Li, Yutao Ma, Chao Zhou", "title": "Segmentation of Drosophila Heart in Optical Coherence Microscopy Images\n  Using Convolutional Neural Networks", "comments": "7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks are powerful tools for image segmentation and\nclassification. Here, we use this method to identify and mark the heart region\nof Drosophila at different developmental stages in the cross-sectional images\nacquired by a custom optical coherence microscopy (OCM) system. With our\nwell-trained convolutional neural network model, the heart regions through\nmultiple heartbeat cycles can be marked with an intersection over union (IOU)\nof ~86%. Various morphological and dynamical cardiac parameters can be\nquantified accurately with automatically segmented heart regions. This study\ndemonstrates an efficient heart segmentation method to analyze OCM images of\nthe beating heart in Drosophila.\n", "versions": [{"version": "v1", "created": "Mon, 5 Mar 2018 22:03:52 GMT"}, {"version": "v2", "created": "Sat, 21 Jul 2018 01:55:59 GMT"}], "update_date": "2018-07-24", "authors_parsed": [["Duan", "Lian", ""], ["Qin", "Xi", ""], ["He", "Yuanhao", ""], ["Sang", "Xialin", ""], ["Pan", "Jinda", ""], ["Xu", "Tao", ""], ["Men", "Jing", ""], ["Tanzi", "Rudolph E.", ""], ["Li", "Airong", ""], ["Ma", "Yutao", ""], ["Zhou", "Chao", ""]]}, {"id": "1803.01967", "submitter": "Kevin Wu", "authors": "Kevin Wu, Eric Wu, Gabriel Kreiman", "title": "Learning Scene Gist with Convolutional Neural Networks to Improve Object\n  Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advancements in convolutional neural networks (CNNs) have made significant\nstrides toward achieving high performance levels on multiple object recognition\ntasks. While some approaches utilize information from the entire scene to\npropose regions of interest, the task of interpreting a particular region or\nobject is still performed independently of other objects and features in the\nimage. Here we demonstrate that a scene's 'gist' can significantly contribute\nto how well humans can recognize objects. These findings are consistent with\nthe notion that humans foveate on an object and incorporate information from\nthe periphery to aid in recognition. We use a biologically inspired two-part\nconvolutional neural network ('GistNet') that models the fovea and periphery to\nprovide a proof-of-principle demonstration that computational object\nrecognition can significantly benefit from the gist of the scene as contextual\ninformation. Our model yields accuracy improvements of up to 50% in certain\nobject categories when incorporating contextual gist, while only increasing the\noriginal model size by 5%. This proposed model mirrors our intuition about how\nthe human visual system recognizes objects, suggesting specific biologically\nplausible constraints to improve machine vision and building initial steps\ntowards the challenge of scene understanding.\n", "versions": [{"version": "v1", "created": "Tue, 6 Mar 2018 00:45:33 GMT"}, {"version": "v2", "created": "Sat, 9 Jun 2018 04:45:00 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Wu", "Kevin", ""], ["Wu", "Eric", ""], ["Kreiman", "Gabriel", ""]]}, {"id": "1803.02007", "submitter": "Kapil Katyal", "authors": "Kapil Katyal, Katie Popek, Chris Paxton, Joseph Moore, Kevin Wolfe,\n  Philippe Burlina, and Gregory D. Hager", "title": "Occupancy Map Prediction Using Generative and Fully Convolutional\n  Networks for Vehicle Navigation", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fast, collision-free motion through unknown environments remains a\nchallenging problem for robotic systems. In these situations, the robot's\nability to reason about its future motion is often severely limited by sensor\nfield of view (FOV). By contrast, biological systems routinely make decisions\nby taking into consideration what might exist beyond their FOV based on prior\nexperience. In this paper, we present an approach for predicting occupancy map\nrepresentations of sensor data for future robot motions using deep neural\nnetworks. We evaluate several deep network architectures, including purely\ngenerative and adversarial models. Testing on both simulated and real\nenvironments we demonstrated performance both qualitatively and quantitatively,\nwith SSIM similarity measure up to 0.899. We showed that it is possible to make\npredictions about occupied space beyond the physical robot's FOV from simulated\ntraining data. In the future, this method will allow robots to navigate through\nunknown environments in a faster, safer manner.\n", "versions": [{"version": "v1", "created": "Tue, 6 Mar 2018 04:01:37 GMT"}], "update_date": "2018-03-07", "authors_parsed": [["Katyal", "Kapil", ""], ["Popek", "Katie", ""], ["Paxton", "Chris", ""], ["Moore", "Joseph", ""], ["Wolfe", "Kevin", ""], ["Burlina", "Philippe", ""], ["Hager", "Gregory D.", ""]]}, {"id": "1803.02009", "submitter": "Jingwei Song", "authors": "Jingwei Song, Jun Wang, Liang Zhao, Shoudong Huang and Gamini\n  Dissanayake", "title": "MIS-SLAM: Real-time Large Scale Dense Deformable SLAM System in Minimal\n  Invasive Surgery Based on Heterogeneous Computing", "comments": "Submitted to conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Real-time simultaneously localization and dense mapping is very helpful for\nproviding Virtual Reality and Augmented Reality for surgeons or even surgical\nrobots. In this paper, we propose MIS-SLAM: a complete real-time large scale\ndense deformable SLAM system with stereoscope in Minimal Invasive Surgery based\non heterogeneous computing by making full use of CPU and GPU. Idled CPU is used\nto perform ORB- SLAM for providing robust global pose. Strategies are taken to\nintegrate modules from CPU and GPU. We solved the key problem raised in\nprevious work, that is, fast movement of scope and blurry images make the scope\ntracking fail. Benefiting from improved localization, MIS-SLAM can achieve\nlarge scale scope localizing and dense mapping in real-time. It transforms and\ndeforms current model and incrementally fuses new observation while keeping\nvivid texture. In-vivo experiments conducted on publicly available datasets\npresented in the form of videos demonstrate the feasibility and practicality of\nMIS-SLAM for potential clinical purpose.\n", "versions": [{"version": "v1", "created": "Tue, 6 Mar 2018 04:19:24 GMT"}, {"version": "v2", "created": "Tue, 27 Mar 2018 04:13:56 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Song", "Jingwei", ""], ["Wang", "Jun", ""], ["Zhao", "Liang", ""], ["Huang", "Shoudong", ""], ["Dissanayake", "Gamini", ""]]}, {"id": "1803.02057", "submitter": "Jatavallabhula Krishna Murthy", "authors": "Junaid Ahmed Ansari, Sarthak Sharma, Anshuman Majumdar, J. Krishna\n  Murthy, and K. Madhava Krishna", "title": "The Earth ain't Flat: Monocular Reconstruction of Vehicles on Steep and\n  Graded Roads from a Moving Camera", "comments": "Submitted to IROS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate localization of other traffic participants is a vital task in\nautonomous driving systems. State-of-the-art systems employ a combination of\nsensing modalities such as RGB cameras and LiDARs for localizing traffic\nparticipants, but most such demonstrations have been confined to plain roads.\nWe demonstrate, to the best of our knowledge, the first results for monocular\nobject localization and shape estimation on surfaces that do not share the same\nplane with the moving monocular camera. We approximate road surfaces by local\nplanar patches and use semantic cues from vehicles in the scene to initialize a\nlocal bundle-adjustment like procedure that simultaneously estimates the pose\nand shape of the vehicles, and the orientation of the local ground plane on\nwhich the vehicle stands as well. We evaluate the proposed approach on the\nKITTI and SYNTHIA-SF benchmarks, for a variety of road plane configurations.\nThe proposed approach significantly improves the state-of-the-art for monocular\nobject localization on arbitrarily-shaped roads.\n", "versions": [{"version": "v1", "created": "Tue, 6 Mar 2018 08:28:18 GMT"}], "update_date": "2018-05-15", "authors_parsed": [["Ansari", "Junaid Ahmed", ""], ["Sharma", "Sarthak", ""], ["Majumdar", "Anshuman", ""], ["Murthy", "J. Krishna", ""], ["Krishna", "K. Madhava", ""]]}, {"id": "1803.02077", "submitter": "Roey Mechrez", "authors": "Roey Mechrez, Itamar Talmi, Lihi Zelnik-Manor", "title": "The Contextual Loss for Image Transformation with Non-Aligned Data", "comments": "ECCV Oral. Paper web page:\n  http://cgm.technion.ac.il/Computer-Graphics-Multimedia/Software/contextual/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feed-forward CNNs trained for image transformation problems rely on loss\nfunctions that measure the similarity between the generated image and a target\nimage. Most of the common loss functions assume that these images are spatially\naligned and compare pixels at corresponding locations. However, for many tasks,\naligned training pairs of images will not be available. We present an\nalternative loss function that does not require alignment, thus providing an\neffective and simple solution for a new space of problems. Our loss is based on\nboth context and semantics -- it compares regions with similar semantic\nmeaning, while considering the context of the entire image. Hence, for example,\nwhen transferring the style of one face to another, it will translate\neyes-to-eyes and mouth-to-mouth. Our code can be found at\nhttps://www.github.com/roimehrez/contextualLoss\n", "versions": [{"version": "v1", "created": "Tue, 6 Mar 2018 09:43:25 GMT"}, {"version": "v2", "created": "Wed, 7 Mar 2018 16:11:15 GMT"}, {"version": "v3", "created": "Mon, 12 Mar 2018 13:08:09 GMT"}, {"version": "v4", "created": "Wed, 18 Jul 2018 09:58:50 GMT"}], "update_date": "2018-07-19", "authors_parsed": [["Mechrez", "Roey", ""], ["Talmi", "Itamar", ""], ["Zelnik-Manor", "Lihi", ""]]}, {"id": "1803.02097", "submitter": "Maarten Bieshaar", "authors": "Maarten Bieshaar", "title": "Where is my Device? - Detecting the Smart Device's Wearing Location in\n  the Context of Active Safety for Vulnerable Road Users", "comments": "10 pages, 3 figures, accepted for publication in Organic Computing:\n  Doctoral Dissertation Colloquium 2017. Volume 11 of Intelligent Embedded\n  Systems. Kassel University Press, Kassel", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article describes an approach to detect the wearing location of smart\ndevices worn by pedestrians and cyclists. The detection, which is based solely\non the sensors of the smart devices, is important context-information which can\nbe used to parametrize subsequent algorithms, e.g. for dead reckoning or\nintention detection to improve the safety of vulnerable road users. The wearing\nlocation recognition can in terms of Organic Computing (OC) be seen as a step\ntowards self-awareness and self-adaptation. For the wearing location detection\na two-stage process is presented. It is subdivided into moving detection\nfollowed by the wearing location classification. Finally, the approach is\nevaluated on a real world dataset consisting of pedestrians and cyclists.\n", "versions": [{"version": "v1", "created": "Tue, 6 Mar 2018 10:34:47 GMT"}], "update_date": "2018-03-07", "authors_parsed": [["Bieshaar", "Maarten", ""]]}, {"id": "1803.02112", "submitter": "Crist\\'ov\\~ao Cruz", "authors": "Crist\\'ov\\~ao Cruz, Alessandro Foi, Vladimir Katkovnik, Karen\n  Egiazarian", "title": "Nonlocality-Reinforced Convolutional Neural Networks for Image Denoising", "comments": "Accepted for publication in IEEE SPL", "journal-ref": null, "doi": "10.1109/LSP.2018.2850222", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a paradigm for nonlocal sparsity reinforced deep convolutional\nneural network denoising. It is a combination of a local multiscale denoising\nby a convolutional neural network (CNN) based denoiser and a nonlocal denoising\nbased on a nonlocal filter (NLF) exploiting the mutual similarities between\ngroups of patches. CNN models are leveraged with noise levels that\nprogressively decrease at every iteration of our framework, while their output\nis regularized by a nonlocal prior implicit within the NLF. Unlike complicated\nneural networks that embed the nonlocality prior within the layers of the\nnetwork, our framework is modular, it uses standard pre-trained CNNs together\nwith standard nonlocal filters. An instance of the proposed framework, called\nNN3D, is evaluated over large grayscale image datasets showing state-of-the-art\nperformance.\n", "versions": [{"version": "v1", "created": "Tue, 6 Mar 2018 11:11:38 GMT"}, {"version": "v2", "created": "Thu, 21 Jun 2018 12:07:27 GMT"}], "update_date": "2018-08-15", "authors_parsed": [["Cruz", "Crist\u00f3v\u00e3o", ""], ["Foi", "Alessandro", ""], ["Katkovnik", "Vladimir", ""], ["Egiazarian", "Karen", ""]]}, {"id": "1803.02129", "submitter": "Felix Altenberger", "authors": "Felix Altenberger, Claus Lenz", "title": "A Non-Technical Survey on Deep Convolutional Neural Network\n  Architectures", "comments": "17 pages (incl. references), 23 Postscript figures, uses IEEEtran", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Artificial neural networks have recently shown great results in many\ndisciplines and a variety of applications, including natural language\nunderstanding, speech processing, games and image data generation. One\nparticular application in which the strong performance of artificial neural\nnetworks was demonstrated is the recognition of objects in images, where deep\nconvolutional neural networks are commonly applied. In this survey, we give a\ncomprehensive introduction to this topic (object recognition with deep\nconvolutional neural networks), with a strong focus on the evolution of network\narchitectures. Therefore, we aim to compress the most important concepts in\nthis field in a simple and non-technical manner to allow for future researchers\nto have a quick general understanding.\n  This work is structured as follows:\n  1. We will explain the basic ideas of (convolutional) neural networks and\ndeep learning and examine their usage for three object recognition tasks: image\nclassification, object localization and object detection.\n  2. We give a review on the evolution of deep convolutional neural networks by\nproviding an extensive overview of the most important network architectures\npresented in chronological order of their appearances.\n", "versions": [{"version": "v1", "created": "Tue, 6 Mar 2018 11:40:46 GMT"}], "update_date": "2018-03-07", "authors_parsed": [["Altenberger", "Felix", ""], ["Lenz", "Claus", ""]]}, {"id": "1803.02140", "submitter": "Christian A. Mueller", "authors": "Christian A. Mueller and Andreas Birk", "title": "Conceptualization of Object Compositions Using Persistent Homology", "comments": "revised version; this work is accepted for IEEE/RSJ International\n  Conference on Intelligent Robots and Systems (IROS) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A topological shape analysis is proposed and utilized to learn concepts that\nreflect shape commonalities. Our approach is two-fold: i) a spatial topology\nanalysis of point cloud segment constellations within objects. Therein\nconstellations are decomposed and described in an hierarchical manner - from\nsingle segments to segment groups until a single group reflects an entire\nobject. ii) a topology analysis of the description space in which segment\ndecompositions are exposed in. Inspired by Persistent Homology, hidden groups\nof shape commonalities are revealed from object segment decompositions.\nExperiments show that extracted persistent groups of commonalities can\nrepresent semantically meaningful shape concepts. We also show the\ngeneralization capability of the proposed approach considering samples of\nexternal datasets.\n", "versions": [{"version": "v1", "created": "Tue, 6 Mar 2018 12:31:43 GMT"}, {"version": "v2", "created": "Mon, 6 Aug 2018 16:45:34 GMT"}, {"version": "v3", "created": "Tue, 20 Nov 2018 22:20:51 GMT"}], "update_date": "2018-11-22", "authors_parsed": [["Mueller", "Christian A.", ""], ["Birk", "Andreas", ""]]}, {"id": "1803.02181", "submitter": "Vandit Gajjar J", "authors": "Vandit Gajjar", "title": "2^B3^C: 2 Box 3 Crop of Facial Image for Gender Classification with\n  Convolutional Networks", "comments": "8 Pages, 7 Figures, Submitted to IEEE Computer Society Biometrics\n  2018 workshop in conjuction with CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we tackle the classification of gender in facial images with\ndeep learning. Our convolutional neural networks (CNN) use the VGG-16\narchitecture [1] and are pretrained on ImageNet for image classification. Our\nproposed method (2^B3^C) first detects the face in the facial image, increases\nthe margin of a detected face by 50%, cropping the face with two boxes three\ncrop schemes (Left, Middle, and Right crop) and extracts the CNN predictions on\nthe cropped schemes. The CNNs of our method is fine-tuned on the Adience and\nLFW with gender annotations. We show the effectiveness of our method by\nachieving 90.8% classification on Adience and achieving competitive 95.3%\nclassification accuracy on LFW dataset. In addition, to check the true ability\nof our method, our gender classification system has a frame rate of 7-10 fps\n(frames per seconds) on a GPU considering real-time scenarios.\n", "versions": [{"version": "v1", "created": "Mon, 5 Mar 2018 11:25:14 GMT"}], "update_date": "2018-03-07", "authors_parsed": [["Gajjar", "Vandit", ""]]}, {"id": "1803.02188", "submitter": "Yuxiang Zhou", "authors": "Riza Alp Guler, Yuxiang Zhou, George Trigeorgis, Epameinondas\n  Antonakos, Patrick Snape, Stefanos Zafeiriou, Iasonas Kokkinos", "title": "DenseReg: Fully Convolutional Dense Shape Regression In-the-Wild", "comments": "arXiv admin note: substantial text overlap with arXiv:1612.01202", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we use deep learning to establish dense correspondences between\na 3D object model and an image \"in the wild\". We introduce \"DenseReg\", a\nfully-convolutional neural network (F-CNN) that densely regresses at every\nforeground pixel a pair of U-V template coordinates in a single feedforward\npass. To train DenseReg we construct a supervision signal by combining 3D\ndeformable model fitting and 2D landmark annotations. We define the regression\ntask in terms of the intrinsic, U-V coordinates of a 3D deformable model that\nis brought into correspondence with image instances at training time. A host of\nother object-related tasks (e.g. part segmentation, landmark localization) are\nshown to be by-products of this task, and to largely improve thanks to its\nintroduction. We obtain highly-accurate regression results by combining ideas\nfrom semantic segmentation with regression networks, yielding a 'quantized\nregression' architecture that first obtains a quantized estimate of position\nthrough classification, and refines it through regression of the residual. We\nshow that such networks can boost the performance of existing state-of-the-art\nsystems for pose estimation. Firstly, we show that our system can serve as an\ninitialization for Statistical Deformable Models, as well as an element of\ncascaded architectures that jointly localize landmarks and estimate dense\ncorrespondences. We also show that the obtained dense correspondence can act as\na source of 'privileged information' that complements and extends the pure\nlandmark-level annotations, accelerating and improving the training of pose\nestimation networks. We report state-of-the-art performance on the challenging\n300W benchmark for facial landmark localization and on the MPII and LSP\ndatasets for human pose estimation.\n", "versions": [{"version": "v1", "created": "Mon, 5 Mar 2018 17:21:35 GMT"}, {"version": "v2", "created": "Sun, 11 Mar 2018 20:24:33 GMT"}], "update_date": "2018-03-13", "authors_parsed": [["Guler", "Riza Alp", ""], ["Zhou", "Yuxiang", ""], ["Trigeorgis", "George", ""], ["Antonakos", "Epameinondas", ""], ["Snape", "Patrick", ""], ["Zafeiriou", "Stefanos", ""], ["Kokkinos", "Iasonas", ""]]}, {"id": "1803.02209", "submitter": "Xinwen Zhou", "authors": "Xinwen Zhou, Xuguang Lan, Hanbo Zhang, Zhiqiang Tian, Yang Zhang and\n  Nanning Zheng", "title": "Fully Convolutional Grasp Detection Network with Oriented Anchor Box", "comments": "8 pages 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a real-time approach to predict multiple grasping\nposes for a parallel-plate robotic gripper using RGB images. A model with\noriented anchor box mechanism is proposed and a new matching strategy is used\nduring the training process. An end-to-end fully convolutional neural network\nis employed in our work. The network consists of two parts: the feature\nextractor and multi-grasp predictor. The feature extractor is a deep\nconvolutional neural network. The multi-grasp predictor regresses grasp\nrectangles from predefined oriented rectangles, called oriented anchor boxes,\nand classifies the rectangles into graspable and ungraspable. On the standard\nCornell Grasp Dataset, our model achieves an accuracy of 97.74% and 96.61% on\nimage-wise split and object-wise split respectively, and outperforms the latest\nstate-of-the-art approach by 1.74% on image-wise split and 0.51% on object-wise\nsplit.\n", "versions": [{"version": "v1", "created": "Tue, 6 Mar 2018 14:21:55 GMT"}], "update_date": "2018-03-07", "authors_parsed": [["Zhou", "Xinwen", ""], ["Lan", "Xuguang", ""], ["Zhang", "Hanbo", ""], ["Tian", "Zhiqiang", ""], ["Zhang", "Yang", ""], ["Zheng", "Nanning", ""]]}, {"id": "1803.02222", "submitter": "Xi Fang", "authors": "Xi Fang, Zengmao Wang, Xinyao Tang, Chen Wu", "title": "Multi-class Active Learning: A Hybrid Informative and Representative\n  Criterion Inspired Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Labeling each instance in a large dataset is extremely labor- and time-\nconsuming . One way to alleviate this problem is active learning, which aims to\nwhich discover the most valuable instances for labeling to construct a powerful\nclassifier. Considering both informativeness and representativeness provides a\npromising way to design a practical active learning. However, most existing\nactive learning methods select instances favoring either informativeness or\nrepresentativeness. Meanwhile, many are designed based on the binary class, so\nthat they may present suboptimal solutions on the datasets with multiple\nclasses. In this paper, a hybrid informative and representative criterion based\nmulti-class active learning approach is proposed. We combine the informative\ninformativeness and representativeness into one formula, which can be solved\nunder a unified framework. The informativeness is measured by the margin\nminimum while the representative information is measured by the maximum mean\ndiscrepancy. By minimizing the upper bound for the true risk, we generalize the\nempirical risk minimization principle to the active learning setting.\nSimultaneously, our proposed method makes full use of the label information,\nand the proposed active learning is designed based on multiple classes. So the\nproposed method is not suitable to the binary class but also the multiple\nclasses. We conduct our experiments on twelve benchmark UCI data sets, and the\nexperimental results demonstrate that the proposed method performs better than\nsome state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 6 Mar 2018 14:32:15 GMT"}], "update_date": "2018-03-07", "authors_parsed": [["Fang", "Xi", ""], ["Wang", "Zengmao", ""], ["Tang", "Xinyao", ""], ["Wu", "Chen", ""]]}, {"id": "1803.02242", "submitter": "Stefan Zernetsch", "authors": "Stefan Zernetsch, Viktor Kress, Bernhard Sick and Konrad Doll", "title": "Early Start Intention Detection of Cyclists Using Motion History Images\n  and a Deep Residual Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we present a novel approach to detect starting motions of\ncyclists in real world traffic scenarios based on Motion History Images (MHIs).\nThe method uses a deep Convolutional Neural Network (CNN) with a residual\nnetwork architecture (ResNet), which is commonly used in image classification\nand detection tasks. By combining MHIs with a ResNet classifier and performing\na frame by frame classification of the MHIs, we are able to detect starting\nmotions in image sequences. The detection is performed using a wide angle\nstereo camera system at an urban intersection. We compare our algorithm to an\nexisting method to detect movement transitions of pedestrians that uses MHIs in\ncombination with a Histograms of Oriented Gradients (HOG) like descriptor and a\nSupport Vector Machine (SVM), which we adapted to cyclists. To train and\nevaluate the methods a dataset containing MHIs of 394 cyclist starting motions\nwas created. The results show that both methods can be used to detect starting\nmotions of cyclists. Using the SVM approach, we were able to safely detect\nstarting motions 0.506 s on average after the bicycle starts moving with an\nF1-score of 97.7%. The ResNet approach achieved an F1-score of 100% at an\naverage detection time of 0.144 s. The ResNet approach outperformed the SVM\napproach in both robustness against false positive detections and detection\ntime.\n", "versions": [{"version": "v1", "created": "Tue, 6 Mar 2018 15:12:27 GMT"}], "update_date": "2018-03-07", "authors_parsed": [["Zernetsch", "Stefan", ""], ["Kress", "Viktor", ""], ["Sick", "Bernhard", ""], ["Doll", "Konrad", ""]]}, {"id": "1803.02256", "submitter": "Xiaoheng Jiang", "authors": "Mingliang Xu, Zhaoyang Ge, Xiaoheng Jiang, Gaoge Cui, Pei Lv, Bing\n  Zhou, Changsheng Xu", "title": "Depth Information Guided Crowd Counting for Complex Crowd Scenes", "comments": "9 pages, 8 figures. The paper is under consideration at Pattern\n  Recognition Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is important to monitor and analyze crowd events for the sake of city\nsafety. In an EDOF (extended depth of field) image with a crowded scene, the\ndistribution of people is highly imbalanced. People far away from the camera\nlook much smaller and often occlude each other heavily, while people close to\nthe camera look larger. In such a case, it is difficult to accurately estimate\nthe number of people by using one technique. In this paper, we propose a Depth\nInformation Guided Crowd Counting (DigCrowd) method to deal with crowded EDOF\nscenes. DigCrowd first uses the depth information of an image to segment the\nscene into a far-view region and a near-view region. Then Digcrowd maps the\nfar-view region to its crowd density map and uses a detection method to count\nthe people in the near-view region. In addition, we introduce a new crowd\ndataset that contains 1000 images. Experimental results demonstrate the\neffectiveness of our DigCrowd method\n", "versions": [{"version": "v1", "created": "Sat, 3 Mar 2018 11:50:45 GMT"}, {"version": "v2", "created": "Mon, 23 Apr 2018 10:30:16 GMT"}], "update_date": "2018-04-24", "authors_parsed": [["Xu", "Mingliang", ""], ["Ge", "Zhaoyang", ""], ["Jiang", "Xiaoheng", ""], ["Cui", "Gaoge", ""], ["Lv", "Pei", ""], ["Zhou", "Bing", ""], ["Xu", "Changsheng", ""]]}, {"id": "1803.02257", "submitter": "Sergey Triputen", "authors": "Sergey Triputen, Atmaraaj Gopal, Thomas Weber, Christian Hofert,\n  Kristiaan Schreve and Matthias Ratsch", "title": "Methodology to analyze the accuracy of 3D objects reconstructed with\n  collaborative robot based monocular LSD-SLAM", "comments": "5 pages, 5 figures, 2018 International Conference on Intelligent\n  Autonomous Systems (ICoIAS 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  SLAM systems are mainly applied for robot navigation while research on\nfeasibility for motion planning with SLAM for tasks like bin-picking, is\nscarce. Accurate 3D reconstruction of objects and environments is important for\nplanning motion and computing optimal gripper pose to grasp objects. In this\nwork, we propose the methods to analyze the accuracy of a 3D environment\nreconstructed using a LSD-SLAM system with a monocular camera mounted onto the\ngripper of a collaborative robot. We discuss and propose a solution to the pose\nspace conversion problem. Finally, we present several criteria to analyze the\n3D reconstruction accuracy. These could be used as guidelines to improve the\naccuracy of 3D reconstructions with monocular LSD-SLAM and other SLAM based\nsolutions.\n", "versions": [{"version": "v1", "created": "Tue, 6 Mar 2018 15:33:15 GMT"}], "update_date": "2018-03-07", "authors_parsed": [["Triputen", "Sergey", ""], ["Gopal", "Atmaraaj", ""], ["Weber", "Thomas", ""], ["Hofert", "Christian", ""], ["Schreve", "Kristiaan", ""], ["Ratsch", "Matthias", ""]]}, {"id": "1803.02266", "submitter": "Demetris Marnerides", "authors": "Demetris Marnerides, Thomas Bashford-Rogers, Jonathan Hatchett and\n  Kurt Debattista", "title": "ExpandNet: A Deep Convolutional Neural Network for High Dynamic Range\n  Expansion from Low Dynamic Range Content", "comments": "Eurographics 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High dynamic range (HDR) imaging provides the capability of handling real\nworld lighting as opposed to the traditional low dynamic range (LDR) which\nstruggles to accurately represent images with higher dynamic range. However,\nmost imaging content is still available only in LDR. This paper presents a\nmethod for generating HDR content from LDR content based on deep Convolutional\nNeural Networks (CNNs) termed ExpandNet. ExpandNet accepts LDR images as input\nand generates images with an expanded range in an end-to-end fashion. The model\nattempts to reconstruct missing information that was lost from the original\nsignal due to quantization, clipping, tone mapping or gamma correction. The\nadded information is reconstructed from learned features, as the network is\ntrained in a supervised fashion using a dataset of HDR images. The approach is\nfully automatic and data driven; it does not require any heuristics or human\nexpertise. ExpandNet uses a multiscale architecture which avoids the use of\nupsampling layers to improve image quality. The method performs well compared\nto expansion/inverse tone mapping operators quantitatively on multiple metrics,\neven for badly exposed inputs.\n", "versions": [{"version": "v1", "created": "Tue, 6 Mar 2018 15:56:51 GMT"}, {"version": "v2", "created": "Wed, 4 Sep 2019 08:10:25 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["Marnerides", "Demetris", ""], ["Bashford-Rogers", "Thomas", ""], ["Hatchett", "Jonathan", ""], ["Debattista", "Kurt", ""]]}, {"id": "1803.02269", "submitter": "Huan Yang", "authors": "Huan Yang, Baoyuan Wang, Noranart Vesdapunt, Minyi Guo, Sing Bing Kang", "title": "Personalized Exposure Control Using Adaptive Metering and Reinforcement\n  Learning", "comments": "17 pages, 20 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a reinforcement learning approach for real-time exposure control\nof a mobile camera that is personalizable. Our approach is based on Markov\nDecision Process (MDP). In the camera viewfinder or live preview mode, given\nthe current frame, our system predicts the change in exposure so as to optimize\nthe trade-off among image quality, fast convergence, and minimal temporal\noscillation. We model the exposure prediction function as a fully convolutional\nneural network that can be trained through Gaussian policy gradient in an\nend-to-end fashion. As a result, our system can associate scene semantics with\nexposure values; it can also be extended to personalize the exposure\nadjustments for a user and device. We improve the learning performance by\nincorporating an adaptive metering module that links semantics with exposure.\nThis adaptive metering module generalizes the conventional spot or matrix\nmetering techniques. We validate our system using the MIT FiveK and our own\ndatasets captured using iPhone 7 and Google Pixel. Experimental results show\nthat our system exhibits stable real-time behavior while improving visual\nquality compared to what is achieved through native camera control.\n", "versions": [{"version": "v1", "created": "Tue, 6 Mar 2018 16:00:54 GMT"}, {"version": "v2", "created": "Sat, 30 Jun 2018 06:31:54 GMT"}, {"version": "v3", "created": "Sun, 5 Aug 2018 08:05:27 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Yang", "Huan", ""], ["Wang", "Baoyuan", ""], ["Vesdapunt", "Noranart", ""], ["Guo", "Minyi", ""], ["Kang", "Sing Bing", ""]]}, {"id": "1803.02276", "submitter": "Jianping Shi", "authors": "Zhichao Yin and Jianping Shi", "title": "GeoNet: Unsupervised Learning of Dense Depth, Optical Flow and Camera\n  Pose", "comments": "Accepted to CVPR 2018; Code will be made available at\n  https://github.com/yzcjtr/GeoNet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose GeoNet, a jointly unsupervised learning framework for monocular\ndepth, optical flow and ego-motion estimation from videos. The three components\nare coupled by the nature of 3D scene geometry, jointly learned by our\nframework in an end-to-end manner. Specifically, geometric relationships are\nextracted over the predictions of individual modules and then combined as an\nimage reconstruction loss, reasoning about static and dynamic scene parts\nseparately. Furthermore, we propose an adaptive geometric consistency loss to\nincrease robustness towards outliers and non-Lambertian regions, which resolves\nocclusions and texture ambiguities effectively. Experimentation on the KITTI\ndriving dataset reveals that our scheme achieves state-of-the-art results in\nall of the three tasks, performing better than previously unsupervised methods\nand comparably with supervised ones.\n", "versions": [{"version": "v1", "created": "Tue, 6 Mar 2018 16:09:21 GMT"}, {"version": "v2", "created": "Mon, 12 Mar 2018 03:02:07 GMT"}], "update_date": "2018-03-13", "authors_parsed": [["Yin", "Zhichao", ""], ["Shi", "Jianping", ""]]}, {"id": "1803.02284", "submitter": "Yuming Shen", "authors": "Yuming Shen, Li Liu, Fumin Shen, Ling Shao", "title": "Zero-Shot Sketch-Image Hashing", "comments": "Accepted as spotlight at CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies show that large-scale sketch-based image retrieval (SBIR) can\nbe efficiently tackled by cross-modal binary representation learning methods,\nwhere Hamming distance matching significantly speeds up the process of\nsimilarity search. Providing training and test data subjected to a fixed set of\npre-defined categories, the cutting-edge SBIR and cross-modal hashing works\nobtain acceptable retrieval performance. However, most of the existing methods\nfail when the categories of query sketches have never been seen during\ntraining. In this paper, the above problem is briefed as a novel but realistic\nzero-shot SBIR hashing task. We elaborate the challenges of this special task\nand accordingly propose a zero-shot sketch-image hashing (ZSIH) model. An\nend-to-end three-network architecture is built, two of which are treated as the\nbinary encoders. The third network mitigates the sketch-image heterogeneity and\nenhances the semantic relations among data by utilizing the Kronecker fusion\nlayer and graph convolution, respectively. As an important part of ZSIH, we\nformulate a generative hashing scheme in reconstructing semantic knowledge\nrepresentations for zero-shot retrieval. To the best of our knowledge, ZSIH is\nthe first zero-shot hashing work suitable for SBIR and cross-modal search.\nComprehensive experiments are conducted on two extended datasets, i.e., Sketchy\nand TU-Berlin with a novel zero-shot train-test split. The proposed model\nremarkably outperforms related works.\n", "versions": [{"version": "v1", "created": "Tue, 6 Mar 2018 16:23:44 GMT"}], "update_date": "2018-03-07", "authors_parsed": [["Shen", "Yuming", ""], ["Liu", "Li", ""], ["Shen", "Fumin", ""], ["Shao", "Ling", ""]]}, {"id": "1803.02285", "submitter": "Marcelo Saval Calvo", "authors": "Hanz Cuevas-Velasquez, Nanbo Li, Radim Tylecek, Marcelo Saval-Calvo\n  and Robert B. Fisher", "title": "Hybrid Multi-camera Visual Servoing to Moving Target", "comments": "6 pages, Published in IROS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual servoing is a well-known task in robotics. However, there are still\nchallenges when multiple visual sources are combined to accurately guide the\nrobot or occlusions appear. In this paper we present a novel visual servoing\napproach using hybrid multi-camera input data to lead a robot arm accurately to\ndynamically moving target points in the presence of partial occlusions. The\napproach uses four RGBD sensors as Eye-to-Hand (EtoH) visual input, and an\narm-mounted stereo camera as Eye-in-Hand (EinH). A Master supervisor task\nselects between using the EtoH or the EinH, depending on the distance between\nthe robot and target. The Master also selects the subset of EtoH cameras that\nbest perceive the target. When the EinH sensor is used, if the target becomes\noccluded or goes out of the sensor's view-frustum, the Master switches back to\nthe EtoH sensors to re-track the object. Using this adaptive visual input data,\nthe robot is then controlled using an iterative planner that uses position,\norientation and joint configuration to estimate the trajectory. Since the\ntarget is dynamic, this trajectory is updated every time-step. Experiments show\ngood performance in four different situations: tracking a ball, targeting a\nbulls-eye, guiding a straw to a mouth and delivering an item to a moving hand.\nThe experiments cover both simple situations such as a ball that is mostly\nvisible from all cameras, and more complex situations such as the mouth which\nis partially occluded from some of the sensors.\n", "versions": [{"version": "v1", "created": "Tue, 6 Mar 2018 16:25:15 GMT"}, {"version": "v2", "created": "Thu, 2 Aug 2018 08:59:14 GMT"}], "update_date": "2018-08-03", "authors_parsed": [["Cuevas-Velasquez", "Hanz", ""], ["Li", "Nanbo", ""], ["Tylecek", "Radim", ""], ["Saval-Calvo", "Marcelo", ""], ["Fisher", "Robert B.", ""]]}, {"id": "1803.02286", "submitter": "Cheng Zhao", "authors": "Cheng Zhao, Li Sun, Pulak Purkait, Tom Duckett and Rustam Stolkin", "title": "Learning monocular visual odometry with dense 3D mapping from dense 3D\n  flow", "comments": "International Conference on Intelligent Robots and Systems(IROS 2018)", "journal-ref": "International Conference on Intelligent Robots and Systems(IROS\n  2018)", "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a fully deep learning approach to monocular SLAM, which\ncan perform simultaneous localization using a neural network for learning\nvisual odometry (L-VO) and dense 3D mapping. Dense 2D flow and a depth image\nare generated from monocular images by sub-networks, which are then used by a\n3D flow associated layer in the L-VO network to generate dense 3D flow. Given\nthis 3D flow, the dual-stream L-VO network can then predict the 6DOF relative\npose and furthermore reconstruct the vehicle trajectory. In order to learn the\ncorrelation between motion directions, the Bivariate Gaussian modelling is\nemployed in the loss function. The L-VO network achieves an overall performance\nof 2.68% for average translational error and 0.0143 deg/m for average\nrotational error on the KITTI odometry benchmark. Moreover, the learned depth\nis fully leveraged to generate a dense 3D map. As a result, an entire visual\nSLAM system, that is, learning monocular odometry combined with dense 3D\nmapping, is achieved.\n", "versions": [{"version": "v1", "created": "Tue, 6 Mar 2018 16:26:06 GMT"}, {"version": "v2", "created": "Wed, 25 Jul 2018 15:40:15 GMT"}], "update_date": "2018-07-26", "authors_parsed": [["Zhao", "Cheng", ""], ["Sun", "Li", ""], ["Purkait", "Pulak", ""], ["Duckett", "Tom", ""], ["Stolkin", "Rustam", ""]]}, {"id": "1803.02310", "submitter": "Youngjun Cho", "authors": "Youngjun Cho, Nadia Bianchi-Berthouze, Nicolai Marquardt and Simon J.\n  Julier", "title": "Deep Thermal Imaging: Proximate Material Type Recognition in the Wild\n  through Deep Learning of Spatial Surface Temperature Patterns", "comments": "Proceedings of the 2018 CHI Conference on Human Factors in Computing\n  Systems", "journal-ref": null, "doi": "10.1145/3173574.3173576", "report-no": null, "categories": "cs.CV cond-mat.mtrl-sci cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Deep Thermal Imaging, a new approach for close-range automatic\nrecognition of materials to enhance the understanding of people and ubiquitous\ntechnologies of their proximal environment. Our approach uses a low-cost mobile\nthermal camera integrated into a smartphone to capture thermal textures. A deep\nneural network classifies these textures into material types. This approach\nworks effectively without the need for ambient light sources or direct contact\nwith materials. Furthermore, the use of a deep learning network removes the\nneed to handcraft the set of features for different materials. We evaluated the\nperformance of the system by training it to recognise 32 material types in both\nindoor and outdoor environments. Our approach produced recognition accuracies\nabove 98% in 14,860 images of 15 indoor materials and above 89% in 26,584\nimages of 17 outdoor materials. We conclude by discussing its potentials for\nreal-time use in HCI applications and future directions.\n", "versions": [{"version": "v1", "created": "Tue, 6 Mar 2018 17:29:08 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Cho", "Youngjun", ""], ["Bianchi-Berthouze", "Nadia", ""], ["Marquardt", "Nicolai", ""], ["Julier", "Simon J.", ""]]}, {"id": "1803.02315", "submitter": "Ivo Matteo Baltruschat", "authors": "Ivo M. Baltruschat, Hannes Nickisch, Michael Grass, Tobias Knopp, Axel\n  Saalbach", "title": "Comparison of Deep Learning Approaches for Multi-Label Chest X-Ray\n  Classification", "comments": "added official split, non-image feature investigation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increased availability of X-ray image archives (e.g. the ChestX-ray14\ndataset from the NIH Clinical Center) has triggered a growing interest in deep\nlearning techniques. To provide better insight into the different approaches,\nand their applications to chest X-ray classification, we investigate a powerful\nnetwork architecture in detail: the ResNet-50. Building on prior work in this\ndomain, we consider transfer learning with and without fine-tuning as well as\nthe training of a dedicated X-ray network from scratch. To leverage the high\nspatial resolution of X-ray data, we also include an extended ResNet-50\narchitecture, and a network integrating non-image data (patient age, gender and\nacquisition type) in the classification process. In a concluding experiment, we\nalso investigate multiple ResNet depths (i.e. ResNet-38 and ResNet-101). In a\nsystematic evaluation, using 5-fold re-sampling and a multi-label loss\nfunction, we compare the performance of the different approaches for pathology\nclassification by ROC statistics and analyze differences between the\nclassifiers using rank correlation. Overall, we observe a considerable spread\nin the achieved performance and conclude that the X-ray-specific ResNet-38,\nintegrating non-image data yields the best overall results. Furthermore, class\nactivation maps are used to understand the classification process, and a\ndetailed analysis of the impact of non-image features is provided.\n", "versions": [{"version": "v1", "created": "Tue, 6 Mar 2018 18:04:25 GMT"}, {"version": "v2", "created": "Tue, 29 Jan 2019 17:10:24 GMT"}], "update_date": "2019-01-30", "authors_parsed": [["Baltruschat", "Ivo M.", ""], ["Nickisch", "Hannes", ""], ["Grass", "Michael", ""], ["Knopp", "Tobias", ""], ["Saalbach", "Axel", ""]]}, {"id": "1803.02326", "submitter": "Hung Luu Viet", "authors": "Hung V. Luu, Manh V. Pham, Chuc D. Man, Hung Q. Bui, Thanh T.N. Nguyen", "title": "Comparison of various image fusion methods for impervious surface\n  classification from VNREDSat-1", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Impervious surface is an important indicator for urban development\nmonitoring. Accurate urban impervious surfaces mapping with VNREDSat-1 remains\nchallenging due to their spectral diversity not captured by individual PAN\nimage. In this artical, five multi-resolution image fusion techniques were\ncompared for classification task of urban impervious surface. The result shows\nthat for VNREDSat-1 dataset, UNB and Wavelet tranform methods are the best\ntechniques reserving spatial and spectral information of original MS image,\nrespectively. However, the UNB technique gives best results when it comes to\nimpervious surface classification especially in the case of shadow area\nincluded in non-impervious surface group.\n", "versions": [{"version": "v1", "created": "Tue, 6 Mar 2018 18:29:45 GMT"}, {"version": "v2", "created": "Fri, 4 May 2018 09:29:28 GMT"}], "update_date": "2018-05-07", "authors_parsed": [["Luu", "Hung V.", ""], ["Pham", "Manh V.", ""], ["Man", "Chuc D.", ""], ["Bui", "Hung Q.", ""], ["Nguyen", "Thanh T. N.", ""]]}, {"id": "1803.02380", "submitter": "Pedro F. Proen\\c{c}a", "authors": "Pedro F. Proen\\c{c}a and Yang Gao", "title": "Fast Cylinder and Plane Extraction from Depth Cameras for Visual\n  Odometry", "comments": "Accepted to IROS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents CAPE, a method to extract planes and cylinder segments\nfrom organized point clouds, which processes 640x480 depth images on a single\nCPU core at an average of 300 Hz, by operating on a grid of planar cells.\nWhile, compared to state-of-the-art plane extraction, the latency of CAPE is\nmore consistent and 4-10 times faster, depending on the scene, we also\ndemonstrate empirically that applying CAPE to visual odometry can improve\ntrajectory estimation on scenes made of cylindrical surfaces (e.g. tunnels),\nwhereas using a plane extraction approach that is not curve-aware deteriorates\nperformance on these scenes. To use these geometric primitives in visual\nodometry, we propose extending a probabilistic RGB-D odometry framework based\non points, lines and planes to cylinder primitives. Following this framework,\nCAPE runs on fused depth maps and the parameters of cylinders are modelled\nprobabilistically to account for uncertainty and weight accordingly the pose\noptimization residuals.\n", "versions": [{"version": "v1", "created": "Tue, 6 Mar 2018 19:07:45 GMT"}, {"version": "v2", "created": "Thu, 8 Mar 2018 11:36:45 GMT"}, {"version": "v3", "created": "Thu, 5 Jul 2018 11:17:01 GMT"}], "update_date": "2018-07-06", "authors_parsed": [["Proen\u00e7a", "Pedro F.", ""], ["Gao", "Yang", ""]]}, {"id": "1803.02403", "submitter": "Feng Zheng", "authors": "Feng Zheng, Grace Tsai, Zhe Zhang, Shaoshan Liu, Chen-Chi Chu, and\n  Hongbing Hu", "title": "Trifo-VIO: Robust and Efficient Stereo Visual Inertial Odometry using\n  Points and Lines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present the Trifo Visual Inertial Odometry (Trifo-VIO), a\ntightly-coupled filtering-based stereo VIO system using both points and lines.\nLine features help improve system robustness in challenging scenarios when\npoint features cannot be reliably detected or tracked, e.g. low-texture\nenvironment or lighting change. In addition, we propose a novel lightweight\nfiltering-based loop closing technique to reduce accumulated drift without\nglobal bundle adjustment or pose graph optimization. We formulate loop closure\nas EKF updates to optimally relocate the current sliding window maintained by\nthe filter to past keyframes. We also present the Trifo Ironsides dataset, a\nnew visual-inertial dataset, featuring high-quality synchronized stereo camera\nand IMU data from the Ironsides sensor [3] with various motion types and\ntextures and millimeter-accuracy groundtruth. To validate the performance of\nthe proposed system, we conduct extensive comparison with state-of-the-art\napproaches (OKVIS, VINS-MONO and S-MSCKF) using both the public EuRoC dataset\nand the Trifo Ironsides dataset.\n", "versions": [{"version": "v1", "created": "Tue, 6 Mar 2018 19:49:00 GMT"}, {"version": "v2", "created": "Tue, 18 Sep 2018 23:05:27 GMT"}], "update_date": "2018-09-20", "authors_parsed": [["Zheng", "Feng", ""], ["Tsai", "Grace", ""], ["Zhang", "Zhe", ""], ["Liu", "Shaoshan", ""], ["Chu", "Chen-Chi", ""], ["Hu", "Hongbing", ""]]}, {"id": "1803.02446", "submitter": "Seab Billings", "authors": "Sean Billings", "title": "Categorical Mixture Models on VGGNet activations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this project, I use unsupervised learning techniques in order to cluster a\nset of yelp restaurant photos under meaningful topics. In order to do this, I\nextract layer activations from a pre-trained implementation of the popular\nVGGNet convolutional neural network. First, I explore using LDA with the\nactivations of convolutional layers as features. Secondly, I explore using the\nobject-recognition powers of VGGNet trained on ImageNet in order to extract\nmeaningful objects from the photos, and then perform LDA to group the photos\nunder topic-archetypes. I find that this second approach finds meaningful\narchetypes, which match the human intuition for photo topics such as\nrestaurant, food, and drinks. Furthermore, these clusters align well and\ndistinctly with the actual yelp photo labels.\n", "versions": [{"version": "v1", "created": "Tue, 6 Mar 2018 22:20:10 GMT"}], "update_date": "2018-03-08", "authors_parsed": [["Billings", "Sean", ""]]}, {"id": "1803.02504", "submitter": "Bowen Wu", "authors": "Bowen Wu, Zhangling Chen, Jun Wang, Huaming Wu", "title": "Exponential Discriminative Metric Embedding in Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the remarkable success achieved by the Convolutional Neural Networks\n(CNNs) in object recognition recently, deep learning is being widely used in\nthe computer vision community. Deep Metric Learning (DML), integrating deep\nlearning with conventional metric learning, has set new records in many fields,\nespecially in classification task. In this paper, we propose a replicable DML\nmethod, called Include and Exclude (IE) loss, to force the distance between a\nsample and its designated class center away from the mean distance of this\nsample to other class centers with a large margin in the exponential feature\nprojection space. With the supervision of IE loss, we can train CNNs to enhance\nthe intra-class compactness and inter-class separability, leading to great\nimprovements on several public datasets ranging from object recognition to face\nverification. We conduct a comparative study of our algorithm with several\ntypical DML methods on three kinds of networks with different capacity.\nExtensive experiments on three object recognition datasets and two face\nrecognition datasets demonstrate that IE loss is always superior to other\nmainstream DML methods and approach the state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Wed, 7 Mar 2018 02:39:34 GMT"}], "update_date": "2018-03-08", "authors_parsed": [["Wu", "Bowen", ""], ["Chen", "Zhangling", ""], ["Wang", "Jun", ""], ["Wu", "Huaming", ""]]}, {"id": "1803.02518", "submitter": "Jing Wu", "authors": "Jing Wu", "title": "Rigid Point Registration with Expectation Conditional Maximization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the issue of matching rigid 3D object points with 2D\nimage points through point registration based on maximum likelihood principle\nin computer simulated images. Perspective projection is necessary when\ntransforming 3D coordinate into 2D. The problem then recasts into a missing\ndata framework where unknown correspondences are handled via mixture models.\nAdopting the Expectation Conditional Maximization for Point Registration\n(ECMPR), two different rotation and translation optimization algorithms are\ncompared in this paper. We analyze in detail the associated consequences in\nterms of estimation of the registration parameters theoretically and\nexperimentally.\n", "versions": [{"version": "v1", "created": "Wed, 7 Mar 2018 03:59:05 GMT"}], "update_date": "2018-03-08", "authors_parsed": [["Wu", "Jing", ""]]}, {"id": "1803.02536", "submitter": "Xingxing Wei", "authors": "Xingxing Wei, Jun Zhu, Hang Su", "title": "Sparse Adversarial Perturbations for Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although adversarial samples of deep neural networks (DNNs) have been\nintensively studied on static images, their extensions in videos are never\nexplored. Compared with images, attacking a video needs to consider not only\nspatial cues but also temporal cues. Moreover, to improve the imperceptibility\nas well as reduce the computation cost, perturbations should be added on as\nfewer frames as possible, i.e., adversarial perturbations are temporally\nsparse. This further motivates the propagation of perturbations, which denotes\nthat perturbations added on the current frame can transfer to the next frames\nvia their temporal interactions. Thus, no (or few) extra perturbations are\nneeded for these frames to misclassify them. To this end, we propose an\nl2,1-norm based optimization algorithm to compute the sparse adversarial\nperturbations for videos. We choose the action recognition as the targeted\ntask, and networks with a CNN+RNN architecture as threat models to verify our\nmethod. Thanks to the propagation, we can compute perturbations on a shortened\nversion video, and then adapt them to the long version video to fool DNNs.\nExperimental results on the UCF101 dataset demonstrate that even only one frame\nin a video is perturbed, the fooling rate can still reach 59.7%.\n", "versions": [{"version": "v1", "created": "Wed, 7 Mar 2018 06:28:43 GMT"}], "update_date": "2018-03-08", "authors_parsed": [["Wei", "Xingxing", ""], ["Zhu", "Jun", ""], ["Su", "Hang", ""]]}, {"id": "1803.02544", "submitter": "Chengliang Yang", "authors": "Chengliang Yang, Anand Rangarajan, Sanjay Ranka", "title": "Visual Explanations From Deep 3D Convolutional Neural Networks for\n  Alzheimer's Disease Classification", "comments": "Accepted by 2018 American Medical Informatics Association Annual\n  Symposium (AMIA2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop three efficient approaches for generating visual explanations from\n3D convolutional neural networks (3D-CNNs) for Alzheimer's disease\nclassification. One approach conducts sensitivity analysis on hierarchical 3D\nimage segmentation, and the other two visualize network activations on a\nspatial map. Visual checks and a quantitative localization benchmark indicate\nthat all approaches identify important brain parts for Alzheimer's disease\ndiagnosis. Comparative analysis show that the sensitivity analysis based\napproach has difficulty handling loosely distributed cerebral cortex, and\napproaches based on visualization of activations are constrained by the\nresolution of the convolutional layer. The complementarity of these methods\nimproves the understanding of 3D-CNNs in Alzheimer's disease classification\nfrom different perspectives.\n", "versions": [{"version": "v1", "created": "Wed, 7 Mar 2018 07:07:39 GMT"}, {"version": "v2", "created": "Thu, 8 Mar 2018 01:29:14 GMT"}, {"version": "v3", "created": "Fri, 6 Jul 2018 00:28:49 GMT"}], "update_date": "2018-07-09", "authors_parsed": [["Yang", "Chengliang", ""], ["Rangarajan", "Anand", ""], ["Ranka", "Sanjay", ""]]}, {"id": "1803.02547", "submitter": "Chaojie Mao", "authors": "Chaojie Mao, Yingming Li, Zhongfei Zhang, Yaqing Zhang and Xi Li", "title": "Pyramid Person Matching Network for Person Re-identification", "comments": "11pages, 3 figures, 4 tables and accepted by Proceedings of 9th Asian\n  Conference on Machine Learning (ACML2017) JMLR Workshop and Conference\n  Proceedings, vol. 77, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present a deep convolutional pyramid person matching network\n(PPMN) with specially designed Pyramid Matching Module to address the problem\nof person re-identification. The architecture takes a pair of RGB images as\ninput, and outputs a similiarity value indicating whether the two input images\nrepresent the same person or not. Based on deep convolutional neural networks,\nour approach first learns the discriminative semantic representation with the\nsemantic-component-aware features for persons and then employs the Pyramid\nMatching Module to match the common semantic-components of persons, which is\nrobust to the variation of spatial scales and misalignment of locations posed\nby viewpoint changes. The above two processes are jointly optimized via a\nunified end-to-end deep learning scheme. Extensive experiments on several\nbenchmark datasets demonstrate the effectiveness of our approach against the\nstate-of-the-art approaches, especially on the rank-1 recognition rate.\n", "versions": [{"version": "v1", "created": "Wed, 7 Mar 2018 07:21:44 GMT"}], "update_date": "2018-03-08", "authors_parsed": [["Mao", "Chaojie", ""], ["Li", "Yingming", ""], ["Zhang", "Zhongfei", ""], ["Zhang", "Yaqing", ""], ["Li", "Xi", ""]]}, {"id": "1803.02555", "submitter": "Prerana Mukherjee", "authors": "Prerana Mukherjee, Brejesh Lall, Snehith Lattupally", "title": "Object cosegmentation using deep Siamese network", "comments": "Appears in International Conference on Pattern Recognition and\n  Artificial Intelligence (ICPRAI), 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object cosegmentation addresses the problem of discovering similar objects\nfrom multiple images and segmenting them as foreground simultaneously. In this\npaper, we propose a novel end-to-end pipeline to segment the similar objects\nsimultaneously from relevant set of images using supervised learning via\ndeep-learning framework. We experiment with multiple set of object proposal\ngeneration techniques and perform extensive numerical evaluations by training\nthe Siamese network with generated object proposals. Similar objects proposals\nfor the test images are retrieved using the ANNOY (Approximate Nearest\nNeighbor) library and deep semantic segmentation is performed on them. Finally,\nwe form a collage from the segmented similar objects based on the relative\nimportance of the objects.\n", "versions": [{"version": "v1", "created": "Wed, 7 Mar 2018 07:49:29 GMT"}, {"version": "v2", "created": "Thu, 8 Mar 2018 08:35:30 GMT"}], "update_date": "2018-03-09", "authors_parsed": [["Mukherjee", "Prerana", ""], ["Lall", "Brejesh", ""], ["Lattupally", "Snehith", ""]]}, {"id": "1803.02558", "submitter": "Chaojie Mao", "authors": "Chaojie Mao, Yingming Li, Yaqing Zhang, Zhongfei Zhang, and Xi Li", "title": "Multi-Channel Pyramid Person Matching Network for Person\n  Re-Identification", "comments": "9 pages, 5 figures, 7 tables and accepted by the 32nd AAAI Conference\n  on Artificial Intelligence", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present a Multi-Channel deep convolutional Pyramid Person\nMatching Network (MC-PPMN) based on the combination of the semantic-components\nand the color-texture distributions to address the problem of person\nre-identification. In particular, we learn separate deep representations for\nsemantic-components and color-texture distributions from two person images and\nthen employ pyramid person matching network (PPMN) to obtain correspondence\nrepresentations. These correspondence representations are fused to perform the\nre-identification task. Further, the proposed framework is optimized via a\nunified end-to-end deep learning scheme. Extensive experiments on several\nbenchmark datasets demonstrate the effectiveness of our approach against the\nstate-of-the-art literature, especially on the rank-1 recognition rate.\n", "versions": [{"version": "v1", "created": "Wed, 7 Mar 2018 08:10:26 GMT"}], "update_date": "2018-03-08", "authors_parsed": [["Mao", "Chaojie", ""], ["Li", "Yingming", ""], ["Zhang", "Yaqing", ""], ["Zhang", "Zhongfei", ""], ["Li", "Xi", ""]]}, {"id": "1803.02563", "submitter": "Tianyi Zhang", "authors": "Tianyi Zhang, Guosheng Lin, Jianfei Cai, Tong Shen, Chunhua Shen, Alex\n  C. Kot", "title": "Decoupled Spatial Neural Attention for Weakly Supervised Semantic\n  Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weakly supervised semantic segmentation receives much research attention\nsince it alleviates the need to obtain a large amount of dense pixel-wise\nground-truth annotations for the training images. Compared with other forms of\nweak supervision, image labels are quite efficient to obtain. In our work, we\nfocus on the weakly supervised semantic segmentation with image label\nannotations. Recent progress for this task has been largely dependent on the\nquality of generated pseudo-annotations. In this work, inspired by spatial\nneural-attention for image captioning, we propose a decoupled spatial neural\nattention network for generating pseudo-annotations. Our decoupled attention\nstructure could simultaneously identify the object regions and localize the\ndiscriminative parts which generates high-quality pseudo-annotations in one\nforward path. The generated pseudo-annotations lead to the segmentation results\nwhich achieve the state-of-the-art in weakly-supervised semantic segmentation.\n", "versions": [{"version": "v1", "created": "Wed, 7 Mar 2018 08:59:35 GMT"}], "update_date": "2018-03-08", "authors_parsed": [["Zhang", "Tianyi", ""], ["Lin", "Guosheng", ""], ["Cai", "Jianfei", ""], ["Shen", "Tong", ""], ["Shen", "Chunhua", ""], ["Kot", "Alex C.", ""]]}, {"id": "1803.02578", "submitter": "Minkyu Choi", "authors": "Minkyu Choi, Takazumi Matsumoto, Minju Jung, Jun Tani", "title": "Generating Goal-Directed Visuomotor Plans Based on Learning Using a\n  Predictive Coding-type Deep Visuomotor Recurrent Neural Network Model", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The current paper presents how a predictive coding type deep recurrent neural\nnetworks can generate vision-based goal-directed plans based on prior learning\nexperience by examining experiment results using a real arm robot. The proposed\ndeep recurrent neural network learns to predict visuo-proprioceptive sequences\nby extracting an adequate predictive model from various visuomotor experiences\nrelated to object-directed behaviors. The predictive model was developed in\nterms of mapping from intention state space to expected visuo-proprioceptive\nsequences space through iterative learning. Our arm robot experiments adopted\nwith three different tasks with different levels of difficulty showed that the\nerror minimization principle in the predictive coding framework applied to\ninference of the optimal intention states for given goal states can generate\ngoal-directed plans even for unlearned goal states with generalization. It was,\nhowever, shown that sufficient generalization requires relatively large number\nof learning trajectories. The paper discusses possible countermeasure to\novercome this problem.\n", "versions": [{"version": "v1", "created": "Wed, 7 Mar 2018 10:21:08 GMT"}, {"version": "v2", "created": "Tue, 5 Jun 2018 06:24:42 GMT"}], "update_date": "2018-06-06", "authors_parsed": [["Choi", "Minkyu", ""], ["Matsumoto", "Takazumi", ""], ["Jung", "Minju", ""], ["Tani", "Jun", ""]]}, {"id": "1803.02579", "submitter": "Abhijit Guha Roy", "authors": "Abhijit Guha Roy, Nassir Navab, Christian Wachinger", "title": "Concurrent Spatial and Channel Squeeze & Excitation in Fully\n  Convolutional Networks", "comments": "Accepted at MICCAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fully convolutional neural networks (F-CNNs) have set the state-of-the-art in\nimage segmentation for a plethora of applications. Architectural innovations\nwithin F-CNNs have mainly focused on improving spatial encoding or network\nconnectivity to aid gradient flow. In this paper, we explore an alternate\ndirection of recalibrating the feature maps adaptively, to boost meaningful\nfeatures, while suppressing weak ones. We draw inspiration from the recently\nproposed squeeze & excitation (SE) module for channel recalibration of feature\nmaps for image classification. Towards this end, we introduce three variants of\nSE modules for image segmentation, (i) squeezing spatially and exciting\nchannel-wise (cSE), (ii) squeezing channel-wise and exciting spatially (sSE)\nand (iii) concurrent spatial and channel squeeze & excitation (scSE). We\neffectively incorporate these SE modules within three different\nstate-of-the-art F-CNNs (DenseNet, SD-Net, U-Net) and observe consistent\nimprovement of performance across all architectures, while minimally effecting\nmodel complexity. Evaluations are performed on two challenging applications:\nwhole brain segmentation on MRI scans (Multi-Atlas Labelling Challenge Dataset)\nand organ segmentation on whole body contrast enhanced CT scans (Visceral\nDataset).\n", "versions": [{"version": "v1", "created": "Wed, 7 Mar 2018 10:22:06 GMT"}, {"version": "v2", "created": "Fri, 8 Jun 2018 15:46:44 GMT"}], "update_date": "2018-06-11", "authors_parsed": [["Roy", "Abhijit Guha", ""], ["Navab", "Nassir", ""], ["Wachinger", "Christian", ""]]}, {"id": "1803.02612", "submitter": "Yue Luo", "authors": "Yue Luo, Jimmy Ren, Mude Lin, Jiahao Pang, Wenxiu Sun, Hongsheng Li,\n  Liang Lin", "title": "Single View Stereo Matching", "comments": "Spotlight in IEEE Conference on Computer Vision and Pattern\n  Recognition (CVPR), 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous monocular depth estimation methods take a single view and directly\nregress the expected results. Though recent advances are made by applying\ngeometrically inspired loss functions during training, the inference procedure\ndoes not explicitly impose any geometrical constraint. Therefore these models\npurely rely on the quality of data and the effectiveness of learning to\ngeneralize. This either leads to suboptimal results or the demand of huge\namount of expensive ground truth labelled data to generate reasonable results.\nIn this paper, we show for the first time that the monocular depth estimation\nproblem can be reformulated as two sub-problems, a view synthesis procedure\nfollowed by stereo matching, with two intriguing properties, namely i)\ngeometrical constraints can be explicitly imposed during inference; ii) demand\non labelled depth data can be greatly alleviated. We show that the whole\npipeline can still be trained in an end-to-end fashion and this new formulation\nplays a critical role in advancing the performance. The resulting model\noutperforms all the previous monocular depth estimation methods as well as the\nstereo block matching method in the challenging KITTI dataset by only using a\nsmall number of real training data. The model also generalizes well to other\nmonocular depth estimation benchmarks. We also discuss the implications and the\nadvantages of solving monocular depth estimation using stereo methods.\n", "versions": [{"version": "v1", "created": "Wed, 7 Mar 2018 12:07:23 GMT"}, {"version": "v2", "created": "Fri, 9 Mar 2018 07:47:18 GMT"}], "update_date": "2018-03-12", "authors_parsed": [["Luo", "Yue", ""], ["Ren", "Jimmy", ""], ["Lin", "Mude", ""], ["Pang", "Jiahao", ""], ["Sun", "Wenxiu", ""], ["Li", "Hongsheng", ""], ["Lin", "Liang", ""]]}, {"id": "1803.02622", "submitter": "Christian Zimmermann", "authors": "Christian Zimmermann, Tim Welschehold, Christian Dornhege, Wolfram\n  Burgard and Thomas Brox", "title": "3D Human Pose Estimation in RGBD Images for Robotic Task Learning", "comments": "Accepted to ICRA 2018. Video and Code (ROS node) are available:\n  http://lmb.informatik.uni-freiburg.de/projects/rgbd-pose3d/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an approach to estimate 3D human pose in real world units from a\nsingle RGBD image and show that it exceeds performance of monocular 3D pose\nestimation approaches from color as well as pose estimation exclusively from\ndepth. Our approach builds on robust human keypoint detectors for color images\nand incorporates depth for lifting into 3D. We combine the system with our\nlearning from demonstration framework to instruct a service robot without the\nneed of markers. Experiments in real world settings demonstrate that our\napproach enables a PR2 robot to imitate manipulation actions observed from a\nhuman teacher.\n", "versions": [{"version": "v1", "created": "Wed, 7 Mar 2018 12:46:18 GMT"}, {"version": "v2", "created": "Tue, 13 Mar 2018 10:18:18 GMT"}], "update_date": "2018-03-14", "authors_parsed": [["Zimmermann", "Christian", ""], ["Welschehold", "Tim", ""], ["Dornhege", "Christian", ""], ["Burgard", "Wolfram", ""], ["Brox", "Thomas", ""]]}, {"id": "1803.02623", "submitter": "Behrouz Bolourian Haghighi", "authors": "Behrouz Bolourian Haghighi, Amir Hossein Taherinia, Amir Hossein\n  Mohajerzadeh", "title": "TRLG: Fragile blind quad watermarking for image tamper detection and\n  recovery by providing compact digests with quality optimized using LWT and GA", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, an efficient fragile blind quad watermarking scheme for image\ntamper detection and recovery based on lifting wavelet transform and genetic\nalgorithm is proposed. TRLG generates four compact digests with super quality\nbased on lifting wavelet transform and halftoning technique by distinguishing\nthe types of image blocks. In other words, for each 2*2 non-overlap blocks,\nfour chances for recovering destroyed blocks are considered. A special\nparameter estimation technique based on genetic algorithm is performed to\nimprove and optimize the quality of digests and watermarked image. Furthermore,\nCCS map is used to determine the mapping block for embedding information,\nencrypting and confusing the embedded information. In order to improve the\nrecovery rate, Mirror-aside and Partner-block are proposed. The experiments\nthat have been conducted to evaluate the performance of TRLG proved the\nsuperiority in terms of quality of the watermarked and recovered image, tamper\nlocalization and security compared with state-of-the-art methods. The results\nindicate that the PSNR and SSIM of the watermarked image are about 46 dB and\napproximately one, respectively. Also, the mean of PSNR and SSIM of several\nrecovered images which has been destroyed about 90% is reached to 24 dB and\n0.86, respectively.\n", "versions": [{"version": "v1", "created": "Wed, 7 Mar 2018 12:47:18 GMT"}], "update_date": "2018-03-09", "authors_parsed": [["Haghighi", "Behrouz Bolourian", ""], ["Taherinia", "Amir Hossein", ""], ["Mohajerzadeh", "Amir Hossein", ""]]}, {"id": "1803.02627", "submitter": "Tobias Hinz", "authors": "Tobias Hinz, Stefan Wermter", "title": "Inferencing Based on Unsupervised Learning of Disentangled\n  Representations", "comments": "Accepted as a conference paper at the European Symposium on\n  Artificial Neural Networks, Computational Intelligence and Machine Learning\n  (ESANN) 2018, 6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Combining Generative Adversarial Networks (GANs) with encoders that learn to\nencode data points has shown promising results in learning data representations\nin an unsupervised way. We propose a framework that combines an encoder and a\ngenerator to learn disentangled representations which encode meaningful\ninformation about the data distribution without the need for any labels. While\ncurrent approaches focus mostly on the generative aspects of GANs, our\nframework can be used to perform inference on both real and generated data\npoints. Experiments on several data sets show that the encoder learns\ninterpretable, disentangled representations which encode descriptive properties\nand can be used to sample images that exhibit specific characteristics.\n", "versions": [{"version": "v1", "created": "Wed, 7 Mar 2018 12:58:53 GMT"}], "update_date": "2018-03-08", "authors_parsed": [["Hinz", "Tobias", ""], ["Wermter", "Stefan", ""]]}, {"id": "1803.02642", "submitter": "Lichao Mou", "authors": "Lichao Mou and Lorenzo Bruzzone and Xiao Xiang Zhu", "title": "Learning Spectral-Spatial-Temporal Features via a Recurrent\n  Convolutional Neural Network for Change Detection in Multispectral Imagery", "comments": null, "journal-ref": null, "doi": "10.1109/TGRS.2018.2863224", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Change detection is one of the central problems in earth observation and was\nextensively investigated over recent decades. In this paper, we propose a novel\nrecurrent convolutional neural network (ReCNN) architecture, which is trained\nto learn a joint spectral-spatial-temporal feature representation in a unified\nframework for change detection in multispectral images. To this end, we bring\ntogether a convolutional neural network (CNN) and a recurrent neural network\n(RNN) into one end-to-end network. The former is able to generate rich\nspectral-spatial feature representations, while the latter effectively analyzes\ntemporal dependency in bi-temporal images. In comparison with previous\napproaches to change detection, the proposed network architecture possesses\nthree distinctive properties: 1) It is end-to-end trainable, in contrast to\nmost existing methods whose components are separately trained or computed; 2)\nit naturally harnesses spatial information that has been proven to be\nbeneficial to change detection task; 3) it is capable of adaptively learning\nthe temporal dependency between multitemporal images, unlike most of algorithms\nthat use fairly simple operation like image differencing or stacking. As far as\nwe know, this is the first time that a recurrent convolutional network\narchitecture has been proposed for multitemporal remote sensing image analysis.\nThe proposed network is validated on real multispectral data sets. Both visual\nand quantitative analysis of experimental results demonstrates competitive\nperformance in the proposed mode.\n", "versions": [{"version": "v1", "created": "Wed, 7 Mar 2018 13:30:59 GMT"}], "update_date": "2019-03-27", "authors_parsed": [["Mou", "Lichao", ""], ["Bruzzone", "Lorenzo", ""], ["Zhu", "Xiao Xiang", ""]]}, {"id": "1803.02699", "submitter": "Rui Kang", "authors": "Rui Kang and Yixiong Liang and Chunyan Lian and Yuan Mao", "title": "CNN-Based Automatic Urinary Particles Recognition", "comments": "The manuscript has been submitted to Journal of Medical Systems on\n  Jul 02. 2017", "journal-ref": null, "doi": "10.1007/s10916-018-1014-6", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The urine sediment analysis of particles in microscopic images can assist\nphysicians in evaluating patients with renal and urinary tract diseases. Manual\nurine sediment examination is labor-intensive, subjective and time-consuming,\nand the traditional automatic algorithms often extract the hand-crafted\nfeatures for recognition. Instead of using the hand-crafted features, in this\npaper, we exploit CNN to learn features in an end-to-end manner to recognize\nthe urine particles. We treat the urine particles recognition as object\ndetection and exploit two state-of-the-art CNN-based object detection methods,\nFaster R-CNN and SSD, as well as their variants for urine particles\nrecognition. We further investigate different factors involving these CNN-based\nobject detection methods for urine particles recognition. We comprehensively\nevaluate these methods on a dataset consisting of 5,376 annotated images\ncorresponding to 7 categories of urine particles, i.e., erythrocyte, leukocyte,\nepithelial cell, crystal, cast, mycete, epithelial nuclei, and obtain a best\nmAP (mean average precision) of 84.1% while taking only 72 ms per image on a\nNVIDIA Titan X GPU.\n", "versions": [{"version": "v1", "created": "Tue, 6 Mar 2018 02:26:43 GMT"}], "update_date": "2020-01-20", "authors_parsed": [["Kang", "Rui", ""], ["Liang", "Yixiong", ""], ["Lian", "Chunyan", ""], ["Mao", "Yuan", ""]]}, {"id": "1803.02735", "submitter": "Muhammad Haris", "authors": "Muhammad Haris, Greg Shakhnarovich, Norimichi Ukita", "title": "Deep Back-Projection Networks For Super-Resolution", "comments": "To appear in CVPR2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The feed-forward architectures of recently proposed deep super-resolution\nnetworks learn representations of low-resolution inputs, and the non-linear\nmapping from those to high-resolution output. However, this approach does not\nfully address the mutual dependencies of low- and high-resolution images. We\npropose Deep Back-Projection Networks (DBPN), that exploit iterative up- and\ndown-sampling layers, providing an error feedback mechanism for projection\nerrors at each stage. We construct mutually-connected up- and down-sampling\nstages each of which represents different types of image degradation and\nhigh-resolution components. We show that extending this idea to allow\nconcatenation of features across up- and down-sampling stages (Dense DBPN)\nallows us to reconstruct further improve super-resolution, yielding superior\nresults and in particular establishing new state of the art results for large\nscaling factors such as 8x across multiple data sets.\n", "versions": [{"version": "v1", "created": "Wed, 7 Mar 2018 16:05:35 GMT"}], "update_date": "2018-03-08", "authors_parsed": [["Haris", "Muhammad", ""], ["Shakhnarovich", "Greg", ""], ["Ukita", "Norimichi", ""]]}, {"id": "1803.02742", "submitter": "Ruixin Zhang", "authors": "Qiuyu Zhu, Ruixin Zhang", "title": "HENet:A Highly Efficient Convolutional Neural Networks Optimized for\n  Accuracy, Speed and Storage", "comments": "11 pages,3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to enhance the real-time performance of convolutional neural\nnetworks(CNNs), more and more researchers are focusing on improving the\nefficiency of CNN. Based on the analysis of some CNN architectures, such as\nResNet, DenseNet, ShuffleNet and so on, we combined their advantages and\nproposed a very efficient model called Highly Efficient Networks(HENet). The\nnew architecture uses an unusual way to combine group convolution and channel\nshuffle which was mentioned in ShuffleNet. Inspired by ResNet and DenseNet, we\nalso proposed a new way to use element-wise addition and concatenation\nconnection with each block. In order to make greater use of feature maps,\npooling operations are removed from HENet. The experiments show that our\nmodel's efficiency is more than 1 times higher than ShuffleNet on many open\nsource datasets, such as CIFAR-10/100 and SVHN.\n", "versions": [{"version": "v1", "created": "Wed, 7 Mar 2018 16:18:51 GMT"}, {"version": "v2", "created": "Thu, 15 Mar 2018 15:18:24 GMT"}], "update_date": "2018-03-16", "authors_parsed": [["Zhu", "Qiuyu", ""], ["Zhang", "Ruixin", ""]]}, {"id": "1803.02758", "submitter": "Mennatullah Siam M.S.", "authors": "Mennatullah Siam, Mostafa Gamal, Moemen Abdel-Razek, Senthil Yogamani,\n  Martin Jagersand", "title": "RTSeg: Real-time Semantic Segmentation Comparative Study", "comments": "Accepted in IEEE ICIP 2018. IEEE Copyrights: Personal use of this\n  material is permitted. Permission from IEEE must be obtained for all other\n  uses", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic segmentation benefits robotics related applications especially\nautonomous driving. Most of the research on semantic segmentation is only on\nincreasing the accuracy of segmentation models with little attention to\ncomputationally efficient solutions. The few work conducted in this direction\ndoes not provide principled methods to evaluate the different design choices\nfor segmentation. In this paper, we address this gap by presenting a real-time\nsemantic segmentation benchmarking framework with a decoupled design for\nfeature extraction and decoding methods. The framework is comprised of\ndifferent network architectures for feature extraction such as VGG16, Resnet18,\nMobileNet, and ShuffleNet. It is also comprised of multiple meta-architectures\nfor segmentation that define the decoding methodology. These include SkipNet,\nUNet, and Dilation Frontend. Experimental results are presented on the\nCityscapes dataset for urban scenes. The modular design allows novel\narchitectures to emerge, that lead to 143x GFLOPs reduction in comparison to\nSegNet. This benchmarking framework is publicly available at\n\"https://github.com/MSiam/TFSegmentation\".\n", "versions": [{"version": "v1", "created": "Wed, 7 Mar 2018 16:49:48 GMT"}, {"version": "v2", "created": "Tue, 13 Mar 2018 03:49:48 GMT"}, {"version": "v3", "created": "Sun, 10 Jun 2018 01:24:08 GMT"}, {"version": "v4", "created": "Thu, 17 Oct 2019 18:54:59 GMT"}, {"version": "v5", "created": "Sat, 16 May 2020 15:11:54 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Siam", "Mennatullah", ""], ["Gamal", "Mostafa", ""], ["Abdel-Razek", "Moemen", ""], ["Yogamani", "Senthil", ""], ["Jagersand", "Martin", ""]]}, {"id": "1803.02784", "submitter": "Yoshikatsu Nakajima", "authors": "Yoshikatsu Nakajima, Keisuke Tateno, Federico Tombari and Hideo Saito", "title": "Fast and Accurate Semantic Mapping through Geometric-based Incremental\n  Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an efficient and scalable method for incrementally building a\ndense, semantically annotated 3D map in real-time. The proposed method assigns\nclass probabilities to each region, not each element (e.g., surfel and voxel),\nof the 3D map which is built up through a robust SLAM framework and\nincrementally segmented with a geometric-based segmentation method. Differently\nfrom all other approaches, our method has a capability of running at over 30Hz\nwhile performing all processing components, including SLAM, segmentation, 2D\nrecognition, and updating class probabilities of each segmentation label at\nevery incoming frame, thanks to the high efficiency that characterizes the\ncomputationally intensive stages of our framework. By utilizing a specifically\ndesigned CNN to improve the frame-wise segmentation result, we can also achieve\nhigh accuracy. We validate our method on the NYUv2 dataset by comparing with\nthe state of the art in terms of accuracy and computational efficiency, and by\nmeans of an analysis in terms of time and space complexity.\n", "versions": [{"version": "v1", "created": "Wed, 7 Mar 2018 17:36:34 GMT"}], "update_date": "2018-03-08", "authors_parsed": [["Nakajima", "Yoshikatsu", ""], ["Tateno", "Keisuke", ""], ["Tombari", "Federico", ""], ["Saito", "Hideo", ""]]}, {"id": "1803.02786", "submitter": "Jianjun Hu", "authors": "Yuxin Cui, Guiying Zhang, Zhonghao Liu, Zheng Xiong, Jianjun Hu", "title": "A Deep Learning Algorithm for One-step Contour Aware Nuclei Segmentation\n  of Histopathological Images", "comments": "13 pages. 12 figures", "journal-ref": "Med Biol Eng Comput 2019", "doi": "10.1007/s11517-019-02008-8", "report-no": "57, 2027--2043", "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper addresses the task of nuclei segmentation in high-resolution\nhistopathological images. We propose an auto- matic end-to-end deep neural\nnetwork algorithm for segmenta- tion of individual nuclei. A nucleus-boundary\nmodel is introduced to predict nuclei and their boundaries simultaneously using\na fully convolutional neural network. Given a color normalized image, the model\ndirectly outputs an estimated nuclei map and a boundary map. A simple, fast and\nparameter-free post-processing procedure is performed on the estimated nuclei\nmap to produce the final segmented nuclei. An overlapped patch extraction and\nassembling method is also designed for seamless prediction of nuclei in large\nwhole-slide images. We also show the effectiveness of data augmentation methods\nfor nuclei segmentation task. Our experiments showed our method outperforms\nprior state-of-the- art methods. Moreover, it is efficient that one 1000X1000\nimage can be segmented in less than 5 seconds. This makes it possible to\nprecisely segment the whole-slide image in acceptable time\n", "versions": [{"version": "v1", "created": "Wed, 7 Mar 2018 17:53:20 GMT"}], "update_date": "2020-03-18", "authors_parsed": [["Cui", "Yuxin", ""], ["Zhang", "Guiying", ""], ["Liu", "Zhonghao", ""], ["Xiong", "Zheng", ""], ["Hu", "Jianjun", ""]]}, {"id": "1803.02983", "submitter": "Mingyue Yuan", "authors": "Mingyue Yuan, Dong Yin, Jingwen Ding, Yuhao Luo, Zhipeng Zhou,\n  Chengfeng Zhu, Rui Zhang", "title": "A framework with updateable joint images re-ranking for Person\n  Re-identification", "comments": "23 pages,5 figures. submitted to JVCI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification plays an important role in realistic video\nsurveillance with increasing demand for public safety. In this paper, we\npropose a novel framework with rules of updating images for person\nre-identification in real-world surveillance system. First, Image Pool is\ngenerated by using mean-shift tracking method to automatically select video\nframe fragments of the target person. Second, features extracted from Image\nPool by convolutional network work together to re-rank original ranking list of\nthe main image and matching results will be generated. In addition, updating\nrules are designed for replacing images in Image Pool when a new image\nsatiating with our updating critical formula in video system. These rules fall\ninto two categories: if the new image is from the same camera as the previous\nupdated image, it will replace one of assist images; otherwise, it will replace\nthe main image directly. Experiments are conduced on Market-1501, iLIDS-VID and\nPRID-2011 and our ITSD datasets to validate that our framework outperforms on\nrank-1 accuracy and mAP for person re-identification. Furthermore, the update\nability of our framework provides consistently remarkable accuracy rate in\nreal-world surveillance system.\n", "versions": [{"version": "v1", "created": "Thu, 8 Mar 2018 06:51:45 GMT"}], "update_date": "2018-03-09", "authors_parsed": [["Yuan", "Mingyue", ""], ["Yin", "Dong", ""], ["Ding", "Jingwen", ""], ["Luo", "Yuhao", ""], ["Zhou", "Zhipeng", ""], ["Zhu", "Chengfeng", ""], ["Zhang", "Rui", ""]]}, {"id": "1803.02987", "submitter": "Zheng Zhang", "authors": "Zheng Zhang, Qin Zou, Yuewei Lin, Long Chen, and Song Wang", "title": "Improved Deep Hashing with Soft Pairwise Similarity for Multi-label\n  Image Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hash coding has been widely used in the approximate nearest neighbor search\nfor large-scale image retrieval. Recently, many deep hashing methods have been\nproposed and shown largely improved performance over traditional\nfeature-learning-based methods. Most of these methods examine the pairwise\nsimilarity on the semantic-level labels, where the pairwise similarity is\ngenerally defined in a hard-assignment way. That is, the pairwise similarity is\n'1' if they share no less than one class label and '0' if they do not share\nany. However, such similarity definition cannot reflect the similarity ranking\nfor pairwise images that hold multiple labels. In this paper, a new deep\nhashing method is proposed for multi-label image retrieval by re-defining the\npairwise similarity into an instance similarity, where the instance similarity\nis quantified into a percentage based on the normalized semantic labels. Based\non the instance similarity, a weighted cross-entropy loss and a minimum mean\nsquare error loss are tailored for loss-function construction, and are\nefficiently used for simultaneous feature learning and hash coding. Experiments\non three popular datasets demonstrate that, the proposed method outperforms the\ncompeting methods and achieves the state-of-the-art performance in multi-label\nimage retrieval.\n", "versions": [{"version": "v1", "created": "Thu, 8 Mar 2018 07:26:20 GMT"}, {"version": "v2", "created": "Mon, 19 Mar 2018 03:41:34 GMT"}, {"version": "v3", "created": "Thu, 17 Oct 2019 02:00:49 GMT"}], "update_date": "2019-10-18", "authors_parsed": [["Zhang", "Zheng", ""], ["Zou", "Qin", ""], ["Lin", "Yuewei", ""], ["Chen", "Long", ""], ["Wang", "Song", ""]]}, {"id": "1803.02988", "submitter": "Weitao Wan", "authors": "Weitao Wan, Yuanyi Zhong, Tianpeng Li, Jiansheng Chen", "title": "Rethinking Feature Distribution for Loss Functions in Image\n  Classification", "comments": "Accepted to CVPR 2018 as spotlight", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a large-margin Gaussian Mixture (L-GM) loss for deep neural\nnetworks in classification tasks. Different from the softmax cross-entropy\nloss, our proposal is established on the assumption that the deep features of\nthe training set follow a Gaussian Mixture distribution. By involving a\nclassification margin and a likelihood regularization, the L-GM loss\nfacilitates both a high classification performance and an accurate modeling of\nthe training feature distribution. As such, the L-GM loss is superior to the\nsoftmax loss and its major variants in the sense that besides classification,\nit can be readily used to distinguish abnormal inputs, such as the adversarial\nexamples, based on their features' likelihood to the training feature\ndistribution. Extensive experiments on various recognition benchmarks like\nMNIST, CIFAR, ImageNet and LFW, as well as on adversarial examples demonstrate\nthe effectiveness of our proposal.\n", "versions": [{"version": "v1", "created": "Thu, 8 Mar 2018 07:28:55 GMT"}], "update_date": "2018-03-09", "authors_parsed": [["Wan", "Weitao", ""], ["Zhong", "Yuanyi", ""], ["Li", "Tianpeng", ""], ["Chen", "Jiansheng", ""]]}, {"id": "1803.03004", "submitter": "Jian-Hao Luo", "authors": "Jianxin Wu and Jian-Hao Luo", "title": "Learning Effective Binary Visual Representations with Deep Networks", "comments": "16 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although traditionally binary visual representations are mainly designed to\nreduce computational and storage costs in the image retrieval research, this\npaper argues that binary visual representations can be applied to large scale\nrecognition and detection problems in addition to hashing in retrieval.\nFurthermore, the binary nature may make it generalize better than its\nreal-valued counterparts. Existing binary hashing methods are either two-stage\nor hinging on loss term regularization or saturated functions, hence converge\nslowly and only emit soft binary values. This paper proposes Approximately\nBinary Clamping (ABC), which is non-saturating, end-to-end trainable, with fast\nconvergence and can output true binary visual representations. ABC achieves\ncomparable accuracy in ImageNet classification as its real-valued counterpart,\nand even generalizes better in object detection. On benchmark image retrieval\ndatasets, ABC also outperforms existing hashing methods.\n", "versions": [{"version": "v1", "created": "Thu, 8 Mar 2018 08:56:17 GMT"}], "update_date": "2018-03-09", "authors_parsed": [["Wu", "Jianxin", ""], ["Luo", "Jian-Hao", ""]]}, {"id": "1803.03025", "submitter": "Raul Acu\\~na", "authors": "Raul Acuna, Volker Willert", "title": "Insights into the robustness of control point configurations for\n  homography and planar pose estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate the influence of the spatial configuration of a\nnumber of $n \\geq 4$ control points on the accuracy and robustness of space\nresection methods, e.g. used by a fiducial marker for pose estimation. We find\nrobust configurations of control points by minimizing the first order perturbed\nsolution of the DLT algorithm which is equivalent to minimizing the condition\nnumber of the data matrix. An empirical statistical evaluation is presented\nverifying that these optimized control point configurations not only increase\nthe performance of the DLT homography estimation but also improve the\nperformance of planar pose estimation methods like IPPE and EPnP, including the\niterative minimization of the reprojection error which is the most accurate\nalgorithm. We provide the characteristics of stable control point\nconfigurations for real-world noisy camera data that are practically\nindependent on the camera pose and form certain symmetric patterns dependent on\nthe number of points. Finally, we present a comparison of optimized\nconfiguration versus the number of control points.\n", "versions": [{"version": "v1", "created": "Thu, 8 Mar 2018 10:14:41 GMT"}, {"version": "v2", "created": "Wed, 9 Jan 2019 11:01:24 GMT"}], "update_date": "2019-01-10", "authors_parsed": [["Acuna", "Raul", ""], ["Willert", "Volker", ""]]}, {"id": "1803.03049", "submitter": "Yashas Annadani", "authors": "Yashas Annadani, Soma Biswas", "title": "Preserving Semantic Relations for Zero-Shot Learning", "comments": "CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Zero-shot learning has gained popularity due to its potential to scale\nrecognition models without requiring additional training data. This is usually\nachieved by associating categories with their semantic information like\nattributes. However, we believe that the potential offered by this paradigm is\nnot yet fully exploited. In this work, we propose to utilize the structure of\nthe space spanned by the attributes using a set of relations. We devise\nobjective functions to preserve these relations in the embedding space, thereby\ninducing semanticity to the embedding space. Through extensive experimental\nevaluation on five benchmark datasets, we demonstrate that inducing semanticity\nto the embedding space is beneficial for zero-shot learning. The proposed\napproach outperforms the state-of-the-art on the standard zero-shot setting as\nwell as the more realistic generalized zero-shot setting. We also demonstrate\nhow the proposed approach can be useful for making approximate semantic\ninferences about an image belonging to a category for which attribute\ninformation is not available.\n", "versions": [{"version": "v1", "created": "Thu, 8 Mar 2018 11:39:30 GMT"}], "update_date": "2018-03-09", "authors_parsed": [["Annadani", "Yashas", ""], ["Biswas", "Soma", ""]]}, {"id": "1803.03095", "submitter": "Xialei Liu", "authors": "Xialei Liu, Joost van de Weijer, Andrew D. Bagdanov", "title": "Leveraging Unlabeled Data for Crowd Counting by Learning to Rank", "comments": "Accepted by CVPR18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We propose a novel crowd counting approach that leverages abundantly\navailable unlabeled crowd imagery in a learning-to-rank framework. To induce a\nranking of cropped images , we use the observation that any sub-image of a\ncrowded scene image is guaranteed to contain the same number or fewer persons\nthan the super-image. This allows us to address the problem of limited size of\nexisting datasets for crowd counting. We collect two crowd scene datasets from\nGoogle using keyword searches and query-by-example image retrieval,\nrespectively. We demonstrate how to efficiently learn from these unlabeled\ndatasets by incorporating learning-to-rank in a multi-task network which\nsimultaneously ranks images and estimates crowd density maps. Experiments on\ntwo of the most challenging crowd counting datasets show that our approach\nobtains state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Thu, 8 Mar 2018 14:10:56 GMT"}], "update_date": "2018-03-09", "authors_parsed": [["Liu", "Xialei", ""], ["van de Weijer", "Joost", ""], ["Bagdanov", "Andrew D.", ""]]}, {"id": "1803.03104", "submitter": "Oliver Lauwers", "authors": "Oliver Lauwers, Bart De Moor", "title": "Applicability and interpretation of the deterministic weighted cepstral\n  distance", "comments": "18 pages, 5 figures, submitted for review to Automatica", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY cs.CV math.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantifying similarity between data objects is an important part of modern\ndata science. Deciding what similarity measure to use is very application\ndependent. In this paper, we combine insights from systems theory and machine\nlearning, and investigate the weighted cepstral distance, which was previously\ndefined for signals coming from ARMA models. We provide an extension of this\ndistance to invertible deterministic linear time invariant single input single\noutput models, and assess its applicability. We show that it can always be\ninterpreted in terms of the poles and zeros of the underlying model, and that,\nin the case of stable, minimum-phase, or unstable, maximum-phase models, a\ngeometrical interpretation in terms of subspace angles can be given. We then\ndevise a method to assess stability and phase-type of the generating models,\nusing only input/output signal information. In this way, we prove a connection\nbetween the extended weighted cepstral distance and a weighted cepstral model\nnorm. In this way, we provide a purely data-driven way to assess different\nunderlying dynamics of input/output signal pairs, without the need for any\nsystem identification step. This can be useful in machine learning tasks such\nas time series clustering. An iPython tutorial is published complementary to\nthis paper, containing implementations of the various methods and algorithms\npresented here, as well as some numerical illustrations of the equivalences\nproven here.\n", "versions": [{"version": "v1", "created": "Thu, 8 Mar 2018 14:31:46 GMT"}], "update_date": "2018-03-09", "authors_parsed": [["Lauwers", "Oliver", ""], ["De Moor", "Bart", ""]]}, {"id": "1803.03200", "submitter": "Donatella Firmani", "authors": "Donatella Firmani, Marco Maiorino, Paolo Merialdo, and Elena Nieddu", "title": "Towards Knowledge Discovery from the Vatican Secret Archives. In Codice\n  Ratio -- Episode 1: Machine Transcription of the Manuscripts", "comments": "Donatella Firmani, Marco Maiorino, Paolo Merialdo, and Elena Nieddu.\n  2018. Towards Knowledge Discovery from the Vatican Secret Archives. In Codice\n  Ratio - Episode 1: Machine Transcription of the Manuscripts. In Proceedings\n  of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data\n  Mining (KDD '18). ACM, New York, NY, USA, 263-272", "journal-ref": null, "doi": "10.1145/3219819.3219879", "report-no": null, "categories": "cs.DL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Codice Ratio is a research project to study tools and techniques for\nanalyzing the contents of historical documents conserved in the Vatican Secret\nArchives (VSA). In this paper, we present our efforts to develop a system to\nsupport the transcription of medieval manuscripts. The goal is to provide\npaleographers with a tool to reduce their efforts in transcribing large\nvolumes, as those stored in the VSA, producing good transcriptions for\nsignificant portions of the manuscripts. We propose an original approach based\non character segmentation. Our solution is able to deal with the dirty\nsegmentation that inevitably occurs in handwritten documents. We use a\nconvolutional neural network to recognize characters and language models to\ncompose word transcriptions. Our approach requires minimal training efforts,\nmaking the transcription process more scalable as the production of training\nsets requires a few pages and can be easily crowdsourced. We have conducted\nexperiments on manuscripts from the Vatican Registers, an unreleased corpus\ncontaining the correspondence of the popes. With training data produced by 120\nhigh school students, our system has been able to produce good transcriptions\nthat can be used by paleographers as a solid basis to speedup the transcription\nprocess at a large scale.\n", "versions": [{"version": "v1", "created": "Thu, 8 Mar 2018 17:01:32 GMT"}, {"version": "v2", "created": "Tue, 8 May 2018 12:20:09 GMT"}, {"version": "v3", "created": "Wed, 12 Sep 2018 12:49:24 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Firmani", "Donatella", ""], ["Maiorino", "Marco", ""], ["Merialdo", "Paolo", ""], ["Nieddu", "Elena", ""]]}, {"id": "1803.03243", "submitter": "Yuhua Chen", "authors": "Yuhua Chen, Wen Li, Christos Sakaridis, Dengxin Dai, Luc Van Gool", "title": "Domain Adaptive Faster R-CNN for Object Detection in the Wild", "comments": "Accepted to CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detection typically assumes that training and test data are drawn from\nan identical distribution, which, however, does not always hold in practice.\nSuch a distribution mismatch will lead to a significant performance drop. In\nthis work, we aim to improve the cross-domain robustness of object detection.\nWe tackle the domain shift on two levels: 1) the image-level shift, such as\nimage style, illumination, etc, and 2) the instance-level shift, such as object\nappearance, size, etc. We build our approach based on the recent\nstate-of-the-art Faster R-CNN model, and design two domain adaptation\ncomponents, on image level and instance level, to reduce the domain\ndiscrepancy. The two domain adaptation components are based on H-divergence\ntheory, and are implemented by learning a domain classifier in adversarial\ntraining manner. The domain classifiers on different levels are further\nreinforced with a consistency regularization to learn a domain-invariant region\nproposal network (RPN) in the Faster R-CNN model. We evaluate our newly\nproposed approach using multiple datasets including Cityscapes, KITTI, SIM10K,\netc. The results demonstrate the effectiveness of our proposed approach for\nrobust object detection in various domain shift scenarios.\n", "versions": [{"version": "v1", "created": "Thu, 8 Mar 2018 18:36:22 GMT"}], "update_date": "2018-03-09", "authors_parsed": [["Chen", "Yuhua", ""], ["Li", "Wen", ""], ["Sakaridis", "Christos", ""], ["Dai", "Dengxin", ""], ["Van Gool", "Luc", ""]]}, {"id": "1803.03254", "submitter": "Noriaki HIrose", "authors": "Noriaki Hirose, Amir Sadeghian, Marynel V\\'azquez, Patrick Goebel, and\n  Silvio Savarese", "title": "GONet: A Semi-Supervised Deep Learning Approach For Traversability\n  Estimation", "comments": "8 pages, 7 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present semi-supervised deep learning approaches for traversability\nestimation from fisheye images. Our method, GONet, and the proposed extensions\nleverage Generative Adversarial Networks (GANs) to effectively predict whether\nthe area seen in the input image(s) is safe for a robot to traverse. These\nmethods are trained with many positive images of traversable places, but just a\nsmall set of negative images depicting blocked and unsafe areas. This makes the\nproposed methods practical. Positive examples can be collected easily by simply\noperating a robot through traversable spaces, while obtaining negative examples\nis time consuming, costly, and potentially dangerous. Through extensive\nexperiments and several demonstrations, we show that the proposed\ntraversability estimation approaches are robust and can generalize to unseen\nscenarios. Further, we demonstrate that our methods are memory efficient and\nfast, allowing for real-time operation on a mobile robot with single or stereo\nfisheye cameras. As part of our contributions, we open-source two new datasets\nfor traversability estimation. These datasets are composed of approximately 24h\nof videos from more than 25 indoor environments. Our methods outperform\nbaseline approaches for traversability estimation on these new datasets.\n", "versions": [{"version": "v1", "created": "Thu, 8 Mar 2018 18:52:03 GMT"}], "update_date": "2018-03-09", "authors_parsed": [["Hirose", "Noriaki", ""], ["Sadeghian", "Amir", ""], ["V\u00e1zquez", "Marynel", ""], ["Goebel", "Patrick", ""], ["Savarese", "Silvio", ""]]}, {"id": "1803.03310", "submitter": "Nam Vo", "authors": "Nam Vo and James Hays", "title": "Generalization in Metric Learning: Should the Embedding Layer be the\n  Embedding Layer?", "comments": "new version for WACV", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work studies deep metric learning under small to medium scale data as we\nbelieve that better generalization could be a contributing factor to the\nimprovement of previous fine-grained image retrieval methods; it should be\nconsidered when designing future techniques. In particular, we investigate\nusing other layers in a deep metric learning system (besides the embedding\nlayer) for feature extraction and analyze how well they perform on training\ndata and generalize to testing data. From this study, we suggest a new\nregularization practice where one can add or choose a more optimal layer for\nfeature extraction. State-of-the-art performance is demonstrated on 3\nfine-grained image retrieval benchmarks: Cars-196, CUB-200-2011, and Stanford\nOnline Product.\n", "versions": [{"version": "v1", "created": "Thu, 8 Mar 2018 21:29:38 GMT"}, {"version": "v2", "created": "Mon, 10 Dec 2018 17:02:32 GMT"}], "update_date": "2018-12-11", "authors_parsed": [["Vo", "Nam", ""], ["Hays", "James", ""]]}, {"id": "1803.03317", "submitter": "Aisha Urooj", "authors": "Aisha Urooj Khan, Ali Borji", "title": "Analysis of Hand Segmentation in the Wild", "comments": "Accepted at CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A large number of works in egocentric vision have concentrated on action and\nobject recognition. Detection and segmentation of hands in first-person videos,\nhowever, has less been explored. For many applications in this domain, it is\nnecessary to accurately segment not only hands of the camera wearer but also\nthe hands of others with whom he is interacting. Here, we take an in-depth look\nat the hand segmentation problem. In the quest for robust hand segmentation\nmethods, we evaluated the performance of the state of the art semantic\nsegmentation methods, off the shelf and fine-tuned, on existing datasets. We\nfine-tune RefineNet, a leading semantic segmentation method, for hand\nsegmentation and find that it does much better than the best contenders.\nExisting hand segmentation datasets are collected in the laboratory settings.\nTo overcome this limitation, we contribute by collecting two new datasets: a)\nEgoYouTubeHands including egocentric videos containing hands in the wild, and\nb) HandOverFace to analyze the performance of our models in presence of similar\nappearance occlusions. We further explore whether conditional random fields can\nhelp refine generated hand segmentations. To demonstrate the benefit of\naccurate hand maps, we train a CNN for hand-based activity recognition and\nachieve higher accuracy when a CNN was trained using hand maps produced by the\nfine-tuned RefineNet. Finally, we annotate a subset of the EgoHands dataset for\nfine-grained action recognition and show that an accuracy of 58.6% can be\nachieved by just looking at a single hand pose which is much better than the\nchance level (12.5%).\n", "versions": [{"version": "v1", "created": "Thu, 8 Mar 2018 21:45:07 GMT"}, {"version": "v2", "created": "Wed, 28 Mar 2018 20:11:20 GMT"}], "update_date": "2018-03-30", "authors_parsed": [["Khan", "Aisha Urooj", ""], ["Borji", "Ali", ""]]}, {"id": "1803.03330", "submitter": "Grigorios Chrysos", "authors": "Grigorios G. Chrysos and Paolo Favaro and Stefanos Zafeiriou", "title": "Motion deblurring of faces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face analysis is a core part of computer vision, in which remarkable progress\nhas been observed in the past decades. Current methods achieve recognition and\ntracking with invariance to fundamental modes of variation such as\nillumination, 3D pose, expressions. Notwithstanding, a much less standing mode\nof variation is motion deblurring, which however presents substantial\nchallenges in face analysis. Recent approaches either make oversimplifying\nassumptions, e.g. in cases of joint optimization with other tasks, or fail to\npreserve the highly structured shape/identity information. Therefore, we\npropose a data-driven method that encourages identity preservation. The\nproposed model includes two parallel streams (sub-networks): the first deblurs\nthe image, the second implicitly extracts and projects the identity of both the\nsharp and the blurred image in similar subspaces. We devise a method for\ncreating realistic motion blur by averaging a variable number of frames to\ntrain our model. The averaged images originate from a 2MF2 dataset with 10\nmillion facial frames, which we introduce for the task. Considering deblurring\nas an intermediate step, we utilize the deblurred outputs to conduct a thorough\nexperimentation on high-level face analysis tasks, i.e. landmark localization\nand face verification. The experimental evaluation demonstrates the superiority\nof our method.\n", "versions": [{"version": "v1", "created": "Thu, 8 Mar 2018 23:07:22 GMT"}], "update_date": "2018-03-12", "authors_parsed": [["Chrysos", "Grigorios G.", ""], ["Favaro", "Paolo", ""], ["Zafeiriou", "Stefanos", ""]]}, {"id": "1803.03341", "submitter": "Horia Porav", "authors": "Horia Porav, Will Maddern and Paul Newman", "title": "Adversarial Training for Adverse Conditions: Robust Metric Localisation\n  using Appearance Transfer", "comments": "Accepted at ICRA2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method of improving visual place recognition and metric\nlocalisation under very strong appear- ance change. We learn an invertable\ngenerator that can trans- form the conditions of images, e.g. from day to\nnight, summer to winter etc. This image transforming filter is explicitly\ndesigned to aid and abet feature-matching using a new loss based on SURF\ndetector and dense descriptor maps. A network is trained to output synthetic\nimages optimised for feature matching given only an input RGB image, and these\ngenerated images are used to localize the robot against a previously built map\nusing traditional sparse matching approaches. We benchmark our results using\nmultiple traversals of the Oxford RobotCar Dataset over a year-long period,\nusing one traversal as a map and the other to localise. We show that this\nmethod significantly improves place recognition and localisation under changing\nand adverse conditions, while reducing the number of mapping runs needed to\nsuccessfully achieve reliable localisation.\n", "versions": [{"version": "v1", "created": "Fri, 9 Mar 2018 00:45:08 GMT"}], "update_date": "2018-03-12", "authors_parsed": [["Porav", "Horia", ""], ["Maddern", "Will", ""], ["Newman", "Paul", ""]]}, {"id": "1803.03345", "submitter": "Wei-Sheng Lai", "authors": "Ziyi Shen, Wei-Sheng Lai, Tingfa Xu, Jan Kautz, Ming-Hsuan Yang", "title": "Deep Semantic Face Deblurring", "comments": "This work is accepted in CVPR 2018. The project website is on\n  https://sites.google.com/site/ziyishenmi/cvpr18_face_deblur", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present an effective and efficient face deblurring\nalgorithm by exploiting semantic cues via deep convolutional neural networks\n(CNNs). As face images are highly structured and share several key semantic\ncomponents (e.g., eyes and mouths), the semantic information of a face provides\na strong prior for restoration. As such, we propose to incorporate global\nsemantic priors as input and impose local structure losses to regularize the\noutput within a multi-scale deep CNN. We train the network with perceptual and\nadversarial losses to generate photo-realistic results and develop an\nincremental training strategy to handle random blur kernels in the wild.\nQuantitative and qualitative evaluations demonstrate that the proposed face\ndeblurring algorithm restores sharp images with more facial details and\nperforms favorably against state-of-the-art methods in terms of restoration\nquality, face recognition and execution speed.\n", "versions": [{"version": "v1", "created": "Fri, 9 Mar 2018 01:11:20 GMT"}, {"version": "v2", "created": "Fri, 16 Mar 2018 07:32:57 GMT"}], "update_date": "2018-03-19", "authors_parsed": [["Shen", "Ziyi", ""], ["Lai", "Wei-Sheng", ""], ["Xu", "Tingfa", ""], ["Kautz", "Jan", ""], ["Yang", "Ming-Hsuan", ""]]}, {"id": "1803.03347", "submitter": "Tharindu Fernando", "authors": "Tharindu Fernando, Simon Denman, Sridha Sridharan and Clinton Fookes", "title": "Tracking by Prediction: A Deep Generative Model for Mutli-Person\n  localisation and Tracking", "comments": "To appear in IEEE Winter Conference on Applications of Computer\n  Vision (WACV), 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current multi-person localisation and tracking systems have an over reliance\non the use of appearance models for target re-identification and almost no\napproaches employ a complete deep learning solution for both objectives. We\npresent a novel, complete deep learning framework for multi-person localisation\nand tracking. In this context we first introduce a light weight sequential\nGenerative Adversarial Network architecture for person localisation, which\novercomes issues related to occlusions and noisy detections, typically found in\na multi person environment. In the proposed tracking framework we build upon\nrecent advances in pedestrian trajectory prediction approaches and propose a\nnovel data association scheme based on predicted trajectories. This removes the\nneed for computationally expensive person re-identification systems based on\nappearance features and generates human like trajectories with minimal\nfragmentation. The proposed method is evaluated on multiple public benchmarks\nincluding both static and dynamic cameras and is capable of generating\noutstanding performance, especially among other recently proposed deep neural\nnetwork based approaches.\n", "versions": [{"version": "v1", "created": "Fri, 9 Mar 2018 01:14:30 GMT"}], "update_date": "2018-03-12", "authors_parsed": [["Fernando", "Tharindu", ""], ["Denman", "Simon", ""], ["Sridharan", "Sridha", ""], ["Fookes", "Clinton", ""]]}, {"id": "1803.03352", "submitter": "Muzammal Naseer", "authors": "Muzammal Naseer, Salman H Khan, Fatih Porikli", "title": "Indoor Scene Understanding in 2.5/3D for Autonomous Agents: A Survey", "comments": "IEEE Access", "journal-ref": "Year: DECEMBER 2019, Volume: 7, Issue:1, Page(s): 1859-1887,", "doi": "10.1109/ACCESS.2018.2886133", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the availability of low-cost and compact 2.5/3D visual sensing devices,\ncomputer vision community is experiencing a growing interest in visual scene\nunderstanding of indoor environments. This survey paper provides a\ncomprehensive background to this research topic. We begin with a historical\nperspective, followed by popular 3D data representations and a comparative\nanalysis of available datasets. Before delving into the application specific\ndetails, this survey provides a succinct introduction to the core technologies\nthat are the underlying methods extensively used in the literature. Afterwards,\nwe review the developed techniques according to a taxonomy based on the scene\nunderstanding tasks. This covers holistic indoor scene understanding as well as\nsubtasks such as scene classification, object detection, pose estimation,\nsemantic segmentation, 3D reconstruction, saliency detection, physics-based\nreasoning and affordance prediction. Later on, we summarize the performance\nmetrics used for evaluation in different tasks and a quantitative comparison\namong the recent state-of-the-art techniques. We conclude this review with the\ncurrent challenges and an outlook towards the open research problems requiring\nfurther investigation.\n", "versions": [{"version": "v1", "created": "Fri, 9 Mar 2018 02:02:26 GMT"}, {"version": "v2", "created": "Thu, 10 Jan 2019 07:56:08 GMT"}], "update_date": "2019-01-11", "authors_parsed": [["Naseer", "Muzammal", ""], ["Khan", "Salman H", ""], ["Porikli", "Fatih", ""]]}, {"id": "1803.03354", "submitter": "Tharindu Fernando", "authors": "Tharindu Fernando, Simon Denman, Sridha Sridharan and Clinton Fookes", "title": "Task Specific Visual Saliency Prediction with Memory Augmented\n  Conditional Generative Adversarial Networks", "comments": "To appear in IEEE Winter Conference on Applications of Computer\n  Vision (WACV), 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual saliency patterns are the result of a variety of factors aside from\nthe image being parsed, however existing approaches have ignored these. To\naddress this limitation, we propose a novel saliency estimation model which\nleverages the semantic modelling power of conditional generative adversarial\nnetworks together with memory architectures which capture the subject's\nbehavioural patterns and task dependent factors. We make contributions aiming\nto bridge the gap between bottom-up feature learning capabilities in modern\ndeep learning architectures and traditional top-down hand-crafted features\nbased methods for task specific saliency modelling. The conditional nature of\nthe proposed framework enables us to learn contextual semantics and\nrelationships among different tasks together, instead of learning them\nseparately for each task. Our studies not only shed light on a novel\napplication area for generative adversarial networks, but also emphasise the\nimportance of task specific saliency modelling and demonstrate the plausibility\nof fully capturing this context via an augmented memory architecture.\n", "versions": [{"version": "v1", "created": "Fri, 9 Mar 2018 02:08:09 GMT"}], "update_date": "2018-03-12", "authors_parsed": [["Fernando", "Tharindu", ""], ["Denman", "Simon", ""], ["Sridharan", "Sridha", ""], ["Fookes", "Clinton", ""]]}, {"id": "1803.03363", "submitter": "Lerenhan Li", "authors": "Lerenhan Li, Jinshan Pan, Wei-Sheng Lai, Changxin Gao, Nong Sang,\n  Ming-Hsuan Yang", "title": "Learning a Discriminative Prior for Blind Image Deblurring", "comments": "This paper is accepted by CVPR2018 as poster", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an effective blind image deblurring method based on a data-driven\ndiscriminative prior.Our work is motivated by the fact that a good image prior\nshould favor clear images over blurred images.In this work, we formulate the\nimage prior as a binary classifier which can be achieved by a deep\nconvolutional neural network (CNN).The learned prior is able to distinguish\nwhether an input image is clear or not.Embedded into the maximum a posterior\n(MAP) framework, it helps blind deblurring in various scenarios, including\nnatural, face, text, and low-illumination images.However, it is difficult to\noptimize the deblurring method with the learned image prior as it involves a\nnon-linear CNN.Therefore, we develop an efficient numerical approach based on\nthe half-quadratic splitting method and gradient decent algorithm to solve the\nproposed model.Furthermore, the proposed model can be easily extended to\nnon-uniform deblurring.Both qualitative and quantitative experimental results\nshow that our method performs favorably against state-of-the-art algorithms as\nwell as domain-specific image deblurring approaches.\n", "versions": [{"version": "v1", "created": "Fri, 9 Mar 2018 02:48:10 GMT"}, {"version": "v2", "created": "Wed, 4 Apr 2018 18:46:53 GMT"}], "update_date": "2018-04-06", "authors_parsed": [["Li", "Lerenhan", ""], ["Pan", "Jinshan", ""], ["Lai", "Wei-Sheng", ""], ["Gao", "Changxin", ""], ["Sang", "Nong", ""], ["Yang", "Ming-Hsuan", ""]]}, {"id": "1803.03391", "submitter": "Runmin Cong", "authors": "Runmin Cong, Jianjun Lei, Huazhu Fu, Ming-Ming Cheng, Weisi Lin, and\n  Qingming Huang", "title": "Review of Visual Saliency Detection with Comprehensive Information", "comments": "18 pages, 11 figures, 7 tables, Accepted by IEEE Transactions on\n  Circuits and Systems for Video Technology 2018, https://rmcong.github.io/", "journal-ref": "IEEE TCSVT 2018", "doi": "10.1109/TCSVT.2018.2870832", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual saliency detection model simulates the human visual system to perceive\nthe scene, and has been widely used in many vision tasks. With the acquisition\ntechnology development, more comprehensive information, such as depth cue,\ninter-image correspondence, or temporal relationship, is available to extend\nimage saliency detection to RGBD saliency detection, co-saliency detection, or\nvideo saliency detection. RGBD saliency detection model focuses on extracting\nthe salient regions from RGBD images by combining the depth information.\nCo-saliency detection model introduces the inter-image correspondence\nconstraint to discover the common salient object in an image group. The goal of\nvideo saliency detection model is to locate the motion-related salient object\nin video sequences, which considers the motion cue and spatiotemporal\nconstraint jointly. In this paper, we review different types of saliency\ndetection algorithms, summarize the important issues of the existing methods,\nand discuss the existent problems and future works. Moreover, the evaluation\ndatasets and quantitative measurements are briefly introduced, and the\nexperimental analysis and discission are conducted to provide a holistic\noverview of different saliency detection methods.\n", "versions": [{"version": "v1", "created": "Fri, 9 Mar 2018 05:55:33 GMT"}, {"version": "v2", "created": "Fri, 14 Sep 2018 04:29:18 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Cong", "Runmin", ""], ["Lei", "Jianjun", ""], ["Fu", "Huazhu", ""], ["Cheng", "Ming-Ming", ""], ["Lin", "Weisi", ""], ["Huang", "Qingming", ""]]}, {"id": "1803.03396", "submitter": "Krishna Regmi", "authors": "Krishna Regmi and Ali Borji", "title": "Cross-View Image Synthesis using Conditional GANs", "comments": "Accepted at CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning to generate natural scenes has always been a challenging task in\ncomputer vision. It is even more painstaking when the generation is conditioned\non images with drastically different views. This is mainly because\nunderstanding, corresponding, and transforming appearance and semantic\ninformation across the views is not trivial. In this paper, we attempt to solve\nthe novel problem of cross-view image synthesis, aerial to street-view and vice\nversa, using conditional generative adversarial networks (cGAN). Two new\narchitectures called Crossview Fork (X-Fork) and Crossview Sequential (X-Seq)\nare proposed to generate scenes with resolutions of 64x64 and 256x256 pixels.\nX-Fork architecture has a single discriminator and a single generator. The\ngenerator hallucinates both the image and its semantic segmentation in the\ntarget view. X-Seq architecture utilizes two cGANs. The first one generates the\ntarget image which is subsequently fed to the second cGAN for generating its\ncorresponding semantic segmentation map. The feedback from the second cGAN\nhelps the first cGAN generate sharper images. Both of our proposed\narchitectures learn to generate natural images as well as their semantic\nsegmentation maps. The proposed methods show that they are able to capture and\nmaintain the true semantics of objects in source and target views better than\nthe traditional image-to-image translation method which considers only the\nvisual appearance of the scene. Extensive qualitative and quantitative\nevaluations support the effectiveness of our frameworks, compared to two state\nof the art methods, for natural scene generation across drastically different\nviews.\n", "versions": [{"version": "v1", "created": "Fri, 9 Mar 2018 06:53:36 GMT"}, {"version": "v2", "created": "Thu, 29 Mar 2018 17:18:49 GMT"}], "update_date": "2018-03-30", "authors_parsed": [["Regmi", "Krishna", ""], ["Borji", "Ali", ""]]}, {"id": "1803.03415", "submitter": "Zheng Zhang", "authors": "Zheng Zhang, Chengfang Song, Qin Zou", "title": "Fusing Hierarchical Convolutional Features for Human Body Segmentation\n  and Clothing Fashion Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The clothing fashion reflects the common aesthetics that people share with\neach other in dressing. To recognize the fashion time of a clothing is\nmeaningful for both an individual and the industry. In this paper, under the\nassumption that the clothing fashion changes year by year, the fashion-time\nrecognition problem is mapped into a clothing-fashion classification problem.\nSpecifically, a novel deep neural network is proposed which achieves accurate\nhuman body segmentation by fusing multi-scale convolutional features in a fully\nconvolutional network, and then feature learning and fashion classification are\nperformed on the segmented parts avoiding the influence of image background. In\nthe experiments, 9,339 fashion images from 8 continuous years are collected for\nperformance evaluation. The results demonstrate the effectiveness of the\nproposed body segmentation and fashion classification methods.\n", "versions": [{"version": "v1", "created": "Fri, 9 Mar 2018 08:46:21 GMT"}, {"version": "v2", "created": "Tue, 13 Mar 2018 02:39:09 GMT"}], "update_date": "2018-03-14", "authors_parsed": [["Zhang", "Zheng", ""], ["Song", "Chengfang", ""], ["Zou", "Qin", ""]]}, {"id": "1803.03420", "submitter": "Yuncong Chen", "authors": "Yuncong Chen, David Kleinfeld, Martyn Goulding, Yoav Freund", "title": "Robust Landmark Detection for Alignment of Mouse Brain Section Images", "comments": "Submitted to MICCAI 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Brightfield and fluorescent imaging of whole brain sections are funda- mental\ntools of research in mouse brain study. As sectioning and imaging become more\nefficient, there is an increasing need to automate the post-processing of sec-\ntions for alignment and three dimensional visualization. There is a further\nneed to facilitate the development of a digital atlas, i.e. a brain-wide map\nannotated with cell type and tract tracing data, which would allow the\nautomatic registra- tion of images stacks to a common coordinate system.\nCurrently, registration of slices requires manual identification of landmarks.\nIn this work we describe the first steps in developing a semi-automated system\nto construct a histology at- las of mouse brainstem that combines atlas-guided\nannotation, landmark-based registration and atlas generation in an iterative\nframework. We describe an unsu- pervised approach for identifying and matching\nregion and boundary landmarks, based on modelling texture. Experiments show\nthat the detected landmarks corre- spond well with brain structures, and\nmatching is robust under distortion. These results will serve as the basis for\nregistration and atlas building.\n", "versions": [{"version": "v1", "created": "Fri, 9 Mar 2018 08:58:18 GMT"}], "update_date": "2018-03-12", "authors_parsed": [["Chen", "Yuncong", ""], ["Kleinfeld", "David", ""], ["Goulding", "Martyn", ""], ["Freund", "Yoav", ""]]}, {"id": "1803.03434", "submitter": "Guoan Zheng", "authors": "Shaowei Jiang, Kaikai Guo, Jun Liao, and Guoan Zheng", "title": "Solving Fourier ptychographic imaging problems via neural network\n  modeling and TensorFlow", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV physics.optics", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fourier ptychography is a recently developed imaging approach for large\nfield-of-view and high-resolution microscopy. Here we model the Fourier\nptychographic forward imaging process using a convolution neural network (CNN)\nand recover the complex object information in the network training process. In\nthis approach, the input of the network is the point spread function in the\nspatial domain or the coherent transfer function in the Fourier domain. The\nobject is treated as 2D learnable weights of a convolution or a multiplication\nlayer. The output of the network is modeled as the loss function we aim to\nminimize. The batch size of the network corresponds to the number of captured\nlow-resolution images in one forward / backward pass. We use a popular\nopen-source machine learning library, TensorFlow, for setting up the network\nand conducting the optimization process. We analyze the performance of\ndifferent learning rates, different solvers, and different batch sizes. It is\nshown that a large batch size with the Adam optimizer achieves the best\nperformance in general. To accelerate the phase retrieval process, we also\ndiscuss a strategy to implement Fourier-magnitude projection using a\nmultiplication neural network model. Since convolution and multiplication are\nthe two most-common operations in imaging modeling, the reported approach may\nprovide a new perspective to examine many coherent and incoherent systems. As a\ndemonstration, we discuss the extensions of the reported networks for modeling\nsingle-pixel imaging and structured illumination microscopy (SIM). 4-frame\nresolution doubling is demonstrated using a neural network for SIM. We have\nmade our implementation code open-source for the broad research community.\n", "versions": [{"version": "v1", "created": "Fri, 9 Mar 2018 09:38:32 GMT"}], "update_date": "2018-03-12", "authors_parsed": [["Jiang", "Shaowei", ""], ["Guo", "Kaikai", ""], ["Liao", "Jun", ""], ["Zheng", "Guoan", ""]]}, {"id": "1803.03474", "submitter": "Chunhua Shen", "authors": "Tong He, Zhi Tian, Weilin Huang, Chunhua Shen, Yu Qiao, Changming Sun", "title": "An end-to-end TextSpotter with Explicit Alignment and Attention", "comments": "Accepted to IEEE Conf. Computer Vision and Pattern Recognition (CVPR)\n  2018. Code is available at: https://github.com/tonghe90/textspotter", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text detection and recognition in natural images have long been considered as\ntwo separate tasks that are processed sequentially. Training of two tasks in a\nunified framework is non-trivial due to significant dif- ferences in\noptimisation difficulties. In this work, we present a conceptually simple yet\nefficient framework that simultaneously processes the two tasks in one shot.\nOur main contributions are three-fold: 1) we propose a novel text-alignment\nlayer that allows it to precisely compute convolutional features of a text\ninstance in ar- bitrary orientation, which is the key to boost the per-\nformance; 2) a character attention mechanism is introduced by using character\nspatial information as explicit supervision, leading to large improvements in\nrecognition; 3) two technologies, together with a new RNN branch for word\nrecognition, are integrated seamlessly into a single model which is end-to-end\ntrainable. This allows the two tasks to work collaboratively by shar- ing\nconvolutional features, which is critical to identify challenging text\ninstances. Our model achieves impressive results in end-to-end recognition on\nthe ICDAR2015 dataset, significantly advancing most recent results, with\nimprovements of F-measure from (0.54, 0.51, 0.47) to (0.82, 0.77, 0.63), by\nusing a strong, weak and generic lexicon respectively. Thanks to joint\ntraining, our method can also serve as a good detec- tor by achieving a new\nstate-of-the-art detection performance on two datasets.\n", "versions": [{"version": "v1", "created": "Fri, 9 Mar 2018 11:30:51 GMT"}, {"version": "v2", "created": "Tue, 20 Mar 2018 23:40:29 GMT"}, {"version": "v3", "created": "Fri, 23 Mar 2018 02:49:42 GMT"}], "update_date": "2018-03-26", "authors_parsed": [["He", "Tong", ""], ["Tian", "Zhi", ""], ["Huang", "Weilin", ""], ["Shen", "Chunhua", ""], ["Qiao", "Yu", ""], ["Sun", "Changming", ""]]}, {"id": "1803.03487", "submitter": "Maarten Bieshaar", "authors": "Maarten Bieshaar and Stefan Zernetsch and Andreas Hubert and Bernhard\n  Sick and Konrad Doll", "title": "Cooperative Starting Movement Detection of Cyclists Using Convolutional\n  Neural Networks and a Boosted Stacking Ensemble", "comments": "10 Pages, 22 figures, accepted for Special Issue of IEEE Transactions\n  on Intelligent Vehicles", "journal-ref": "IEEE Transactions on Intelligent Vehicles 3 (2018), Nr. 4", "doi": "10.1109/TIV.2018.2873900", "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In future, vehicles and other traffic participants will be interconnected and\nequipped with various types of sensors, allowing for cooperation on different\nlevels, such as situation prediction or intention detection. In this article we\npresent a cooperative approach for starting movement detection of cyclists\nusing a boosted stacking ensemble approach realizing feature- and decision\nlevel cooperation. We introduce a novel method based on a 3D Convolutional\nNeural Network (CNN) to detect starting motions on image sequences by learning\nspatio-temporal features. The CNN is complemented by a smart device based\nstarting movement detection originating from smart devices carried by the\ncyclist. Both model outputs are combined in a stacking ensemble approach using\nan extreme gradient boosting classifier resulting in a fast and yet robust\ncooperative starting movement detector. We evaluate our cooperative approach on\nreal-world data originating from experiments with 49 test subjects consisting\nof 84 starting motions.\n", "versions": [{"version": "v1", "created": "Fri, 9 Mar 2018 12:27:14 GMT"}, {"version": "v2", "created": "Tue, 9 Oct 2018 13:05:43 GMT"}], "update_date": "2018-10-10", "authors_parsed": [["Bieshaar", "Maarten", ""], ["Zernetsch", "Stefan", ""], ["Hubert", "Andreas", ""], ["Sick", "Bernhard", ""], ["Doll", "Konrad", ""]]}, {"id": "1803.03562", "submitter": "Xiaohui Yang", "authors": "Xiaohui Yang, Wenming Wu, Yunmei Chen, Xianqi Li, Juan Zhang, Dan\n  Long, Lijun Yang", "title": "An Integrated Inverse Space Sparse Representation Framework for Tumor\n  Classification", "comments": "40 pages, 18 figures, 13 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Microarray gene expression data-based tumor classification is an active and\nchallenging issue. In this paper, an integrated tumor classification framework\nis presented, which aims to exploit information in existing available samples,\nand focuses on the small sample problem and unbalanced classification problem.\nFirstly, an inverse space sparse representation based classification (ISSRC)\nmodel is proposed by considering the characteristics of gene-based tumor data,\nsuch as sparsity and a small number of training samples. A decision information\nfactors (DIF)-based gene selection method is constructed to enhance the\nrepresentation ability of the ISSRC. It is worth noting that the DIF is\nestablished from reducing clinical misdiagnosis rate and dimension of small\nsample data. For further improving the representation ability and\nclassification stability of the ISSRC, feature learning is conducted on the\nselected gene subset. The feature learning method is constructed by\ncomplementing the advantages of non-negative matrix factorization (NMF) and\ndeep learning. Without confusion, the ISSRC combined with gene selection and\nfeature learning is called the integrated ISSRC, whose stability, optimization\nand the corresponding convergence are analyzed. Extensive experiments on six\npublic microarray gene expression datasets show the integrated ISSRC-based\ntumor classification framework is superior to classical and state-of-the-art\nmethods. There are significant improvements in classification accuracy,\nspecificity and sensitivity, whether there is a tumor in the early diagnosis,\nwhat kind of tumor, or whether metastasis occurs after tumor surgery.\n", "versions": [{"version": "v1", "created": "Fri, 9 Mar 2018 15:29:20 GMT"}, {"version": "v2", "created": "Fri, 6 Apr 2018 22:29:34 GMT"}, {"version": "v3", "created": "Tue, 17 Apr 2018 12:33:51 GMT"}, {"version": "v4", "created": "Wed, 27 Feb 2019 22:35:11 GMT"}], "update_date": "2019-03-01", "authors_parsed": [["Yang", "Xiaohui", ""], ["Wu", "Wenming", ""], ["Chen", "Yunmei", ""], ["Li", "Xianqi", ""], ["Zhang", "Juan", ""], ["Long", "Dan", ""], ["Yang", "Lijun", ""]]}, {"id": "1803.03577", "submitter": "Stefan Zernetsch", "authors": "Michael Goldhammer, Sebastian K\\\"ohler, Stefan Zernetsch, Konrad Doll,\n  Bernhard Sick, Klaus Dietmayer", "title": "Intentions of Vulnerable Road Users - Detection and Forecasting by Means\n  of Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Avoiding collisions with vulnerable road users (VRUs) using sensor-based\nearly recognition of critical situations is one of the manifold opportunities\nprovided by the current development in the field of intelligent vehicles. As\nespecially pedestrians and cyclists are very agile and have a variety of\nmovement options, modeling their behavior in traffic scenes is a challenging\ntask. In this article we propose movement models based on machine learning\nmethods, in particular artificial neural networks, in order to classify the\ncurrent motion state and to predict the future trajectory of VRUs. Both model\ntypes are also combined to enable the application of specifically trained\nmotion predictors based on a continuously updated pseudo probabilistic state\nclassification. Furthermore, the architecture is used to evaluate\nmotion-specific physical models for starting and stopping and video-based\npedestrian motion classification. A comprehensive dataset consisting of 1068\npedestrian and 494 cyclist scenes acquired at an urban intersection is used for\noptimization, training, and evaluation of the different models. The results\nshow substantial higher classification rates and the ability to earlier\nrecognize motion state changes with the machine learning approaches compared to\ninteracting multiple model (IMM) Kalman Filtering. The trajectory prediction\nquality is also improved for all kinds of test scenes, especially when starting\nand stopping motions are included. Here, 37\\% and 41\\% lower position errors\nwere achieved on average, respectively.\n", "versions": [{"version": "v1", "created": "Fri, 9 Mar 2018 15:49:07 GMT"}], "update_date": "2018-03-12", "authors_parsed": [["Goldhammer", "Michael", ""], ["K\u00f6hler", "Sebastian", ""], ["Zernetsch", "Stefan", ""], ["Doll", "Konrad", ""], ["Sick", "Bernhard", ""], ["Dietmayer", "Klaus", ""]]}, {"id": "1803.03711", "submitter": "Pascal Getreuer", "authors": "Frank Ong and Peyman Milanfar and Pascal Getreuer", "title": "Local Kernels that Approximate Bayesian Regularization and Proximal\n  Operators", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2019.2893071", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we broadly connect kernel-based filtering (e.g. approaches such\nas the bilateral filters and nonlocal means, but also many more) with general\nvariational formulations of Bayesian regularized least squares, and the related\nconcept of proximal operators. The latter set of variational/Bayesian/proximal\nformulations often result in optimization problems that do not have closed-form\nsolutions, and therefore typically require global iterative solutions. Our main\ncontribution here is to establish how one can approximate the solution of the\nresulting global optimization problems with use of locally adaptive filters\nwith specific kernels. Our results are valid for small regularization strength\nbut the approach is powerful enough to be useful for a wide range of\napplications because we expose how to derive a \"kernelized\" solution to these\nproblems that approximates the global solution in one-shot, using only local\noperations. As another side benefit in the reverse direction, given a local\ndata-adaptive filter constructed with a particular choice of kernel, we enable\nthe interpretation of such filters in the variational/Bayesian/proximal\nframework.\n", "versions": [{"version": "v1", "created": "Fri, 9 Mar 2018 22:19:38 GMT"}], "update_date": "2019-05-01", "authors_parsed": [["Ong", "Frank", ""], ["Milanfar", "Peyman", ""], ["Getreuer", "Pascal", ""]]}, {"id": "1803.03724", "submitter": "Pablo Su\\'arez-Serrato", "authors": "P. Su\\'arez-Serrato, E.I. Vel\\'azquez Richards", "title": "Contour Parametrization via Anisotropic Mean Curvature Flows", "comments": "30 pages, 20 images, source code for our numerical implementation is\n  available in this URL https://github.com/V3du4rd0/AMCF", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.DG cs.CG cs.CV math.AP math.NA", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present a new implementation of anisotropic mean curvature flow for\ncontour recognition. Our procedure couples the mean curvature flow of planar\nclosed smooth curves, with an external field from a potential of point-wise\ncharges. This coupling constrains the motion when the curve matches a picture\nplaced as background. We include a stability criteria for our numerical\napproximation.\n", "versions": [{"version": "v1", "created": "Sat, 10 Mar 2018 00:05:01 GMT"}], "update_date": "2018-03-13", "authors_parsed": [["Su\u00e1rez-Serrato", "P.", ""], ["Richards", "E. I. Vel\u00e1zquez", ""]]}, {"id": "1803.03729", "submitter": "Jordan Malof", "authors": "Jordan M. Malof, Daniel Reichman, Andrew Karem, Hichem Frigui, Dominic\n  K. C. Ho, Joseph N. Wilson, Wen-Hsiung Lee, William Cummings, and Leslie M.\n  Collins", "title": "A Large-Scale Multi-Institutional Evaluation of Advanced Discrimination\n  Algorithms for Buried Threat Detection in Ground Penetrating Radar", "comments": "IEEE Transactions on Geoscience and Remote Sensing (2019)", "journal-ref": null, "doi": "10.1109/TGRS.2019.2909665", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider the development of algorithms for the automatic\ndetection of buried threats using ground penetrating radar (GPR) measurements.\nGPR is one of the most studied and successful modalities for automatic buried\nthreat detection (BTD), and a large variety of BTD algorithms have been\nproposed for it. Despite this, large-scale comparisons of GPR-based BTD\nalgorithms are rare in the literature. In this work we report the results of a\nmulti-institutional effort to develop advanced buried threat detection\nalgorithms for a real-world GPR BTD system. The effort involved five\ninstitutions with substantial experience with the development of GPR-based BTD\nalgorithms. In this paper we report the technical details of the advanced\nalgorithms submitted by each institution, representing their latest technical\nadvances, and many state-of-the-art GPR-based BTD algorithms. We also report\nthe results of evaluating the algorithms from each institution on the large\nexperimental dataset used for development. The experimental dataset comprised\n120,000 m^2 of GPR data using surface area, from 13 different lanes across two\nUS test sites. The data was collected using a vehicle-mounted GPR system, the\nvariants of which have supplied data for numerous publications. Using these\nresults, we identify the most successful and common processing strategies among\nthe submitted algorithms, and make recommendations for GPR-based BTD algorithm\ndesign.\n", "versions": [{"version": "v1", "created": "Sat, 10 Mar 2018 00:37:44 GMT"}, {"version": "v2", "created": "Thu, 7 Jun 2018 18:49:29 GMT"}], "update_date": "2019-05-16", "authors_parsed": [["Malof", "Jordan M.", ""], ["Reichman", "Daniel", ""], ["Karem", "Andrew", ""], ["Frigui", "Hichem", ""], ["Ho", "Dominic K. C.", ""], ["Wilson", "Joseph N.", ""], ["Lee", "Wen-Hsiung", ""], ["Cummings", "William", ""], ["Collins", "Leslie M.", ""]]}, {"id": "1803.03778", "submitter": "Liangfu Chen", "authors": "Liangfu Chen, Zeng Yang, Jianjun Ma, Zheng Luo", "title": "Driving Scene Perception Network: Real-time Joint Detection, Depth\n  Estimation and Semantic Segmentation", "comments": "9 pages, 7 figures, WACV'18", "journal-ref": null, "doi": "10.1109/WACV.2018.00145", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the demand for enabling high-level autonomous driving has increased in\nrecent years and visual perception is one of the critical features to enable\nfully autonomous driving, in this paper, we introduce an efficient approach for\nsimultaneous object detection, depth estimation and pixel-level semantic\nsegmentation using a shared convolutional architecture. The proposed network\nmodel, which we named Driving Scene Perception Network (DSPNet), uses\nmulti-level feature maps and multi-task learning to improve the accuracy and\nefficiency of object detection, depth estimation and image segmentation tasks\nfrom a single input image. Hence, the resulting network model uses less than\n850 MiB of GPU memory and achieves 14.0 fps on NVIDIA GeForce GTX 1080 with a\n1024x512 input image, and both precision and efficiency have been improved over\ncombination of single tasks.\n", "versions": [{"version": "v1", "created": "Sat, 10 Mar 2018 08:55:46 GMT"}], "update_date": "2018-03-13", "authors_parsed": [["Chen", "Liangfu", ""], ["Yang", "Zeng", ""], ["Ma", "Jianjun", ""], ["Luo", "Zheng", ""]]}, {"id": "1803.03816", "submitter": "Mostafa Gamal", "authors": "Mostafa Gamal, Mennatullah Siam, Moemen Abdel-Razek", "title": "ShuffleSeg: Real-time Semantic Segmentation Network", "comments": "6 pages, under review by ICIP 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-time semantic segmentation is of significant importance for mobile and\nrobotics related applications. We propose a computationally efficient\nsegmentation network which we term as ShuffleSeg. The proposed architecture is\nbased on grouped convolution and channel shuffling in its encoder for improving\nthe performance. An ablation study of different decoding methods is compared\nincluding Skip architecture, UNet, and Dilation Frontend. Interesting insights\non the speed and accuracy tradeoff is discussed. It is shown that skip\narchitecture in the decoding method provides the best compromise for the goal\nof real-time performance, while it provides adequate accuracy by utilizing\nhigher resolution feature maps for a more accurate segmentation. ShuffleSeg is\nevaluated on CityScapes and compared against the state of the art real-time\nsegmentation networks. It achieves 2x GFLOPs reduction, while it provides on\npar mean intersection over union of 58.3% on CityScapes test set. ShuffleSeg\nruns at 15.7 frames per second on NVIDIA Jetson TX2, which makes it of great\npotential for real-time applications.\n", "versions": [{"version": "v1", "created": "Sat, 10 Mar 2018 14:28:45 GMT"}, {"version": "v2", "created": "Thu, 15 Mar 2018 10:08:00 GMT"}], "update_date": "2018-03-16", "authors_parsed": [["Gamal", "Mostafa", ""], ["Siam", "Mennatullah", ""], ["Abdel-Razek", "Moemen", ""]]}, {"id": "1803.03827", "submitter": "Albert Gatt", "authors": "Albert Gatt, Marc Tanti, Adrian Muscat, Patrizia Paggio, Reuben A.\n  Farrugia, Claudia Borg, Kenneth P. Camilleri, Mike Rosner, Lonneke van der\n  Plas", "title": "Face2Text: Collecting an Annotated Image Description Corpus for the\n  Generation of Rich Face Descriptions", "comments": "Proceedings of the 11th edition of the Language Resources and\n  Evaluation Conference (LREC'18)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The past few years have witnessed renewed interest in NLP tasks at the\ninterface between vision and language. One intensively-studied problem is that\nof automatically generating text from images. In this paper, we extend this\nproblem to the more specific domain of face description. Unlike scene\ndescriptions, face descriptions are more fine-grained and rely on attributes\nextracted from the image, rather than objects and relations. Given that no data\nexists for this task, we present an ongoing crowdsourcing study to collect a\ncorpus of descriptions of face images taken `in the wild'. To gain a better\nunderstanding of the variation we find in face description and the possible\nissues that this may raise, we also conducted an annotation study on a subset\nof the corpus. Primarily, we found descriptions to refer to a mixture of\nattributes, not only physical, but also emotional and inferential, which is\nbound to create further challenges for current image-to-text methods.\n", "versions": [{"version": "v1", "created": "Sat, 10 Mar 2018 15:52:08 GMT"}, {"version": "v2", "created": "Fri, 5 Mar 2021 07:32:51 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Gatt", "Albert", ""], ["Tanti", "Marc", ""], ["Muscat", "Adrian", ""], ["Paggio", "Patrizia", ""], ["Farrugia", "Reuben A.", ""], ["Borg", "Claudia", ""], ["Camilleri", "Kenneth P.", ""], ["Rosner", "Mike", ""], ["van der Plas", "Lonneke", ""]]}, {"id": "1803.03828", "submitter": "Oluwarotimi Giwa", "authors": "Oluwarotimi Giwa and Abdsamad Benkrid", "title": "Fire detection in a still image using colour information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Colour analysis is a crucial step in image-based fire detection algorithms.\nMany of the proposed fire detection algorithms in a still image are prone to\nfalse alarms caused by objects with a colour similar to fire. To design a\ncolour-based system with a better false alarm rate, a new\ncolour-differentiating conversion matrix, efficient on images of high colour\ncomplexity, is proposed. The elements of this conversion matrix are obtained by\nperforming K-medoids clustering and Particle Swarm Optimisation procedures on a\nfire sample image with a background of high fire-colour similarity. The\nproposed conversion matrix is then used to construct two new fire colour\ndetection frameworks. The first detection method is a two-stage non-linear\nimage transformation framework, while the second is a direct transformation of\nan image with the proposed conversion matrix. A performance comparison of the\nproposed methods with alternate methods in the literature was carried out.\nExperimental results indicate that the linear image transformation method\noutperforms other methods regarding false alarm rate while the non-linear\ntwo-stage image transformation method has the best performance on the F-score\nmetric and provides a better trade-off between missed detection and false alarm\nrate.\n", "versions": [{"version": "v1", "created": "Sat, 10 Mar 2018 15:52:46 GMT"}], "update_date": "2018-03-13", "authors_parsed": [["Giwa", "Oluwarotimi", ""], ["Benkrid", "Abdsamad", ""]]}, {"id": "1803.03837", "submitter": "Zhigang Jia", "authors": "Meixiang Zhao, Zhigang Jia, Dunwei Gong", "title": "Sample-Relaxed Two-Dimensional Color Principal Component Analysis for\n  Face Recognition and Image Reconstruction", "comments": "18 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A sample-relaxed two-dimensional color principal component analysis\n(SR-2DCPCA) approach is presented for face recognition and image reconstruction\nbased on quaternion models. A relaxation vector is automatically generated\naccording to the variances of training color face images with the same label. A\nsample-relaxed, low-dimensional covariance matrix is constructed based on all\nthe training samples relaxed by a relaxation vector, and its eigenvectors\ncorresponding to the $r$ largest eigenvalues are defined as the optimal\nprojection. The SR-2DCPCA aims to enlarge the global variance rather than to\nmaximize the variance of the projected training samples. The numerical results\nbased on real face data sets validate that SR-2DCPCA has a higher recognition\nrate than state-of-the-art methods and is efficient in image reconstruction.\n", "versions": [{"version": "v1", "created": "Sat, 10 Mar 2018 16:49:54 GMT"}], "update_date": "2018-03-13", "authors_parsed": [["Zhao", "Meixiang", ""], ["Jia", "Zhigang", ""], ["Gong", "Dunwei", ""]]}, {"id": "1803.03849", "submitter": "Arda Senocak", "authors": "Arda Senocak, Tae-Hyun Oh, Junsik Kim, Ming-Hsuan Yang, In So Kweon", "title": "Learning to Localize Sound Source in Visual Scenes", "comments": "To appear in CVPR 2018. Total 9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual events are usually accompanied by sounds in our daily lives. We pose\nthe question: Can the machine learn the correspondence between visual scene and\nthe sound, and localize the sound source only by observing sound and visual\nscene pairs like human? In this paper, we propose a novel unsupervised\nalgorithm to address the problem of localizing the sound source in visual\nscenes. A two-stream network structure which handles each modality, with\nattention mechanism is developed for sound source localization. Moreover,\nalthough our network is formulated within the unsupervised learning framework,\nit can be extended to a unified architecture with a simple modification for the\nsupervised and semi-supervised learning settings as well. Meanwhile, a new\nsound source dataset is developed for performance evaluation. Our empirical\nevaluation shows that the unsupervised method eventually go through false\nconclusion in some cases. We show that even with a few supervision, false\nconclusion is able to be corrected and the source of sound in a visual scene\ncan be localized effectively.\n", "versions": [{"version": "v1", "created": "Sat, 10 Mar 2018 18:19:02 GMT"}], "update_date": "2019-02-18", "authors_parsed": [["Senocak", "Arda", ""], ["Oh", "Tae-Hyun", ""], ["Kim", "Junsik", ""], ["Yang", "Ming-Hsuan", ""], ["Kweon", "In So", ""]]}, {"id": "1803.03851", "submitter": "Pan Li", "authors": "Pan Li and Olgica Milenkovic", "title": "Revisiting Decomposable Submodular Function Minimization with Incidence\n  Relations", "comments": "A part of this work will be presented in NIPS2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new approach to decomposable submodular function minimization\n(DSFM) that exploits incidence relations. Incidence relations describe which\nvariables effectively influence the component functions, and when properly\nutilized, they allow for improving the convergence rates of DSFM solvers. Our\nmain results include the precise parametrization of the DSFM problem based on\nincidence relations, the development of new scalable alternative projections\nand parallel coordinate descent methods and an accompanying rigorous analysis\nof their convergence rates.\n", "versions": [{"version": "v1", "created": "Sat, 10 Mar 2018 18:40:07 GMT"}, {"version": "v2", "created": "Sat, 8 Sep 2018 07:37:50 GMT"}, {"version": "v3", "created": "Mon, 24 Sep 2018 22:35:05 GMT"}], "update_date": "2018-09-26", "authors_parsed": [["Li", "Pan", ""], ["Milenkovic", "Olgica", ""]]}, {"id": "1803.03852", "submitter": "Nils Gessert", "authors": "Nils Gessert and Matthias Schl\\\"uter and Alexander Schlaefer", "title": "A Deep Learning Approach for Pose Estimation from Volumetric OCT Data", "comments": "https://doi.org/10.1016/j.media.2018.03.002", "journal-ref": null, "doi": "10.1016/j.media.2018.03.002", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tracking the pose of instruments is a central problem in image-guided\nsurgery. For microscopic scenarios, optical coherence tomography (OCT) is\nincreasingly used as an imaging modality. OCT is suitable for accurate pose\nestimation due to its micrometer range resolution and volumetric field of view.\nHowever, OCT image processing is challenging due to speckle noise and\nreflection artifacts in addition to the images' 3D nature. We address pose\nestimation from OCT volume data with a new deep learning-based tracking\nframework. For this purpose, we design a new 3D convolutional neural network\n(CNN) architecture to directly predict the 6D pose of a small marker geometry\nfrom OCT volumes. We use a hexapod robot to automatically acquire labeled data\npoints which we use to train 3D CNN architectures for multi-output regression.\nWe use this setup to provide an in-depth analysis on deep learning-based pose\nestimation from volumes. Specifically, we demonstrate that exploiting volume\ninformation for pose estimation yields higher accuracy than relying on 2D\nrepresentations with depth information. Supporting this observation, we provide\nquantitative and qualitative results that 3D CNNs effectively exploit the depth\nstructure of marker objects. Regarding the deep learning aspect, we present\nefficient design principles for 3D CNNs, making use of insights from the 2D\ndeep learning community. In particular, we present Inception3D as a new\narchitecture which performs best for our application. We show that our deep\nlearning approach reaches errors at our ground-truth label's resolution. We\nachieve a mean average error of $\\SI{14.89 \\pm 9.3}{\\micro\\metre}$ and\n$\\SI{0.096 \\pm 0.072}{\\degree}$ for position and orientation learning,\nrespectively.\n", "versions": [{"version": "v1", "created": "Sat, 10 Mar 2018 18:48:18 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Gessert", "Nils", ""], ["Schl\u00fcter", "Matthias", ""], ["Schlaefer", "Alexander", ""]]}, {"id": "1803.03857", "submitter": "Li Niu", "authors": "Li Niu, Qingtao Tang, Ashok Veeraraghavan, Ashu Sabharwal", "title": "Learning from Noisy Web Data with Category-level Supervision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As tons of photos are being uploaded to public websites (e.g., Flickr, Bing,\nand Google) every day, learning from web data has become an increasingly\npopular research direction because of freely available web resources, which is\nalso referred to as webly supervised learning. Nevertheless, the performance\ngap between webly supervised learning and traditional supervised learning is\nstill very large, owning to the label noise of web data. To be exact, the\nlabels of images crawled from public websites are very noisy and often\ninaccurate. Some existing works tend to facilitate learning from web data with\nthe aid of extra information, such as augmenting or purifying web data by\nvirtue of instance-level supervision, which is usually in demand of heavy\nmanual annotation. Instead, we propose to tackle the label noise by leveraging\nmore accessible category-level supervision. In particular, we build our method\nupon variational autoencoder (VAE), in which the classification network is\nattached on the hidden layer of VAE in a way that the classification network\nand VAE can jointly leverage the category-level hybrid semantic information.\nThe effectiveness of our proposed method is clearly demonstrated by extensive\nexperiments on three benchmark datasets.\n", "versions": [{"version": "v1", "created": "Sat, 10 Mar 2018 19:30:43 GMT"}, {"version": "v2", "created": "Mon, 9 Apr 2018 17:19:43 GMT"}, {"version": "v3", "created": "Thu, 24 May 2018 05:41:17 GMT"}], "update_date": "2018-05-25", "authors_parsed": [["Niu", "Li", ""], ["Tang", "Qingtao", ""], ["Veeraraghavan", "Ashok", ""], ["Sabharwal", "Ashu", ""]]}, {"id": "1803.03879", "submitter": "Kan Chen", "authors": "Kan Chen and Jiyang Gao and Ram Nevatia", "title": "Knowledge Aided Consistency for Weakly Supervised Phrase Grounding", "comments": "CVPR 2018 conference paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a natural language query, a phrase grounding system aims to localize\nmentioned objects in an image. In weakly supervised scenario, mapping between\nimage regions (i.e., proposals) and language is not available in the training\nset. Previous methods address this deficiency by training a grounding system\nvia learning to reconstruct language information contained in input queries\nfrom predicted proposals. However, the optimization is solely guided by the\nreconstruction loss from the language modality, and ignores rich visual\ninformation contained in proposals and useful cues from external knowledge. In\nthis paper, we explore the consistency contained in both visual and language\nmodalities, and leverage complementary external knowledge to facilitate weakly\nsupervised grounding. We propose a novel Knowledge Aided Consistency Network\n(KAC Net) which is optimized by reconstructing input query and proposal's\ninformation. To leverage complementary knowledge contained in the visual\nfeatures, we introduce a Knowledge Based Pooling (KBP) gate to focus on\nquery-related proposals. Experiments show that KAC Net provides a significant\nimprovement on two popular datasets.\n", "versions": [{"version": "v1", "created": "Sun, 11 Mar 2018 02:00:24 GMT"}], "update_date": "2018-03-13", "authors_parsed": [["Chen", "Kan", ""], ["Gao", "Jiyang", ""], ["Nevatia", "Ram", ""]]}, {"id": "1803.03893", "submitter": "Huangying Zhan", "authors": "Huangying Zhan, Ravi Garg, Chamara Saroj Weerasekera, Kejie Li, Harsh\n  Agarwal, Ian Reid", "title": "Unsupervised Learning of Monocular Depth Estimation and Visual Odometry\n  with Deep Feature Reconstruction", "comments": "8 pages, 6 figures, CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite learning based methods showing promising results in single view depth\nestimation and visual odometry, most existing approaches treat the tasks in a\nsupervised manner. Recent approaches to single view depth estimation explore\nthe possibility of learning without full supervision via minimizing photometric\nerror. In this paper, we explore the use of stereo sequences for learning depth\nand visual odometry. The use of stereo sequences enables the use of both\nspatial (between left-right pairs) and temporal (forward backward) photometric\nwarp error, and constrains the scene depth and camera motion to be in a common,\nreal-world scale. At test time our framework is able to estimate single view\ndepth and two-view odometry from a monocular sequence. We also show how we can\nimprove on a standard photometric warp loss by considering a warp of deep\nfeatures. We show through extensive experiments that: (i) jointly training for\nsingle view depth and visual odometry improves depth prediction because of the\nadditional constraint imposed on depths and achieves competitive results for\nvisual odometry; (ii) deep feature-based warping loss improves upon simple\nphotometric warp loss for both single view depth estimation and visual\nodometry. Our method outperforms existing learning based methods on the KITTI\ndriving dataset in both tasks. The source code is available at\nhttps://github.com/Huangying-Zhan/Depth-VO-Feat\n", "versions": [{"version": "v1", "created": "Sun, 11 Mar 2018 03:56:29 GMT"}, {"version": "v2", "created": "Tue, 13 Mar 2018 05:55:35 GMT"}, {"version": "v3", "created": "Thu, 5 Apr 2018 01:12:43 GMT"}], "update_date": "2018-04-06", "authors_parsed": [["Zhan", "Huangying", ""], ["Garg", "Ravi", ""], ["Weerasekera", "Chamara Saroj", ""], ["Li", "Kejie", ""], ["Agarwal", "Harsh", ""], ["Reid", "Ian", ""]]}, {"id": "1803.03906", "submitter": "Kurt Riedel", "authors": "Alexander Sidorenko, Kurt S. Riedel", "title": "Adaptive Kernel Estimation of the Spectral Density with Boundary Kernel\n  Analysis", "comments": null, "journal-ref": "Approximation Theory VIII: Approximation And Interpolation, pg\n  519-528, edited by Chui, Schumaker, 1995 World Scientific", "doi": null, "report-no": null, "categories": "stat.ME cs.CV eess.AS eess.SP math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A hybrid estimator of the log-spectral density of a stationary time series is\nproposed. First, a multiple taper estimate is performed, followed by kernel\nsmoothing the log-multitaper estimate. This procedure reduces the expected mean\nsquare error by $({\\pi^2 \\over 4})^{.8}$ over simply smoothing the log tapered\nperiodogram. The optimal number of tapers is $O(N^{8/15})$. A data adaptive\nimplementation of a variable bandwidth kernel smoother is given. When the\nspectral density is discontinuous, one sided smoothing estimates are used.\n", "versions": [{"version": "v1", "created": "Sun, 11 Mar 2018 05:14:42 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Sidorenko", "Alexander", ""], ["Riedel", "Kurt S.", ""]]}, {"id": "1803.03932", "submitter": "Marius Huber", "authors": "Marius Huber, Timo Hinzmann, Roland Siegwart, and Larry H. Matthies", "title": "Cubic Range Error Model for Stereo Vision with Illuminators", "comments": "6 pages, to be published at ICRA 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Use of low-cost depth sensors, such as a stereo camera setup with\nilluminators, is of particular interest for numerous applications ranging from\nrobotics and transportation to mixed and augmented reality. The ability to\nquantify noise is crucial for these applications, e.g., when the sensor is used\nfor map generation or to develop a sensor scheduling policy in a multi-sensor\nsetup. Range error models provide uncertainty estimates and help weigh the data\ncorrectly in instances where range measurements are taken from different\nvantage points or with different sensors. The weighing is important to fuse\nrange data into a map in a meaningful way, i.e., the high confidence data is\nrelied on most heavily. Such a model is derived in this work. We show that the\nrange error for stereo systems with integrated illuminators is cubic and\nvalidate the proposed model experimentally with an off-the-shelf structured\nlight stereo system. The experiments confirm the validity of the model and\nsimplify the application of this type of sensor in robotics. The proposed error\nmodel is relevant to any stereo system with low ambient light where the main\nlight source is located at the camera system. Among others, this is the case\nfor structured light stereo systems and night stereo systems with headlights.\nIn this work, we propose that the range error is cubic in range for stereo\nsystems with integrated illuminators. Experimental validation with an\noff-the-shelf structured light stereo system shows that the exponent is between\n2.4 and 2.6. The deviation is attributed to our model considering only shot\nnoise.\n", "versions": [{"version": "v1", "created": "Sun, 11 Mar 2018 10:23:25 GMT"}], "update_date": "2018-03-13", "authors_parsed": [["Huber", "Marius", ""], ["Hinzmann", "Timo", ""], ["Siegwart", "Roland", ""], ["Matthies", "Larry H.", ""]]}, {"id": "1803.03963", "submitter": "Song Guo", "authors": "Song Guo, Kai Wang, Hong Kang, Yujun Zhang, Yingqi Gao, Tao Li", "title": "BTS-DSN: Deeply Supervised Neural Network with Short Connections for\n  Retinal Vessel Segmentation", "comments": null, "journal-ref": "International Journal of Medical Informatics 126, 105-113 (Jun,\n  2019)", "doi": "10.1016/j.ijmedinf.2019.03.015", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background and Objective: The condition of vessel of the human eye is an\nimportant factor for the diagnosis of ophthalmological diseases. Vessel\nsegmentation in fundus images is a challenging task due to complex vessel\nstructure, the presence of similar structures such as microaneurysms and\nhemorrhages, micro-vessel with only one to several pixels wide, and\nrequirements for finer results. Methods:In this paper, we present a multi-scale\ndeeply supervised network with short connections (BTS-DSN) for vessel\nsegmentation. We used short connections to transfer semantic information\nbetween side-output layers. Bottom-top short connections pass low level\nsemantic information to high level for refining results in high-level\nside-outputs, and top-bottom short connection passes much structural\ninformation to low level for reducing noises in low-level side-outputs. In\naddition, we employ cross-training to show that our model is suitable for real\nworld fundus images. Results: The proposed BTS-DSN has been verified on DRIVE,\nSTARE and CHASE_DB1 datasets, and showed competitive performance over other\nstate-of-the-art methods. Specially, with patch level input, the network\nachieved 0.7891/0.8212 sensitivity, 0.9804/0.9843 specificity, 0.9806/0.9859\nAUC, and 0.8249/0.8421 F1-score on DRIVE and STARE, respectively. Moreover, our\nmodel behaves better than other methods in cross-training experiments.\nConclusions: BTS-DSN achieves competitive performance in vessel segmentation\ntask on three public datasets. It is suitable for vessel segmentation. The\nsource code of our method is available at https://github.com/guomugong/BTS-DSN.\n", "versions": [{"version": "v1", "created": "Sun, 11 Mar 2018 14:10:28 GMT"}, {"version": "v2", "created": "Mon, 23 Sep 2019 02:06:55 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Guo", "Song", ""], ["Wang", "Kai", ""], ["Kang", "Hong", ""], ["Zhang", "Yujun", ""], ["Gao", "Yingqi", ""], ["Li", "Tao", ""]]}, {"id": "1803.04022", "submitter": "Shahin Aghdam", "authors": "Shahin Mahdizadehaghdam, Ashkan Panahi, Hamid Krim, Liyi Dai", "title": "Deep Dictionary Learning: A PARametric NETwork Approach", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2019.2914376", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep dictionary learning seeks multiple dictionaries at different image\nscales to capture complementary coherent characteristics. We propose a method\nfor learning a hierarchy of synthesis dictionaries with an image classification\ngoal. The dictionaries and classification parameters are trained by a\nclassification objective, and the sparse features are extracted by reducing a\nreconstruction loss in each layer. The reconstruction objectives in some sense\nregularize the classification problem and inject source signal information in\nthe extracted features. The performance of the proposed hierarchical method\nincreases by adding more layers, which consequently makes this model easier to\ntune and adapt. The proposed algorithm furthermore, shows remarkably lower\nfooling rate in presence of adversarial perturbation. The validation of the\nproposed approach is based on its classification performance using four\nbenchmark datasets and is compared to a CNN of similar size.\n", "versions": [{"version": "v1", "created": "Sun, 11 Mar 2018 19:29:56 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Mahdizadehaghdam", "Shahin", ""], ["Panahi", "Ashkan", ""], ["Krim", "Hamid", ""], ["Dai", "Liyi", ""]]}, {"id": "1803.04033", "submitter": "Bartosz Micha{\\l} Zieli\\'nski", "authors": "Bartosz Zieli\\'nski, {\\L}ukasz Struski, Marek \\'Smieja, Jacek Tabor", "title": "Cascade context encoder for improved inpainting", "comments": "Supplemental materials are available at\n  http://www.ii.uj.edu.pl/~zielinsb", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we analyze if cascade usage of the context encoder with\nincreasing input can improve the results of the inpainting. For this purpose,\nwe train context encoder for 64x64 pixels images in a standard way and use its\nresized output to fill in the missing input region of the 128x128 context\nencoder, both in training and evaluation phase. As the result, the inpainting\nis visibly more plausible. In order to thoroughly verify the results, we\nintroduce normalized squared-distortion, a measure for quantitative inpainting\nevaluation, and we provide its mathematical explanation. This is the first\nattempt to formalize the inpainting measure, which is based on the properties\nof latent feature representation, instead of L2 reconstruction loss.\n", "versions": [{"version": "v1", "created": "Sun, 11 Mar 2018 20:31:39 GMT"}], "update_date": "2018-03-13", "authors_parsed": [["Zieli\u0144ski", "Bartosz", ""], ["Struski", "\u0141ukasz", ""], ["\u015amieja", "Marek", ""], ["Tabor", "Jacek", ""]]}, {"id": "1803.04048", "submitter": "Xiaoxiao Du", "authors": "Xiaoxiao Du and Alina Zare", "title": "Multiple Instance Choquet Integral Classifier Fusion and Regression for\n  Remote Sensing Applications", "comments": null, "journal-ref": null, "doi": "10.1109/TGRS.2018.2876687", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In classifier (or regression) fusion the aim is to combine the outputs of\nseveral algorithms to boost overall performance. Standard supervised fusion\nalgorithms often require accurate and precise training labels. However,\naccurate labels may be difficult to obtain in many remote sensing applications.\nThis paper proposes novel classification and regression fusion models that can\nbe trained given ambiguosly and imprecisely labeled training data in which\ntraining labels are associated with sets of data points (i.e., \"bags\") instead\nof individual data points (i.e., \"instances\") following a multiple instance\nlearning framework. Experiments were conducted based on the proposed algorithms\non both synthetic data and applications such as target detection and crop yield\nprediction given remote sensing data. The proposed algorithms show effective\nclassification and regression performance.\n", "versions": [{"version": "v1", "created": "Sun, 11 Mar 2018 21:39:45 GMT"}, {"version": "v2", "created": "Mon, 18 Feb 2019 17:34:57 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Du", "Xiaoxiao", ""], ["Zare", "Alina", ""]]}, {"id": "1803.04053", "submitter": "Navaneeth Kamballur Kottayil", "authors": "Navaneeth Kamballur Kottayil, Giuseppe Valenzise, Frederic Dufaux and\n  Irene Cheng", "title": "Learning Local Distortion Visibility From Image Quality Data-sets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate prediction of local distortion visibility thresholds is critical in\nmany image and video processing applications. Existing methods require an\naccurate modeling of the human visual system, and are derived through\npshycophysical experiments with simple, artificial stimuli. These approaches,\nhowever, are difficult to generalize to natural images with complex types of\ndistortion. In this paper, we explore a different perspective, and we\ninvestigate whether it is possible to learn local distortion visibility from\nimage quality scores. We propose a convolutional neural network based\noptimization framework to infer local detection thresholds in a distorted\nimage. Our model is trained on multiple quality datasets, and the results are\ncorrelated with empirical visibility thresholds collected on complex stimuli in\na recent study. Our results are comparable to state-of-the-art mathematical\nmodels that were trained on phsycovisual data directly. This suggests that it\nis possible to predict psychophysical phenomena from visibility information\nembedded in image quality scores.\n", "versions": [{"version": "v1", "created": "Sun, 11 Mar 2018 22:01:37 GMT"}], "update_date": "2018-03-13", "authors_parsed": [["Kottayil", "Navaneeth Kamballur", ""], ["Valenzise", "Giuseppe", ""], ["Dufaux", "Frederic", ""], ["Cheng", "Irene", ""]]}, {"id": "1803.04054", "submitter": "Azad Aminpour", "authors": "Kamyar Nazeri, Azad Aminpour, Mehran Ebrahimi", "title": "Two-Stage Convolutional Neural Network for Breast Cancer Histology Image\n  Classification", "comments": "10 pages, 5 figures, ICIAR 2018 conference", "journal-ref": "LNCS 10882 (2018) 717-726", "doi": "10.1007/978-3-319-93000-8_81", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper explores the problem of breast tissue classification of microscopy\nimages. Based on the predominant cancer type the goal is to classify images\ninto four categories of normal, benign, in situ carcinoma, and invasive\ncarcinoma. Given a suitable training dataset, we utilize deep learning\ntechniques to address the classification problem. Due to the large size of each\nimage in the training dataset, we propose a patch-based technique which\nconsists of two consecutive convolutional neural networks. The first\n\"patch-wise\" network acts as an auto-encoder that extracts the most salient\nfeatures of image patches while the second \"image-wise\" network performs\nclassification of the whole image. The first network is pre-trained and aimed\nat extracting local information while the second network obtains global\ninformation of an input image. We trained the networks using the ICIAR 2018\ngrand challenge on BreAst Cancer Histology (BACH) dataset. The proposed method\nyields 95 % accuracy on the validation set compared to previously reported 77 %\naccuracy rates in the literature. Our code is publicly available at\nhttps://github.com/ImagingLab/ICIAR2018\n", "versions": [{"version": "v1", "created": "Sun, 11 Mar 2018 22:05:33 GMT"}, {"version": "v2", "created": "Sun, 1 Apr 2018 16:37:19 GMT"}], "update_date": "2018-06-29", "authors_parsed": [["Nazeri", "Kamyar", ""], ["Aminpour", "Azad", ""], ["Ebrahimi", "Mehran", ""]]}, {"id": "1803.04103", "submitter": "Aditee Shrotre", "authors": "Aditee Shrotre, Lina Karam", "title": "Full Reference Objective Quality Assessment for Reconstructed Background\n  Images", "comments": "Associated source code: https://github.com/ashrotre/RBQI, Associated\n  Database:\n  https://drive.google.com/drive/folders/1bg8YRPIBcxpKIF9BIPisULPBPcA5x-Bk?usp=sharing\n  (Email for permissions at: ashrotre<at>asu<dot>edu)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With an increased interest in applications that require a clean background\nimage, such as video surveillance, object tracking, street view imaging and\nlocation-based services on web-based maps, multiple algorithms have been\ndeveloped to reconstruct a background image from cluttered scenes.\nTraditionally, statistical measures and existing image quality techniques have\nbeen applied for evaluating the quality of the reconstructed background images.\nThough these quality assessment methods have been widely used in the past,\ntheir performance in evaluating the perceived quality of the reconstructed\nbackground image has not been verified. In this work, we discuss the\nshortcomings in existing metrics and propose a full reference Reconstructed\nBackground image Quality Index (RBQI) that combines color and structural\ninformation at multiple scales using a probability summation model to predict\nthe perceived quality in the reconstructed background image given a reference\nimage. To compare the performance of the proposed quality index with existing\nimage quality assessment measures, we construct two different datasets\nconsisting of reconstructed background images and corresponding subjective\nscores. The quality assessment measures are evaluated by correlating their\nobjective scores with human subjective ratings. The correlation results show\nthat the proposed RBQI outperforms all the existing approaches. Additionally,\nthe constructed datasets and the corresponding subjective scores provide a\nbenchmark to evaluate the performance of future metrics that are developed to\nevaluate the perceived quality of reconstructed background images.\n", "versions": [{"version": "v1", "created": "Mon, 12 Mar 2018 03:26:56 GMT"}, {"version": "v2", "created": "Wed, 11 Apr 2018 18:15:25 GMT"}], "update_date": "2018-04-13", "authors_parsed": [["Shrotre", "Aditee", ""], ["Karam", "Lina", ""]]}, {"id": "1803.04108", "submitter": "Xuanyi Dong", "authors": "Xuanyi Dong, Yan Yan, Wanli Ouyang, Yi Yang", "title": "Style Aggregated Network for Facial Landmark Detection", "comments": "Accepted to CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in facial landmark detection achieve success by learning\ndiscriminative features from rich deformation of face shapes and poses. Besides\nthe variance of faces themselves, the intrinsic variance of image styles, e.g.,\ngrayscale vs. color images, light vs. dark, intense vs. dull, and so on, has\nconstantly been overlooked. This issue becomes inevitable as increasing web\nimages are collected from various sources for training neural networks. In this\nwork, we propose a style-aggregated approach to deal with the large intrinsic\nvariance of image styles for facial landmark detection. Our method transforms\noriginal face images to style-aggregated images by a generative adversarial\nmodule. The proposed scheme uses the style-aggregated image to maintain face\nimages that are more robust to environmental changes. Then the original face\nimages accompanying with style-aggregated ones play a duet to train a landmark\ndetector which is complementary to each other. In this way, for each face, our\nmethod takes two images as input, i.e., one in its original style and the other\nin the aggregated style. In experiments, we observe that the large variance of\nimage styles would degenerate the performance of facial landmark detectors.\nMoreover, we show the robustness of our method to the large variance of image\nstyles by comparing to a variant of our approach, in which the generative\nadversarial module is removed, and no style-aggregated images are used. Our\napproach is demonstrated to perform well when compared with state-of-the-art\nalgorithms on benchmark datasets AFLW and 300-W. Code is publicly available on\nGitHub: https://github.com/D-X-Y/SAN\n", "versions": [{"version": "v1", "created": "Mon, 12 Mar 2018 03:46:12 GMT"}, {"version": "v2", "created": "Tue, 13 Mar 2018 03:43:32 GMT"}, {"version": "v3", "created": "Thu, 15 Mar 2018 17:42:18 GMT"}, {"version": "v4", "created": "Thu, 22 Mar 2018 12:26:08 GMT"}], "update_date": "2018-03-23", "authors_parsed": [["Dong", "Xuanyi", ""], ["Yan", "Yan", ""], ["Ouyang", "Wanli", ""], ["Yang", "Yi", ""]]}, {"id": "1803.04125", "submitter": "Shervan Fekri-Ershad", "authors": "Shervan Fekri-Ershad", "title": "Innovative Texture Database Collecting Approach and Feature Extraction\n  Method based on Combination of Gray Tone Difference Matrixes, Local Binary\n  Patterns,and K-means Clustering", "comments": "First International Conference on Computer, Information Technology\n  and Communications (CCITC), 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Texture analysis and classification are some of the problems which have been\npaid much attention by image processing scientists since late 80s. If texture\nanalysis is done accurately, it can be used in many cases such as object\ntracking, visual pattern recognition, and face recognition.Since now, so many\nmethods are offered to solve this problem. Against their technical differences,\nall of them used same popular databases to evaluate their performance such\nasBrodatz or Outex, which may be made their performance biased on these\ndatabases. In this paper, an approach is proposed to collect more efficient\ndatabases of texture images. The proposed approach is included two stages. The\nfirst one is developing feature representation based on gray tone difference\nmatrixes and local binary patterns features and the next one is consisted an\ninnovative algorithm which is based on K-means clustering to collect images\nbased on evaluated features. In order to evaluate the performance of the\nproposed approach, a texture database is collected and fisher rate is computed\nfor collected one and well known databases. Also, texture classification is\nevaluated based on offered feature extraction and the accuracy is compared by\nsome state of the art texture classification methods.\n", "versions": [{"version": "v1", "created": "Mon, 12 Mar 2018 06:15:49 GMT"}], "update_date": "2018-03-13", "authors_parsed": [["Fekri-Ershad", "Shervan", ""]]}, {"id": "1803.04137", "submitter": "Xuefei Zhe", "authors": "Xuefei Zhe, Shifeng Chen, Hong Yan", "title": "Deep Class-Wise Hashing: Semantics-Preserving Hashing via Class-wise\n  Loss", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep supervised hashing has emerged as an influential solution to large-scale\nsemantic image retrieval problems in computer vision. In the light of recent\nprogress, convolutional neural network based hashing methods typically seek\npair-wise or triplet labels to conduct the similarity preserving learning.\nHowever, complex semantic concepts of visual contents are hard to capture by\nsimilar/dissimilar labels, which limits the retrieval performance. Generally,\npair-wise or triplet losses not only suffer from expensive training costs but\nalso lack in extracting sufficient semantic information. In this regard, we\npropose a novel deep supervised hashing model to learn more compact class-level\nsimilarity preserving binary codes. Our deep learning based model is motivated\nby deep metric learning that directly takes semantic labels as supervised\ninformation in training and generates corresponding discriminant hashing code.\nSpecifically, a novel cubic constraint loss function based on Gaussian\ndistribution is proposed, which preserves semantic variations while penalizes\nthe overlap part of different classes in the embedding space. To address the\ndiscrete optimization problem introduced by binary codes, a two-step\noptimization strategy is proposed to provide efficient training and avoid the\nproblem of gradient vanishing. Extensive experiments on four large-scale\nbenchmark databases show that our model can achieve the state-of-the-art\nretrieval performance. Moreover, when training samples are limited, our method\nsurpasses other supervised deep hashing methods with non-negligible margins.\n", "versions": [{"version": "v1", "created": "Mon, 12 Mar 2018 07:19:04 GMT"}], "update_date": "2018-03-13", "authors_parsed": [["Zhe", "Xuefei", ""], ["Chen", "Shifeng", ""], ["Yan", "Hong", ""]]}, {"id": "1803.04189", "submitter": "Samuli Laine", "authors": "Jaakko Lehtinen, Jacob Munkberg, Jon Hasselgren, Samuli Laine, Tero\n  Karras, Miika Aittala, Timo Aila", "title": "Noise2Noise: Learning Image Restoration without Clean Data", "comments": "Added link to official implementation and updated MRI results to\n  match it", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We apply basic statistical reasoning to signal reconstruction by machine\nlearning -- learning to map corrupted observations to clean signals -- with a\nsimple and powerful conclusion: it is possible to learn to restore images by\nonly looking at corrupted examples, at performance at and sometimes exceeding\ntraining using clean data, without explicit image priors or likelihood models\nof the corruption. In practice, we show that a single model learns photographic\nnoise removal, denoising synthetic Monte Carlo images, and reconstruction of\nundersampled MRI scans -- all corrupted by different processes -- based on\nnoisy data only.\n", "versions": [{"version": "v1", "created": "Mon, 12 Mar 2018 11:07:58 GMT"}, {"version": "v2", "created": "Thu, 9 Aug 2018 12:08:44 GMT"}, {"version": "v3", "created": "Mon, 29 Oct 2018 10:29:23 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Lehtinen", "Jaakko", ""], ["Munkberg", "Jacob", ""], ["Hasselgren", "Jon", ""], ["Laine", "Samuli", ""], ["Karras", "Tero", ""], ["Aittala", "Miika", ""], ["Aila", "Timo", ""]]}, {"id": "1803.04200", "submitter": "Ignacio Alvarez Illan", "authors": "Ignacio Alvarez Illan, Javier Ramirez, Juan M. Gorriz, Maria Adele\n  Marino, Daly Avenda\\~no, Thomas Helbich, Pascal Baltzer, Katja Pinker, Anke\n  Meyer-Baese", "title": "Automated detection and segmentation of non-mass enhancing breast tumors\n  with dynamic contrast-enhanced magnetic resonance imaging", "comments": "20 pages, 9 figures, Contrast Media and Molecular Imaging, in press", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-mass enhancing lesions (NME) constitute a diagnostic challenge in dynamic\ncontrast enhanced magnetic resonance imaging (DCE-MRI) of the breast. Computer\nAided Diagnosis (CAD) systems provide physicians with advanced tools for\nanalysis, assessment and evaluation that have a significant impact on the\ndiagnostic performance. Here, we propose a new approach to address the\nchallenge of NME detection and segmentation, taking advantage of independent\ncomponent analysis (ICA) to extract data-driven dynamic lesion\ncharacterizations. A set of independent sources was obtained from DCE-MRI\ndataset of breast patients, and the dynamic behavior of the different tissues\nwas described by multiple dynamic curves, together with a set of eigenimages\ndescribing the scores for each voxel. A new test image is projected onto the\nindependent source space using the unmixing matrix, and each voxel is\nclassified by a support vector machine (SVM) that has already been trained with\nmanually delineated data. A solution to the high false positive rate problem is\nproposed by controlling the SVM hyperplane location, outperforming previously\npublished approaches.\n", "versions": [{"version": "v1", "created": "Mon, 12 Mar 2018 11:26:43 GMT"}, {"version": "v2", "created": "Wed, 26 Sep 2018 11:55:18 GMT"}], "update_date": "2018-09-27", "authors_parsed": [["Illan", "Ignacio Alvarez", ""], ["Ramirez", "Javier", ""], ["Gorriz", "Juan M.", ""], ["Marino", "Maria Adele", ""], ["Avenda\u00f1o", "Daly", ""], ["Helbich", "Thomas", ""], ["Baltzer", "Pascal", ""], ["Pinker", "Katja", ""], ["Meyer-Baese", "Anke", ""]]}, {"id": "1803.04228", "submitter": "Tsun-Hsuan Wang", "authors": "Tsun-Hsuan Wang, Hung-Jui Huang, Juan-Ting Lin, Chan-Wei Hu, Kuo-Hao\n  Zeng, Min Sun", "title": "Omnidirectional CNN for Visual Place Recognition and Navigation", "comments": "8 pages. 6 figures. Accepted to 2018 IEEE International Conference on\n  Robotics and Automation (ICRA 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  $ $Visual place recognition is challenging, especially when only a few place\nexemplars are given. To mitigate the challenge, we consider place recognition\nmethod using omnidirectional cameras and propose a novel Omnidirectional\nConvolutional Neural Network (O-CNN) to handle severe camera pose variation.\nGiven a visual input, the task of the O-CNN is not to retrieve the matched\nplace exemplar, but to retrieve the closest place exemplar and estimate the\nrelative distance between the input and the closest place. With the ability to\nestimate relative distance, a heuristic policy is proposed to navigate a robot\nto the retrieved closest place. Note that the network is designed to take\nadvantage of the omnidirectional view by incorporating circular padding and\nrotation invariance. To train a powerful O-CNN, we build a virtual world for\ntraining on a large scale. We also propose a continuous lifted structured\nfeature embedding loss to learn the concept of distance efficiently. Finally,\nour experimental results confirm that our method achieves state-of-the-art\naccuracy and speed with both the virtual world and real-world datasets.\n", "versions": [{"version": "v1", "created": "Mon, 12 Mar 2018 12:49:50 GMT"}], "update_date": "2018-03-13", "authors_parsed": [["Wang", "Tsun-Hsuan", ""], ["Huang", "Hung-Jui", ""], ["Lin", "Juan-Ting", ""], ["Hu", "Chan-Wei", ""], ["Zeng", "Kuo-Hao", ""], ["Sun", "Min", ""]]}, {"id": "1803.04242", "submitter": "Xiaoxiao Li", "authors": "Xiaoxiao Li, Chen Change Loy", "title": "Video Object Segmentation with Joint Re-identification and\n  Attention-Aware Mask Propagation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of video object segmentation can become extremely challenging\nwhen multiple instances co-exist. While each instance may exhibit large scale\nand pose variations, the problem is compounded when instances occlude each\nother causing failures in tracking. In this study, we formulate a deep\nrecurrent network that is capable of segmenting and tracking objects in video\nsimultaneously by their temporal continuity, yet able to re-identify them when\nthey re-appear after a prolonged occlusion. We combine both temporal\npropagation and re-identification functionalities into a single framework that\ncan be trained end-to-end. In particular, we present a re-identification module\nwith template expansion to retrieve missing objects despite their large\nappearance changes. In addition, we contribute a new attention-based recurrent\nmask propagation approach that is robust to distractors not belonging to the\ntarget segment. Our approach achieves a new state-of-the-art global mean\n(Region Jaccard and Boundary F measure) of 68.2 on the challenging DAVIS 2017\nbenchmark (test-dev set), outperforming the winning solution which achieves a\nglobal mean of 66.1 on the same partition.\n", "versions": [{"version": "v1", "created": "Mon, 12 Mar 2018 13:25:19 GMT"}, {"version": "v2", "created": "Wed, 14 Mar 2018 12:37:50 GMT"}], "update_date": "2018-03-15", "authors_parsed": [["Li", "Xiaoxiao", ""], ["Loy", "Chen Change", ""]]}, {"id": "1803.04249", "submitter": "Jiaxin Li", "authors": "Jiaxin Li, Ben M. Chen, Gim Hee Lee", "title": "SO-Net: Self-Organizing Network for Point Cloud Analysis", "comments": "17 pages, CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents SO-Net, a permutation invariant architecture for deep\nlearning with orderless point clouds. The SO-Net models the spatial\ndistribution of point cloud by building a Self-Organizing Map (SOM). Based on\nthe SOM, SO-Net performs hierarchical feature extraction on individual points\nand SOM nodes, and ultimately represents the input point cloud by a single\nfeature vector. The receptive field of the network can be systematically\nadjusted by conducting point-to-node k nearest neighbor search. In recognition\ntasks such as point cloud reconstruction, classification, object part\nsegmentation and shape retrieval, our proposed network demonstrates performance\nthat is similar with or better than state-of-the-art approaches. In addition,\nthe training speed is significantly faster than existing point cloud\nrecognition networks because of the parallelizability and simplicity of the\nproposed architecture. Our code is available at the project website.\nhttps://github.com/lijx10/SO-Net\n", "versions": [{"version": "v1", "created": "Mon, 12 Mar 2018 13:49:14 GMT"}, {"version": "v2", "created": "Wed, 14 Mar 2018 16:23:06 GMT"}, {"version": "v3", "created": "Thu, 15 Mar 2018 14:32:47 GMT"}, {"version": "v4", "created": "Tue, 27 Mar 2018 02:59:48 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Li", "Jiaxin", ""], ["Chen", "Ben M.", ""], ["Lee", "Gim Hee", ""]]}, {"id": "1803.04271", "submitter": "Charis Lanaras", "authors": "Charis Lanaras, Jos\\'e Bioucas-Dias, Silvano Galliani, Emmanuel\n  Baltsavias, Konrad Schindler", "title": "Super-resolution of Sentinel-2 images: Learning a globally applicable\n  deep neural network", "comments": "19 pages, 11 figures", "journal-ref": "ISPRS Journal of Photogrammetry and Remote Sensing, 146 (2018),\n  pp. 305-319", "doi": "10.1016/j.isprsjprs.2018.09.018", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The Sentinel-2 satellite mission delivers multi-spectral imagery with 13\nspectral bands, acquired at three different spatial resolutions. The aim of\nthis research is to super-resolve the lower-resolution (20 m and 60 m Ground\nSampling Distance - GSD) bands to 10 m GSD, so as to obtain a complete data\ncube at the maximal sensor resolution. We employ a state-of-the-art\nconvolutional neural network (CNN) to perform end-to-end upsampling, which is\ntrained with data at lower resolution, i.e., from 40->20 m, respectively\n360->60 m GSD. In this way, one has access to a virtually infinite amount of\ntraining data, by downsampling real Sentinel-2 images. We use data sampled\nglobally over a wide range of geographical locations, to obtain a network that\ngeneralises across different climate zones and land-cover types, and can\nsuper-resolve arbitrary Sentinel-2 images without the need of retraining. In\nquantitative evaluations (at lower scale, where ground truth is available), our\nnetwork, which we call DSen2, outperforms the best competing approach by almost\n50% in RMSE, while better preserving the spectral characteristics. It also\ndelivers visually convincing results at the full 10 m GSD. The code is\navailable at https://github.com/lanha/DSen2\n", "versions": [{"version": "v1", "created": "Mon, 12 Mar 2018 14:15:07 GMT"}, {"version": "v2", "created": "Mon, 1 Oct 2018 15:54:43 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Lanaras", "Charis", ""], ["Bioucas-Dias", "Jos\u00e9", ""], ["Galliani", "Silvano", ""], ["Baltsavias", "Emmanuel", ""], ["Schindler", "Konrad", ""]]}, {"id": "1803.04279", "submitter": "Jan Egger", "authors": "Jan Egger, Xiaojun Chen, Lucas Bettac, Mark H\\\"anle, Tilmann Gr\\\"ater,\n  Wolfram Zoller, Dieter Schmalstieg, Alexander Hann", "title": "In-depth Assessment of an Interactive Graph-based Approach for the\n  Segmentation for Pancreatic Metastasis in Ultrasound Acquisitions of the\n  Liver with two Specialists in Internal Medicine", "comments": "5 pages, 3 Figures, 1 Table, The 2017 Biomedical Engineering\n  International Conference (BMEiCON-2017)", "journal-ref": null, "doi": "10.1109/BMEiCON.2017.8229099", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The manual outlining of hepatic metastasis in (US) ultrasound acquisitions\nfrom patients suffering from pancreatic cancer is common practice. However,\nsuch pure manual measurements are often very time consuming, and the results\nrepeatedly differ between the raters. In this contribution, we study the\nin-depth assessment of an interactive graph-based approach for the segmentation\nfor pancreatic metastasis in US images of the liver with two specialists in\nInternal Medicine. Thereby, evaluating the approach with over one hundred\ndifferent acquisitions of metastases. The two physicians or the algorithm had\nnever assessed the acquisitions before the evaluation. In summary, the\nphysicians first performed a pure manual outlining followed by an algorithmic\nsegmentation over one month later. As a result, the experts satisfied in up to\nninety percent of algorithmic segmentation results. Furthermore, the\nalgorithmic segmentation was much faster than manual outlining and achieved a\nmedian Dice Similarity Coefficient (DSC) of over eighty percent. Ultimately,\nthe algorithm enables a fast and accurate segmentation of liver metastasis in\nclinical US images, which can support the manual outlining in daily practice.\n", "versions": [{"version": "v1", "created": "Mon, 12 Mar 2018 14:24:39 GMT"}], "update_date": "2018-03-13", "authors_parsed": [["Egger", "Jan", ""], ["Chen", "Xiaojun", ""], ["Bettac", "Lucas", ""], ["H\u00e4nle", "Mark", ""], ["Gr\u00e4ter", "Tilmann", ""], ["Zoller", "Wolfram", ""], ["Schmalstieg", "Dieter", ""], ["Hann", "Alexander", ""]]}, {"id": "1803.04332", "submitter": "R\\'emi Cura", "authors": "Remi Cura, Julien Perret, Nicolas Paparoditis", "title": "A state of the art of urban reconstruction: street, street network,\n  vegetation, urban feature", "comments": "Extracted from PhD (chap1)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.OH cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  World population is raising, especially the part of people living in cities.\nWith increased population and complex roles regarding their inhabitants and\ntheir surroundings, cities concentrate difficulties for design, planning and\nanalysis. These tasks require a way to reconstruct/model a city. Traditionally,\nmuch attention has been given to buildings reconstruction, yet an essential\npart of city were neglected: streets. Streets reconstruction has been seldom\nresearched. Streets are also complex compositions of urban features, and have a\nunique role for transportation (as they comprise roads). We aim at completing\nthe recent state of the art for building reconstruction (Musialski2012) by\nconsidering all other aspect of urban reconstruction. We introduce the need for\ncity models. Because reconstruction always necessitates data, we first analyse\nwhich data are available. We then expose a state of the art of street\nreconstruction, street network reconstruction, urban features\nreconstruction/modelling, vegetation , and urban objects\nreconstruction/modelling.\n  Although reconstruction strategies vary widely, we can order them by the role\nthe model plays, from data driven approach, to model-based approach, to inverse\nprocedural modelling and model catalogue matching. The main challenges seems to\ncome from the complex nature of urban environment and from the limitations of\nthe available data. Urban features have strong relationships, between them, and\nto their surrounding, as well as in hierarchical relations. Procedural\nmodelling has the power to express these relations, and could be applied to the\nreconstruction of urban features via the Inverse Procedural Modelling paradigm.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jan 2018 02:11:18 GMT"}], "update_date": "2018-03-13", "authors_parsed": [["Cura", "Remi", ""], ["Perret", "Julien", ""], ["Paparoditis", "Nicolas", ""]]}, {"id": "1803.04337", "submitter": "Mike Voets", "authors": "Mike Voets, Kajsa M{\\o}llersen, Lars Ailo Bongo", "title": "Replication study: Development and validation of deep learning algorithm\n  for detection of diabetic retinopathy in retinal fundus photographs", "comments": "The third version of this paper includes results from replication\n  after certain hyper-parameters were published in later article. 16 pages, 6\n  figures, 1 table, presented at NOBIM 2018", "journal-ref": null, "doi": "10.1371/journal.pone.0217541", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Replication studies are essential for validation of new methods, and are\ncrucial to maintain the high standards of scientific publications, and to use\nthe results in practice. We have attempted to replicate the main method in\n'Development and validation of a deep learning algorithm for detection of\ndiabetic retinopathy in retinal fundus photographs' published in JAMA 2016;\n316(22). We re-implemented the method since the source code is not available,\nand we used publicly available data sets. The original study used non-public\nfundus images from EyePACS and three hospitals in India for training. We used a\ndifferent EyePACS data set from Kaggle. The original study used the benchmark\ndata set Messidor-2 to evaluate the algorithm's performance. We used the same\ndata set. In the original study, ophthalmologists re-graded all images for\ndiabetic retinopathy, macular edema, and image gradability. There was one\ndiabetic retinopathy grade per image for our data sets, and we assessed image\ngradability ourselves. Hyper-parameter settings were not described in the\noriginal study. But some of these were later published. We were not able to\nreplicate the original study. Our algorithm's area under the receiver operating\ncurve (AUC) of 0.94 on the Kaggle EyePACS test set and 0.80 on Messidor-2 did\nnot come close to the reported AUC of 0.99 in the original study. This may be\ncaused by the use of a single grade per image, different data, or different not\ndescribed hyper-parameter settings. This study shows the challenges of\nreplicating deep learning, and the need for more replication studies to\nvalidate deep learning methods, especially for medical image analysis.\n  Our source code and instructions are available at:\nhttps://github.com/mikevoets/jama16-retina-replication\n", "versions": [{"version": "v1", "created": "Mon, 12 Mar 2018 16:04:28 GMT"}, {"version": "v2", "created": "Mon, 30 Apr 2018 14:51:54 GMT"}, {"version": "v3", "created": "Thu, 30 Aug 2018 00:28:12 GMT"}], "update_date": "2019-06-19", "authors_parsed": [["Voets", "Mike", ""], ["M\u00f8llersen", "Kajsa", ""], ["Bongo", "Lars Ailo", ""]]}, {"id": "1803.04347", "submitter": "Charles Jekel", "authors": "Charles F Jekel, Raphael T. Haftka", "title": "Classifying Online Dating Profiles on Tinder using FaceNet Facial\n  Embeddings", "comments": "6 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.SI eess.IV stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  A method to produce personalized classification models to automatically\nreview online dating profiles on Tinder is proposed, based on the user's\nhistorical preference. The method takes advantage of a FaceNet facial\nclassification model to extract features which may be related to facial\nattractiveness. The embeddings from a FaceNet model were used as the features\nto describe an individual's face. A user reviewed 8,545 online dating profiles.\nFor each reviewed online dating profile, a feature set was constructed from the\nprofile images which contained just one face. Two approaches are presented to\ngo from the set of features for each face, to a set of profile features. A\nsimple logistic regression trained on the embeddings from just 20 profiles\ncould obtain a 65% validation accuracy. A point of diminishing marginal returns\nwas identified to occur around 80 profiles, at which the model accuracy of 73%\nwould only improve marginally after reviewing a significant number of\nadditional profiles.\n", "versions": [{"version": "v1", "created": "Mon, 12 Mar 2018 16:14:24 GMT"}], "update_date": "2018-03-13", "authors_parsed": [["Jekel", "Charles F", ""], ["Haftka", "Raphael T.", ""]]}, {"id": "1803.04351", "submitter": "Gonzalo De Polavieja", "authors": "Francisco Romero-Ferrero, Mattia G. Bergomi, Robert Hinz, Francisco J.\n  H. Heras, Gonzalo G. de Polavieja", "title": "idtracker.ai: Tracking all individuals in large collectives of unmarked\n  animals", "comments": "44 pages, 1 main figure, 13 supplementary figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Our understanding of collective animal behavior is limited by our ability to\ntrack each of the individuals. We describe an algorithm and software,\nidtracker.ai, that extracts from video all trajectories with correct identities\nat a high accuracy for collectives of up to 100 individuals. It uses two deep\nnetworks, one detecting when animals touch or cross and another one for animal\nidentification, trained adaptively to conditions and difficulty of the video.\n", "versions": [{"version": "v1", "created": "Mon, 12 Mar 2018 16:21:19 GMT"}], "update_date": "2018-03-13", "authors_parsed": [["Romero-Ferrero", "Francisco", ""], ["Bergomi", "Mattia G.", ""], ["Hinz", "Robert", ""], ["Heras", "Francisco J. H.", ""], ["de Polavieja", "Gonzalo G.", ""]]}, {"id": "1803.04360", "submitter": "Viktor Larsson", "authors": "Viktor Larsson, Magnus Oskarsson, Kalle {\\AA}str\\\"om, Alge Wallis,\n  Zuzana Kukelova, Tomas Pajdla", "title": "Beyond Gr\\\"obner Bases: Basis Selection for Minimal Solvers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many computer vision applications require robust estimation of the underlying\ngeometry, in terms of camera motion and 3D structure of the scene. These robust\nmethods often rely on running minimal solvers in a RANSAC framework. In this\npaper we show how we can make polynomial solvers based on the action matrix\nmethod faster, by careful selection of the monomial bases. These monomial bases\nhave traditionally been based on a Gr\\\"obner basis for the polynomial ideal.\nHere we describe how we can enumerate all such bases in an efficient way. We\nalso show that going beyond Gr\\\"obner bases leads to more efficient solvers in\nmany cases. We present a novel basis sampling scheme that we evaluate on a\nnumber of problems.\n", "versions": [{"version": "v1", "created": "Mon, 12 Mar 2018 16:36:13 GMT"}], "update_date": "2018-03-13", "authors_parsed": [["Larsson", "Viktor", ""], ["Oskarsson", "Magnus", ""], ["\u00c5str\u00f6m", "Kalle", ""], ["Wallis", "Alge", ""], ["Kukelova", "Zuzana", ""], ["Pajdla", "Tomas", ""]]}, {"id": "1803.04376", "submitter": "Ruotian Luo", "authors": "Ruotian Luo, Brian Price, Scott Cohen, Gregory Shakhnarovich", "title": "Discriminability objective for training descriptive captions", "comments": "CVPR2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One property that remains lacking in image captions generated by contemporary\nmethods is discriminability: being able to tell two images apart given the\ncaption for one of them. We propose a way to improve this aspect of caption\ngeneration. By incorporating into the captioning training objective a loss\ncomponent directly related to ability (by a machine) to disambiguate\nimage/caption matches, we obtain systems that produce much more discriminative\ncaption, according to human evaluation. Remarkably, our approach leads to\nimprovement in other aspects of generated captions, reflected by a battery of\nstandard scores such as BLEU, SPICE etc. Our approach is modular and can be\napplied to a variety of model/loss combinations commonly proposed for image\ncaptioning.\n", "versions": [{"version": "v1", "created": "Mon, 12 Mar 2018 17:09:26 GMT"}, {"version": "v2", "created": "Fri, 8 Jun 2018 18:09:36 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Luo", "Ruotian", ""], ["Price", "Brian", ""], ["Cohen", "Scott", ""], ["Shakhnarovich", "Gregory", ""]]}, {"id": "1803.04459", "submitter": "Rayyan Ahmad Khan", "authors": "Rayyan Ahmad Khan, Rana Ali Amjad, Martin Kleinsteuber", "title": "Extended Affinity Propagation: Global Discovery and Local Insights", "comments": "Submitted to TKDE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a new clustering algorithm, Extended Affinity Propagation, based\non pairwise similarities. Extended Affinity Propagation is developed by\nmodifying Affinity Propagation such that the desirable features of Affinity\nPropagation, e.g., exemplars, reasonable computational complexity and no need\nto specify number of clusters, are preserved while the shortcomings, e.g., the\nlack of global structure discovery, that limit the applicability of Affinity\nPropagation are overcome. Extended Affinity Propagation succeeds not only in\nachieving this goal but can also provide various additional insights into the\ninternal structure of the individual clusters, e.g., refined confidence values,\nrelative cluster densities and local cluster strength in different regions of a\ncluster, which are valuable for an analyst. We briefly discuss how these\ninsights can help in easily tuning the hyperparameters. We also illustrate\nthese desirable features and the performance of Extended Affinity Propagation\non various synthetic and real world datasets.\n", "versions": [{"version": "v1", "created": "Mon, 12 Mar 2018 18:56:38 GMT"}, {"version": "v2", "created": "Mon, 15 Apr 2019 18:43:44 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Khan", "Rayyan Ahmad", ""], ["Amjad", "Rana Ali", ""], ["Kleinsteuber", "Martin", ""]]}, {"id": "1803.04460", "submitter": "Hongliu Cao", "authors": "Hongliu Cao, Simon Bernard, Laurent Heutte and Robert Sabourin", "title": "Dissimilarity-based representation for radiomics applications", "comments": "conference, 6 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Radiomics is a term which refers to the analysis of the large amount of\nquantitative tumor features extracted from medical images to find useful\npredictive, diagnostic or prognostic information. Many recent studies have\nproved that radiomics can offer a lot of useful information that physicians\ncannot extract from the medical images and can be associated with other\ninformation like gene or protein data. However, most of the classification\nstudies in radiomics report the use of feature selection methods without\nidentifying the machine learning challenges behind radiomics. In this paper, we\nfirst show that the radiomics problem should be viewed as an high dimensional,\nlow sample size, multi view learning problem, then we compare different\nsolutions proposed in multi view learning for classifying radiomics data. Our\nexperiments, conducted on several real world multi view datasets, show that the\nintermediate integration methods work significantly better than filter and\nembedded feature selection methods commonly used in radiomics.\n", "versions": [{"version": "v1", "created": "Mon, 12 Mar 2018 18:56:47 GMT"}], "update_date": "2018-03-14", "authors_parsed": [["Cao", "Hongliu", ""], ["Bernard", "Simon", ""], ["Heutte", "Laurent", ""], ["Sabourin", "Robert", ""]]}, {"id": "1803.04469", "submitter": "He Huang", "authors": "He Huang, Philip S. Yu, Changhu Wang", "title": "An Introduction to Image Synthesis with Generative Adversarial Nets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  There has been a drastic growth of research in Generative Adversarial Nets\n(GANs) in the past few years. Proposed in 2014, GAN has been applied to various\napplications such as computer vision and natural language processing, and\nachieves impressive performance. Among the many applications of GAN, image\nsynthesis is the most well-studied one, and research in this area has already\ndemonstrated the great potential of using GAN in image synthesis. In this\npaper, we provide a taxonomy of methods used in image synthesis, review\ndifferent models for text-to-image synthesis and image-to-image translation,\nand discuss some evaluation metrics as well as possible future research\ndirections in image synthesis with GAN.\n", "versions": [{"version": "v1", "created": "Mon, 12 Mar 2018 19:14:35 GMT"}, {"version": "v2", "created": "Sat, 17 Nov 2018 15:25:11 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Huang", "He", ""], ["Yu", "Philip S.", ""], ["Wang", "Changhu", ""]]}, {"id": "1803.04477", "submitter": "Subarna Tripathi", "authors": "Subarna Tripathi and Zachary C. Lipton and Truong Q. Nguyen", "title": "Correction by Projection: Denoising Images with Generative Adversarial\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative adversarial networks (GANs) transform low-dimensional latent\nvectors into visually plausible images. If the real dataset contains only clean\nimages, then ostensibly, the manifold learned by the GAN should contain only\nclean images. In this paper, we propose to denoise corrupted images by finding\nthe nearest point on the GAN manifold, recovering latent vectors by minimizing\ndistances in image space. We first demonstrate that given a corrupted version\nof an image that truly lies on the GAN manifold, we can approximately recover\nthe latent vector and denoise the image, obtaining significantly higher\nquality, comparing with BM3D. Next, we demonstrate that latent vectors\nrecovered from noisy images exhibit a consistent bias. By subtracting this bias\nbefore projecting back to image space, we improve denoising results even\nfurther. Finally, even for unseen images, our method performs better at\ndenoising better than BM3D. Notably, the basic version of our method (without\nbias correction) requires no prior knowledge on the noise variance. To achieve\nthe highest possible denoising quality, the best performing signal processing\nbased methods, such as BM3D, require an estimate of the blur kernel.\n", "versions": [{"version": "v1", "created": "Mon, 12 Mar 2018 19:30:01 GMT"}], "update_date": "2018-03-14", "authors_parsed": [["Tripathi", "Subarna", ""], ["Lipton", "Zachary C.", ""], ["Nguyen", "Truong Q.", ""]]}, {"id": "1803.04523", "submitter": "Anton Mitrokhin", "authors": "Anton Mitrokhin, Cornelia Fermuller, Chethan Parameshwara and Yiannis\n  Aloimonos", "title": "Event-based Moving Object Detection and Tracking", "comments": "8 pages, 8 figures", "journal-ref": null, "doi": "10.1109/IROS.2018.8593805", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Event-based vision sensors, such as the Dynamic Vision Sensor (DVS), are\nideally suited for real-time motion analysis. The unique properties encompassed\nin the readings of such sensors provide high temporal resolution, superior\nsensitivity to light and low latency. These properties provide the grounds to\nestimate motion extremely reliably in the most sophisticated scenarios but they\ncome at a price - modern event-based vision sensors have extremely low\nresolution and produce a lot of noise. Moreover, the asynchronous nature of the\nevent stream calls for novel algorithms.\n  This paper presents a new, efficient approach to object tracking with\nasynchronous cameras. We present a novel event stream representation which\nenables us to utilize information about the dynamic (temporal) component of the\nevent stream, and not only the spatial component, at every moment of time. This\nis done by approximating the 3D geometry of the event stream with a parametric\nmodel; as a result, the algorithm is capable of producing the\nmotion-compensated event stream (effectively approximating egomotion), and\nwithout using any form of external sensors in extremely low-light and noisy\nconditions without any form of feature tracking or explicit optical flow\ncomputation. We demonstrate our framework on the task of independent motion\ndetection and tracking, where we use the temporal model inconsistencies to\nlocate differently moving objects in challenging situations of very fast\nmotion.\n", "versions": [{"version": "v1", "created": "Mon, 12 Mar 2018 20:43:59 GMT"}, {"version": "v2", "created": "Mon, 23 Jul 2018 02:25:54 GMT"}, {"version": "v3", "created": "Sun, 12 Jan 2020 23:55:30 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Mitrokhin", "Anton", ""], ["Fermuller", "Cornelia", ""], ["Parameshwara", "Chethan", ""], ["Aloimonos", "Yiannis", ""]]}, {"id": "1803.04556", "submitter": "Pan Wei", "authors": "Pan Wei, John E. Ball, Derek T. Anderson, Archit Harsh, Christopher\n  Archibald", "title": "Measuring Conflict in a Multi-Source Environment as a Normal Measure", "comments": "4 pages, 8 figures, conference paper", "journal-ref": "IEEE International Workshop on Computational Advances in\n  Multi-Sensor Adaptive Processing (CAMSAP), December, 2015", "doi": null, "report-no": null, "categories": "eess.SP cs.AI cs.CV eess.IV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  In a multi-source environment, each source has its own credibility. If there\nis no external knowledge about credibility then we can use the information\nprovided by the sources to assess their credibility. In this paper, we propose\na way to measure conflict in a multi-source environment as a normal measure. We\nexamine our algorithm using three simulated examples of increasing conflict and\none experimental example. The results demonstrate that the proposed measure can\nrepresent conflict in a meaningful way similar to what a human might expect and\nfrom it we can identify conflict within our sources.\n", "versions": [{"version": "v1", "created": "Mon, 12 Mar 2018 22:27:11 GMT"}], "update_date": "2018-03-14", "authors_parsed": [["Wei", "Pan", ""], ["Ball", "John E.", ""], ["Anderson", "Derek T.", ""], ["Harsh", "Archit", ""], ["Archibald", "Christopher", ""]]}, {"id": "1803.04565", "submitter": "Sasa Grbic", "authors": "Sebastian Guendel, Sasa Grbic, Bogdan Georgescu, Kevin Zhou, Ludwig\n  Ritschl, Andreas Meier and Dorin Comaniciu", "title": "Learning to recognize Abnormalities in Chest X-Rays with Location-Aware\n  Dense Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chest X-ray is the most common medical imaging exam used to assess multiple\npathologies. Automated algorithms and tools have the potential to support the\nreading workflow, improve efficiency, and reduce reading errors. With the\navailability of large scale data sets, several methods have been proposed to\nclassify pathologies on chest X-ray images. However, most methods report\nperformance based on random image based splitting, ignoring the high\nprobability of the same patient appearing in both training and test set. In\naddition, most methods fail to explicitly incorporate the spatial information\nof abnormalities or utilize the high resolution images. We propose a novel\napproach based on location aware Dense Networks (DNetLoc), whereby we\nincorporate both high-resolution image data and spatial information for\nabnormality classification. We evaluate our method on the largest data set\nreported in the community, containing a total of 86,876 patients and 297,541\nchest X-ray images. We achieve (i) the best average AUC score for published\ntraining and test splits on the single benchmarking data set (ChestX-Ray14),\nand (ii) improved AUC scores when the pathology location information is\nexplicitly used. To foster future research we demonstrate the limitations of\nthe current benchmarking setup and provide new reference patient-wise splits\nfor the used data sets. This could support consistent and meaningful\nbenchmarking of future methods on the largest publicly available data sets.\n", "versions": [{"version": "v1", "created": "Mon, 12 Mar 2018 22:57:18 GMT"}], "update_date": "2018-03-14", "authors_parsed": [["Guendel", "Sebastian", ""], ["Grbic", "Sasa", ""], ["Georgescu", "Bogdan", ""], ["Zhou", "Kevin", ""], ["Ritschl", "Ludwig", ""], ["Meier", "Andreas", ""], ["Comaniciu", "Dorin", ""]]}, {"id": "1803.04610", "submitter": "Phil Ammirato", "authors": "Phil Ammirato, Cheng-Yang Fu, Mykhailo Shvets, Jana Kosecka, Alexander\n  C. Berg", "title": "Target Driven Instance Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While state-of-the-art general object detectors are getting better and\nbetter, there are not many systems specifically designed to take advantage of\nthe instance detection problem. For many applications, such as household\nrobotics, a system may need to recognize a few very specific instances at a\ntime. Speed can be critical in these applications, as can the need to recognize\npreviously unseen instances. We introduce a Target Driven Instance\nDetector(TDID), which modifies existing general object detectors for the\ninstance recognition setting. TDID not only improves performance on instances\nseen during training, with a fast runtime, but is also able to generalize to\ndetect novel instances.\n", "versions": [{"version": "v1", "created": "Tue, 13 Mar 2018 03:56:36 GMT"}, {"version": "v2", "created": "Mon, 2 Jul 2018 23:31:15 GMT"}, {"version": "v3", "created": "Tue, 17 Jul 2018 17:42:05 GMT"}, {"version": "v4", "created": "Wed, 28 Nov 2018 17:48:26 GMT"}, {"version": "v5", "created": "Sat, 27 Jul 2019 15:10:47 GMT"}, {"version": "v6", "created": "Tue, 1 Oct 2019 15:32:03 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["Ammirato", "Phil", ""], ["Fu", "Cheng-Yang", ""], ["Shvets", "Mykhailo", ""], ["Kosecka", "Jana", ""], ["Berg", "Alexander C.", ""]]}, {"id": "1803.04620", "submitter": "Tutika Chetan Sai", "authors": "Chetan Sai Tutika, Charan Vallapaneni, Karthik R, Bharath KP, N Ruban\n  Rajesh Kumar Muthu", "title": "Image Segmentation and Processing for Efficient Parking Space Analysis", "comments": "6 pages, 2018 International Conference on Informatics Computing in\n  Engineering Systems (ICICES)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we develop a method to detect vacant parking spaces in an\nenvironment with unclear segments and contours with the help of MATLAB image\nprocessing capabilities. Due to the anomalies present in the parking spaces,\nsuch as uneven illumination, distorted slot lines and overlapping of cars. The\npresent-day conventional algorithms have difficulties processing the image for\naccurate results. The algorithm proposed uses a combination of image\npre-processing and false contour detection techniques to improve the detection\nefficiency. The proposed method also eliminates the need to employ individual\nsensors to detect a car, instead uses real-time static images to consider a\ngroup of slots together, instead of the usual single slot method. This greatly\ndecreases the expenses required to design an efficient parking system. We\ncompare the performance of our algorithm to that of other techniques. These\ncomparisons show that the proposed algorithm can detect the vacancies in the\nparking spots while ignoring the false data and other distortions.\n", "versions": [{"version": "v1", "created": "Tue, 13 Mar 2018 05:05:35 GMT"}], "update_date": "2018-03-14", "authors_parsed": [["Tutika", "Chetan Sai", ""], ["Vallapaneni", "Charan", ""], ["R", "Karthik", ""], ["KP", "Bharath", ""], ["Muthu", "N Ruban Rajesh Kumar", ""]]}, {"id": "1803.04626", "submitter": "Roey Mechrez", "authors": "Roey Mechrez, Itamar Talmi, Firas Shama, Lihi Zelnik-Manor", "title": "Maintaining Natural Image Statistics with the Contextual Loss", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Maintaining natural image statistics is a crucial factor in restoration and\ngeneration of realistic looking images. When training CNNs, photorealism is\nusually attempted by adversarial training (GAN), that pushes the output images\nto lie on the manifold of natural images. GANs are very powerful, but not\nperfect. They are hard to train and the results still often suffer from\nartifacts. In this paper we propose a complementary approach, that could be\napplied with or without GAN, whose goal is to train a feed-forward CNN to\nmaintain natural internal statistics. We look explicitly at the distribution of\nfeatures in an image and train the network to generate images with natural\nfeature distributions. Our approach reduces by orders of magnitude the number\nof images required for training and achieves state-of-the-art results on both\nsingle-image super-resolution, and high-resolution surface normal estimation.\n", "versions": [{"version": "v1", "created": "Tue, 13 Mar 2018 05:19:26 GMT"}, {"version": "v2", "created": "Wed, 14 Mar 2018 23:33:31 GMT"}, {"version": "v3", "created": "Wed, 18 Jul 2018 12:32:51 GMT"}], "update_date": "2018-07-19", "authors_parsed": [["Mechrez", "Roey", ""], ["Talmi", "Itamar", ""], ["Shama", "Firas", ""], ["Zelnik-Manor", "Lihi", ""]]}, {"id": "1803.04636", "submitter": "Guanying Chen", "authors": "Guanying Chen and Kai Han and Kwan-Yee K. Wong", "title": "TOM-Net: Learning Transparent Object Matting from a Single Image", "comments": "CVPR 2018. Project Page: https://guanyingc.github.io/TOM-Net", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of transparent object matting. Existing\nimage matting approaches for transparent objects often require tedious\ncapturing procedures and long processing time, which limit their practical use.\nIn this paper, we first formulate transparent object matting as a refractive\nflow estimation problem. We then propose a deep learning framework, called\nTOM-Net, for learning the refractive flow. Our framework comprises two parts,\nnamely a multi-scale encoder-decoder network for producing a coarse prediction,\nand a residual network for refinement. At test time, TOM-Net takes a single\nimage as input, and outputs a matte (consisting of an object mask, an\nattenuation mask and a refractive flow field) in a fast feed-forward pass. As\nno off-the-shelf dataset is available for transparent object matting, we create\na large-scale synthetic dataset consisting of 158K images of transparent\nobjects rendered in front of images sampled from the Microsoft COCO dataset. We\nalso collect a real dataset consisting of 876 samples using 14 transparent\nobjects and 60 background images. Promising experimental results have been\nachieved on both synthetic and real data, which clearly demonstrate the\neffectiveness of our approach.\n", "versions": [{"version": "v1", "created": "Tue, 13 Mar 2018 06:03:42 GMT"}, {"version": "v2", "created": "Sat, 17 Mar 2018 11:29:27 GMT"}, {"version": "v3", "created": "Thu, 29 Mar 2018 02:41:59 GMT"}], "update_date": "2018-03-30", "authors_parsed": [["Chen", "Guanying", ""], ["Han", "Kai", ""], ["Wong", "Kwan-Yee K.", ""]]}, {"id": "1803.04667", "submitter": "Chaitanya Chinni", "authors": "Stefanie Anna Baby, Bimal Vinod, Chaitanya Chinni, Kaushik Mitra", "title": "Dynamic Vision Sensors for Human Activity Recognition", "comments": "6 pages, 9 figures, accepted at the 4th Asian Conference on Pattern\n  Recognition (ACPR) 2017", "journal-ref": null, "doi": "10.1109/ACPR.2017.136", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Unlike conventional cameras which capture video at a fixed frame rate,\nDynamic Vision Sensors (DVS) record only changes in pixel intensity values. The\noutput of DVS is simply a stream of discrete ON/OFF events based on the\npolarity of change in its pixel values. DVS has many attractive features such\nas low power consumption, high temporal resolution, high dynamic range and\nfewer storage requirements. All these make DVS a very promising camera for\npotential applications in wearable platforms where power consumption is a major\nconcern.\n  In this paper, we explore the feasibility of using DVS for Human Activity\nRecognition (HAR). We propose to use the various slices (such as $x-y$, $x-t$,\nand $y-t$) of the DVS video as a feature map for HAR and denote them as Motion\nMaps. We show that fusing motion maps with Motion Boundary Histogram (MBH) give\ngood performance on the benchmark DVS dataset as well as on a real DVS gesture\ndataset collected by us. Interestingly, the performance of DVS is comparable to\nthat of conventional videos although DVS captures only sparse motion\ninformation.\n", "versions": [{"version": "v1", "created": "Tue, 13 Mar 2018 07:49:06 GMT"}], "update_date": "2019-01-09", "authors_parsed": [["Baby", "Stefanie Anna", ""], ["Vinod", "Bimal", ""], ["Chinni", "Chaitanya", ""], ["Mitra", "Kaushik", ""]]}, {"id": "1803.04680", "submitter": "Ren Yang", "authors": "Ren Yang, Mai Xu, Zulin Wang, Tianyi Li", "title": "Multi-Frame Quality Enhancement for Compressed Video", "comments": "to appear in CVPR 2018", "journal-ref": null, "doi": "10.1109/CVPR.2018.00697", "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The past few years have witnessed great success in applying deep learning to\nenhance the quality of compressed image/video. The existing approaches mainly\nfocus on enhancing the quality of a single frame, ignoring the similarity\nbetween consecutive frames. In this paper, we investigate that heavy quality\nfluctuation exists across compressed video frames, and thus low quality frames\ncan be enhanced using the neighboring high quality frames, seen as Multi-Frame\nQuality Enhancement (MFQE). Accordingly, this paper proposes an MFQE approach\nfor compressed video, as a first attempt in this direction. In our approach, we\nfirstly develop a Support Vector Machine (SVM) based detector to locate Peak\nQuality Frames (PQFs) in compressed video. Then, a novel Multi-Frame\nConvolutional Neural Network (MF-CNN) is designed to enhance the quality of\ncompressed video, in which the non-PQF and its nearest two PQFs are as the\ninput. The MF-CNN compensates motion between the non-PQF and PQFs through the\nMotion Compensation subnet (MC-subnet). Subsequently, the Quality Enhancement\nsubnet (QE-subnet) reduces compression artifacts of the non-PQF with the help\nof its nearest PQFs. Finally, the experiments validate the effectiveness and\ngenerality of our MFQE approach in advancing the state-of-the-art quality\nenhancement of compressed video. The code of our MFQE approach is available at\nhttps://github.com/ryangBUAA/MFQE.git\n", "versions": [{"version": "v1", "created": "Tue, 13 Mar 2018 08:40:15 GMT"}, {"version": "v2", "created": "Wed, 14 Mar 2018 01:18:08 GMT"}, {"version": "v3", "created": "Thu, 15 Mar 2018 01:55:21 GMT"}, {"version": "v4", "created": "Fri, 16 Mar 2018 02:09:36 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Yang", "Ren", ""], ["Xu", "Mai", ""], ["Wang", "Zulin", ""], ["Li", "Tianyi", ""]]}, {"id": "1803.04687", "submitter": "Abrar  Abdulnabi", "authors": "Abrar H. Abdulnabi, Bing Shuai, Zhen Zuo, Lap-Pui Chau, Gang Wang", "title": "Multimodal Recurrent Neural Networks with Information Transfer Layers\n  for Indoor Scene Labeling", "comments": "15 pages, 13 figures, IEEE TMM 2017", "journal-ref": "IEEE Transactions on Multimedia 2017", "doi": "10.1109/TMM.2017.2774007", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new method called Multimodal RNNs for RGB-D scene\nsemantic segmentation. It is optimized to classify image pixels given two input\nsources: RGB color channels and Depth maps. It simultaneously performs training\nof two recurrent neural networks (RNNs) that are crossly connected through\ninformation transfer layers, which are learnt to adaptively extract relevant\ncross-modality features. Each RNN model learns its representations from its own\nprevious hidden states and transferred patterns from the other RNNs previous\nhidden states; thus, both model-specific and crossmodality features are\nretained. We exploit the structure of quad-directional 2D-RNNs to model the\nshort and long range contextual information in the 2D input image. We carefully\ndesigned various baselines to efficiently examine our proposed model structure.\nWe test our Multimodal RNNs method on popular RGB-D benchmarks and show how it\noutperforms previous methods significantly and achieves competitive results\nwith other state-of-the-art works.\n", "versions": [{"version": "v1", "created": "Tue, 13 Mar 2018 09:08:49 GMT"}], "update_date": "2018-03-14", "authors_parsed": [["Abdulnabi", "Abrar H.", ""], ["Shuai", "Bing", ""], ["Zuo", "Zhen", ""], ["Chau", "Lap-Pui", ""], ["Wang", "Gang", ""]]}, {"id": "1803.04722", "submitter": "Xiao Song", "authors": "Xiao Song, Xu Zhao, Tianwei Lin", "title": "Face Spoofing Detection by Fusing Binocular Depth and Spatial Pyramid\n  Coding Micro-Texture Features", "comments": "5 pages, 2 figures, accepted by 2017 IEEE International Conference on\n  Image Processing (ICIP)", "journal-ref": null, "doi": "10.1109/ICIP.2017.8296250", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust features are of vital importance to face spoofing detection, because\nvarious situations make feature space extremely complicated to partition. Thus\nin this paper, two novel and robust features for anti-spoofing are proposed.\nThe first one is a binocular camera based depth feature called Template Face\nMatched Binocular Depth (TFBD) feature. The second one is a high-level\nmicro-texture based feature called Spatial Pyramid Coding Micro-Texture (SPMT)\nfeature. Novel template face registration algorithm and spatial pyramid coding\nalgorithm are also introduced along with the two novel features. Multi-modal\nface spoofing detection is implemented based on these two robust features.\nExperiments are conducted on a widely used dataset and a comprehensive dataset\nconstructed by ourselves. The results reveal that face spoofing detection with\nthe fusion of our proposed features is of strong robustness and time\nefficiency, meanwhile outperforming other state-of-the-art traditional methods.\n", "versions": [{"version": "v1", "created": "Tue, 13 Mar 2018 10:49:45 GMT"}], "update_date": "2018-03-14", "authors_parsed": [["Song", "Xiao", ""], ["Zhao", "Xu", ""], ["Lin", "Tianwei", ""]]}, {"id": "1803.04758", "submitter": "Thiemo Alldieck", "authors": "Thiemo Alldieck, Marcus Magnor, Weipeng Xu, Christian Theobalt, Gerard\n  Pons-Moll", "title": "Video Based Reconstruction of 3D People Models", "comments": "CVPR 2018 Spotlight, IEEE Conference on Computer Vision and Pattern\n  Recognition 2018 (CVPR)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes how to obtain accurate 3D body models and texture of\narbitrary people from a single, monocular video in which a person is moving.\nBased on a parametric body model, we present a robust processing pipeline\nachieving 3D model fits with 5mm accuracy also for clothed people. Our main\ncontribution is a method to nonrigidly deform the silhouette cones\ncorresponding to the dynamic human silhouettes, resulting in a visual hull in a\ncommon reference frame that enables surface reconstruction. This enables\nefficient estimation of a consensus 3D shape, texture and implanted animation\nskeleton based on a large number of frames. We present evaluation results for a\nnumber of test subjects and analyze overall performance. Requiring only a\nsmartphone or webcam, our method enables everyone to create their own fully\nanimatable digital double, e.g., for social VR applications or virtual try-on\nfor online fashion shopping.\n", "versions": [{"version": "v1", "created": "Tue, 13 Mar 2018 12:56:28 GMT"}, {"version": "v2", "created": "Wed, 28 Mar 2018 09:54:03 GMT"}, {"version": "v3", "created": "Mon, 16 Apr 2018 10:08:19 GMT"}], "update_date": "2018-04-17", "authors_parsed": [["Alldieck", "Thiemo", ""], ["Magnor", "Marcus", ""], ["Xu", "Weipeng", ""], ["Theobalt", "Christian", ""], ["Pons-Moll", "Gerard", ""]]}, {"id": "1803.04775", "submitter": "Helge Rhodin", "authors": "Helge Rhodin, J\\\"org Sp\\\"orri, Isinsu Katircioglu, Victor Constantin,\n  Fr\\'ed\\'eric Meyer, Erich M\\\"uller, Mathieu Salzmann and Pascal Fua", "title": "Learning Monocular 3D Human Pose Estimation from Multi-view Images", "comments": "CVPR 2018, Ski-Pose PTZ-Camera Dataset available", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate 3D human pose estimation from single images is possible with\nsophisticated deep-net architectures that have been trained on very large\ndatasets. However, this still leaves open the problem of capturing motions for\nwhich no such database exists. Manual annotation is tedious, slow, and\nerror-prone. In this paper, we propose to replace most of the annotations by\nthe use of multiple views, at training time only. Specifically, we train the\nsystem to predict the same pose in all views. Such a consistency constraint is\nnecessary but not sufficient to predict accurate poses. We therefore complement\nit with a supervised loss aiming to predict the correct pose in a small set of\nlabeled images, and with a regularization term that penalizes drift from\ninitial predictions. Furthermore, we propose a method to estimate camera pose\njointly with human pose, which lets us utilize multi-view footage where\ncalibration is difficult, e.g., for pan-tilt or moving handheld cameras. We\ndemonstrate the effectiveness of our approach on established benchmarks, as\nwell as on a new Ski dataset with rotating cameras and expert ski motion, for\nwhich annotations are truly hard to obtain.\n", "versions": [{"version": "v1", "created": "Tue, 13 Mar 2018 13:14:47 GMT"}, {"version": "v2", "created": "Sat, 24 Mar 2018 17:24:45 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Rhodin", "Helge", ""], ["Sp\u00f6rri", "J\u00f6rg", ""], ["Katircioglu", "Isinsu", ""], ["Constantin", "Victor", ""], ["Meyer", "Fr\u00e9d\u00e9ric", ""], ["M\u00fcller", "Erich", ""], ["Salzmann", "Mathieu", ""], ["Fua", "Pascal", ""]]}, {"id": "1803.04792", "submitter": "Youcheng Sun", "authors": "Youcheng Sun and Xiaowei Huang and Daniel Kroening and James Sharp and\n  Matthew Hill and Rob Ashmore", "title": "Testing Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) have a wide range of applications, and software\nemploying them must be thoroughly tested, especially in safety-critical\ndomains. However, traditional software test coverage metrics cannot be applied\ndirectly to DNNs. In this paper, inspired by the MC/DC coverage criterion, we\npropose a family of four novel test criteria that are tailored to structural\nfeatures of DNNs and their semantics. We validate the criteria by demonstrating\nthat the generated test inputs guided via our proposed coverage criteria are\nable to capture undesired behaviours in a DNN. Test cases are generated using a\nsymbolic approach and a gradient-based heuristic search. By comparing them with\nexisting methods, we show that our criteria achieve a balance between their\nability to find bugs (proxied using adversarial examples) and the computational\ncost of test case generation. Our experiments are conducted on state-of-the-art\nDNNs obtained using popular open source datasets, including MNIST, CIFAR-10 and\nImageNet.\n", "versions": [{"version": "v1", "created": "Sat, 10 Mar 2018 23:19:13 GMT"}, {"version": "v2", "created": "Thu, 15 Mar 2018 17:53:58 GMT"}, {"version": "v3", "created": "Sun, 18 Mar 2018 23:40:19 GMT"}, {"version": "v4", "created": "Mon, 15 Apr 2019 16:49:14 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Sun", "Youcheng", ""], ["Huang", "Xiaowei", ""], ["Kroening", "Daniel", ""], ["Sharp", "James", ""], ["Hill", "Matthew", ""], ["Ashmore", "Rob", ""]]}, {"id": "1803.04793", "submitter": "Xiaohui Yang", "authors": "Xiaohui Yang, Xiaoying Jiang, Wenming Wu, Juan Zhang, Dan Long, Funa\n  Zhou, Yiming Xu", "title": "Low Rank Variation Dictionary and Inverse Projection Group Sparse\n  Representation Model for Breast Tumor Classification", "comments": "31 pages, 14 figures, 12 tables. arXiv admin note: text overlap with\n  arXiv:1803.03562", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Sparse representation classification achieves good results by addressing\nrecognition problem with sufficient training samples per subject. However, SRC\nperforms not very well for small sample data. In this paper, an\ninverse-projection group sparse representation model is presented for breast\ntumor classification, which is based on constructing low-rank variation\ndictionary. The proposed low-rank variation dictionary tackles tumor\nrecognition problem from the viewpoint of detecting and using variations in\ngene expression profiles of normal and patients, rather than directly using\nthese samples. The inverse projection group sparsity representation model is\nconstructed based on taking full using of exist samples and group effect of\nmicroarray gene data. Extensive experiments on public breast tumor microarray\ngene expression datasets demonstrate the proposed technique is competitive with\nstate-of-the-art methods. The results of Breast-1, Breast-2 and Breast-3\ndatabases are 80.81%, 89.10% and 100% respectively, which are better than the\nlatest literature.\n", "versions": [{"version": "v1", "created": "Sat, 10 Mar 2018 03:59:13 GMT"}], "update_date": "2018-03-14", "authors_parsed": [["Yang", "Xiaohui", ""], ["Jiang", "Xiaoying", ""], ["Wu", "Wenming", ""], ["Zhang", "Juan", ""], ["Long", "Dan", ""], ["Zhou", "Funa", ""], ["Xu", "Yiming", ""]]}, {"id": "1803.04827", "submitter": "Amin Banitalebi-Dehkordi", "authors": "Amin Banitalebi-Dehkordi, Yuanyuan Dong, Mahsa T. Pourazad, and Panos\n  Nasiopoulos", "title": "A Learning-Based Visual Saliency Fusion Model for High Dynamic Range\n  Video (LBVS-HDR)", "comments": null, "journal-ref": "EUSIPCO, 2015", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Saliency prediction for Standard Dynamic Range (SDR) videos has been well\nexplored in the last decade. However, limited studies are available on High\nDynamic Range (HDR) Visual Attention Models (VAMs). Considering that the\ncharacteristic of HDR content in terms of dynamic range and color gamut is\nquite different than those of SDR content, it is essential to identify the\nimportance of different saliency attributes of HDR videos for designing a VAM\nand understand how to combine these features. To this end we propose a\nlearning-based visual saliency fusion method for HDR content (LVBS-HDR) to\ncombine various visual saliency features. In our approach various conspicuity\nmaps are extracted from HDR data, and then for fusing conspicuity maps, a\nRandom Forests algorithm is used to train a model based on the collected data\nfrom an eye-tracking experiment. Performance evaluations demonstrate the\nsuperiority of the proposed fusion method against other existing fusion\nmethods.\n", "versions": [{"version": "v1", "created": "Tue, 13 Mar 2018 14:21:09 GMT"}], "update_date": "2018-03-14", "authors_parsed": [["Banitalebi-Dehkordi", "Amin", ""], ["Dong", "Yuanyuan", ""], ["Pourazad", "Mahsa T.", ""], ["Nasiopoulos", "Panos", ""]]}, {"id": "1803.04831", "submitter": "Shuai Li", "authors": "Shuai Li, Wanqing Li, Chris Cook, Ce Zhu, Yanbo Gao", "title": "Independently Recurrent Neural Network (IndRNN): Building A Longer and\n  Deeper RNN", "comments": "CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent neural networks (RNNs) have been widely used for processing\nsequential data. However, RNNs are commonly difficult to train due to the\nwell-known gradient vanishing and exploding problems and hard to learn\nlong-term patterns. Long short-term memory (LSTM) and gated recurrent unit\n(GRU) were developed to address these problems, but the use of hyperbolic\ntangent and the sigmoid action functions results in gradient decay over layers.\nConsequently, construction of an efficiently trainable deep network is\nchallenging. In addition, all the neurons in an RNN layer are entangled\ntogether and their behaviour is hard to interpret. To address these problems, a\nnew type of RNN, referred to as independently recurrent neural network\n(IndRNN), is proposed in this paper, where neurons in the same layer are\nindependent of each other and they are connected across layers. We have shown\nthat an IndRNN can be easily regulated to prevent the gradient exploding and\nvanishing problems while allowing the network to learn long-term dependencies.\nMoreover, an IndRNN can work with non-saturated activation functions such as\nrelu (rectified linear unit) and be still trained robustly. Multiple IndRNNs\ncan be stacked to construct a network that is deeper than the existing RNNs.\nExperimental results have shown that the proposed IndRNN is able to process\nvery long sequences (over 5000 time steps), can be used to construct very deep\nnetworks (21 layers used in the experiment) and still be trained robustly.\nBetter performances have been achieved on various tasks by using IndRNNs\ncompared with the traditional RNN and LSTM. The code is available at\nhttps://github.com/Sunnydreamrain/IndRNN_Theano_Lasagne.\n", "versions": [{"version": "v1", "created": "Tue, 13 Mar 2018 14:27:42 GMT"}, {"version": "v2", "created": "Tue, 15 May 2018 04:46:08 GMT"}, {"version": "v3", "created": "Tue, 22 May 2018 11:54:28 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Li", "Shuai", ""], ["Li", "Wanqing", ""], ["Cook", "Chris", ""], ["Zhu", "Ce", ""], ["Gao", "Yanbo", ""]]}, {"id": "1803.04836", "submitter": "Amin Banitalebi-Dehkordi", "authors": "Amin Banitalebi Dehkordi", "title": "3D Video Quality Assessment", "comments": "PhD Thesis, UBC, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key factor in designing 3D systems is to understand how different visual\ncues and distortions affect the perceptual quality of 3D video. The ultimate\nway to assess video quality is through subjective tests. However, subjective\nevaluation is time consuming, expensive, and in most cases not even possible.\nAn alternative solution is objective quality metrics, which attempt to model\nthe Human Visual System (HVS) in order to assess the perceptual quality. The\npotential of 3D technology to significantly improve the immersiveness of video\ncontent has been hampered by the difficulty of objectively assessing Quality of\nExperience (QoE). A no-reference (NR) objective 3D quality metric, which could\nhelp determine capturing parameters and improve playback perceptual quality,\nwould be welcomed by camera and display manufactures. Network providers would\nembrace a full-reference (FR) 3D quality metric, as they could use it to ensure\nefficient QoE-based resource management during compression and Quality of\nService (QoS) during transmission.\n", "versions": [{"version": "v1", "created": "Tue, 13 Mar 2018 14:31:15 GMT"}], "update_date": "2018-03-14", "authors_parsed": [["Dehkordi", "Amin Banitalebi", ""]]}, {"id": "1803.04840", "submitter": "Bert Moons", "authors": "Matthijs Van keirsbilck, Bert Moons, Marian Verhelst", "title": "Resource aware design of a deep convolutional-recurrent neural network\n  for speech recognition through audio-visual sensor fusion", "comments": "Tech. report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today's Automatic Speech Recognition systems only rely on acoustic signals\nand often don't perform well under noisy conditions. Performing multi-modal\nspeech recognition - processing acoustic speech signals and lip-reading video\nsimultaneously - significantly enhances the performance of such systems,\nespecially in noisy environments. This work presents the design of such an\naudio-visual system for Automated Speech Recognition, taking memory and\ncomputation requirements into account. First, a Long-Short-Term-Memory neural\nnetwork for acoustic speech recognition is designed. Second, Convolutional\nNeural Networks are used to model lip-reading features. These are combined with\nan LSTM network to model temporal dependencies and perform automatic\nlip-reading on video. Finally, acoustic-speech and visual lip-reading networks\nare combined to process acoustic and visual features simultaneously. An\nattention mechanism ensures performance of the model in noisy environments.\nThis system is evaluated on the TCD-TIMIT 'lipspeaker' dataset for audio-visual\nphoneme recognition with clean audio and with additive white noise at an SNR of\n0dB. It achieves 75.70% and 58.55% phoneme accuracy respectively, over 14\npercentage points better than the state-of-the-art for all noise levels.\n", "versions": [{"version": "v1", "created": "Tue, 13 Mar 2018 14:35:00 GMT"}], "update_date": "2018-03-14", "authors_parsed": [["Van keirsbilck", "Matthijs", ""], ["Moons", "Bert", ""], ["Verhelst", "Marian", ""]]}, {"id": "1803.04842", "submitter": "Amin Banitalebi-Dehkordi", "authors": "Amin Banitalebi-Dehkordi, Mahsa T. Pourazad, and Panos Nasiopoulos", "title": "A Learning-Based Visual Saliency Prediction Model for Stereoscopic 3D\n  Video (LBVS-3D)", "comments": null, "journal-ref": "Multimedia Tools and Applications, 2016", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past decade, many computational saliency prediction models have been\nproposed for 2D images and videos. Considering that the human visual system has\nevolved in a natural 3D environment, it is only natural to want to design\nvisual attention models for 3D content. Existing monocular saliency models are\nnot able to accurately predict the attentive regions when applied to 3D\nimage/video content, as they do not incorporate depth information. This paper\nexplores stereoscopic video saliency prediction by exploiting both low-level\nattributes such as brightness, color, texture, orientation, motion, and depth,\nas well as high-level cues such as face, person, vehicle, animal, text, and\nhorizon. Our model starts with a rough segmentation and quantifies several\nintuitive observations such as the effects of visual discomfort level, depth\nabruptness, motion acceleration, elements of surprise, size and compactness of\nthe salient regions, and emphasizing only a few salient objects in a scene. A\nnew fovea-based model of spatial distance between the image regions is adopted\nfor considering local and global feature calculations. To efficiently fuse the\nconspicuity maps generated by our method to one single saliency map that is\nhighly correlated with the eye-fixation data, a random forest based algorithm\nis utilized. The performance of the proposed saliency model is evaluated\nagainst the results of an eye-tracking experiment, which involved 24 subjects\nand an in-house database of 61 captured stereoscopic videos. Our stereo video\ndatabase as well as the eye-tracking data are publicly available along with\nthis paper. Experiment results show that the proposed saliency prediction\nmethod achieves competitive performance compared to the state-of-the-art\napproaches.\n", "versions": [{"version": "v1", "created": "Tue, 13 Mar 2018 14:37:58 GMT"}], "update_date": "2018-03-14", "authors_parsed": [["Banitalebi-Dehkordi", "Amin", ""], ["Pourazad", "Mahsa T.", ""], ["Nasiopoulos", "Panos", ""]]}, {"id": "1803.04858", "submitter": "Jimmy Wu", "authors": "Jimmy Wu, Diondra Peck, Scott Hsieh, Vandana Dialani, Constance D.\n  Lehman, Bolei Zhou, Vasilis Syrgkanis, Lester Mackey, and Genevieve Patterson", "title": "Expert identification of visual primitives used by CNNs during mammogram\n  classification", "comments": null, "journal-ref": "Medical Imaging 2018: Computer-Aided Diagnosis, Proc. of SPIE Vol.\n  10575, 105752T", "doi": "10.1117/12.2293890", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work interprets the internal representations of deep neural networks\ntrained for classification of diseased tissue in 2D mammograms. We propose an\nexpert-in-the-loop interpretation method to label the behavior of internal\nunits in convolutional neural networks (CNNs). Expert radiologists identify\nthat the visual patterns detected by the units are correlated with meaningful\nmedical phenomena such as mass tissue and calcificated vessels. We demonstrate\nthat several trained CNN models are able to produce explanatory descriptions to\nsupport the final classification decisions. We view this as an important first\nstep toward interpreting the internal representations of medical classification\nCNNs and explaining their predictions.\n", "versions": [{"version": "v1", "created": "Tue, 13 Mar 2018 14:54:38 GMT"}], "update_date": "2018-03-14", "authors_parsed": [["Wu", "Jimmy", ""], ["Peck", "Diondra", ""], ["Hsieh", "Scott", ""], ["Dialani", "Vandana", ""], ["Lehman", "Constance D.", ""], ["Zhou", "Bolei", ""], ["Syrgkanis", "Vasilis", ""], ["Mackey", "Lester", ""], ["Patterson", "Genevieve", ""]]}, {"id": "1803.04873", "submitter": "Krunoslav Vinicki", "authors": "Krunoslav Vinicki, Pierluigi Ferrari, Maja Belic, Romana Turk", "title": "Using Convolutional Neural Networks for Determining Reticulocyte\n  Percentage in Cats", "comments": "10 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in artificial intelligence (AI), specifically in computer\nvision (CV) and deep learning (DL), have created opportunities for novel\nsystems in many fields. In the last few years, deep learning applications have\ndemonstrated impressive results not only in fields such as autonomous driving\nand robotics, but also in the field of medicine, where they have, in some\ncases, even exceeded human-level performance. However, despite the huge\npotential, adoption of deep learning-based methods is still slow in many areas,\nespecially in veterinary medicine, where we haven't been able to find any\nresearch papers using modern convolutional neural networks (CNNs) in medical\nimage processing. We believe that using deep learning-based medical imaging can\nenable more accurate, faster and less expensive diagnoses in veterinary\nmedicine. In order to do so, however, these methods have to be accessible to\neveryone in this field, not just to computer scientists. To show the potential\nof this technology, we present results on a real-world task in veterinary\nmedicine that is usually done manually: feline reticulocyte percentage. Using\nan open source Keras implementation of the Single-Shot MultiBox Detector (SSD)\nmodel architecture and training it on only 800 labeled images, we achieve an\naccuracy of 98.7% at predicting the correct number of aggregate reticulocytes\nin microscope images of cat blood smears. The main motivation behind this paper\nis to show not only that deep learning can approach or even exceed human-level\nperformance on a task like this, but also that anyone in the field can\nimplement it, even without a background in computer science.\n", "versions": [{"version": "v1", "created": "Tue, 13 Mar 2018 15:17:30 GMT"}, {"version": "v2", "created": "Wed, 14 Mar 2018 15:30:00 GMT"}], "update_date": "2018-03-15", "authors_parsed": [["Vinicki", "Krunoslav", ""], ["Ferrari", "Pierluigi", ""], ["Belic", "Maja", ""], ["Turk", "Romana", ""]]}, {"id": "1803.04907", "submitter": "Xiaowei Xu", "authors": "Xiaowei Xu, Qing Lu, Yu Hu, Lin Yang, Sharon Hu, Danny Chen, Yiyu Shi", "title": "Quantization of Fully Convolutional Networks for Accurate Biomedical\n  Image Segmentation", "comments": "9 pages, 11 Figs, 1 Table, Accepted by CVPR", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With pervasive applications of medical imaging in health-care, biomedical\nimage segmentation plays a central role in quantitative analysis, clinical\ndiagno- sis, and medical intervention. Since manual anno- tation su ers limited\nreproducibility, arduous e orts, and excessive time, automatic segmentation is\ndesired to process increasingly larger scale histopathological data. Recently,\ndeep neural networks (DNNs), par- ticularly fully convolutional networks\n(FCNs), have been widely applied to biomedical image segmenta- tion, attaining\nmuch improved performance. At the same time, quantization of DNNs has become an\nac- tive research topic, which aims to represent weights with less memory\n(precision) to considerably reduce memory and computation requirements of DNNs\nwhile maintaining acceptable accuracy. In this paper, we apply quantization\ntechniques to FCNs for accurate biomedical image segmentation. Unlike existing\nlitera- ture on quantization which primarily targets memory and computation\ncomplexity reduction, we apply quan- tization as a method to reduce over tting\nin FCNs for better accuracy. Speci cally, we focus on a state-of- the-art\nsegmentation framework, suggestive annotation [22], which judiciously extracts\nrepresentative annota- tion samples from the original training dataset, obtain-\ning an e ective small-sized balanced training dataset. We develop two new\nquantization processes for this framework: (1) suggestive annotation with\nquantiza- tion for highly representative training samples, and (2) network\ntraining with quantization for high accuracy. Extensive experiments on the\nMICCAI Gland dataset show that both quantization processes can improve the\nsegmentation performance, and our proposed method exceeds the current\nstate-of-the-art performance by up to 1%. In addition, our method has a\nreduction of up to 6.4x on memory usage.\n", "versions": [{"version": "v1", "created": "Tue, 13 Mar 2018 16:06:13 GMT"}], "update_date": "2018-03-14", "authors_parsed": [["Xu", "Xiaowei", ""], ["Lu", "Qing", ""], ["Hu", "Yu", ""], ["Yang", "Lin", ""], ["Hu", "Sharon", ""], ["Chen", "Danny", ""], ["Shi", "Yiyu", ""]]}, {"id": "1803.04953", "submitter": "Andrew Khalel", "authors": "Andrew Khalel, Motaz El-Saban", "title": "Automatic Pixelwise Object Labeling for Aerial Imagery Using Stacked\n  U-Nets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automation of objects labeling in aerial imagery is a computer vision task\nwith numerous practical applications. Fields like energy exploration require an\nautomated method to process a continuous stream of imagery on a daily basis. In\nthis paper we propose a pipeline to tackle this problem using a stack of\nconvolutional neural networks (U-Net architecture) arranged end-to-end. Each\nnetwork works as post-processor to the previous one. Our model outperforms\ncurrent state-of-the-art on two different datasets: Inria Aerial Image Labeling\ndataset and Massachusetts Buildings dataset each with different characteristics\nsuch as spatial resolution, object shapes and scales. Moreover, we\nexperimentally validate computation time savings by processing sub-sampled\nimages and later upsampling pixelwise labeling. These savings come at a\nnegligible degradation in segmentation quality. Though the conducted\nexperiments in this paper cover only aerial imagery, the technique presented is\ngeneral and can handle other types of images.\n", "versions": [{"version": "v1", "created": "Tue, 13 Mar 2018 17:39:01 GMT"}], "update_date": "2018-03-14", "authors_parsed": [["Khalel", "Andrew", ""], ["El-Saban", "Motaz", ""]]}, {"id": "1803.04964", "submitter": "Pan Wei", "authors": "Archit Harsh, John E. Ball, Pan Wei", "title": "Onion-Peeling Outlier Detection in 2-D data Sets", "comments": "6 pages, 4 figures, journal paper", "journal-ref": "International Journal of Computer Application, Vol.139 (3),\n  pp.26-31, April, 2016", "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.SP", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Outlier Detection is a critical and cardinal research task due its array of\napplications in variety of domains ranging from data mining, clustering,\nstatistical analysis, fraud detection, network intrusion detection and\ndiagnosis of diseases etc. Over the last few decades, distance-based outlier\ndetection algorithms have gained significant reputation as a viable alternative\nto the more traditional statistical approaches due to their scalable,\nnon-parametric and simple implementation. In this paper, we present a modified\nonion peeling (Convex hull) genetic algorithm to detect outliers in a Gaussian\n2-D point data set. We present three different scenarios of outlier detection\nusing a) Euclidean Distance Metric b) Standardized Euclidean Distance Metric\nand c) Mahalanobis Distance Metric. Finally, we analyze the performance and\nevaluate the results.\n", "versions": [{"version": "v1", "created": "Mon, 12 Mar 2018 22:37:14 GMT"}], "update_date": "2018-03-15", "authors_parsed": [["Harsh", "Archit", ""], ["Ball", "John E.", ""], ["Wei", "Pan", ""]]}, {"id": "1803.04969", "submitter": "Faisal Qureshi", "authors": "Jordan Stadler and Faisal Z. Qureshi", "title": "A Framework for Video-Driven Crowd Synthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a framework for video-driven crowd synthesis. Motion vectors\nextracted from input crowd video are processed to compute global motion paths.\nThese paths encode the dominant motions observed in the input video. These\npaths are then fed into a behavior-based crowd simulation framework, which is\nresponsible for synthesizing crowd animations that respect the motion patterns\nobserved in the video. Our system synthesizes 3D virtual crowds by animating\nvirtual humans along the trajectories returned by the crowd simulation\nframework. We also propose a new metric for comparing the \"visual similarity\"\nbetween the synthesized crowd and exemplar crowd. We demonstrate the proposed\napproach on crowd videos collected under different settings.\n", "versions": [{"version": "v1", "created": "Tue, 13 Mar 2018 16:58:27 GMT"}], "update_date": "2018-03-15", "authors_parsed": [["Stadler", "Jordan", ""], ["Qureshi", "Faisal Z.", ""]]}, {"id": "1803.04988", "submitter": "Dawei Li", "authors": "Kai Xu, Dawei Li, Nick Cassimatis, Xiaolong Wang", "title": "LCANet: End-to-End Lipreading with Cascaded Attention-CTC", "comments": "FG 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine lipreading is a special type of automatic speech recognition (ASR)\nwhich transcribes human speech by visually interpreting the movement of related\nface regions including lips, face, and tongue. Recently, deep neural network\nbased lipreading methods show great potential and have exceeded the accuracy of\nexperienced human lipreaders in some benchmark datasets. However, lipreading is\nstill far from being solved, and existing methods tend to have high error rates\non the wild data. In this paper, we propose LCANet, an end-to-end deep neural\nnetwork based lipreading system. LCANet encodes input video frames using a\nstacked 3D convolutional neural network (CNN), highway network and\nbidirectional GRU network. The encoder effectively captures both short-term and\nlong-term spatio-temporal information. More importantly, LCANet incorporates a\ncascaded attention-CTC decoder to generate output texts. By cascading CTC with\nattention, it partially eliminates the defect of the conditional independence\nassumption of CTC within the hidden neural layers, and this yields notably\nperformance improvement as well as faster convergence. The experimental results\nshow the proposed system achieves a 1.3% CER and 3.0% WER on the GRID corpus\ndatabase, leading to a 12.3% improvement compared to the state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Tue, 13 Mar 2018 18:04:10 GMT"}], "update_date": "2018-03-15", "authors_parsed": [["Xu", "Kai", ""], ["Li", "Dawei", ""], ["Cassimatis", "Nick", ""], ["Wang", "Xiaolong", ""]]}, {"id": "1803.05011", "submitter": "Yingying Zhu", "authors": "Yingying Zhu, Mert R. Sabuncu", "title": "A Probabilistic Disease Progression Model for Predicting Future Clinical\n  Outcome", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we consider the problem of predicting the course of a\nprogressive disease, such as cancer or Alzheimer's. Progressive diseases often\nstart with mild symptoms that might precede a diagnosis, and each patient\nfollows their own trajectory. Patient trajectories exhibit wild variability,\nwhich can be associated with many factors such as genotype, age, or sex. An\nadditional layer of complexity is that, in real life, the amount and type of\ndata available for each patient can differ significantly. For example, for one\npatient we might have no prior history, whereas for another patient we might\nhave detailed clinical assessments obtained at multiple prior time-points. This\npaper presents a probabilistic model that can handle multiple modalities\n(including images and clinical assessments) and variable patient histories with\nirregular timings and missing entries, to predict clinical scores at future\ntime-points. We use a sigmoidal function to model latent disease progression,\nwhich gives rise to clinical observations in our generative model. We\nimplemented an approximate Bayesian inference strategy on the proposed model to\nestimate the parameters on data from a large population of subjects.\nFurthermore, the Bayesian framework enables the model to automatically\nfine-tune its predictions based on historical observations that might be\navailable on the test subject. We applied our method to a longitudinal\nAlzheimer's disease dataset with more than 3000 subjects [23] and present a\ndetailed empirical analysis of prediction performance under different\nscenarios, with comparisons against several benchmarks. We also demonstrate how\nthe proposed model can be interrogated to glean insights about temporal\ndynamics in Alzheimer's disease.\n", "versions": [{"version": "v1", "created": "Tue, 13 Mar 2018 19:05:08 GMT"}], "update_date": "2018-03-19", "authors_parsed": [["Zhu", "Yingying", ""], ["Sabuncu", "Mert R.", ""]]}, {"id": "1803.05026", "submitter": "Vaneet Aggarwal", "authors": "Wenqi Wang and Vaneet Aggarwal and Shuchin Aeron", "title": "Principal Component Analysis with Tensor Train Subspace", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.IT cs.NA math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tensor train is a hierarchical tensor network structure that helps alleviate\nthe curse of dimensionality by parameterizing large-scale multidimensional data\nvia a set of network of low-rank tensors. Associated with such a construction\nis a notion of Tensor Train subspace and in this paper we propose a TT-PCA\nalgorithm for estimating this structured subspace from the given data. By\nmaintaining low rank tensor structure, TT-PCA is more robust to noise comparing\nwith PCA or Tucker-PCA. This is borne out numerically by testing the proposed\napproach on the Extended YaleFace Dataset B.\n", "versions": [{"version": "v1", "created": "Tue, 13 Mar 2018 19:58:46 GMT"}], "update_date": "2018-03-15", "authors_parsed": [["Wang", "Wenqi", ""], ["Aggarwal", "Vaneet", ""], ["Aeron", "Shuchin", ""]]}, {"id": "1803.05070", "submitter": "Sugumar Murugesan", "authors": "Ashok Sundaresan, Sugumar Murugesan, Sean Davis, Karthik Kappaganthu,\n  ZhongYi Jin, Divya Jain, Anurag Maunder", "title": "A Multi-Modal Approach to Infer Image Affect", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The group affect or emotion in an image of people can be inferred by\nextracting features about both the people in the picture and the overall makeup\nof the scene. The state-of-the-art on this problem investigates a combination\nof facial features, scene extraction and even audio tonality. This paper\ncombines three additional modalities, namely, human pose, text-based tagging\nand CNN extracted features / predictions. To the best of our knowledge, this is\nthe first time all of the modalities were extracted using deep neural networks.\nWe evaluate the performance of our approach against baselines and identify\ninsights throughout this paper.\n", "versions": [{"version": "v1", "created": "Tue, 13 Mar 2018 23:07:45 GMT"}], "update_date": "2018-03-15", "authors_parsed": [["Sundaresan", "Ashok", ""], ["Murugesan", "Sugumar", ""], ["Davis", "Sean", ""], ["Kappaganthu", "Karthik", ""], ["Jin", "ZhongYi", ""], ["Jain", "Divya", ""], ["Maunder", "Anurag", ""]]}, {"id": "1803.05082", "submitter": "Md Amirul Islam", "authors": "Md Amirul Islam, Mahmoud Kalash, Neil D. B. Bruce", "title": "Revisiting Salient Object Detection: Simultaneous Detection, Ranking,\n  and Subitizing of Multiple Salient Objects", "comments": "To appear in CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Salient object detection is a problem that has been considered in detail and\nmany solutions proposed. In this paper, we argue that work to date has\naddressed a problem that is relatively ill-posed. Specifically, there is not\nuniversal agreement about what constitutes a salient object when multiple\nobservers are queried. This implies that some objects are more likely to be\njudged salient than others, and implies a relative rank exists on salient\nobjects. The solution presented in this paper solves this more general problem\nthat considers relative rank, and we propose data and metrics suitable to\nmeasuring success in a relative object saliency landscape. A novel deep\nlearning solution is proposed based on a hierarchical representation of\nrelative saliency and stage-wise refinement. We also show that the problem of\nsalient object subitizing can be addressed with the same network, and our\napproach exceeds performance of any prior work across all metrics considered\n(both traditional and newly proposed).\n", "versions": [{"version": "v1", "created": "Wed, 14 Mar 2018 00:20:28 GMT"}, {"version": "v2", "created": "Fri, 23 Mar 2018 14:59:48 GMT"}], "update_date": "2018-03-26", "authors_parsed": [["Islam", "Md Amirul", ""], ["Kalash", "Mahmoud", ""], ["Bruce", "Neil D. B.", ""]]}, {"id": "1803.05120", "submitter": "Yufan He", "authors": "Yufan He, Aaron Carass, Bruno M. Jedynak, Sharon D. Solomon, Shiv\n  Saidha, Peter A. Calabresi, Jerry L. Prince", "title": "Topology guaranteed segmentation of the human retina from OCT using\n  convolutional neural networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optical coherence tomography (OCT) is a noninvasive imaging modality which\ncan be used to obtain depth images of the retina. The changing layer\nthicknesses can thus be quantified by analyzing these OCT images, moreover\nthese changes have been shown to correlate with disease progression in multiple\nsclerosis. Recent automated retinal layer segmentation tools use machine\nlearning methods to perform pixel-wise labeling and graph methods to guarantee\nthe layer hierarchy or topology. However, graph parameters like distance and\nsmoothness constraints must be experimentally assigned by retinal region and\npathology, thus degrading the flexibility and time efficiency of the whole\nframework. In this paper, we develop cascaded deep networks to provide a\ntopologically correct segmentation of the retinal layers in a single feed\nforward propagation. The first network (S-Net) performs pixel-wise labeling and\nthe second regression network (R-Net) takes the topologically unconstrained\nS-Net results and outputs layer thicknesses for each layer and each position.\nRelu activation is used as the final operation of the R-Net which guarantees\nnon-negativity of the output layer thickness. Since the segmentation boundary\nposition is acquired by summing up the corresponding non-negative layer\nthicknesses, the layer ordering (i.e., topology) of the reconstructed\nboundaries is guaranteed even at the fovea where the distances between\nboundaries can be zero. The R-Net is trained using simulated masks and thus can\nbe generalized to provide topology guaranteed segmentation for other layered\nstructures. This deep network has achieved comparable mean absolute boundary\nerror (2.82 {\\mu}m) to state-of-the-art graph methods (2.83 {\\mu}m).\n", "versions": [{"version": "v1", "created": "Wed, 14 Mar 2018 03:21:01 GMT"}], "update_date": "2018-03-15", "authors_parsed": [["He", "Yufan", ""], ["Carass", "Aaron", ""], ["Jedynak", "Bruno M.", ""], ["Solomon", "Sharon D.", ""], ["Saidha", "Shiv", ""], ["Calabresi", "Peter A.", ""], ["Prince", "Jerry L.", ""]]}, {"id": "1803.05137", "submitter": "Arghya Pal", "authors": "Arghya Pal, Vineeth N Balasubramanian", "title": "Adversarial Data Programming: Using GANs to Relax the Bottleneck of\n  Curated Labeled Data", "comments": "CVPR 2018 main conference paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Paucity of large curated hand-labeled training data for every\ndomain-of-interest forms a major bottleneck in the deployment of machine\nlearning models in computer vision and other fields. Recent work (Data\nProgramming) has shown how distant supervision signals in the form of labeling\nfunctions can be used to obtain labels for given data in near-constant time. In\nthis work, we present Adversarial Data Programming (ADP), which presents an\nadversarial methodology to generate data as well as a curated aggregated label\nhas given a set of weak labeling functions. We validated our method on the\nMNIST, Fashion MNIST, CIFAR 10 and SVHN datasets, and it outperformed many\nstate-of-the-art models. We conducted extensive experiments to study its\nusefulness, as well as showed how the proposed ADP framework can be used for\ntransfer learning as well as multi-task learning, where data from two domains\nare generated simultaneously using the framework along with the label\ninformation. Our future work will involve understanding the theoretical\nimplications of this new framework from a game-theoretic perspective, as well\nas explore the performance of the method on more complex datasets.\n", "versions": [{"version": "v1", "created": "Wed, 14 Mar 2018 04:54:02 GMT"}], "update_date": "2018-03-15", "authors_parsed": [["Pal", "Arghya", ""], ["Balasubramanian", "Vineeth N", ""]]}, {"id": "1803.05192", "submitter": "Andreas Selmar Hauptmann", "authors": "Andreas Hauptmann, Simon Arridge, Felix Lucka, Vivek Muthurangu, and\n  Jennifer A. Steeden", "title": "Real-time Cardiovascular MR with Spatio-temporal Artifact Suppression\n  using Deep Learning - Proof of Concept in Congenital Heart Disease", "comments": null, "journal-ref": null, "doi": "10.1002/mrm.27480", "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  PURPOSE: Real-time assessment of ventricular volumes requires high\nacceleration factors. Residual convolutional neural networks (CNN) have shown\npotential for removing artifacts caused by data undersampling. In this study we\ninvestigated the effect of different radial sampling patterns on the accuracy\nof a CNN. We also acquired actual real-time undersampled radial data in\npatients with congenital heart disease (CHD), and compare CNN reconstruction to\nCompressed Sensing (CS).\n  METHODS: A 3D (2D plus time) CNN architecture was developed, and trained\nusing 2276 gold-standard paired 3D data sets, with 14x radial undersampling.\nFour sampling schemes were tested, using 169 previously unseen 3D 'synthetic'\ntest data sets. Actual real-time tiny Golden Angle (tGA) radial SSFP data was\nacquired in 10 new patients (122 3D data sets), and reconstructed using the 3D\nCNN as well as a CS algorithm; GRASP.\n  RESULTS: Sampling pattern was shown to be important for image quality, and\naccurate visualisation of cardiac structures. For actual real-time data,\noverall reconstruction time with CNN (including creation of aliased images) was\nshown to be more than 5x faster than GRASP. Additionally, CNN image quality and\naccuracy of biventricular volumes was observed to be superior to GRASP for the\nsame raw data.\n  CONCLUSION: This paper has demonstrated the potential for the use of a 3D CNN\nfor deep de-aliasing of real-time radial data, within the clinical setting.\nClinical measures of ventricular volumes using real-time data with CNN\nreconstruction are not statistically significantly different from the\ngold-standard, cardiac gated, BH techniques.\n", "versions": [{"version": "v1", "created": "Wed, 14 Mar 2018 10:25:35 GMT"}, {"version": "v2", "created": "Wed, 28 Mar 2018 21:27:41 GMT"}, {"version": "v3", "created": "Thu, 14 Jun 2018 13:03:43 GMT"}], "update_date": "2020-09-07", "authors_parsed": [["Hauptmann", "Andreas", ""], ["Arridge", "Simon", ""], ["Lucka", "Felix", ""], ["Muthurangu", "Vivek", ""], ["Steeden", "Jennifer A.", ""]]}, {"id": "1803.05196", "submitter": "Xiao Song", "authors": "Xiao Song, Xu Zhao, Hanwen Hu, Liangji Fang", "title": "EdgeStereo: A Context Integrated Residual Pyramid Network for Stereo\n  Matching", "comments": "Accepted by Asian Conference on Computer Vision (ACCV) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent convolutional neural networks, especially end-to-end disparity\nestimation models, achieve remarkable performance on stereo matching task.\nHowever, existed methods, even with the complicated cascade structure, may fail\nin the regions of non-textures, boundaries and tiny details. Focus on these\nproblems, we propose a multi-task network EdgeStereo that is composed of a\nbackbone disparity network and an edge sub-network. Given a binocular image\npair, our model enables end-to-end prediction of both disparity map and edge\nmap. Basically, we design a context pyramid to encode multi-scale context\ninformation in disparity branch, followed by a compact residual pyramid for\ncascaded refinement. To further preserve subtle details, our EdgeStereo model\nintegrates edge cues by feature embedding and edge-aware smoothness loss\nregularization. Comparative results demonstrates that stereo matching and edge\ndetection can help each other in the unified model. Furthermore, our method\nachieves state-of-art performance on both KITTI Stereo and Scene Flow\nbenchmarks, which proves the effectiveness of our design.\n", "versions": [{"version": "v1", "created": "Wed, 14 Mar 2018 10:36:54 GMT"}, {"version": "v2", "created": "Tue, 14 Aug 2018 10:05:02 GMT"}, {"version": "v3", "created": "Sun, 23 Sep 2018 14:36:00 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Song", "Xiao", ""], ["Zhao", "Xu", ""], ["Hu", "Hanwen", ""], ["Fang", "Liangji", ""]]}, {"id": "1803.05200", "submitter": "Nibaran Das", "authors": "Aritra Das, Swarnendu Ghosh, Ritesh Sarkhel, Sandipan Choudhuri,\n  Nibaran Das, and Mita Nasipuri", "title": "Combining Multi-level Contexts of Superpixel using Convolutional Neural\n  Networks to perform Natural Scene Labeling", "comments": "Accepted for publication in the Proceedings of Second International\n  Conference on Computing and Communication 2018, Sikkim Manipal Institute of\n  Technology, Sikkim, India", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern deep learning algorithms have triggered various image segmentation\napproaches. However most of them deal with pixel based segmentation. However,\nsuperpixels provide a certain degree of contextual information while reducing\ncomputation cost. In our approach, we have performed superpixel level semantic\nsegmentation considering 3 various levels as neighbours for semantic contexts.\nFurthermore, we have enlisted a number of ensemble approaches like max-voting\nand weighted-average. We have also used the Dempster-Shafer theory of\nuncertainty to analyze confusion among various classes. Our method has proved\nto be superior to a number of different modern approaches on the same dataset.\n", "versions": [{"version": "v1", "created": "Wed, 14 Mar 2018 10:56:40 GMT"}], "update_date": "2018-03-15", "authors_parsed": [["Das", "Aritra", ""], ["Ghosh", "Swarnendu", ""], ["Sarkhel", "Ritesh", ""], ["Choudhuri", "Sandipan", ""], ["Das", "Nibaran", ""], ["Nasipuri", "Mita", ""]]}, {"id": "1803.05210", "submitter": "Luca Ghiani", "authors": "Valerio Mura, Giulia Orr\\`u, Roberto Casula, Alessandra Sibiriu,\n  Giulia Loi, Pierluigi Tuveri, Luca Ghiani, and Gian Luca Marcialis", "title": "LivDet 2017 Fingerprint Liveness Detection Competition 2017", "comments": "presented at ICB 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fingerprint Presentation Attack Detection (FPAD) deals with distinguishing\nimages coming from artificial replicas of the fingerprint characteristic, made\nup of materials like silicone, gelatine or latex, and images coming from alive\nfingerprints. Images are captured by modern scanners, typically relying on\nsolid-state or optical technologies. Since from 2009, the Fingerprint Liveness\nDetection Competition (LivDet) aims to assess the performance of the\nstate-of-the-art algorithms according to a rigorous experimental protocol and,\nat the same time, a simple overview of the basic achievements. The competition\nis open to all academics research centers and all companies that work in this\nfield. The positive, increasing trend of the participants number, which\nsupports the success of this initiative, is confirmed even this year: 17\nalgorithms were submitted to the competition, with a larger involvement of\ncompanies and academies. This means that the topic is relevant for both sides,\nand points out that a lot of work must be done in terms of fundamental and\napplied research.\n", "versions": [{"version": "v1", "created": "Wed, 14 Mar 2018 11:28:52 GMT"}], "update_date": "2018-03-15", "authors_parsed": [["Mura", "Valerio", ""], ["Orr\u00f9", "Giulia", ""], ["Casula", "Roberto", ""], ["Sibiriu", "Alessandra", ""], ["Loi", "Giulia", ""], ["Tuveri", "Pierluigi", ""], ["Ghiani", "Luca", ""], ["Marcialis", "Gian Luca", ""]]}, {"id": "1803.05215", "submitter": "Filippos Kokkinos", "authors": "Filippos Kokkinos, Stamatios Lefkimmiatis", "title": "Deep Image Demosaicking using a Cascade of Convolutional Residual\n  Denoising Networks", "comments": "Camera ready paper to appear in the Proceedings of ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Demosaicking and denoising are among the most crucial steps of modern digital\ncamera pipelines and their joint treatment is a highly ill-posed inverse\nproblem where at-least two-thirds of the information are missing and the rest\nare corrupted by noise. This poses a great challenge in obtaining meaningful\nreconstructions and a special care for the efficient treatment of the problem\nis required. While there are several machine learning approaches that have been\nrecently introduced to deal with joint image demosaicking-denoising, in this\nwork we propose a novel deep learning architecture which is inspired by\npowerful classical image regularization methods and large-scale convex\noptimization techniques. Consequently, our derived network is more transparent\nand has a clear interpretation compared to alternative competitive deep\nlearning approaches. Our extensive experiments demonstrate that our network\noutperforms any previous approaches on both noisy and noise-free data. This\nimprovement in reconstruction quality is attributed to the principled way we\ndesign our network architecture, which also requires fewer trainable parameters\nthan the current state-of-the-art deep network solution. Finally, we show that\nour network has the ability to generalize well even when it is trained on small\ndatasets, while keeping the overall number of trainable parameters low.\n", "versions": [{"version": "v1", "created": "Wed, 14 Mar 2018 11:44:08 GMT"}, {"version": "v2", "created": "Fri, 30 Mar 2018 10:39:42 GMT"}, {"version": "v3", "created": "Wed, 11 Jul 2018 13:55:31 GMT"}, {"version": "v4", "created": "Thu, 12 Jul 2018 07:32:27 GMT"}], "update_date": "2018-07-13", "authors_parsed": [["Kokkinos", "Filippos", ""], ["Lefkimmiatis", "Stamatios", ""]]}, {"id": "1803.05258", "submitter": "Pouya Samangouei", "authors": "Pouya Samangouei, Mahyar Najibi, Larry Davis, Rama Chellappa", "title": "Face-MagNet: Magnifying Feature Maps to Detect Small Faces", "comments": "Accepted in WACV18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce the Face Magnifier Network (Face-MageNet), a face\ndetector based on the Faster-RCNN framework which enables the flow of\ndiscriminative information of small scale faces to the classifier without any\nskip or residual connections. To achieve this, Face-MagNet deploys a set of\nConvTranspose, also known as deconvolution, layers in the Region Proposal\nNetwork (RPN) and another set before the Region of Interest (RoI) pooling layer\nto facilitate detection of finer faces. In addition, we also design, train, and\nevaluate three other well-tuned architectures that represent the conventional\nsolutions to the scale problem: context pooling, skip connections, and scale\npartitioning. Each of these three networks achieves comparable results to the\nstate-of-the-art face detectors. With extensive experiments, we show that\nFace-MagNet based on a VGG16 architecture achieves better results than the\nrecently proposed ResNet101-based HR method on the task of face detection on\nWIDER dataset and also achieves similar results on the hard set as our other\nmethod SSH.\n", "versions": [{"version": "v1", "created": "Wed, 14 Mar 2018 13:22:05 GMT"}], "update_date": "2018-03-15", "authors_parsed": [["Samangouei", "Pouya", ""], ["Najibi", "Mahyar", ""], ["Davis", "Larry", ""], ["Chellappa", "Rama", ""]]}, {"id": "1803.05263", "submitter": "Kai Yi", "authors": "Kai Yi, Zhiqiang Jian, Shitao Chen and Nanning Zheng", "title": "Feature Selective Small Object Detection via Knowledge-based Recurrent\n  Attentive Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  At present, the performance of deep neural network in general object\ndetection is comparable to or even surpasses that of human beings. However, due\nto the limitations of deep learning itself, the small proportion of feature\npixels, and the occurence of blur and occlusion, the detection of small objects\nin complex scenes is still an open question. But we can not deny that real-time\nand accurate object detection is fundamental to automatic perception and\nsubsequent perception-based decision-making and planning tasks of autonomous\ndriving.\n  Considering the characteristics of small objects in autonomous driving scene,\nwe proposed a novel method named KB-RANN, which based on domain knowledge,\nintuitive experience and feature attentive selection. It can focus on\nparticular parts of image features, and then it tries to stress the importance\nof these features and strengthenes the learning parameters of them. Our\ncomparative experiments on KITTI and COCO datasets show that our proposed\nmethod can achieve considerable results both in speed and accuracy, and can\nimprove the effect of small object detection through self-selection of\nimportant features and continuous enhancement of proposed method, and deployed\nit in our self-developed autonomous driving car.\n", "versions": [{"version": "v1", "created": "Tue, 13 Mar 2018 12:45:55 GMT"}, {"version": "v2", "created": "Thu, 15 Mar 2018 13:55:00 GMT"}, {"version": "v3", "created": "Wed, 2 May 2018 15:04:28 GMT"}, {"version": "v4", "created": "Sat, 20 Apr 2019 08:31:05 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Yi", "Kai", ""], ["Jian", "Zhiqiang", ""], ["Chen", "Shitao", ""], ["Zheng", "Nanning", ""]]}, {"id": "1803.05265", "submitter": "Minghui Liao", "authors": "Minghui Liao, Zhen Zhu, Baoguang Shi, Gui-song Xia, Xiang Bai", "title": "Rotation-Sensitive Regression for Oriented Scene Text Detection", "comments": "accepted by CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text in natural images is of arbitrary orientations, requiring detection in\nterms of oriented bounding boxes. Normally, a multi-oriented text detector\noften involves two key tasks: 1) text presence detection, which is a\nclassification problem disregarding text orientation; 2) oriented bounding box\nregression, which concerns about text orientation. Previous methods rely on\nshared features for both tasks, resulting in degraded performance due to the\nincompatibility of the two tasks. To address this issue, we propose to perform\nclassification and regression on features of different characteristics,\nextracted by two network branches of different designs. Concretely, the\nregression branch extracts rotation-sensitive features by actively rotating the\nconvolutional filters, while the classification branch extracts\nrotation-invariant features by pooling the rotation-sensitive features. The\nproposed method named Rotation-sensitive Regression Detector (RRD) achieves\nstate-of-the-art performance on three oriented scene text benchmark datasets,\nincluding ICDAR 2015, MSRA-TD500, RCTW-17 and COCO-Text. Furthermore, RRD\nachieves a significant improvement on a ship collection dataset, demonstrating\nits generality on oriented object detection.\n", "versions": [{"version": "v1", "created": "Wed, 14 Mar 2018 13:29:33 GMT"}], "update_date": "2018-03-15", "authors_parsed": [["Liao", "Minghui", ""], ["Zhu", "Zhen", ""], ["Shi", "Baoguang", ""], ["Xia", "Gui-song", ""], ["Bai", "Xiang", ""]]}, {"id": "1803.05266", "submitter": "Jie Luo", "authors": "Jie Luo, Alireza Sedghi, Karteek Popuri, Dana Cobzas, Miaomiao Zhang,\n  Frank Preiswerk, Matthew Toews, Alexandra Golby, Masashi Sugiyama, William M.\n  Wells III and Sarah Frisken", "title": "On the Applicability of Registration Uncertainty", "comments": "MICCAI 2019. arXiv admin note: text overlap with arXiv:1704.08121", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Estimating the uncertainty in (probabilistic) image registration enables,\ne.g., surgeons to assess the operative risk based on the trustworthiness of the\nregistered image data. If surgeons receive inaccurately calculated registration\nuncertainty and misplace unwarranted confidence in the alignment solutions,\nsevere consequences may result. For probabilistic image registration (PIR), the\npredominant way to quantify the registration uncertainty is using summary\nstatistics of the distribution of transformation parameters. The majority of\nexisting research focuses on trying out different summary statistics as well as\na means to exploit them. Distinctively, in this paper, we study two rarely\nexamined topics: (1) whether those summary statistics of the transformation\ndistribution most informatively represent the registration uncertainty; (2)\nDoes utilizing the registration uncertainty always be beneficial. We show that\nthere are two types of uncertainties: the transformation uncertainty, Ut, and\nlabel uncertainty Ul. The conventional way of using Ut to quantify Ul is\ninappropriate and can be misleading. By a real data experiment, we also share a\npotentially critical finding that making use of the registration uncertainty\nmay not always be an improvement.\n", "versions": [{"version": "v1", "created": "Wed, 14 Mar 2018 13:30:25 GMT"}, {"version": "v2", "created": "Tue, 20 Mar 2018 22:57:40 GMT"}, {"version": "v3", "created": "Thu, 23 Apr 2020 03:02:12 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["Luo", "Jie", ""], ["Sedghi", "Alireza", ""], ["Popuri", "Karteek", ""], ["Cobzas", "Dana", ""], ["Zhang", "Miaomiao", ""], ["Preiswerk", "Frank", ""], ["Toews", "Matthew", ""], ["Golby", "Alexandra", ""], ["Sugiyama", "Masashi", ""], ["Wells", "William M.", "III"], ["Frisken", "Sarah", ""]]}, {"id": "1803.05268", "submitter": "David Mascharka", "authors": "David Mascharka, Philip Tran, Ryan Soklaski, Arjun Majumdar", "title": "Transparency by Design: Closing the Gap Between Performance and\n  Interpretability in Visual Reasoning", "comments": "CVPR 2018 pre-print", "journal-ref": null, "doi": "10.1109/CVPR.2018.00519", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual question answering requires high-order reasoning about an image, which\nis a fundamental capability needed by machine systems to follow complex\ndirectives. Recently, modular networks have been shown to be an effective\nframework for performing visual reasoning tasks. While modular networks were\ninitially designed with a degree of model transparency, their performance on\ncomplex visual reasoning benchmarks was lacking. Current state-of-the-art\napproaches do not provide an effective mechanism for understanding the\nreasoning process. In this paper, we close the performance gap between\ninterpretable models and state-of-the-art visual reasoning methods. We propose\na set of visual-reasoning primitives which, when composed, manifest as a model\ncapable of performing complex reasoning tasks in an explicitly-interpretable\nmanner. The fidelity and interpretability of the primitives' outputs enable an\nunparalleled ability to diagnose the strengths and weaknesses of the resulting\nmodel. Critically, we show that these primitives are highly performant,\nachieving state-of-the-art accuracy of 99.1% on the CLEVR dataset. We also show\nthat our model is able to effectively learn generalized representations when\nprovided a small amount of data containing novel object attributes. Using the\nCoGenT generalization task, we show more than a 20 percentage point improvement\nover the current state of the art.\n", "versions": [{"version": "v1", "created": "Wed, 14 Mar 2018 13:33:06 GMT"}, {"version": "v2", "created": "Mon, 2 Jul 2018 18:48:31 GMT"}], "update_date": "2019-01-24", "authors_parsed": [["Mascharka", "David", ""], ["Tran", "Philip", ""], ["Soklaski", "Ryan", ""], ["Majumdar", "Arjun", ""]]}, {"id": "1803.05347", "submitter": "Chengyang Li", "authors": "Chengyang Li, Dan Song, Ruofeng Tong, Min Tang", "title": "Illumination-aware Faster R-CNN for Robust Multispectral Pedestrian\n  Detection", "comments": "Accepted for Publication in Pattern Recognition", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multispectral images of color-thermal pairs have shown more effective than a\nsingle color channel for pedestrian detection, especially under challenging\nillumination conditions. However, there is still a lack of studies on how to\nfuse the two modalities effectively. In this paper, we deeply compare six\ndifferent convolutional network fusion architectures and analyse their\nadaptations, enabling a vanilla architecture to obtain detection performances\ncomparable to the state-of-the-art results. Further, we discover that\npedestrian detection confidences from color or thermal images are correlated\nwith illumination conditions. With this in mind, we propose an\nIllumination-aware Faster R-CNN (IAF RCNN). Specifically, an Illumination-aware\nNetwork is introduced to give an illumination measure of the input image. Then\nwe adaptively merge color and thermal sub-networks via a gate function defined\nover the illumination value. The experimental results on KAIST Multispectral\nPedestrian Benchmark validate the effectiveness of the proposed IAF R-CNN.\n", "versions": [{"version": "v1", "created": "Wed, 14 Mar 2018 15:15:58 GMT"}, {"version": "v2", "created": "Tue, 14 Aug 2018 17:34:09 GMT"}], "update_date": "2018-08-15", "authors_parsed": [["Li", "Chengyang", ""], ["Song", "Dan", ""], ["Tong", "Ruofeng", ""], ["Tang", "Min", ""]]}, {"id": "1803.05387", "submitter": "Thomas Ciarfuglia", "authors": "Gabriele Costante and Thomas A. Ciarfuglia and Filippo Biondi", "title": "Towards Monocular Digital Elevation Model (DEM) Estimation by\n  Convolutional Neural Networks - Application on Synthetic Aperture Radar\n  Images", "comments": "Accepted for publication in Proceedings of the 12th European\n  Conference on Synthetic Aperture Radar", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Synthetic aperture radar (SAR) interferometry (InSAR) is performed using\nrepeat-pass geometry. InSAR technique is used to estimate the topographic\nreconstruction of the earth surface. The main problem of the range-Doppler\nfocusing technique is the nature of the two-dimensional SAR result, affected by\nthe layover indetermination. In order to resolve this problem, a minimum of two\nsensor acquisitions, separated by a baseline and extended in the\ncross-slant-range, are needed. However, given its multi-temporal nature, these\ntechniques are vulnerable to atmosphere and Earth environment parameters\nvariation in addition to physical platform instabilities. Furthermore, either\ntwo radars are needed or an interferometric cycle is required (that spans from\ndays to weeks), which makes real time DEM estimation impossible. In this work,\nthe authors propose a novel experimental alternative to the InSAR method that\nuses single-pass acquisitions, using a data driven approach implemented by Deep\nNeural Networks. We propose a fully Convolutional Neural Network (CNN)\nEncoder-Decoder architecture, training it on radar images in order to estimate\nDEMs from single pass image acquisitions. Our results on a set of Sentinel\nimages show that this method is able to learn to some extent the statistical\nproperties of the DEM. The results of this exploratory analysis are encouraging\nand open the way to the solution of single-pass DEM estimation problem with\ndata driven approaches.\n", "versions": [{"version": "v1", "created": "Wed, 14 Mar 2018 16:32:18 GMT"}], "update_date": "2018-03-15", "authors_parsed": [["Costante", "Gabriele", ""], ["Ciarfuglia", "Thomas A.", ""], ["Biondi", "Filippo", ""]]}, {"id": "1803.05400", "submitter": "Kamyar Nazeri", "authors": "Kamyar Nazeri, Eric Ng, Mehran Ebrahimi", "title": "Image Colorization with Generative Adversarial Networks", "comments": "Lecture Notes in Computer Science, Proceedings of tenth international\n  conference on Articulated Motion and Deformable Objects (AMDO), Palma,\n  Mallorca, Spain, 12-13 July 2018", "journal-ref": "LNCS 10945 (2018) 85-94", "doi": "10.1007/978-3-319-94544-6_9", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Over the last decade, the process of automatic image colorization has been of\nsignificant interest for several application areas including restoration of\naged or degraded images. This problem is highly ill-posed due to the large\ndegrees of freedom during the assignment of color information. Many of the\nrecent developments in automatic colorization involve images that contain a\ncommon theme or require highly processed data such as semantic maps as input.\nIn our approach, we attempt to fully generalize the colorization procedure\nusing a conditional Deep Convolutional Generative Adversarial Network (DCGAN),\nextend current methods to high-resolution images and suggest training\nstrategies that speed up the process and greatly stabilize it. The network is\ntrained over datasets that are publicly available such as CIFAR-10 and\nPlaces365. The results of the generative model and traditional deep neural\nnetworks are compared.\n", "versions": [{"version": "v1", "created": "Wed, 14 Mar 2018 16:53:03 GMT"}, {"version": "v2", "created": "Sun, 1 Apr 2018 16:50:06 GMT"}, {"version": "v3", "created": "Sun, 29 Apr 2018 18:00:50 GMT"}, {"version": "v4", "created": "Fri, 11 May 2018 18:25:47 GMT"}, {"version": "v5", "created": "Wed, 16 May 2018 12:52:00 GMT"}], "update_date": "2018-07-16", "authors_parsed": [["Nazeri", "Kamyar", ""], ["Ng", "Eric", ""], ["Ebrahimi", "Mehran", ""]]}, {"id": "1803.05401", "submitter": "Abhijit Suprem", "authors": "Abhijit Suprem and Polo Chau", "title": "Approximate Query Matching for Image Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional image recognition involves identifying the key object in a\nportrait-type image with a single object focus (ILSVRC, AlexNet, and VGG). More\nrecent approaches consider dense image recognition - segmenting an image with\nappropriate bounding boxes and performing image recognition within these\nbounding boxes (Semantic segmentation). The Visual Genome dataset [5] is an\nattempt to bridge these various approaches to a cohesive dataset for each\nsubtask - bounding box generation, image recognition, captioning, and a new\noperation: scene graph generation. Our focus is on using such scene graphs to\nperform graph search on image databases to holistically retrieve images based\non a search criteria. We develop a method to store scene graphs and metadata in\ngraph databases (using Neo4J) and to perform fast approximate retrieval of\nimages based on a graph search query. We process more complex queries than\nsingle object search, e.g. \"girl eating cake\" retrieves images that contain the\nspecified relation as well as variations.\n", "versions": [{"version": "v1", "created": "Wed, 14 Mar 2018 16:57:50 GMT"}], "update_date": "2018-03-15", "authors_parsed": [["Suprem", "Abhijit", ""], ["Chau", "Polo", ""]]}, {"id": "1803.05407", "submitter": "Andrew Wilson", "authors": "Pavel Izmailov, Dmitrii Podoprikhin, Timur Garipov, Dmitry Vetrov,\n  Andrew Gordon Wilson", "title": "Averaging Weights Leads to Wider Optima and Better Generalization", "comments": "Appears at the Conference on Uncertainty in Artificial Intelligence\n  (UAI), 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks are typically trained by optimizing a loss function with\nan SGD variant, in conjunction with a decaying learning rate, until\nconvergence. We show that simple averaging of multiple points along the\ntrajectory of SGD, with a cyclical or constant learning rate, leads to better\ngeneralization than conventional training. We also show that this Stochastic\nWeight Averaging (SWA) procedure finds much flatter solutions than SGD, and\napproximates the recent Fast Geometric Ensembling (FGE) approach with a single\nmodel. Using SWA we achieve notable improvement in test accuracy over\nconventional SGD training on a range of state-of-the-art residual networks,\nPyramidNets, DenseNets, and Shake-Shake networks on CIFAR-10, CIFAR-100, and\nImageNet. In short, SWA is extremely easy to implement, improves\ngeneralization, and has almost no computational overhead.\n", "versions": [{"version": "v1", "created": "Wed, 14 Mar 2018 17:09:27 GMT"}, {"version": "v2", "created": "Wed, 8 Aug 2018 08:49:15 GMT"}, {"version": "v3", "created": "Mon, 25 Feb 2019 14:18:11 GMT"}], "update_date": "2019-02-26", "authors_parsed": [["Izmailov", "Pavel", ""], ["Podoprikhin", "Dmitrii", ""], ["Garipov", "Timur", ""], ["Vetrov", "Dmitry", ""], ["Wilson", "Andrew Gordon", ""]]}, {"id": "1803.05431", "submitter": "Holger Roth", "authors": "Holger R. Roth, Hirohisa Oda, Xiangrong Zhou, Natsuki Shimizu, Ying\n  Yang, Yuichiro Hayashi, Masahiro Oda, Michitaka Fujiwara, Kazunari Misawa,\n  Kensaku Mori", "title": "An application of cascaded 3D fully convolutional networks for medical\n  image segmentation", "comments": "Preprint accepted for publication in Computerized Medical Imaging and\n  Graphics. Substantial extension of arXiv:1704.06382; Corrected references to\n  figure numbers in this version", "journal-ref": "Computerized Medical Imaging and Graphics, Elsevier, Volume 66,\n  June 2018, Pages 90-99", "doi": "10.1016/j.compmedimag.2018.03.001", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in 3D fully convolutional networks (FCN) have made it\nfeasible to produce dense voxel-wise predictions of volumetric images. In this\nwork, we show that a multi-class 3D FCN trained on manually labeled CT scans of\nseveral anatomical structures (ranging from the large organs to thin vessels)\ncan achieve competitive segmentation results, while avoiding the need for\nhandcrafting features or training class-specific models.\n  To this end, we propose a two-stage, coarse-to-fine approach that will first\nuse a 3D FCN to roughly define a candidate region, which will then be used as\ninput to a second 3D FCN. This reduces the number of voxels the second FCN has\nto classify to ~10% and allows it to focus on more detailed segmentation of the\norgans and vessels.\n  We utilize training and validation sets consisting of 331 clinical CT images\nand test our models on a completely unseen data collection acquired at a\ndifferent hospital that includes 150 CT scans, targeting three anatomical\norgans (liver, spleen, and pancreas). In challenging organs such as the\npancreas, our cascaded approach improves the mean Dice score from 68.5 to\n82.2%, achieving the highest reported average score on this dataset. We compare\nwith a 2D FCN method on a separate dataset of 240 CT scans with 18 classes and\nachieve a significantly higher performance in small organs and vessels.\nFurthermore, we explore fine-tuning our models to different datasets.\n  Our experiments illustrate the promise and robustness of current 3D FCN based\nsemantic segmentation of medical images, achieving state-of-the-art results.\nOur code and trained models are available for download:\nhttps://github.com/holgerroth/3Dunet_abdomen_cascade.\n", "versions": [{"version": "v1", "created": "Wed, 14 Mar 2018 07:20:37 GMT"}, {"version": "v2", "created": "Tue, 20 Mar 2018 05:22:07 GMT"}], "update_date": "2018-04-13", "authors_parsed": [["Roth", "Holger R.", ""], ["Oda", "Hirohisa", ""], ["Zhou", "Xiangrong", ""], ["Shimizu", "Natsuki", ""], ["Yang", "Ying", ""], ["Hayashi", "Yuichiro", ""], ["Oda", "Masahiro", ""], ["Fujiwara", "Michitaka", ""], ["Misawa", "Kazunari", ""], ["Mori", "Kensaku", ""]]}, {"id": "1803.05471", "submitter": "Zhang Li", "authors": "Zhang Li, Zheyu Hu, Jiaolong Xu, Tao Tan, Hui Chen, Zhi Duan, Ping\n  Liu, Jun Tang, Guoping Cai, Quchang Ouyang, Yuling Tang, Geert Litjens, Qiang\n  Li", "title": "Computer-aided diagnosis of lung carcinoma using deep learning - a pilot\n  study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aim: Early detection and correct diagnosis of lung cancer are the most\nimportant steps in improving patient outcome. This study aims to assess which\ndeep learning models perform best in lung cancer diagnosis. Methods: Non-small\ncell lung carcinoma and small cell lung carcinoma biopsy specimens were\nconsecutively obtained and stained. The specimen slides were diagnosed by two\nexperienced pathologists (over 20 years). Several deep learning models were\ntrained to discriminate cancer and non-cancer biopsies. Result: Deep learning\nmodels give reasonable AUC from 0.8810 to 0.9119. Conclusion: The deep learning\nanalysis could help to speed up the detection process for the whole-slide image\n(WSI) and keep the comparable detection rate with human observer.\n", "versions": [{"version": "v1", "created": "Wed, 14 Mar 2018 18:46:17 GMT"}], "update_date": "2018-03-16", "authors_parsed": [["Li", "Zhang", ""], ["Hu", "Zheyu", ""], ["Xu", "Jiaolong", ""], ["Tan", "Tao", ""], ["Chen", "Hui", ""], ["Duan", "Zhi", ""], ["Liu", "Ping", ""], ["Tang", "Jun", ""], ["Cai", "Guoping", ""], ["Ouyang", "Quchang", ""], ["Tang", "Yuling", ""], ["Litjens", "Geert", ""], ["Li", "Qiang", ""]]}, {"id": "1803.05482", "submitter": "Evgeny Burnaev", "authors": "Vladimir Ignatiev, Alexey Trekin, Viktor Lobachev, Georgy Potapov,\n  Evgeny Burnaev", "title": "Targeted change detection in remote sensing images", "comments": "10 pages, 1 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CE eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent developments in the remote sensing systems and image processing made\nit possible to propose a new method of the object classification and detection\nof the specific changes in the series of satellite Earth images (so called\ntargeted change detection). In this paper we propose a formal problem statement\nthat allows to use effectively the deep learning approach to analyze\ntime-dependent series of remote sensing images. We also introduce a new\nframework for the development of deep learning models for targeted change\ndetection and demonstrate some cases of business applications it can be used\nfor.\n", "versions": [{"version": "v1", "created": "Wed, 14 Mar 2018 19:09:24 GMT"}], "update_date": "2018-03-16", "authors_parsed": [["Ignatiev", "Vladimir", ""], ["Trekin", "Alexey", ""], ["Lobachev", "Viktor", ""], ["Potapov", "Georgy", ""], ["Burnaev", "Evgeny", ""]]}, {"id": "1803.05494", "submitter": "Shubhra Aich", "authors": "Shubhra Aich and Ian Stavness", "title": "Improving Object Counting with Heatmap Regulation", "comments": "Code repository: https://github.com/littleaich/heatmap-regulation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a simple and effective way to improve one-look\nregression models for object counting from images. We use class activation map\nvisualizations to illustrate the drawbacks of learning a pure one-look\nregression model for a counting task. Based on these insights, we enhance\none-look regression counting models by regulating activation maps from the\nfinal convolution layer of the network with coarse ground-truth activation maps\ngenerated from simple dot annotations. We call this strategy heatmap regulation\n(HR). We show that this simple enhancement effectively suppresses false\ndetections generated by the corresponding one-look baseline model and also\nimproves the performance in terms of false negatives. Evaluations are performed\non four different counting datasets --- two for car counting (CARPK, PUCPR+),\none for crowd counting (WorldExpo) and another for biological cell counting\n(VGG-Cells). Adding HR to a simple VGG front-end improves performance on all\nthese benchmarks compared to a simple one-look baseline model and results in\nstate-of-the-art performance for car counting.\n", "versions": [{"version": "v1", "created": "Wed, 14 Mar 2018 19:52:43 GMT"}, {"version": "v2", "created": "Wed, 23 May 2018 21:43:47 GMT"}], "update_date": "2018-05-25", "authors_parsed": [["Aich", "Shubhra", ""], ["Stavness", "Ian", ""]]}, {"id": "1803.05526", "submitter": "Jiuxiang Gu Mr", "authors": "Jiuxiang Gu, Shafiq Joty, Jianfei Cai, Gang Wang", "title": "Unpaired Image Captioning by Language Pivoting", "comments": "17 pages, 4 figures, Accepted at ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image captioning is a multimodal task involving computer vision and natural\nlanguage processing, where the goal is to learn a mapping from the image to its\nnatural language description. In general, the mapping function is learned from\na training set of image-caption pairs. However, for some language, large scale\nimage-caption paired corpus might not be available. We present an approach to\nthis unpaired image captioning problem by language pivoting. Our method can\neffectively capture the characteristics of an image captioner from the pivot\nlanguage (Chinese) and align it to the target language (English) using another\npivot-target (Chinese-English) sentence parallel corpus. We evaluate our method\non two image-to-English benchmark datasets: MSCOCO and Flickr30K. Quantitative\ncomparisons against several baseline approaches demonstrate the effectiveness\nof our method.\n", "versions": [{"version": "v1", "created": "Wed, 14 Mar 2018 22:41:35 GMT"}, {"version": "v2", "created": "Wed, 18 Jul 2018 08:19:19 GMT"}], "update_date": "2018-07-19", "authors_parsed": [["Gu", "Jiuxiang", ""], ["Joty", "Shafiq", ""], ["Cai", "Jianfei", ""], ["Wang", "Gang", ""]]}, {"id": "1803.05530", "submitter": "Long Chen", "authors": "Long Chen and Wen Tang and Nigel John", "title": "Self-Supervised Monocular Image Depth Learning and Confidence Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNNs) need large amounts of data with ground\ntruth annotation, which is a challenging problem that has limited the\ndevelopment and fast deployment of CNNs for many computer vision tasks. We\npropose a novel framework for depth estimation from monocular images with\ncorresponding confidence in a self-supervised manner. A fully differential\npatch-based cost function is proposed by using the Zero-Mean Normalized Cross\nCorrelation (ZNCC) that takes multi-scale patches as a matching strategy. This\napproach greatly increases the accuracy and robustness of the depth learning.\nIn addition, the proposed patch-based cost function can provide a 0 to 1\nconfidence, which is then used to supervise the training of a parallel network\nfor confidence map learning and estimation. Evaluation on KITTI dataset shows\nthat our method outperforms the state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Wed, 14 Mar 2018 22:59:01 GMT"}], "update_date": "2018-03-16", "authors_parsed": [["Chen", "Long", ""], ["Tang", "Wen", ""], ["John", "Nigel", ""]]}, {"id": "1803.05536", "submitter": "Zhenhua Feng", "authors": "Zhen-Hua Feng, Patrik Huber, Josef Kittler, Peter JB Hancock, Xiao-Jun\n  Wu, Qijun Zhao, Paul Koppen, Matthias R\\\"atsch", "title": "Evaluation of Dense 3D Reconstruction from 2D Face Images in the Wild", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper investigates the evaluation of dense 3D face reconstruction from a\nsingle 2D image in the wild. To this end, we organise a competition that\nprovides a new benchmark dataset that contains 2000 2D facial images of 135\nsubjects as well as their 3D ground truth face scans. In contrast to previous\ncompetitions or challenges, the aim of this new benchmark dataset is to\nevaluate the accuracy of a 3D dense face reconstruction algorithm using real,\naccurate and high-resolution 3D ground truth face scans. In addition to the\ndataset, we provide a standard protocol as well as a Python script for the\nevaluation. Last, we report the results obtained by three state-of-the-art 3D\nface reconstruction systems on the new benchmark dataset. The competition is\norganised along with the 2018 13th IEEE Conference on Automatic Face & Gesture\nRecognition.\n", "versions": [{"version": "v1", "created": "Wed, 14 Mar 2018 23:12:12 GMT"}, {"version": "v2", "created": "Fri, 20 Apr 2018 23:08:42 GMT"}], "update_date": "2018-04-24", "authors_parsed": [["Feng", "Zhen-Hua", ""], ["Huber", "Patrik", ""], ["Kittler", "Josef", ""], ["Hancock", "Peter JB", ""], ["Wu", "Xiao-Jun", ""], ["Zhao", "Qijun", ""], ["Koppen", "Paul", ""], ["R\u00e4tsch", "Matthias", ""]]}, {"id": "1803.05541", "submitter": "Long Chen", "authors": "Long Chen, Wen Tang, Nigel John, Tao Ruan Wan, Jian Jun Zhang", "title": "Context-Aware Mixed Reality: A Framework for Ubiquitous Interaction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixed Reality (MR) is a powerful interactive technology that yields new types\nof user experience. We present a semantic based interactive MR framework that\nexceeds the current geometry level approaches, a step change in generating\nhigh-level context-aware interactions. Our key insight is to build semantic\nunderstanding in MR that not only can greatly enhance user experience through\nobject-specific behaviours, but also pave the way for solving complex\ninteraction design challenges. The framework generates semantic properties of\nthe real world environment through dense scene reconstruction and deep image\nunderstanding. We demonstrate our approach with a material-aware prototype\nsystem for generating context-aware physical interactions between the real and\nthe virtual objects. Quantitative and qualitative evaluations are carried out\nand the results show that the framework delivers accurate and fast semantic\ninformation in interactive MR environment, providing effective semantic level\ninteractions.\n", "versions": [{"version": "v1", "created": "Wed, 14 Mar 2018 23:38:54 GMT"}], "update_date": "2018-03-16", "authors_parsed": [["Chen", "Long", ""], ["Tang", "Wen", ""], ["John", "Nigel", ""], ["Wan", "Tao Ruan", ""], ["Zhang", "Jian Jun", ""]]}, {"id": "1803.05549", "submitter": "Gedas Bertasius", "authors": "Gedas Bertasius, Lorenzo Torresani, and Jianbo Shi", "title": "Object Detection in Video with Spatiotemporal Sampling Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a Spatiotemporal Sampling Network (STSN) that uses deformable\nconvolutions across time for object detection in videos. Our STSN performs\nobject detection in a video frame by learning to spatially sample features from\nthe adjacent frames. This naturally renders the approach robust to occlusion or\nmotion blur in individual frames. Our framework does not require additional\nsupervision, as it optimizes sampling locations directly with respect to object\ndetection performance. Our STSN outperforms the state-of-the-art on the\nImageNet VID dataset and compared to prior video object detection methods it\nuses a simpler design, and does not require optical flow data for training.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2018 00:23:22 GMT"}, {"version": "v2", "created": "Tue, 24 Jul 2018 04:34:29 GMT"}], "update_date": "2018-07-25", "authors_parsed": [["Bertasius", "Gedas", ""], ["Torresani", "Lorenzo", ""], ["Shi", "Jianbo", ""]]}, {"id": "1803.05576", "submitter": "Ying-Cong Chen", "authors": "Ying-Cong Chen, Huaijia Lin, Michelle Shu, Ruiyu Li, Xin Tao, Yangang\n  Ye, Xiaoyong Shen, Jiaya Jia", "title": "Facelet-Bank for Fast Portrait Manipulation", "comments": "Accepted by CVPR 2018. Code is available on\n  https://github.com/yingcong/Facelet_Bank", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Digital face manipulation has become a popular and fascinating way to touch\nimages with the prevalence of smartphones and social networks. With a wide\nvariety of user preferences, facial expressions, and accessories, a general and\nflexible model is necessary to accommodate different types of facial editing.\nIn this paper, we propose a model to achieve this goal based on an end-to-end\nconvolutional neural network that supports fast inference, edit-effect control,\nand quick partial-model update. In addition, this model learns from unpaired\nimage sets with different attributes. Experimental results show that our\nframework can handle a wide range of expressions, accessories, and makeup\neffects. It produces high-resolution and high-quality results in fast speed.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2018 02:48:55 GMT"}, {"version": "v2", "created": "Fri, 23 Mar 2018 13:38:51 GMT"}, {"version": "v3", "created": "Fri, 30 Mar 2018 08:42:55 GMT"}], "update_date": "2018-04-02", "authors_parsed": [["Chen", "Ying-Cong", ""], ["Lin", "Huaijia", ""], ["Shu", "Michelle", ""], ["Li", "Ruiyu", ""], ["Tao", "Xin", ""], ["Ye", "Yangang", ""], ["Shen", "Xiaoyong", ""], ["Jia", "Jiaya", ""]]}, {"id": "1803.05588", "submitter": "Zhiwen Shao", "authors": "Zhiwen Shao, Zhilei Liu, Jianfei Cai, Lizhuang Ma", "title": "Deep Adaptive Attention for Joint Facial Action Unit Detection and Face\n  Alignment", "comments": "This paper has been accepted by ECCV 2018", "journal-ref": null, "doi": "10.1007/978-3-030-01261-8_43", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial action unit (AU) detection and face alignment are two highly\ncorrelated tasks since facial landmarks can provide precise AU locations to\nfacilitate the extraction of meaningful local features for AU detection. Most\nexisting AU detection works often treat face alignment as a preprocessing and\nhandle the two tasks independently. In this paper, we propose a novel\nend-to-end deep learning framework for joint AU detection and face alignment,\nwhich has not been explored before. In particular, multi-scale shared features\nare learned firstly, and high-level features of face alignment are fed into AU\ndetection. Moreover, to extract precise local features, we propose an adaptive\nattention learning module to refine the attention map of each AU adaptively.\nFinally, the assembled local features are integrated with face alignment\nfeatures and global features for AU detection. Experiments on BP4D and DISFA\nbenchmarks demonstrate that our framework significantly outperforms the\nstate-of-the-art methods for AU detection.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2018 04:24:02 GMT"}, {"version": "v2", "created": "Tue, 24 Jul 2018 09:05:07 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Shao", "Zhiwen", ""], ["Liu", "Zhilei", ""], ["Cai", "Jianfei", ""], ["Ma", "Lizhuang", ""]]}, {"id": "1803.05619", "submitter": "Huikai Wu", "authors": "Huikai Wu, Shuai Zheng, Junge Zhang and Kaiqi Huang", "title": "Fast End-to-End Trainable Guided Filter", "comments": "Accepted by CVPR 2018. GitHub:\n  https://github.com/wuhuikai/DeepGuidedFilter", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Dense pixel-wise image prediction has been advanced by harnessing the\ncapabilities of Fully Convolutional Networks (FCNs). One central issue of FCNs\nis the limited capacity to handle joint upsampling. To address the problem, we\npresent a novel building block for FCNs, namely guided filtering layer, which\nis designed for efficiently generating a high-resolution output given the\ncorresponding low-resolution one and a high-resolution guidance map. Such a\nlayer contains learnable parameters, which can be integrated with FCNs and\njointly optimized through end-to-end training. To further take advantage of\nend-to-end training, we plug in a trainable transformation function for\ngenerating the task-specific guidance map. Based on the proposed layer, we\npresent a general framework for pixel-wise image prediction, named deep guided\nfiltering network (DGF). The proposed network is evaluated on five image\nprocessing tasks. Experiments on MIT-Adobe FiveK Dataset demonstrate that DGF\nruns 10-100 times faster and achieves the state-of-the-art performance. We also\nshow that DGF helps to improve the performance of multiple computer vision\ntasks.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2018 07:31:24 GMT"}, {"version": "v2", "created": "Wed, 25 Sep 2019 07:58:10 GMT"}], "update_date": "2019-09-26", "authors_parsed": [["Wu", "Huikai", ""], ["Zheng", "Shuai", ""], ["Zhang", "Junge", ""], ["Huang", "Kaiqi", ""]]}, {"id": "1803.05648", "submitter": "Zhenheng Yang", "authors": "Zhenheng Yang, Peng Wang, Yang Wang, Wei Xu, Ram Nevatia", "title": "LEGO: Learning Edge with Geometry all at Once by Watching Videos", "comments": "Accepted to CVPR 2018 as spotlight; Camera ready plus supplementary\n  material. Code will come", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning to estimate 3D geometry in a single image by watching unlabeled\nvideos via deep convolutional network is attracting significant attention. In\nthis paper, we introduce a \"3D as-smooth-as-possible (3D-ASAP)\" prior inside\nthe pipeline, which enables joint estimation of edges and 3D scene, yielding\nresults with significant improvement in accuracy for fine detailed structures.\nSpecifically, we define the 3D-ASAP prior by requiring that any two points\nrecovered in 3D from an image should lie on an existing planar surface if no\nother cues provided. We design an unsupervised framework that Learns Edges and\nGeometry (depth, normal) all at Once (LEGO). The predicted edges are embedded\ninto depth and surface normal smoothness terms, where pixels without edges\nin-between are constrained to satisfy the prior. In our framework, the\npredicted depths, normals and edges are forced to be consistent all the time.\nWe conduct experiments on KITTI to evaluate our estimated geometry and\nCityScapes to perform edge evaluation. We show that in all of the tasks,\ni.e.depth, normal and edge, our algorithm vastly outperforms other\nstate-of-the-art (SOTA) algorithms, demonstrating the benefits of our approach.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2018 09:14:11 GMT"}, {"version": "v2", "created": "Sat, 24 Mar 2018 00:31:11 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Yang", "Zhenheng", ""], ["Wang", "Peng", ""], ["Wang", "Yang", ""], ["Xu", "Wei", ""], ["Nevatia", "Ram", ""]]}, {"id": "1803.05657", "submitter": "Lei Zhou", "authors": "Lei Zhou, Xiao Bai, Xianglong Liu, Jun Zhou and Hancock Edwin", "title": "Fast Subspace Clustering Based on the Kronecker Product", "comments": "16 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Subspace clustering is a useful technique for many computer vision\napplications in which the intrinsic dimension of high-dimensional data is often\nsmaller than the ambient dimension. Spectral clustering, as one of the main\napproaches to subspace clustering, often takes on a sparse representation or a\nlow-rank representation to learn a block diagonal self-representation matrix\nfor subspace generation. However, existing methods require solving a large\nscale convex optimization problem with a large set of data, with computational\ncomplexity reaches O(N^3) for N data points. Therefore, the efficiency and\nscalability of traditional spectral clustering methods can not be guaranteed\nfor large scale datasets. In this paper, we propose a subspace clustering model\nbased on the Kronecker product. Due to the property that the Kronecker product\nof a block diagonal matrix with any other matrix is still a block diagonal\nmatrix, we can efficiently learn the representation matrix which is formed by\nthe Kronecker product of k smaller matrices. By doing so, our model\nsignificantly reduces the computational complexity to O(kN^{3/k}). Furthermore,\nour model is general in nature, and can be adapted to different regularization\nbased subspace clustering methods. Experimental results on two public datasets\nshow that our model significantly improves the efficiency compared with several\nstate-of-the-art methods. Moreover, we have conducted experiments on synthetic\ndata to verify the scalability of our model for large scale datasets.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2018 09:31:44 GMT"}], "update_date": "2018-03-16", "authors_parsed": [["Zhou", "Lei", ""], ["Bai", "Xiao", ""], ["Liu", "Xianglong", ""], ["Zhou", "Jun", ""], ["Edwin", "Hancock", ""]]}, {"id": "1803.05675", "submitter": "Panagiotis Meletis", "authors": "Panagiotis Meletis, Gijs Dubbelman", "title": "Training of Convolutional Networks on Multiple Heterogeneous Datasets\n  for Street Scene Semantic Segmentation", "comments": "IEEE Intelligent Vehicles 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a convolutional network with hierarchical classifiers for\nper-pixel semantic segmentation, which is able to be trained on multiple,\nheterogeneous datasets and exploit their semantic hierarchy. Our network is the\nfirst to be simultaneously trained on three different datasets from the\nintelligent vehicles domain, i.e. Cityscapes, GTSDB and Mapillary Vistas, and\nis able to handle different semantic level-of-detail, class imbalances, and\ndifferent annotation types, i.e. dense per-pixel and sparse bounding-box\nlabels. We assess our hierarchical approach, by comparing against flat,\nnon-hierarchical classifiers and we show improvements in mean pixel accuracy of\n13.0% for Cityscapes classes and 2.4% for Vistas classes and 32.3% for GTSDB\nclasses. Our implementation achieves inference rates of 17 fps at a resolution\nof 520x706 for 108 classes running on a GPU.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2018 10:24:43 GMT"}, {"version": "v2", "created": "Sun, 8 Jul 2018 14:04:13 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["Meletis", "Panagiotis", ""], ["Dubbelman", "Gijs", ""]]}, {"id": "1803.05719", "submitter": "Vandit Gajjar", "authors": "Ayesha Gurnani, Kenil Shah, Vandit Gajjar, Viraj Mavani, Yash\n  Khandhediya", "title": "SAF- BAGE: Salient Approach for Facial Soft-Biometric Classification -\n  Age, Gender, and Facial Expression", "comments": "9 Pages, 4 Figures, 6 Tables; Accepted for publication in IEEE Winter\n  Conference on Application of Computer Vision (WACV) 2019, Hawaii, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How can we improve the facial soft-biometric classification with help of the\nhuman visual system? This paper explores the use of saliency which is\nequivalent to the human visual system to classify Age, Gender and Facial\nExpression soft-biometric for facial images. Using the Deep Multi-level Network\n(ML-Net) [1] and off-the-shelf face detector [2], we propose our approach -\nSAF-BAGE, which first detects the face in the test image, increases the\nBounding Box (B-Box) margin by 30%, finds the saliency map using ML-Net, with\n30% reweighted ratio of saliency map, it multiplies with the input cropped face\nand extracts the Convolutional Neural Networks (CNN) predictions on the\nmultiplied reweighted salient face. Our CNN uses the model AlexNet [3], which\nis pre-trained on ImageNet. The proposed approach surpasses the performance of\nother approaches, increasing the state-of-the-art by approximately 0.8% on the\nwidely-used Adience [28] dataset for Age and Gender classification and by\nnearly 3% on the recent AffectNet [36] dataset for Facial Expression\nclassification. We hope our simple, reproducible and effective approach will\nhelp ease future research in facial soft-biometric classification using\nsaliency.\n", "versions": [{"version": "v1", "created": "Tue, 13 Mar 2018 21:11:11 GMT"}, {"version": "v2", "created": "Wed, 21 Nov 2018 07:50:13 GMT"}], "update_date": "2018-11-22", "authors_parsed": [["Gurnani", "Ayesha", ""], ["Shah", "Kenil", ""], ["Gajjar", "Vandit", ""], ["Mavani", "Viraj", ""], ["Khandhediya", "Yash", ""]]}, {"id": "1803.05729", "submitter": "Dong Wang", "authors": "Dong Wang, Lei Zhou, Xueni Zhang, Xiao Bai and Jun Zhou", "title": "Exploring Linear Relationship in Feature Map Subspace for ConvNets\n  Compression", "comments": "17 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While the research on convolutional neural networks (CNNs) is progressing\nquickly, the real-world deployment of these models is often limited by\ncomputing resources and memory constraints. In this paper, we address this\nissue by proposing a novel filter pruning method to compress and accelerate\nCNNs. Our work is based on the linear relationship identified in different\nfeature map subspaces via visualization of feature maps. Such linear\nrelationship implies that the information in CNNs is redundant. Our method\neliminates the redundancy in convolutional filters by applying subspace\nclustering to feature maps. In this way, most of the representative information\nin the network can be retained in each cluster. Therefore, our method provides\nan effective solution to filter pruning for which most existing methods\ndirectly remove filters based on simple heuristics. The proposed method is\nindependent of the network structure, thus it can be adopted by any\noff-the-shelf deep learning libraries. Experiments on different networks and\ntasks show that our method outperforms existing techniques before fine-tuning,\nand achieves the state-of-the-art results after fine-tuning.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2018 13:20:16 GMT"}], "update_date": "2018-03-16", "authors_parsed": [["Wang", "Dong", ""], ["Zhou", "Lei", ""], ["Zhang", "Xueni", ""], ["Bai", "Xiao", ""], ["Zhou", "Jun", ""]]}, {"id": "1803.05748", "submitter": "Carsten Haubold", "authors": "Carsten Haubold, Virginie Uhlmann, Michael Unser, Fred A. Hamprecht", "title": "Diverse M-Best Solutions by Dynamic Programming", "comments": "Includes supplementary and corrigendum", "journal-ref": "Haubold, C., Uhlmann, V., Unser, M., Hamprecht, F. A.: Diverse\n  M-best solutions by dynamic programming. In: Roth, V., Vetter, T. (eds.)\n  Pattern Recognition. GCPR 2017. vol 10496, pp. 255-267. Springer, Cham (2017)", "doi": "10.1007/978-3-319-66709-6_21", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many computer vision pipelines involve dynamic programming primitives such as\nfinding a shortest path or the minimum energy solution in a tree-shaped\nprobabilistic graphical model. In such cases, extracting not merely the best,\nbut the set of M-best solutions is useful to generate a rich collection of\ncandidate proposals that can be used in downstream processing. In this work, we\nshow how M-best solutions of tree-shaped graphical models can be obtained by\ndynamic programming on a special graph with M layers. The proposed multi-layer\nconcept is optimal for searching M-best solutions, and so flexible that it can\nalso approximate M-best diverse solutions. We illustrate the usefulness with\napplications to object detection, panorama stitching and centerline extraction.\n  Note: We have observed that an assumption in section 4 of our paper is not\nalways fulfilled, see the attached corrigendum for details.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2018 13:52:01 GMT"}], "update_date": "2018-03-16", "authors_parsed": [["Haubold", "Carsten", ""], ["Uhlmann", "Virginie", ""], ["Unser", "Michael", ""], ["Hamprecht", "Fred A.", ""]]}, {"id": "1803.05753", "submitter": "Sen He", "authors": "Sen He, Ali Borji, Yang Mi, Nicolas Pugeault", "title": "What Catches the Eye? Visualizing and Understanding Deep Saliency Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep convolutional neural networks have demonstrated high performances for\nfixation prediction in recent years. How they achieve this, however, is less\nexplored and they remain to be black box models. Here, we attempt to shed light\non the internal structure of deep saliency models and study what features they\nextract for fixation prediction. Specifically, we use a simple yet powerful\narchitecture, consisting of only one CNN and a single resolution input,\ncombined with a new loss function for pixel-wise fixation prediction during\nfree viewing of natural scenes. We show that our simple method is on par or\nbetter than state-of-the-art complicated saliency models. Furthermore, we\npropose a method, related to saliency model evaluation metrics, to visualize\ndeep models for fixation prediction. Our method reveals the inner\nrepresentations of deep models for fixation prediction and provides evidence\nthat saliency, as experienced by humans, is likely to involve high-level\nsemantic knowledge in addition to low-level perceptual cues. Our results can be\nuseful to measure the gap between current saliency models and the human\ninter-observer model and to build new models to close this gap.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2018 14:00:28 GMT"}, {"version": "v2", "created": "Wed, 21 Mar 2018 15:36:42 GMT"}, {"version": "v3", "created": "Thu, 22 Mar 2018 09:26:00 GMT"}], "update_date": "2018-03-23", "authors_parsed": [["He", "Sen", ""], ["Borji", "Ali", ""], ["Mi", "Yang", ""], ["Pugeault", "Nicolas", ""]]}, {"id": "1803.05759", "submitter": "Sen He", "authors": "Sen He, Nicolas Pugeault", "title": "Salient Region Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Saliency prediction is a well studied problem in computer vision. Early\nsaliency models were based on low-level hand-crafted feature derived from\ninsights gained in neuroscience and psychophysics. In the wake of deep learning\nbreakthrough, a new cohort of models were proposed based on neural network\narchitectures, allowing significantly higher gaze prediction than previous\nshallow models, on all metrics.\n  However, most models treat the saliency prediction as a \\textit{regression}\nproblem, and accurate regression of high-dimensional data is known to be a hard\nproblem. Furthermore, it is unclear that intermediate levels of saliency (ie,\nneither very high, nor very low) are meaningful: Something is either salient,\nor it is not.\n  Drawing from those two observations, we reformulate the saliency prediction\nproblem as a salient region \\textit{segmentation} problem. We demonstrate that\nthe reformulation allows for faster convergence than the classical regression\nproblem, while performance is comparable to state-of-the-art.\n  We also visualise the general features learned by the model, which are showed\nto be consistent with insights from psychophysics.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2018 14:09:47 GMT"}], "update_date": "2018-03-16", "authors_parsed": [["He", "Sen", ""], ["Pugeault", "Nicolas", ""]]}, {"id": "1803.05778", "submitter": "Yatin Saraiya", "authors": "Yatin Saraiya", "title": "Using accumulation to optimize deep residual neural nets", "comments": "7 pages, 6 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Residual Neural Networks [1] won first place in all five main tracks of the\nImageNet and COCO 2015 competitions. This kind of network involves the creation\nof pluggable modules such that the output contains a residual from the input.\nThe residual in that paper is the identity function. We propose to include\nresiduals from all lower layers, suitably normalized, to create the residual.\nThis way, all previous layers contribute equally to the output of a layer. We\nshow that our approach is an improvement on [1] for the CIFAR-10 dataset.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jan 2018 07:17:00 GMT"}], "update_date": "2018-03-16", "authors_parsed": [["Saraiya", "Yatin", ""]]}, {"id": "1803.05779", "submitter": "Yatin Saraiya", "authors": "Yatin Saraiya", "title": "A predictor-corrector method for the training of deep neural networks", "comments": "6 pages, 2 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The training of deep neural nets is expensive. We present a predictor-\ncorrector method for the training of deep neural nets. It alternates a\npredictor pass with a corrector pass using stochastic gradient descent with\nbackpropagation such that there is no loss in validation accuracy. No special\nmodifications to SGD with backpropagation is required by this methodology. Our\nexperiments showed a time improvement of 9% on the CIFAR-10 dataset.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jan 2018 06:30:59 GMT"}], "update_date": "2018-03-16", "authors_parsed": [["Saraiya", "Yatin", ""]]}, {"id": "1803.05785", "submitter": "Sen He", "authors": "Sen He, Dmitry Kangin, Yang Mi, Nicolas Pugeault", "title": "Aggregated Sparse Attention for Steering Angle Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we apply the attention mechanism to autonomous driving for\nsteering angle prediction. We propose the first model, applying the recently\nintroduced sparse attention mechanism to visual domain, as well as the\naggregated extension for this model. We show the improvement of the proposed\nmethod, comparing to no attention as well as to different types of attention.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2018 14:48:47 GMT"}], "update_date": "2018-03-16", "authors_parsed": [["He", "Sen", ""], ["Kangin", "Dmitry", ""], ["Mi", "Yang", ""], ["Pugeault", "Nicolas", ""]]}, {"id": "1803.05787", "submitter": "Tao Liu", "authors": "Zihao Liu, Qi Liu, Tao Liu, Nuo Xu, Xue Lin, Yanzhi Wang, Wujie Wen", "title": "Feature Distillation: DNN-Oriented JPEG Compression Against Adversarial\n  Examples", "comments": "2019 Conference on Computer Vision and Pattern Recognition (CVPR\n  2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image compression-based approaches for defending against the\nadversarial-example attacks, which threaten the safety use of deep neural\nnetworks (DNN), have been investigated recently. However, prior works mainly\nrely on directly tuning parameters like compression rate, to blindly reduce\nimage features, thereby lacking guarantee on both defense efficiency (i.e.\naccuracy of polluted images) and classification accuracy of benign images,\nafter applying defense methods. To overcome these limitations, we propose a\nJPEG-based defensive compression framework, namely \"feature distillation\", to\neffectively rectify adversarial examples without impacting classification\naccuracy on benign data. Our framework significantly escalates the defense\nefficiency with marginal accuracy reduction using a two-step method: First, we\nmaximize malicious features filtering of adversarial input perturbations by\ndeveloping defensive quantization in frequency domain of JPEG compression or\ndecompression, guided by a semi-analytical method; Second, we suppress the\ndistortions of benign features to restore classification accuracy through a\nDNN-oriented quantization refine process. Our experimental results show that\nproposed \"feature distillation\" can significantly surpass the latest\ninput-transformation based mitigations such as Quilting and TV Minimization in\nthree aspects, including defense efficiency (improve classification accuracy\nfrom $\\sim20\\%$ to $\\sim90\\%$ on adversarial examples), accuracy of benign\nimages after defense ($\\le1\\%$ accuracy degradation), and processing time per\nimage ($\\sim259\\times$ Speedup). Moreover, our solution can also provide the\nbest defense efficiency ($\\sim60\\%$ accuracy) against the recent adaptive\nattack with least accuracy reduction ($\\sim1\\%$) on benign images when compared\nwith other input-transformation based defense methods.\n", "versions": [{"version": "v1", "created": "Wed, 14 Mar 2018 02:24:59 GMT"}, {"version": "v2", "created": "Tue, 16 Apr 2019 16:46:16 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Liu", "Zihao", ""], ["Liu", "Qi", ""], ["Liu", "Tao", ""], ["Xu", "Nuo", ""], ["Lin", "Xue", ""], ["Wang", "Yanzhi", ""], ["Wen", "Wujie", ""]]}, {"id": "1803.05788", "submitter": "Tao Liu", "authors": "Zihao Liu, Tao Liu, Wujie Wen, Lei Jiang, Jie Xu, Yanzhi Wang, Gang\n  Quan", "title": "DeepN-JPEG: A Deep Neural Network Favorable JPEG-based Image Compression\n  Framework", "comments": "55th Design Automation Conference (DAC2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As one of most fascinating machine learning techniques, deep neural network\n(DNN) has demonstrated excellent performance in various intelligent tasks such\nas image classification. DNN achieves such performance, to a large extent, by\nperforming expensive training over huge volumes of training data. To reduce the\ndata storage and transfer overhead in smart resource-limited Internet-of-Thing\n(IoT) systems, effective data compression is a \"must-have\" feature before\ntransferring real-time produced dataset for training or classification. While\nthere have been many well-known image compression approaches (such as JPEG), we\nfor the first time find that a human-visual based image compression approach\nsuch as JPEG compression is not an optimized solution for DNN systems,\nespecially with high compression ratios. To this end, we develop an image\ncompression framework tailored for DNN applications, named \"DeepN-JPEG\", to\nembrace the nature of deep cascaded information process mechanism of DNN\narchitecture. Extensive experiments, based on \"ImageNet\" dataset with various\nstate-of-the-art DNNs, show that \"DeepN-JPEG\" can achieve ~3.5x higher\ncompression rate over the popular JPEG solution while maintaining the same\naccuracy level for image recognition, demonstrating its great potential of\nstorage and power efficiency in DNN-based smart IoT system design.\n", "versions": [{"version": "v1", "created": "Wed, 14 Mar 2018 02:18:55 GMT"}], "update_date": "2018-03-16", "authors_parsed": [["Liu", "Zihao", ""], ["Liu", "Tao", ""], ["Wen", "Wujie", ""], ["Jiang", "Lei", ""], ["Xu", "Jie", ""], ["Wang", "Yanzhi", ""], ["Quan", "Gang", ""]]}, {"id": "1803.05790", "submitter": "Yan Zhang", "authors": "Yan Zhang, He Sun, Siyu Tang, Heiko Neumann", "title": "Temporal Human Action Segmentation via Dynamic Clustering", "comments": "comparing with the 1st version, only corrected typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an effective dynamic clustering algorithm for the task of temporal\nhuman action segmentation, which has comprehensive applications such as\nrobotics, motion analysis, and patient monitoring. Our proposed algorithm is\nunsupervised, fast, generic to process various types of features, and\napplicable in both the online and offline settings. We perform extensive\nexperiments of processing data streams, and show that our algorithm achieves\nthe state-of-the-art results for both online and offline settings.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2018 14:55:22 GMT"}, {"version": "v2", "created": "Sun, 18 Mar 2018 23:26:10 GMT"}], "update_date": "2018-03-20", "authors_parsed": [["Zhang", "Yan", ""], ["Sun", "He", ""], ["Tang", "Siyu", ""], ["Neumann", "Heiko", ""]]}, {"id": "1803.05817", "submitter": "Rahman Attar", "authors": "Rahman Attar, Xiang Xie, Zhihua Wang, and Shigang Yue", "title": "2D Reconstruction of Small Intestine's Interior Wall", "comments": "Journal draft", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Examining and interpreting of a large number of wireless endoscopic images\nfrom the gastrointestinal tract is a tiresome task for physicians. A practical\nsolution is to automatically construct a two dimensional representation of the\ngastrointestinal tract for easy inspection. However, little has been done on\nwireless endoscopic image stitching, let alone systematic investigation. The\nproposed new wireless endoscopic image stitching method consists of two main\nsteps to improve the accuracy and efficiency of image registration. First, the\nkeypoints are extracted by Principle Component Analysis and Scale Invariant\nFeature Transform (PCA-SIFT) algorithm and refined with Maximum Likelihood\nEstimation SAmple Consensus (MLESAC) outlier removal to find the most reliable\nkeypoints. Second, the optimal transformation parameters obtained from first\nstep are fed to the Normalised Mutual Information (NMI) algorithm as an initial\nsolution. With modified Marquardt-Levenberg search strategy in a multiscale\nframework, the NMI can find the optimal transformation parameters in the\nshortest time. The proposed methodology has been tested on two different\ndatasets - one with real wireless endoscopic images and another with images\nobtained from Micro-Ball (a new wireless cubic endoscopy system with six image\nsensors). The results have demonstrated the accuracy and robustness of the\nproposed methodology both visually and quantitatively.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2018 15:46:00 GMT"}], "update_date": "2018-03-16", "authors_parsed": [["Attar", "Rahman", ""], ["Xie", "Xiang", ""], ["Wang", "Zhihua", ""], ["Yue", "Shigang", ""]]}, {"id": "1803.05827", "submitter": "Chu Wang", "authors": "Chu Wang, Babak Samari, Kaleem Siddiqi", "title": "Local Spectral Graph Convolution for Point Set Feature Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature learning on point clouds has shown great promise, with the\nintroduction of effective and generalizable deep learning frameworks such as\npointnet++. Thus far, however, point features have been abstracted in an\nindependent and isolated manner, ignoring the relative layout of neighboring\npoints as well as their features. In the present article, we propose to\novercome this limitation by using spectral graph convolution on a local graph,\ncombined with a novel graph pooling strategy. In our approach, graph\nconvolution is carried out on a nearest neighbor graph constructed from a\npoint's neighborhood, such that features are jointly learned. We replace the\nstandard max pooling step with a recursive clustering and pooling strategy,\ndevised to aggregate information from within clusters of nodes that are close\nto one another in their spectral coordinates, leading to richer overall feature\ndescriptors. Through extensive experiments on diverse datasets, we show a\nconsistent demonstrable advantage for the tasks of both point set\nclassification and segmentation.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2018 16:00:50 GMT"}], "update_date": "2018-03-16", "authors_parsed": [["Wang", "Chu", ""], ["Samari", "Babak", ""], ["Siddiqi", "Kaleem", ""]]}, {"id": "1803.05845", "submitter": "Manna Dai", "authors": "Manna Dai, Shuying Cheng, Xiangjian He, Dadong Wang", "title": "A Structural Correlation Filter Combined with A Multi-task Gaussian\n  Particle Filter for Visual Tracking", "comments": "10 pages. arXiv admin note: text overlap with arXiv:1703.05020 by\n  other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  In this paper, we propose a novel structural correlation filter combined with\na multi-task Gaussian particle filter (KCF-GPF) model for robust visual\ntracking. We first present an assemble structure where several KCF trackers as\nweak experts provide a preliminary decision for a Gaussian particle filter to\nmake a final decision. The proposed method is designed to exploit and\ncomplement the strength of a KCF and a Gaussian particle filter. Compared with\nthe existing tracking methods based on correlation filters or particle filters,\nthe proposed tracker has several advantages. First, it can detect the tracked\ntarget in a large-scale search scope via weak KCF trackers and evaluate the\nreliability of weak trackers\\rq decisions for a Gaussian particle filter to\nmake a strong decision, and hence it can tackle fast motions, appearance\nvariations, occlusions and re-detections. Second, it can effectively handle\nlarge-scale variations via a Gaussian particle filter. Third, it can be\namenable to fully parallel implementation using importance sampling without\nresampling, thereby it is convenient for VLSI implementation and can lower the\ncomputational costs. Extensive experiments on the OTB-2013 dataset containing\n50 challenging sequences demonstrate that the proposed algorithm performs\nfavourably against 16 state-of-the-art trackers.\n", "versions": [{"version": "v1", "created": "Sat, 3 Mar 2018 11:11:17 GMT"}], "update_date": "2018-03-18", "authors_parsed": [["Dai", "Manna", ""], ["Cheng", "Shuying", ""], ["He", "Xiangjian", ""], ["Wang", "Dadong", ""]]}, {"id": "1803.05846", "submitter": "Hongying Meng Dr", "authors": "Asim Jan, Huaxiong Ding, Hongying Meng, Liming Chen, Huibin Li", "title": "Accurate Facial Parts Localization and Deep Learning for 3D Facial\n  Expression Recognition", "comments": "7 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Meaningful facial parts can convey key cues for both facial action unit\ndetection and expression prediction. Textured 3D face scan can provide both\ndetailed 3D geometric shape and 2D texture appearance cues of the face which\nare beneficial for Facial Expression Recognition (FER). However, accurate\nfacial parts extraction as well as their fusion are challenging tasks. In this\npaper, a novel system for 3D FER is designed based on accurate facial parts\nextraction and deep feature fusion of facial parts. In particular, each\ntextured 3D face scan is firstly represented as a 2D texture map and a depth\nmap with one-to-one dense correspondence. Then, the facial parts of both\ntexture map and depth map are extracted using a novel 4-stage process consists\nof facial landmark localization, facial rotation correction, facial resizing,\nfacial parts bounding box extraction and post-processing procedures. Finally,\ndeep fusion Convolutional Neural Networks (CNNs) features of all facial parts\nare learned from both texture maps and depth maps, respectively and nonlinear\nSVMs are used for expression prediction. Experiments are conducted on the\nBU-3DFE database, demonstrating the effectiveness of combing different facial\nparts, texture and depth cues and reporting the state-of-the-art results in\ncomparison with all existing methods under the same setting.\n", "versions": [{"version": "v1", "created": "Sun, 4 Mar 2018 15:04:23 GMT"}], "update_date": "2018-03-16", "authors_parsed": [["Jan", "Asim", ""], ["Ding", "Huaxiong", ""], ["Meng", "Hongying", ""], ["Chen", "Liming", ""], ["Li", "Huibin", ""]]}, {"id": "1803.05847", "submitter": "Lingxiao Wei", "authors": "Lingxiao Wei, Bo Luo, Yu Li, Yannan Liu and Qiang Xu", "title": "I Know What You See: Power Side-Channel Attack on Convolutional Neural\n  Network Accelerators", "comments": null, "journal-ref": null, "doi": "10.1145/3274694.3274696", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has become the de-facto computational paradigm for various\nkinds of perception problems, including many privacy-sensitive applications\nsuch as online medical image analysis. No doubt to say, the data privacy of\nthese deep learning systems is a serious concern. Different from previous\nresearch focusing on exploiting privacy leakage from deep learning models, in\nthis paper, we present the first attack on the implementation of deep learning\nmodels. To be specific, we perform the attack on an FPGA-based convolutional\nneural network accelerator and we manage to recover the input image from the\ncollected power traces without knowing the detailed parameters in the neural\nnetwork. For the MNIST dataset, our power side-channel attack is able to\nachieve up to 89% recognition accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 5 Mar 2018 11:35:14 GMT"}, {"version": "v2", "created": "Fri, 29 Nov 2019 15:11:58 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Wei", "Lingxiao", ""], ["Luo", "Bo", ""], ["Li", "Yu", ""], ["Liu", "Yannan", ""], ["Xu", "Qiang", ""]]}, {"id": "1803.05848", "submitter": "Zhiyang Liu", "authors": "Zhiyang Liu, Chen Cao, Shuxue Ding, Tong Han, Hong Wu, Sheng Liu", "title": "Towards Clinical Diagnosis: Automated Stroke Lesion Segmentation on\n  Multimodal MR Image Using Convolutional Neural Network", "comments": "submitted to Neuroimage: Clinical", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The patient with ischemic stroke can benefit most from the earliest possible\ndefinitive diagnosis. While the high quality medical resources are quite scarce\nacross the globe, an automated diagnostic tool is expected in analyzing the\nmagnetic resonance (MR) images to provide reference in clinical diagnosis. In\nthis paper, we propose a deep learning method to automatically segment ischemic\nstroke lesions from multi-modal MR images. By using atrous convolution and\nglobal convolution network, our proposed residual-structured fully\nconvolutional network (Res-FCN) is able to capture features from large\nreceptive fields. The network architecture is validated on a large dataset of\n212 clinically acquired multi-modal MR images, which is shown to achieve a mean\ndice coefficient of 0.645 with a mean number of false negative lesions of\n1.515. The false negatives can reach a value that close to a common medical\nimage doctor, making it exceptive for a real clinical application.\n", "versions": [{"version": "v1", "created": "Mon, 5 Mar 2018 14:27:41 GMT"}], "update_date": "2018-03-16", "authors_parsed": [["Liu", "Zhiyang", ""], ["Cao", "Chen", ""], ["Ding", "Shuxue", ""], ["Han", "Tong", ""], ["Wu", "Hong", ""], ["Liu", "Sheng", ""]]}, {"id": "1803.05849", "submitter": "Renzo Andri", "authors": "Andrawes Al Bahou, Geethan Karunaratne, Renzo Andri, Lukas Cavigelli,\n  Luca Benini", "title": "XNORBIN: A 95 TOp/s/W Hardware Accelerator for Binary Convolutional\n  Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.AR cs.NE eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deploying state-of-the-art CNNs requires power-hungry processors and off-chip\nmemory. This precludes the implementation of CNNs in low-power embedded\nsystems. Recent research shows CNNs sustain extreme quantization, binarizing\ntheir weights and intermediate feature maps, thereby saving 8-32\\x memory and\ncollapsing energy-intensive sum-of-products into XNOR-and-popcount operations.\n  We present XNORBIN, an accelerator for binary CNNs with computation tightly\ncoupled to memory for aggressive data reuse. Implemented in UMC 65nm technology\nXNORBIN achieves an energy efficiency of 95 TOp/s/W and an area efficiency of\n2.0 TOp/s/MGE at 0.8 V.\n", "versions": [{"version": "v1", "created": "Mon, 5 Mar 2018 15:41:28 GMT"}], "update_date": "2018-09-12", "authors_parsed": [["Bahou", "Andrawes Al", ""], ["Karunaratne", "Geethan", ""], ["Andri", "Renzo", ""], ["Cavigelli", "Lukas", ""], ["Benini", "Luca", ""]]}, {"id": "1803.05850", "submitter": "E. Jared Shamwell PhD", "authors": "E. Jared Shamwell, Sarah Leung, William D. Nothwang", "title": "Vision-Aided Absolute Trajectory Estimation Using an Unsupervised Deep\n  Network with Online Error Correction", "comments": "Submitted to IROS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an unsupervised deep neural network approach to the fusion of\nRGB-D imagery with inertial measurements for absolute trajectory estimation.\nOur network, dubbed the Visual-Inertial-Odometry Learner (VIOLearner), learns\nto perform visual-inertial odometry (VIO) without inertial measurement unit\n(IMU) intrinsic parameters (corresponding to gyroscope and accelerometer bias\nor white noise) or the extrinsic calibration between an IMU and camera. The\nnetwork learns to integrate IMU measurements and generate hypothesis\ntrajectories which are then corrected online according to the Jacobians of\nscaled image projection errors with respect to a spatial grid of pixel\ncoordinates. We evaluate our network against state-of-the-art (SOA)\nvisual-inertial odometry, visual odometry, and visual simultaneous localization\nand mapping (VSLAM) approaches on the KITTI Odometry dataset and demonstrate\ncompetitive odometry performance.\n", "versions": [{"version": "v1", "created": "Thu, 8 Mar 2018 11:59:49 GMT"}], "update_date": "2018-03-16", "authors_parsed": [["Shamwell", "E. Jared", ""], ["Leung", "Sarah", ""], ["Nothwang", "William D.", ""]]}, {"id": "1803.05851", "submitter": "Guoqiang Xiang", "authors": "Xiaofang Wang, Guoqiang Xiang, Xinyue Zhang, Wei Wei", "title": "Image Registration Based Flicker Solving in Video Face Replacement and\n  Analysis Based Sub-pixel Image Registration", "comments": "9 Pages, 10 figures, partly accepted by 2017 Intelligent Computing\n  and Information Systems (ICIS2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a framework of video face replacement is proposed and it deals\nwith the flicker of swapped face in video sequence. This framework contains two\nmain innovations: 1) the technique of image registration is exploited to align\nthe source and target video faces for eliminating the flicker or jitter of the\nsegmented video face sequence; 2) a fast subpixel image registration method is\nproposed for farther accuracy and efficiency. Unlike the priori works, it\nminimizes the overlapping region and takes spatiotemporal coherence into\naccount. Flicker in resulted videos is usually caused by the frequently changed\nbound of the blending target face and unregistered faces between and along\nvideo sequences. The subpixel image registration method is proposed to solve\nthe flicker problem. During the alignment process, integer pixel registration\nis formulated by maximizing the similarity of images with down sampling\nstrategy speeding up the process and sub-pixel image registration is a\nsingle-step image match via analytic method. Experimental results show the\nproposed algorithm reduces the computation time and gets a high accuracy when\nconducting experiments on different data sets.\n", "versions": [{"version": "v1", "created": "Fri, 9 Mar 2018 06:37:53 GMT"}], "update_date": "2018-03-16", "authors_parsed": [["Wang", "Xiaofang", ""], ["Xiang", "Guoqiang", ""], ["Zhang", "Xinyue", ""], ["Wei", "Wei", ""]]}, {"id": "1803.05853", "submitter": "Aarti Jajoo", "authors": "Aarti Jajoo, Matthew Nicol, Jaime Gateno, Ken-Chung Chen, Zhen Tang,\n  Tasadduk Chowdhury, Jainfu Li, Steve Goufang Shen, James J. Xia", "title": "Calculating the Midsagittal Plane for Symmetrical Bilateral Shapes:\n  Applications to Clinical Facial Surgical Planning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is difficult to estimate the midsagittal plane of human subjects with\ncraniomaxillofacial (CMF) deformities. We have developed a LAndmark GEometric\nRoutine (LAGER), which automatically estimates a midsagittal plane for such\nsubjects. The LAGER algorithm was based on the assumption that the optimal\nmidsagittal plane of a patient with a deformity is the premorbid midsagittal\nplane of the patient (i.e. hypothetically normal without deformity). The LAGER\nalgorithm consists of three steps. The first step quantifies the asymmetry of\nthe landmarks using a Euclidean distance matrix analysis and ranks the\nlandmarks according to their degree of asymmetry. The second step uses a\nrecursive algorithm to drop outlier landmarks. The third step inputs the\nremaining landmarks into an optimization algorithm to determine an optimal\nmidsaggital plane. We validate LAGER on 20 synthetic models mimicking the\nskulls of real patients with CMF deformities. The results indicated that all\nthe LAGER algorithm-generated midsagittal planes met clinical criteria. Thus it\ncan be used clinically to determine the midsagittal plane for patients with CMF\ndeformities.\n", "versions": [{"version": "v1", "created": "Sun, 11 Mar 2018 01:31:15 GMT"}], "update_date": "2018-03-16", "authors_parsed": [["Jajoo", "Aarti", ""], ["Nicol", "Matthew", ""], ["Gateno", "Jaime", ""], ["Chen", "Ken-Chung", ""], ["Tang", "Zhen", ""], ["Chowdhury", "Tasadduk", ""], ["Li", "Jainfu", ""], ["Shen", "Steve Goufang", ""], ["Xia", "James J.", ""]]}, {"id": "1803.05854", "submitter": "Sasank Chilamkurthy", "authors": "Sasank Chilamkurthy, Rohit Ghosh, Swetha Tanamala, Mustafa Biviji,\n  Norbert G. Campeau, Vasantha Kumar Venugopal, Vidur Mahajan, Pooja Rao,\n  Prashant Warier", "title": "Development and Validation of Deep Learning Algorithms for Detection of\n  Critical Findings in Head CT Scans", "comments": "Improved operating points, updated link to CQ500 dataset:\n  headctstudy.qure.ai/dataset", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Importance: Non-contrast head CT scan is the current standard for initial\nimaging of patients with head trauma or stroke symptoms.\n  Objective: To develop and validate a set of deep learning algorithms for\nautomated detection of following key findings from non-contrast head CT scans:\nintracranial hemorrhage (ICH) and its types, intraparenchymal (IPH),\nintraventricular (IVH), subdural (SDH), extradural (EDH) and subarachnoid (SAH)\nhemorrhages, calvarial fractures, midline shift and mass effect.\n  Design and Settings: We retrospectively collected a dataset containing\n313,318 head CT scans along with their clinical reports from various centers. A\npart of this dataset (Qure25k dataset) was used to validate and the rest to\ndevelop algorithms. Additionally, a dataset (CQ500 dataset) was collected from\ndifferent centers in two batches B1 & B2 to clinically validate the algorithms.\n  Main Outcomes and Measures: Original clinical radiology report and consensus\nof three independent radiologists were considered as gold standard for Qure25k\nand CQ500 datasets respectively. Area under receiver operating characteristics\ncurve (AUC) for each finding was primarily used to evaluate the algorithms.\n  Results: Qure25k dataset contained 21,095 scans (mean age 43.31; 42.87%\nfemale) while batches B1 and B2 of CQ500 dataset consisted of 214 (mean age\n43.40; 43.92% female) and 277 (mean age 51.70; 30.31% female) scans\nrespectively. On Qure25k dataset, the algorithms achieved AUCs of 0.9194,\n0.8977, 0.9559, 0.9161, 0.9288 and 0.9044 for detecting ICH, IPH, IVH, SDH, EDH\nand SAH respectively. AUCs for the same on CQ500 dataset were 0.9419, 0.9544,\n0.9310, 0.9521, 0.9731 and 0.9574 respectively. For detecting calvarial\nfractures, midline shift and mass effect, AUCs on Qure25k dataset were 0.9244,\n0.9276 and 0.8583 respectively, while AUCs on CQ500 dataset were 0.9624, 0.9697\nand 0.9216 respectively.\n", "versions": [{"version": "v1", "created": "Tue, 13 Mar 2018 17:43:30 GMT"}, {"version": "v2", "created": "Thu, 12 Apr 2018 06:32:23 GMT"}], "update_date": "2018-04-13", "authors_parsed": [["Chilamkurthy", "Sasank", ""], ["Ghosh", "Rohit", ""], ["Tanamala", "Swetha", ""], ["Biviji", "Mustafa", ""], ["Campeau", "Norbert G.", ""], ["Venugopal", "Vasantha Kumar", ""], ["Mahajan", "Vidur", ""], ["Rao", "Pooja", ""], ["Warier", "Prashant", ""]]}, {"id": "1803.05858", "submitter": "Xiangyun Zhao", "authors": "Xiangyun Zhao, Shuang Liang, Yichen Wei", "title": "Pseudo Mask Augmented Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present a novel and effective framework to facilitate object\ndetection with the instance-level segmentation information that is only\nsupervised by bounding box annotation. Starting from the joint object detection\nand instance segmentation network, we propose to recursively estimate the\npseudo ground-truth object masks from the instance-level object segmentation\nnetwork training, and then enhance the detection network with top-down\nsegmentation feedbacks. The pseudo ground truth mask and network parameters are\noptimized alternatively to mutually benefit each other. To obtain the promising\npseudo masks in each iteration, we embed a graphical inference that\nincorporates the low-level image appearance consistency and the bounding box\nannotations to refine the segmentation masks predicted by the segmentation\nnetwork. Our approach progressively improves the object detection performance\nby incorporating the detailed pixel-wise information learned from the\nweakly-supervised segmentation network. Extensive evaluation on the detection\ntask in PASCAL VOC 2007 and 2012 [12] verifies that the proposed approach is\neffective.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2018 16:51:57 GMT"}, {"version": "v2", "created": "Tue, 27 Mar 2018 02:36:38 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Zhao", "Xiangyun", ""], ["Liang", "Shuang", ""], ["Wei", "Yichen", ""]]}, {"id": "1803.05863", "submitter": "Alexander Ororbia", "authors": "Alexander G. Ororbia, Ankur Mali, Jian Wu, Scott O'Connell, David\n  Miller, C. Lee Giles", "title": "Learned Neural Iterative Decoding for Lossy Image Compression Systems", "comments": "Vastly updated version, now includes JP2", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For lossy image compression systems, we develop an algorithm, iterative\nrefinement, to improve the decoder's reconstruction compared to standard\ndecoding techniques. Specifically, we propose a recurrent neural network\napproach for nonlinear, iterative decoding. Our decoder, which works with any\nencoder, employs self-connected memory units that make use of causal and\nnon-causal spatial context information to progressively reduce reconstruction\nerror over a fixed number of steps. We experiment with variants of our\nestimator and find that iterative refinement consistently creates lower\ndistortion images of higher perceptual quality compared to other approaches.\nSpecifically, on the Kodak Lossless True Color Image Suite, we observe as much\nas a 0.871 decibel (dB) gain over JPEG, a 1.095 dB gain over JPEG 2000, and a\n0.971 dB gain over a competitive neural model.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2018 16:58:45 GMT"}, {"version": "v2", "created": "Sun, 18 Mar 2018 20:36:27 GMT"}, {"version": "v3", "created": "Sat, 10 Nov 2018 06:24:57 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Ororbia", "Alexander G.", ""], ["Mali", "Ankur", ""], ["Wu", "Jian", ""], ["O'Connell", "Scott", ""], ["Miller", "David", ""], ["Giles", "C. Lee", ""]]}, {"id": "1803.05872", "submitter": "Qiang Qiu", "authors": "Albert Gong, Qiang Qiu, Guillermo Sapiro", "title": "Virtual CNN Branching: Efficient Feature Ensemble for Person\n  Re-Identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce an ensemble method for convolutional neural\nnetwork (CNN), called \"virtual branching,\" which can be implemented with nearly\nno additional parameters and computation on top of standard CNNs. We propose\nour method in the context of person re-identification (re-ID). Our CNN model\nconsists of shared bottom layers, followed by \"virtual\" branches, where neurons\nfrom a block of regular convolutional and fully-connected layers are\npartitioned into multiple sets. Each virtual branch is trained with different\ndata to specialize in different aspects, e.g., a specific body region or pose\norientation. In this way, robust ensemble representations are obtained against\nhuman body misalignment, deformations, or variations in viewing angles, at\nnearly no any additional cost. The proposed method achieves competitive\nperformance on multiple person re-ID benchmark datasets, including Market-1501,\nCUHK03, and DukeMTMC-reID.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2018 17:11:07 GMT"}], "update_date": "2018-03-16", "authors_parsed": [["Gong", "Albert", ""], ["Qiu", "Qiang", ""], ["Sapiro", "Guillermo", ""]]}, {"id": "1803.05873", "submitter": "Meysam Madadi", "authors": "Ciprian A. Corneanu, Meysam Madadi, Sergio Escalera", "title": "Deep Structure Inference Network for Facial Action Unit Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial expressions are combinations of basic components called Action Units\n(AU). Recognizing AUs is key for developing general facial expression analysis.\nIn recent years, most efforts in automatic AU recognition have been dedicated\nto learning combinations of local features and to exploiting correlations\nbetween Action Units. In this paper, we propose a deep neural architecture that\ntackles both problems by combining learned local and global features in its\ninitial stages and replicating a message passing algorithm between classes\nsimilar to a graphical model inference approach in later stages. We show that\nby training the model end-to-end with increased supervision we improve\nstate-of-the-art by 5.3% and 8.2% performance on BP4D and DISFA datasets,\nrespectively.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2018 17:14:35 GMT"}, {"version": "v2", "created": "Fri, 23 Mar 2018 11:17:17 GMT"}], "update_date": "2018-03-26", "authors_parsed": [["Corneanu", "Ciprian A.", ""], ["Madadi", "Meysam", ""], ["Escalera", "Sergio", ""]]}, {"id": "1803.05900", "submitter": "Stylianos Venieris", "authors": "Stylianos I. Venieris, Alexandros Kouris, Christos-Savvas Bouganis", "title": "Toolflows for Mapping Convolutional Neural Networks on FPGAs: A Survey\n  and Future Directions", "comments": "Accepted for publication at the ACM Computing Surveys (CSUR) journal,\n  2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the past decade, Convolutional Neural Networks (CNNs) have demonstrated\nstate-of-the-art performance in various Artificial Intelligence tasks. To\naccelerate the experimentation and development of CNNs, several software\nframeworks have been released, primarily targeting power-hungry CPUs and GPUs.\nIn this context, reconfigurable hardware in the form of FPGAs constitutes a\npotential alternative platform that can be integrated in the existing deep\nlearning ecosystem to provide a tunable balance between performance, power\nconsumption and programmability. In this paper, a survey of the existing\nCNN-to-FPGA toolflows is presented, comprising a comparative study of their key\ncharacteristics which include the supported applications, architectural\nchoices, design space exploration methods and achieved performance. Moreover,\nmajor challenges and objectives introduced by the latest trends in CNN\nalgorithmic research are identified and presented. Finally, a uniform\nevaluation methodology is proposed, aiming at the comprehensive, complete and\nin-depth evaluation of CNN-to-FPGA toolflows.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2018 17:58:19 GMT"}], "update_date": "2018-03-16", "authors_parsed": [["Venieris", "Stylianos I.", ""], ["Kouris", "Alexandros", ""], ["Bouganis", "Christos-Savvas", ""]]}, {"id": "1803.05909", "submitter": "Maurice Yang", "authors": "Maurice Yang, Mahmoud Faraj, Assem Hussein, Vincent Gaudet", "title": "Efficient Hardware Realization of Convolutional Neural Networks using\n  Intra-Kernel Regular Pruning", "comments": "6 pages, 8 figures, ISMVL 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent trend toward increasingly deep convolutional neural networks\n(CNNs) leads to a higher demand of computational power and memory storage.\nConsequently, the deployment of CNNs in hardware has become more challenging.\nIn this paper, we propose an Intra-Kernel Regular (IKR) pruning scheme to\nreduce the size and computational complexity of the CNNs by removing redundant\nweights at a fine-grained level. Unlike other pruning methods such as\nFine-Grained pruning, IKR pruning maintains regular kernel structures that are\nexploitable in a hardware accelerator. Experimental results demonstrate up to\n10x parameter reduction and 7x computational reduction at a cost of less than\n1% degradation in accuracy versus the un-pruned case.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2018 21:00:17 GMT"}], "update_date": "2018-03-19", "authors_parsed": [["Yang", "Maurice", ""], ["Faraj", "Mahmoud", ""], ["Hussein", "Assem", ""], ["Gaudet", "Vincent", ""]]}, {"id": "1803.05940", "submitter": "Mariella Dimiccoli", "authors": "Stefan Lonn, Petia Radeva, Mariella Dimiccoli", "title": "Smartphone picture organization: A hierarchical approach", "comments": null, "journal-ref": "Computer Vision and Image Understanding (CVIU), Volume 187,\n  October 2019, 102789", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We live in a society where the large majority of the population has a\ncamera-equipped smartphone. In addition, hard drives and cloud storage are\ngetting cheaper and cheaper, leading to a tremendous growth in stored personal\nphotos. Unlike photo collections captured by a digital camera, which typically\nare pre-processed by the user who organizes them into event-related folders,\nsmartphone pictures are automatically stored in the cloud. As a consequence,\nphoto collections captured by a smartphone are highly unstructured and because\nsmartphones are ubiquitous, they present a larger variability compared to\npictures captured by a digital camera. To solve the need of organizing large\nsmartphone photo collections automatically, we propose here a new methodology\nfor hierarchical photo organization into topics and topic-related categories.\nOur approach successfully estimates latent topics in the pictures by applying\nprobabilistic Latent Semantic Analysis, and automatically assigns a name to\neach topic by relying on a lexical database. Topic-related categories are then\nestimated by using a set of topic-specific Convolutional Neuronal Networks. To\nvalidate our approach, we ensemble and make public a large dataset of more than\n8,000 smartphone pictures from 10 persons. Experimental results demonstrate\nbetter user satisfaction with respect to state of the art solutions in terms of\norganization.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2018 18:37:50 GMT"}, {"version": "v2", "created": "Fri, 6 Sep 2019 19:16:10 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Lonn", "Stefan", ""], ["Radeva", "Petia", ""], ["Dimiccoli", "Mariella", ""]]}, {"id": "1803.05959", "submitter": "Weipeng Xu", "authors": "Weipeng Xu, Avishek Chatterjee, Michael Zollhoefer, Helge Rhodin,\n  Pascal Fua, Hans-Peter Seidel, Christian Theobalt", "title": "Mo2Cap2: Real-time Mobile 3D Motion Capture with a Cap-mounted Fisheye\n  Camera", "comments": "IEEE TVCG Proc. VR 2019. Webpage:\n  http://gvv.mpi-inf.mpg.de/projects/wxu/Mo2Cap2/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose the first real-time approach for the egocentric estimation of 3D\nhuman body pose in a wide range of unconstrained everyday activities. This\nsetting has a unique set of challenges, such as mobility of the hardware setup,\nand robustness to long capture sessions with fast recovery from tracking\nfailures. We tackle these challenges based on a novel lightweight setup that\nconverts a standard baseball cap to a device for high-quality pose estimation\nbased on a single cap-mounted fisheye camera. From the captured egocentric live\nstream, our CNN based 3D pose estimation approach runs at 60Hz on a\nconsumer-level GPU. In addition to the novel hardware setup, our other main\ncontributions are: 1) a large ground truth training corpus of top-down fisheye\nimages and 2) a novel disentangled 3D pose estimation approach that takes the\nunique properties of the egocentric viewpoint into account. As shown by our\nevaluation, we achieve lower 3D joint error as well as better 2D overlay than\nthe existing baselines.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2018 19:20:12 GMT"}, {"version": "v2", "created": "Wed, 23 Jan 2019 10:30:21 GMT"}], "update_date": "2019-01-24", "authors_parsed": [["Xu", "Weipeng", ""], ["Chatterjee", "Avishek", ""], ["Zollhoefer", "Michael", ""], ["Rhodin", "Helge", ""], ["Fua", "Pascal", ""], ["Seidel", "Hans-Peter", ""], ["Theobalt", "Christian", ""]]}, {"id": "1803.05963", "submitter": "Charlotte Bunne", "authors": "Charlotte Bunne, Lukas Rahmann, and Thomas Wolf", "title": "Studying Invariances of Trained Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNNs) define an exceptionally powerful class\nof models for image classification, but the theoretical background and the\nunderstanding of how invariances to certain transformations are learned is\nlimited. In a large scale screening with images modified by different affine\nand nonaffine transformations of varying magnitude, we analyzed the behavior of\nthe CNN architectures AlexNet and ResNet. If the magnitude of different\ntransformations does not exceed a class- and transformation dependent\nthreshold, both architectures show invariant behavior. In this work we\nfurthermore introduce a new learnable module, the Invariant Transformer Net,\nwhich enables us to learn differentiable parameters for a set of affine\ntransformations. This allows us to extract the space of transformations to\nwhich the CNN is invariant and its class prediction robust.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2018 19:27:41 GMT"}], "update_date": "2018-03-19", "authors_parsed": [["Bunne", "Charlotte", ""], ["Rahmann", "Lukas", ""], ["Wolf", "Thomas", ""]]}, {"id": "1803.05982", "submitter": "Seyed Sadegh Mohseni Salehi", "authors": "Seyed Sadegh Mohseni Salehi, Shadab Khan, Deniz Erdogmus, Ali\n  Gholipour", "title": "Real-time Deep Pose Estimation with Geodesic Loss for Image-to-Template\n  Rigid Registration", "comments": "This work has been submitted to TMI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With an aim to increase the capture range and accelerate the performance of\nstate-of-the-art inter-subject and subject-to-template 3D registration, we\npropose deep learning-based methods that are trained to find the 3D position of\narbitrarily oriented subjects or anatomy based on slices or volumes of medical\nimages. For this, we propose regression CNNs that learn to predict the\nangle-axis representation of 3D rotations and translations using image\nfeatures. We use and compare mean square error and geodesic loss to train\nregression CNNs for 3D pose estimation used in two different scenarios:\nslice-to-volume registration and volume-to-volume registration. Our results\nshow that in such registration applications that are amendable to learning, the\nproposed deep learning methods with geodesic loss minimization can achieve\naccurate results with a wide capture range in real-time (<100ms). We also\ntested the generalization capability of the trained CNNs on an expanded age\nrange and on images of newborn subjects with similar and different MR image\ncontrasts. We trained our models on T2-weighted fetal brain MRI scans and used\nthem to predict the 3D pose of newborn brains based on T1-weighted MRI scans.\nWe showed that the trained models generalized well for the new domain when we\nperformed image contrast transfer through a conditional generative adversarial\nnetwork. This indicates that the domain of application of the trained deep\nregression CNNs can be further expanded to image modalities and contrasts other\nthan those used in training. A combination of our proposed methods with\naccelerated optimization-based registration algorithms can dramatically enhance\nthe performance of automatic imaging devices and image processing methods of\nthe future.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2018 20:07:59 GMT"}, {"version": "v2", "created": "Mon, 19 Mar 2018 17:19:11 GMT"}, {"version": "v3", "created": "Sun, 3 Jun 2018 20:11:37 GMT"}, {"version": "v4", "created": "Sat, 18 Aug 2018 19:59:46 GMT"}], "update_date": "2018-08-21", "authors_parsed": [["Salehi", "Seyed Sadegh Mohseni", ""], ["Khan", "Shadab", ""], ["Erdogmus", "Deniz", ""], ["Gholipour", "Ali", ""]]}, {"id": "1803.05984", "submitter": "Siyuan Qiao", "authors": "Siyuan Qiao, Wei Shen, Zhishuai Zhang, Bo Wang, Alan Yuille", "title": "Deep Co-Training for Semi-Supervised Image Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the problem of semi-supervised image recognition,\nwhich is to learn classifiers using both labeled and unlabeled images. We\npresent Deep Co-Training, a deep learning based method inspired by the\nCo-Training framework. The original Co-Training learns two classifiers on two\nviews which are data from different sources that describe the same instances.\nTo extend this concept to deep learning, Deep Co-Training trains multiple deep\nneural networks to be the different views and exploits adversarial examples to\nencourage view difference, in order to prevent the networks from collapsing\ninto each other. As a result, the co-trained networks provide different and\ncomplementary information about the data, which is necessary for the\nCo-Training framework to achieve good results. We test our method on SVHN,\nCIFAR-10/100 and ImageNet datasets, and our method outperforms the previous\nstate-of-the-art methods by a large margin.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2018 20:13:07 GMT"}], "update_date": "2018-03-19", "authors_parsed": [["Qiao", "Siyuan", ""], ["Shen", "Wei", ""], ["Zhang", "Zhishuai", ""], ["Wang", "Bo", ""], ["Yuille", "Alan", ""]]}, {"id": "1803.06049", "submitter": "Shafin Rahman", "authors": "Shafin Rahman, Salman Khan and Fatih Porikli", "title": "Zero-Shot Object Detection: Learning to Simultaneously Recognize and\n  Localize Novel Concepts", "comments": null, "journal-ref": "ACCV 2018", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Current Zero-Shot Learning (ZSL) approaches are restricted to recognition of\na single dominant unseen object category in a test image. We hypothesize that\nthis setting is ill-suited for real-world applications where unseen objects\nappear only as a part of a complex scene, warranting both the `recognition' and\n`localization' of an unseen category. To address this limitation, we introduce\na new \\emph{`Zero-Shot Detection'} (ZSD) problem setting, which aims at\nsimultaneously recognizing and locating object instances belonging to novel\ncategories without any training examples. We also propose a new experimental\nprotocol for ZSD based on the highly challenging ILSVRC dataset, adhering to\npractical issues, e.g., the rarity of unseen objects. To the best of our\nknowledge, this is the first end-to-end deep network for ZSD that jointly\nmodels the interplay between visual and semantic domain information. To\novercome the noise in the automatically derived semantic descriptions, we\nutilize the concept of meta-classes to design an original loss function that\nachieves synergy between max-margin class separation and semantic space\nclustering. Furthermore, we present a baseline approach extended from\nrecognition to detection setting. Our extensive experiments show significant\nperformance boost over the baseline on the imperative yet difficult ZSD\nproblem.\n", "versions": [{"version": "v1", "created": "Fri, 16 Mar 2018 01:18:29 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Rahman", "Shafin", ""], ["Khan", "Salman", ""], ["Porikli", "Fatih", ""]]}, {"id": "1803.06051", "submitter": "Shafin Rahman", "authors": "Shafin Rahman and Salman Khan", "title": "Deep Multiple Instance Learning for Zero-shot Image Tagging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In-line with the success of deep learning on traditional recognition problem,\nseveral end-to-end deep models for zero-shot recognition have been proposed in\nthe literature. These models are successful to predict a single unseen label\ngiven an input image, but does not scale to cases where multiple unseen objects\nare present. In this paper, we model this problem within the framework of\nMultiple Instance Learning (MIL). To the best of our knowledge, we propose the\nfirst end-to-end trainable deep MIL framework for the multi-label zero-shot\ntagging problem. Due to its novel design, the proposed framework has several\ninteresting features: (1) Unlike previous deep MIL models, it does not use any\noff-line procedure (e.g., Selective Search or EdgeBoxes) for bag generation.\n(2) During test time, it can process any number of unseen labels given their\nsemantic embedding vectors. (3) Using only seen labels per image as weak\nannotation, it can produce a bounding box for each predicted labels. We\nexperiment with the NUS-WIDE dataset and achieve superior performance across\nconventional, zero-shot and generalized zero-shot tagging tasks.\n", "versions": [{"version": "v1", "created": "Fri, 16 Mar 2018 01:25:04 GMT"}], "update_date": "2018-03-19", "authors_parsed": [["Rahman", "Shafin", ""], ["Khan", "Salman", ""]]}, {"id": "1803.06067", "submitter": "Xiaodan Liang", "authors": "Xiaodan Liang and Hongfei Zhou and Eric Xing", "title": "Dynamic-structured Semantic Propagation Network", "comments": "CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic concept hierarchy is still under-explored for semantic segmentation\ndue to the inefficiency and complicated optimization of incorporating\nstructural inference into dense prediction. This lack of modeling semantic\ncorrelations also makes prior works must tune highly-specified models for each\ntask due to the label discrepancy across datasets. It severely limits the\ngeneralization capability of segmentation models for open set concept\nvocabulary and annotation utilization. In this paper, we propose a\nDynamic-Structured Semantic Propagation Network (DSSPN) that builds a semantic\nneuron graph by explicitly incorporating the semantic concept hierarchy into\nnetwork construction. Each neuron represents the instantiated module for\nrecognizing a specific type of entity such as a super-class (e.g. food) or a\nspecific concept (e.g. pizza). During training, DSSPN performs the\ndynamic-structured neuron computation graph by only activating a sub-graph of\nneurons for each image in a principled way. A dense semantic-enhanced neural\nblock is proposed to propagate the learned knowledge of all ancestor neurons\ninto each fine-grained child neuron for feature evolving. Another merit of such\nsemantic explainable structure is the ability of learning a unified model\nconcurrently on diverse datasets by selectively activating different neuron\nsub-graphs for each annotation at each step. Extensive experiments on four\npublic semantic segmentation datasets (i.e. ADE20K, COCO-Stuff, Cityscape and\nMapillary) demonstrate the superiority of our DSSPN over state-of-the-art\nsegmentation models. Moreoever, we demonstrate a universal segmentation model\nthat is jointly trained on diverse datasets can surpass the performance of the\ncommon fine-tuning scheme for exploiting multiple domain knowledge.\n", "versions": [{"version": "v1", "created": "Fri, 16 Mar 2018 03:28:22 GMT"}], "update_date": "2018-03-19", "authors_parsed": [["Liang", "Xiaodan", ""], ["Zhou", "Hongfei", ""], ["Xing", "Eric", ""]]}, {"id": "1803.06077", "submitter": "Albert Davies", "authors": "Iljoo Baek, Albert Davies, Geng Yan, and Ragunathan (Raj) Rajkumar", "title": "Real-time Detection, Tracking, and Classification of Moving and\n  Stationary Objects using Multiple Fisheye Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to detect pedestrians and other moving objects is crucial for an\nautonomous vehicle. This must be done in real-time with minimum system\noverhead. This paper discusses the implementation of a surround view system to\nidentify moving as well as static objects that are close to the ego vehicle.\nThe algorithm works on 4 views captured by fisheye cameras which are merged\ninto a single frame. The moving object detection and tracking solution uses\nminimal system overhead to isolate regions of interest (ROIs) containing moving\nobjects. These ROIs are then analyzed using a deep neural network (DNN) to\ncategorize the moving object. With deployment and testing on a real car in\nurban environments, we have demonstrated the practical feasibility of the\nsolution. The video demos of our algorithm have been uploaded to Youtube:\nhttps://youtu.be/vpoCfC724iA, https://youtu.be/2X4aqH2bMBs\n", "versions": [{"version": "v1", "created": "Fri, 16 Mar 2018 05:29:12 GMT"}, {"version": "v2", "created": "Fri, 31 Aug 2018 09:16:33 GMT"}], "update_date": "2018-09-03", "authors_parsed": [["Baek", "Iljoo", "", "Raj"], ["Davies", "Albert", "", "Raj"], ["Yan", "Geng", "", "Raj"], ["Ragunathan", "", "", "Raj"], ["Rajkumar", "", ""]]}, {"id": "1803.06091", "submitter": "Ming-Ming Cheng Prof.", "authors": "Deng-Ping Fan and Ming-Ming Cheng and Jiang-Jiang Liu and Shang-Hua\n  Gao and Qibin Hou and Ali Borji", "title": "Salient Objects in Clutter: Bringing Salient Object Detection to the\n  Foreground", "comments": "ECCV 2018", "journal-ref": "ECCV 2018", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a comprehensive evaluation of salient object detection (SOD)\nmodels. Our analysis identifies a serious design bias of existing SOD datasets\nwhich assumes that each image contains at least one clearly outstanding salient\nobject in low clutter. The design bias has led to a saturated high performance\nfor state-of-the-art SOD models when evaluated on existing datasets. The\nmodels, however, still perform far from being satisfactory when applied to\nreal-world daily scenes. Based on our analyses, we first identify 7 crucial\naspects that a comprehensive and balanced dataset should fulfill. Then, we\npropose a new high quality dataset and update the previous saliency benchmark.\nSpecifically, our SOC (Salient Objects in Clutter) dataset, includes images\nwith salient and non-salient objects from daily object categories. Beyond\nobject category annotations, each salient image is accompanied by attributes\nthat reflect common challenges in real-world scenes. Finally, we report\nattribute-based performance assessment on our dataset.\n", "versions": [{"version": "v1", "created": "Fri, 16 Mar 2018 06:52:22 GMT"}, {"version": "v2", "created": "Sun, 22 Jul 2018 10:46:02 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Fan", "Deng-Ping", ""], ["Cheng", "Ming-Ming", ""], ["Liu", "Jiang-Jiang", ""], ["Gao", "Shang-Hua", ""], ["Hou", "Qibin", ""], ["Borji", "Ali", ""]]}, {"id": "1803.06092", "submitter": "Guangyu Robert Yang", "authors": "Guangyu Robert Yang, Igor Ganichev, Xiao-Jing Wang, Jonathon Shlens,\n  David Sussillo", "title": "A Dataset and Architecture for Visual Reasoning with a Working Memory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A vexing problem in artificial intelligence is reasoning about events that\noccur in complex, changing visual stimuli such as in video analysis or game\nplay. Inspired by a rich tradition of visual reasoning and memory in cognitive\npsychology and neuroscience, we developed an artificial, configurable visual\nquestion and answer dataset (COG) to parallel experiments in humans and\nanimals. COG is much simpler than the general problem of video analysis, yet it\naddresses many of the problems relating to visual and logical reasoning and\nmemory -- problems that remain challenging for modern deep learning\narchitectures. We additionally propose a deep learning architecture that\nperforms competitively on other diagnostic VQA datasets (i.e. CLEVR) as well as\neasy settings of the COG dataset. However, several settings of COG result in\ndatasets that are progressively more challenging to learn. After training, the\nnetwork can zero-shot generalize to many new tasks. Preliminary analyses of the\nnetwork architectures trained on COG demonstrate that the network accomplishes\nthe task in a manner interpretable to humans.\n", "versions": [{"version": "v1", "created": "Fri, 16 Mar 2018 06:53:45 GMT"}, {"version": "v2", "created": "Fri, 20 Jul 2018 14:12:49 GMT"}], "update_date": "2018-07-23", "authors_parsed": [["Yang", "Guangyu Robert", ""], ["Ganichev", "Igor", ""], ["Wang", "Xiao-Jing", ""], ["Shlens", "Jonathon", ""], ["Sussillo", "David", ""]]}, {"id": "1803.06107", "submitter": "Max Liu", "authors": "Kanglin Liu, Guoping Qiu", "title": "Lipschitz Constrained GANs via Boundedness and Continuity", "comments": "20 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the challenges in the study of Generative Adversarial Networks (GANs)\nis the difficulty of its performance control. Lipschitz constraint is essential\nin guaranteeing training stability for GANs. Although heuristic methods such as\nweight clipping, gradient penalty and spectral normalization have been proposed\nto enforce Lipschitz constraint, it is still difficult to achieve a solution\nthat is both practically effective and theoretically provably satisfying a\nLipschitz constraint. In this paper, we introduce the boundedness and\ncontinuity ($BC$) conditions to enforce the Lipschitz constraint on the\ndiscriminator functions of GANs. We prove theoretically that GANs with\ndiscriminators meeting the BC conditions satisfy the Lipschitz constraint. We\npresent a practically very effective implementation of a GAN based on a\nconvolutional neural network (CNN) by forcing the CNN to satisfy the $BC$\nconditions (BC-GAN). We show that as compared to recent techniques including\ngradient penalty and spectral normalization, BC-GANs not only have better\nperformances but also lower computational complexity.\n", "versions": [{"version": "v1", "created": "Fri, 16 Mar 2018 08:15:09 GMT"}, {"version": "v2", "created": "Mon, 8 Oct 2018 08:37:00 GMT"}, {"version": "v3", "created": "Wed, 23 Dec 2020 00:49:13 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Liu", "Kanglin", ""], ["Qiu", "Guoping", ""]]}, {"id": "1803.06131", "submitter": "Michael Tschannen", "authors": "Robert Torfason, Fabian Mentzer, Eirikur Agustsson, Michael Tschannen,\n  Radu Timofte, Luc Van Gool", "title": "Towards Image Understanding from Deep Compression without Decoding", "comments": "ICLR 2018 conference paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by recent work on deep neural network (DNN)-based image compression\nmethods showing potential improvements in image quality, savings in storage,\nand bandwidth reduction, we propose to perform image understanding tasks such\nas classification and segmentation directly on the compressed representations\nproduced by these compression methods. Since the encoders and decoders in\nDNN-based compression methods are neural networks with feature-maps as internal\nrepresentations of the images, we directly integrate these with architectures\nfor image understanding. This bypasses decoding of the compressed\nrepresentation into RGB space and reduces computational cost. Our study shows\nthat accuracies comparable to networks that operate on compressed RGB images\ncan be achieved while reducing the computational complexity up to $2\\times$.\nFurthermore, we show that synergies are obtained by jointly training\ncompression networks with classification networks on the compressed\nrepresentations, improving image quality, classification accuracy, and\nsegmentation performance. We find that inference from compressed\nrepresentations is particularly advantageous compared to inference from\ncompressed RGB images for aggressive compression rates.\n", "versions": [{"version": "v1", "created": "Fri, 16 Mar 2018 09:51:09 GMT"}], "update_date": "2018-03-19", "authors_parsed": [["Torfason", "Robert", ""], ["Mentzer", "Fabian", ""], ["Agustsson", "Eirikur", ""], ["Tschannen", "Michael", ""], ["Timofte", "Radu", ""], ["Van Gool", "Luc", ""]]}, {"id": "1803.06141", "submitter": "Hossein Kashiyani", "authors": "Hossein Kashiyani, Shahriar B. Shokouhi", "title": "Patchwise object tracking via structural local sparse appearance model", "comments": "6 pages, 3 figures, Accepted by ICCKE 2017", "journal-ref": null, "doi": "10.1109/ICCKE.2017.8167940", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we propose a robust visual tracking method which exploits the\nrelationships of targets in adjacent frames using patchwise joint sparse\nrepresentation. Two sets of overlapping patches with different sizes are\nextracted from target candidates to construct two dictionaries with\nconsideration of joint sparse representation. By applying this representation\ninto structural sparse appearance model, we can take two-fold advantages.\nFirst, the correlation of target patches over time is considered. Second, using\nthis local appearance model with different patch sizes takes into account local\nfeatures of target thoroughly. Furthermore, the position of candidate patches\nand their occlusion levels are utilized simultaneously to obtain the final\nlikelihood of target candidates. Evaluations on recent challenging benchmark\nshow that our tracking method outperforms the state-of-the-art trackers.\n", "versions": [{"version": "v1", "created": "Fri, 16 Mar 2018 10:06:37 GMT"}], "update_date": "2018-03-19", "authors_parsed": [["Kashiyani", "Hossein", ""], ["Shokouhi", "Shahriar B.", ""]]}, {"id": "1803.06152", "submitter": "Anh Nguyen", "authors": "Anh Nguyen, Thanh-Toan Do, Ian Reid, Darwin G. Caldwell, Nikos G.\n  Tsagarakis", "title": "Object Captioning and Retrieval with Natural Language", "comments": "8 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of jointly learning vision and language to understand\nthe object in a fine-grained manner. The key idea of our approach is the use of\nobject descriptions to provide the detailed understanding of an object. Based\non this idea, we propose two new architectures to solve two related problems:\nobject captioning and natural language-based object retrieval. The goal of the\nobject captioning task is to simultaneously detect the object and generate its\nassociated description, while in the object retrieval task, the goal is to\nlocalize an object given an input query. We demonstrate that both problems can\nbe solved effectively using hybrid end-to-end CNN-LSTM networks. The\nexperimental results on our new challenging dataset show that our methods\noutperform recent methods by a fair margin, while providing a detailed\nunderstanding of the object and having fast inference time. The source code\nwill be made available.\n", "versions": [{"version": "v1", "created": "Fri, 16 Mar 2018 10:32:25 GMT"}], "update_date": "2018-03-19", "authors_parsed": [["Nguyen", "Anh", ""], ["Do", "Thanh-Toan", ""], ["Reid", "Ian", ""], ["Caldwell", "Darwin G.", ""], ["Tsagarakis", "Nikos G.", ""]]}, {"id": "1803.06167", "submitter": "Stergios Christodoulidis Mr.", "authors": "Marios Anthimopoulos, Stergios Christodoulidis, Lukas Ebner, Thomas\n  Geiser, Andreas Christe, Stavroula Mougiakakou", "title": "Semantic Segmentation of Pathological Lung Tissue with Dilated Fully\n  Convolutional Networks", "comments": null, "journal-ref": null, "doi": "10.1109/JBHI.2018.2818620", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Early and accurate diagnosis of interstitial lung diseases (ILDs) is crucial\nfor making treatment decisions, but can be challenging even for experienced\nradiologists. The diagnostic procedure is based on the detection and\nrecognition of the different ILD pathologies in thoracic CT scans, yet their\nmanifestation often appears similar. In this study, we propose the use of a\ndeep purely convolutional neural network for the semantic segmentation of ILD\npatterns, as the basic component of a computer aided diagnosis (CAD) system for\nILDs. The proposed CNN, which consists of convolutional layers with dilated\nfilters, takes as input a lung CT image of arbitrary size and outputs the\ncorresponding label map. We trained and tested the network on a dataset of 172\nsparsely annotated CT scans, within a cross-validation scheme. The training was\nperformed in an end-to-end and semi-supervised fashion, utilizing both labeled\nand non-labeled image regions. The experimental results show significant\nperformance improvement with respect to the state of the art.\n", "versions": [{"version": "v1", "created": "Fri, 16 Mar 2018 11:23:38 GMT"}], "update_date": "2018-04-02", "authors_parsed": [["Anthimopoulos", "Marios", ""], ["Christodoulidis", "Stergios", ""], ["Ebner", "Lukas", ""], ["Geiser", "Thomas", ""], ["Christe", "Andreas", ""], ["Mougiakakou", "Stavroula", ""]]}, {"id": "1803.06184", "submitter": "Xinyu Huang", "authors": "Xinyu Huang, Peng Wang, Xinjing Cheng, Dingfu Zhou, Qichuan Geng,\n  Ruigang Yang", "title": "The ApolloScape Open Dataset for Autonomous Driving and its Application", "comments": "Version 4: Accepted by TPAMI. Version 3: 17 pages, 10 tables, 11\n  figures, added the application (DeLS-3D) based on the ApolloScape Dataset.\n  Version 2: 7 pages, 6 figures, added comparison with BDD100K dataset", "journal-ref": null, "doi": "10.1109/TPAMI.2019.2926463", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous driving has attracted tremendous attention especially in the past\nfew years. The key techniques for a self-driving car include solving tasks like\n3D map construction, self-localization, parsing the driving road and\nunderstanding objects, which enable vehicles to reason and act. However, large\nscale data set for training and system evaluation is still a bottleneck for\ndeveloping robust perception models. In this paper, we present the ApolloScape\ndataset [1] and its applications for autonomous driving. Compared with existing\npublic datasets from real scenes, e.g. KITTI [2] or Cityscapes [3], ApolloScape\ncontains much large and richer labelling including holistic semantic dense\npoint cloud for each site, stereo, per-pixel semantic labelling, lanemark\nlabelling, instance segmentation, 3D car instance, high accurate location for\nevery frame in various driving videos from multiple sites, cities and daytimes.\nFor each task, it contains at lease 15x larger amount of images than SOTA\ndatasets. To label such a complete dataset, we develop various tools and\nalgorithms specified for each task to accelerate the labelling process, such as\n3D-2D segment labeling tools, active labelling in videos etc. Depend on\nApolloScape, we are able to develop algorithms jointly consider the learning\nand inference of multiple tasks. In this paper, we provide a sensor fusion\nscheme integrating camera videos, consumer-grade motion sensors (GPS/IMU), and\na 3D semantic map in order to achieve robust self-localization and semantic\nsegmentation for autonomous driving. We show that practically, sensor fusion\nand joint learning of multiple tasks are beneficial to achieve a more robust\nand accurate system. We expect our dataset and proposed relevant algorithms can\nsupport and motivate researchers for further development of multi-sensor fusion\nand multi-task learning in the field of computer vision.\n", "versions": [{"version": "v1", "created": "Fri, 16 Mar 2018 12:15:58 GMT"}, {"version": "v2", "created": "Thu, 12 Jul 2018 10:11:43 GMT"}, {"version": "v3", "created": "Wed, 26 Sep 2018 07:50:34 GMT"}, {"version": "v4", "created": "Thu, 4 Jul 2019 04:10:39 GMT"}], "update_date": "2019-07-05", "authors_parsed": [["Huang", "Xinyu", ""], ["Wang", "Peng", ""], ["Cheng", "Xinjing", ""], ["Zhou", "Dingfu", ""], ["Geng", "Qichuan", ""], ["Yang", "Ruigang", ""]]}, {"id": "1803.06189", "submitter": "He Xinwei", "authors": "Xinwei He, Yang Zhou, Zhichao Zhou, Song Bai, Xiang Bai", "title": "Triplet-Center Loss for Multi-View 3D Object Retrieval", "comments": "accepted by CVPR2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most existing 3D object recognition algorithms focus on leveraging the strong\ndiscriminative power of deep learning models with softmax loss for the\nclassification of 3D data, while learning discriminative features with deep\nmetric learning for 3D object retrieval is more or less neglected. In the\npaper, we study variants of deep metric learning losses for 3D object\nretrieval, which did not receive enough attention from this area. First , two\nkinds of representative losses, triplet loss and center loss, are introduced\nwhich could learn more discriminative features than traditional classification\nloss. Then, we propose a novel loss named triplet-center loss, which can\nfurther enhance the discriminative power of the features. The proposed\ntriplet-center loss learns a center for each class and requires that the\ndistances between samples and centers from the same class are closer than those\nfrom different classes. Extensive experimental results on two popular 3D object\nretrieval benchmarks and two widely-adopted sketch-based 3D shape retrieval\nbenchmarks consistently demonstrate the effectiveness of our proposed loss, and\nsignificant improvements have been achieved compared with the\nstate-of-the-arts.\n", "versions": [{"version": "v1", "created": "Fri, 16 Mar 2018 12:31:24 GMT"}], "update_date": "2018-03-19", "authors_parsed": [["He", "Xinwei", ""], ["Zhou", "Yang", ""], ["Zhou", "Zhichao", ""], ["Bai", "Song", ""], ["Bai", "Xiang", ""]]}, {"id": "1803.06192", "submitter": "Stefan Milz", "authors": "Varun Ravi Kumar, Stefan Milz, Martin Simon, Christian Witt, Karl\n  Amende, Johannes Petzold, Senthil Yogamani, Timo Pech", "title": "Monocular Fisheye Camera Depth Estimation Using Sparse LiDAR Supervision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Near field depth estimation around a self driving car is an important\nfunction that can be achieved by four wide angle fisheye cameras having a field\nof view of over 180. Depth estimation based on convolutional neural networks\n(CNNs) produce state of the art results, but progress is hindered because depth\nannotation cannot be obtained manually. Synthetic datasets are commonly used\nbut they have limitations. For instance, they do not capture the extensive\nvariability in the appearance of objects like vehicles present in real\ndatasets. There is also a domain shift while performing inference on natural\nimages illustrated by many attempts to handle the domain adaptation explicitly.\nIn this work, we explore an alternate approach of training using sparse LiDAR\ndata as ground truth for depth estimation for fisheye camera. We built our own\ndataset using our self driving car setup which has a 64 beam Velodyne LiDAR and\nfour wide angle fisheye cameras. To handle the difference in view points of\nLiDAR and fisheye camera, an occlusion resolution mechanism was implemented. We\nstarted with Eigen's multiscale convolutional network architecture and improved\nby modifying activation function and optimizer. We obtained promising results\non our dataset with RMSE errors comparable to the state of the art results\nobtained on KITTI.\n", "versions": [{"version": "v1", "created": "Fri, 16 Mar 2018 12:40:18 GMT"}, {"version": "v2", "created": "Mon, 19 Mar 2018 08:31:15 GMT"}, {"version": "v3", "created": "Mon, 24 Sep 2018 13:21:29 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Kumar", "Varun Ravi", ""], ["Milz", "Stefan", ""], ["Simon", "Martin", ""], ["Witt", "Christian", ""], ["Amende", "Karl", ""], ["Petzold", "Johannes", ""], ["Yogamani", "Senthil", ""], ["Pech", "Timo", ""]]}, {"id": "1803.06199", "submitter": "Stefan Milz", "authors": "Martin Simon, Stefan Milz, Karl Amende, Horst-Michael Gross", "title": "Complex-YOLO: Real-time 3D Object Detection on Point Clouds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lidar based 3D object detection is inevitable for autonomous driving, because\nit directly links to environmental understanding and therefore builds the base\nfor prediction and motion planning. The capacity of inferencing highly sparse\n3D data in real-time is an ill-posed problem for lots of other application\nareas besides automated vehicles, e.g. augmented reality, personal robotics or\nindustrial automation. We introduce Complex-YOLO, a state of the art real-time\n3D object detection network on point clouds only. In this work, we describe a\nnetwork that expands YOLOv2, a fast 2D standard object detector for RGB images,\nby a specific complex regression strategy to estimate multi-class 3D boxes in\nCartesian space. Thus, we propose a specific Euler-Region-Proposal Network\n(E-RPN) to estimate the pose of the object by adding an imaginary and a real\nfraction to the regression network. This ends up in a closed complex space and\navoids singularities, which occur by single angle estimations. The E-RPN\nsupports to generalize well during training. Our experiments on the KITTI\nbenchmark suite show that we outperform current leading methods for 3D object\ndetection specifically in terms of efficiency. We achieve state of the art\nresults for cars, pedestrians and cyclists by being more than five times faster\nthan the fastest competitor. Further, our model is capable of estimating all\neight KITTI-classes, including Vans, Trucks or sitting pedestrians\nsimultaneously with high accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 16 Mar 2018 12:54:40 GMT"}, {"version": "v2", "created": "Mon, 24 Sep 2018 09:27:33 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Simon", "Martin", ""], ["Milz", "Stefan", ""], ["Amende", "Karl", ""], ["Gross", "Horst-Michael", ""]]}, {"id": "1803.06252", "submitter": "Manuel Carbonell", "authors": "Manuel Carbonell, Mauricio Villegas, Alicia Forn\\'es, Josep Llad\\'os", "title": "Joint Recognition of Handwritten Text and Named Entities with a Neural\n  End-to-end Model", "comments": "To appear in IAPR International Workshop on Document Analysis Systems\n  2018 (DAS 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When extracting information from handwritten documents, text transcription\nand named entity recognition are usually faced as separate subsequent tasks.\nThis has the disadvantage that errors in the first module affect heavily the\nperformance of the second module. In this work we propose to do both tasks\njointly, using a single neural network with a common architecture used for\nplain text recognition. Experimentally, the work has been tested on a\ncollection of historical marriage records. Results of experiments are presented\nto show the effect on the performance for different configurations: different\nways of encoding the information, doing or not transfer learning and processing\nat text line or multi-line region level. The results are comparable to state of\nthe art reported in the ICDAR 2017 Information Extraction competition, even\nthough the proposed technique does not use any dictionaries, language modeling\nor post processing.\n", "versions": [{"version": "v1", "created": "Fri, 16 Mar 2018 14:47:58 GMT"}, {"version": "v2", "created": "Thu, 22 Mar 2018 12:27:08 GMT"}], "update_date": "2018-03-23", "authors_parsed": [["Carbonell", "Manuel", ""], ["Villegas", "Mauricio", ""], ["Forn\u00e9s", "Alicia", ""], ["Llad\u00f3s", "Josep", ""]]}, {"id": "1803.06253", "submitter": "Diego Marcos", "authors": "Diego Marcos, Michele Volpi, Benjamin Kellenberger and Devis Tuia", "title": "Land cover mapping at very high resolution with rotation equivariant\n  CNNs: towards small yet accurate models", "comments": null, "journal-ref": "ISPRS Journal of Photogrammetry and Remote Sensing, 2018", "doi": "10.1016/j.isprsjprs.2018.01.021", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In remote sensing images, the absolute orientation of objects is arbitrary.\nDepending on an object's orientation and on a sensor's flight path, objects of\nthe same semantic class can be observed in different orientations in the same\nimage. Equivariance to rotation, in this context understood as responding with\na rotated semantic label map when subject to a rotation of the input image, is\ntherefore a very desirable feature, in particular for high capacity models,\nsuch as Convolutional Neural Networks (CNNs). If rotation equivariance is\nencoded in the network, the model is confronted with a simpler task and does\nnot need to learn specific (and redundant) weights to address rotated versions\nof the same object class. In this work we propose a CNN architecture called\nRotation Equivariant Vector Field Network (RotEqNet) to encode rotation\nequivariance in the network itself. By using rotating convolutions as building\nblocks and passing only the the values corresponding to the maximally\nactivating orientation throughout the network in the form of orientation\nencoding vector fields, RotEqNet treats rotated versions of the same object\nwith the same filter bank and therefore achieves state-of-the-art performances\neven when using very small architectures trained from scratch. We test RotEqNet\nin two challenging sub-decimeter resolution semantic labeling problems, and\nshow that we can perform better than a standard CNN while requiring one order\nof magnitude less parameters.\n", "versions": [{"version": "v1", "created": "Fri, 16 Mar 2018 14:48:58 GMT"}], "update_date": "2018-03-19", "authors_parsed": [["Marcos", "Diego", ""], ["Volpi", "Michele", ""], ["Kellenberger", "Benjamin", ""], ["Tuia", "Devis", ""]]}, {"id": "1803.06267", "submitter": "Boris Bukh", "authors": "Boris Bukh, Xavier Goaoc, Alfredo Hubard, Matthew Trager", "title": "Consistent sets of lines with no colorful incidence", "comments": "20 pages, 4 color figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.CV math.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider incidences among colored sets of lines in $\\mathbb{R}^d$ and\nexamine whether the existence of certain concurrences between lines of $k$\ncolors force the existence of at least one concurrence between lines of $k+1$\ncolors. This question is relevant for problems in 3D reconstruction in computer\nvision.\n", "versions": [{"version": "v1", "created": "Fri, 16 Mar 2018 15:25:43 GMT"}], "update_date": "2018-03-19", "authors_parsed": [["Bukh", "Boris", ""], ["Goaoc", "Xavier", ""], ["Hubard", "Alfredo", ""], ["Trager", "Matthew", ""]]}, {"id": "1803.06301", "submitter": "Ruud Barth", "authors": "Ruud Barth, Jochen Hemming, Eldert J. van Henten", "title": "Improved Part Segmentation Performance by Optimising Realism of\n  Synthetic Images using Cycle Generative Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In this paper we report on improved part segmentation performance using\nconvolutional neural networks to reduce the dependency on the large amount of\nmanually annotated empirical images. This was achieved by optimising the visual\nrealism of synthetic agricultural images.In Part I, a cycle consistent\ngenerative adversarial network was applied to synthetic and empirical images\nwith the objective to generate more realistic synthetic images by translating\nthem to the empirical domain. We first hypothesise and confirm that plant part\nimage features such as color and texture become more similar to the empirical\ndomain after translation of the synthetic images.Results confirm this with an\nimproved mean color distribution correlation with the empirical data prior of\n0.62 and post translation of 0.90. Furthermore, the mean image features of\ncontrast, homogeneity, energy and entropy moved closer to the empirical mean,\npost translation. In Part II, 7 experiments were performed using convolutional\nneural networks with different combinations of synthetic, synthetic translated\nto empirical and empirical images. We hypothesised that the translated images\ncan be used for (i) improved learning of empirical images, and (ii) that\nlearning without any fine-tuning with empirical images is improved by\nbootstrapping with translated images over bootstrapping with synthetic images.\nResults confirm our second and third hypotheses. First a maximum\nintersection-over-union performance was achieved of 0.52 when bootstrapping\nwith translated images and fine-tuning with empirical images; an 8% increase\ncompared to only using synthetic images. Second, training without any empirical\nfine-tuning resulted in an average IOU of 0.31; a 55% performance increase over\nprevious methods that only used synthetic images.\n", "versions": [{"version": "v1", "created": "Fri, 16 Mar 2018 16:36:43 GMT"}], "update_date": "2018-03-19", "authors_parsed": [["Barth", "Ruud", ""], ["Hemming", "Jochen", ""], ["van Henten", "Eldert J.", ""]]}, {"id": "1803.06312", "submitter": "Mark Buckler", "authors": "Mark Buckler, Philip Bedoukian, Suren Jayasuriya, Adrian Sampson", "title": "EVA$^2$: Exploiting Temporal Redundancy in Live Computer Vision", "comments": "Appears in ISCA 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hardware support for deep convolutional neural networks (CNNs) is critical to\nadvanced computer vision in mobile and embedded devices. Current designs,\nhowever, accelerate generic CNNs; they do not exploit the unique\ncharacteristics of real-time vision. We propose to use the temporal redundancy\nin natural video to avoid unnecessary computation on most frames. A new\nalgorithm, activation motion compensation, detects changes in the visual input\nand incrementally updates a previously-computed output. The technique takes\ninspiration from video compression and applies well-known motion estimation\ntechniques to adapt to visual changes. We use an adaptive key frame rate to\ncontrol the trade-off between efficiency and vision quality as the input\nchanges. We implement the technique in hardware as an extension to existing\nstate-of-the-art CNN accelerator designs. The new unit reduces the average\nenergy per frame by 54.2%, 61.7%, and 87.6% for three CNNs with less than 1%\nloss in vision accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 16 Mar 2018 16:59:47 GMT"}, {"version": "v2", "created": "Tue, 17 Apr 2018 02:26:35 GMT"}], "update_date": "2018-04-18", "authors_parsed": [["Buckler", "Mark", ""], ["Bedoukian", "Philip", ""], ["Jayasuriya", "Suren", ""], ["Sampson", "Adrian", ""]]}, {"id": "1803.06316", "submitter": "Aj Piergiovanni", "authors": "AJ Piergiovanni and Michael S. Ryoo", "title": "Temporal Gaussian Mixture Layer for Videos", "comments": "ICML 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new convolutional layer named the Temporal Gaussian Mixture\n(TGM) layer and present how it can be used to efficiently capture longer-term\ntemporal information in continuous activity videos. The TGM layer is a temporal\nconvolutional layer governed by a much smaller set of parameters (e.g.,\nlocation/variance of Gaussians) that are fully differentiable. We present our\nfully convolutional video models with multiple TGM layers for activity\ndetection. The extensive experiments on multiple datasets, including Charades\nand MultiTHUMOS, confirm the effectiveness of TGM layers, significantly\noutperforming the state-of-the-arts.\n", "versions": [{"version": "v1", "created": "Fri, 16 Mar 2018 17:09:54 GMT"}, {"version": "v2", "created": "Tue, 20 Mar 2018 02:14:03 GMT"}, {"version": "v3", "created": "Mon, 1 Oct 2018 14:35:04 GMT"}, {"version": "v4", "created": "Wed, 30 Jan 2019 18:54:09 GMT"}, {"version": "v5", "created": "Tue, 14 May 2019 18:01:08 GMT"}, {"version": "v6", "created": "Thu, 1 Aug 2019 18:53:33 GMT"}], "update_date": "2019-08-05", "authors_parsed": [["Piergiovanni", "AJ", ""], ["Ryoo", "Michael S.", ""]]}, {"id": "1803.06320", "submitter": "Florian Bernard", "authors": "Florian Bernard, Johan Thunberg, Jorge Goncalves, Christian Theobalt", "title": "Synchronisation of Partial Multi-Matchings via Non-negative\n  Factorisations", "comments": null, "journal-ref": null, "doi": "10.1016/j.patcog.2019.03.021", "report-no": null, "categories": "cs.CV math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we study permutation synchronisation for the challenging case of\npartial permutations, which plays an important role for the problem of matching\nmultiple objects (e.g. images or shapes). The term synchronisation refers to\nthe property that the set of pairwise matchings is cycle-consistent, i.e. in\nthe full matching case all compositions of pairwise matchings over cycles must\nbe equal to the identity. Motivated by clustering and matrix factorisation\nperspectives of cycle-consistency, we derive an algorithm to tackle the\npermutation synchronisation problem based on non-negative factorisations. In\norder to deal with the inherent non-convexity of the permutation\nsynchronisation problem, we use an initialisation procedure based on a novel\nrotation scheme applied to the solution of the spectral relaxation. Moreover,\nthis rotation scheme facilitates a convenient Euclidean projection to obtain a\nbinary solution after solving our relaxed problem. In contrast to\nstate-of-the-art methods, our approach is guaranteed to produce\ncycle-consistent results. We experimentally demonstrate the efficacy of our\nmethod and show that it achieves better results compared to existing methods.\n", "versions": [{"version": "v1", "created": "Fri, 16 Mar 2018 17:17:05 GMT"}, {"version": "v2", "created": "Thu, 12 Jul 2018 07:01:49 GMT"}, {"version": "v3", "created": "Mon, 25 Mar 2019 13:33:00 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Bernard", "Florian", ""], ["Thunberg", "Johan", ""], ["Goncalves", "Jorge", ""], ["Theobalt", "Christian", ""]]}, {"id": "1803.06329", "submitter": "Diego Marcos", "authors": "Diego Marcos, Devis Tuia, Benjamin Kellenberger, Lisa Zhang, Min Bai,\n  Renjie Liao and Raquel Urtasun", "title": "Learning deep structured active contours end-to-end", "comments": "To appear, CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The world is covered with millions of buildings, and precisely knowing each\ninstance's position and extents is vital to a multitude of applications.\nRecently, automated building footprint segmentation models have shown superior\ndetection accuracy thanks to the usage of Convolutional Neural Networks (CNN).\nHowever, even the latest evolutions struggle to precisely delineating borders,\nwhich often leads to geometric distortions and inadvertent fusion of adjacent\nbuilding instances. We propose to overcome this issue by exploiting the\ndistinct geometric properties of buildings. To this end, we present Deep\nStructured Active Contours (DSAC), a novel framework that integrates priors and\nconstraints into the segmentation process, such as continuous boundaries,\nsmooth edges, and sharp corners. To do so, DSAC employs Active Contour Models\n(ACM), a family of constraint- and prior-based polygonal models. We learn ACM\nparameterizations per instance using a CNN, and show how to incorporate all\ncomponents in a structured output model, making DSAC trainable end-to-end. We\nevaluate DSAC on three challenging building instance segmentation datasets,\nwhere it compares favorably against state-of-the-art. Code will be made\navailable.\n", "versions": [{"version": "v1", "created": "Fri, 16 Mar 2018 17:30:37 GMT"}], "update_date": "2018-03-19", "authors_parsed": [["Marcos", "Diego", ""], ["Tuia", "Devis", ""], ["Kellenberger", "Benjamin", ""], ["Zhang", "Lisa", ""], ["Bai", "Min", ""], ["Liao", "Renjie", ""], ["Urtasun", "Raquel", ""]]}, {"id": "1803.06340", "submitter": "Renjiao Yi", "authors": "Renjiao Yi, Chenyang Zhu, Ping Tan, Stephen Lin", "title": "Faces as Lighting Probes via Unsupervised Deep Highlight Extraction", "comments": "42 pages, with supplementary material, to appear in ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for estimating detailed scene illumination using human\nfaces in a single image. In contrast to previous works that estimate lighting\nin terms of low-order basis functions or distant point lights, our technique\nestimates illumination at a higher precision in the form of a non-parametric\nenvironment map. Based on the observation that faces can exhibit strong\nhighlight reflections from a broad range of lighting directions, we propose a\ndeep neural network for extracting highlights from faces, and then trace these\nreflections back to the scene to acquire the environment map. Since real\ntraining data for highlight extraction is very limited, we introduce an\nunsupervised scheme for finetuning the network on real images, based on the\nconsistent diffuse chromaticity of a given face seen in multiple real images.\nIn tracing the estimated highlights to the environment, we reduce the blurring\neffect of skin reflectance on reflected light through a deconvolution\ndetermined by prior knowledge on face material properties. Comparisons to\nprevious techniques for highlight extraction and illumination estimation show\nthe state-of-the-art performance of this approach on a variety of indoor and\noutdoor scenes.\n", "versions": [{"version": "v1", "created": "Fri, 16 Mar 2018 17:53:06 GMT"}, {"version": "v2", "created": "Sat, 21 Jul 2018 00:06:47 GMT"}], "update_date": "2018-07-24", "authors_parsed": [["Yi", "Renjiao", ""], ["Zhu", "Chenyang", ""], ["Tan", "Ping", ""], ["Lin", "Stephen", ""]]}, {"id": "1803.06355", "submitter": "Tales Cesar De Oliveira Imbiriba", "authors": "Tales Imbiriba, Ricardo Augusto Borsoi, Jos\\'e Carlos Moreira Bermudez", "title": "A Low-rank Tensor Regularization Strategy for Hyperspectral Unmixing", "comments": null, "journal-ref": null, "doi": "10.1109/SSP.2018.8450853", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tensor-based methods have recently emerged as a more natural and effective\nformulation to address many problems in hyperspectral imaging. In hyperspectral\nunmixing (HU), low-rank constraints on the abundance maps have been shown to\nact as a regularization which adequately accounts for the multidimensional\nstructure of the underlying signal. However, imposing a strict low-rank\nconstraint for the abundance maps does not seem to be adequate, as important\ninformation that may be required to represent fine scale abundance behavior may\nbe discarded. This paper introduces a new low-rank tensor regularization that\nadequately captures the low-rank structure underlying the abundance maps\nwithout hindering the flexibility of the solution. Simulation results with\nsynthetic and real data show that the the extra flexibility introduced by the\nproposed regularization significantly improves the unmixing results.\n", "versions": [{"version": "v1", "created": "Fri, 16 Mar 2018 18:06:46 GMT"}], "update_date": "2018-11-14", "authors_parsed": [["Imbiriba", "Tales", ""], ["Borsoi", "Ricardo Augusto", ""], ["Bermudez", "Jos\u00e9 Carlos Moreira", ""]]}, {"id": "1803.06407", "submitter": "Calvin Murdock", "authors": "Calvin Murdock, Ming-Fang Chang, Simon Lucey", "title": "Deep Component Analysis via Alternating Direction Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite a lack of theoretical understanding, deep neural networks have\nachieved unparalleled performance in a wide range of applications. On the other\nhand, shallow representation learning with component analysis is associated\nwith rich intuition and theory, but smaller capacity often limits its\nusefulness. To bridge this gap, we introduce Deep Component Analysis (DeepCA),\nan expressive multilayer model formulation that enforces hierarchical structure\nthrough constraints on latent variables in each layer. For inference, we\npropose a differentiable optimization algorithm implemented using recurrent\nAlternating Direction Neural Networks (ADNNs) that enable parameter learning\nusing standard backpropagation. By interpreting feed-forward networks as\nsingle-iteration approximations of inference in our model, we provide both a\nnovel theoretical perspective for understanding them and a practical technique\nfor constraining predictions with prior knowledge. Experimentally, we\ndemonstrate performance improvements on a variety of tasks, including\nsingle-image depth prediction with sparse output constraints.\n", "versions": [{"version": "v1", "created": "Fri, 16 Mar 2018 21:40:02 GMT"}], "update_date": "2018-03-20", "authors_parsed": [["Murdock", "Calvin", ""], ["Chang", "Ming-Fang", ""], ["Lucey", "Simon", ""]]}, {"id": "1803.06414", "submitter": "Tal Remez", "authors": "Tal Remez, Jonathan Huang, Matthew Brown", "title": "Learning to Segment via Cut-and-Paste", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a weakly-supervised approach to object instance\nsegmentation. Starting with known or predicted object bounding boxes, we learn\nobject masks by playing a game of cut-and-paste in an adversarial learning\nsetup. A mask generator takes a detection box and Faster R-CNN features, and\nconstructs a segmentation mask that is used to cut-and-paste the object into a\nnew image location. The discriminator tries to distinguish between real\nobjects, and those cut and pasted via the generator, giving a learning signal\nthat leads to improved object masks. We verify our method experimentally using\nCityscapes, COCO, and aerial image datasets, learning to segment objects\nwithout ever having seen a mask in training. Our method exceeds the performance\nof existing weakly supervised methods, without requiring hand-tuned segment\nproposals, and reaches 90% of supervised performance.\n", "versions": [{"version": "v1", "created": "Fri, 16 Mar 2018 21:58:51 GMT"}], "update_date": "2018-03-20", "authors_parsed": [["Remez", "Tal", ""], ["Huang", "Jonathan", ""], ["Brown", "Matthew", ""]]}, {"id": "1803.06453", "submitter": "Sathya N. Ravi", "authors": "Sathya N. Ravi, Tuan Dinh, Vishnu Lokhande, Vikas Singh", "title": "Constrained Deep Learning using Conditional Gradient and Applications in\n  Computer Vision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A number of results have recently demonstrated the benefits of incorporating\nvarious constraints when training deep architectures in vision and machine\nlearning. The advantages range from guarantees for statistical generalization\nto better accuracy to compression. But support for general constraints within\nwidely used libraries remains scarce and their broader deployment within many\napplications that can benefit from them remains under-explored. Part of the\nreason is that Stochastic gradient descent (SGD), the workhorse for training\ndeep neural networks, does not natively deal with constraints with global scope\nvery well. In this paper, we revisit a classical first order scheme from\nnumerical optimization, Conditional Gradients (CG), that has, thus far had\nlimited applicability in training deep models. We show via rigorous analysis\nhow various constraints can be naturally handled by modifications of this\nalgorithm. We provide convergence guarantees and show a suite of immediate\nbenefits that are possible -- from training ResNets with fewer layers but\nbetter accuracy simply by substituting in our version of CG to faster training\nof GANs with 50% fewer epochs in image inpainting applications to provably\nbetter generalization guarantees using efficiently implementable forms of\nrecently proposed regularizers.\n", "versions": [{"version": "v1", "created": "Sat, 17 Mar 2018 03:59:34 GMT"}], "update_date": "2019-05-27", "authors_parsed": [["Ravi", "Sathya N.", ""], ["Dinh", "Tuan", ""], ["Lokhande", "Vishnu", ""], ["Singh", "Vikas", ""]]}, {"id": "1803.06459", "submitter": "Yen-Chang Hsu", "authors": "Yen-Chang Hsu, Zheng Xu, Zsolt Kira, Jiawei Huang", "title": "Learning to Cluster for Proposal-Free Instance Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work proposed a novel learning objective to train a deep neural network\nto perform end-to-end image pixel clustering. We applied the approach to\ninstance segmentation, which is at the intersection of image semantic\nsegmentation and object detection. We utilize the most fundamental property of\ninstance labeling -- the pairwise relationship between pixels -- as the\nsupervision to formulate the learning objective, then apply it to train a fully\nconvolutional network (FCN) for learning to perform pixel-wise clustering. The\nresulting clusters can be used as the instance labeling directly. To support\nlabeling of an unlimited number of instance, we further formulate ideas from\ngraph coloring theory into the proposed learning objective. The evaluation on\nthe Cityscapes dataset demonstrates strong performance and therefore proof of\nthe concept. Moreover, our approach won the second place in the lane detection\ncompetition of 2017 CVPR Autonomous Driving Challenge, and was the top\nperformer without using external data.\n", "versions": [{"version": "v1", "created": "Sat, 17 Mar 2018 04:35:21 GMT"}], "update_date": "2018-03-20", "authors_parsed": [["Hsu", "Yen-Chang", ""], ["Xu", "Zheng", ""], ["Kira", "Zsolt", ""], ["Huang", "Jiawei", ""]]}, {"id": "1803.06480", "submitter": "Santhosh Kelathodi", "authors": "Santhosh Kelathodi Kumaran, Debi Prosad Dogra, Partha Pratim Roy", "title": "Queuing Theory Guided Intelligent Traffic Scheduling through Video\n  Analysis using Dirichlet Process Mixture Model", "comments": null, "journal-ref": "Expert Systems with Applications Volume 118, 15 March 2019, Pages\n  169-181", "doi": "10.1016/j.eswa.2018.09.057", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate prediction of traffic signal duration for roadway junction is a\nchallenging problem due to the dynamic nature of traffic flows. Though\nsupervised learning can be used, parameters may vary across roadway junctions.\nIn this paper, we present a computer vision guided expert system that can learn\nthe departure rate of a given traffic junction modeled using traditional\nqueuing theory. First, we temporally group the optical flow of the moving\nvehicles using Dirichlet Process Mixture Model (DPMM). These groups are\nreferred to as tracklets or temporal clusters. Tracklet features are then used\nto learn the dynamic behavior of a traffic junction, especially during on/off\ncycles of a signal. The proposed queuing theory based approach can predict the\nsignal open duration for the next cycle with higher accuracy when compared with\nother popular features used for tracking. The hypothesis has been verified on\ntwo publicly available video datasets. The results reveal that the DPMM based\nfeatures are better than existing tracking frameworks to estimate $\\mu$. Thus,\nsignal duration prediction is more accurate when tested on these datasets.The\nmethod can be used for designing intelligent operator-independent traffic\ncontrol systems for roadway junctions at cities and highways.\n", "versions": [{"version": "v1", "created": "Sat, 17 Mar 2018 07:49:00 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["Kumaran", "Santhosh Kelathodi", ""], ["Dogra", "Debi Prosad", ""], ["Roy", "Partha Pratim", ""]]}, {"id": "1803.06490", "submitter": "Hongmin Li", "authors": "Hongmin Li, and Luping Shi", "title": "Robust event-stream pattern tracking based on correlative filter", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object tracking based on retina-inspired and event-based dynamic vision\nsensor (DVS) is challenging for the noise events, rapid change of event-stream\nshape, chaos of complex background textures, and occlusion. To address these\nchallenges, this paper presents a robust event-stream pattern tracking method\nbased on correlative filter mechanism. In the proposed method, rate coding is\nused to encode the event-stream object in each segment. Feature representations\nfrom hierarchical convolutional layers of a deep convolutional neural network\n(CNN) are used to represent the appearance of the rate encoded event-stream\nobject. The results prove that our method not only achieves good tracking\nperformance in many complicated scenes with noise events, complex background\ntextures, occlusion, and intersected trajectories, but also is robust to\nvariable scale, variable pose, and non-rigid deformations. In addition, this\ncorrelative filter based event-stream tracking has the advantage of high speed.\nThe proposed approach will promote the potential applications of these\nevent-based vision sensors in self-driving, robots and many other high-speed\nscenes.\n", "versions": [{"version": "v1", "created": "Sat, 17 Mar 2018 11:15:32 GMT"}], "update_date": "2018-03-20", "authors_parsed": [["Li", "Hongmin", ""], ["Shi", "Luping", ""]]}, {"id": "1803.06492", "submitter": "Bin Wang", "authors": "Bin Wang, Yanan Sun, Bing Xue and Mengjie Zhang", "title": "Evolving Deep Convolutional Neural Networks by Variable-length Particle\n  Swarm Optimization for Image Classification", "comments": "accepted by IEEE CEC 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) are one of the most effective deep\nlearning methods to solve image classification problems, but the best\narchitecture of a CNN to solve a specific problem can be extremely complicated\nand hard to design. This paper focuses on utilising Particle Swarm Optimisation\n(PSO) to automatically search for the optimal architecture of CNNs without any\nmanual work involved. In order to achieve the goal, three improvements are made\nbased on traditional PSO. First, a novel encoding strategy inspired by computer\nnetworks which empowers particle vectors to easily encode CNN layers is\nproposed; Second, in order to allow the proposed method to learn\nvariable-length CNN architectures, a Disabled layer is designed to hide some\ndimensions of the particle vector to achieve variable-length particles; Third,\nsince the learning process on large data is slow, partial datasets are randomly\npicked for the evaluation to dramatically speed it up. The proposed algorithm\nis examined and compared with 12 existing algorithms including the state-of-art\nmethods on three widely used image classification benchmark datasets. The\nexperimental results show that the proposed algorithm is a strong competitor to\nthe state-of-art algorithms in terms of classification error. This is the first\nwork using PSO for automatically evolving the architectures of CNNs.\n", "versions": [{"version": "v1", "created": "Sat, 17 Mar 2018 11:47:43 GMT"}], "update_date": "2018-03-20", "authors_parsed": [["Wang", "Bin", ""], ["Sun", "Yanan", ""], ["Xue", "Bing", ""], ["Zhang", "Mengjie", ""]]}, {"id": "1803.06503", "submitter": "Guanbin Li", "authors": "Guanbin Li, Yuan Xie, Liang Lin", "title": "Weakly Supervised Salient Object Detection Using Image Labels", "comments": "Accept by AAAI2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning based salient object detection has recently achieved great\nsuccess with its performance greatly outperforms any other unsupervised\nmethods. However, annotating per-pixel saliency masks is a tedious and\ninefficient procedure. In this paper, we note that superior salient object\ndetection can be obtained by iteratively mining and correcting the labeling\nambiguity on saliency maps from traditional unsupervised methods. We propose to\nuse the combination of a coarse salient object activation map from the\nclassification network and saliency maps generated from unsupervised methods as\npixel-level annotation, and develop a simple yet very effective algorithm to\ntrain fully convolutional networks for salient object detection supervised by\nthese noisy annotations. Our algorithm is based on alternately exploiting a\ngraphical model and training a fully convolutional network for model updating.\nThe graphical model corrects the internal labeling ambiguity through spatial\nconsistency and structure preserving while the fully convolutional network\nhelps to correct the cross-image semantic ambiguity and simultaneously update\nthe coarse activation map for next iteration. Experimental results demonstrate\nthat our proposed method greatly outperforms all state-of-the-art unsupervised\nsaliency detection methods and can be comparable to the current best\nstrongly-supervised methods training with thousands of pixel-level saliency map\nannotations on all public benchmarks.\n", "versions": [{"version": "v1", "created": "Sat, 17 Mar 2018 13:40:00 GMT"}], "update_date": "2018-03-20", "authors_parsed": [["Li", "Guanbin", ""], ["Xie", "Yuan", ""], ["Lin", "Liang", ""]]}, {"id": "1803.06506", "submitter": "Syed Ashar Javed", "authors": "Syed Ashar Javed, Shreyas Saxena and Vineet Gandhi", "title": "Learning Unsupervised Visual Grounding Through Semantic Self-Supervision", "comments": "NIPS Workshop 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Localizing natural language phrases in images is a challenging problem that\nrequires joint understanding of both the textual and visual modalities. In the\nunsupervised setting, lack of supervisory signals exacerbate this difficulty.\nIn this paper, we propose a novel framework for unsupervised visual grounding\nwhich uses concept learning as a proxy task to obtain self-supervision. The\nsimple intuition behind this idea is to encourage the model to localize to\nregions which can explain some semantic property in the data, in our case, the\nproperty being the presence of a concept in a set of images. We present\nthorough quantitative and qualitative experiments to demonstrate the efficacy\nof our approach and show a 5.6% improvement over the current state of the art\non Visual Genome dataset, a 5.8% improvement on the ReferItGame dataset and\ncomparable to state-of-art performance on the Flickr30k dataset.\n", "versions": [{"version": "v1", "created": "Sat, 17 Mar 2018 13:46:59 GMT"}, {"version": "v2", "created": "Wed, 5 Sep 2018 14:51:14 GMT"}, {"version": "v3", "created": "Fri, 16 Nov 2018 21:25:43 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Javed", "Syed Ashar", ""], ["Saxena", "Shreyas", ""], ["Gandhi", "Vineet", ""]]}, {"id": "1803.06508", "submitter": "Syed Ashar Javed", "authors": "Krishnam Gupta, Syed Ashar Javed, Vineet Gandhi and K. Madhava Krishna", "title": "MergeNet: A Deep Net Architecture for Small Obstacle Discovery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present here, a novel network architecture called MergeNet for discovering\nsmall obstacles for on-road scenes in the context of autonomous driving. The\nbasis of the architecture rests on the central consideration of training with\nless amount of data since the physical setup and the annotation process for\nsmall obstacles is hard to scale. For making effective use of the limited data,\nwe propose a multi-stage training procedure involving weight-sharing, separate\nlearning of low and high level features from the RGBD input and a refining\nstage which learns to fuse the obtained complementary features. The model is\ntrained and evaluated on the Lost and Found dataset and is able to achieve\nstate-of-art results with just 135 images in comparison to the 1000 images used\nby the previous benchmark. Additionally, we also compare our results with\nrecent methods trained on 6000 images and show that our method achieves\ncomparable performance with only 1000 training samples.\n", "versions": [{"version": "v1", "created": "Sat, 17 Mar 2018 14:07:34 GMT"}], "update_date": "2018-03-20", "authors_parsed": [["Gupta", "Krishnam", ""], ["Javed", "Syed Ashar", ""], ["Gandhi", "Vineet", ""], ["Krishna", "K. Madhava", ""]]}, {"id": "1803.06524", "submitter": "Wei Hu", "authors": "Wei Hu, Yangyu Huang, Fan Zhang, Ruirui Li, Wei Li, Guodong Yuan", "title": "SeqFace: Make full use of sequence information for face recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks (CNNs) have greatly improved the Face\nRecognition (FR) performance in recent years. Almost all CNNs in FR are trained\non the carefully labeled datasets containing plenty of identities. However,\nsuch high-quality datasets are very expensive to collect, which restricts many\nresearchers to achieve state-of-the-art performance. In this paper, we propose\na framework, called SeqFace, for learning discriminative face features. Besides\na traditional identity training dataset, the designed SeqFace can train CNNs by\nusing an additional dataset which includes a large number of face sequences\ncollected from videos. Moreover, the label smoothing regularization (LSR) and a\nnew proposed discriminative sequence agent (DSA) loss are employed to enhance\ndiscrimination power of deep face features via making full use of the sequence\ndata. Our method achieves excellent performance on Labeled Faces in the Wild\n(LFW), YouTube Faces (YTF), only with a single ResNet. The code and models are\npublicly available on-line (https://github.com/huangyangyu/SeqFace).\n", "versions": [{"version": "v1", "created": "Sat, 17 Mar 2018 15:49:07 GMT"}, {"version": "v2", "created": "Sat, 24 Mar 2018 02:55:59 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Hu", "Wei", ""], ["Huang", "Yangyu", ""], ["Zhang", "Fan", ""], ["Li", "Ruirui", ""], ["Li", "Wei", ""], ["Yuan", "Guodong", ""]]}, {"id": "1803.06541", "submitter": "Mahaman-Sani Chaibou", "authors": "Mahaman Sani Chaibou, Pierre-Henri Conze, Karim Kalti, Basel Solaiman,\n  Mohamed Ali Mahjoub", "title": "Adaptive strategy for superpixel-based region-growing image segmentation", "comments": null, "journal-ref": "J. of Electronic Imaging, 26(6), 061605 (2017)", "doi": "10.1117/1.JEI.26.6.061605", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents a region-growing image segmentation approach based on\nsuperpixel decomposition. From an initial contour-constrained over-segmentation\nof the input image, the image segmentation is achieved by iteratively merging\nsimilar superpixels into regions. This approach raises two key issues: (1) how\nto compute the similarity between superpixels in order to perform accurate\nmerging and (2) in which order those superpixels must be merged together. In\nthis perspective, we firstly introduce a robust adaptive multi-scale superpixel\nsimilarity in which region comparisons are made both at content and common\nborder level. Secondly, we propose a global merging strategy to efficiently\nguide the region merging process. Such strategy uses an adpative merging\ncriterion to ensure that best region aggregations are given highest priorities.\nThis allows to reach a final segmentation into consistent regions with strong\nboundary adherence. We perform experiments on the BSDS500 image dataset to\nhighlight to which extent our method compares favorably against other\nwell-known image segmentation algorithms. The obtained results demonstrate the\npromising potential of the proposed approach.\n", "versions": [{"version": "v1", "created": "Sat, 17 Mar 2018 17:13:09 GMT"}], "update_date": "2018-03-20", "authors_parsed": [["Chaibou", "Mahaman Sani", ""], ["Conze", "Pierre-Henri", ""], ["Kalti", "Karim", ""], ["Solaiman", "Basel", ""], ["Mahjoub", "Mohamed Ali", ""]]}, {"id": "1803.06542", "submitter": "Yuhang Wu", "authors": "Yuhang Wu, Le Anh Vu Ha, Xiang Xu, Ioannis A. Kakadiaris", "title": "Convolutional Point-set Representation: A Convolutional Bridge Between a\n  Densely Annotated Image and 3D Face Alignment", "comments": "Preprint Submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a robust method for estimating the facial pose and shape\ninformation from a densely annotated facial image. The method relies on\nConvolutional Point-set Representation (CPR), a carefully designed matrix\nrepresentation to summarize different layers of information encoded in the set\nof detected points in the annotated image. The CPR disentangles the\ndependencies of shape and different pose parameters and enables updating\ndifferent parameters in a sequential manner via convolutional neural networks\nand recurrent layers. When updating the pose parameters, we sample reprojection\nerrors along with a predicted direction and update the parameters based on the\npattern of reprojection errors. This technique boosts the model's capability in\nsearching a local minimum under challenging scenarios. We also demonstrate that\nannotation from different sources can be merged under the framework of CPR and\ncontributes to outperforming the current state-of-the-art solutions for 3D face\nalignment. Experiments indicate the proposed CPRFA (CPR-based Face Alignment)\nsignificantly improves 3D alignment accuracy when the densely annotated image\ncontains noise and missing values, which is common under \"in-the-wild\"\nacquisition scenarios.\n", "versions": [{"version": "v1", "created": "Sat, 17 Mar 2018 17:20:25 GMT"}, {"version": "v2", "created": "Mon, 2 Apr 2018 21:49:54 GMT"}], "update_date": "2018-04-04", "authors_parsed": [["Wu", "Yuhang", ""], ["Ha", "Le Anh Vu", ""], ["Xu", "Xiang", ""], ["Kakadiaris", "Ioannis A.", ""]]}, {"id": "1803.06554", "submitter": "Pan Wei", "authors": "Pan Wei, John E. Ball, Derek T. Anderson", "title": "Fusion of an Ensemble of Augmented Image Detectors for Robust Object\n  Detection", "comments": "21 pages, 12 figures, journal paper, MDPI Sensors, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI eess.IV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  A significant challenge in object detection is accurate identification of an\nobject's position in image space, whereas one algorithm with one set of\nparameters is usually not enough, and the fusion of multiple algorithms and/or\nparameters can lead to more robust results. Herein, a new computational\nintelligence fusion approach based on the dynamic analysis of agreement among\nobject detection outputs is proposed. Furthermore, we propose an online versus\njust in training image augmentation strategy. Experiments comparing the results\nboth with and without fusion are presented. We demonstrate that the augmented\nand fused combination results are the best, with respect to higher accuracy\nrates and reduction of outlier influences. The approach is demonstrated in the\ncontext of cone, pedestrian and box detection for Advanced Driver Assistance\nSystems (ADAS) applications.\n", "versions": [{"version": "v1", "created": "Sat, 17 Mar 2018 19:16:26 GMT"}], "update_date": "2018-03-20", "authors_parsed": [["Wei", "Pan", ""], ["Ball", "John E.", ""], ["Anderson", "Derek T.", ""]]}, {"id": "1803.06579", "submitter": "Mahdyar Ravanbakhsh", "authors": "Mohamad Baydoun, Mahdyar Ravanbakhsh, Damian Campo, Pablo Marin, David\n  Martin, Lucio Marcenaro, Andrea Cavallaro, Carlo S. Regazzoni", "title": "A Multi-perspective Approach To Anomaly Detection For Self-aware\n  Embodied Agents", "comments": "IEEE International Conference on Acoustics, Speech, and Signal\n  Processing (ICASSP) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on multi-sensor anomaly detection for moving cognitive\nagents using both external and private first-person visual observations. Both\nobservation types are used to characterize agents' motion in a given\nenvironment. The proposed method generates locally uniform motion models by\ndividing a Gaussian process that approximates agents' displacements on the\nscene and provides a Shared Level (SL) self-awareness based on Environment\nCentered (EC) models. Such models are then used to train in a semi-unsupervised\nway a set of Generative Adversarial Networks (GANs) that produce an estimation\nof external and internal parameters of moving agents. Obtained results\nexemplify the feasibility of using multi-perspective data for predicting and\nanalyzing trajectory information.\n", "versions": [{"version": "v1", "created": "Sat, 17 Mar 2018 22:00:08 GMT"}], "update_date": "2018-03-20", "authors_parsed": [["Baydoun", "Mohamad", ""], ["Ravanbakhsh", "Mahdyar", ""], ["Campo", "Damian", ""], ["Marin", "Pablo", ""], ["Martin", "David", ""], ["Marcenaro", "Lucio", ""], ["Cavallaro", "Andrea", ""], ["Regazzoni", "Carlo S.", ""]]}, {"id": "1803.06594", "submitter": "Ulugbek Kamilov", "authors": "Yu Sun and Zhihao Xia and Ulugbek S. Kamilov", "title": "Efficient and accurate inversion of multiple scattering with deep\n  learning", "comments": null, "journal-ref": null, "doi": "10.1364/OE.26.014678", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image reconstruction under multiple light scattering is crucial in a number\nof applications such as diffraction tomography. The reconstruction problem is\noften formulated as a nonconvex optimization, where a nonlinear measurement\nmodel is used to account for multiple scattering and regularization is used to\nenforce prior constraints on the object. In this paper, we propose a powerful\nalternative to this optimization-based view of image reconstruction by\ndesigning and training a deep convolutional neural network that can invert\nmultiple scattered measurements to produce a high-quality image of the\nrefractive index. Our results on both simulated and experimental datasets show\nthat the proposed approach is substantially faster and achieves higher imaging\nquality compared to the state-of-the-art methods based on optimization.\n", "versions": [{"version": "v1", "created": "Sun, 18 Mar 2018 03:11:48 GMT"}, {"version": "v2", "created": "Thu, 5 Apr 2018 17:02:41 GMT"}], "update_date": "2018-07-04", "authors_parsed": [["Sun", "Yu", ""], ["Xia", "Zhihao", ""], ["Kamilov", "Ulugbek S.", ""]]}, {"id": "1803.06598", "submitter": "Tao Hu", "authors": "Tao Hu, Honggang Qi, Jizheng Xu, Qingming Huang", "title": "Facial Landmarks Detection by Self-Iterative Regression based\n  Landmarks-Attention Network", "comments": "Accepted in AAAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cascaded Regression (CR) based methods have been proposed to solve facial\nlandmarks detection problem, which learn a series of descent directions by\nmultiple cascaded regressors separately trained in coarse and fine stages. They\noutperform the traditional gradient descent based methods in both accuracy and\nrunning speed. However, cascaded regression is not robust enough because each\nregressor's training data comes from the output of previous regressor.\nMoreover, training multiple regressors requires lots of computing resources,\nespecially for deep learning based methods. In this paper, we develop a\nSelf-Iterative Regression (SIR) framework to improve the model efficiency. Only\none self-iterative regressor is trained to learn the descent directions for\nsamples from coarse stages to fine stages, and parameters are iteratively\nupdated by the same regressor. Specifically, we proposed Landmarks-Attention\nNetwork (LAN) as our regressor, which concurrently learns features around each\nlandmark and obtains the holistic location increment. By doing so, not only the\nrest of regressors are removed to simplify the training process, but the number\nof model parameters is significantly decreased. The experiments demonstrate\nthat with only 3.72M model parameters, our proposed method achieves the\nstate-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Sun, 18 Mar 2018 03:32:35 GMT"}], "update_date": "2018-03-20", "authors_parsed": [["Hu", "Tao", ""], ["Qi", "Honggang", ""], ["Xu", "Jizheng", ""], ["Huang", "Qingming", ""]]}, {"id": "1803.06613", "submitter": "Santhosh Kelathodi Kumaran", "authors": "Santhosh Kelathodi Kumaran, Debi Prosad Dogra, Partha Pratim Roy and\n  Bidyut Baran Chaudhuri", "title": "Trajectory-based Scene Understanding using Dirichlet Process Mixture\n  Model", "comments": "14 pages, 27 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Appropriate modeling of a surveillance scene is essential for detection of\nanomalies in road traffic. Learning usual paths can provide valuable insight\ninto road traffic conditions and thus can help in identifying unusual routes\ntaken by commuters/vehicles. If usual traffic paths are learned in a\nnonparametric way, manual interventions in road marking road can be avoided. In\nthis paper, we propose an unsupervised and nonparametric method to learn\nfrequently used paths from the tracks of moving objects in $\\Theta(kn)$ time,\nwhere $k$ denotes the number of paths and $n$ represents the number of tracks.\nIn the proposed method, temporal dependencies of the moving objects are\nconsidered to make the clustering meaningful using Temporally Incremental\nGravity Model (TIGM). In addition, the distance-based scene learning makes it\nintuitive to estimate the model parameters. Further, we have extended TIGM\nhierarchically as Dynamically Evolving Model (DEM) to represent notable traffic\ndynamics of a scene. Experimental validation reveals that the proposed method\ncan learn a scene quickly without prior knowledge about the number of paths\n($k$). We have compared the results with various state-of-the-art methods. We\nhave also highlighted the advantages of the proposed method over existing\ntechniques popularly used for designing traffic monitoring applications. It can\nbe used for administrative decision making to control traffic at junctions or\ncrowded places and generate alarm signals, if necessary.\n", "versions": [{"version": "v1", "created": "Sun, 18 Mar 2018 06:40:34 GMT"}, {"version": "v2", "created": "Wed, 16 Jan 2019 13:20:51 GMT"}, {"version": "v3", "created": "Sun, 16 Jun 2019 14:30:11 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Kumaran", "Santhosh Kelathodi", ""], ["Dogra", "Debi Prosad", ""], ["Roy", "Partha Pratim", ""], ["Chaudhuri", "Bidyut Baran", ""]]}, {"id": "1803.06626", "submitter": "Liping Jing Dr.", "authors": "Juanying Xie, Qi Hou, Yinghuan Shi, Lv Peng, Liping Jing, Fuzhen\n  Zhuang, Junping Zhang, Xiaoyang Tang, Shengquan Xu", "title": "The Automatic Identification of Butterfly Species", "comments": "9 pages, in Chinese, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The available butterfly data sets comprise a few limited species, and the\nimages in the data sets are always standard patterns without the images of\nbutterflies in their living environment. To overcome the aforementioned\nlimitations in the butterfly data sets, we build a butterfly data set composed\nof all species of butterflies in China with 4270 standard pattern images of\n1176 butterfly species, and 1425 images from living environment of 111 species.\nWe propose to use the deep learning technique Faster-Rcnn to train an automatic\nbutterfly identification system including butterfly position detection and\nspecies recognition. We delete those species with only one living environment\nimage from data set, then partition the rest images from living environment\ninto two subsets, one used as test subset, the other as training subset\nrespectively combined with all standard pattern butterfly images or the\nstandard pattern butterfly images with the same species of the images from\nliving environment. In order to construct the training subset for FasterRcnn,\nnine methods were adopted to amplifying the images in the training subset\nincluding the turning of up and down, and left and right, rotation with\ndifferent angles, adding noises, blurring, and contrast ratio adjusting etc.\nThree prediction models were trained. The mAP (Mean Average prediction)\ncriterion was used to evaluate the performance of the prediction model. The\nexperimental results demonstrate that our Faster-Rcnn based butterfly automatic\nidentification system performed well, and its worst mAP is up to 60%, and can\nsimultaneously detect the positions of more than one butterflies in one images\nfrom living environment and recognize the species of those butterflies as well.\n", "versions": [{"version": "v1", "created": "Sun, 18 Mar 2018 08:47:20 GMT"}], "update_date": "2018-03-20", "authors_parsed": [["Xie", "Juanying", ""], ["Hou", "Qi", ""], ["Shi", "Yinghuan", ""], ["Peng", "Lv", ""], ["Jing", "Liping", ""], ["Zhuang", "Fuzhen", ""], ["Zhang", "Junping", ""], ["Tang", "Xiaoyang", ""], ["Xu", "Shengquan", ""]]}, {"id": "1803.06629", "submitter": "Yuta Hiasa", "authors": "Yuta Hiasa, Yoshito Otake, Masaki Takao, Takumi Matsuoka, Kazuma\n  Takashima, Jerry L. Prince, Nobuhiko Sugano, Yoshinobu Sato", "title": "Cross-modality image synthesis from unpaired data using CycleGAN:\n  Effects of gradient consistency loss and training data size", "comments": "10 pages, 7 figures, MICCAI 2018 Workshop on Simulation and Synthesis\n  in Medical Imaging", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  CT is commonly used in orthopedic procedures. MRI is used along with CT to\nidentify muscle structures and diagnose osteonecrosis due to its superior soft\ntissue contrast. However, MRI has poor contrast for bone structures. Clearly,\nit would be helpful if a corresponding CT were available, as bone boundaries\nare more clearly seen and CT has standardized (i.e., Hounsfield) units.\nTherefore, we aim at MR-to-CT synthesis. The CycleGAN was successfully applied\nto unpaired CT and MR images of the head, these images do not have as much\nvariation of intensity pairs as do images in the pelvic region due to the\npresence of joints and muscles. In this paper, we extended the CycleGAN\napproach by adding the gradient consistency loss to improve the accuracy at the\nboundaries. We conducted two experiments. To evaluate image synthesis, we\ninvestigated dependency of image synthesis accuracy on 1) the number of\ntraining data and 2) the gradient consistency loss. To demonstrate the\napplicability of our method, we also investigated a segmentation accuracy on\nsynthesized images.\n", "versions": [{"version": "v1", "created": "Sun, 18 Mar 2018 09:13:51 GMT"}, {"version": "v2", "created": "Sun, 17 Jun 2018 21:55:27 GMT"}, {"version": "v3", "created": "Tue, 31 Jul 2018 07:21:17 GMT"}], "update_date": "2018-08-01", "authors_parsed": [["Hiasa", "Yuta", ""], ["Otake", "Yoshito", ""], ["Takao", "Masaki", ""], ["Matsuoka", "Takumi", ""], ["Takashima", "Kazuma", ""], ["Prince", "Jerry L.", ""], ["Sugano", "Nobuhiko", ""], ["Sato", "Yoshinobu", ""]]}, {"id": "1803.06641", "submitter": "Jiahao Pang", "authors": "Jiahao Pang, Wenxiu Sun, Chengxi Yang, Jimmy Ren, Ruichao Xiao, Jin\n  Zeng, Liang Lin", "title": "Zoom and Learn: Generalizing Deep Stereo Matching to Novel Domains", "comments": "Accepted at CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the recent success of stereo matching with convolutional neural\nnetworks (CNNs), it remains arduous to generalize a pre-trained deep stereo\nmodel to a novel domain. A major difficulty is to collect accurate ground-truth\ndisparities for stereo pairs in the target domain. In this work, we propose a\nself-adaptation approach for CNN training, utilizing both synthetic training\ndata (with ground-truth disparities) and stereo pairs in the new domain\n(without ground-truths). Our method is driven by two empirical observations. By\nfeeding real stereo pairs of different domains to stereo models pre-trained\nwith synthetic data, we see that: i) a pre-trained model does not generalize\nwell to the new domain, producing artifacts at boundaries and ill-posed\nregions; however, ii) feeding an up-sampled stereo pair leads to a disparity\nmap with extra details. To avoid i) while exploiting ii), we formulate an\niterative optimization problem with graph Laplacian regularization. At each\niteration, the CNN adapts itself better to the new domain: we let the CNN learn\nits own higher-resolution output; at the meanwhile, a graph Laplacian\nregularization is imposed to discriminatively keep the desired edges while\nsmoothing out the artifacts. We demonstrate the effectiveness of our method in\ntwo domains: daily scenes collected by smartphone cameras, and street views\ncaptured in a driving car.\n", "versions": [{"version": "v1", "created": "Sun, 18 Mar 2018 11:11:10 GMT"}], "update_date": "2018-03-20", "authors_parsed": [["Pang", "Jiahao", ""], ["Sun", "Wenxiu", ""], ["Yang", "Chengxi", ""], ["Ren", "Jimmy", ""], ["Xiao", "Ruichao", ""], ["Zeng", "Jin", ""], ["Lin", "Liang", ""]]}, {"id": "1803.06647", "submitter": "Jinning Li", "authors": "Jinning Li, Siqi Liu, and Mengyao Cao", "title": "Line Artist: A Multiple Style Sketch to Painting Synthesis Scheme", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Drawing a beautiful painting is a dream of many people since childhood. In\nthis paper, we propose a novel scheme, Line Artist, to synthesize artistic\nstyle paintings with freehand sketch images, leveraging the power of deep\nlearning and advanced algorithms. Our scheme includes three models. The Sketch\nImage Extraction (SIE) model is applied to generate the training data. It\nincludes smoothing reality images and pencil sketch extraction. The Detailed\nImage Synthesis (DIS) model trains a conditional generative adversarial network\nto generate detailed real-world information. The Adaptively Weighted Artistic\nStyle Transfer (AWAST) model is capable to combine multiple style images with a\ncontent with the VGG19 network and PageRank algorithm. The appealing artistic\nimages are then generated by optimization iterations. Experiments are operated\non the Kaggle Cats dataset and The Oxford Buildings Dataset. Our synthesis\nresults are proved to be artistic, beautiful and robust.\n", "versions": [{"version": "v1", "created": "Sun, 18 Mar 2018 11:54:22 GMT"}], "update_date": "2018-03-20", "authors_parsed": [["Li", "Jinning", ""], ["Liu", "Siqi", ""], ["Cao", "Mengyao", ""]]}, {"id": "1803.06655", "submitter": "Yifang Xu", "authors": "Yifang Xu, Jing Chen, and Tianli Liao", "title": "Ratio-Preserving Half-Cylindrical Warps for Natural Image Stitching", "comments": "3 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel warp for natural image stitching is proposed that utilizes the\nproperty of cylindrical warp and a horizontal pixel selection strategy. The\nproposed ratio-preserving half-cylindrical warp is a combination of homography\nand cylindrical warps which guarantees alignment by homography and possesses\nless projective distortion by cylindrical warp. Unlike previous approaches\napplying cylindrical warp before homography, we use partition lines to divide\nthe image into different parts and apply homography in the overlapping region\nwhile a composition of homography and cylindrical warps in the non-overlapping\nregion. The pixel selection strategy then samples the points in horizontal and\nreconstructs the image via interpolation to further reduce horizontal\ndistortion by maintaining the ratio as similarity. With applying\nhalf-cylindrical warp and horizontal pixel selection, the projective distortion\nin vertical and horizontal is mitigated simultaneously. Experiments show that\nour warp is efficient and produces a more natural-looking stitched result than\nprevious methods.\n", "versions": [{"version": "v1", "created": "Sun, 18 Mar 2018 13:13:30 GMT"}], "update_date": "2018-03-20", "authors_parsed": [["Xu", "Yifang", ""], ["Chen", "Jing", ""], ["Liao", "Tianli", ""]]}, {"id": "1803.06657", "submitter": "Can Pu", "authors": "Can Pu, Runzi Song, Radim Tylecek, Nanbo Li, Robert B Fisher", "title": "Sdf-GAN: Semi-supervised Depth Fusion with Multi-scale Adversarial\n  Networks", "comments": "This is our draft and accepted by the journal Remote Sensing. There\n  is a little difference between the title on Arxiv and that on Remote Sensing.\n  Two small corrections have been made in \"Performance on Kitti2015 Dataset\" in\n  this latest version (which is slightly different from the previous version in\n  Remote Sensing)", "journal-ref": "Remote Sens. 2019, 11, 487", "doi": "10.3390/rs11050487", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Refining raw disparity maps from different algorithms to exploit their\ncomplementary advantages is still challenging. Uncertainty estimation and\ncomplex disparity relationships among pixels limit the accuracy and robustness\nof existing methods and there is no standard method for fusion of different\nkinds of depth data. In this paper, we introduce a new method to fuse disparity\nmaps from different sources, while incorporating supplementary information\n(intensity, gradient, etc.) into a refiner network to better refine raw\ndisparity inputs. A discriminator network classifies disparities at different\nreceptive fields and scales. Assuming a Markov Random Field for the refined\ndisparity map produces better estimates of the true disparity distribution.\nBoth fully supervised and semi-supervised versions of the algorithm are\nproposed. The approach includes a more robust loss function to inpaint invalid\ndisparity values and requires much less labeled data to train in the\nsemi-supervised learning mode. The algorithm can be generalized to fuse depths\nfrom different kinds of depth sources. Experiments explored different fusion\nopportunities: stereo-monocular fusion, stereo-ToF fusion and stereo-stereo\nfusion. The experiments show the superiority of the proposed algorithm compared\nwith the most recent algorithms on public synthetic datasets (Scene Flow,\nSYNTH3, our synthetic garden dataset) and real datasets (Kitti2015 dataset and\nTrimbot2020 Garden dataset).\n", "versions": [{"version": "v1", "created": "Sun, 18 Mar 2018 13:17:16 GMT"}, {"version": "v2", "created": "Sat, 2 Mar 2019 12:38:12 GMT"}, {"version": "v3", "created": "Mon, 6 May 2019 15:48:51 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Pu", "Can", ""], ["Song", "Runzi", ""], ["Tylecek", "Radim", ""], ["Li", "Nanbo", ""], ["Fisher", "Robert B", ""]]}, {"id": "1803.06731", "submitter": "Yan Li", "authors": "Yan Li, Junge Zhang, Jianguo Zhang, Kaiqi Huang", "title": "Discriminative Learning of Latent Features for Zero-Shot Recognition", "comments": "CVPR 2018 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zero-shot learning (ZSL) aims to recognize unseen image categories by\nlearning an embedding space between image and semantic representations. For\nyears, among existing works, it has been the center task to learn the proper\nmapping matrices aligning the visual and semantic space, whilst the importance\nto learn discriminative representations for ZSL is ignored. In this work, we\nretrospect existing methods and demonstrate the necessity to learn\ndiscriminative representations for both visual and semantic instances of ZSL.\nWe propose an end-to-end network that is capable of 1) automatically\ndiscovering discriminative regions by a zoom network; and 2) learning\ndiscriminative semantic representations in an augmented space introduced for\nboth user-defined and latent attributes. Our proposed method is tested\nextensively on two challenging ZSL datasets, and the experiment results show\nthat the proposed method significantly outperforms state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sun, 18 Mar 2018 20:18:48 GMT"}], "update_date": "2018-03-20", "authors_parsed": [["Li", "Yan", ""], ["Zhang", "Junge", ""], ["Zhang", "Jianguo", ""], ["Huang", "Kaiqi", ""]]}, {"id": "1803.06744", "submitter": "Purushotham Kamath", "authors": "Purushotham Kamath, Abhishek Singh, and Debo Dutta", "title": "Fast Neural Architecture Construction using EnvelopeNets", "comments": "A shorter version of this paper appeared in the Workshop on\n  MetaLearning 2018 (MetaLearning 2018 at NeurIPS 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fast Neural Architecture Construction (NAC) is a method to construct deep\nnetwork architectures by pruning and expansion of a base network. In recent\nyears, several automated search methods for neural network architectures have\nbeen proposed using methods such as evolutionary algorithms and reinforcement\nlearning. These methods use a single scalar objective function (usually\naccuracy) that is evaluated after a full training and evaluation cycle. In\ncontrast NAC directly compares the utility of different filters using\nstatistics derived from filter featuremaps reach a state where the utility of\ndifferent filters within a network can be compared and hence can be used to\nconstruct networks. The training epochs needed for filters within a network to\nreach this state is much less than the training epochs needed for the accuracy\nof a network to stabilize. NAC exploits this finding to construct convolutional\nneural nets (CNNs) with close to state of the art accuracy, in < 1 GPU day,\nfaster than most of the current neural architecture search methods. The\nconstructed networks show close to state of the art performance on the image\nclassification problem on well known datasets (CIFAR-10, ImageNet) and\nconsistently show better performance than hand constructed and randomly\ngenerated networks of the same depth, operators and approximately the same\nnumber of parameters.\n", "versions": [{"version": "v1", "created": "Sun, 18 Mar 2018 21:28:03 GMT"}, {"version": "v2", "created": "Tue, 22 May 2018 18:48:06 GMT"}, {"version": "v3", "created": "Fri, 14 Dec 2018 00:34:40 GMT"}], "update_date": "2018-12-17", "authors_parsed": [["Kamath", "Purushotham", ""], ["Singh", "Abhishek", ""], ["Dutta", "Debo", ""]]}, {"id": "1803.06782", "submitter": "Dakai Jin", "authors": "Dakai Jin, Ziyue Xu, Adam P. Harrison, Daniel J. Mollura", "title": "White matter hyperintensity segmentation from T1 and FLAIR images using\n  fully convolutional neural networks enhanced with residual connections", "comments": "IEEE International Symposium on Biomedical Imaging (ISBI)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Segmentation and quantification of white matter hyperintensities (WMHs) are\nof great importance in studying and understanding various neurological and\ngeriatric disorders. Although automatic methods have been proposed for WMH\nsegmentation on magnetic resonance imaging (MRI), manual corrections are often\nnecessary to achieve clinically practical results. Major challenges for WMH\nsegmentation stem from their inhomogeneous MRI intensities, random location and\nsize distributions, and MRI noise. The presence of other brain anatomies or\ndiseases with enhanced intensities adds further difficulties. To cope with\nthese challenges, we present a specifically designed fully convolutional neural\nnetwork (FCN) with residual connections to segment WMHs by using combined T1\nand fluid-attenuated inversion recovery (FLAIR) images. Our customized FCN is\ndesigned to be straightforward and generalizable, providing efficient\nend-to-end training due to its enhanced information propagation. We tested our\nmethod on the open WMH Segmentation Challenge MICCAI2017 dataset, and, despite\nour method's relative simplicity, results show that it performs amongst the\nleading techniques across five metrics. More importantly, our method achieves\nthe best score for hausdorff distance and average volume difference in testing\ndatasets from two MRI scanners that were not included in training,\ndemonstrating better generalization ability of our proposed method over its\ncompetitors.\n", "versions": [{"version": "v1", "created": "Mon, 19 Mar 2018 02:07:00 GMT"}], "update_date": "2018-03-20", "authors_parsed": [["Jin", "Dakai", ""], ["Xu", "Ziyue", ""], ["Harrison", "Adam P.", ""], ["Mollura", "Daniel J.", ""]]}, {"id": "1803.06784", "submitter": "Fausto Milletari", "authors": "Fausto Milletari, Johann Frei, Seyed-Ahmad Ahmadi", "title": "TOMAAT: volumetric medical image analysis as a cloud service", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has been recently applied to a multitude of computer vision and\nmedical image analysis problems. Although recent research efforts have improved\nthe state of the art, most of the methods cannot be easily accessed, compared\nor used by either researchers or the general public. Researchers often publish\ntheir code and trained models on the internet, but this does not always enable\nthese approaches to be easily used or integrated in stand-alone applications\nand existing workflows. In this paper we propose a framework which allows easy\ndeployment and access of deep learning methods for segmentation through a\ncloud-based architecture. Our approach comprises three parts: a server, which\nwraps trained deep learning models and their pre- and post-processing data\npipelines and makes them available on the cloud; a client which interfaces with\nthe server to obtain predictions on user data; a service registry that informs\nclients about available prediction endpoints that are available in the cloud.\nThese three parts constitute the open-source TOMAAT framework.\n", "versions": [{"version": "v1", "created": "Mon, 19 Mar 2018 02:21:36 GMT"}, {"version": "v2", "created": "Wed, 25 Apr 2018 09:19:03 GMT"}], "update_date": "2018-04-26", "authors_parsed": [["Milletari", "Fausto", ""], ["Frei", "Johann", ""], ["Ahmadi", "Seyed-Ahmad", ""]]}, {"id": "1803.06791", "submitter": "Weiyue Wang", "authors": "Weiyue Wang and Ulrich Neumann", "title": "Depth-aware CNN for RGB-D Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNN) are limited by the lack of capability to\nhandle geometric information due to the fixed grid kernel structure. The\navailability of depth data enables progress in RGB-D semantic segmentation with\nCNNs. State-of-the-art methods either use depth as additional images or process\nspatial information in 3D volumes or point clouds. These methods suffer from\nhigh computation and memory cost. To address these issues, we present\nDepth-aware CNN by introducing two intuitive, flexible and effective\noperations: depth-aware convolution and depth-aware average pooling. By\nleveraging depth similarity between pixels in the process of information\npropagation, geometry is seamlessly incorporated into CNN. Without introducing\nany additional parameters, both operators can be easily integrated into\nexisting CNNs. Extensive experiments and ablation studies on challenging RGB-D\nsemantic segmentation benchmarks validate the effectiveness and flexibility of\nour approach.\n", "versions": [{"version": "v1", "created": "Mon, 19 Mar 2018 03:09:42 GMT"}], "update_date": "2018-03-20", "authors_parsed": [["Wang", "Weiyue", ""], ["Neumann", "Ulrich", ""]]}, {"id": "1803.06795", "submitter": "Xinyuan Zhang", "authors": "Xinyuan Zhang, Xin Yuan, Lawrence Carin", "title": "Nonlocal Low-Rank Tensor Factor Analysis for Image Restoration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low-rank signal modeling has been widely leveraged to capture non-local\ncorrelation in image processing applications. We propose a new method that\nemploys low-rank tensor factor analysis for tensors generated by grouped image\npatches. The low-rank tensors are fed into the alternative direction multiplier\nmethod (ADMM) to further improve image reconstruction. The motivating\napplication is compressive sensing (CS), and a deep convolutional architecture\nis adopted to approximate the expensive matrix inversion in CS applications. An\niterative algorithm based on this low-rank tensor factorization strategy,\ncalled NLR-TFA, is presented in detail. Experimental results on noiseless and\nnoisy CS measurements demonstrate the superiority of the proposed approach,\nespecially at low CS sampling rates.\n", "versions": [{"version": "v1", "created": "Mon, 19 Mar 2018 03:48:14 GMT"}], "update_date": "2018-03-20", "authors_parsed": [["Zhang", "Xinyuan", ""], ["Yuan", "Xin", ""], ["Carin", "Lawrence", ""]]}, {"id": "1803.06798", "submitter": "Dacheng Tao", "authors": "Xinyuan Chen, Chang Xu, Xiaokang Yang and Dacheng Tao", "title": "Attention-GAN for Object Transfiguration in Wild Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the object transfiguration problem in wild images. The\ngenerative network in classical GANs for object transfiguration often\nundertakes a dual responsibility: to detect the objects of interests and to\nconvert the object from source domain to target domain. In contrast, we\ndecompose the generative network into two separat networks, each of which is\nonly dedicated to one particular sub-task. The attention network predicts\nspatial attention maps of images, and the transformation network focuses on\ntranslating objects. Attention maps produced by attention network are\nencouraged to be sparse, so that major attention can be paid to objects of\ninterests. No matter before or after object transfiguration, attention maps\nshould remain constant. In addition, learning attention network can receive\nmore instructions, given the available segmentation annotations of images.\nExperimental results demonstrate the necessity of investigating attention in\nobject transfiguration, and that the proposed algorithm can learn accurate\nattention to improve quality of generated images.\n", "versions": [{"version": "v1", "created": "Mon, 19 Mar 2018 04:00:13 GMT"}], "update_date": "2018-03-20", "authors_parsed": [["Chen", "Xinyuan", ""], ["Xu", "Chang", ""], ["Yang", "Xiaokang", ""], ["Tao", "Dacheng", ""]]}, {"id": "1803.06799", "submitter": "Bowen Cheng", "authors": "Bowen Cheng and Yunchao Wei and Honghui Shi and Rogerio Feris and\n  Jinjun Xiong and Thomas Huang", "title": "Revisiting RCNN: On Awakening the Classification Power of Faster RCNN", "comments": "ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent region-based object detectors are usually built with separate\nclassification and localization branches on top of shared feature extraction\nnetworks. In this paper, we analyze failure cases of state-of-the-art detectors\nand observe that most hard false positives result from classification instead\nof localization. We conjecture that: (1) Shared feature representation is not\noptimal due to the mismatched goals of feature learning for classification and\nlocalization; (2) multi-task learning helps, yet optimization of the multi-task\nloss may result in sub-optimal for individual tasks; (3) large receptive field\nfor different scales leads to redundant context information for small\nobjects.We demonstrate the potential of detector classification power by a\nsimple, effective, and widely-applicable Decoupled Classification Refinement\n(DCR) network. DCR samples hard false positives from the base classifier in\nFaster RCNN and trains a RCNN-styled strong classifier. Experiments show new\nstate-of-the-art results on PASCAL VOC and COCO without any bells and whistles.\n", "versions": [{"version": "v1", "created": "Mon, 19 Mar 2018 04:13:06 GMT"}, {"version": "v2", "created": "Tue, 20 Mar 2018 22:52:28 GMT"}, {"version": "v3", "created": "Sat, 14 Jul 2018 04:50:20 GMT"}], "update_date": "2018-07-17", "authors_parsed": [["Cheng", "Bowen", ""], ["Wei", "Yunchao", ""], ["Shi", "Honghui", ""], ["Feris", "Rogerio", ""], ["Xiong", "Jinjun", ""], ["Huang", "Thomas", ""]]}, {"id": "1803.06802", "submitter": "Qianyi Wu", "authors": "Qianyi Wu, Juyong Zhang, Yu-Kun Lai, Jianmin Zheng and Jianfei Cai", "title": "Alive Caricature from 2D to 3D", "comments": "Accepted to CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Caricature is an art form that expresses subjects in abstract, simple and\nexaggerated view. While many caricatures are 2D images, this paper presents an\nalgorithm for creating expressive 3D caricatures from 2D caricature images with\na minimum of user interaction. The key idea of our approach is to introduce an\nintrinsic deformation representation that has a capacity of extrapolation\nenabling us to create a deformation space from standard face dataset, which\nmaintains face constraints and meanwhile is sufficiently large for producing\nexaggerated face models. Built upon the proposed deformation representation, an\noptimization model is formulated to find the 3D caricature that captures the\nstyle of the 2D caricature image automatically. The experiments show that our\napproach has better capability in expressing caricatures than those fitting\napproaches directly using classical parametric face models such as 3DMM and\nFaceWareHouse. Moreover, our approach is based on standard face datasets and\navoids constructing complicated 3D caricature training set, which provides\ngreat flexibility in real applications.\n", "versions": [{"version": "v1", "created": "Mon, 19 Mar 2018 04:47:16 GMT"}, {"version": "v2", "created": "Tue, 20 Mar 2018 06:56:20 GMT"}, {"version": "v3", "created": "Tue, 27 Mar 2018 06:38:08 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Wu", "Qianyi", ""], ["Zhang", "Juyong", ""], ["Lai", "Yu-Kun", ""], ["Zheng", "Jianmin", ""], ["Cai", "Jianfei", ""]]}, {"id": "1803.06813", "submitter": "Srikrishna V", "authors": "Srikrishna Varadarajan, Muktabh Mayank Srivastava", "title": "Weakly Supervised Object Localization on grocery shelves using simple\n  FCN and Synthetic Dataset", "comments": "Published at The Indian Conference on Computer Vision, Graphics and\n  Image Processing (ICVGIP) 2018. ( https://cvit.iiit.ac.in/icvgip18/ )", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We propose a weakly supervised method using two algorithms to predict object\nbounding boxes given only an image classification dataset. First algorithm is a\nsimple Fully Convolutional Network (FCN) trained to classify object instances.\nWe use the property of FCN to return a mask for images larger than training\nimages to get a primary output segmentation mask during test time by passing an\nimage pyramid to it. We enhance the FCN output mask into final output bounding\nboxes by a Convolutional Encoder-Decoder (ConvAE) viz. the second algorithm.\nConvAE is trained to localize objects on an artificially generated dataset of\noutput segmentation masks. We demonstrate the effectiveness of this method in\nlocalizing objects in grocery shelves where annotating data for object\ndetection is hard due to variety of objects. This method can be extended to any\nproblem domain where collecting images of objects is easy and annotating their\ncoordinates is hard.\n", "versions": [{"version": "v1", "created": "Mon, 19 Mar 2018 06:34:29 GMT"}, {"version": "v2", "created": "Wed, 9 Jan 2019 09:54:08 GMT"}], "update_date": "2019-01-10", "authors_parsed": [["Varadarajan", "Srikrishna", ""], ["Srivastava", "Muktabh Mayank", ""]]}, {"id": "1803.06815", "submitter": "Sachin Mehta", "authors": "Sachin Mehta and Mohammad Rastegari and Anat Caspi and Linda Shapiro\n  and Hannaneh Hajishirzi", "title": "ESPNet: Efficient Spatial Pyramid of Dilated Convolutions for Semantic\n  Segmentation", "comments": "Accepted at ECCV'18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a fast and efficient convolutional neural network, ESPNet, for\nsemantic segmentation of high resolution images under resource constraints.\nESPNet is based on a new convolutional module, efficient spatial pyramid (ESP),\nwhich is efficient in terms of computation, memory, and power. ESPNet is 22\ntimes faster (on a standard GPU) and 180 times smaller than the\nstate-of-the-art semantic segmentation network PSPNet, while its category-wise\naccuracy is only 8% less. We evaluated ESPNet on a variety of semantic\nsegmentation datasets including Cityscapes, PASCAL VOC, and a breast biopsy\nwhole slide image dataset. Under the same constraints on memory and\ncomputation, ESPNet outperforms all the current efficient CNN networks such as\nMobileNet, ShuffleNet, and ENet on both standard metrics and our newly\nintroduced performance metrics that measure efficiency on edge devices. Our\nnetwork can process high resolution images at a rate of 112 and 9 frames per\nsecond on a standard GPU and edge device, respectively.\n", "versions": [{"version": "v1", "created": "Mon, 19 Mar 2018 06:42:47 GMT"}, {"version": "v2", "created": "Wed, 21 Mar 2018 05:25:54 GMT"}, {"version": "v3", "created": "Wed, 25 Jul 2018 00:45:02 GMT"}], "update_date": "2018-07-26", "authors_parsed": [["Mehta", "Sachin", ""], ["Rastegari", "Mohammad", ""], ["Caspi", "Anat", ""], ["Shapiro", "Linda", ""], ["Hajishirzi", "Hannaneh", ""]]}, {"id": "1803.06898", "submitter": "Yaniv Shachor", "authors": "Yaniv Shachor, Hayit Greenspan, Jacob Goldberger", "title": "A Mixture of Views Network with Applications to the Classification of\n  Breast Microcalcifications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we examine data fusion methods for multi-view data\nclassification. We present a decision concept which explicitly takes into\naccount the input multi-view structure, where for each case there is a\ndifferent subset of relevant views. This data fusion concept, which we dub\nMixture of Views, is implemented by a special purpose neural network\narchitecture. It is demonstrated on the task of classifying breast\nmicrocalcifications as benign or malignant based on CC and MLO mammography\nviews. The single view decisions are combined by a data-driven decision,\naccording to the relevance of each view in a given case, into a global\ndecision. The method is evaluated on a large multi-view dataset extracted from\nthe standardized digital database for screening mammography (DDSM). The\nexperimental results show that our method outperforms previously suggested\nfusion methods.\n", "versions": [{"version": "v1", "created": "Mon, 19 Mar 2018 13:11:10 GMT"}], "update_date": "2018-03-20", "authors_parsed": [["Shachor", "Yaniv", ""], ["Greenspan", "Hayit", ""], ["Goldberger", "Jacob", ""]]}, {"id": "1803.06904", "submitter": "SeyedMajid Azimi", "authors": "Seyed Majid Azimi, Peter Fischer, Marco K\\\"orner, Peter Reinartz", "title": "Aerial LaneNet: Lane Marking Semantic Segmentation in Aerial Imagery\n  using Wavelet-Enhanced Cost-sensitive Symmetric Fully Convolutional Neural\n  Networks", "comments": "IEEE TGRS 2018 - Accepted", "journal-ref": null, "doi": "10.1109/TGRS.2018.2878510", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The knowledge about the placement and appearance of lane markings is a\nprerequisite for the creation of maps with high precision, necessary for\nautonomous driving, infrastructure monitoring, lane-wise traffic management,\nand urban planning. Lane markings are one of the important components of such\nmaps. Lane markings convey the rules of roads to drivers. While these rules are\nlearned by humans, an autonomous driving vehicle should be taught to learn them\nto localize itself. Therefore, accurate and reliable lane marking semantic\nsegmentation in the imagery of roads and highways is needed to achieve such\ngoals. We use airborne imagery which can capture a large area in a short period\nof time by introducing an aerial lane marking dataset. In this work, we propose\na Symmetric Fully Convolutional Neural Network enhanced by Wavelet Transform in\norder to automatically carry out lane marking segmentation in aerial imagery.\nDue to a heavily unbalanced problem in terms of number of lane marking pixels\ncompared with background pixels, we use a customized loss function as well as a\nnew type of data augmentation step. We achieve a very high accuracy in\npixel-wise localization of lane markings without using 3rd-party information.\nIn this work, we introduce the first high-quality dataset used within our\nexperiments which contains a broad range of situations and classes of lane\nmarkings representative of current transportation systems. This dataset will be\npublicly available and hence, it can be used as the benchmark dataset for\nfuture algorithms within this domain.\n", "versions": [{"version": "v1", "created": "Mon, 19 Mar 2018 13:32:27 GMT"}, {"version": "v2", "created": "Thu, 1 Nov 2018 16:16:51 GMT"}], "update_date": "2019-08-26", "authors_parsed": [["Azimi", "Seyed Majid", ""], ["Fischer", "Peter", ""], ["K\u00f6rner", "Marco", ""], ["Reinartz", "Peter", ""]]}, {"id": "1803.06911", "submitter": "Sheng Jin", "authors": "Sheng Jin", "title": "Unsupervised Semantic Deep Hashing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, deep hashing methods have been proved to be efficient since\nit employs convolutional neural network to learn features and hashing codes\nsimultaneously. However, these methods are mostly supervised. In real-world\napplication, it is a time-consuming and overloaded task for annotating a large\nnumber of images. In this paper, we propose a novel unsupervised deep hashing\nmethod for large-scale image retrieval. Our method, namely unsupervised\nsemantic deep hashing (\\textbf{USDH}), uses semantic information preserved in\nthe CNN feature layer to guide the training of network. We enforce four\ncriteria on hashing codes learning based on VGG-19 model: 1) preserving\nrelevant information of feature space in hashing space; 2) minimizing\nquantization loss between binary-like codes and hashing codes; 3) improving the\nusage of each bit in hashing codes by using maximum information entropy, and 4)\ninvariant to image rotation. Extensive experiments on CIFAR-10, NUSWIDE have\ndemonstrated that \\textbf{USDH} outperforms several state-of-the-art\nunsupervised hashing methods for image retrieval. We also conduct experiments\non Oxford 17 datasets for fine-grained classification to verify its efficiency\nfor other computer vision tasks.\n", "versions": [{"version": "v1", "created": "Mon, 19 Mar 2018 13:42:23 GMT"}], "update_date": "2018-03-20", "authors_parsed": [["Jin", "Sheng", ""]]}, {"id": "1803.06936", "submitter": "Feng Liu", "authors": "Feng Liu, Tao Xiang, Timothy M. Hospedales, Wankou Yang, and Changyin\n  Sun", "title": "Inverse Visual Question Answering: A New Benchmark and VQA Diagnosis\n  Tool", "comments": "arXiv admin note: text overlap with arXiv:1710.03370", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, visual question answering (VQA) has become topical. The\npremise of VQA's significance as a benchmark in AI, is that both the image and\ntextual question need to be well understood and mutually grounded in order to\ninfer the correct answer. However, current VQA models perhaps `understand' less\nthan initially hoped, and instead master the easier task of exploiting cues\ngiven away in the question and biases in the answer distribution. In this paper\nwe propose the inverse problem of VQA (iVQA). The iVQA task is to generate a\nquestion that corresponds to a given image and answer pair. We propose a\nvariational iVQA model that can generate diverse, grammatically correct and\ncontent correlated questions that match the given answer. Based on this model,\nwe show that iVQA is an interesting benchmark for visuo-linguistic\nunderstanding, and a more challenging alternative to VQA because an iVQA model\nneeds to understand the image better to be successful. As a second\ncontribution, we show how to use iVQA in a novel reinforcement learning\nframework to diagnose any existing VQA model by way of exposing its belief set:\nthe set of question-answer pairs that the VQA model would predict true for a\ngiven image. This provides a completely new window into what VQA models\n`believe' about images. We show that existing VQA models have more erroneous\nbeliefs than previously thought, revealing their intrinsic weaknesses.\nSuggestions are then made on how to address these weaknesses going forward.\n", "versions": [{"version": "v1", "created": "Fri, 16 Mar 2018 07:58:21 GMT"}], "update_date": "2018-03-20", "authors_parsed": [["Liu", "Feng", ""], ["Xiang", "Tao", ""], ["Hospedales", "Timothy M.", ""], ["Yang", "Wankou", ""], ["Sun", "Changyin", ""]]}, {"id": "1803.06951", "submitter": "Silvia-Laura Pintea", "authors": "Silvia L. Pintea, Jan C. van Gemert, and Arnold W. M. Smeulders", "title": "Deja Vu: Motion Prediction in Static Images", "comments": "Published in the proceedings of the European Conference on Computer\n  Vision (ECCV), 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes motion prediction in single still images by learning it\nfrom a set of videos. The building assumption is that similar motion is\ncharacterized by similar appearance. The proposed method learns local motion\npatterns given a specific appearance and adds the predicted motion in a number\nof applications. This work (i) introduces a novel method to predict motion from\nappearance in a single static image, (ii) to that end, extends of the\nStructured Random Forest with regression derived from first principles, and\n(iii) shows the value of adding motion predictions in different tasks such as:\nweak frame-proposals containing unexpected events, action recognition, motion\nsaliency. Illustrative results indicate that motion prediction is not only\nfeasible, but also provides valuable information for a number of applications.\n", "versions": [{"version": "v1", "created": "Mon, 19 Mar 2018 14:32:39 GMT"}, {"version": "v2", "created": "Wed, 21 Mar 2018 09:48:41 GMT"}], "update_date": "2018-03-22", "authors_parsed": [["Pintea", "Silvia L.", ""], ["van Gemert", "Jan C.", ""], ["Smeulders", "Arnold W. M.", ""]]}, {"id": "1803.06952", "submitter": "Silvia-Laura Pintea", "authors": "Silvia L. Pintea, and Jan C. van Gemert, and Arnold W. M. Smeulders", "title": "Asymmetric kernel in Gaussian Processes for learning target variance", "comments": "Accepted in Pattern Recognition Letters, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work incorporates the multi-modality of the data distribution into a\nGaussian Process regression model. We approach the problem from a\ndiscriminative perspective by learning, jointly over the training data, the\ntarget space variance in the neighborhood of a certain sample through metric\nlearning. We start by using data centers rather than all training samples.\nSubsequently, each center selects an individualized kernel metric. This enables\neach center to adjust the kernel space in its vicinity in correspondence with\nthe topology of the targets --- a multi-modal approach. We additionally add\ndescriptiveness by allowing each center to learn a precision matrix. We\ndemonstrate empirically the reliability of the model.\n", "versions": [{"version": "v1", "created": "Mon, 19 Mar 2018 14:34:35 GMT"}], "update_date": "2018-03-20", "authors_parsed": [["Pintea", "Silvia L.", ""], ["van Gemert", "Jan C.", ""], ["Smeulders", "Arnold W. M.", ""]]}, {"id": "1803.06962", "submitter": "Silvia-Laura Pintea", "authors": "Silvia L. Pintea, and Pascal S. Mettes, and Jan C. van Gemert, and\n  Arnold W. M. Smeulders", "title": "Featureless: Bypassing feature extraction in action categorization", "comments": "Published in the proceedings of the International Conference on Image\n  Processing (ICIP), 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This method introduces an efficient manner of learning action categories\nwithout the need of feature estimation. The approach starts from low-level\nvalues, in a similar style to the successful CNN methods. However, rather than\nextracting general image features, we learn to predict specific video\nrepresentations from raw video data. The benefit of such an approach is that at\nthe same computational expense it can predict 2 D video representations as well\nas 3 D ones, based on motion. The proposed model relies on discriminative\nWaldboost, which we enhance to a multiclass formulation for the purpose of\nlearning video representations. The suitability of the proposed approach as\nwell as its time efficiency are tested on the UCF11 action recognition dataset.\n", "versions": [{"version": "v1", "created": "Mon, 19 Mar 2018 14:47:09 GMT"}], "update_date": "2018-03-20", "authors_parsed": [["Pintea", "Silvia L.", ""], ["Mettes", "Pascal S.", ""], ["van Gemert", "Jan C.", ""], ["Smeulders", "Arnold W. M.", ""]]}, {"id": "1803.06978", "submitter": "Cihang Xie", "authors": "Cihang Xie, Zhishuai Zhang, Yuyin Zhou, Song Bai, Jianyu Wang, Zhou\n  Ren, Alan Yuille", "title": "Improving Transferability of Adversarial Examples with Input Diversity", "comments": "CVPR 2019, code is available at:\n  https://github.com/cihangxie/DI-2-FGSM", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Though CNNs have achieved the state-of-the-art performance on various vision\ntasks, they are vulnerable to adversarial examples --- crafted by adding\nhuman-imperceptible perturbations to clean images. However, most of the\nexisting adversarial attacks only achieve relatively low success rates under\nthe challenging black-box setting, where the attackers have no knowledge of the\nmodel structure and parameters. To this end, we propose to improve the\ntransferability of adversarial examples by creating diverse input patterns.\nInstead of only using the original images to generate adversarial examples, our\nmethod applies random transformations to the input images at each iteration.\nExtensive experiments on ImageNet show that the proposed attack method can\ngenerate adversarial examples that transfer much better to different networks\nthan existing baselines. By evaluating our method against top defense solutions\nand official baselines from NIPS 2017 adversarial competition, the enhanced\nattack reaches an average success rate of 73.0%, which outperforms the top-1\nattack submission in the NIPS competition by a large margin of 6.6%. We hope\nthat our proposed attack strategy can serve as a strong benchmark baseline for\nevaluating the robustness of networks to adversaries and the effectiveness of\ndifferent defense methods in the future. Code is available at\nhttps://github.com/cihangxie/DI-2-FGSM.\n", "versions": [{"version": "v1", "created": "Mon, 19 Mar 2018 15:07:51 GMT"}, {"version": "v2", "created": "Mon, 11 Jun 2018 00:15:38 GMT"}, {"version": "v3", "created": "Wed, 27 Mar 2019 02:29:17 GMT"}, {"version": "v4", "created": "Sat, 1 Jun 2019 17:12:24 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Xie", "Cihang", ""], ["Zhang", "Zhishuai", ""], ["Zhou", "Yuyin", ""], ["Bai", "Song", ""], ["Wang", "Jianyu", ""], ["Ren", "Zhou", ""], ["Yuille", "Alan", ""]]}, {"id": "1803.07015", "submitter": "Erkan Bostanci", "authors": "Ali Canberk Anar, Erkan Bostanci, Mehmet Serdar Guzel", "title": "Live Target Detection with Deep Learning Neural Network and Unmanned\n  Aerial Vehicle on Android Mobile Device", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the stages faced during the development of an Android\nprogram which obtains and decodes live images from DJI Phantom 3 Professional\nDrone and implements certain features of the TensorFlow Android Camera Demo\napplication. Test runs were made and outputs of the application were noted. A\nlake was classified as seashore, breakwater and pier with the proximities of\n24.44%, 21.16% and 12.96% respectfully. The joystick of the UAV controller and\nlaptop keyboard was classified with the proximities of 19.10% and 13.96%\nrespectfully. The laptop monitor was classified as screen, monitor and\ntelevision with the proximities of 18.77%, 14.76% and 14.00% respectfully. The\ncomputer used during the development of this study was classified as notebook\nand laptop with the proximities of 20.04% and 11.68% respectfully. A tractor\nparked at a parking lot was classified with the proximity of 12.88%. A group of\ncars in the same parking lot were classified as sports car, racer and\nconvertible with the proximities of 31.75%, 18.64% and 13.45% respectfully at\nan inference time of 851ms.\n", "versions": [{"version": "v1", "created": "Mon, 19 Mar 2018 16:15:22 GMT"}, {"version": "v2", "created": "Thu, 22 Mar 2018 17:57:00 GMT"}], "update_date": "2018-03-23", "authors_parsed": [["Anar", "Ali Canberk", ""], ["Bostanci", "Erkan", ""], ["Guzel", "Mehmet Serdar", ""]]}, {"id": "1803.07031", "submitter": "Agisilaos Chartsias", "authors": "Agisilaos Chartsias, Thomas Joyce, Giorgos Papanastasiou, Scott\n  Semple, Michelle Williams, David Newby, Rohan Dharmakumar, Sotirios A.\n  Tsaftaris", "title": "Factorised spatial representation learning: application in\n  semi-supervised myocardial segmentation", "comments": "Accepted in MICCAI 2018", "journal-ref": "International Conference on Medical Image Computing and Computer\n  Assisted Intervention (2018) pp. 490-498. Springer, Cham", "doi": "10.1007/978-3-030-00934-2_55", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The success and generalisation of deep learning algorithms heavily depend on\nlearning good feature representations. In medical imaging this entails\nrepresenting anatomical information, as well as properties related to the\nspecific imaging setting. Anatomical information is required to perform further\nanalysis, whereas imaging information is key to disentangle scanner variability\nand potential artefacts. The ability to factorise these would allow for\ntraining algorithms only on the relevant information according to the task. To\ndate, such factorisation has not been attempted. In this paper, we propose a\nmethodology of latent space factorisation relying on the cycle-consistency\nprinciple. As an example application, we consider cardiac MR segmentation,\nwhere we separate information related to the myocardium from other features\nrelated to imaging and surrounding substructures. We demonstrate the proposed\nmethod's utility in a semi-supervised setting: we use very few labelled images\ntogether with many unlabelled images to train a myocardium segmentation neural\nnetwork. Specifically, we achieve comparable performance to fully supervised\nnetworks using a fraction of labelled images in experiments on ACDC and a\ndataset from Edinburgh Imaging Facility QMRI. Code will be made available at\nhttps://github.com/agis85/spatial_factorisation.\n", "versions": [{"version": "v1", "created": "Mon, 19 Mar 2018 16:44:00 GMT"}, {"version": "v2", "created": "Fri, 2 Nov 2018 14:37:12 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Chartsias", "Agisilaos", ""], ["Joyce", "Thomas", ""], ["Papanastasiou", "Giorgos", ""], ["Semple", "Scott", ""], ["Williams", "Michelle", ""], ["Newby", "David", ""], ["Dharmakumar", "Rohan", ""], ["Tsaftaris", "Sotirios A.", ""]]}, {"id": "1803.07066", "submitter": "Jifeng Dai", "authors": "Jiayuan Gu, Han Hu, Liwei Wang, Yichen Wei, Jifeng Dai", "title": "Learning Region Features for Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While most steps in the modern object detection methods are learnable, the\nregion feature extraction step remains largely hand-crafted, featured by RoI\npooling methods. This work proposes a general viewpoint that unifies existing\nregion feature extraction methods and a novel method that is end-to-end\nlearnable. The proposed method removes most heuristic choices and outperforms\nits RoI pooling counterparts. It moves further towards fully learnable object\ndetection.\n", "versions": [{"version": "v1", "created": "Mon, 19 Mar 2018 17:58:50 GMT"}], "update_date": "2018-03-20", "authors_parsed": [["Gu", "Jiayuan", ""], ["Hu", "Han", ""], ["Wang", "Liwei", ""], ["Wei", "Yichen", ""], ["Dai", "Jifeng", ""]]}, {"id": "1803.07100", "submitter": "Jiawei Chen", "authors": "Jiawei Chen, Janusz Konrad, Prakash Ishwar", "title": "VGAN-Based Image Representation Learning for Privacy-Preserving Facial\n  Expression Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reliable facial expression recognition plays a critical role in human-machine\ninteractions. However, most of the facial expression analysis methodologies\nproposed to date pay little or no attention to the protection of a user's\nprivacy. In this paper, we propose a Privacy-Preserving Representation-Learning\nVariational Generative Adversarial Network (PPRL-VGAN) to learn an image\nrepresentation that is explicitly disentangled from the identity information.\nAt the same time, this representation is discriminative from the standpoint of\nfacial expression recognition and generative as it allows expression-equivalent\nface image synthesis. We evaluate the proposed model on two public datasets\nunder various threat scenarios. Quantitative and qualitative results\ndemonstrate that our approach strikes a balance between the preservation of\nprivacy and data utility. We further demonstrate that our model can be\neffectively applied to other tasks such as expression morphing and image\ncompletion.\n", "versions": [{"version": "v1", "created": "Mon, 19 Mar 2018 18:14:35 GMT"}, {"version": "v2", "created": "Fri, 7 Sep 2018 14:28:15 GMT"}], "update_date": "2018-09-10", "authors_parsed": [["Chen", "Jiawei", ""], ["Konrad", "Janusz", ""], ["Ishwar", "Prakash", ""]]}, {"id": "1803.07113", "submitter": "Pengkai Zhu", "authors": "Pengkai Zhu, Hanxiao Wang, Venkatesh Saligrama", "title": "Zero-Shot Detection", "comments": "Published in IEEE Transactions on Circuits and Systems for Video\n  Technology", "journal-ref": null, "doi": "10.1109/TCSVT.2019.2899569", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As we move towards large-scale object detection, it is unrealistic to expect\nannotated training data, in the form of bounding box annotations around\nobjects, for all object classes at sufficient scale, and so methods capable of\nunseen object detection are required. We propose a novel zero-shot method based\non training an end-to-end model that fuses semantic attribute prediction with\nvisual features to propose object bounding boxes for seen and unseen classes.\nWhile we utilize semantic features during training, our method is agnostic to\nsemantic information for unseen classes at test-time. Our method retains the\nefficiency and effectiveness of YOLOv2 for objects seen during training, while\nimproving its performance for novel and unseen objects. The ability of\nstate-of-art detection methods to learn discriminative object features to\nreject background proposals also limits their performance for unseen objects.\nWe posit that, to detect unseen objects, we must incorporate semantic\ninformation into the visual domain so that the learned visual features reflect\nthis information and leads to improved recall rates for unseen objects. We test\nour method on PASCAL VOC and MS COCO dataset and observed significant\nimprovements on the average precision of unseen classes.\n", "versions": [{"version": "v1", "created": "Mon, 19 Mar 2018 18:48:41 GMT"}, {"version": "v2", "created": "Tue, 19 Mar 2019 16:38:24 GMT"}], "update_date": "2019-03-20", "authors_parsed": [["Zhu", "Pengkai", ""], ["Wang", "Hanxiao", ""], ["Saligrama", "Venkatesh", ""]]}, {"id": "1803.07125", "submitter": "Jeng-Hau Lin", "authors": "Jeng-Hau Lin, Yunfan Yang, Rajesh Gupta, Zhuowen Tu", "title": "Local Binary Pattern Networks", "comments": "14 pages, 10 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Memory and computation efficient deep learning architec- tures are crucial to\ncontinued proliferation of machine learning capabili- ties to new platforms and\nsystems. Binarization of operations in convo- lutional neural networks has\nshown promising results in reducing model size and computing efficiency. In\nthis paper, we tackle the problem us- ing a strategy different from the\nexisting literature by proposing local binary pattern networks or LBPNet, that\nis able to learn and perform binary operations in an end-to-end fashion.\nLBPNet1 uses local binary comparisons and random projection in place of\nconventional convolu- tion (or approximation of convolution) operations. These\noperations can be implemented efficiently on different platforms including\ndirect hard- ware implementation. We applied LBPNet and its variants on\nstandard benchmarks. The results are promising across benchmarks while provid-\ning an important means to improve memory and speed efficiency that is\nparticularly suited for small footprint devices and hardware accelerators.\n", "versions": [{"version": "v1", "created": "Mon, 19 Mar 2018 19:12:19 GMT"}, {"version": "v2", "created": "Thu, 22 Mar 2018 03:06:13 GMT"}], "update_date": "2018-03-23", "authors_parsed": [["Lin", "Jeng-Hau", ""], ["Yang", "Yunfan", ""], ["Gupta", "Rajesh", ""], ["Tu", "Zhuowen", ""]]}, {"id": "1803.07140", "submitter": "Brandon RichardWebster", "authors": "Brandon RichardWebster, So Yon Kwon, Christopher Clarizio, Samuel E.\n  Anthony, Walter J. Scheirer", "title": "Visual Psychophysics for Making Face Recognition Algorithms More\n  Explainable", "comments": "20 pages, 5 figures. To appear Proceedings of the European Conference\n  on Computer Vision (ECCV). For supplemental material see\n  http://bjrichardwebster.com/papers/menagerie/supp", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scientific fields that are interested in faces have developed their own sets\nof concepts and procedures for understanding how a target model system (be it a\nperson or algorithm) perceives a face under varying conditions. In computer\nvision, this has largely been in the form of dataset evaluation for recognition\ntasks where summary statistics are used to measure progress. While aggregate\nperformance has continued to improve, understanding individual causes of\nfailure has been difficult, as it is not always clear why a particular face\nfails to be recognized, or why an impostor is recognized by an algorithm.\nImportantly, other fields studying vision have addressed this via the use of\nvisual psychophysics: the controlled manipulation of stimuli and careful study\nof the responses they evoke in a model system. In this paper, we suggest that\nvisual psychophysics is a viable methodology for making face recognition\nalgorithms more explainable. A comprehensive set of procedures is developed for\nassessing face recognition algorithm behavior, which is then deployed over\nstate-of-the-art convolutional neural networks and more basic, yet still widely\nused, shallow and handcrafted feature-based approaches.\n", "versions": [{"version": "v1", "created": "Mon, 19 Mar 2018 19:50:54 GMT"}, {"version": "v2", "created": "Thu, 19 Jul 2018 18:07:53 GMT"}], "update_date": "2018-07-23", "authors_parsed": [["RichardWebster", "Brandon", ""], ["Kwon", "So Yon", ""], ["Clarizio", "Christopher", ""], ["Anthony", "Samuel E.", ""], ["Scheirer", "Walter J.", ""]]}, {"id": "1803.07179", "submitter": "Qilin Zhang", "authors": "Jinliang Zang, Le Wang, Ziyi Liu, Qilin Zhang, Zhenxing Niu, Gang Hua,\n  Nanning Zheng", "title": "Attention-based Temporal Weighted Convolutional Neural Network for\n  Action Recognition", "comments": "14th International Conference on Artificial Intelligence Applications\n  and Innovations (AIAI 2018), May 25-27, 2018, Rhodes, Greece", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research in human action recognition has accelerated significantly since the\nintroduction of powerful machine learning tools such as Convolutional Neural\nNetworks (CNNs). However, effective and efficient methods for incorporation of\ntemporal information into CNNs are still being actively explored in the recent\nliterature. Motivated by the popular recurrent attention models in the research\narea of natural language processing, we propose the Attention-based Temporal\nWeighted CNN (ATW), which embeds a visual attention model into a temporal\nweighted multi-stream CNN. This attention model is simply implemented as\ntemporal weighting yet it effectively boosts the recognition performance of\nvideo representations. Besides, each stream in the proposed ATW framework is\ncapable of end-to-end training, with both network parameters and temporal\nweights optimized by stochastic gradient descent (SGD) with backpropagation.\nOur experiments show that the proposed attention mechanism contributes\nsubstantially to the performance gains with the more discriminative snippets by\nfocusing on more relevant video segments.\n", "versions": [{"version": "v1", "created": "Mon, 19 Mar 2018 22:09:49 GMT"}], "update_date": "2018-03-21", "authors_parsed": [["Zang", "Jinliang", ""], ["Wang", "Le", ""], ["Liu", "Ziyi", ""], ["Zhang", "Qilin", ""], ["Niu", "Zhenxing", ""], ["Hua", "Gang", ""], ["Zheng", "Nanning", ""]]}, {"id": "1803.07187", "submitter": "Luca Calatroni", "authors": "Luca Calatroni, Marie d'Autume, Rob Hocking, Stella Panayotova, Simone\n  Parisotto, Paola Ricciardi, Carola-Bibiane Sch\\\"onlieb", "title": "Unveiling the invisible - mathematical methods for restoring and\n  interpreting illuminated manuscripts", "comments": null, "journal-ref": null, "doi": "10.1186/s40494-018-0216-z", "report-no": null, "categories": "cs.CV eess.IV math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The last fifty years have seen an impressive development of mathematical\nmethods for the analysis and processing of digital images, mostly in the\ncontext of photography, biomedical imaging and various forms of engineering.\nThe arts have been mostly overlooked in this process, apart from a few\nexceptional works in the last ten years. With the rapid emergence of\ndigitisation in the arts, however, the arts domain is becoming increasingly\nreceptive to digital image processing methods and the importance of paying\nattention to this therefore increases. In this paper we discuss a range of\nmathematical methods for digital image restoration and digital visualisation\nfor illuminated manuscripts. The latter provide an interesting opportunity for\ndigital manipulation because they traditionally remain physically untouched. At\nthe same time they also serve as an example for the possibilities mathematics\nand digital restoration offer as a generic and objective toolkit for the arts.\n", "versions": [{"version": "v1", "created": "Mon, 19 Mar 2018 22:50:53 GMT"}], "update_date": "2019-04-01", "authors_parsed": [["Calatroni", "Luca", ""], ["d'Autume", "Marie", ""], ["Hocking", "Rob", ""], ["Panayotova", "Stella", ""], ["Parisotto", "Simone", ""], ["Ricciardi", "Paola", ""], ["Sch\u00f6nlieb", "Carola-Bibiane", ""]]}, {"id": "1803.07191", "submitter": "Tolga Birdal", "authors": "Tolga Birdal, Benjamin Busam, Nassir Navab, Slobodan Ilic and Peter\n  Sturm", "title": "A Minimalist Approach to Type-Agnostic Detection of Quadrics in Point\n  Clouds", "comments": "Accepted for publication at CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a segmentation-free, automatic and efficient procedure to\ndetect general geometric quadric forms in point clouds, where clutter and\nocclusions are inevitable. Our everyday world is dominated by man-made objects\nwhich are designed using 3D primitives (such as planes, cones, spheres,\ncylinders, etc.). These objects are also omnipresent in industrial\nenvironments. This gives rise to the possibility of abstracting 3D scenes\nthrough primitives, thereby positions these geometric forms as an integral part\nof perception and high level 3D scene understanding.\n  As opposed to state-of-the-art, where a tailored algorithm treats each\nprimitive type separately, we propose to encapsulate all types in a single\nrobust detection procedure. At the center of our approach lies a closed form 3D\nquadric fit, operating in both primal & dual spaces and requiring as low as 4\noriented-points. Around this fit, we design a novel, local null-space voting\nstrategy to reduce the 4-point case to 3. Voting is coupled with the famous\nRANSAC and makes our algorithm orders of magnitude faster than its conventional\ncounterparts. This is the first method capable of performing a generic\ncross-type multi-object primitive detection in difficult scenes. Results on\nsynthetic and real datasets support the validity of our method.\n", "versions": [{"version": "v1", "created": "Mon, 19 Mar 2018 23:01:01 GMT"}], "update_date": "2018-03-21", "authors_parsed": [["Birdal", "Tolga", ""], ["Busam", "Benjamin", ""], ["Navab", "Nassir", ""], ["Ilic", "Slobodan", ""], ["Sturm", "Peter", ""]]}, {"id": "1803.07192", "submitter": "Raunak Dey", "authors": "Raunak Dey, Zhongjie Lu, Yi Hong", "title": "Diagnostic Classification Of Lung Nodules Using 3D Neural Networks", "comments": "Accepted for publication in IEEE International Symposium on\n  Biomedical Imaging (ISBI) 2018 Copyright c 2018 IEEE", "journal-ref": null, "doi": "10.1109/ISBI.2018.8363687", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lung cancer is the leading cause of cancer-related death worldwide. Early\ndiagnosis of pulmonary nodules in Computed Tomography (CT) chest scans provides\nan opportunity for designing effective treatment and making financial and care\nplans. In this paper, we consider the problem of diagnostic classification\nbetween benign and malignant lung nodules in CT images, which aims to learn a\ndirect mapping from 3D images to class labels. To achieve this goal, four\ntwo-pathway Convolutional Neural Networks (CNN) are proposed, including a basic\n3D CNN, a novel multi-output network, a 3D DenseNet, and an augmented 3D\nDenseNet with multi-outputs. These four networks are evaluated on the public\nLIDC-IDRI dataset and outperform most existing methods. In particular, the 3D\nmulti-output DenseNet (MoDenseNet) achieves the state-of-the-art classification\naccuracy on the task of end-to-end lung nodule diagnosis. In addition, the\nnetworks pretrained on the LIDC-IDRI dataset can be further extended to handle\nsmaller datasets using transfer learning. This is demonstrated on our dataset\nwith encouraging prediction accuracy in lung nodule classification.\n", "versions": [{"version": "v1", "created": "Mon, 19 Mar 2018 23:02:37 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Dey", "Raunak", ""], ["Lu", "Zhongjie", ""], ["Hong", "Yi", ""]]}, {"id": "1803.07195", "submitter": "Ebrahim Karami", "authors": "Ebrahim Karami, Mohamed Shehata, and Andrew Smith", "title": "Adaptive Polar Active Contour for Segmentation and Tracking in\n  Ultrasound Videos", "comments": "Accepted for publication in IEEE Transactions on Circuit and Systems\n  for Video Technology", "journal-ref": null, "doi": "10.1109/TCSVT.2018.2818072", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detection of relative changes in circulating blood volume is important to\nguide resuscitation and manage a variety of medical conditions including\nsepsis, trauma, dialysis and congestive heart failure. Recent studies have\nshown that estimates of circulating blood volume can be obtained from the\ncross-sectional area (CSA) of the internal jugular vein (IJV) from ultrasound\nimages. However, accurate segmentation and tracking of the IJV in ultrasound\nimaging is a challenging task and is significantly influenced by a number of\nparameters such as the image quality, shape, and temporal variation. In this\npaper, we propose a novel adaptive polar active contour (Ad-PAC) algorithm for\nthe segmentation and tracking of the IJV in ultrasound videos. In the proposed\nalgorithm, the parameters of the Ad-PAC algorithm are adapted based on the\nresults of segmentation in previous frames. The Ad-PAC algorithm is applied to\n65 ultrasound videos captured from 13 healthy subjects, with each video\ncontaining 450 frames. The results show that spatial and temporal adaptation of\nthe energy function significantly improves segmentation performance when\ncompared to current state-of-the-art active contour algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 19 Mar 2018 23:30:06 GMT"}], "update_date": "2018-03-21", "authors_parsed": [["Karami", "Ebrahim", ""], ["Shehata", "Mohamed", ""], ["Smith", "Andrew", ""]]}, {"id": "1803.07201", "submitter": "Wenqian Liu", "authors": "Wenqian Liu, Abhishek Sharma, Octavia Camps, Mario Sznaier", "title": "DYAN: A Dynamical Atoms-Based Network for Video Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to anticipate the future is essential when making real time\ncritical decisions, provides valuable information to understand dynamic natural\nscenes, and can help unsupervised video representation learning. State-of-art\nvideo prediction is based on LSTM recursive networks and/or generative\nadversarial network learning. These are complex architectures that need to\nlearn large numbers of parameters, are potentially hard to train, slow to run,\nand may produce blurry predictions. In this paper, we introduce DYAN, a novel\nnetwork with very few parameters and easy to train, which produces accurate,\nhigh quality frame predictions, significantly faster than previous approaches.\nDYAN owes its good qualities to its encoder and decoder, which are designed\nfollowing concepts from systems identification theory and exploit the\ndynamics-based invariants of the data. Extensive experiments using several\nstandard video datasets show that DYAN is superior generating frames and that\nit generalizes well across domains.\n", "versions": [{"version": "v1", "created": "Tue, 20 Mar 2018 00:14:23 GMT"}, {"version": "v2", "created": "Fri, 14 Sep 2018 21:25:54 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Liu", "Wenqian", ""], ["Sharma", "Abhishek", ""], ["Camps", "Octavia", ""], ["Sznaier", "Mario", ""]]}, {"id": "1803.07212", "submitter": "Noranart Vesdapunt", "authors": "Baoyuan Wang, Noranart Vesdapunt, Utkarsh Sinha, Lei Zhang", "title": "Real-time Burst Photo Selection Using a Light-Head Adversarial Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an automatic moment capture system that runs in real-time on\nmobile cameras. The system is designed to run in the viewfinder mode and\ncapture a burst sequence of frames before and after the shutter is pressed. For\neach frame, the system predicts in real-time a \"goodness\" score, based on which\nthe best moment in the burst can be selected immediately after the shutter is\nreleased, without any user interference. To solve the problem, we develop a\nhighly efficient deep neural network ranking model, which implicitly learns a\n\"latent relative attribute\" space to capture subtle visual differences within a\nsequence of burst images. Then the overall goodness is computed as a linear\naggregation of the goodnesses of all the latent attributes. The latent relative\nattributes and the aggregation function can be seamlessly integrated in one\nfully convolutional network and trained in an end-to-end fashion. To obtain a\ncompact model which can run on mobile devices in real-time, we have explored\nand evaluated a wide range of network design choices, taking into account the\nconstraints of model size, computational cost, and accuracy. Extensive studies\nshow that the best frame predicted by our model hit users' top-1 (out of 11 on\naverage) choice for $64.1\\%$ cases and top-3 choices for $86.2\\%$ cases.\nMoreover, the model(only 0.47M Bytes) can run in real time on mobile devices,\ne.g. only 13ms on iPhone 7 for one frame prediction.\n", "versions": [{"version": "v1", "created": "Tue, 20 Mar 2018 01:26:33 GMT"}], "update_date": "2018-03-21", "authors_parsed": [["Wang", "Baoyuan", ""], ["Vesdapunt", "Noranart", ""], ["Sinha", "Utkarsh", ""], ["Zhang", "Lei", ""]]}, {"id": "1803.07218", "submitter": "Ximeng Sun", "authors": "Ximeng Sun, Ryan Szeto, Jason J. Corso", "title": "A Temporally-Aware Interpolation Network for Video Frame Inpainting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose the first deep learning solution to video frame inpainting, a\nchallenging instance of the general video inpainting problem with applications\nin video editing, manipulation, and forensics. Our task is less ambiguous than\nframe interpolation and video prediction because we have access to both the\ntemporal context and a partial glimpse of the future, allowing us to better\nevaluate the quality of a model's predictions objectively. We devise a pipeline\ncomposed of two modules: a bidirectional video prediction module, and a\ntemporally-aware frame interpolation module. The prediction module makes two\nintermediate predictions of the missing frames, one conditioned on the\npreceding frames and the other conditioned on the following frames, using a\nshared convolutional LSTM-based encoder-decoder. The interpolation module\nblends the intermediate predictions to form the final result. Specifically, it\nutilizes time information and hidden activations from the video prediction\nmodule to resolve disagreements between the predictions. Our experiments\ndemonstrate that our approach produces more accurate and qualitatively\nsatisfying results than a state-of-the-art video prediction method and many\nstrong frame inpainting baselines.\n", "versions": [{"version": "v1", "created": "Tue, 20 Mar 2018 02:01:37 GMT"}, {"version": "v2", "created": "Sat, 3 Nov 2018 23:34:33 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Sun", "Ximeng", ""], ["Szeto", "Ryan", ""], ["Corso", "Jason J.", ""]]}, {"id": "1803.07226", "submitter": "Jinshi Yu", "authors": "Jinshi Yu, Guoxu Zhou, Andrzej Cichocki, Shengli Xie", "title": "Learning the Hierarchical Parts of Objects by Deep Non-Smooth\n  Nonnegative Matrix Factorization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DS cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonsmooth Nonnegative Matrix Factorization (nsNMF) is capable of producing\nmore localized, less overlapped feature representations than other variants of\nNMF while keeping satisfactory fit to data. However, nsNMF as well as other\nexisting NMF methods is incompetent to learn hierarchical features of complex\ndata due to its shallow structure. To fill this gap, we propose a deep nsNMF\nmethod coined by the fact that it possesses a deeper architecture compared with\nstandard nsNMF. The deep nsNMF not only gives parts-based features due to the\nnonnegativity constraints, but also creates higher-level, more abstract\nfeatures by combing lower-level ones. The in-depth description of how deep\narchitecture can help to efficiently discover abstract features in dnsNMF is\npresented. And we also show that the deep nsNMF has close relationship with the\ndeep autoencoder, suggesting that the proposed model inherits the major\nadvantages from both deep learning and NMF. Extensive experiments demonstrate\nthe standout performance of the proposed method in clustering analysis.\n", "versions": [{"version": "v1", "created": "Tue, 20 Mar 2018 02:39:44 GMT"}], "update_date": "2018-03-21", "authors_parsed": [["Yu", "Jinshi", ""], ["Zhou", "Guoxu", ""], ["Cichocki", "Andrzej", ""], ["Xie", "Shengli", ""]]}, {"id": "1803.07231", "submitter": "Quoc-Huy Tran", "authors": "Mohammed E. Fathy, Quoc-Huy Tran, M. Zeeshan Zia, Paul Vernaza,\n  Manmohan Chandraker", "title": "Hierarchical Metric Learning and Matching for 2D and 3D Geometric\n  Correspondences", "comments": "Accepted to ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interest point descriptors have fueled progress on almost every problem in\ncomputer vision. Recent advances in deep neural networks have enabled\ntask-specific learned descriptors that outperform hand-crafted descriptors on\nmany problems. We demonstrate that commonly used metric learning approaches do\nnot optimally leverage the feature hierarchies learned in a Convolutional\nNeural Network (CNN), especially when applied to the task of geometric feature\nmatching. While a metric loss applied to the deepest layer of a CNN, is often\nexpected to yield ideal features irrespective of the task, in fact the growing\nreceptive field as well as striding effects cause shallower features to be\nbetter at high precision matching tasks. We leverage this insight together with\nexplicit supervision at multiple levels of the feature hierarchy for better\nregularization, to learn more effective descriptors in the context of geometric\nmatching tasks. Further, we propose to use activation maps at different layers\nof a CNN, as an effective and principled replacement for the multi-resolution\nimage pyramids often used for matching tasks. We propose concrete CNN\narchitectures employing these ideas, and evaluate them on multiple datasets for\n2D and 3D geometric matching as well as optical flow, demonstrating\nstate-of-the-art results and generalization across datasets.\n", "versions": [{"version": "v1", "created": "Tue, 20 Mar 2018 03:11:41 GMT"}, {"version": "v2", "created": "Tue, 27 Mar 2018 02:09:09 GMT"}, {"version": "v3", "created": "Thu, 2 Aug 2018 03:12:43 GMT"}], "update_date": "2018-08-03", "authors_parsed": [["Fathy", "Mohammed E.", ""], ["Tran", "Quoc-Huy", ""], ["Zia", "M. Zeeshan", ""], ["Vernaza", "Paul", ""], ["Chandraker", "Manmohan", ""]]}, {"id": "1803.07240", "submitter": "Arnold Wiliem", "authors": "Teng Zhang, Johanna Carvajal, Daniel F. Smith, Kun Zhao, Arnold\n  Wiliem, Peter Hobson, Anthony Jennings and Brian C. Lovell", "title": "SlideNet: Fast and Accurate Slide Quality Assessment Based on Deep\n  Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work tackles the automatic fine-grained slide quality assessment problem\nfor digitized direct smears test using the Gram staining protocol. Automatic\nquality assessment can provide useful information for the pathologists and the\nwhole digital pathology workflow. For instance, if the system found a slide to\nhave a low staining quality, it could send a request to the automatic slide\npreparation system to remake the slide. If the system detects severe damage in\nthe slides, it could notify the experts that manual microscope reading may be\nrequired. In order to address the quality assessment problem, we propose a deep\nneural network based framework to automatically assess the slide quality in a\nsemantic way. Specifically, the first step of our framework is to perform dense\nfine-grained region classification on the whole slide and calculate the region\ndistribution histogram. Next, our framework will generate assessments of the\nslide quality from various perspectives: staining quality, information density,\ndamage level and which regions are more valuable for subsequent\nhigh-magnification analysis. To make the information more accessible, we\npresent our results in the form of a heat map and text summaries. Additionally,\nin order to stimulate research in this direction, we propose a novel dataset\nfor slide quality assessment. Experiments show that the proposed framework\noutperforms recent related works.\n", "versions": [{"version": "v1", "created": "Tue, 20 Mar 2018 03:29:03 GMT"}], "update_date": "2018-03-21", "authors_parsed": [["Zhang", "Teng", ""], ["Carvajal", "Johanna", ""], ["Smith", "Daniel F.", ""], ["Zhao", "Kun", ""], ["Wiliem", "Arnold", ""], ["Hobson", "Peter", ""], ["Jennings", "Anthony", ""], ["Lovell", "Brian C.", ""]]}, {"id": "1803.07252", "submitter": "Jin Zeng", "authors": "Jin Zeng, Gene Cheung, Michael Ng, Jiahao Pang and Cheng Yang", "title": "3D Point Cloud Denoising using Graph Laplacian Regularization of a Low\n  Dimensional Manifold Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D point cloud - a new signal representation of volumetric objects - is a\ndiscrete collection of triples marking exterior object surface locations in 3D\nspace. Conventional imperfect acquisition processes of 3D point cloud - e.g.,\nstereo-matching from multiple viewpoint images or depth data acquired directly\nfrom active light sensors - imply non-negligible noise in the data. In this\npaper, we adopt a previously proposed low-dimensional manifold model for the\nsurface patches in the point cloud and seek self-similar patches to denoise\nthem simultaneously using the patch manifold prior. Due to discrete\nobservations of the patches on the manifold, we approximate the manifold\ndimension computation defined in the continuous domain with a patch-based graph\nLaplacian regularizer and propose a new discrete patch distance measure to\nquantify the similarity between two same-sized surface patches for graph\nconstruction that is robust to noise. We show that our graph Laplacian\nregularizer has a natural graph spectral interpretation, and has desirable\nnumerical stability properties via eigenanalysis. Extensive simulation results\nshow that our proposed denoising scheme can outperform state-of-the-art methods\nin objective metrics and can better preserve visually salient structural\nfeatures like edges.\n", "versions": [{"version": "v1", "created": "Tue, 20 Mar 2018 04:29:46 GMT"}, {"version": "v2", "created": "Tue, 30 Apr 2019 05:39:28 GMT"}], "update_date": "2019-05-01", "authors_parsed": [["Zeng", "Jin", ""], ["Cheung", "Gene", ""], ["Ng", "Michael", ""], ["Pang", "Jiahao", ""], ["Yang", "Cheng", ""]]}, {"id": "1803.07253", "submitter": "Lu Xu", "authors": "Lu Xu, Jinhai Xiang, Xiaohui Yuan", "title": "Transferring Rich Deep Features for Facial Beauty Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature extraction plays a significant part in computer vision tasks. In this\npaper, we propose a method which transfers rich deep features from a pretrained\nmodel on face verification task and feeds the features into Bayesian ridge\nregression algorithm for facial beauty prediction. We leverage the deep neural\nnetworks that extracts more abstract features from stacked layers. Through\nsimple but effective feature fusion strategy, our method achieves improved or\ncomparable performance on SCUT-FBP dataset and ECCV HotOrNot dataset. Our\nexperiments demonstrate the effectiveness of the proposed method and clarify\nthe inner interpretability of facial beauty perception.\n", "versions": [{"version": "v1", "created": "Tue, 20 Mar 2018 04:39:28 GMT"}], "update_date": "2018-03-21", "authors_parsed": [["Xu", "Lu", ""], ["Xiang", "Jinhai", ""], ["Yuan", "Xiaohui", ""]]}, {"id": "1803.07268", "submitter": "Tianyu Yang", "authors": "Tianyu Yang, Antoni B. Chan", "title": "Learning Dynamic Memory Networks for Object Tracking", "comments": "ECCV2018 camera ready. Code is available at\n  https://github.com/skyoung/MemTrack", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Template-matching methods for visual tracking have gained popularity recently\ndue to their comparable performance and fast speed. However, they lack\neffective ways to adapt to changes in the target object's appearance, making\ntheir tracking accuracy still far from state-of-the-art. In this paper, we\npropose a dynamic memory network to adapt the template to the target's\nappearance variations during tracking. An LSTM is used as a memory controller,\nwhere the input is the search feature map and the outputs are the control\nsignals for the reading and writing process of the memory block. As the\nlocation of the target is at first unknown in the search feature map, an\nattention mechanism is applied to concentrate the LSTM input on the potential\ntarget. To prevent aggressive model adaptivity, we apply gated residual\ntemplate learning to control the amount of retrieved memory that is used to\ncombine with the initial template. Unlike tracking-by-detection methods where\nthe object's information is maintained by the weight parameters of neural\nnetworks, which requires expensive online fine-tuning to be adaptable, our\ntracker runs completely feed-forward and adapts to the target's appearance\nchanges by updating the external memory. Moreover, unlike other tracking\nmethods where the model capacity is fixed after offline training --- the\ncapacity of our tracker can be easily enlarged as the memory requirements of a\ntask increase, which is favorable for memorizing long-term object information.\nExtensive experiments on OTB and VOT demonstrates that our tracker MemTrack\nperforms favorably against state-of-the-art tracking methods while retaining\nreal-time speed of 50 fps.\n", "versions": [{"version": "v1", "created": "Tue, 20 Mar 2018 06:20:15 GMT"}, {"version": "v2", "created": "Sun, 2 Sep 2018 05:09:02 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Yang", "Tianyu", ""], ["Chan", "Antoni B.", ""]]}, {"id": "1803.07278", "submitter": "Tanvi Goswami Ms", "authors": "Tanvi Goswami and Zankhana Barad and Prof. Nikita P. Desai", "title": "Text Detection and Recognition in images: A survey", "comments": "The paper is found to be incomplete, it was just for assignment\n  purpose so it was uploaded but some extra efforts are needed and supposed to\n  have many changes because as a student we might not be aware of many\n  things,so it might not be useful for others to refer it", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text Detection and recognition is a one of the important aspect of image\nprocessing. This paper analyzes and compares the methods to handle this task.\nIt summarizes the fundamental problems and enumerates factors that need\nconsideration when addressing these problems. Existing techniques are\ncategorized as either stepwise or integrated and sub-problems are highlighted\nincluding digit localization, verification, segmentation and recognition.\nSpecial issues associated with the enhancement of degraded text and the\nprocessing of video text and multi-oriented text are also addressed. The\ncategories and sub-categories of text are illustrated, benchmark datasets are\nenumerated, and the performance of the most representative approaches is\ncompared. This review also provides a fundamental comparison and analysis of\nthe remaining problems in the field.\n", "versions": [{"version": "v1", "created": "Tue, 20 Mar 2018 07:36:48 GMT"}, {"version": "v2", "created": "Wed, 2 May 2018 14:45:42 GMT"}], "update_date": "2018-05-03", "authors_parsed": [["Goswami", "Tanvi", ""], ["Barad", "Zankhana", ""], ["Desai", "Prof. Nikita P.", ""]]}, {"id": "1803.07288", "submitter": "Nikita Desai Prof", "authors": "Raunak Dave, Ankit Vyas, Nikita P Desai", "title": "Face Recognition Techniques: A Survey", "comments": "Work in progress", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays research has expanded to extracting auxiliary information from\nvarious biometric techniques like fingerprints, face, iris, palm and voice .\nThis information contains some major features like gender, age, beard,\nmustache, scars, height, hair, skin color, glasses, weight, facial marks and\ntattoos. All this information contributes strongly to identification of human.\nThe major challenges that come across face recognition are to find age & gender\nof the person. This paper contributes a survey of various face recognition\ntechniques for finding the age and gender. The existing techniques are\ndiscussed based on their performances. This paper also provides future\ndirections for further research.\n", "versions": [{"version": "v1", "created": "Tue, 20 Mar 2018 08:15:54 GMT"}, {"version": "v2", "created": "Wed, 11 Apr 2018 07:00:19 GMT"}, {"version": "v3", "created": "Sat, 12 May 2018 13:24:48 GMT"}, {"version": "v4", "created": "Sat, 25 Aug 2018 05:02:45 GMT"}, {"version": "v5", "created": "Tue, 28 Aug 2018 16:06:38 GMT"}, {"version": "v6", "created": "Sat, 30 Jan 2021 07:34:50 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Dave", "Raunak", ""], ["Vyas", "Ankit", ""], ["Desai", "Nikita P", ""]]}, {"id": "1803.07289", "submitter": "Patrick Wieschollek", "authors": "Fabian Groh, Patrick Wieschollek, Hendrik P.A. Lensch", "title": "Flex-Convolution (Million-Scale Point-Cloud Learning Beyond Grid-Worlds)", "comments": "accepted at ACCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional convolution layers are specifically designed to exploit the\nnatural data representation of images -- a fixed and regular grid. However,\nunstructured data like 3D point clouds containing irregular neighborhoods\nconstantly breaks the grid-based data assumption. Therefore applying\nbest-practices and design choices from 2D-image learning methods towards\nprocessing point clouds are not readily possible. In this work, we introduce a\nnatural generalization flex-convolution of the conventional convolution layer\nalong with an efficient GPU implementation. We demonstrate competitive\nperformance on rather small benchmark sets using fewer parameters and lower\nmemory consumption and obtain significant improvements on a million-scale\nreal-world dataset. Ours is the first which allows to efficiently process 7\nmillion points concurrently.\n", "versions": [{"version": "v1", "created": "Tue, 20 Mar 2018 08:18:29 GMT"}, {"version": "v2", "created": "Sun, 8 Jul 2018 12:14:27 GMT"}, {"version": "v3", "created": "Thu, 25 Oct 2018 09:06:48 GMT"}, {"version": "v4", "created": "Wed, 15 Apr 2020 07:22:36 GMT"}], "update_date": "2020-04-16", "authors_parsed": [["Groh", "Fabian", ""], ["Wieschollek", "Patrick", ""], ["Lensch", "Hendrik P. A.", ""]]}, {"id": "1803.07293", "submitter": "Jianming Lv", "authors": "Jianming Lv, Weihang Chen, Qing Li, Can Yang", "title": "Unsupervised Cross-dataset Person Re-identification by Transfer Learning\n  of Spatial-Temporal Patterns", "comments": "Accepted by CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of the proposed person re-identification algorithms conduct supervised\ntraining and testing on single labeled datasets with small size, so directly\ndeploying these trained models to a large-scale real-world camera network may\nlead to poor performance due to underfitting. It is challenging to\nincrementally optimize the models by using the abundant unlabeled data\ncollected from the target domain. To address this challenge, we propose an\nunsupervised incremental learning algorithm, TFusion, which is aided by the\ntransfer learning of the pedestrians' spatio-temporal patterns in the target\ndomain. Specifically, the algorithm firstly transfers the visual classifier\ntrained from small labeled source dataset to the unlabeled target dataset so as\nto learn the pedestrians' spatial-temporal patterns. Secondly, a Bayesian\nfusion model is proposed to combine the learned spatio-temporal patterns with\nvisual features to achieve a significantly improved classifier. Finally, we\npropose a learning-to-rank based mutual promotion procedure to incrementally\noptimize the classifiers based on the unlabeled data in the target domain.\nComprehensive experiments based on multiple real surveillance datasets are\nconducted, and the results show that our algorithm gains significant\nimprovement compared with the state-of-art cross-dataset unsupervised person\nre-identification algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 20 Mar 2018 08:33:08 GMT"}], "update_date": "2018-03-21", "authors_parsed": [["Lv", "Jianming", ""], ["Chen", "Weihang", ""], ["Li", "Qing", ""], ["Yang", "Can", ""]]}, {"id": "1803.07301", "submitter": "Xiaohang Fu", "authors": "Xiaohang Fu, Tong Liu, Zhaohan Xiong, Bruce H. Smaill, Martin K.\n  Stiles, Jichao Zhao", "title": "Segmentation of histological images and fibrosis identification with a\n  convolutional neural network", "comments": null, "journal-ref": null, "doi": "10.1016/j.compbiomed.2018.05.015", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Segmentation of histological images is one of the most crucial tasks for many\nbiomedical analyses including quantification of certain tissue type. However,\nchallenges are posed by high variability and complexity of structural features\nin such images, in addition to imaging artifacts. Further, the conventional\napproach of manual thresholding is labor-intensive, and highly sensitive to\ninter- and intra-image intensity variations. An accurate and robust automated\nsegmentation method is of high interest. We propose and evaluate an elegant\nconvolutional neural network (CNN) designed for segmentation of histological\nimages, particularly those with Masson's trichrome stain. The network comprises\nof 11 successive convolutional - rectified linear unit - batch normalization\nlayers, and outperformed state-of-the-art CNNs on a dataset of cardiac\nhistological images (labeling fibrosis, myocytes, and background) with a Dice\nsimilarity coefficient of 0.947. With 100 times fewer (only 300 thousand)\ntrainable parameters, our CNN is less susceptible to overfitting, and is\nefficient. Additionally, it retains image resolution from input to output,\ncaptures fine-grained details, and can be trained end-to-end smoothly. To the\nbest of our knowledge, this is the first deep CNN tailored for the problem of\nconcern, and may be extended to solve similar segmentation tasks to facilitate\ninvestigations into pathology and clinical treatment.\n", "versions": [{"version": "v1", "created": "Tue, 20 Mar 2018 08:48:24 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Fu", "Xiaohang", ""], ["Liu", "Tong", ""], ["Xiong", "Zhaohan", ""], ["Smaill", "Bruce H.", ""], ["Stiles", "Martin K.", ""], ["Zhao", "Jichao", ""]]}, {"id": "1803.07349", "submitter": "Alex Locher", "authors": "Alex Locher, Michal Havlena and Luc Van Gool (ETH Z\\\"urich)", "title": "Progressive Structure from Motion", "comments": "Accepted to ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structure from Motion or the sparse 3D reconstruction out of individual\nphotos is a long studied topic in computer vision. Yet none of the existing\nreconstruction pipelines fully addresses a progressive scenario where images\nare only getting available during the reconstruction process and intermediate\nresults are delivered to the user. Incremental pipelines are capable of growing\na 3D model but often get stuck in local minima due to wrong (binding) decisions\ntaken based on incomplete information. Global pipelines on the other hand need\nthe access to the complete viewgraph and are not capable of delivering\nintermediate results. In this paper we propose a new reconstruction pipeline\nworking in a progressive manner rather than in a batch processing scheme. The\npipeline is able to recover from failed reconstructions in early stages, avoids\nto take binding decisions, delivers a progressive output and yet maintains the\ncapabilities of existing pipelines. We demonstrate and evaluate our method on\ndiverse challenging public and dedicated datasets including those with highly\nsymmetric structures and compare to the state of the art.\n", "versions": [{"version": "v1", "created": "Tue, 20 Mar 2018 10:25:06 GMT"}, {"version": "v2", "created": "Tue, 10 Jul 2018 21:25:53 GMT"}], "update_date": "2018-07-12", "authors_parsed": [["Locher", "Alex", "", "ETH Z\u00fcrich"], ["Havlena", "Michal", "", "ETH Z\u00fcrich"], ["Van Gool", "Luc", "", "ETH Z\u00fcrich"]]}, {"id": "1803.07351", "submitter": "Ruobing Shen", "authors": "Ruobing Shen, Xiaoyu Chen, Xiangrui Zheng, Gerhard Reinelt", "title": "Discrete Potts Model for Generating Superpixels on Noisy Images", "comments": "23 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many computer vision applications, such as object recognition and\nsegmentation, increasingly build on superpixels. However, there have been so\nfar few superpixel algorithms that systematically deal with noisy images. We\npropose to first decompose the image into equal-sized rectangular patches,\nwhich also sets the maximum superpixel size. Within each patch, a Potts model\nfor simultaneous segmentation and denoising is applied, that guarantees\nconnected and non-overlapping superpixels and also produces a denoised image.\nThe corresponding optimization problem is formulated as a mixed integer linear\nprogram (MILP), and solved by a commercial solver. Extensive experiments on the\nBSDS500 dataset images with noises are compared with other state-of-the-art\nsuperpixel methods. Our method achieves the best result in terms of a combined\nscore (OP) composed of the under-segmentation error, boundary recall and\ncompactness.\n", "versions": [{"version": "v1", "created": "Tue, 20 Mar 2018 10:32:55 GMT"}], "update_date": "2018-03-21", "authors_parsed": [["Shen", "Ruobing", ""], ["Chen", "Xiaoyu", ""], ["Zheng", "Xiangrui", ""], ["Reinelt", "Gerhard", ""]]}, {"id": "1803.07360", "submitter": "Wang Jiaxing", "authors": "Jiaxing Wang, Jihua Zhu, Shanmin Pang, Zhongyu Li, Yaochen Li, Xueming\n  Qian", "title": "Adaptive Co-weighting Deep Convolutional Features For Object Retrieval", "comments": "6 pages,5 figures,ICME2018 poster", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aggregating deep convolutional features into a global image vector has\nattracted sustained attention in image retrieval. In this paper, we propose an\nefficient unsupervised aggregation method that uses an adaptive Gaussian filter\nand an elementvalue sensitive vector to co-weight deep features. Specifically,\nthe Gaussian filter assigns large weights to features of region-of-interests\n(RoI) by adaptively determining the RoI's center, while the element-value\nsensitive channel vector suppresses burstiness phenomenon by assigning small\nweights to feature maps with large sum values of all locations. Experimental\nresults on benchmark datasets validate the proposed two weighting schemes both\neffectively improve the discrimination power of image vectors. Furthermore,\nwith the same experimental setting, our method outperforms other very recent\naggregation approaches by a considerable margin.\n", "versions": [{"version": "v1", "created": "Tue, 20 Mar 2018 10:47:09 GMT"}], "update_date": "2018-03-21", "authors_parsed": [["Wang", "Jiaxing", ""], ["Zhu", "Jihua", ""], ["Pang", "Shanmin", ""], ["Li", "Zhongyu", ""], ["Li", "Yaochen", ""], ["Qian", "Xueming", ""]]}, {"id": "1803.07385", "submitter": "Maneet Singh", "authors": "Maneet Singh, Shruti Nagpal, Mayank Vatsa, Richa Singh", "title": "Are you eligible? Predicting adulthood from face images via class\n  specific mean autoencoder", "comments": "Accepted for publication in Pattern Recognition Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting if a person is an adult or a minor has several applications such\nas inspecting underage driving, preventing purchase of alcohol and tobacco by\nminors, and granting restricted access. The challenging nature of this problem\narises due to the complex and unique physiological changes that are observed\nwith age progression. This paper presents a novel deep learning based\nformulation, termed as Class Specific Mean Autoencoder, to learn the\nintra-class similarity and extract class-specific features. We propose that the\nfeature of a particular class if brought similar/closer to the mean feature of\nthat class can help in learning class-specific representations. The proposed\nformulation is applied for the task of adulthood classification which predicts\nwhether the given face image is of an adult or not. Experiments are performed\non two large databases and the results show that the proposed algorithm yields\nhigher classification accuracy compared to existing algorithms and a\nCommercial-Off-The-Shelf system.\n", "versions": [{"version": "v1", "created": "Tue, 20 Mar 2018 11:58:40 GMT"}], "update_date": "2018-03-21", "authors_parsed": [["Singh", "Maneet", ""], ["Nagpal", "Shruti", ""], ["Vatsa", "Mayank", ""], ["Singh", "Richa", ""]]}, {"id": "1803.07386", "submitter": "Maneet Singh", "authors": "Akshay Sethi, Maneet Singh, Richa Singh, Mayank Vatsa", "title": "Residual Codean Autoencoder for Facial Attribute Analysis", "comments": "Accepted in Pattern Recognition Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial attributes can provide rich ancillary information which can be\nutilized for different applications such as targeted marketing, human computer\ninteraction, and law enforcement. This research focuses on facial attribute\nprediction using a novel deep learning formulation, termed as R-Codean\nautoencoder. The paper first presents Cosine similarity based loss function in\nan autoencoder which is then incorporated into the Euclidean distance based\nautoencoder to formulate R-Codean. The proposed loss function thus aims to\nincorporate both magnitude and direction of image vectors during feature\nlearning. Further, inspired by the utility of shortcut connections in deep\nmodels to facilitate learning of optimal parameters, without incurring the\nproblem of vanishing gradient, the proposed formulation is extended to\nincorporate shortcut connections in the architecture. The proposed R-Codean\nautoencoder is utilized in facial attribute prediction framework which\nincorporates patch-based weighting mechanism for assigning higher weights to\nrelevant patches for each attribute. The experimental results on publicly\navailable CelebA and LFWA datasets demonstrate the efficacy of the proposed\napproach in addressing this challenging problem.\n", "versions": [{"version": "v1", "created": "Tue, 20 Mar 2018 12:05:33 GMT"}], "update_date": "2018-03-21", "authors_parsed": [["Sethi", "Akshay", ""], ["Singh", "Maneet", ""], ["Singh", "Richa", ""], ["Vatsa", "Mayank", ""]]}, {"id": "1803.07422", "submitter": "Ugur Demir", "authors": "Ugur Demir and Gozde Unal", "title": "Patch-Based Image Inpainting with Generative Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Area of image inpainting over relatively large missing regions recently\nadvanced substantially through adaptation of dedicated deep neural networks.\nHowever, current network solutions still introduce undesired artifacts and\nnoise to the repaired regions. We present an image inpainting method that is\nbased on the celebrated generative adversarial network (GAN) framework. The\nproposed PGGAN method includes a discriminator network that combines a global\nGAN (G-GAN) architecture with a patchGAN approach. PGGAN first shares network\nlayers between G-GAN and patchGAN, then splits paths to produce two adversarial\nlosses that feed the generator network in order to capture both local\ncontinuity of image texture and pervasive global features in images. The\nproposed framework is evaluated extensively, and the results including\ncomparison to recent state-of-the-art demonstrate that it achieves considerable\nimprovements on both visual and quantitative evaluations.\n", "versions": [{"version": "v1", "created": "Tue, 20 Mar 2018 13:38:52 GMT"}], "update_date": "2018-03-21", "authors_parsed": [["Demir", "Ugur", ""], ["Unal", "Gozde", ""]]}, {"id": "1803.07423", "submitter": "Chengjia Wang", "authors": "Chengjia Wang, Keith A. Goatman, James Boardman, Erin Beveridge, David\n  Newby, and Scott Semple", "title": "A Distance Oriented Kalman Filter Particle Swarm Optimizer Applied to\n  Multi-Modality Image Registration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we describe improvements to the particle swarm optimizer (PSO)\nmade by inclusion of an unscented Kalman filter to guide particle motion. We\ndemonstrate the effectiveness of the unscented Kalman filter PSO by comparing\nit with the original PSO algorithm and its variants designed to improve\nperformance. The PSOs were tested firstly on a number of common synthetic\nbenchmarking functions, and secondly applied to a practical three-dimensional\nimage registration problem. The proposed methods displayed better performances\nfor 4 out of 8 benchmark functions, and reduced the target registration errors\nby at least 2mm when registering down-sampled benchmark brain images. Our\nmethods also demonstrated an ability to align images featuring motion related\nartefacts which all other methods failed to register. These new PSO methods\nprovide a novel, efficient mechanism to integrate prior knowledge into each\niteration of the optimization process, which can enhance the accuracy and speed\nof convergence in the application of medical image registration.\n", "versions": [{"version": "v1", "created": "Tue, 20 Mar 2018 13:40:36 GMT"}], "update_date": "2018-03-21", "authors_parsed": [["Wang", "Chengjia", ""], ["Goatman", "Keith A.", ""], ["Boardman", "James", ""], ["Beveridge", "Erin", ""], ["Newby", "David", ""], ["Semple", "Scott", ""]]}, {"id": "1803.07426", "submitter": "Can Pu", "authors": "Can Pu, Nanbo Li, Radim Tylecek, Robert B Fisher", "title": "DUGMA: Dynamic Uncertainty-Based Gaussian Mixture Alignment", "comments": "Accepted by 3DV 2018. 9 pages. arXiv admin note: text overlap with\n  arXiv:1707.08626", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Registering accurately point clouds from a cheap low-resolution sensor is a\nchallenging task. Existing rigid registration methods failed to use the\nphysical 3D uncertainty distribution of each point from a real sensor in the\ndynamic alignment process mainly because the uncertainty model for a point is\nstatic and invariant and it is hard to describe the change of these physical\nuncertainty models in the registration process. Additionally, the existing\nGaussian mixture alignment architecture cannot be efficiently implement these\ndynamic changes.\n  This paper proposes a simple architecture combining error estimation from\nsample covariances and dual dynamic global probability alignment using the\nconvolution of uncertainty-based Gaussian Mixture Models (GMM) from point\nclouds. Firstly, we propose an efficient way to describe the change of each 3D\nuncertainty model, which represents the structure of the point cloud much\nbetter. Unlike the invariant GMM (representing a fixed point cloud) in\ntraditional Gaussian mixture alignment, we use two uncertainty-based GMMs that\nchange and interact with each other in each iteration. In order to have a wider\nbasin of convergence than other local algorithms, we design a more robust\nenergy function by convolving efficiently the two GMMs over the whole 3D space.\n  Tens of thousands of trials have been conducted on hundreds of models from\nmultiple datasets to demonstrate the proposed method's superior performance\ncompared with the current state-of-the-art methods. The new dataset and code is\navailable from https://github.com/Canpu999\n", "versions": [{"version": "v1", "created": "Sun, 18 Mar 2018 13:06:24 GMT"}, {"version": "v2", "created": "Thu, 2 Aug 2018 10:25:53 GMT"}], "update_date": "2018-08-03", "authors_parsed": [["Pu", "Can", ""], ["Li", "Nanbo", ""], ["Tylecek", "Radim", ""], ["Fisher", "Robert B", ""]]}, {"id": "1803.07427", "submitter": "Soujanya Poria", "authors": "Soujanya Poria, Navonil Majumder, Devamanyu Hazarika, Erik Cambria,\n  Alexander Gelbukh, Amir Hussain", "title": "Multimodal Sentiment Analysis: Addressing Key Issues and Setting up the\n  Baselines", "comments": "IEEE Intelligence Systems. arXiv admin note: substantial text overlap\n  with arXiv:1707.09538", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.IR", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We compile baselines, along with dataset split, for multimodal sentiment\nanalysis. In this paper, we explore three different deep-learning based\narchitectures for multimodal sentiment classification, each improving upon the\nprevious. Further, we evaluate these architectures with multiple datasets with\nfixed train/test partition. We also discuss some major issues, frequently\nignored in multimodal sentiment analysis research, e.g., role of\nspeaker-exclusive models, importance of different modalities, and\ngeneralizability. This framework illustrates the different facets of analysis\nto be considered while performing multimodal sentiment analysis and, hence,\nserves as a new benchmark for future research in this emerging field.\n", "versions": [{"version": "v1", "created": "Mon, 19 Mar 2018 02:23:30 GMT"}, {"version": "v2", "created": "Tue, 12 Feb 2019 02:42:19 GMT"}], "update_date": "2019-02-13", "authors_parsed": [["Poria", "Soujanya", ""], ["Majumder", "Navonil", ""], ["Hazarika", "Devamanyu", ""], ["Cambria", "Erik", ""], ["Gelbukh", "Alexander", ""], ["Hussain", "Amir", ""]]}, {"id": "1803.07436", "submitter": "Ribana Roscher", "authors": "Katharina Franz, Ribana Roscher, Andres Milioto, Susanne Wenzel,\n  J\\\"urgen Kusche", "title": "Ocean Eddy Identification and Tracking using Neural Networks", "comments": "accepted for International Geoscience and Remote Sensing Symposium\n  2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Global climate change plays an essential role in our daily life. Mesoscale\nocean eddies have a significant impact on global warming, since they affect the\nocean dynamics, the energy as well as the mass transports of ocean circulation.\nFrom satellite altimetry we can derive high-resolution, global maps containing\nocean signals with dominating coherent eddy structures. The aim of this study\nis the development and evaluation of a deep-learning based approach for the\nanalysis of eddies. In detail, we develop an eddy identification and tracking\nframework with two different approaches that are mainly based on feature\nlearning with convolutional neural networks. Furthermore, state-of-the-art\nimage processing tools and object tracking methods are used to support the eddy\ntracking. In contrast to previous methods, our framework is able to learn a\nrepresentation of the data in which eddies can be detected and tracked in more\nobjective and robust way. We show the detection and tracking results on sea\nlevel anomalies (SLA) data from the area of Australia and the East Australia\ncurrent, and compare our two eddy detection and tracking approaches to identify\nthe most robust and objective method.\n", "versions": [{"version": "v1", "created": "Tue, 20 Mar 2018 13:48:40 GMT"}, {"version": "v2", "created": "Tue, 15 May 2018 09:33:38 GMT"}], "update_date": "2018-05-16", "authors_parsed": [["Franz", "Katharina", ""], ["Roscher", "Ribana", ""], ["Milioto", "Andres", ""], ["Wenzel", "Susanne", ""], ["Kusche", "J\u00fcrgen", ""]]}, {"id": "1803.07441", "submitter": "Shiv Ram Dubey", "authors": "Shiv Ram Dubey, Snehasis Mukherjee", "title": "LDOP: Local Directional Order Pattern for Robust Face Retrieval", "comments": "Published in Multimedia Tools and Applications, Springer", "journal-ref": "Multimedia Tools and Applications, Springer, 2019", "doi": "10.1007/s11042-019-08370-x", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The local descriptors have gained wide range of attention due to their\nenhanced discriminative abilities. It has been proved that the consideration of\nmulti-scale local neighborhood improves the performance of the descriptor,\nthough at the cost of increased dimension. This paper proposes a novel method\nto construct a local descriptor using multi-scale neighborhood by finding the\nlocal directional order among the intensity values at different scales in a\nparticular direction. Local directional order is the multi-radius relationship\nfactor in a particular direction. The proposed local directional order pattern\n(LDOP) for a particular pixel is computed by finding the relationship between\nthe center pixel and local directional order indexes. It is required to\ntransform the center value into the range of neighboring orders. Finally, the\nhistogram of LDOP is computed over whole image to construct the descriptor. In\ncontrast to the state-of-the-art descriptors, the dimension of the proposed\ndescriptor does not depend upon the number of neighbors involved to compute the\norder; it only depends upon the number of directions. The introduced descriptor\nis evaluated over the image retrieval framework and compared with the\nstate-of-the-art descriptors over challenging face databases such as PaSC, LFW,\nPubFig, FERET, AR, AT&T, and ExtendedYale. The experimental results confirm the\nsuperiority and robustness of the LDOP descriptor.\n", "versions": [{"version": "v1", "created": "Wed, 28 Feb 2018 16:04:08 GMT"}, {"version": "v2", "created": "Tue, 27 Nov 2018 17:40:14 GMT"}, {"version": "v3", "created": "Tue, 24 Dec 2019 05:33:56 GMT"}], "update_date": "2019-12-25", "authors_parsed": [["Dubey", "Shiv Ram", ""], ["Mukherjee", "Snehasis", ""]]}, {"id": "1803.07452", "submitter": "Adrian Shajkofci", "authors": "Adrian Shajkofci, Michael Liebling", "title": "Semi-Blind Spatially-Variant Deconvolution in Optical Microscopy with\n  Local Point Spread Function Estimation By Use Of Convolutional Neural\n  Networks", "comments": "2018/02/11: submitted to IEEE ICIP 2018 - 2018/05/04: accepted to\n  IEEE ICIP 2018", "journal-ref": "2018 25th IEEE International Conference on Image Processing (ICIP)", "doi": "10.1109/ICIP.2018.8451736", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a semi-blind, spatially-variant deconvolution technique aimed at\noptical microscopy that combines a local estimation step of the point spread\nfunction (PSF) and deconvolution using a spatially variant, regularized\nRichardson-Lucy algorithm. To find the local PSF map in a computationally\ntractable way, we train a convolutional neural network to perform regression of\nan optical parametric model on synthetically blurred image patches. We\ndeconvolved both synthetic and experimentally-acquired data, and achieved an\nimprovement of image SNR of 1.00 dB on average, compared to other deconvolution\nalgorithms.\n", "versions": [{"version": "v1", "created": "Tue, 20 Mar 2018 14:29:12 GMT"}, {"version": "v2", "created": "Mon, 28 May 2018 13:49:44 GMT"}, {"version": "v3", "created": "Thu, 7 Jun 2018 13:42:13 GMT"}, {"version": "v4", "created": "Fri, 17 May 2019 18:17:54 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Shajkofci", "Adrian", ""], ["Liebling", "Michael", ""]]}, {"id": "1803.07461", "submitter": "Seyed Ali Jalalifar", "authors": "Seyed Ali Jalalifar, Hosein Hasani, Hamid Aghajan", "title": "Speech-Driven Facial Reenactment Using Conditional Generative\n  Adversarial Networks", "comments": "Submitted for ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel approach to generating photo-realistic images of a face\nwith accurate lip sync, given an audio input. By using a recurrent neural\nnetwork, we achieved mouth landmarks based on audio features. We exploited the\npower of conditional generative adversarial networks to produce\nhighly-realistic face conditioned on a set of landmarks. These two networks\ntogether are capable of producing a sequence of natural faces in sync with an\ninput audio track.\n", "versions": [{"version": "v1", "created": "Tue, 20 Mar 2018 14:42:49 GMT"}], "update_date": "2018-03-21", "authors_parsed": [["Jalalifar", "Seyed Ali", ""], ["Hasani", "Hosein", ""], ["Aghajan", "Hamid", ""]]}, {"id": "1803.07464", "submitter": "Qing Li", "authors": "Qing Li, Qingyi Tao, Shafiq Joty, Jianfei Cai, Jiebo Luo", "title": "VQA-E: Explaining, Elaborating, and Enhancing Your Answers for Visual\n  Questions", "comments": "ECCV 2018 Camera ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most existing works in visual question answering (VQA) are dedicated to\nimproving the accuracy of predicted answers, while disregarding the\nexplanations. We argue that the explanation for an answer is of the same or\neven more importance compared with the answer itself, since it makes the\nquestion and answering process more understandable and traceable. To this end,\nwe propose a new task of VQA-E (VQA with Explanation), where the computational\nmodels are required to generate an explanation with the predicted answer. We\nfirst construct a new dataset, and then frame the VQA-E problem in a multi-task\nlearning architecture. Our VQA-E dataset is automatically derived from the VQA\nv2 dataset by intelligently exploiting the available captions. We have\nconducted a user study to validate the quality of explanations synthesized by\nour method. We quantitatively show that the additional supervision from\nexplanations can not only produce insightful textual sentences to justify the\nanswers, but also improve the performance of answer prediction. Our model\noutperforms the state-of-the-art methods by a clear margin on the VQA v2\ndataset.\n", "versions": [{"version": "v1", "created": "Tue, 20 Mar 2018 14:50:03 GMT"}, {"version": "v2", "created": "Sat, 25 Aug 2018 04:07:24 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Li", "Qing", ""], ["Tao", "Qingyi", ""], ["Joty", "Shafiq", ""], ["Cai", "Jianfei", ""], ["Luo", "Jiebo", ""]]}, {"id": "1803.07469", "submitter": "Daniel Barath", "authors": "Daniel Barath, Jana Noskova, Jiri Matas", "title": "MAGSAC: marginalizing sample consensus", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A method called, sigma-consensus, is proposed to eliminate the need for a\nuser-defined inlier-outlier threshold in RANSAC. Instead of estimating the\nnoise sigma, it is marginalized over a range of noise scales. The optimized\nmodel is obtained by weighted least-squares fitting where the weights come from\nthe marginalization over sigma of the point likelihoods of being inliers. A new\nquality function is proposed not requiring sigma and, thus, a set of inliers to\ndetermine the model quality. Also, a new termination criterion for RANSAC is\nbuilt on the proposed marginalization approach. Applying sigma-consensus,\nMAGSAC is proposed with no need for a user-defined sigma and improving the\naccuracy of robust estimation significantly. It is superior to the\nstate-of-the-art in terms of geometric accuracy on publicly available\nreal-world datasets for epipolar geometry (F and E) and homography estimation.\nIn addition, applying sigma-consensus only once as a post-processing step to\nthe RANSAC output always improved the model quality on a wide range of vision\nproblems without noticeable deterioration in processing time, adding a few\nmilliseconds. The source code is at https://github.com/danini/magsac.\n", "versions": [{"version": "v1", "created": "Tue, 20 Mar 2018 15:01:11 GMT"}, {"version": "v2", "created": "Tue, 4 Jun 2019 23:06:14 GMT"}], "update_date": "2019-06-06", "authors_parsed": [["Barath", "Daniel", ""], ["Noskova", "Jana", ""], ["Matas", "Jiri", ""]]}, {"id": "1803.07474", "submitter": "Shaohui Liu", "authors": "Shaohui Liu, Yi Wei, Jiwen Lu, Jie Zhou", "title": "An Improved Evaluation Framework for Generative Adversarial Networks", "comments": "21 pages, 9 figures, 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an improved quantitative evaluation framework for\nGenerative Adversarial Networks (GANs) on generating domain-specific images,\nwhere we improve conventional evaluation methods on two levels: the feature\nrepresentation and the evaluation metric. Unlike most existing evaluation\nframeworks which transfer the representation of ImageNet inception model to map\nimages onto the feature space, our framework uses a specialized encoder to\nacquire fine-grained domain-specific representation. Moreover, for datasets\nwith multiple classes, we propose Class-Aware Frechet Distance (CAFD), which\nemploys a Gaussian mixture model on the feature space to better fit the\nmulti-manifold feature distribution. Experiments and analysis on both the\nfeature level and the image level were conducted to demonstrate improvements of\nour proposed framework over the recently proposed state-of-the-art FID method.\nTo our best knowledge, we are the first to provide counter examples where FID\ngives inconsistent results with human judgments. It is shown in the experiments\nthat our framework is able to overcome the shortness of FID and improves\nrobustness. Code will be made available.\n", "versions": [{"version": "v1", "created": "Tue, 20 Mar 2018 15:09:09 GMT"}, {"version": "v2", "created": "Tue, 27 Mar 2018 17:48:50 GMT"}, {"version": "v3", "created": "Thu, 19 Jul 2018 23:55:22 GMT"}], "update_date": "2018-07-23", "authors_parsed": [["Liu", "Shaohui", ""], ["Wei", "Yi", ""], ["Lu", "Jiwen", ""], ["Zhou", "Jie", ""]]}, {"id": "1803.07485", "submitter": "Kirill Gavrilyuk", "authors": "Kirill Gavrilyuk, Amir Ghodrati, Zhenyang Li, Cees G.M. Snoek", "title": "Actor and Action Video Segmentation from a Sentence", "comments": "Accepted to CVPR 2018 as oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper strives for pixel-level segmentation of actors and their actions\nin video content. Different from existing works, which all learn to segment\nfrom a fixed vocabulary of actor and action pairs, we infer the segmentation\nfrom a natural language input sentence. This allows to distinguish between\nfine-grained actors in the same super-category, identify actor and action\ninstances, and segment pairs that are outside of the actor and action\nvocabulary. We propose a fully-convolutional model for pixel-level actor and\naction segmentation using an encoder-decoder architecture optimized for video.\nTo show the potential of actor and action video segmentation from a sentence,\nwe extend two popular actor and action datasets with more than 7,500 natural\nlanguage descriptions. Experiments demonstrate the quality of the\nsentence-guided segmentations, the generalization ability of our model, and its\nadvantage for traditional actor and action segmentation compared to the\nstate-of-the-art.\n", "versions": [{"version": "v1", "created": "Tue, 20 Mar 2018 15:29:38 GMT"}], "update_date": "2018-03-21", "authors_parsed": [["Gavrilyuk", "Kirill", ""], ["Ghodrati", "Amir", ""], ["Li", "Zhenyang", ""], ["Snoek", "Cees G. M.", ""]]}, {"id": "1803.07487", "submitter": "Tai-Xiang Jiang", "authors": "Tai-Xiang Jiang, Ting-Zhu Huang, Xi-Le Zhao, Liang-Jian Deng and Yao\n  Wang", "title": "FastDeRain: A Novel Video Rain Streak Removal Method Using Directional\n  Gradient Priors", "comments": "codes available at https://github.com/TaiXiangJiang/FastDeRain", "journal-ref": null, "doi": "10.1109/TIP.2018.2880512", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rain streak removal is an important issue in outdoor vision systems and has\nrecently been investigated extensively. In this paper, we propose a novel video\nrain streak removal approach FastDeRain, which fully considers the\ndiscriminative characteristics of rain streaks and the clean video in the\ngradient domain. Specifically, on the one hand, rain streaks are sparse and\nsmooth along the direction of the raindrops, whereas on the other hand, clean\nvideos exhibit piecewise smoothness along the rain-perpendicular direction and\ncontinuity along the temporal direction. Theses smoothness and continuity\nresults in the sparse distribution in the different directional gradient\ndomain, respectively. Thus, we minimize 1) the $\\ell_1$ norm to enhance the\nsparsity of the underlying rain streaks, 2) two $\\ell_1$ norm of unidirectional\nTotal Variation (TV) regularizers to guarantee the anisotropic spatial\nsmoothness, and 3) an $\\ell_1$ norm of the time-directional difference operator\nto characterize the temporal continuity. A split augmented Lagrangian shrinkage\nalgorithm (SALSA) based algorithm is designed to solve the proposed\nminimization model. Experiments conducted on synthetic and real data\ndemonstrate the effectiveness and efficiency of the proposed method. According\nto comprehensive quantitative performance measures, our approach outperforms\nother state-of-the-art methods especially on account of the running time. The\ncode of FastDeRain can be downloaded at\nhttps://github.com/TaiXiangJiang/FastDeRain.\n", "versions": [{"version": "v1", "created": "Tue, 20 Mar 2018 15:32:54 GMT"}, {"version": "v2", "created": "Thu, 26 Apr 2018 14:11:55 GMT"}, {"version": "v3", "created": "Mon, 16 Jul 2018 16:24:30 GMT"}], "update_date": "2019-05-16", "authors_parsed": [["Jiang", "Tai-Xiang", ""], ["Huang", "Ting-Zhu", ""], ["Zhao", "Xi-Le", ""], ["Deng", "Liang-Jian", ""], ["Wang", "Yao", ""]]}, {"id": "1803.07512", "submitter": "Guido de Croon", "authors": "Diogo Martins, Kevin van Hecke, Guido de Croon", "title": "Fusion of stereo and still monocular depth estimates in a\n  self-supervised learning context", "comments": "To be published at ICRA 2018, 8 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study how autonomous robots can learn by themselves to improve their depth\nestimation capability. In particular, we investigate a self-supervised learning\nsetup in which stereo vision depth estimates serve as targets for a\nconvolutional neural network (CNN) that transforms a single still image to a\ndense depth map. After training, the stereo and mono estimates are fused with a\nnovel fusion method that preserves high confidence stereo estimates, while\nleveraging the CNN estimates in the low-confidence regions. The main\ncontribution of the article is that it is shown that the fused estimates lead\nto a higher performance than the stereo vision estimates alone. Experiments are\nperformed on the KITTI dataset, and on board of a Parrot SLAMDunk, showing that\neven rather limited CNNs can help provide stereo vision equipped robots with\nmore reliable depth maps for autonomous navigation.\n", "versions": [{"version": "v1", "created": "Tue, 20 Mar 2018 16:24:21 GMT"}], "update_date": "2018-03-21", "authors_parsed": [["Martins", "Diogo", ""], ["van Hecke", "Kevin", ""], ["de Croon", "Guido", ""]]}, {"id": "1803.07544", "submitter": "Zhe Li", "authors": "Zhe Li, Xiaolong Ma, Hongjia Li, Qiyuan An, Aditya Singh Rathore,\n  Qinru Qiu, Wenyao Xu, Yanzhi Wang", "title": "C3PO: Database and Benchmark for Early-stage Malicious Activity\n  Detection in 3D Printing", "comments": "The paper contains error, and the project is suspended, the results\n  will be not updated in the near future, withdraw is better option than\n  replace", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Increasing malicious users have sought practices to leverage 3D printing\ntechnology to produce unlawful tools in criminal activities. Current\nregulations are inadequate to deal with the rapid growth of 3D printers. It is\nof vital importance to enable 3D printers to identify the objects to be\nprinted, so that the manufacturing procedure of an illegal weapon can be\nterminated at the early stage. Deep learning yields significant rises in\nperformance in the object recognition tasks. However, the lack of large-scale\ndatabases in 3D printing domain stalls the advancement of automatic illegal\nweapon recognition.\n  This paper presents a new 3D printing image database, namely C3PO, which\ncompromises two subsets for the different system working scenarios. We extract\nimages from the numerical control programming code files of 22 3D models, and\nthen categorize the images into 10 distinct labels. The first set consists of\n62,200 images which represent the object projections on the three planes in a\nCartesian coordinate system. And the second sets consists of sequences of total\n671,677 images to simulate the cameras' captures of the printed objects.\nImportantly, we demonstrate that the weapons can be recognized in either\nscenario using deep learning based approaches using our proposed database. % We\nalso use the trained deep models to build a prototype of object-aware 3D\nprinter. The quantitative results are promising, and the future exploration of\nthe database and the crime prevention in 3D printing are demanding tasks.\n", "versions": [{"version": "v1", "created": "Tue, 20 Mar 2018 17:41:50 GMT"}, {"version": "v2", "created": "Thu, 2 Aug 2018 21:28:36 GMT"}], "update_date": "2018-08-06", "authors_parsed": [["Li", "Zhe", ""], ["Ma", "Xiaolong", ""], ["Li", "Hongjia", ""], ["An", "Qiyuan", ""], ["Rathore", "Aditya Singh", ""], ["Qiu", "Qinru", ""], ["Xu", "Wenyao", ""], ["Wang", "Yanzhi", ""]]}, {"id": "1803.07549", "submitter": "Angjoo Kanazawa", "authors": "Angjoo Kanazawa, Shubham Tulsiani, Alexei A. Efros, Jitendra Malik", "title": "Learning Category-Specific Mesh Reconstruction from Image Collections", "comments": "Project URL: https://akanazawa.github.io/cmr/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a learning framework for recovering the 3D shape, camera, and\ntexture of an object from a single image. The shape is represented as a\ndeformable 3D mesh model of an object category where a shape is parameterized\nby a learned mean shape and per-instance predicted deformation. Our approach\nallows leveraging an annotated image collection for training, where the\ndeformable model and the 3D prediction mechanism are learned without relying on\nground-truth 3D or multi-view supervision. Our representation enables us to go\nbeyond existing 3D prediction approaches by incorporating texture inference as\nprediction of an image in a canonical appearance space. Additionally, we show\nthat semantic keypoints can be easily associated with the predicted shapes. We\npresent qualitative and quantitative results of our approach on CUB and\nPASCAL3D datasets and show that we can learn to predict diverse shapes and\ntextures across objects using only annotated image collections. The project\nwebsite can be found at https://akanazawa.github.io/cmr/.\n", "versions": [{"version": "v1", "created": "Tue, 20 Mar 2018 17:44:30 GMT"}, {"version": "v2", "created": "Mon, 30 Jul 2018 18:22:59 GMT"}], "update_date": "2018-08-01", "authors_parsed": [["Kanazawa", "Angjoo", ""], ["Tulsiani", "Shubham", ""], ["Efros", "Alexei A.", ""], ["Malik", "Jitendra", ""]]}, {"id": "1803.07599", "submitter": "Benjamin Riggan", "authors": "Benjamin S. Riggan, Nathaniel J. Short, Shuowen Hu", "title": "Thermal to Visible Synthesis of Face Images using Multiple Regions", "comments": "Accepted IEEE WACV 2018, received best paper award", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Synthesis of visible spectrum faces from thermal facial imagery is a\npromising approach for heterogeneous face recognition; enabling existing face\nrecognition software trained on visible imagery to be leveraged, and allowing\nhuman analysts to verify cross-spectrum matches more effectively. We propose a\nnew synthesis method to enhance the discriminative quality of synthesized\nvisible face imagery by leveraging both global (e.g., entire face) and local\nregions (e.g., eyes, nose, and mouth). Here, each region provides (1) an\nindependent representation for the corresponding area, and (2) additional\nregularization terms, which impact the overall quality of synthesized images.\nWe analyze the effects of using multiple regions to synthesize a visible face\nimage from a thermal face. We demonstrate that our approach improves\ncross-spectrum verification rates over recently published synthesis approaches.\nMoreover, using our synthesized imagery, we report the results on facial\nlandmark detection-commonly used for image registration-which is a critical\npart of the face recognition process.\n", "versions": [{"version": "v1", "created": "Tue, 20 Mar 2018 18:41:30 GMT"}], "update_date": "2018-03-22", "authors_parsed": [["Riggan", "Benjamin S.", ""], ["Short", "Nathaniel J.", ""], ["Hu", "Shuowen", ""]]}, {"id": "1803.07608", "submitter": "Tarique Anwer", "authors": "Jahanzaib Shabbir and Tarique Anwer", "title": "A Survey of Deep Learning Techniques for Mobile Robot Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advancements in deep learning over the years have attracted research into how\ndeep artificial neural networks can be used in robotic systems. This research\nsurvey will present a summarization of the current research with a specific\nfocus on the gains and obstacles for deep learning to be applied to mobile\nrobotics.\n", "versions": [{"version": "v1", "created": "Tue, 20 Mar 2018 19:12:05 GMT"}], "update_date": "2018-03-22", "authors_parsed": [["Shabbir", "Jahanzaib", ""], ["Anwer", "Tarique", ""]]}, {"id": "1803.07616", "submitter": "Ronan Riochet", "authors": "Ronan Riochet, Mario Ynocente Castro, Mathieu Bernard, Adam Lerer, Rob\n  Fergus, V\\'eronique Izard and Emmanuel Dupoux", "title": "IntPhys: A Framework and Benchmark for Visual Intuitive Physics\n  Reasoning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to reach human performance on complexvisual tasks, artificial\nsystems need to incorporate a sig-nificant amount of understanding of the world\nin termsof macroscopic objects, movements, forces, etc. Inspiredby work on\nintuitive physics in infants, we propose anevaluation benchmark which diagnoses\nhow much a givensystem understands about physics by testing whether itcan tell\napart well matched videos of possible versusimpossible events constructed with\na game engine. Thetest requires systems to compute a physical plausibilityscore\nover an entire video. It is free of bias and cantest a range of basic physical\nreasoning concepts. Wethen describe two Deep Neural Networks systems aimedat\nlearning intuitive physics in an unsupervised way,using only physically\npossible videos. The systems aretrained with a future semantic mask prediction\nobjectiveand tested on the possible versus impossible discrimi-nation task. The\nanalysis of their results compared tohuman data gives novel insights in the\npotentials andlimitations of next frame prediction architectures.\n", "versions": [{"version": "v1", "created": "Tue, 20 Mar 2018 19:29:46 GMT"}, {"version": "v2", "created": "Tue, 26 Jun 2018 09:20:37 GMT"}, {"version": "v3", "created": "Tue, 11 Feb 2020 10:05:20 GMT"}], "update_date": "2020-02-12", "authors_parsed": [["Riochet", "Ronan", ""], ["Castro", "Mario Ynocente", ""], ["Bernard", "Mathieu", ""], ["Lerer", "Adam", ""], ["Fergus", "Rob", ""], ["Izard", "V\u00e9ronique", ""], ["Dupoux", "Emmanuel", ""]]}, {"id": "1803.07624", "submitter": "Jialin Wu", "authors": "Jialin Wu, Dai Li, Yu Yang, Chandrajit Bajaj, Xiangyang Ji", "title": "Dynamic Filtering with Large Sampling Field for ConvNets", "comments": "ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a dynamic filtering strategy with large sampling field for\nConvNets (LS-DFN), where the position-specific kernels learn from not only the\nidentical position but also multiple sampled neighbor regions. During sampling,\nresidual learning is introduced to ease training and an attention mechanism is\napplied to fuse features from different samples. Such multiple samples enlarge\nthe kernels' receptive fields significantly without requiring more parameters.\nWhile LS-DFN inherits the advantages of DFN, namely avoiding feature map\nblurring by position-wise kernels while keeping translation invariance, it also\nefficiently alleviates the overfitting issue caused by much more parameters\nthan normal CNNs. Our model is efficient and can be trained end-to-end via\nstandard back-propagation. We demonstrate the merits of our LS-DFN on both\nsparse and dense prediction tasks involving object detection, semantic\nsegmentation, and flow estimation. Our results show LS-DFN enjoys stronger\nrecognition abilities in object detection and semantic segmentation tasks on\nVOC benchmark and sharper responses in flow estimation on FlyingChairs dataset\ncompared to strong baselines.\n", "versions": [{"version": "v1", "created": "Tue, 20 Mar 2018 19:52:16 GMT"}, {"version": "v2", "created": "Thu, 22 Mar 2018 14:03:36 GMT"}, {"version": "v3", "created": "Mon, 9 Sep 2019 14:37:15 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Wu", "Jialin", ""], ["Li", "Dai", ""], ["Yang", "Yu", ""], ["Bajaj", "Chandrajit", ""], ["Ji", "Xiangyang", ""]]}, {"id": "1803.07679", "submitter": "Fabio Daolio", "authors": "\\^Angelo Cardoso, Fabio Daolio and Sa\\'ul Vargas", "title": "Product Characterisation towards Personalisation: Learning Attributes\n  from Unstructured Data to Recommend Fashion Products", "comments": "Under submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.CV cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we describe a solution to tackle a common set of challenges in\ne-commerce, which arise from the fact that new products are continually being\nadded to the catalogue. The challenges involve properly personalising the\ncustomer experience, forecasting demand and planning the product range. We\nargue that the foundational piece to solve all of these problems is having\nconsistent and detailed information about each product, information that is\nrarely available or consistent given the multitude of suppliers and types of\nproducts. We describe in detail the architecture and methodology implemented at\nASOS, one of the world's largest fashion e-commerce retailers, to tackle this\nproblem. We then show how this quantitative understanding of the products can\nbe leveraged to improve recommendations in a hybrid recommender system\napproach.\n", "versions": [{"version": "v1", "created": "Tue, 20 Mar 2018 22:25:29 GMT"}], "update_date": "2018-03-22", "authors_parsed": [["Cardoso", "\u00c2ngelo", ""], ["Daolio", "Fabio", ""], ["Vargas", "Sa\u00fal", ""]]}, {"id": "1803.07682", "submitter": "Jie Luo", "authors": "Jie Luo, Matt Toews, Ines Machado, Sarah Frisken, Miaomiao Zhang,\n  Frank Preiswerk, Alireza Sedghi, Hongyi Ding, Steve Pieper, Polina Golland,\n  Alexandra Golby, Masashi Sugiyama and William M. Wells III", "title": "A Feature-Driven Active Framework for Ultrasound-Based Brain Shift\n  Compensation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A reliable Ultrasound (US)-to-US registration method to compensate for brain\nshift would substantially improve Image-Guided Neurological Surgery. Developing\nsuch a registration method is very challenging, due to factors such as missing\ncorrespondence in images, the complexity of brain pathology and the demand for\nfast computation. We propose a novel feature-driven active framework. Here,\nlandmarks and their displacement are first estimated from a pair of US images\nusing corresponding local image features. Subsequently, a Gaussian Process (GP)\nmodel is used to interpolate a dense deformation field from the sparse\nlandmarks. Kernels of the GP are estimated by using variograms and a discrete\ngrid search method. If necessary, the user can actively add new landmarks based\non the image context and visualization of the uncertainty measure provided by\nthe GP to further improve the result. We retrospectively demonstrate our\nregistration framework as a robust and accurate brain shift compensation\nsolution on clinical data acquired during neurosurgery.\n", "versions": [{"version": "v1", "created": "Tue, 20 Mar 2018 22:50:37 GMT"}], "update_date": "2018-03-22", "authors_parsed": [["Luo", "Jie", ""], ["Toews", "Matt", ""], ["Machado", "Ines", ""], ["Frisken", "Sarah", ""], ["Zhang", "Miaomiao", ""], ["Preiswerk", "Frank", ""], ["Sedghi", "Alireza", ""], ["Ding", "Hongyi", ""], ["Pieper", "Steve", ""], ["Golland", "Polina", ""], ["Golby", "Alexandra", ""], ["Sugiyama", "Masashi", ""], ["Wells", "William M.", "III"]]}, {"id": "1803.07702", "submitter": "Sunghoon Im", "authors": "Sunghoon Im, Hae-Gon Jeon, In So Kweon", "title": "Robust Depth Estimation from Auto Bracketed Images", "comments": "To appear in CVPR 2018. Total 9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As demand for advanced photographic applications on hand-held devices grows,\nthese electronics require the capture of high quality depth. However, under\nlow-light conditions, most devices still suffer from low imaging quality and\ninaccurate depth acquisition. To address the problem, we present a robust depth\nestimation method from a short burst shot with varied intensity (i.e., Auto\nBracketing) or strong noise (i.e., High ISO). We introduce a geometric\ntransformation between flow and depth tailored for burst images, enabling our\nlearning-based multi-view stereo matching to be performed effectively. We then\ndescribe our depth estimation pipeline that incorporates the geometric\ntransformation into our residual-flow network. It allows our framework to\nproduce an accurate depth map even with a bracketed image sequence. We\ndemonstrate that our method outperforms state-of-the-art methods for various\ndatasets captured by a smartphone and a DSLR camera. Moreover, we show that the\nestimated depth is applicable for image quality enhancement and photographic\nediting.\n", "versions": [{"version": "v1", "created": "Wed, 21 Mar 2018 00:35:43 GMT"}], "update_date": "2018-03-22", "authors_parsed": [["Im", "Sunghoon", ""], ["Jeon", "Hae-Gon", ""], ["Kweon", "In So", ""]]}, {"id": "1803.07703", "submitter": "Li Yao", "authors": "Li Yao, Jordan Prosky, Eric Poblenz, Ben Covington, Kevin Lyman", "title": "Weakly Supervised Medical Diagnosis and Localization from Multiple\n  Resolutions", "comments": "submitted to ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diagnostic imaging often requires the simultaneous identification of a\nmultitude of findings of varied size and appearance. Beyond global indication\nof said findings, the prediction and display of localization information\nimproves trust in and understanding of results when augmenting clinical\nworkflow. Medical training data rarely includes more than global image-level\nlabels as segmentations are time-consuming and expensive to collect. We\nintroduce an approach to managing these practical constraints by applying a\nnovel architecture which learns at multiple resolutions while generating\nsaliency maps with weak supervision. Further, we parameterize the Log-Sum-Exp\npooling function with a learnable lower-bounded adaptation (LSE-LBA) to build\nin a sharpness prior and better handle localizing abnormalities of different\nsizes using only image-level labels. Applying this approach to interpreting\nchest x-rays, we set the state of the art on 9 abnormalities in the NIH's CXR14\ndataset while generating saliency maps with the highest resolution to date.\n", "versions": [{"version": "v1", "created": "Wed, 21 Mar 2018 00:40:57 GMT"}], "update_date": "2018-03-22", "authors_parsed": [["Yao", "Li", ""], ["Prosky", "Jordan", ""], ["Poblenz", "Eric", ""], ["Covington", "Ben", ""], ["Lyman", "Kevin", ""]]}, {"id": "1803.07716", "submitter": "Hai Pham", "authors": "Hai X. Pham and Yuting Wang and Vladimir Pavlovic", "title": "Generative Adversarial Talking Head: Bringing Portraits to Life with a\n  Weakly Supervised Neural Network", "comments": "Fix typos, add youtube link of supplementary video", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents Generative Adversarial Talking Head (GATH), a novel deep\ngenerative neural network that enables fully automatic facial expression\nsynthesis of an arbitrary portrait with continuous action unit (AU)\ncoefficients. Specifically, our model directly manipulates image pixels to make\nthe unseen subject in the still photo express various emotions controlled by\nvalues of facial AU coefficients, while maintaining her personal\ncharacteristics, such as facial geometry, skin color and hair style, as well as\nthe original surrounding background. In contrast to prior work, GATH is purely\ndata-driven and it requires neither a statistical face model nor image\nprocessing tricks to enact facial deformations. Additionally, our model is\ntrained from unpaired data, where the input image, with its auxiliary identity\nlabel taken from abundance of still photos in the wild, and the target frame\nare from different persons. In order to effectively learn such model, we\npropose a novel weakly supervised adversarial learning framework that consists\nof a generator, a discriminator, a classifier and an action unit estimator. Our\nwork gives rise to template-and-target-free expression editing, where still\nfaces can be effortlessly animated with arbitrary AU coefficients provided by\nthe user.\n", "versions": [{"version": "v1", "created": "Wed, 21 Mar 2018 01:56:41 GMT"}, {"version": "v2", "created": "Wed, 28 Mar 2018 04:42:06 GMT"}], "update_date": "2018-03-29", "authors_parsed": [["Pham", "Hai X.", ""], ["Wang", "Yuting", ""], ["Pavlovic", "Vladimir", ""]]}, {"id": "1803.07721", "submitter": "Alexandra Carlson", "authors": "Alexandra Carlson, Katherine A. Skinner, Ram Vasudevan, and Matthew\n  Johnson-Roberson", "title": "Modeling Camera Effects to Improve Visual Learning from Synthetic Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work has focused on generating synthetic imagery to increase the size\nand variability of training data for learning visual tasks in urban scenes.\nThis includes increasing the occurrence of occlusions or varying environmental\nand weather effects. However, few have addressed modeling variation in the\nsensor domain. Sensor effects can degrade real images, limiting\ngeneralizability of network performance on visual tasks trained on synthetic\ndata and tested in real environments. This paper proposes an efficient,\nautomatic, physically-based augmentation pipeline to vary sensor effects\n--chromatic aberration, blur, exposure, noise, and color cast-- for synthetic\nimagery. In particular, this paper illustrates that augmenting synthetic\ntraining datasets with the proposed pipeline reduces the domain gap between\nsynthetic and real domains for the task of object detection in urban driving\nscenes.\n", "versions": [{"version": "v1", "created": "Wed, 21 Mar 2018 02:36:11 GMT"}, {"version": "v2", "created": "Tue, 27 Mar 2018 16:57:40 GMT"}, {"version": "v3", "created": "Tue, 17 Apr 2018 20:54:57 GMT"}, {"version": "v4", "created": "Mon, 4 Jun 2018 19:52:22 GMT"}, {"version": "v5", "created": "Mon, 17 Sep 2018 15:28:40 GMT"}, {"version": "v6", "created": "Tue, 2 Oct 2018 02:34:46 GMT"}], "update_date": "2018-10-03", "authors_parsed": [["Carlson", "Alexandra", ""], ["Skinner", "Katherine A.", ""], ["Vasudevan", "Ram", ""], ["Johnson-Roberson", "Matthew", ""]]}, {"id": "1803.07724", "submitter": "Jasdeep Singh", "authors": "Jasdeep Singh, Vincent Ying, Alex Nutkiewicz", "title": "Attention on Attention: Architectures for Visual Question Answering\n  (VQA)", "comments": "Visual Question Answering Project", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual Question Answering (VQA) is an increasingly popular topic in deep\nlearning research, requiring coordination of natural language processing and\ncomputer vision modules into a single architecture. We build upon the model\nwhich placed first in the VQA Challenge by developing thirteen new attention\nmechanisms and introducing a simplified classifier. We performed 300 GPU hours\nof extensive hyperparameter and architecture searches and were able to achieve\nan evaluation score of 64.78%, outperforming the existing state-of-the-art\nsingle model's validation score of 63.15%.\n", "versions": [{"version": "v1", "created": "Wed, 21 Mar 2018 03:05:58 GMT"}], "update_date": "2018-03-22", "authors_parsed": [["Singh", "Jasdeep", ""], ["Ying", "Vincent", ""], ["Nutkiewicz", "Alex", ""]]}, {"id": "1803.07728", "submitter": "Spyros Gidaris", "authors": "Spyros Gidaris, Praveer Singh, Nikos Komodakis", "title": "Unsupervised Representation Learning by Predicting Image Rotations", "comments": "Accepted at ICLR2018. Code and models will be published on:\n  https://github.com/gidariss/FeatureLearningRotNet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the last years, deep convolutional neural networks (ConvNets) have\ntransformed the field of computer vision thanks to their unparalleled capacity\nto learn high level semantic image features. However, in order to successfully\nlearn those features, they usually require massive amounts of manually labeled\ndata, which is both expensive and impractical to scale. Therefore, unsupervised\nsemantic feature learning, i.e., learning without requiring manual annotation\neffort, is of crucial importance in order to successfully harvest the vast\namount of visual data that are available today. In our work we propose to learn\nimage features by training ConvNets to recognize the 2d rotation that is\napplied to the image that it gets as input. We demonstrate both qualitatively\nand quantitatively that this apparently simple task actually provides a very\npowerful supervisory signal for semantic feature learning. We exhaustively\nevaluate our method in various unsupervised feature learning benchmarks and we\nexhibit in all of them state-of-the-art performance. Specifically, our results\non those benchmarks demonstrate dramatic improvements w.r.t. prior\nstate-of-the-art approaches in unsupervised representation learning and thus\nsignificantly close the gap with supervised feature learning. For instance, in\nPASCAL VOC 2007 detection task our unsupervised pre-trained AlexNet model\nachieves the state-of-the-art (among unsupervised methods) mAP of 54.4% that is\nonly 2.4 points lower from the supervised case. We get similarly striking\nresults when we transfer our unsupervised learned features on various other\ntasks, such as ImageNet classification, PASCAL classification, PASCAL\nsegmentation, and CIFAR-10 classification. The code and models of our paper\nwill be published on: https://github.com/gidariss/FeatureLearningRotNet .\n", "versions": [{"version": "v1", "created": "Wed, 21 Mar 2018 03:21:14 GMT"}], "update_date": "2018-03-22", "authors_parsed": [["Gidaris", "Spyros", ""], ["Singh", "Praveer", ""], ["Komodakis", "Nikos", ""]]}, {"id": "1803.07729", "submitter": "Xin Wang", "authors": "Xin Wang, Wenhan Xiong, Hongmin Wang, William Yang Wang", "title": "Look Before You Leap: Bridging Model-Free and Model-Based Reinforcement\n  Learning for Planned-Ahead Vision-and-Language Navigation", "comments": "21 pages, 7 figures, with supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing research studies on vision and language grounding for robot\nnavigation focus on improving model-free deep reinforcement learning (DRL)\nmodels in synthetic environments. However, model-free DRL models do not\nconsider the dynamics in the real-world environments, and they often fail to\ngeneralize to new scenes. In this paper, we take a radical approach to bridge\nthe gap between synthetic studies and real-world practices---We propose a\nnovel, planned-ahead hybrid reinforcement learning model that combines\nmodel-free and model-based reinforcement learning to solve a real-world\nvision-language navigation task. Our look-ahead module tightly integrates a\nlook-ahead policy model with an environment model that predicts the next state\nand the reward. Experimental results suggest that our proposed method\nsignificantly outperforms the baselines and achieves the best on the real-world\nRoom-to-Room dataset. Moreover, our scalable method is more generalizable when\ntransferring to unseen environments.\n", "versions": [{"version": "v1", "created": "Wed, 21 Mar 2018 03:21:38 GMT"}, {"version": "v2", "created": "Thu, 26 Jul 2018 06:10:27 GMT"}], "update_date": "2018-07-27", "authors_parsed": [["Wang", "Xin", ""], ["Xiong", "Wenhan", ""], ["Wang", "Hongmin", ""], ["Wang", "William Yang", ""]]}, {"id": "1803.07737", "submitter": "Daniel Du", "authors": "Xu Tang, Daniel K. Du, Zeqiang He and Jingtuo Liu", "title": "PyramidBox: A Context-assisted Single Shot Face Detector", "comments": "21 pages, 12 figures", "journal-ref": "ECCV2018", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face detection has been well studied for many years and one of remaining\nchallenges is to detect small, blurred and partially occluded faces in\nuncontrolled environment. This paper proposes a novel context-assisted single\nshot face detector, named \\emph{PyramidBox} to handle the hard face detection\nproblem. Observing the importance of the context, we improve the utilization of\ncontextual information in the following three aspects. First, we design a novel\ncontext anchor to supervise high-level contextual feature learning by a\nsemi-supervised method, which we call it PyramidAnchors. Second, we propose the\nLow-level Feature Pyramid Network to combine adequate high-level context\nsemantic feature and Low-level facial feature together, which also allows the\nPyramidBox to predict faces of all scales in a single shot. Third, we introduce\na context-sensitive structure to increase the capacity of prediction network to\nimprove the final accuracy of output. In addition, we use the method of\nData-anchor-sampling to augment the training samples across different scales,\nwhich increases the diversity of training data for smaller faces. By exploiting\nthe value of context, PyramidBox achieves superior performance among the\nstate-of-the-art over the two common face detection benchmarks, FDDB and WIDER\nFACE. Our code is available in PaddlePaddle:\n\\href{https://github.com/PaddlePaddle/models/tree/develop/fluid/face_detection}{\\url{https://github.com/PaddlePaddle/models/tree/develop/fluid/face_detection}}.\n", "versions": [{"version": "v1", "created": "Wed, 21 Mar 2018 03:50:53 GMT"}, {"version": "v2", "created": "Fri, 17 Aug 2018 02:43:53 GMT"}], "update_date": "2018-08-20", "authors_parsed": [["Tang", "Xu", ""], ["Du", "Daniel K.", ""], ["He", "Zeqiang", ""], ["Liu", "Jingtuo", ""]]}, {"id": "1803.07739", "submitter": "Hossein Hosseini", "authors": "Hossein Hosseini, Baicen Xiao, Mayoore Jaiswal and Radha Poovendran", "title": "Assessing Shape Bias Property of Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is known that humans display \"shape bias\" when classifying new items,\ni.e., they prefer to categorize objects based on their shape rather than color.\nConvolutional Neural Networks (CNNs) are also designed to take into account the\nspatial structure of image data. In fact, experiments on image datasets,\nconsisting of triples of a probe image, a shape-match and a color-match, have\nshown that one-shot learning models display shape bias as well.\n  In this paper, we examine the shape bias property of CNNs. In order to\nconduct large scale experiments, we propose using the model accuracy on images\nwith reversed brightness as a metric to evaluate the shape bias property. Such\nimages, called negative images, contain objects that have the same shape as\noriginal images, but with different colors. Through extensive systematic\nexperiments, we investigate the role of different factors, such as training\ndata, model architecture, initialization and regularization techniques, on the\nshape bias property of CNNs. We show that it is possible to design different\nCNNs that achieve similar accuracy on original images, but perform\nsignificantly different on negative images, suggesting that CNNs do not\nintrinsically display shape bias. We then show that CNNs are able to learn and\ngeneralize the structures, when the model is properly initialized or data is\nproperly augmented, and if batch normalization is used.\n", "versions": [{"version": "v1", "created": "Wed, 21 Mar 2018 03:54:18 GMT"}], "update_date": "2018-03-22", "authors_parsed": [["Hosseini", "Hossein", ""], ["Xiao", "Baicen", ""], ["Jaiswal", "Mayoore", ""], ["Poovendran", "Radha", ""]]}, {"id": "1803.07742", "submitter": "Samvit Jain", "authors": "Samvit Jain and Joseph E. Gonzalez", "title": "Fast Semantic Segmentation on Video Using Block Motion-Based Feature\n  Interpolation", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional networks optimized for accuracy on challenging, dense\nprediction tasks are prohibitively slow to run on each frame in a video. The\nspatial similarity of nearby video frames, however, suggests opportunity to\nreuse computation. Existing work has explored basic feature reuse and feature\nwarping based on optical flow, but has encountered limits to the speedup\nattainable with these techniques. In this paper, we present a new, two part\napproach to accelerating inference on video. First, we propose a fast feature\npropagation technique that utilizes the block motion vectors present in\ncompressed video (e.g. H.264 codecs) to cheaply propagate features from frame\nto frame. Second, we develop a novel feature estimation scheme, termed feature\ninterpolation, that fuses features propagated from enclosing keyframes to\nrender accurate feature estimates, even at sparse keyframe frequencies. We\nevaluate our system on the Cityscapes and CamVid datasets, comparing to both a\nframe-by-frame baseline and related work. We find that we are able to\nsubstantially accelerate segmentation on video, achieving near real-time frame\nrates (20.1 frames per second) on large images (960 x 720 pixels), while\nmaintaining competitive accuracy. This represents an improvement of almost 6x\nover the single-frame baseline and 2.5x over the fastest prior work.\n", "versions": [{"version": "v1", "created": "Wed, 21 Mar 2018 04:05:45 GMT"}, {"version": "v2", "created": "Thu, 22 Mar 2018 18:29:57 GMT"}, {"version": "v3", "created": "Fri, 6 Jul 2018 23:58:55 GMT"}, {"version": "v4", "created": "Tue, 9 Oct 2018 23:29:09 GMT"}, {"version": "v5", "created": "Sun, 25 Nov 2018 00:41:01 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Jain", "Samvit", ""], ["Gonzalez", "Joseph E.", ""]]}, {"id": "1803.07780", "submitter": "Huy-Hieu Pham", "authors": "Huy-Hieu Pham, Louahdi Khoudour, Alain Crouzil, Pablo Zegers, and\n  Sergio A. Velastin", "title": "Learning and Recognizing Human Action from Skeleton Movement with Deep\n  Residual Neural Networks", "comments": "The 8th International Conference of Pattern Recognition Systems\n  (ICPRS 2017), Madrid, Spain", "journal-ref": null, "doi": "10.1049/cp.2017.0154", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Automatic human action recognition is indispensable for almost artificial\nintelligent systems such as video surveillance, human-computer interfaces,\nvideo retrieval, etc. Despite a lot of progress, recognizing actions in an\nunknown video is still a challenging task in computer vision. Recently, deep\nlearning algorithms have proved its great potential in many vision-related\nrecognition tasks. In this paper, we propose the use of Deep Residual Neural\nNetworks (ResNets) to learn and recognize human action from skeleton data\nprovided by Kinect sensor. Firstly, the body joint coordinates are transformed\ninto 3D-arrays and saved in RGB images space. Five different deep learning\nmodels based on ResNet have been designed to extract image features and\nclassify them into classes. Experiments are conducted on two public video\ndatasets for human action recognition containing various challenges. The\nresults show that our method achieves the state-of-the-art performance\ncomparing with existing approaches.\n", "versions": [{"version": "v1", "created": "Wed, 21 Mar 2018 07:43:44 GMT"}], "update_date": "2018-03-22", "authors_parsed": [["Pham", "Huy-Hieu", ""], ["Khoudour", "Louahdi", ""], ["Crouzil", "Alain", ""], ["Zegers", "Pablo", ""], ["Velastin", "Sergio A.", ""]]}, {"id": "1803.07781", "submitter": "Huy-Hieu Pham", "authors": "Huy-Hieu Pham, Louahdi Khoudour, Alain Crouzil, Pablo Zegers, Sergio\n  A. Velastin", "title": "Exploiting deep residual networks for human action recognition from\n  skeletal data", "comments": "This version corresponds to the pre-print of the paper accepted for\n  Computer Vision and Image Understanding (CVIU)", "journal-ref": null, "doi": "10.1016/j.cviu.2018.03.003", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The computer vision community is currently focusing on solving action\nrecognition problems in real videos, which contain thousands of samples with\nmany challenges. In this process, Deep Convolutional Neural Networks (D-CNNs)\nhave played a significant role in advancing the state-of-the-art in various\nvision-based action recognition systems. Recently, the introduction of residual\nconnections in conjunction with a more traditional CNN model in a single\narchitecture called Residual Network (ResNet) has shown impressive performance\nand great potential for image recognition tasks. In this paper, we investigate\nand apply deep ResNets for human action recognition using skeletal data\nprovided by depth sensors. Firstly, the 3D coordinates of the human body joints\ncarried in skeleton sequences are transformed into image-based representations\nand stored as RGB images. These color images are able to capture the\nspatial-temporal evolutions of 3D motions from skeleton sequences and can be\nefficiently learned by D-CNNs. We then propose a novel deep learning\narchitecture based on ResNets to learn features from obtained color-based\nrepresentations and classify them into action classes. The proposed method is\nevaluated on three challenging benchmark datasets including MSR Action 3D,\nKARD, and NTU-RGB+D datasets. Experimental results demonstrate that our method\nachieves state-of-the-art performance for all these benchmarks whilst requiring\nless computation resource. In particular, the proposed method surpasses\nprevious approaches by a significant margin of 3.4% on MSR Action 3D dataset,\n0.67% on KARD dataset, and 2.5% on NTU-RGB+D dataset.\n", "versions": [{"version": "v1", "created": "Wed, 21 Mar 2018 07:43:53 GMT"}], "update_date": "2018-03-22", "authors_parsed": [["Pham", "Huy-Hieu", ""], ["Khoudour", "Louahdi", ""], ["Crouzil", "Alain", ""], ["Zegers", "Pablo", ""], ["Velastin", "Sergio A.", ""]]}, {"id": "1803.07801", "submitter": "Dogucan Yaman", "authors": "Fevziye Irem Eyiokur, Dogucan Yaman and Haz{\\i}m Kemal Ekenel", "title": "Domain Adaptation for Ear Recognition Using Deep Convolutional Neural\n  Networks", "comments": "12 pages, 7 figures, IET Biometrics", "journal-ref": null, "doi": "10.1049/iet-bmt.2017.0209", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we have extensively investigated the unconstrained ear\nrecognition problem. We have first shown the importance of domain adaptation,\nwhen deep convolutional neural network models are used for ear recognition. To\nenable domain adaptation, we have collected a new ear dataset using the\nMulti-PIE face dataset, which we named as Multi-PIE ear dataset. To improve the\nperformance further, we have combined different deep convolutional neural\nnetwork models. We have analyzed in depth the effect of ear image quality, for\nexample illumination and aspect ratio, on the classification performance.\nFinally, we have addressed the problem of dataset bias in the ear recognition\nfield. Experiments on the UERC dataset have shown that domain adaptation leads\nto a significant performance improvement. For example, when VGG-16 model is\nused and the domain adaptation is applied, an absolute increase of around 10\\%\nhas been achieved. Combining different deep convolutional neural network models\nhas further improved the accuracy by 4\\%. It has also been observed that image\nquality has an influence on the results. In the experiments that we have\nconducted to examine the dataset bias, given an ear image, we were able to\nclassify the dataset that it has come from with 99.71\\% accuracy, which\nindicates a strong bias among the ear recognition datasets.\n", "versions": [{"version": "v1", "created": "Wed, 21 Mar 2018 08:55:28 GMT"}], "update_date": "2018-03-22", "authors_parsed": [["Eyiokur", "Fevziye Irem", ""], ["Yaman", "Dogucan", ""], ["Ekenel", "Haz\u0131m Kemal", ""]]}, {"id": "1803.07817", "submitter": "Xuenan Cui", "authors": "Eunsoo Park, Xuenan Cui, Weonjin Kim, Jinsong Liu, and Hakil Kim", "title": "Patch-based Fake Fingerprint Detection Using a Fully Convolutional\n  Neural Network with a Small Number of Parameters and an Optimal Threshold", "comments": "12 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fingerprint authentication is widely used in biometrics due to its simple\nprocess, but it is vulnerable to fake fingerprints. This study proposes a\npatch-based fake fingerprint detection method using a fully convolutional\nneural network with a small number of parameters and an optimal threshold to\nsolve the above-mentioned problem. Unlike the existing methods that classify a\nfingerprint as live or fake, the proposed method classifies fingerprints as\nfake, live, or background, so preprocessing methods such as segmentation are\nnot needed. The proposed convolutional neural network (CNN) structure applies\nthe Fire module of SqueezeNet, and the fewer parameters used require only 2.0\nMB of memory. The network that has completed training is applied to the\ntraining data in a fully convolutional way, and the optimal threshold to\ndistinguish fake fingerprints is determined, which is used in the final test.\nAs a result of this study experiment, the proposed method showed an average\nclassification error of 1.35%, demonstrating a fake fingerprint detection\nmethod using a high-performance CNN with a small number of parameters.\n", "versions": [{"version": "v1", "created": "Wed, 21 Mar 2018 09:50:25 GMT"}], "update_date": "2018-03-22", "authors_parsed": [["Park", "Eunsoo", ""], ["Cui", "Xuenan", ""], ["Kim", "Weonjin", ""], ["Liu", "Jinsong", ""], ["Kim", "Hakil", ""]]}, {"id": "1803.07830", "submitter": "Xuenan Cui", "authors": "Eunsoo Park, Xuenan Cui, Weonjin Kim, Hakil Kim", "title": "End-to-End Fingerprints Liveness Detection using Convolutional Networks\n  with Gram module", "comments": "15 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes an end-to-end CNN(Convolutional Neural Networks) model\nthat uses gram modules with parameters that are approximately 1.2MB in size to\ndetect fake fingerprints. The proposed method assumes that texture is the most\nappropriate characteristic in fake fingerprint detection, and implements the\ngram module to extract textures from the CNN. The proposed CNN structure uses\nthe fire module as the base model and uses the gram module for texture\nextraction. Tensors that passed the fire module will be joined with gram\nmodules to create a gram matrix with the same spatial size. After 3 gram\nmatrices extracted from different layers are combined with the channel axis, it\nbecomes the basis for categorizing fake fingerprints. The experiment results\nhad an average detection error of 2.61% from the LivDet 2011, 2013, 2015 data,\nproving that an end-to-end CNN structure with few parameters that is able to be\nused in fake fingerprint detection can be designed.\n", "versions": [{"version": "v1", "created": "Wed, 21 Mar 2018 10:11:27 GMT"}], "update_date": "2018-03-22", "authors_parsed": [["Park", "Eunsoo", ""], ["Cui", "Xuenan", ""], ["Kim", "Weonjin", ""], ["Kim", "Hakil", ""]]}, {"id": "1803.07835", "submitter": "Yao Feng", "authors": "Yao Feng, Fan Wu, Xiaohu Shao, Yanfeng Wang, Xi Zhou", "title": "Joint 3D Face Reconstruction and Dense Alignment with Position Map\n  Regression Network", "comments": "18 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  We propose a straightforward method that simultaneously reconstructs the 3D\nfacial structure and provides dense alignment. To achieve this, we design a 2D\nrepresentation called UV position map which records the 3D shape of a complete\nface in UV space, then train a simple Convolutional Neural Network to regress\nit from a single 2D image. We also integrate a weight mask into the loss\nfunction during training to improve the performance of the network. Our method\ndoes not rely on any prior face model, and can reconstruct full facial geometry\nalong with semantic meaning. Meanwhile, our network is very light-weighted and\nspends only 9.8ms to process an image, which is extremely faster than previous\nworks. Experiments on multiple challenging datasets show that our method\nsurpasses other state-of-the-art methods on both reconstruction and alignment\ntasks by a large margin.\n", "versions": [{"version": "v1", "created": "Wed, 21 Mar 2018 10:27:04 GMT"}], "update_date": "2018-03-22", "authors_parsed": [["Feng", "Yao", ""], ["Wu", "Fan", ""], ["Shao", "Xiaohu", ""], ["Wang", "Yanfeng", ""], ["Zhou", "Xi", ""]]}, {"id": "1803.07913", "submitter": "Manuele Brambilla", "authors": "Amos Sironi, Manuele Brambilla, Nicolas Bourdis, Xavier Lagorce, Ryad\n  Benosman", "title": "HATS: Histograms of Averaged Time Surfaces for Robust Event-based Object\n  Classification", "comments": "Accepted for publication at CVPR2018. Dataset available at\n  http://www.prophesee.ai/dataset-n-cars/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Event-based cameras have recently drawn the attention of the Computer Vision\ncommunity thanks to their advantages in terms of high temporal resolution, low\npower consumption and high dynamic range, compared to traditional frame-based\ncameras. These properties make event-based cameras an ideal choice for\nautonomous vehicles, robot navigation or UAV vision, among others. However, the\naccuracy of event-based object classification algorithms, which is of crucial\nimportance for any reliable system working in real-world conditions, is still\nfar behind their frame-based counterparts. Two main reasons for this\nperformance gap are: 1. The lack of effective low-level representations and\narchitectures for event-based object classification and 2. The absence of large\nreal-world event-based datasets. In this paper we address both problems. First,\nwe introduce a novel event-based feature representation together with a new\nmachine learning architecture. Compared to previous approaches, we use local\nmemory units to efficiently leverage past temporal information and build a\nrobust event-based representation. Second, we release the first large\nreal-world event-based dataset for object classification. We compare our method\nto the state-of-the-art with extensive experiments, showing better\nclassification performance and real-time computation.\n", "versions": [{"version": "v1", "created": "Wed, 21 Mar 2018 13:37:24 GMT"}], "update_date": "2018-03-22", "authors_parsed": [["Sironi", "Amos", ""], ["Brambilla", "Manuele", ""], ["Bourdis", "Nicolas", ""], ["Lagorce", "Xavier", ""], ["Benosman", "Ryad", ""]]}, {"id": "1803.07915", "submitter": "Antonio Sgorbissa", "authors": "Roberto Menicatti, Barbara Bruno and Antonio Sgorbissa", "title": "Modelling the Influence of Cultural Information on Vision-Based Human\n  Home Activity Recognition", "comments": "7 pages, 4 figures, Proc. URAI2017, International Conference on\n  Ubiquitous Robots and Ambient Intelligence, Maison Glad Jeju, Jeju, Korea\n  from June 28-July 2017", "journal-ref": "Proc. URAI2017, International Conference on Ubiquitous Robots and\n  Ambient Intelligence, Maison Glad Jeju, Jeju, Korea from June 28-July 2017", "doi": "10.1109/URAI.2017.7992880", "report-no": null, "categories": "cs.CV cs.CY cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Daily life activities, such as eating and sleeping, are deeply influenced by\na person's culture, hence generating differences in the way a same activity is\nperformed by individuals belonging to different cultures. We argue that taking\ncultural information into account can improve the performance of systems for\nthe automated recognition of human activities. We propose four different\nsolutions to the problem and present a system which uses a Naive Bayes model to\nassociate cultural information with semantic information extracted from still\nimages. Preliminary experiments with a dataset of images of individuals lying\non the floor, sleeping on a futon and sleeping on a bed suggest that: i)\nsolutions explicitly taking cultural information into account are more accurate\nthan culture-unaware solutions; and ii) the proposed system is a promising\nstarting point for the development of culture-aware Human Activity Recognition\nmethods.\n", "versions": [{"version": "v1", "created": "Wed, 21 Mar 2018 13:41:10 GMT"}], "update_date": "2018-03-23", "authors_parsed": [["Menicatti", "Roberto", ""], ["Bruno", "Barbara", ""], ["Sgorbissa", "Antonio", ""]]}, {"id": "1803.07950", "submitter": "Lijun Li", "authors": "Lijun Li and Boqing Gong", "title": "End-to-End Video Captioning with Multitask Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although end-to-end (E2E) learning has led to impressive progress on a\nvariety of visual understanding tasks, it is often impeded by hardware\nconstraints (e.g., GPU memory) and is prone to overfitting. When it comes to\nvideo captioning, one of the most challenging benchmark tasks in computer\nvision, those limitations of E2E learning are especially amplified by the fact\nthat both the input videos and output captions are lengthy sequences. Indeed,\nstate-of-the-art methods for video captioning process video frames by\nconvolutional neural networks and generate captions by unrolling recurrent\nneural networks. If we connect them in an E2E manner, the resulting model is\nboth memory-consuming and data-hungry, making it extremely hard to train. In\nthis paper, we propose a multitask reinforcement learning approach to training\nan E2E video captioning model. The main idea is to mine and construct as many\neffective tasks (e.g., attributes, rewards, and the captions) as possible from\nthe human captioned videos such that they can jointly regulate the search space\nof the E2E neural network, from which an E2E video captioning model can be\nfound and generalized to the testing phase. To the best of our knowledge, this\nis the first video captioning model that is trained end-to-end from the raw\nvideo input to the caption output. Experimental results show that such a model\noutperforms existing ones to a large margin on two benchmark video captioning\ndatasets.\n", "versions": [{"version": "v1", "created": "Wed, 21 Mar 2018 14:51:17 GMT"}, {"version": "v2", "created": "Tue, 1 Jan 2019 10:40:07 GMT"}], "update_date": "2019-01-03", "authors_parsed": [["Li", "Lijun", ""], ["Gong", "Boqing", ""]]}, {"id": "1803.07955", "submitter": "Chongyi Li", "authors": "Chongyi Li and Jichang Guo and Fatih Porikli and Huazhu Fu and Yanwei\n  Pang", "title": "A Cascaded Convolutional Neural Network for Single Image Dehazing", "comments": "This manuscript is accepted by IEEE ACCESS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Images captured under outdoor scenes usually suffer from low contrast and\nlimited visibility due to suspended atmospheric particles, which directly\naffects the quality of photos. Despite numerous image dehazing methods have\nbeen proposed, effective hazy image restoration remains a challenging problem.\nExisting learning-based methods usually predict the medium transmission by\nConvolutional Neural Networks (CNNs), but ignore the key global atmospheric\nlight. Different from previous learning-based methods, we propose a flexible\ncascaded CNN for single hazy image restoration, which considers the medium\ntransmission and global atmospheric light jointly by two task-driven\nsubnetworks. Specifically, the medium transmission estimation subnetwork is\ninspired by the densely connected CNN while the global atmospheric light\nestimation subnetwork is a light-weight CNN. Besides, these two subnetworks are\ncascaded by sharing the common features. Finally, with the estimated model\nparameters, the haze-free image is obtained by the atmospheric scattering model\ninversion, which achieves more accurate and effective restoration performance.\nQualitatively and quantitatively experimental results on the synthetic and\nreal-world hazy images demonstrate that the proposed method effectively removes\nhaze from such images, and outperforms several state-of-the-art dehazing\nmethods.\n", "versions": [{"version": "v1", "created": "Wed, 21 Mar 2018 15:01:21 GMT"}], "update_date": "2018-03-22", "authors_parsed": [["Li", "Chongyi", ""], ["Guo", "Jichang", ""], ["Porikli", "Fatih", ""], ["Fu", "Huazhu", ""], ["Pang", "Yanwei", ""]]}, {"id": "1803.07973", "submitter": "Hang Dai", "authors": "Hang Dai, Nick Pears, William Smith", "title": "Non-rigid 3D Shape Registration using an Adaptive Template", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new fully-automatic non-rigid 3D shape registration (morphing)\nframework comprising (1) a new 3D landmarking and pose normalisation method;\n(2) an adaptive shape template method to accelerate the convergence of\nregistration algorithms and achieve a better final shape correspondence and (3)\na new iterative registration method that combines Iterative Closest Points with\nCoherent Point Drift (CPD) to achieve a more stable and accurate correspondence\nestablishment than standard CPD. We call this new morphing approach Iterative\nCoherent Point Drift (ICPD). Our proposed framework is evaluated qualitatively\nand quantitatively on three datasets and compared with several other methods.\nThe proposed framework is shown to give state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Wed, 21 Mar 2018 15:44:36 GMT"}, {"version": "v2", "created": "Sat, 22 Sep 2018 18:07:12 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Dai", "Hang", ""], ["Pears", "Nick", ""], ["Smith", "William", ""]]}, {"id": "1803.07985", "submitter": "Hauke J\\\"urgen M\\\"onck", "authors": "Hauke J\\\"urgen M\\\"onck, Andreas J\\\"org, Tobias von Falkenhausen,\n  Julian Tanke, Benjamin Wild, David Dormagen, Jonas Piotrowski, Claudia\n  Winklmayr, David Bierbach, Tim Landgraf", "title": "BioTracker: An Open-Source Computer Vision Framework for Visual Animal\n  Tracking", "comments": "4 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The study of animal behavior increasingly relies on (semi-) automatic methods\nfor the extraction of relevant behavioral features from video or picture data.\nTo date, several specialized software products exist to detect and track\nanimals' positions in simple (laboratory) environments. Tracking animals in\ntheir natural environments, however, often requires substantial customization\nof the image processing algorithms to the problem-specific image\ncharacteristics. Here we introduce BioTracker, an open-source computer vision\nframework, that provides programmers with core functionalities that are\nessential parts of a tracking software, such as video I/O, graphics overlays\nand mouse and keyboard interfaces. BioTracker additionally provides a number of\ndifferent tracking algorithms suitable for a variety of image recording\nconditions. The main feature of BioTracker is however the straightforward\nimplementation of new problem-specific tracking modules and vision algorithms\nthat can build upon BioTracker's core functionalities. With this open-source\nframework the scientific community can accelerate their research and focus on\nthe development of new vision algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 21 Mar 2018 16:12:18 GMT"}], "update_date": "2018-03-22", "authors_parsed": [["M\u00f6nck", "Hauke J\u00fcrgen", ""], ["J\u00f6rg", "Andreas", ""], ["von Falkenhausen", "Tobias", ""], ["Tanke", "Julian", ""], ["Wild", "Benjamin", ""], ["Dormagen", "David", ""], ["Piotrowski", "Jonas", ""], ["Winklmayr", "Claudia", ""], ["Bierbach", "David", ""], ["Landgraf", "Tim", ""]]}, {"id": "1803.07991", "submitter": "Florian Dubost", "authors": "Filipe Marques, Florian Dubost, Mariette Kemner-van de Corput, Harm\n  A.W. Tiddens, Marleen de Bruijne", "title": "Quantification of Lung Abnormalities in Cystic Fibrosis using Deep\n  Networks", "comments": "SPIE - Medical Imaging 2018: Image Processing", "journal-ref": "Proc. SPIE 10574, Medical Imaging 2018: Image Processing, 105741G\n  (2 March 2018)", "doi": "10.1117/12.2292188", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cystic fibrosis is a genetic disease which may appear in early life with\nstructural abnormalities in lung tissues. We propose to detect these\nabnormalities using a texture classification approach. Our method is a cascade\nof two convolutional neural networks. The first network detects the presence of\nabnormal tissues. The second network identifies the type of the structural\nabnormalities: bronchiectasis, atelectasis or mucus plugging.We also propose a\nnetwork computing pixel-wise heatmaps of abnormality presence learning only\nfrom the patch-wise annotations. Our database consists of CT scans of 194\nsubjects. We use 154 subjects to train our algorithms and the 40 remaining ones\nas a test set. We compare our method with random forest and a single neural\nnetwork approach. The first network reaches an accuracy of 0,94 for disease\ndetection, 0,18 higher than the random forest classifier and 0,37 higher than\nthe single neural network. Our cascade approach yields a final class-averaged\nF1-score of 0,33, outperforming the baseline method and the single network by\n0,10 and 0,12.\n", "versions": [{"version": "v1", "created": "Wed, 21 Mar 2018 16:18:46 GMT"}], "update_date": "2018-03-22", "authors_parsed": [["Marques", "Filipe", ""], ["Dubost", "Florian", ""], ["de Corput", "Mariette Kemner-van", ""], ["Tiddens", "Harm A. W.", ""], ["de Bruijne", "Marleen", ""]]}, {"id": "1803.07994", "submitter": "Joachim Folz", "authors": "Joachim Folz and Sebastian Palacio and Joern Hees and Damian Borth and\n  Andreas Dengel", "title": "Adversarial Defense based on Structure-to-Signal Autoencoders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial attack methods have demonstrated the fragility of deep neural\nnetworks. Their imperceptible perturbations are frequently able fool\nclassifiers into potentially dangerous misclassifications. We propose a novel\nway to interpret adversarial perturbations in terms of the effective input\nsignal that classifiers actually use. Based on this, we apply specially trained\nautoencoders, referred to as S2SNets, as defense mechanism. They follow a\ntwo-stage training scheme: first unsupervised, followed by a fine-tuning of the\ndecoder, using gradients from an existing classifier. S2SNets induce a shift in\nthe distribution of gradients propagated through them, stripping them from\nclass-dependent signal. We analyze their robustness against several white-box\nand gray-box scenarios on the large ImageNet dataset. Our approach reaches\ncomparable resilience in white-box attack scenarios as other state-of-the-art\ndefenses in gray-box scenarios. We further analyze the relationships of\nAlexNet, VGG 16, ResNet 50 and Inception v3 in adversarial space, and found\nthat VGG 16 is the easiest to fool, while perturbations from ResNet 50 are the\nmost transferable.\n", "versions": [{"version": "v1", "created": "Wed, 21 Mar 2018 16:25:26 GMT"}], "update_date": "2018-03-22", "authors_parsed": [["Folz", "Joachim", ""], ["Palacio", "Sebastian", ""], ["Hees", "Joern", ""], ["Borth", "Damian", ""], ["Dengel", "Andreas", ""]]}, {"id": "1803.08006", "submitter": "Anna Khoreva", "authors": "Anna Khoreva, Anna Rohrbach, and Bernt Schiele", "title": "Video Object Segmentation with Language Referring Expressions", "comments": "ACCV 2018: 14th Asian Conference on Computer Vision", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most state-of-the-art semi-supervised video object segmentation methods rely\non a pixel-accurate mask of a target object provided for the first frame of a\nvideo. However, obtaining a detailed segmentation mask is expensive and\ntime-consuming. In this work we explore an alternative way of identifying a\ntarget object, namely by employing language referring expressions. Besides\nbeing a more practical and natural way of pointing out a target object, using\nlanguage specifications can help to avoid drift as well as make the system more\nrobust to complex dynamics and appearance variations. Leveraging recent\nadvances of language grounding models designed for images, we propose an\napproach to extend them to video data, ensuring temporally coherent\npredictions. To evaluate our method we augment the popular video object\nsegmentation benchmarks, DAVIS'16 and DAVIS'17 with language descriptions of\ntarget objects. We show that our language-supervised approach performs on par\nwith the methods which have access to a pixel-level mask of the target object\non DAVIS'16 and is competitive to methods using scribbles on the challenging\nDAVIS'17 dataset.\n", "versions": [{"version": "v1", "created": "Wed, 21 Mar 2018 16:44:19 GMT"}, {"version": "v2", "created": "Wed, 9 May 2018 14:40:55 GMT"}, {"version": "v3", "created": "Tue, 5 Feb 2019 10:15:25 GMT"}], "update_date": "2019-02-06", "authors_parsed": [["Khoreva", "Anna", ""], ["Rohrbach", "Anna", ""], ["Schiele", "Bernt", ""]]}, {"id": "1803.08018", "submitter": "Akhil Gurram", "authors": "Akhil Gurram, Onay Urfalioglu, Ibrahim Halfaoui, Fahd Bouzaraa and\n  Antonio M. Lopez", "title": "Monocular Depth Estimation by Learning from Heterogeneous Datasets", "comments": "Accepted in IEEE-Intelligent Vehicles Symposium, IV'2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Depth estimation provides essential information to perform autonomous driving\nand driver assistance. Especially, Monocular Depth Estimation is interesting\nfrom a practical point of view, since using a single camera is cheaper than\nmany other options and avoids the need for continuous calibration strategies as\nrequired by stereo-vision approaches. State-of-the-art methods for Monocular\nDepth Estimation are based on Convolutional Neural Networks (CNNs). A promising\nline of work consists of introducing additional semantic information about the\ntraffic scene when training CNNs for depth estimation. In practice, this means\nthat the depth data used for CNN training is complemented with images having\npixel-wise semantic labels, which usually are difficult to annotate (e.g.\ncrowded urban images). Moreover, so far it is common practice to assume that\nthe same raw training data is associated with both types of ground truth, i.e.,\ndepth and semantic labels. The main contribution of this paper is to show that\nthis hard constraint can be circumvented, i.e., that we can train CNNs for\ndepth estimation by leveraging the depth and semantic information coming from\nheterogeneous datasets. In order to illustrate the benefits of our approach, we\ncombine KITTI depth and Cityscapes semantic segmentation datasets,\noutperforming state-of-the-art results on Monocular Depth Estimation.\n", "versions": [{"version": "v1", "created": "Wed, 21 Mar 2018 17:18:25 GMT"}, {"version": "v2", "created": "Wed, 12 Sep 2018 17:40:58 GMT"}], "update_date": "2018-09-13", "authors_parsed": [["Gurram", "Akhil", ""], ["Urfalioglu", "Onay", ""], ["Halfaoui", "Ibrahim", ""], ["Bouzaraa", "Fahd", ""], ["Lopez", "Antonio M.", ""]]}, {"id": "1803.08024", "submitter": "Kuang-Huei Lee", "authors": "Kuang-Huei Lee, Xi Chen, Gang Hua, Houdong Hu, Xiaodong He", "title": "Stacked Cross Attention for Image-Text Matching", "comments": "Accepted to ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the problem of image-text matching. Inferring the\nlatent semantic alignment between objects or other salient stuff (e.g. snow,\nsky, lawn) and the corresponding words in sentences allows to capture\nfine-grained interplay between vision and language, and makes image-text\nmatching more interpretable. Prior work either simply aggregates the similarity\nof all possible pairs of regions and words without attending differentially to\nmore and less important words or regions, or uses a multi-step attentional\nprocess to capture limited number of semantic alignments which is less\ninterpretable. In this paper, we present Stacked Cross Attention to discover\nthe full latent alignments using both image regions and words in a sentence as\ncontext and infer image-text similarity. Our approach achieves the\nstate-of-the-art results on the MS-COCO and Flickr30K datasets. On Flickr30K,\nour approach outperforms the current best methods by 22.1% relatively in text\nretrieval from image query, and 18.2% relatively in image retrieval with text\nquery (based on Recall@1). On MS-COCO, our approach improves sentence retrieval\nby 17.8% relatively and image retrieval by 16.6% relatively (based on Recall@1\nusing the 5K test set). Code has been made available at:\nhttps://github.com/kuanghuei/SCAN.\n", "versions": [{"version": "v1", "created": "Wed, 21 Mar 2018 17:22:27 GMT"}, {"version": "v2", "created": "Mon, 23 Jul 2018 04:41:57 GMT"}], "update_date": "2018-07-24", "authors_parsed": [["Lee", "Kuang-Huei", ""], ["Chen", "Xi", ""], ["Hua", "Gang", ""], ["Hu", "Houdong", ""], ["He", "Xiaodong", ""]]}, {"id": "1803.08035", "submitter": "Xiaolong Wang", "authors": "Xiaolong Wang, Yufei Ye, Abhinav Gupta", "title": "Zero-shot Recognition via Semantic Embeddings and Knowledge Graphs", "comments": "CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of zero-shot recognition: learning a visual\nclassifier for a category with zero training examples, just using the word\nembedding of the category and its relationship to other categories, which\nvisual data are provided. The key to dealing with the unfamiliar or novel\ncategory is to transfer knowledge obtained from familiar classes to describe\nthe unfamiliar class. In this paper, we build upon the recently introduced\nGraph Convolutional Network (GCN) and propose an approach that uses both\nsemantic embeddings and the categorical relationships to predict the\nclassifiers. Given a learned knowledge graph (KG), our approach takes as input\nsemantic embeddings for each node (representing visual category). After a\nseries of graph convolutions, we predict the visual classifier for each\ncategory. During training, the visual classifiers for a few categories are\ngiven to learn the GCN parameters. At test time, these filters are used to\npredict the visual classifiers of unseen categories. We show that our approach\nis robust to noise in the KG. More importantly, our approach provides\nsignificant improvement in performance compared to the current state-of-the-art\nresults (from 2 ~ 3% on some metrics to whopping 20% on a few).\n", "versions": [{"version": "v1", "created": "Wed, 21 Mar 2018 17:52:42 GMT"}, {"version": "v2", "created": "Sun, 8 Apr 2018 18:53:39 GMT"}], "update_date": "2018-04-10", "authors_parsed": [["Wang", "Xiaolong", ""], ["Ye", "Yufei", ""], ["Gupta", "Abhinav", ""]]}, {"id": "1803.08071", "submitter": "Zheng Dang", "authors": "Zheng Dang, Kwang Moo Yi, Yinlin Hu, Fei Wang, Pascal Fua, Mathieu\n  Salzmann", "title": "Eigendecomposition-free Training of Deep Networks with Zero\n  Eigenvalue-based Losses", "comments": "25 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Many classical Computer Vision problems, such as essential matrix computation\nand pose estimation from 3D to 2D correspondences, can be solved by finding the\neigenvector corresponding to the smallest, or zero, eigenvalue of a matrix\nrepresenting a linear system. Incorporating this in deep learning frameworks\nwould allow us to explicitly encode known notions of geometry, instead of\nhaving the network implicitly learn them from data. However, performing\neigendecomposition within a network requires the ability to differentiate this\noperation. Unfortunately, while theoretically doable, this introduces numerical\ninstability in the optimization process in practice.\n  In this paper, we introduce an eigendecomposition-free approach to training a\ndeep network whose loss depends on the eigenvector corresponding to a zero\neigenvalue of a matrix predicted by the network. We demonstrate on several\ntasks, including keypoint matching and 3D pose estimation, that our approach is\nmuch more robust than explicit differentiation of the eigendecomposition, It\nhas better convergence properties and yields state-of-the-art results on both\ntasks.\n", "versions": [{"version": "v1", "created": "Wed, 21 Mar 2018 18:13:54 GMT"}, {"version": "v2", "created": "Mon, 26 Mar 2018 12:36:08 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Dang", "Zheng", ""], ["Yi", "Kwang Moo", ""], ["Hu", "Yinlin", ""], ["Wang", "Fei", ""], ["Fua", "Pascal", ""], ["Salzmann", "Mathieu", ""]]}, {"id": "1803.08085", "submitter": "Jiawei He", "authors": "Jiawei He, Andreas Lehrmann, Joseph Marino, Greg Mori, Leonid Sigal", "title": "Probabilistic Video Generation using Holistic Attribute Control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Videos express highly structured spatio-temporal patterns of visual data. A\nvideo can be thought of as being governed by two factors: (i) temporally\ninvariant (e.g., person identity), or slowly varying (e.g., activity),\nattribute-induced appearance, encoding the persistent content of each frame,\nand (ii) an inter-frame motion or scene dynamics (e.g., encoding evolution of\nthe person ex-ecuting the action). Based on this intuition, we propose a\ngenerative framework for video generation and future prediction. The proposed\nframework generates a video (short clip) by decoding samples sequentially drawn\nfrom a latent space distribution into full video frames. Variational\nAutoencoders (VAEs) are used as a means of encoding/decoding frames into/from\nthe latent space and RNN as a wayto model the dynamics in the latent space. We\nimprove the video generation consistency through temporally-conditional\nsampling and quality by structuring the latent space with attribute controls;\nensuring that attributes can be both inferred and conditioned on during\nlearning/generation. As a result, given attributes and/orthe first frame, our\nmodel is able to generate diverse but highly consistent sets ofvideo sequences,\naccounting for the inherent uncertainty in the prediction task. Experimental\nresults on Chair CAD, Weizmann Human Action, and MIT-Flickr datasets, along\nwith detailed comparison to the state-of-the-art, verify effectiveness of the\nframework.\n", "versions": [{"version": "v1", "created": "Wed, 21 Mar 2018 18:39:47 GMT"}], "update_date": "2018-03-26", "authors_parsed": [["He", "Jiawei", ""], ["Lehrmann", "Andreas", ""], ["Marino", "Joseph", ""], ["Mori", "Greg", ""], ["Sigal", "Leonid", ""]]}, {"id": "1803.08094", "submitter": "Madan Ravi Ganesh", "authors": "Madan Ravi Ganesh, Eric Hofesmann, Byungsu Min, Nadha Gafoor and Jason\n  J. Corso", "title": "T-RECS: Training for Rate-Invariant Embeddings by Controlling Speed for\n  Action Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An action should remain identifiable when modifying its speed: consider the\ncontrast between an expert chef and a novice chef each chopping an onion. Here,\nwe expect the novice chef to have a relatively measured and slow approach to\nchopping when compared to the expert. In general, the speed at which actions\nare performed, whether slower or faster than average, should not dictate how\nthey are recognized. We explore the erratic behavior caused by this phenomena\non state-of-the-art deep network-based methods for action recognition in terms\nof maximum performance and stability in recognition accuracy across a range of\ninput video speeds. By observing the trends in these metrics and summarizing\nthem based on expected temporal behaviour w.r.t. variations in input video\nspeeds, we find two distinct types of network architectures. In this paper, we\npropose a preprocessing method named T-RECS, as a way to extend\ndeep-network-based methods for action recognition to explicitly account for\nspeed variability in the data. We do so by adaptively resampling the inputs to\na given model. T-RECS is agnostic to the specific deep-network model; we apply\nit to four state-of-the-art action recognition architectures, C3D, I3D, TSN,\nand ConvNet+LSTM. On HMDB51 and UCF101, T-RECS-based I3D models show a peak\nimprovement of at least 2.9% in performance over the baseline while\nT-RECS-based C3D models achieve a maximum improvement in stability by 59% over\nthe baseline, on the HMDB51 dataset.\n", "versions": [{"version": "v1", "created": "Wed, 21 Mar 2018 19:05:15 GMT"}, {"version": "v2", "created": "Fri, 23 Mar 2018 04:11:36 GMT"}], "update_date": "2018-03-26", "authors_parsed": [["Ganesh", "Madan Ravi", ""], ["Hofesmann", "Eric", ""], ["Min", "Byungsu", ""], ["Gafoor", "Nadha", ""], ["Corso", "Jason J.", ""]]}, {"id": "1803.08103", "submitter": "Chi Li", "authors": "Chi Li, Jin Bai and Gregory D. Hager", "title": "A Unified Framework for Multi-View Multi-Class Object Pose Estimation", "comments": "Accepted in ECCV2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One core challenge in object pose estimation is to ensure accurate and robust\nperformance for large numbers of diverse foreground objects amidst complex\nbackground clutter. In this work, we present a scalable framework for\naccurately inferring six Degree-of-Freedom (6-DoF) pose for a large number of\nobject classes from single or multiple views. To learn discriminative pose\nfeatures, we integrate three new capabilities into a deep Convolutional Neural\nNetwork (CNN): an inference scheme that combines both classification and pose\nregression based on a uniform tessellation of the Special Euclidean group in\nthree dimensions (SE(3)), the fusion of class priors into the training process\nvia a tiled class map, and an additional regularization using deep supervision\nwith an object mask. Further, an efficient multi-view framework is formulated\nto address single-view ambiguity. We show that this framework consistently\nimproves the performance of the single-view network. We evaluate our method on\nthree large-scale benchmarks: YCB-Video, JHUScene-50 and ObjectNet-3D. Our\napproach achieves competitive or superior performance over the current\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 21 Mar 2018 19:41:33 GMT"}, {"version": "v2", "created": "Sat, 6 Oct 2018 16:57:11 GMT"}], "update_date": "2018-10-09", "authors_parsed": [["Li", "Chi", ""], ["Bai", "Jin", ""], ["Hager", "Gregory D.", ""]]}, {"id": "1803.08134", "submitter": "Qing Tian", "authors": "Qing Tian, Tal Arbel, James J. Clark", "title": "Task dependent Deep LDA pruning of neural networks", "comments": null, "journal-ref": "Computer Vision and Image Understanding 203C (2021) 103154", "doi": "10.1016/j.cviu.2020.103154", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With deep learning's success, a limited number of popular deep nets have been\nwidely adopted for various vision tasks. However, this usually results in\nunnecessarily high complexities and possibly many features of low task utility.\nIn this paper, we address this problem by introducing a task-dependent deep\npruning framework based on Fisher's Linear Discriminant Analysis (LDA). The\napproach can be applied to convolutional, fully-connected, and module-based\ndeep network structures, in all cases leveraging the high decorrelation of\nneuron motifs found in the pre-decision space and cross-layer deconv\ndependency. Moreover, we examine our approach's potential in network\narchitecture search for specific tasks and analyze the influence of our pruning\non model robustness to noises and adversarial attacks. Experimental results on\ndatasets of generic objects (ImageNet, CIFAR100) as well as domain specific\ntasks (Adience, and LFWA) illustrate our framework's superior performance over\nstate-of-the-art pruning approaches and fixed compact nets (e.g. SqueezeNet,\nMobileNet). The proposed method successfully maintains comparable accuracies\neven after discarding most parameters (98%-99% for VGG16, up to 82% for the\nalready compact InceptionNet) and with significant FLOP reductions (83% for\nVGG16, up to 64% for InceptionNet). Through pruning, we can also derive\nsmaller, but more accurate and more robust models suitable for the task.\n", "versions": [{"version": "v1", "created": "Wed, 21 Mar 2018 20:52:32 GMT"}, {"version": "v2", "created": "Sun, 18 Nov 2018 01:20:41 GMT"}, {"version": "v3", "created": "Sun, 2 Dec 2018 02:53:34 GMT"}, {"version": "v4", "created": "Wed, 1 May 2019 15:28:54 GMT"}, {"version": "v5", "created": "Mon, 7 Oct 2019 08:35:54 GMT"}, {"version": "v6", "created": "Tue, 24 Nov 2020 05:56:56 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Tian", "Qing", ""], ["Arbel", "Tal", ""], ["Clark", "James J.", ""]]}, {"id": "1803.08137", "submitter": "Sathya N. Ravi", "authors": "Sathya N. Ravi, Ronak Mehta, Vikas Singh", "title": "Robust Blind Deconvolution via Mirror Descent", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit the Blind Deconvolution problem with a focus on understanding its\nrobustness and convergence properties. Provable robustness to noise and other\nperturbations is receiving recent interest in vision, from obtaining immunity\nto adversarial attacks to assessing and describing failure modes of algorithms\nin mission critical applications. Further, many blind deconvolution methods\nbased on deep architectures internally make use of or optimize the basic\nformulation, so a clearer understanding of how this sub-module behaves, when it\ncan be solved, and what noise injection it can tolerate is a first order\nrequirement. We derive new insights into the theoretical underpinnings of blind\ndeconvolution. The algorithm that emerges has nice convergence guarantees and\nis provably robust in a sense we formalize in the paper. Interestingly, these\ntechnical results play out very well in practice, where on standard datasets\nour algorithm yields results competitive with or superior to the state of the\nart. Keywords: blind deconvolution, robust continuous optimization\n", "versions": [{"version": "v1", "created": "Wed, 21 Mar 2018 20:55:26 GMT"}], "update_date": "2018-03-23", "authors_parsed": [["Ravi", "Sathya N.", ""], ["Mehta", "Ronak", ""], ["Singh", "Vikas", ""]]}, {"id": "1803.08138", "submitter": "Aydogan Ozcan", "authors": "Yichen Wu, Yair Rivenson, Yibo Zhang, Zhensong Wei, Harun Gunaydin,\n  Xing Lin, Aydogan Ozcan", "title": "Extended depth-of-field in holographic image reconstruction using deep\n  learning based auto-focusing and phase-recovery", "comments": null, "journal-ref": "Optica, Vol. 5, Issue 6, pp. 704-710 (2018)", "doi": "10.1364/OPTICA.5.000704", "report-no": null, "categories": "cs.CV cs.LG physics.optics", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Holography encodes the three dimensional (3D) information of a sample in the\nform of an intensity-only recording. However, to decode the original sample\nimage from its hologram(s), auto-focusing and phase-recovery are needed, which\nare in general cumbersome and time-consuming to digitally perform. Here we\ndemonstrate a convolutional neural network (CNN) based approach that\nsimultaneously performs auto-focusing and phase-recovery to significantly\nextend the depth-of-field (DOF) in holographic image reconstruction. For this,\na CNN is trained by using pairs of randomly de-focused back-propagated\nholograms and their corresponding in-focus phase-recovered images. After this\ntraining phase, the CNN takes a single back-propagated hologram of a 3D sample\nas input to rapidly achieve phase-recovery and reconstruct an in focus image of\nthe sample over a significantly extended DOF. This deep learning based DOF\nextension method is non-iterative, and significantly improves the algorithm\ntime-complexity of holographic image reconstruction from O(nm) to O(1), where n\nrefers to the number of individual object points or particles within the sample\nvolume, and m represents the focusing search space within which each object\npoint or particle needs to be individually focused. These results highlight\nsome of the unique opportunities created by data-enabled statistical image\nreconstruction methods powered by machine learning, and we believe that the\npresented approach can be broadly applicable to computationally extend the DOF\nof other imaging modalities.\n", "versions": [{"version": "v1", "created": "Wed, 21 Mar 2018 20:59:33 GMT"}], "update_date": "2018-12-31", "authors_parsed": [["Wu", "Yichen", ""], ["Rivenson", "Yair", ""], ["Zhang", "Yibo", ""], ["Wei", "Zhensong", ""], ["Gunaydin", "Harun", ""], ["Lin", "Xing", ""], ["Ozcan", "Aydogan", ""]]}, {"id": "1803.08181", "submitter": "R. Karnik Ram", "authors": "Ganesh Iyer, R. Karnik Ram., J. Krishna Murthy, and K. Madhava Krishna", "title": "CalibNet: Geometrically Supervised Extrinsic Calibration using 3D\n  Spatial Transformer Networks", "comments": "Appeared in the proccedings of the IEEE International Conference on\n  Intelligent Robots and Systems (IROS) 2018", "journal-ref": null, "doi": "10.1109/IROS.2018.8593693", "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D LiDARs and 2D cameras are increasingly being used alongside each other in\nsensor rigs for perception tasks. Before these sensors can be used to gather\nmeaningful data, however, their extrinsics (and intrinsics) need to be\naccurately calibrated, as the performance of the sensor rig is extremely\nsensitive to these calibration parameters. A vast majority of existing\ncalibration techniques require significant amounts of data and/or calibration\ntargets and human effort, severely impacting their applicability in large-scale\nproduction systems. We address this gap with CalibNet: a self-supervised deep\nnetwork capable of automatically estimating the 6-DoF rigid body transformation\nbetween a 3D LiDAR and a 2D camera in real-time. CalibNet alleviates the need\nfor calibration targets, thereby resulting in significant savings in\ncalibration efforts. During training, the network only takes as input a LiDAR\npoint cloud, the corresponding monocular image, and the camera calibration\nmatrix K. At train time, we do not impose direct supervision (i.e., we do not\ndirectly regress to the calibration parameters, for example). Instead, we train\nthe network to predict calibration parameters that maximize the geometric and\nphotometric consistency of the input images and point clouds. CalibNet learns\nto iteratively solve the underlying geometric problem and accurately predicts\nextrinsic calibration parameters for a wide range of mis-calibrations, without\nrequiring retraining or domain adaptation. The project page is hosted at\nhttps://epiception.github.io/CalibNet\n", "versions": [{"version": "v1", "created": "Thu, 22 Mar 2018 00:24:04 GMT"}, {"version": "v2", "created": "Sun, 4 Aug 2019 10:40:14 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Iyer", "Ganesh", ""], ["Ram.", "R. Karnik", ""], ["Murthy", "J. Krishna", ""], ["Krishna", "K. Madhava", ""]]}, {"id": "1803.08190", "submitter": "Geonho Cha", "authors": "Geonho Cha, Minsik Lee, Jungchan Cho, Songhwai Oh", "title": "Deep Pose Consensus Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the problem of estimating a 3D human pose from a\nsingle image, which is important but difficult to solve due to many reasons,\nsuch as self-occlusions, wild appearance changes, and inherent ambiguities of\n3D estimation from a 2D cue. These difficulties make the problem ill-posed,\nwhich have become requiring increasingly complex estimators to enhance the\nperformance. On the other hand, most existing methods try to handle this\nproblem based on a single complex estimator, which might not be good solutions.\nIn this paper, to resolve this issue, we propose a\nmultiple-partial-hypothesis-based framework for the problem of estimating 3D\nhuman pose from a single image, which can be fine-tuned in an end-to-end\nfashion. We first select several joint groups from a human joint model using\nthe proposed sampling scheme, and estimate the 3D poses of each joint group\nseparately based on deep neural networks. After that, they are aggregated to\nobtain the final 3D poses using the proposed robust optimization formula. The\noverall procedure can be fine-tuned in an end-to-end fashion, resulting in\nbetter performance. In the experiments, the proposed framework shows the\nstate-of-the-art performances on popular benchmark data sets, namely Human3.6M\nand HumanEva, which demonstrate the effectiveness of the proposed framework.\n", "versions": [{"version": "v1", "created": "Thu, 22 Mar 2018 01:22:50 GMT"}, {"version": "v2", "created": "Mon, 7 Oct 2019 04:44:03 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Cha", "Geonho", ""], ["Lee", "Minsik", ""], ["Cho", "Jungchan", ""], ["Oh", "Songhwai", ""]]}, {"id": "1803.08207", "submitter": "Tristan Bepler", "authors": "Tristan Bepler, Andrew Morin, Julia Brasch, Lawrence Shapiro, Alex J.\n  Noble, and Bonnie Berger", "title": "Positive-unlabeled convolutional neural networks for particle picking in\n  cryo-electron micrographs", "comments": "43 pages, 5 main figures, 6 supplemental figures", "journal-ref": "Nature Methods (2019)", "doi": "10.1038/s41592-019-0575-8", "report-no": null, "categories": "q-bio.QM cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cryo-electron microscopy (cryoEM) is an increasingly popular method for\nprotein structure determination. However, identifying a sufficient number of\nparticles for analysis (often >100,000) can take months of manual effort.\nCurrent computational approaches are limited by high false positive rates and\nrequire significant ad-hoc post-processing, especially for unusually shaped\nparticles. To address this shortcoming, we develop Topaz, an efficient and\naccurate particle picking pipeline using neural networks trained with few\nlabeled particles by newly leveraging the remaining unlabeled particles through\nthe framework of positive-unlabeled (PU) learning. Remarkably, despite using\nminimal labeled particles, Topaz allows us to improve reconstruction resolution\nby up to 0.15 {\\AA} over published particles on three public cryoEM datasets\nwithout any post-processing. Furthermore, we show that our novel\ngeneralized-expectation criteria approach to PU learning outperforms existing\ngeneral PU learning approaches when applied to particle detection, especially\nfor challenging datasets of non-globular proteins. We expect Topaz to be an\nessential component of cryoEM analysis.\n", "versions": [{"version": "v1", "created": "Thu, 22 Mar 2018 02:24:22 GMT"}, {"version": "v2", "created": "Mon, 8 Oct 2018 19:18:18 GMT"}], "update_date": "2019-10-17", "authors_parsed": [["Bepler", "Tristan", ""], ["Morin", "Andrew", ""], ["Brasch", "Julia", ""], ["Shapiro", "Lawrence", ""], ["Noble", "Alex J.", ""], ["Berger", "Bonnie", ""]]}, {"id": "1803.08208", "submitter": "Xiongwei Wu", "authors": "Xiongwei Wu, Daoxin Zhang, Jianke Zhu, Steven C.H. Hoi", "title": "Single-Shot Bidirectional Pyramid Networks for High-Quality Object\n  Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have witnessed many exciting achievements for object detection\nusing deep learning techniques. Despite achieving significant progresses, most\nexisting detectors are designed to detect objects with relatively low-quality\nprediction of locations, i.e., often trained with the threshold of Intersection\nover Union (IoU) set to 0.5 by default, which can yield low-quality or even\nnoisy detections. It remains an open challenge for how to devise and train a\nhigh-quality detector that can achieve more precise localization (i.e.,\nIoU$>$0.5) without sacrificing the detection performance. In this paper, we\npropose a novel single-shot detection framework of Bidirectional Pyramid\nNetworks (BPN) towards high-quality object detection, which consists of two\nnovel components: (i) a Bidirectional Feature Pyramid structure for more\neffective and robust feature representations; and (ii) a Cascade Anchor\nRefinement to gradually refine the quality of predesigned anchors for more\neffective training. Our experiments showed that the proposed BPN achieves the\nbest performances among all the single-stage object detectors on both PASCAL\nVOC and MS COCO datasets, especially for high-quality detections.\n", "versions": [{"version": "v1", "created": "Thu, 22 Mar 2018 02:25:54 GMT"}], "update_date": "2018-03-23", "authors_parsed": [["Wu", "Xiongwei", ""], ["Zhang", "Daoxin", ""], ["Zhu", "Jianke", ""], ["Hoi", "Steven C. H.", ""]]}, {"id": "1803.08225", "submitter": "George Papandreou", "authors": "George Papandreou, Tyler Zhu, Liang-Chieh Chen, Spyros Gidaris,\n  Jonathan Tompson, Kevin Murphy", "title": "PersonLab: Person Pose Estimation and Instance Segmentation with a\n  Bottom-Up, Part-Based, Geometric Embedding Model", "comments": "Person detection and pose estimation, segmentation and grouping", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a box-free bottom-up approach for the tasks of pose estimation and\ninstance segmentation of people in multi-person images using an efficient\nsingle-shot model. The proposed PersonLab model tackles both semantic-level\nreasoning and object-part associations using part-based modeling. Our model\nemploys a convolutional network which learns to detect individual keypoints and\npredict their relative displacements, allowing us to group keypoints into\nperson pose instances. Further, we propose a part-induced geometric embedding\ndescriptor which allows us to associate semantic person pixels with their\ncorresponding person instance, delivering instance-level person segmentations.\nOur system is based on a fully-convolutional architecture and allows for\nefficient inference, with runtime essentially independent of the number of\npeople present in the scene. Trained on COCO data alone, our system achieves\nCOCO test-dev keypoint average precision of 0.665 using single-scale inference\nand 0.687 using multi-scale inference, significantly outperforming all previous\nbottom-up pose estimation systems. We are also the first bottom-up method to\nreport competitive results for the person class in the COCO instance\nsegmentation task, achieving a person category average precision of 0.417.\n", "versions": [{"version": "v1", "created": "Thu, 22 Mar 2018 04:31:02 GMT"}], "update_date": "2018-03-23", "authors_parsed": [["Papandreou", "George", ""], ["Zhu", "Tyler", ""], ["Chen", "Liang-Chieh", ""], ["Gidaris", "Spyros", ""], ["Tompson", "Jonathan", ""], ["Murphy", "Kevin", ""]]}, {"id": "1803.08244", "submitter": "Yasunori Kudo", "authors": "Yasunori Kudo, Keisuke Ogaki, Yusuke Matsui, Yuri Odagiri", "title": "Unsupervised Adversarial Learning of 3D Human Pose from 2D Joint\n  Locations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of three-dimensional (3D) human pose estimation from a single image\ncan be divided into two parts: (1) Two-dimensional (2D) human joint detection\nfrom the image and (2) estimating a 3D pose from the 2D joints. Herein, we\nfocus on the second part, i.e., a 3D pose estimation from 2D joint locations.\nThe problem with existing methods is that they require either (1) a 3D pose\ndataset or (2) 2D joint locations in consecutive frames taken from a video\nsequence. We aim to solve these problems. For the first time, we propose a\nmethod that learns a 3D human pose without any 3D datasets. Our method can\npredict a 3D pose from 2D joint locations in a single image. Our system is\nbased on the generative adversarial networks, and the networks are trained in\nan unsupervised manner. Our primary idea is that, if the network can predict a\n3D human pose correctly, the 3D pose that is projected onto a 2D plane should\nnot collapse even if it is rotated perpendicularly. We evaluated the\nperformance of our method using Human3.6M and the MPII dataset and showed that\nour network can predict a 3D pose well even if the 3D dataset is not available\nduring training.\n", "versions": [{"version": "v1", "created": "Thu, 22 Mar 2018 06:41:23 GMT"}], "update_date": "2018-03-23", "authors_parsed": [["Kudo", "Yasunori", ""], ["Ogaki", "Keisuke", ""], ["Matsui", "Yusuke", ""], ["Odagiri", "Yuri", ""]]}, {"id": "1803.08314", "submitter": "Xihui Liu", "authors": "Xihui Liu, Hongsheng Li, Jing Shao, Dapeng Chen, Xiaogang Wang", "title": "Show, Tell and Discriminate: Image Captioning by Self-retrieval with\n  Partially Labeled Data", "comments": "Accepted by ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of image captioning is to generate captions by machine to describe\nimage contents. Despite many efforts, generating discriminative captions for\nimages remains non-trivial. Most traditional approaches imitate the language\nstructure patterns, thus tend to fall into a stereotype of replicating frequent\nphrases or sentences and neglect unique aspects of each image. In this work, we\npropose an image captioning framework with a self-retrieval module as training\nguidance, which encourages generating discriminative captions. It brings unique\nadvantages: (1) the self-retrieval guidance can act as a metric and an\nevaluator of caption discriminativeness to assure the quality of generated\ncaptions. (2) The correspondence between generated captions and images are\nnaturally incorporated in the generation process without human annotations, and\nhence our approach could utilize a large amount of unlabeled images to boost\ncaptioning performance with no additional laborious annotations. We demonstrate\nthe effectiveness of the proposed retrieval-guided method on COCO and Flickr30k\ncaptioning datasets, and show its superior captioning performance with more\ndiscriminative captions.\n", "versions": [{"version": "v1", "created": "Thu, 22 Mar 2018 11:52:10 GMT"}, {"version": "v2", "created": "Wed, 11 Jul 2018 09:30:32 GMT"}, {"version": "v3", "created": "Mon, 23 Jul 2018 01:35:21 GMT"}], "update_date": "2018-07-24", "authors_parsed": [["Liu", "Xihui", ""], ["Li", "Hongsheng", ""], ["Shao", "Jing", ""], ["Chen", "Dapeng", ""], ["Wang", "Xiaogang", ""]]}, {"id": "1803.08319", "submitter": "Matteo Fabbri Ing.", "authors": "Matteo Fabbri, Fabio Lanzi, Simone Calderara, Andrea Palazzi, Roberto\n  Vezzani, Rita Cucchiara", "title": "Learning to Detect and Track Visible and Occluded Body Joints in a\n  Virtual World", "comments": "Accepted at ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-People Tracking in an open-world setting requires a special effort in\nprecise detection. Moreover, temporal continuity in the detection phase gains\nmore importance when scene cluttering introduces the challenging problems of\noccluded targets. For the purpose, we propose a deep network architecture that\njointly extracts people body parts and associates them across short temporal\nspans. Our model explicitly deals with occluded body parts, by hallucinating\nplausible solutions of not visible joints. We propose a new end-to-end\narchitecture composed by four branches (visible heatmaps, occluded heatmaps,\npart affinity fields and temporal affinity fields) fed by a time linker feature\nextractor. To overcome the lack of surveillance data with tracking, body part\nand occlusion annotations we created the vastest Computer Graphics dataset for\npeople tracking in urban scenarios by exploiting a photorealistic videogame. It\nis up to now the vastest dataset (about 500.000 frames, almost 10 million body\nposes) of human body parts for people tracking in urban scenarios. Our\narchitecture trained on virtual data exhibits good generalization capabilities\nalso on public real tracking benchmarks, when image resolution and sharpness\nare high enough, producing reliable tracklets useful for further batch data\nassociation or re-id modules.\n", "versions": [{"version": "v1", "created": "Thu, 22 Mar 2018 12:03:19 GMT"}, {"version": "v2", "created": "Fri, 23 Mar 2018 10:37:34 GMT"}, {"version": "v3", "created": "Tue, 18 Sep 2018 14:34:14 GMT"}], "update_date": "2018-09-19", "authors_parsed": [["Fabbri", "Matteo", ""], ["Lanzi", "Fabio", ""], ["Calderara", "Simone", ""], ["Palazzi", "Andrea", ""], ["Vezzani", "Roberto", ""], ["Cucchiara", "Rita", ""]]}, {"id": "1803.08323", "submitter": "Christian Mostegel", "authors": "Christian Mostegel and Friedrich Fraundorfer and Horst Bischof", "title": "Prioritized Multi-View Stereo Depth Map Generation Using Confidence\n  Prediction", "comments": "This paper was accepted to ISPRS Journal of Photogrammetry and Remote\n  Sensing\n  (https://www.journals.elsevier.com/isprs-journal-of-photogrammetry-and-remote-sensing)\n  on March 21, 2018. The official version will be made available on\n  ScienceDirect (https://www.sciencedirect.com)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a novel approach to prioritize the depth map\ncomputation of multi-view stereo (MVS) to obtain compact 3D point clouds of\nhigh quality and completeness at low computational cost. Our prioritization\napproach operates before the MVS algorithm is executed and consists of two\nsteps. In the first step, we aim to find a good set of matching partners for\neach view. In the second step, we rank the resulting view clusters (i.e. key\nviews with matching partners) according to their impact on the fulfillment of\ndesired quality parameters such as completeness, ground resolution and\naccuracy. Additional to geometric analysis, we use a novel machine learning\ntechnique for training a confidence predictor. The purpose of this confidence\npredictor is to estimate the chances of a successful depth reconstruction for\neach pixel in each image for one specific MVS algorithm based on the RGB images\nand the image constellation. The underlying machine learning technique does not\nrequire any ground truth or manually labeled data for training, but instead\nadapts ideas from depth map fusion for providing a supervision signal. The\ntrained confidence predictor allows us to evaluate the quality of image\nconstellations and their potential impact to the resulting 3D reconstruction\nand thus builds a solid foundation for our prioritization approach. In our\nexperiments, we are thus able to reach more than 70% of the maximal reachable\nquality fulfillment using only 5% of the available images as key views. For\nevaluating our approach within and across different domains, we use two\ncompletely different scenarios, i.e. cultural heritage preservation and\nreconstruction of single family houses.\n", "versions": [{"version": "v1", "created": "Thu, 22 Mar 2018 12:21:21 GMT"}], "update_date": "2018-03-23", "authors_parsed": [["Mostegel", "Christian", ""], ["Fraundorfer", "Friedrich", ""], ["Bischof", "Horst", ""]]}, {"id": "1803.08326", "submitter": "Yanlin Qian", "authors": "Yanlin Qian, Said Pertuz, Jarno Nikkanen, Joni-Kristian\n  K\\\"am\\\"ar\\\"ainen, Jiri Matas", "title": "Revisiting Gray Pixel for Statistical Illumination Estimation", "comments": "updated and will appear in VISSAP 2019 (long paper)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a statistical color constancy method that relies on novel gray\npixel detection and mean shift clustering. The method, called Mean Shifted Grey\nPixel -- MSGP, is based on the observation: true-gray pixels are aligned\ntowards one single direction. Our solution is compact, easy to compute and\nrequires no training. Experiments on two real-world benchmarks show that the\nproposed approach outperforms state-of-the-art methods in the camera-agnostic\nscenario. In the setting where the camera is known, MSGP outperforms all\nstatistical methods.\n", "versions": [{"version": "v1", "created": "Thu, 22 Mar 2018 12:45:36 GMT"}, {"version": "v2", "created": "Thu, 5 Jul 2018 16:56:46 GMT"}, {"version": "v3", "created": "Fri, 7 Dec 2018 14:40:11 GMT"}, {"version": "v4", "created": "Wed, 9 Jan 2019 14:19:24 GMT"}], "update_date": "2019-01-10", "authors_parsed": [["Qian", "Yanlin", ""], ["Pertuz", "Said", ""], ["Nikkanen", "Jarno", ""], ["K\u00e4m\u00e4r\u00e4inen", "Joni-Kristian", ""], ["Matas", "Jiri", ""]]}, {"id": "1803.08337", "submitter": "Joachim Folz", "authors": "Sebastian Palacio and Joachim Folz and J\\\"orn Hees and Federico Raue\n  and Damian Borth and Andreas Dengel", "title": "What do Deep Networks Like to See?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel way to measure and understand convolutional neural\nnetworks by quantifying the amount of input signal they let in. To do this, an\nautoencoder (AE) was fine-tuned on gradients from a pre-trained classifier with\nfixed parameters. We compared the reconstructed samples from AEs that were\nfine-tuned on a set of image classifiers (AlexNet, VGG16, ResNet-50, and\nInception~v3) and found substantial differences. The AE learns which aspects of\nthe input space to preserve and which ones to ignore, based on the information\nencoded in the backpropagated gradients. Measuring the changes in accuracy when\nthe signal of one classifier is used by a second one, a relation of total order\nemerges. This order depends directly on each classifier's input signal but it\ndoes not correlate with classification accuracy or network size. Further\nevidence of this phenomenon is provided by measuring the normalized mutual\ninformation between original images and auto-encoded reconstructions from\ndifferent fine-tuned AEs. These findings break new ground in the area of neural\nnetwork understanding, opening a new way to reason, debug, and interpret their\nresults. We present four concrete examples in the literature where observations\ncan now be explained in terms of the input signal that a model uses.\n", "versions": [{"version": "v1", "created": "Thu, 22 Mar 2018 13:10:47 GMT"}], "update_date": "2018-03-23", "authors_parsed": [["Palacio", "Sebastian", ""], ["Folz", "Joachim", ""], ["Hees", "J\u00f6rn", ""], ["Raue", "Federico", ""], ["Borth", "Damian", ""], ["Dengel", "Andreas", ""]]}, {"id": "1803.08375", "submitter": "Abien Fred Agarap", "authors": "Abien Fred Agarap", "title": "Deep Learning using Rectified Linear Units (ReLU)", "comments": "7 pages, 11 figures, 9 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We introduce the use of rectified linear units (ReLU) as the classification\nfunction in a deep neural network (DNN). Conventionally, ReLU is used as an\nactivation function in DNNs, with Softmax function as their classification\nfunction. However, there have been several studies on using a classification\nfunction other than Softmax, and this study is an addition to those. We\naccomplish this by taking the activation of the penultimate layer $h_{n - 1}$\nin a neural network, then multiply it by weight parameters $\\theta$ to get the\nraw scores $o_{i}$. Afterwards, we threshold the raw scores $o_{i}$ by $0$,\ni.e. $f(o) = \\max(0, o_{i})$, where $f(o)$ is the ReLU function. We provide\nclass predictions $\\hat{y}$ through argmax function, i.e. argmax $f(x)$.\n", "versions": [{"version": "v1", "created": "Thu, 22 Mar 2018 14:30:17 GMT"}, {"version": "v2", "created": "Thu, 7 Feb 2019 06:13:13 GMT"}], "update_date": "2019-02-08", "authors_parsed": [["Agarap", "Abien Fred", ""]]}, {"id": "1803.08394", "submitter": "Andrey Kuehlkamp", "authors": "Andrey Kuehlkamp and Kevin Bowyer", "title": "Found a good match: should I keep searching? - Accuracy and Performance\n  in Iris Matching Using 1-to-First Search", "comments": null, "journal-ref": "Image and Vision Computing vol 73, May 2018, pp. 17-27", "doi": "10.1016/j.imavis.2018.03.003", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Iris recognition is used in many applications around the world, with\nenrollment sizes as large as over one billion persons in India's Aadhaar\nprogram. Large enrollment sizes can require special optimizations in order to\nachieve fast database searches. One such optimization that has been used in\nsome operational scenarios is 1:First search. In this approach, instead of\nscanning the entire database, the search is terminated when the first\nsufficiently good match is found. This saves time, but ignores potentially\nbetter matches that may exist in the unexamined portion of the enrollments. At\nleast one prominent and successful border-crossing program used this approach\nfor nearly a decade, in order to allow users a fast \"token-free\" search. Our\nwork investigates the search accuracy of 1:First and compares it to the\ntraditional 1:N search. Several different scenarios are considered trying to\nemulate real environments as best as possible: a range of enrollment sizes,\nclosed- and open-set configurations, two iris matchers, and different\npermutations of the galleries. Results confirm the expected accuracy\ndegradation using 1:First search, and also allow us to identify acceptable\nworking parameters where significant search time reduction is achieved, while\nmaintaining accuracy similar to 1:N search.\n", "versions": [{"version": "v1", "created": "Thu, 22 Mar 2018 15:07:53 GMT"}], "update_date": "2018-04-20", "authors_parsed": [["Kuehlkamp", "Andrey", ""], ["Bowyer", "Kevin", ""]]}, {"id": "1803.08396", "submitter": "He Zhang", "authors": "He Zhang and Vishal M. Patel", "title": "Densely Connected Pyramid Dehazing Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new end-to-end single image dehazing method, called Densely\nConnected Pyramid Dehazing Network (DCPDN), which can jointly learn the\ntransmission map, atmospheric light and dehazing all together. The end-to-end\nlearning is achieved by directly embedding the atmospheric scattering model\ninto the network, thereby ensuring that the proposed method strictly follows\nthe physics-driven scattering model for dehazing. Inspired by the dense network\nthat can maximize the information flow along features from different levels, we\npropose a new edge-preserving densely connected encoder-decoder structure with\nmulti-level pyramid pooling module for estimating the transmission map. This\nnetwork is optimized using a newly introduced edge-preserving loss function. To\nfurther incorporate the mutual structural information between the estimated\ntransmission map and the dehazed result, we propose a joint-discriminator based\non generative adversarial network framework to decide whether the corresponding\ndehazed image and the estimated transmission map are real or fake. An ablation\nstudy is conducted to demonstrate the effectiveness of each module evaluated at\nboth estimated transmission map and dehazed result. Extensive experiments\ndemonstrate that the proposed method achieves significant improvements over the\nstate-of-the-art methods. Code will be made available at:\nhttps://github.com/hezhangsprinter\n", "versions": [{"version": "v1", "created": "Thu, 22 Mar 2018 15:09:53 GMT"}], "update_date": "2018-03-23", "authors_parsed": [["Zhang", "He", ""], ["Patel", "Vishal M.", ""]]}, {"id": "1803.08407", "submitter": "Kai Xu", "authors": "Yifei Shi, Kai Xu, Matthias Niessner, Szymon Rusinkiewicz, Thomas\n  Funkhouser", "title": "PlaneMatch: Patch Coplanarity Prediction for Robust RGB-D Reconstruction", "comments": "ECCV 2018 oral paper; Supplemental material included", "journal-ref": "ECCV 2018", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel RGB-D patch descriptor designed for detecting coplanar\nsurfaces in SLAM reconstruction. The core of our method is a deep convolutional\nneural net that takes in RGB, depth, and normal information of a planar patch\nin an image and outputs a descriptor that can be used to find coplanar patches\nfrom other images.We train the network on 10 million triplets of coplanar and\nnon-coplanar patches, and evaluate on a new coplanarity benchmark created from\ncommodity RGB-D scans. Experiments show that our learned descriptor outperforms\nalternatives extended for this new task by a significant margin. In addition,\nwe demonstrate the benefits of coplanarity matching in a robust RGBD\nreconstruction formulation.We find that coplanarity constraints detected with\nour method are sufficient to get reconstruction results comparable to\nstate-of-the-art frameworks on most scenes, but outperform other methods on\nstandard benchmarks when combined with a simple keypoint method.\n", "versions": [{"version": "v1", "created": "Thu, 22 Mar 2018 15:29:21 GMT"}, {"version": "v2", "created": "Sat, 24 Mar 2018 14:35:40 GMT"}, {"version": "v3", "created": "Fri, 27 Jul 2018 04:43:15 GMT"}], "update_date": "2018-07-30", "authors_parsed": [["Shi", "Yifei", ""], ["Xu", "Kai", ""], ["Niessner", "Matthias", ""], ["Rusinkiewicz", "Szymon", ""], ["Funkhouser", "Thomas", ""]]}, {"id": "1803.08410", "submitter": "Congcong Wang", "authors": "Congcong Wang, Faouzi Alaya Cheikh, Mounir Kaaniche, Ole Jacob Elle", "title": "A Smoke Removal Method for Laparoscopic Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In laparoscopic surgery, image quality can be severely degraded by surgical\nsmoke, which not only introduces error for the image processing (used in image\nguided surgery), but also reduces the visibility of the surgeons. In this\npaper, we propose to enhance the laparoscopic images by decomposing them into\nunwanted smoke part and enhanced part using a variational approach. The\nproposed method relies on the observation that smoke has low contrast and low\ninter-channel differences. A cost function is defined based on this prior\nknowledge and is solved using an augmented Lagrangian method. The obtained\nunwanted smoke component is then subtracted from the original degraded image,\nresulting in the enhanced image. The obtained quantitative scores in terms of\nFADE, JNBM and RE metrics show that our proposed method performs rather well.\nFurthermore, the qualitative visual inspection of the results show that it\nremoves smoke effectively from the laparoscopic images.\n", "versions": [{"version": "v1", "created": "Thu, 22 Mar 2018 15:34:40 GMT"}], "update_date": "2018-03-23", "authors_parsed": [["Wang", "Congcong", ""], ["Cheikh", "Faouzi Alaya", ""], ["Kaaniche", "Mounir", ""], ["Elle", "Ole Jacob", ""]]}, {"id": "1803.08412", "submitter": "Zhiyuan Zha", "authors": "Zhiyuan Zha, Xinggan Zhang, Qiong Wang, Yechao Bai, Lan Tang and Xin\n  Yuan", "title": "Group Sparsity Residual with Non-Local Samples for Image Denoising", "comments": null, "journal-ref": "International Conference on Acoustics, Speech and Signal\n  Processing 2018", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Inspired by group-based sparse coding, recently proposed group sparsity\nresidual (GSR) scheme has demonstrated superior performance in image\nprocessing. However, one challenge in GSR is to estimate the residual by using\na proper reference of the group-based sparse coding (GSC), which is desired to\nbe as close to the truth as possible. Previous researches utilized the\nestimations from other algorithms (i.e., GMM or BM3D), which are either not\naccurate or too slow. In this paper, we propose to use the Non-Local Samples\n(NLS) as reference in the GSR regime for image denoising, thus termed GSR-NLS.\nMore specifically, we first obtain a good estimation of the group sparse\ncoefficients by the image nonlocal self-similarity, and then solve the GSR\nmodel by an effective iterative shrinkage algorithm. Experimental results\ndemonstrate that the proposed GSR-NLS not only outperforms many\nstate-of-the-art methods, but also delivers the competitive advantage of speed.\n", "versions": [{"version": "v1", "created": "Thu, 22 Mar 2018 15:37:17 GMT"}], "update_date": "2018-03-23", "authors_parsed": [["Zha", "Zhiyuan", ""], ["Zhang", "Xinggan", ""], ["Wang", "Qiong", ""], ["Bai", "Yechao", ""], ["Tang", "Lan", ""], ["Yuan", "Xin", ""]]}, {"id": "1803.08414", "submitter": "Minh-Tan Pham", "authors": "Minh-Tan Pham and S\\'ebastien Lef\\`evre", "title": "Buried object detection from B-scan ground penetrating radar data using\n  Faster-RCNN", "comments": "4 pages, to appear in IGARSS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we adapt the Faster-RCNN framework for the detection of\nunderground buried objects (i.e. hyperbola reflections) in B-scan ground\npenetrating radar (GPR) images. Due to the lack of real data for training, we\npropose to incorporate more simulated radargrams generated from different\nconfigurations using the gprMax toolbox. Our designed CNN is first pre-trained\non the grayscale Cifar-10 database. Then, the Faster-RCNN framework based on\nthe pre-trained CNN is trained and fine-tuned on both real and simulated GPR\ndata. Preliminary detection results show that the proposed technique can\nprovide significant improvements compared to classical computer vision methods\nand hence becomes quite promising to deal with this kind of specific GPR data\neven with few training samples.\n", "versions": [{"version": "v1", "created": "Thu, 22 Mar 2018 15:41:43 GMT"}], "update_date": "2018-03-23", "authors_parsed": [["Pham", "Minh-Tan", ""], ["Lef\u00e8vre", "S\u00e9bastien", ""]]}, {"id": "1803.08420", "submitter": "Jose Cambronero Sanchez", "authors": "Jose Cambronero and Phillip Stanley-Marbell and Martin Rinard", "title": "Incremental Color Quantization for Color-Vision-Deficient Observers\n  Using Mobile Gaming Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The sizes of compressed images depend on their spatial resolution (number of\npixels) and on their color resolution (number of color quantization levels). We\nintroduce DaltonQuant, a new color quantization technique for image compression\nthat cloud services can apply to images destined for a specific user with known\ncolor vision deficiencies. DaltonQuant improves compression in a user-specific\nbut reversible manner thereby improving a user's network bandwidth and data\nstorage efficiency. DaltonQuant quantizes image data to account for\nuser-specific color perception anomalies, using a new method for incremental\ncolor quantization based on a large corpus of color vision acuity data obtained\nfrom a popular mobile game. Servers that host images can revert DaltonQuant's\nimage requantization and compression when those images must be transmitted to a\ndifferent user, making the technique practical to deploy on a large scale. We\nevaluate DaltonQuant's compression performance on the Kodak PC reference image\nset and show that it improves compression by an additional 22%-29% over the\nstate-of-the-art compressors TinyPNG and pngquant.\n", "versions": [{"version": "v1", "created": "Thu, 22 Mar 2018 15:54:43 GMT"}], "update_date": "2018-03-23", "authors_parsed": [["Cambronero", "Jose", ""], ["Stanley-Marbell", "Phillip", ""], ["Rinard", "Martin", ""]]}, {"id": "1803.08435", "submitter": "Yinan Zhao", "authors": "Yinan Zhao, Brian Price, Scott Cohen, Danna Gurari", "title": "Guided Image Inpainting: Replacing an Image Region by Pulling Content\n  from Another Image", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep generative models have shown success in automatically synthesizing\nmissing image regions using surrounding context. However, users cannot directly\ndecide what content to synthesize with such approaches. We propose an\nend-to-end network for image inpainting that uses a different image to guide\nthe synthesis of new content to fill the hole. A key challenge addressed by our\napproach is synthesizing new content in regions where the guidance image and\nthe context of the original image are inconsistent. We conduct four studies\nthat demonstrate our results yield more realistic image inpainting results over\nseven baselines.\n", "versions": [{"version": "v1", "created": "Thu, 22 Mar 2018 16:20:45 GMT"}], "update_date": "2018-03-23", "authors_parsed": [["Zhao", "Yinan", ""], ["Price", "Brian", ""], ["Cohen", "Scott", ""], ["Gurari", "Danna", ""]]}, {"id": "1803.08450", "submitter": "St\\'ephane Lathuili\\`ere", "authors": "St\\'ephane Lathuili\\`ere, Pablo Mesejo, Xavier Alameda-Pineda, Radu\n  Horaud", "title": "A Comprehensive Analysis of Deep Regression", "comments": "Published in IEEE TPAMI", "journal-ref": "IEEE TPAMI Volume: 42 , Issue: 9 , Sept. 1 2020", "doi": "10.1109/TPAMI.2019.2910523", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning revolutionized data science, and recently its popularity has\ngrown exponentially, as did the amount of papers employing deep networks.\nVision tasks, such as human pose estimation, did not escape from this trend.\nThere is a large number of deep models, where small changes in the network\narchitecture, or in the data pre-processing, together with the stochastic\nnature of the optimization procedures, produce notably different results,\nmaking extremely difficult to sift methods that significantly outperform\nothers. This situation motivates the current study, in which we perform a\nsystematic evaluation and statistical analysis of vanilla deep regression, i.e.\nconvolutional neural networks with a linear regression top layer. This is the\nfirst comprehensive analysis of deep regression techniques. We perform\nexperiments on four vision problems, and report confidence intervals for the\nmedian performance as well as the statistical significance of the results, if\nany. Surprisingly, the variability due to different data pre-processing\nprocedures generally eclipses the variability due to modifications in the\nnetwork architecture. Our results reinforce the hypothesis according to which,\nin general, a general-purpose network (e.g. VGG-16 or ResNet-50) adequately\ntuned can yield results close to the state-of-the-art without having to resort\nto more complex and ad-hoc regression models.\n", "versions": [{"version": "v1", "created": "Thu, 22 Mar 2018 16:46:39 GMT"}, {"version": "v2", "created": "Wed, 13 Feb 2019 13:40:05 GMT"}, {"version": "v3", "created": "Thu, 24 Sep 2020 15:10:03 GMT"}], "update_date": "2020-09-25", "authors_parsed": [["Lathuili\u00e8re", "St\u00e9phane", ""], ["Mesejo", "Pablo", ""], ["Alameda-Pineda", "Xavier", ""], ["Horaud", "Radu", ""]]}, {"id": "1803.08457", "submitter": "Sharon Fogel", "authors": "Sharon Fogel, Hadar Averbuch-Elor, Jacov Goldberger and Daniel\n  Cohen-Or", "title": "Clustering-driven Deep Embedding with Pairwise Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, there has been increasing interest to leverage the competence of\nneural networks to analyze data. In particular, new clustering methods that\nemploy deep embeddings have been presented. In this paper, we depart from\ncentroid-based models and suggest a new framework, called Clustering-driven\ndeep embedding with PAirwise Constraints (CPAC), for non-parametric clustering\nusing a neural network. We present a clustering-driven embedding based on a\nSiamese network that encourages pairs of data points to output similar\nrepresentations in the latent space. Our pair-based model allows augmenting the\ninformation with labeled pairs to constitute a semi-supervised framework. Our\napproach is based on analyzing the losses associated with each pair to refine\nthe set of constraints. We show that clustering performance increases when\nusing this scheme, even with a limited amount of user queries. We demonstrate\nhow our architecture is adapted for various types of data and present the first\ndeep framework to cluster 3D shapes.\n", "versions": [{"version": "v1", "created": "Thu, 22 Mar 2018 16:58:06 GMT"}, {"version": "v2", "created": "Wed, 25 Apr 2018 19:01:51 GMT"}, {"version": "v3", "created": "Sun, 3 Jun 2018 06:31:53 GMT"}, {"version": "v4", "created": "Mon, 16 Jul 2018 14:13:30 GMT"}, {"version": "v5", "created": "Fri, 19 Oct 2018 15:29:28 GMT"}], "update_date": "2018-10-22", "authors_parsed": [["Fogel", "Sharon", ""], ["Averbuch-Elor", "Hadar", ""], ["Goldberger", "Jacov", ""], ["Cohen-Or", "Daniel", ""]]}, {"id": "1803.08460", "submitter": "Yi Zhu", "authors": "Yi Zhu, Yang Long, Yu Guan, Shawn Newsam, Ling Shao", "title": "Towards Universal Representation for Unseen Action Recognition", "comments": "Accepted at CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unseen Action Recognition (UAR) aims to recognise novel action categories\nwithout training examples. While previous methods focus on inner-dataset\nseen/unseen splits, this paper proposes a pipeline using a large-scale training\nsource to achieve a Universal Representation (UR) that can generalise to a more\nrealistic Cross-Dataset UAR (CD-UAR) scenario. We first address UAR as a\nGeneralised Multiple-Instance Learning (GMIL) problem and discover\n'building-blocks' from the large-scale ActivityNet dataset using distribution\nkernels. Essential visual and semantic components are preserved in a shared\nspace to achieve the UR that can efficiently generalise to new datasets.\nPredicted UR exemplars can be improved by a simple semantic adaptation, and\nthen an unseen action can be directly recognised using UR during the test.\nWithout further training, extensive experiments manifest significant\nimprovements over the UCF101 and HMDB51 benchmarks.\n", "versions": [{"version": "v1", "created": "Thu, 22 Mar 2018 17:02:45 GMT"}], "update_date": "2018-03-23", "authors_parsed": [["Zhu", "Yi", ""], ["Long", "Yang", ""], ["Guan", "Yu", ""], ["Newsam", "Shawn", ""], ["Shao", "Ling", ""]]}, {"id": "1803.08467", "submitter": "Zili Yi", "authors": "Zili Yi, Zhiqin Chen, Hao Cai, Wendong Mao, Minglun Gong, Hao Zhang", "title": "BSD-GAN: Branched Generative Adversarial Network for Scale-Disentangled\n  Representation Learning and Image Synthesis", "comments": "12 pages, 20 figures, accepted to IEEE Transaction on Image\n  Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce BSD-GAN, a novel multi-branch and scale-disentangled training\nmethod which enables unconditional Generative Adversarial Networks (GANs) to\nlearn image representations at multiple scales, benefiting a wide range of\ngeneration and editing tasks. The key feature of BSD-GAN is that it is trained\nin multiple branches, progressively covering both the breadth and depth of the\nnetwork, as resolutions of the training images increase to reveal finer-scale\nfeatures. Specifically, each noise vector, as input to the generator network of\nBSD-GAN, is deliberately split into several sub-vectors, each corresponding to,\nand is trained to learn, image representations at a particular scale. During\ntraining, we progressively \"de-freeze\" the sub-vectors, one at a time, as a new\nset of higher-resolution images is employed for training and more network\nlayers are added. A consequence of such an explicit sub-vector designation is\nthat we can directly manipulate and even combine latent (sub-vector) codes\nwhich model different feature scales.Extensive experiments demonstrate the\neffectiveness of our training method in scale-disentangled learning of image\nrepresentations and synthesis of novel image contents, without any extra labels\nand without compromising quality of the synthesized high-resolution images. We\nfurther demonstrate several image generation and manipulation applications\nenabled or improved by BSD-GAN. Source codes are available at\nhttps://github.com/duxingren14/BSD-GAN.\n", "versions": [{"version": "v1", "created": "Thu, 22 Mar 2018 17:07:32 GMT"}, {"version": "v2", "created": "Wed, 28 Nov 2018 20:14:26 GMT"}, {"version": "v3", "created": "Tue, 21 Jul 2020 01:31:51 GMT"}, {"version": "v4", "created": "Sat, 25 Jul 2020 07:04:23 GMT"}, {"version": "v5", "created": "Tue, 4 Aug 2020 02:17:13 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Yi", "Zili", ""], ["Chen", "Zhiqin", ""], ["Cai", "Hao", ""], ["Mao", "Wendong", ""], ["Gong", "Minglun", ""], ["Zhang", "Hao", ""]]}, {"id": "1803.08489", "submitter": "Hanhe Lin", "authors": "Hanhe Lin, Vlad Hosu and Dietmar Saupe", "title": "KonIQ-10k: Towards an ecologically valid and large-scale IQA database", "comments": "Image database, image quality assessment, diversity sampling,\n  crowdsourcing", "journal-ref": null, "doi": "10.1109/TIP.2020.2967829", "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main challenge in applying state-of-the-art deep learning methods to\npredict image quality in-the-wild is the relatively small size of existing\nquality scored datasets. The reason for the lack of larger datasets is the\nmassive resources required in generating diverse and publishable content. We\npresent a new systematic and scalable approach to create large-scale, authentic\nand diverse image datasets for Image Quality Assessment (IQA). We show how we\nbuilt an IQA database, KonIQ-10k, consisting of 10,073 images, on which we\nperformed very large scale crowdsourcing experiments in order to obtain\nreliable quality ratings from 1,467 crowd workers (1.2 million ratings). We\nargue for its ecological validity by analyzing the diversity of the dataset, by\ncomparing it to state-of-the-art IQA databases, and by checking the reliability\nof our user studies.\n", "versions": [{"version": "v1", "created": "Thu, 22 Mar 2018 17:50:05 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Lin", "Hanhe", ""], ["Hosu", "Vlad", ""], ["Saupe", "Dietmar", ""]]}, {"id": "1803.08494", "submitter": "Kaiming He", "authors": "Yuxin Wu, Kaiming He", "title": "Group Normalization", "comments": "v3: Update trained-from-scratch results in COCO to 41.0AP. Code and\n  models at\n  https://github.com/facebookresearch/Detectron/blob/master/projects/GN", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Batch Normalization (BN) is a milestone technique in the development of deep\nlearning, enabling various networks to train. However, normalizing along the\nbatch dimension introduces problems --- BN's error increases rapidly when the\nbatch size becomes smaller, caused by inaccurate batch statistics estimation.\nThis limits BN's usage for training larger models and transferring features to\ncomputer vision tasks including detection, segmentation, and video, which\nrequire small batches constrained by memory consumption. In this paper, we\npresent Group Normalization (GN) as a simple alternative to BN. GN divides the\nchannels into groups and computes within each group the mean and variance for\nnormalization. GN's computation is independent of batch sizes, and its accuracy\nis stable in a wide range of batch sizes. On ResNet-50 trained in ImageNet, GN\nhas 10.6% lower error than its BN counterpart when using a batch size of 2;\nwhen using typical batch sizes, GN is comparably good with BN and outperforms\nother normalization variants. Moreover, GN can be naturally transferred from\npre-training to fine-tuning. GN can outperform its BN-based counterparts for\nobject detection and segmentation in COCO, and for video classification in\nKinetics, showing that GN can effectively replace the powerful BN in a variety\nof tasks. GN can be easily implemented by a few lines of code in modern\nlibraries.\n", "versions": [{"version": "v1", "created": "Thu, 22 Mar 2018 17:57:16 GMT"}, {"version": "v2", "created": "Tue, 24 Apr 2018 18:12:19 GMT"}, {"version": "v3", "created": "Mon, 11 Jun 2018 22:48:02 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Wu", "Yuxin", ""], ["He", "Kaiming", ""]]}, {"id": "1803.08495", "submitter": "Kevin Chen", "authors": "Kevin Chen, Christopher B. Choy, Manolis Savva, Angel X. Chang, Thomas\n  Funkhouser, Silvio Savarese", "title": "Text2Shape: Generating Shapes from Natural Language by Learning Joint\n  Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for generating colored 3D shapes from natural language.\nTo this end, we first learn joint embeddings of freeform text descriptions and\ncolored 3D shapes. Our model combines and extends learning by association and\nmetric learning approaches to learn implicit cross-modal connections, and\nproduces a joint representation that captures the many-to-many relations\nbetween language and physical properties of 3D shapes such as color and shape.\nTo evaluate our approach, we collect a large dataset of natural language\ndescriptions for physical 3D objects in the ShapeNet dataset. With this learned\njoint embedding we demonstrate text-to-shape retrieval that outperforms\nbaseline approaches. Using our embeddings with a novel conditional Wasserstein\nGAN framework, we generate colored 3D shapes from text. Our method is the first\nto connect natural language text with realistic 3D objects exhibiting rich\nvariations in color, texture, and shape detail. See video at\nhttps://youtu.be/zraPvRdl13Q\n", "versions": [{"version": "v1", "created": "Thu, 22 Mar 2018 17:57:47 GMT"}], "update_date": "2018-03-23", "authors_parsed": [["Chen", "Kevin", ""], ["Choy", "Christopher B.", ""], ["Savva", "Manolis", ""], ["Chang", "Angel X.", ""], ["Funkhouser", "Thomas", ""], ["Savarese", "Silvio", ""]]}, {"id": "1803.08496", "submitter": "Khan W. Mahmud", "authors": "John K. Leffingwell, Donald J. Meagher, Khan W. Mahmud and Scott\n  Ackerson", "title": "Generalized Scene Reconstruction", "comments": "14 pages, 21 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new passive approach called Generalized Scene Reconstruction (GSR) enables\n\"generalized scenes\" to be effectively reconstructed. Generalized scenes are\ndefined to be \"boundless\" spaces that include non-Lambertian, partially\ntransmissive, textureless and finely-structured matter. A new data structure\ncalled a plenoptic octree is introduced to enable efficient (database-like)\nlight and matter field reconstruction in devices such as mobile phones,\naugmented reality (AR) glasses and drones. To satisfy threshold requirements\nfor GSR accuracy, scenes are represented as systems of partially polarized\nlight, radiometrically interacting with matter. To demonstrate GSR, a prototype\nimaging polarimeter is used to reconstruct (in generalized light fields) highly\nreflective, hail-damaged automobile body panels. Follow-on GSR experiments are\ndescribed.\n", "versions": [{"version": "v1", "created": "Thu, 22 Mar 2018 17:59:19 GMT"}, {"version": "v2", "created": "Fri, 23 Mar 2018 17:56:52 GMT"}, {"version": "v3", "created": "Thu, 24 May 2018 20:47:35 GMT"}], "update_date": "2018-05-28", "authors_parsed": [["Leffingwell", "John K.", ""], ["Meagher", "Donald J.", ""], ["Mahmud", "Khan W.", ""], ["Ackerson", "Scott", ""]]}, {"id": "1803.08542", "submitter": "Hunter Goforth", "authors": "Hunter Goforth, Simon Lucey", "title": "Aligning Across Large Gaps in Time", "comments": "15 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method of temporally-invariant image registration for outdoor\nscenes, with invariance across time of day, across seasonal variations, and\nacross decade-long periods, for low- and high-texture scenes. Our method can be\nuseful for applications in remote sensing, GPS-denied UAV localization, 3D\nreconstruction, and many others. Our method leverages a recently proposed\napproach to image registration, where fully-convolutional neural networks are\nused to create feature maps which can be registered using the\nInverse-Composition Lucas-Kanade algorithm (ICLK). We show that invariance that\nis learned from satellite imagery can be transferable to time-lapse data\ncaptured by webcams mounted on buildings near ground-level.\n", "versions": [{"version": "v1", "created": "Thu, 22 Mar 2018 18:48:56 GMT"}], "update_date": "2018-03-26", "authors_parsed": [["Goforth", "Hunter", ""], ["Lucey", "Simon", ""]]}, {"id": "1803.08544", "submitter": "Sumita Mishra", "authors": "Sachin Kumar, Sumita Mishra, Pallavi Asthana, Pragya", "title": "Automated Detection of Acute Leukemia using K-mean Clustering Algorithm", "comments": "Presented in ICCCCS 2016", "journal-ref": "Advances in Intelligent Systems and Computing, vol 554. Springer,\n  2018", "doi": "10.1007/978-981-10-3773-3_64", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Leukemia is a hematologic cancer which develops in blood tissue and triggers\nrapid production of immature and abnormal shaped white blood cells. Based on\nstatistics it is found that the leukemia is one of the leading causes of death\nin men and women alike. Microscopic examination of blood sample or bone marrow\nsmear is the most effective technique for diagnosis of leukemia. Pathologists\nanalyze microscopic samples to make diagnostic assessments on the basis of\ncharacteristic cell features. Recently, computerized methods for cancer\ndetection have been explored towards minimizing human intervention and\nproviding accurate clinical information. This paper presents an algorithm for\nautomated image based acute leukemia detection systems. The method implemented\nuses basic enhancement, morphology, filtering and segmenting technique to\nextract region of interest using k-means clustering algorithm. The proposed\nalgorithm achieved an accuracy of 92.8% and is tested with Nearest Neighbor\n(KNN) and Naive Bayes Classifier on the data-set of 60 samples.\n", "versions": [{"version": "v1", "created": "Tue, 6 Mar 2018 06:24:01 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Kumar", "Sachin", ""], ["Mishra", "Sumita", ""], ["Asthana", "Pallavi", ""], ["Pragya", "", ""]]}, {"id": "1803.08580", "submitter": "Heng Fan", "authors": "Zhigang Chang, Qin Zhou, Heng Fan, Hang Su, Hua Yang, Shibao Zheng,\n  Haibin Ling", "title": "Weighted Bilinear Coding over Salient Body Parts for Person\n  Re-identification", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks (CNNs) have demonstrated dominant\nperformance in person re-identification (Re-ID). Existing CNN based methods\nutilize global average pooling (GAP) to aggregate intermediate convolutional\nfeatures for Re-ID. However, this strategy only considers the first-order\nstatistics of local features and treats local features at different locations\nequally important, leading to sub-optimal feature representation. To deal with\nthese issues, we propose a novel weighted bilinear coding (WBC) framework for\nlocal feature aggregation in CNN networks to pursue more representative and\ndiscriminative feature representations, which can adapt to other\nstate-of-the-art methods and improve their performance. In specific, bilinear\ncoding is used to encode the channel-wise feature correlations to capture\nricher feature interactions. Meanwhile, a weighting scheme is applied on the\nbilinear coding to adaptively adjust the weights of local features at different\nlocations based on their importance in recognition, further improving the\ndiscriminability of feature aggregation. To handle the spatial misalignment\nissue, we use a salient part net (spatial attention module) to derive salient\nbody parts, and apply the WBC model on each part. The final representation,\nformed by concatenating the WBC encoded features of each part, is both\ndiscriminative and resistant to spatial misalignment. Experiments on three\nbenchmarks including Market-1501, DukeMTMC-reID and CUHK03 evidence the\nfavorable performance of our method against other outstanding methods.\n", "versions": [{"version": "v1", "created": "Thu, 22 Mar 2018 20:51:26 GMT"}, {"version": "v2", "created": "Mon, 30 Apr 2018 23:11:02 GMT"}, {"version": "v3", "created": "Wed, 8 Jan 2020 14:39:21 GMT"}], "update_date": "2020-01-09", "authors_parsed": [["Chang", "Zhigang", ""], ["Zhou", "Qin", ""], ["Fan", "Heng", ""], ["Su", "Hang", ""], ["Yang", "Hua", ""], ["Zheng", "Shibao", ""], ["Ling", "Haibin", ""]]}, {"id": "1803.08602", "submitter": "Pulak Purkait", "authors": "Pulak Purkait and Christopher Zach and Anders Eriksson", "title": "Maximum Consensus Parameter Estimation by Reweighted $\\ell_1$ Methods", "comments": "EMMCVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust parameter estimation in computer vision is frequently accomplished by\nsolving the maximum consensus (MaxCon) problem. Widely used randomized methods\nfor MaxCon, however, can only produce {random} approximate solutions, while\nglobal methods are too slow to exercise on realistic problem sizes. Here we\nanalyse MaxCon as iterative reweighted algorithms on the data residuals. We\npropose a smooth surrogate function, the minimization of which leads to an\nextremely simple iteratively reweighted algorithm for MaxCon. We show that our\nalgorithm is very efficient and in many cases, yields the global solution. This\nmakes it an attractive alternative for randomized methods and global\noptimizers. The convergence analysis of our method and its fundamental\ndifferences from the other iteratively reweighted methods are also presented.\n", "versions": [{"version": "v1", "created": "Thu, 22 Mar 2018 22:35:48 GMT"}], "update_date": "2018-03-26", "authors_parsed": [["Purkait", "Pulak", ""], ["Zach", "Christopher", ""], ["Eriksson", "Anders", ""]]}, {"id": "1803.08606", "submitter": "Mathias Unberath", "authors": "Mathias Unberath, Jan-Nico Zaech, Sing Chun Lee, Bastian Bier, Javad\n  Fotouhi, Mehran Armand, Nassir Navab", "title": "DeepDRR -- A Catalyst for Machine Learning in Fluoroscopy-guided\n  Procedures", "comments": "MU and JNZ have contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.med-ph cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning-based approaches outperform competing methods in most\ndisciplines relevant to diagnostic radiology. Interventional radiology,\nhowever, has not yet benefited substantially from the advent of deep learning,\nin particular because of two reasons: 1) Most images acquired during the\nprocedure are never archived and are thus not available for learning, and 2)\neven if they were available, annotations would be a severe challenge due to the\nvast amounts of data. When considering fluoroscopy-guided procedures, an\ninteresting alternative to true interventional fluoroscopy is in silico\nsimulation of the procedure from 3D diagnostic CT. In this case, labeling is\ncomparably easy and potentially readily available, yet, the appropriateness of\nresulting synthetic data is dependent on the forward model. In this work, we\npropose DeepDRR, a framework for fast and realistic simulation of fluoroscopy\nand digital radiography from CT scans, tightly integrated with the software\nplatforms native to deep learning. We use machine learning for material\ndecomposition and scatter estimation in 3D and 2D, respectively, combined with\nanalytic forward projection and noise injection to achieve the required\nperformance. On the example of anatomical landmark detection in X-ray images of\nthe pelvis, we demonstrate that machine learning models trained on DeepDRRs\ngeneralize to unseen clinically acquired data without the need for re-training\nor domain adaptation. Our results are promising and promote the establishment\nof machine learning in fluoroscopy-guided procedures.\n", "versions": [{"version": "v1", "created": "Thu, 22 Mar 2018 23:04:16 GMT"}], "update_date": "2018-03-26", "authors_parsed": [["Unberath", "Mathias", ""], ["Zaech", "Jan-Nico", ""], ["Lee", "Sing Chun", ""], ["Bier", "Bastian", ""], ["Fotouhi", "Javad", ""], ["Armand", "Mehran", ""], ["Navab", "Nassir", ""]]}, {"id": "1803.08607", "submitter": "Tao Sheng", "authors": "Tao Sheng, Chen Feng, Shaojie Zhuo, Xiaopeng Zhang, Liang Shen, Mickey\n  Aleksic", "title": "A Quantization-Friendly Separable Convolution for MobileNets", "comments": "Accepted At THE 1ST WORKSHOP ON ENERGY EFFICIENT MACHINE LEARNING AND\n  COGNITIVE COMPUTING FOR EMBEDDED APPLICATIONS (EMC^2 2018)", "journal-ref": "https://ieeexplore.ieee.org/document/8524017, 2018", "doi": "10.1109/EMC2.2018.00011", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As deep learning (DL) is being rapidly pushed to edge computing, researchers\ninvented various ways to make inference computation more efficient on\nmobile/IoT devices, such as network pruning, parameter compression, and etc.\nQuantization, as one of the key approaches, can effectively offload GPU, and\nmake it possible to deploy DL on fixed-point pipeline. Unfortunately, not all\nexisting networks design are friendly to quantization. For example, the popular\nlightweight MobileNetV1, while it successfully reduces parameter size and\ncomputation latency with separable convolution, our experiment shows its\nquantized models have large accuracy gap against its float point models. To\nresolve this, we analyzed the root cause of quantization loss and proposed a\nquantization-friendly separable convolution architecture. By evaluating the\nimage classification task on ImageNet2012 dataset, our modified MobileNetV1\nmodel can archive 8-bit inference top-1 accuracy in 68.03%, almost closed the\ngap to the float pipeline.\n", "versions": [{"version": "v1", "created": "Thu, 22 Mar 2018 23:06:38 GMT"}, {"version": "v2", "created": "Sat, 9 Mar 2019 07:32:09 GMT"}, {"version": "v3", "created": "Tue, 12 Mar 2019 17:58:19 GMT"}], "update_date": "2019-03-13", "authors_parsed": [["Sheng", "Tao", ""], ["Feng", "Chen", ""], ["Zhuo", "Shaojie", ""], ["Zhang", "Xiaopeng", ""], ["Shen", "Liang", ""], ["Aleksic", "Mickey", ""]]}, {"id": "1803.08608", "submitter": "Mathias Unberath", "authors": "Bastian Bier, Mathias Unberath, Jan-Nico Zaech, Javad Fotouhi, Mehran\n  Armand, Greg Osgood, Nassir Navab, Andreas Maier", "title": "X-ray-transform Invariant Anatomical Landmark Detection for Pelvic\n  Trauma Surgery", "comments": "BB and MU have contributed equally and are listed alphabetically", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  X-ray image guidance enables percutaneous alternatives to complex procedures.\nUnfortunately, the indirect view onto the anatomy in addition to projective\nsimplification substantially increase the task-load for the surgeon. Additional\n3D information such as knowledge of anatomical landmarks can benefit surgical\ndecision making in complicated scenarios. Automatic detection of these\nlandmarks in transmission imaging is challenging since image-domain features\ncharacteristic to a certain landmark change substantially depending on the\nviewing direction. Consequently and to the best of our knowledge, the above\nproblem has not yet been addressed. In this work, we present a method to\nautomatically detect anatomical landmarks in X-ray images independent of the\nviewing direction. To this end, a sequential prediction framework based on\nconvolutional layers is trained on synthetically generated data of the pelvic\nanatomy to predict 23 landmarks in single X-ray images. View independence is\ncontingent on training conditions and, here, is achieved on a spherical segment\ncovering (120 x 90) degrees in LAO/RAO and CRAN/CAUD, respectively, centered\naround AP. On synthetic data, the proposed approach achieves a mean prediction\nerror of 5.6 +- 4.5 mm. We demonstrate that the proposed network is immediately\napplicable to clinically acquired data of the pelvis. In particular, we show\nthat our intra-operative landmark detection together with pre-operative CT\nenables X-ray pose estimation which, ultimately, benefits initialization of\nimage-based 2D/3D registration.\n", "versions": [{"version": "v1", "created": "Thu, 22 Mar 2018 23:09:50 GMT"}], "update_date": "2018-03-26", "authors_parsed": [["Bier", "Bastian", ""], ["Unberath", "Mathias", ""], ["Zaech", "Jan-Nico", ""], ["Fotouhi", "Javad", ""], ["Armand", "Mehran", ""], ["Osgood", "Greg", ""], ["Navab", "Nassir", ""], ["Maier", "Andreas", ""]]}, {"id": "1803.08610", "submitter": "Mathias Unberath", "authors": "Jonas Hajek, Mathias Unberath, Javad Fotouhi, Bastian Bier, Sing Chun\n  Lee, Greg Osgood, Andreas Maier, Mehran Armand, Nassir Navab", "title": "Closing the Calibration Loop: An Inside-out-tracking Paradigm for\n  Augmented Reality in Orthopedic Surgery", "comments": "JH, MU, and JF have contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In percutaneous orthopedic interventions the surgeon attempts to reduce and\nfixate fractures in bony structures. The complexity of these interventions\narises when the surgeon performs the challenging task of navigating surgical\ntools percutaneously only under the guidance of 2D interventional X-ray\nimaging. Moreover, the intra-operatively acquired data is only visualized\nindirectly on external displays. In this work, we propose a flexible Augmented\nReality (AR) paradigm using optical see-through head mounted displays. The key\ntechnical contribution of this work includes the marker-less and dynamic\ntracking concept which closes the calibration loop between patient, C-arm and\nthe surgeon. This calibration is enabled using Simultaneous Localization and\nMapping of the environment of the operating theater. In return, the proposed\nsolution provides in situ visualization of pre- and intra-operative 3D medical\ndata directly at the surgical site. We demonstrate pre-clinical evaluation of a\nprototype system, and report errors for calibration and target registration.\nFinally, we demonstrate the usefulness of the proposed inside-out tracking\nsystem in achieving \"bull's eye\" view for C-arm-guided punctures. This AR\nsolution provides an intuitive visualization of the anatomy and can simplify\nthe hand-eye coordination for the orthopedic surgeon.\n", "versions": [{"version": "v1", "created": "Thu, 22 Mar 2018 23:15:59 GMT"}], "update_date": "2018-03-26", "authors_parsed": [["Hajek", "Jonas", ""], ["Unberath", "Mathias", ""], ["Fotouhi", "Javad", ""], ["Bier", "Bastian", ""], ["Lee", "Sing Chun", ""], ["Osgood", "Greg", ""], ["Maier", "Andreas", ""], ["Armand", "Mehran", ""], ["Navab", "Nassir", ""]]}, {"id": "1803.08624", "submitter": "G. Adam Cox", "authors": "G. A. Cox and S. Egly and G. R. Harp and J. Richards and S. Vinodababu\n  and J. Voien", "title": "Classification of simulated radio signals using Wide Residual Networks\n  for use in the search for extra-terrestrial intelligence", "comments": "16 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG astro-ph.IM cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We describe a new approach and algorithm for the detection of artificial\nsignals and their classification in the search for extraterrestrial\nintelligence (SETI). The characteristics of radio signals observed during SETI\nresearch are often most apparent when those signals are represented as\nspectrograms. Additionally, many observed signals tend to share the same\ncharacteristics, allowing for sorting of the signals into different classes.\nFor this work, complex-valued time-series data were simulated to produce a\ncorpus of 140,000 signals from seven different signal classes. A wide residual\nneural network was then trained to classify these signal types using the\ngray-scale 2D spectrogram representation of those signals. An average $F_1$\nscore of 95.11\\% was attained when tested on previously unobserved simulated\nsignals. We also report on the performance of the model across a range of\nsignal amplitudes.\n", "versions": [{"version": "v1", "created": "Fri, 23 Mar 2018 00:56:13 GMT"}], "update_date": "2018-04-16", "authors_parsed": [["Cox", "G. A.", ""], ["Egly", "S.", ""], ["Harp", "G. R.", ""], ["Richards", "J.", ""], ["Vinodababu", "S.", ""], ["Voien", "J.", ""]]}, {"id": "1803.08635", "submitter": "Samiran Ganguly", "authors": "Samiran Ganguly, Yunfei Gu, Mircea R. Stan, Avik W. Ghosh", "title": "Hardware based Spatio-Temporal Neural Processing Backend for Imaging\n  Sensors: Towards a Smart Camera", "comments": "11 pages, 5 figures. To be presented in SPIE DCS 2018: Image Sensing\n  Technologies: Materials, Devices, Systems, and Applications V", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.ET cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we show how we can build a technology platform for cognitive\nimaging sensors using recent advances in recurrent neural network architectures\nand training methods inspired from biology. We demonstrate learning and\nprocessing tasks specific to imaging sensors, including enhancement of\nsensitivity and signal-to-noise ratio (SNR) purely through neural filtering\nbeyond the fundamental limits sensor materials, and inferencing and\nspatio-temporal pattern recognition capabilities of these networks with\napplications in object detection, motion tracking and prediction. We then show\ndesigns of unit hardware cells built using complementary metal-oxide\nsemiconductor (CMOS) and emerging materials technologies for ultra-compact and\nenergy-efficient embedded neural processors for smart cameras.\n", "versions": [{"version": "v1", "created": "Fri, 23 Mar 2018 01:57:49 GMT"}], "update_date": "2018-03-26", "authors_parsed": [["Ganguly", "Samiran", ""], ["Gu", "Yunfei", ""], ["Stan", "Mircea R.", ""], ["Ghosh", "Avik W.", ""]]}, {"id": "1803.08636", "submitter": "Chunbiao Zhu", "authors": "Chunbiao Zhu, Xing Cai, Kan Huang, Thomas H Li, Ge Li", "title": "PDNet: Prior-model Guided Depth-enhanced Network for Salient Object\n  Detection", "comments": "This paper is under review. Project website:\n  https://github.com/ChunbiaoZhu/PDNet/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fully convolutional neural networks (FCNs) have shown outstanding performance\nin many computer vision tasks including salient object detection. However,\nthere still remains two issues needed to be addressed in deep learning based\nsaliency detection. One is the lack of tremendous amount of annotated data to\ntrain a network. The other is the lack of robustness for extracting salient\nobjects in images containing complex scenes. In this paper, we present a new\narchitecture$ - $PDNet, a robust prior-model guided depth-enhanced network for\nRGB-D salient object detection. In contrast to existing works, in which RGB-D\nvalues of image pixels are fed directly to a network, the proposed architecture\nis composed of a master network for processing RGB values, and a sub-network\nmaking full use of depth cues and incorporate depth-based features into the\nmaster network. To overcome the limited size of the labeled RGB-D dataset for\ntraining, we employ a large conventional RGB dataset to pre-train the master\nnetwork, which proves to contribute largely to the final accuracy. Extensive\nevaluations over five benchmark datasets demonstrate that our proposed method\nperforms favorably against the state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Fri, 23 Mar 2018 02:04:47 GMT"}, {"version": "v2", "created": "Sat, 13 Oct 2018 13:53:32 GMT"}], "update_date": "2018-10-16", "authors_parsed": [["Zhu", "Chunbiao", ""], ["Cai", "Xing", ""], ["Huang", "Kan", ""], ["Li", "Thomas H", ""], ["Li", "Ge", ""]]}, {"id": "1803.08647", "submitter": "Hao Ge", "authors": "Hao Ge, Yin Xia, Xu Chen, Randall Berry and Ying Wu", "title": "Fictitious GAN: Training GANs with Historical Models", "comments": "19 pages. First three authors have equal contributions", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative adversarial networks (GANs) are powerful tools for learning\ngenerative models. In practice, the training may suffer from lack of\nconvergence. GANs are commonly viewed as a two-player zero-sum game between two\nneural networks. Here, we leverage this game theoretic view to study the\nconvergence behavior of the training process. Inspired by the fictitious play\nlearning process, a novel training method, referred to as Fictitious GAN, is\nintroduced. Fictitious GAN trains the deep neural networks using a mixture of\nhistorical models. Specifically, the discriminator (resp. generator) is updated\naccording to the best-response to the mixture outputs from a sequence of\npreviously trained generators (resp. discriminators). It is shown that\nFictitious GAN can effectively resolve some convergence issues that cannot be\nresolved by the standard training approach. It is proved that asymptotically\nthe average of the generator outputs has the same distribution as the data\nsamples.\n", "versions": [{"version": "v1", "created": "Fri, 23 Mar 2018 03:46:12 GMT"}, {"version": "v2", "created": "Wed, 11 Jul 2018 18:50:03 GMT"}], "update_date": "2018-07-13", "authors_parsed": [["Ge", "Hao", ""], ["Xia", "Yin", ""], ["Chen", "Xu", ""], ["Berry", "Randall", ""], ["Wu", "Ying", ""]]}, {"id": "1803.08660", "submitter": "Michael Moeller", "authors": "Peter Ochs, Tim Meinhardt, Laura Leal-Taixe, Michael Moeller", "title": "Lifting Layers: Analysis and Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The great advances of learning-based approaches in image processing and\ncomputer vision are largely based on deeply nested networks that compose linear\ntransfer functions with suitable non-linearities. Interestingly, the most\nfrequently used non-linearities in imaging applications (variants of the\nrectified linear unit) are uncommon in low dimensional approximation problems.\nIn this paper we propose a novel non-linear transfer function, called lifting,\nwhich is motivated from a related technique in convex optimization. A lifting\nlayer increases the dimensionality of the input, naturally yields a linear\nspline when combined with a fully connected layer, and therefore closes the gap\nbetween low and high dimensional approximation problems. Moreover, applying the\nlifting operation to the loss layer of the network allows us to handle\nnon-convex and flat (zero-gradient) cost functions. We analyze the proposed\nlifting theoretically, exemplify interesting properties in synthetic\nexperiments and demonstrate its effectiveness in deep learning approaches to\nimage classification and denoising.\n", "versions": [{"version": "v1", "created": "Fri, 23 Mar 2018 05:47:50 GMT"}], "update_date": "2018-03-26", "authors_parsed": [["Ochs", "Peter", ""], ["Meinhardt", "Tim", ""], ["Leal-Taixe", "Laura", ""], ["Moeller", "Michael", ""]]}, {"id": "1803.08664", "submitter": "Namhyuk Ahn", "authors": "Namhyuk Ahn, Byungkon Kang, and Kyung-Ah Sohn", "title": "Fast, Accurate, and Lightweight Super-Resolution with Cascading Residual\n  Network", "comments": "European Conference on Computer Vision (ECCV), 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, deep learning methods have been successfully applied to\nsingle-image super-resolution tasks. Despite their great performances, deep\nlearning methods cannot be easily applied to real-world applications due to the\nrequirement of heavy computation. In this paper, we address this issue by\nproposing an accurate and lightweight deep network for image super-resolution.\nIn detail, we design an architecture that implements a cascading mechanism upon\na residual network. We also present variant models of the proposed cascading\nresidual network to further improve efficiency. Our extensive experiments show\nthat even with much fewer parameters and operations, our models achieve\nperformance comparable to that of state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 23 Mar 2018 06:07:20 GMT"}, {"version": "v2", "created": "Tue, 24 Jul 2018 13:35:08 GMT"}, {"version": "v3", "created": "Wed, 25 Jul 2018 12:51:49 GMT"}, {"version": "v4", "created": "Tue, 7 Aug 2018 02:53:22 GMT"}, {"version": "v5", "created": "Thu, 4 Oct 2018 21:47:19 GMT"}], "update_date": "2018-10-08", "authors_parsed": [["Ahn", "Namhyuk", ""], ["Kang", "Byungkon", ""], ["Sohn", "Kyung-Ah", ""]]}, {"id": "1803.08669", "submitter": "Jia-Ren Chang", "authors": "Jia-Ren Chang, Yong-Sheng Chen", "title": "Pyramid Stereo Matching Network", "comments": "CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work has shown that depth estimation from a stereo pair of images can\nbe formulated as a supervised learning task to be resolved with convolutional\nneural networks (CNNs). However, current architectures rely on patch-based\nSiamese networks, lacking the means to exploit context information for finding\ncorrespondence in illposed regions. To tackle this problem, we propose PSMNet,\na pyramid stereo matching network consisting of two main modules: spatial\npyramid pooling and 3D CNN. The spatial pyramid pooling module takes advantage\nof the capacity of global context information by aggregating context in\ndifferent scales and locations to form a cost volume. The 3D CNN learns to\nregularize cost volume using stacked multiple hourglass networks in conjunction\nwith intermediate supervision. The proposed approach was evaluated on several\nbenchmark datasets. Our method ranked first in the KITTI 2012 and 2015\nleaderboards before March 18, 2018. The codes of PSMNet are available at:\nhttps://github.com/JiaRenChang/PSMNet.\n", "versions": [{"version": "v1", "created": "Fri, 23 Mar 2018 06:40:09 GMT"}], "update_date": "2018-03-26", "authors_parsed": [["Chang", "Jia-Ren", ""], ["Chen", "Yong-Sheng", ""]]}, {"id": "1803.08670", "submitter": "Toru Ogawa", "authors": "Toru Ogawa, Atsushi Otsubo, Rei Narita, Yusuke Matsui, Toshihiko\n  Yamasaki, Kiyoharu Aizawa", "title": "Object Detection for Comics using Manga109 Annotations", "comments": "http://www.manga109.org/en/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the growth of digitized comics, image understanding techniques are\nbecoming important. In this paper, we focus on object detection, which is a\nfundamental task of image understanding. Although convolutional neural networks\n(CNN)-based methods archived good performance in object detection for\nnaturalistic images, there are two problems in applying these methods to the\ncomic object detection task. First, there is no large-scale annotated comics\ndataset. The CNN-based methods require large-scale annotations for training.\nSecondly, the objects in comics are highly overlapped compared to naturalistic\nimages. This overlap causes the assignment problem in the existing CNN-based\nmethods. To solve these problems, we proposed a new annotation dataset and a\nnew CNN model. We annotated an existing image dataset of comics and created the\nlargest annotation dataset, named Manga109-annotations. For the assignment\nproblem, we proposed a new CNN-based detector, SSD300-fork. We compared\nSSD300-fork with other detection methods using Manga109-annotations and\nconfirmed that our model outperformed them based on the mAP score.\n", "versions": [{"version": "v1", "created": "Fri, 23 Mar 2018 06:54:48 GMT"}, {"version": "v2", "created": "Mon, 26 Mar 2018 05:35:40 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Ogawa", "Toru", ""], ["Otsubo", "Atsushi", ""], ["Narita", "Rei", ""], ["Matsui", "Yusuke", ""], ["Yamasaki", "Toshihiko", ""], ["Aizawa", "Kiyoharu", ""]]}, {"id": "1803.08673", "submitter": "Junjie Hu", "authors": "Junjie Hu, Mete Ozay, Yan Zhang, Takayuki Okatani", "title": "Revisiting Single Image Depth Estimation: Toward Higher Resolution Maps\n  with Accurate Object Boundaries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the problem of single image depth estimation. The\nemployment of convolutional neural networks (CNNs) has recently brought about\nsignificant advancements in the research of this problem. However, most\nexisting methods suffer from loss of spatial resolution in the estimated depth\nmaps; a typical symptom is distorted and blurry reconstruction of object\nboundaries. In this paper, toward more accurate estimation with a focus on\ndepth maps with higher spatial resolution, we propose two improvements to\nexisting approaches. One is about the strategy of fusing features extracted at\ndifferent scales, for which we propose an improved network architecture\nconsisting of four modules: an encoder, decoder, multi-scale feature fusion\nmodule, and refinement module. The other is about loss functions for measuring\ninference errors used in training. We show that three loss terms, which measure\nerrors in depth, gradients and surface normals, respectively, contribute to\nimprovement of accuracy in an complementary fashion. Experimental results show\nthat these two improvements enable to attain higher accuracy than the current\nstate-of-the-arts, which is given by finer resolution reconstruction, for\nexample, with small objects and object boundaries.\n", "versions": [{"version": "v1", "created": "Fri, 23 Mar 2018 07:16:33 GMT"}, {"version": "v2", "created": "Sat, 22 Sep 2018 07:30:39 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Hu", "Junjie", ""], ["Ozay", "Mete", ""], ["Zhang", "Yan", ""], ["Okatani", "Takayuki", ""]]}, {"id": "1803.08679", "submitter": "Feng Li", "authors": "Feng Li, Cheng Tian, Wangmeng Zuo, Lei Zhang and Ming-Hsuan Yang", "title": "Learning Spatial-Temporal Regularized Correlation Filters for Visual\n  Tracking", "comments": "Accepted at CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discriminative Correlation Filters (DCF) are efficient in visual tracking but\nsuffer from unwanted boundary effects. Spatially Regularized DCF (SRDCF) has\nbeen suggested to resolve this issue by enforcing spatial penalty on DCF\ncoefficients, which, inevitably, improves the tracking performance at the price\nof increasing complexity. To tackle online updating, SRDCF formulates its model\non multiple training images, further adding difficulties in improving\nefficiency. In this work, by introducing temporal regularization to SRDCF with\nsingle sample, we present our spatial-temporal regularized correlation filters\n(STRCF). Motivated by online Passive-Agressive (PA) algorithm, we introduce the\ntemporal regularization to SRDCF with single sample, thus resulting in our\nspatial-temporal regularized correlation filters (STRCF). The STRCF formulation\ncan not only serve as a reasonable approximation to SRDCF with multiple\ntraining samples, but also provide a more robust appearance model than SRDCF in\nthe case of large appearance variations. Besides, it can be efficiently solved\nvia the alternating direction method of multipliers (ADMM). By incorporating\nboth temporal and spatial regularization, our STRCF can handle boundary effects\nwithout much loss in efficiency and achieve superior performance over SRDCF in\nterms of accuracy and speed. Experiments are conducted on three benchmark\ndatasets: OTB-2015, Temple-Color, and VOT-2016. Compared with SRDCF, STRCF with\nhand-crafted features provides a 5 times speedup and achieves a gain of 5.4%\nand 3.6% AUC score on OTB-2015 and Temple-Color, respectively. Moreover, STRCF\ncombined with CNN features also performs favorably against state-of-the-art\nCNN-based trackers and achieves an AUC score of 68.3% on OTB-2015.\n", "versions": [{"version": "v1", "created": "Fri, 23 Mar 2018 07:55:24 GMT"}], "update_date": "2018-03-26", "authors_parsed": [["Li", "Feng", ""], ["Tian", "Cheng", ""], ["Zuo", "Wangmeng", ""], ["Zhang", "Lei", ""], ["Yang", "Ming-Hsuan", ""]]}, {"id": "1803.08680", "submitter": "Daniel Jakubovitz", "authors": "Daniel Jakubovitz, Raja Giryes", "title": "Improving DNN Robustness to Adversarial Attacks using Jacobian\n  Regularization", "comments": "ECCV 2018 Conference Paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have lately shown tremendous performance in various\napplications including vision and speech processing tasks. However, alongside\ntheir ability to perform these tasks with such high accuracy, it has been shown\nthat they are highly susceptible to adversarial attacks: a small change in the\ninput would cause the network to err with high confidence. This phenomenon\nexposes an inherent fault in these networks and their ability to generalize\nwell. For this reason, providing robustness to adversarial attacks is an\nimportant challenge in networks training, which has led to extensive research.\nIn this work, we suggest a theoretically inspired novel approach to improve the\nnetworks' robustness. Our method applies regularization using the Frobenius\nnorm of the Jacobian of the network, which is applied as post-processing, after\nregular training has finished. We demonstrate empirically that it leads to\nenhanced robustness results with a minimal change in the original network's\naccuracy.\n", "versions": [{"version": "v1", "created": "Fri, 23 Mar 2018 07:57:04 GMT"}, {"version": "v2", "created": "Mon, 9 Jul 2018 16:02:08 GMT"}, {"version": "v3", "created": "Sun, 26 Aug 2018 16:43:36 GMT"}, {"version": "v4", "created": "Tue, 28 May 2019 09:48:05 GMT"}], "update_date": "2019-05-29", "authors_parsed": [["Jakubovitz", "Daniel", ""], ["Giryes", "Raja", ""]]}, {"id": "1803.08687", "submitter": "Zhenyu He", "authors": "Nana Fan, Zhenyu He", "title": "Region-filtering Correlation Tracking", "comments": "16 pages, 6 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, correlation filters have demonstrated the excellent performance in\nvisual tracking. However, the base training sample region is larger than the\nobject region,including the Interference Region(IR). The IRs in training\nsamples from cyclic shifts of the base training sample severely degrade the\nquality of a tracking model. In this paper, we propose the novel\nRegion-filtering Correlation Tracking (RFCT) to address this problem. We\nimmediately filter training samples by introducing a spatial map into the\nstandard CF formulation. Compared with existing correlation filter trackers,\nour proposed tracker has the following advantages: (1) The correlation filter\ncan be learned on a larger search region without the interference of the IR by\na spatial map. (2) Due to processing training samples by a spatial map, it is\nmore general way to control background information and target information in\ntraining samples. The values of the spatial map are not restricted, then a\nbetter spatial map can be explored. (3) The weight proportions of accurate\nfilters are increased to alleviate model corruption. Experiments are performed\non two benchmark datasets: OTB-2013 and OTB-2015. Quantitative evaluations on\nthese benchmarks demonstrate that the proposed RFCT algorithm performs\nfavorably against several state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 23 Mar 2018 08:37:43 GMT"}], "update_date": "2018-03-26", "authors_parsed": [["Fan", "Nana", ""], ["He", "Zhenyu", ""]]}, {"id": "1803.08691", "submitter": "Holger Roth", "authors": "Holger R. Roth, Chen Shen, Hirohisa Oda, Masahiro Oda, Yuichiro\n  Hayashi, Kazunari Misawa, Kensaku Mori", "title": "Deep learning and its application to medical image segmentation", "comments": "Accepted for publication in the journal of the Japanese Society of\n  Medical Imaging Technology (JAMIT)", "journal-ref": "Medical Imaging Technology, Volume 36 (2018), Issue 2, p. 63-71", "doi": "10.11409/mit.36.63", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the most common tasks in medical imaging is semantic segmentation.\nAchieving this segmentation automatically has been an active area of research,\nbut the task has been proven very challenging due to the large variation of\nanatomy across different patients. However, recent advances in deep learning\nhave made it possible to significantly improve the performance of image\nrecognition and semantic segmentation methods in the field of computer vision.\nDue to the data driven approaches of hierarchical feature learning in deep\nlearning frameworks, these advances can be translated to medical images without\nmuch difficulty. Several variations of deep convolutional neural networks have\nbeen successfully applied to medical images. Especially fully convolutional\narchitectures have been proven efficient for segmentation of 3D medical images.\nIn this article, we describe how to build a 3D fully convolutional network\n(FCN) that can process 3D images in order to produce automatic semantic\nsegmentations. The model is trained and evaluated on a clinical computed\ntomography (CT) dataset and shows state-of-the-art performance in multi-organ\nsegmentation.\n", "versions": [{"version": "v1", "created": "Fri, 23 Mar 2018 08:55:10 GMT"}], "update_date": "2018-04-10", "authors_parsed": [["Roth", "Holger R.", ""], ["Shen", "Chen", ""], ["Oda", "Hirohisa", ""], ["Oda", "Masahiro", ""], ["Hayashi", "Yuichiro", ""], ["Misawa", "Kazunari", ""], ["Mori", "Kensaku", ""]]}, {"id": "1803.08696", "submitter": "Saritha S", "authors": "S Saritha, G Santhosh Kumar", "title": "An Incremental Boolean Tensor Factorization approach to model Change\n  Patterns of Objects in Images", "comments": "This work is not submitted to any journals/conferences", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Change detection process has recently progressed from a post-classification\nmethod to an expert knowledge interpretation process of the time-series data.\nThe technique finds applications mainly in remote sensing images and can be\nutilized to analyze urbanization and monitor forest regions. In this paper, a\nframework to perform a knowledge based interpretation of the changes/no changes\nobserved in a spatiotemporal domain using tensor based approaches is presented.\nAn incremental approach to Boolean Tensor Factorization method is proposed in\nthis work, which is adopted to model the change patterns of objects/classes as\nwell as their associated features. The framework is evaluated under different\ndatasets to visualize the performance for the dependency factors. The algorithm\nis also validated in comparison with the tradition Boolean Tensor Factorization\nmethod and the results are substantial.\n", "versions": [{"version": "v1", "created": "Fri, 23 Mar 2018 09:08:35 GMT"}], "update_date": "2018-03-26", "authors_parsed": [["Saritha", "S", ""], ["Kumar", "G Santhosh", ""]]}, {"id": "1803.08707", "submitter": "Radu Tudor Ionescu", "authors": "Petru Soviany, Radu Tudor Ionescu", "title": "Optimizing the Trade-off between Single-Stage and Two-Stage Object\n  Detectors using Image Difficulty Prediction", "comments": "Accepted at SYNASC 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are mainly two types of state-of-the-art object detectors. On one hand,\nwe have two-stage detectors, such as Faster R-CNN (Region-based Convolutional\nNeural Networks) or Mask R-CNN, that (i) use a Region Proposal Network to\ngenerate regions of interests in the first stage and (ii) send the region\nproposals down the pipeline for object classification and bounding-box\nregression. Such models reach the highest accuracy rates, but are typically\nslower. On the other hand, we have single-stage detectors, such as YOLO (You\nOnly Look Once) and SSD (Singe Shot MultiBox Detector), that treat object\ndetection as a simple regression problem by taking an input image and learning\nthe class probabilities and bounding box coordinates. Such models reach lower\naccuracy rates, but are much faster than two-stage object detectors. In this\npaper, we propose to use an image difficulty predictor to achieve an optimal\ntrade-off between accuracy and speed in object detection. The image difficulty\npredictor is applied on the test images to split them into easy versus hard\nimages. Once separated, the easy images are sent to the faster single-stage\ndetector, while the hard images are sent to the more accurate two-stage\ndetector. Our experiments on PASCAL VOC 2007 show that using image difficulty\ncompares favorably to a random split of the images. Our method is flexible, in\nthat it allows to choose a desired threshold for splitting the images into easy\nversus hard.\n", "versions": [{"version": "v1", "created": "Fri, 23 Mar 2018 09:35:05 GMT"}, {"version": "v2", "created": "Fri, 15 Jun 2018 14:56:08 GMT"}, {"version": "v3", "created": "Fri, 31 Aug 2018 13:13:53 GMT"}], "update_date": "2018-09-03", "authors_parsed": [["Soviany", "Petru", ""], ["Ionescu", "Radu Tudor", ""]]}, {"id": "1803.08709", "submitter": "Andreas Eberle", "authors": "Andreas Eberle", "title": "Pose-Driven Deep Models for Person Re-Identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification (re-id) is the task of recognizing and matching\npersons at different locations recorded by cameras with non-overlapping views.\nOne of the main challenges of re-id is the large variance in person poses and\ncamera angles since neither of them can be influenced by the re-id system. In\nthis work, an effective approach to integrate coarse camera view information as\nwell as fine-grained pose information into a convolutional neural network (CNN)\nmodel for learning discriminative re-id embeddings is introduced. In most\nrecent work pose information is either explicitly modeled within the re-id\nsystem or explicitly used for pre-processing, for example by pose-normalizing\nperson images. In contrast, the proposed approach shows that a direct use of\ncamera view as well as the detected body joint locations into a standard CNN\ncan be used to significantly improve the robustness of learned re-id\nembeddings. On four challenging surveillance and video re-id datasets\nsignificant improvements over the current state of the art have been achieved.\nFurthermore, a novel reordering of the MARS dataset, called X-MARS is\nintroduced to allow cross-validation of models trained for single-image re-id\non tracklet data.\n", "versions": [{"version": "v1", "created": "Fri, 23 Mar 2018 09:38:42 GMT"}], "update_date": "2018-03-26", "authors_parsed": [["Eberle", "Andreas", ""]]}, {"id": "1803.08716", "submitter": "Hainan Cui", "authors": "Hainan Cui, Shuhan Shen, Xiang Gao, Zhanyi Hu", "title": "CSfM: Community-based Structure from Motion", "comments": "Our paper has been published in ICIP2017", "journal-ref": "2017 IEEE International Conference on Image Processing (ICIP2017)", "doi": "10.1109/ICIP.2017.8297137", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structure-from-Motion approaches could be broadly divided into two classes:\nincremental and global. While incremental manner is robust to outliers, it\nsuffers from error accumulation and heavy computation load. The global manner\nhas the advantage of simultaneously estimating all camera poses, but it is\nusually sensitive to epipolar geometry outliers. In this paper, we propose an\nadaptive community-based SfM (CSfM) method which takes both robustness and\nefficiency into consideration. First, the epipolar geometry graph is\npartitioned into separate communities. Then, the reconstruction problem is\nsolved for each community in parallel. Finally, the reconstruction results are\nmerged by a novel global similarity averaging method, which solves three convex\n$L1$ optimization problems. Experimental results show that our method performs\nbetter than many of the state-of-the-art global SfM approaches in terms of\ncomputational efficiency, while achieves similar or better reconstruction\naccuracy and robustness than many of the state-of-the-art incremental SfM\napproaches.\n", "versions": [{"version": "v1", "created": "Fri, 23 Mar 2018 10:27:01 GMT"}], "update_date": "2018-03-26", "authors_parsed": [["Cui", "Hainan", ""], ["Shen", "Shuhan", ""], ["Gao", "Xiang", ""], ["Hu", "Zhanyi", ""]]}, {"id": "1803.08740", "submitter": "Elisa Maiettini", "authors": "Elisa Maiettini, Giulia Pasquale, Lorenzo Rosasco and Lorenzo Natale", "title": "Speeding-up Object Detection Training for Robotics with FALKON", "comments": null, "journal-ref": "IEEE/RSJ International Conference on Intelligent Robots and\n  Systems (IROS), 2018", "doi": "10.1109/IROS.2018.8593990", "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latest deep learning methods for object detection provide remarkable\nperformance, but have limits when used in robotic applications. One of the most\nrelevant issues is the long training time, which is due to the large size and\nimbalance of the associated training sets, characterized by few positive and a\nlarge number of negative examples (i.e. background). Proposed approaches are\nbased on end-to-end learning by back-propagation [22] or kernel methods trained\nwith Hard Negatives Mining on top of deep features [8]. These solutions are\neffective, but prohibitively slow for on-line applications. In this paper we\npropose a novel pipeline for object detection that overcomes this problem and\nprovides comparable performance, with a 60x training speedup. Our pipeline\ncombines (i) the Region Proposal Network and the deep feature extractor from\n[22] to efficiently select candidate RoIs and encode them into powerful\nrepresentations, with (ii) the FALKON [23] algorithm, a novel kernel-based\nmethod that allows fast training on large scale problems (millions of points).\nWe address the size and imbalance of training data by exploiting the stochastic\nsubsampling intrinsic into the method and a novel, fast, bootstrapping\napproach. We assess the effectiveness of the approach on a standard Computer\nVision dataset (PASCAL VOC 2007 [5]) and demonstrate its applicability to a\nreal robotic scenario with the iCubWorld Transformations [18] dataset.\n", "versions": [{"version": "v1", "created": "Fri, 23 Mar 2018 11:13:29 GMT"}, {"version": "v2", "created": "Mon, 27 Aug 2018 15:19:38 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Maiettini", "Elisa", ""], ["Pasquale", "Giulia", ""], ["Rosasco", "Lorenzo", ""], ["Natale", "Lorenzo", ""]]}, {"id": "1803.08763", "submitter": "Xinghao Ding", "authors": "Liyan Sun, Zhiwen Fan, Yue Huang, Xinghao Ding, John Paisley", "title": "A Deep Error Correction Network for Compressed Sensing MRI", "comments": "7 pages, 7 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compressed sensing for magnetic resonance imaging (CS-MRI) exploits image\nsparsity properties to reconstruct MRI from very few Fourier k-space\nmeasurements. The goal is to minimize any structural errors in the\nreconstruction that could have a negative impact on its diagnostic quality. To\nthis end, we propose a deep error correction network (DECN) for CS-MRI. The\nDECN model consists of three parts, which we refer to as modules: a guide, or\ntemplate, module, an error correction module, and a data fidelity module.\nExisting CS-MRI algorithms can serve as the template module for guiding the\nreconstruction. Using this template as a guide, the error correction module\nlearns a convolutional neural network (CNN) to map the k-space data in a way\nthat adjusts for the reconstruction error of the template image. Our\nexperimental results show the proposed DECN CS-MRI reconstruction framework can\nconsiderably improve upon existing inversion algorithms by supplementing with\nan error-correcting CNN.\n", "versions": [{"version": "v1", "created": "Fri, 23 Mar 2018 12:18:25 GMT"}], "update_date": "2018-03-26", "authors_parsed": [["Sun", "Liyan", ""], ["Fan", "Zhiwen", ""], ["Huang", "Yue", ""], ["Ding", "Xinghao", ""], ["Paisley", "John", ""]]}, {"id": "1803.08794", "submitter": "Hichem Sahbi", "authors": "Mingyuan Jiu and Hichem Sahbi", "title": "Learning Deep Context-Network Architectures for Image Annotation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Context plays an important role in visual pattern recognition as it provides\ncomplementary clues for different learning tasks including image classification\nand annotation. In the particular scenario of kernel learning, the general\nrecipe of context-based kernel design consists in learning positive\nsemi-definite similarity functions that return high values not only when data\nshare similar content but also similar context. However, in spite of having a\npositive impact on performance, the use of context in these kernel design\nmethods has not been fully explored; indeed, context has been handcrafted\ninstead of being learned. In this paper, we introduce a novel context-aware\nkernel design framework based on deep learning. Our method discriminatively\nlearns spatial geometric context as the weights of a deep network (DN). The\narchitecture of this network is fully determined by the solution of an\nobjective function that mixes content, context and regularization, while the\nparameters of this network determine the most relevant (discriminant) parts of\nthe learned context. We apply this context and kernel learning framework to\nimage classification using the challenging ImageCLEF Photo Annotation\nbenchmark; the latter shows that our deep context learning provides highly\neffective kernels for image classification as corroborated through extensive\nexperiments.\n", "versions": [{"version": "v1", "created": "Fri, 23 Mar 2018 13:58:21 GMT"}], "update_date": "2018-03-26", "authors_parsed": [["Jiu", "Mingyuan", ""], ["Sahbi", "Hichem", ""]]}, {"id": "1803.08805", "submitter": "Weizhe Liu", "authors": "Weizhe Liu, Krzysztof Lis, Mathieu Salzmann, Pascal Fua", "title": "Geometric and Physical Constraints for Drone-Based Head Plane Crowd\n  Density Estimation", "comments": "IROS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art methods for counting people in crowded scenes rely on deep\nnetworks to estimate crowd density in the image plane. While useful for this\npurpose, this image-plane density has no immediate physical meaning because it\nis subject to perspective distortion. This is a concern in sequences acquired\nby drones because the viewpoint changes often. This distortion is usually\nhandled implicitly by either learning scale-invariant features or estimating\ndensity in patches of different sizes, neither of which accounts for the fact\nthat scale changes must be consistent over the whole scene.\n  In this paper, we explicitly model the scale changes and reason in terms of\npeople per square-meter. We show that feeding the perspective model to the\nnetwork allows us to enforce global scale consistency and that this model can\nbe obtained on the fly from the drone sensors. In addition, it also enables us\nto enforce physically-inspired temporal consistency constraints that do not\nhave to be learned. This yields an algorithm that outperforms state-of-the-art\nmethods in inferring crowd density from a moving drone camera especially when\nperspective effects are strong.\n", "versions": [{"version": "v1", "created": "Fri, 23 Mar 2018 14:19:13 GMT"}, {"version": "v2", "created": "Wed, 17 Jul 2019 14:00:35 GMT"}, {"version": "v3", "created": "Thu, 18 Jul 2019 09:05:50 GMT"}], "update_date": "2019-07-19", "authors_parsed": [["Liu", "Weizhe", ""], ["Lis", "Krzysztof", ""], ["Salzmann", "Mathieu", ""], ["Fua", "Pascal", ""]]}, {"id": "1803.08834", "submitter": "Isma Hadji", "authors": "Isma Hadji and Richard P. Wildes", "title": "What Do We Understand About Convolutional Networks?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This document will review the most prominent proposals using multilayer\nconvolutional architectures. Importantly, the various components of a typical\nconvolutional network will be discussed through a review of different\napproaches that base their design decisions on biological findings and/or sound\ntheoretical bases. In addition, the different attempts at understanding\nConvNets via visualizations and empirical studies will be reviewed. The\nultimate goal is to shed light on the role of each layer of processing involved\nin a ConvNet architecture, distill what we currently understand about ConvNets\nand highlight critical open problems.\n", "versions": [{"version": "v1", "created": "Fri, 23 Mar 2018 15:22:01 GMT"}], "update_date": "2018-03-26", "authors_parsed": [["Hadji", "Isma", ""], ["Wildes", "Richard P.", ""]]}, {"id": "1803.08840", "submitter": "Agnieszka Szczotka", "authors": "Daniele Rav\\`i and Agnieszka Barbara Szczotka and Dzhoshkun Ismail\n  Shakir and Stephen P Pereira and Tom Vercauteren", "title": "Effective deep learning training for single-image super-resolution in\n  endomicroscopy exploiting video-registration-based reconstruction", "comments": null, "journal-ref": null, "doi": "10.1007/s11548-018-1764-0", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: Probe-based Confocal Laser Endomicroscopy (pCLE) is a recent imaging\nmodality that allows performing in vivo optical biopsies. The design of pCLE\nhardware, and its reliance on an optical fibre bundle, fundamentally limits the\nimage quality with a few tens of thousands fibres, each acting as the\nequivalent of a single-pixel detector, assembled into a single fibre bundle.\nVideo-registration techniques can be used to estimate high-resolution (HR)\nimages by exploiting the temporal information contained in a sequence of\nlow-resolution (LR) images. However, the alignment of LR frames, required for\nthe fusion, is computationally demanding and prone to artefacts. Methods: In\nthis work, we propose a novel synthetic data generation approach to train\nexemplar-based Deep Neural Networks (DNNs). HR pCLE images with enhanced\nquality are recovered by the models trained on pairs of estimated HR images\n(generated by the video-registration algorithm) and realistic synthetic LR\nimages. Performance of three different state-of-the-art DNNs techniques were\nanalysed on a Smart Atlas database of 8806 images from 238 pCLE video\nsequences. The results were validated through an extensive Image Quality\nAssessment (IQA) that takes into account different quality scores, including a\nMean Opinion Score (MOS). Results: Results indicate that the proposed solution\nproduces an effective improvement in the quality of the obtained reconstructed\nimage. Conclusion: The proposed training strategy and associated DNNs allows us\nto perform convincing super-resolution of pCLE images.\n", "versions": [{"version": "v1", "created": "Fri, 23 Mar 2018 15:31:40 GMT"}], "update_date": "2018-04-25", "authors_parsed": [["Rav\u00ec", "Daniele", ""], ["Szczotka", "Agnieszka Barbara", ""], ["Shakir", "Dzhoshkun Ismail", ""], ["Pereira", "Stephen P", ""], ["Vercauteren", "Tom", ""]]}, {"id": "1803.08842", "submitter": "Yapeng Tian", "authors": "Yapeng Tian, Jing Shi, Bochen Li, Zhiyao Duan, and Chenliang Xu", "title": "Audio-Visual Event Localization in Unconstrained Videos", "comments": "23 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a novel problem of audio-visual event\nlocalization in unconstrained videos. We define an audio-visual event as an\nevent that is both visible and audible in a video segment. We collect an\nAudio-Visual Event(AVE) dataset to systemically investigate three temporal\nlocalization tasks: supervised and weakly-supervised audio-visual event\nlocalization, and cross-modality localization. We develop an audio-guided\nvisual attention mechanism to explore audio-visual correlations, propose a dual\nmultimodal residual network (DMRN) to fuse information over the two modalities,\nand introduce an audio-visual distance learning network to handle the\ncross-modality localization. Our experiments support the following findings:\njoint modeling of auditory and visual modalities outperforms independent\nmodeling, the learned attention can capture semantics of sounding objects,\ntemporal alignment is important for audio-visual fusion, the proposed DMRN is\neffective in fusing audio-visual features, and strong correlations between the\ntwo modalities enable cross-modality localization.\n", "versions": [{"version": "v1", "created": "Fri, 23 Mar 2018 15:34:03 GMT"}], "update_date": "2018-03-26", "authors_parsed": [["Tian", "Yapeng", ""], ["Shi", "Jing", ""], ["Li", "Bochen", ""], ["Duan", "Zhiyao", ""], ["Xu", "Chenliang", ""]]}, {"id": "1803.08887", "submitter": "Ngoc-Trung Tran", "authors": "Ngoc-Trung Tran, Tuan-Anh Bui, Ngai-Man Cheung", "title": "Dist-GAN: An Improved GAN using Distance Constraints", "comments": "Published as a conference paper at ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce effective training algorithms for Generative Adversarial\nNetworks (GAN) to alleviate mode collapse and gradient vanishing. In our\nsystem, we constrain the generator by an Autoencoder (AE). We propose a\nformulation to consider the reconstructed samples from AE as \"real\" samples for\nthe discriminator. This couples the convergence of the AE with that of the\ndiscriminator, effectively slowing down the convergence of discriminator and\nreducing gradient vanishing. Importantly, we propose two novel distance\nconstraints to improve the generator. First, we propose a latent-data distance\nconstraint to enforce compatibility between the latent sample distances and the\ncorresponding data sample distances. We use this constraint to explicitly\nprevent the generator from mode collapse. Second, we propose a\ndiscriminator-score distance constraint to align the distribution of the\ngenerated samples with that of the real samples through the discriminator\nscore. We use this constraint to guide the generator to synthesize samples that\nresemble the real ones. Our proposed GAN using these distance constraints,\nnamely Dist-GAN, can achieve better results than state-of-the-art methods\nacross benchmark datasets: synthetic, MNIST, MNIST-1K, CelebA, CIFAR-10 and\nSTL-10 datasets. Our code is published here (https://github.com/tntrung/gan)\nfor research.\n", "versions": [{"version": "v1", "created": "Fri, 23 Mar 2018 17:06:26 GMT"}, {"version": "v2", "created": "Fri, 27 Jul 2018 08:04:52 GMT"}, {"version": "v3", "created": "Sat, 15 Dec 2018 08:32:35 GMT"}], "update_date": "2018-12-18", "authors_parsed": [["Tran", "Ngoc-Trung", ""], ["Bui", "Tuan-Anh", ""], ["Cheung", "Ngai-Man", ""]]}, {"id": "1803.08896", "submitter": "Somak Aditya", "authors": "Somak Aditya, Yezhou Yang, Chitta Baral", "title": "Explicit Reasoning over End-to-End Neural Architectures for Visual\n  Question Answering", "comments": "9 pages, 3 figures, AAAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many vision and language tasks require commonsense reasoning beyond\ndata-driven image and natural language processing. Here we adopt Visual\nQuestion Answering (VQA) as an example task, where a system is expected to\nanswer a question in natural language about an image. Current state-of-the-art\nsystems attempted to solve the task using deep neural architectures and\nachieved promising performance. However, the resulting systems are generally\nopaque and they struggle in understanding questions for which extra knowledge\nis required. In this paper, we present an explicit reasoning layer on top of a\nset of penultimate neural network based systems. The reasoning layer enables\nreasoning and answering questions where additional knowledge is required, and\nat the same time provides an interpretable interface to the end users.\nSpecifically, the reasoning layer adopts a Probabilistic Soft Logic (PSL) based\nengine to reason over a basket of inputs: visual relations, the semantic parse\nof the question, and background ontological knowledge from word2vec and\nConceptNet. Experimental analysis of the answers and the key evidential\npredicates generated on the VQA dataset validate our approach.\n", "versions": [{"version": "v1", "created": "Fri, 23 Mar 2018 17:17:16 GMT"}], "update_date": "2018-03-26", "authors_parsed": [["Aditya", "Somak", ""], ["Yang", "Yezhou", ""], ["Baral", "Chitta", ""]]}, {"id": "1803.08904", "submitter": "Hang Zhang", "authors": "Hang Zhang, Kristin Dana, Jianping Shi, Zhongyue Zhang, Xiaogang Wang,\n  Ambrish Tyagi, Amit Agrawal", "title": "Context Encoding for Semantic Segmentation", "comments": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR),\n  2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work has made significant progress in improving spatial resolution for\npixelwise labeling with Fully Convolutional Network (FCN) framework by\nemploying Dilated/Atrous convolution, utilizing multi-scale features and\nrefining boundaries. In this paper, we explore the impact of global contextual\ninformation in semantic segmentation by introducing the Context Encoding\nModule, which captures the semantic context of scenes and selectively\nhighlights class-dependent featuremaps. The proposed Context Encoding Module\nsignificantly improves semantic segmentation results with only marginal extra\ncomputation cost over FCN. Our approach has achieved new state-of-the-art\nresults 51.7% mIoU on PASCAL-Context, 85.9% mIoU on PASCAL VOC 2012. Our single\nmodel achieves a final score of 0.5567 on ADE20K test set, which surpass the\nwinning entry of COCO-Place Challenge in 2017. In addition, we also explore how\nthe Context Encoding Module can improve the feature representation of\nrelatively shallow networks for the image classification on CIFAR-10 dataset.\nOur 14 layer network has achieved an error rate of 3.45%, which is comparable\nwith state-of-the-art approaches with over 10 times more layers. The source\ncode for the complete system are publicly available.\n", "versions": [{"version": "v1", "created": "Fri, 23 Mar 2018 17:34:21 GMT"}], "update_date": "2018-03-26", "authors_parsed": [["Zhang", "Hang", ""], ["Dana", "Kristin", ""], ["Shi", "Jianping", ""], ["Zhang", "Zhongyue", ""], ["Wang", "Xiaogang", ""], ["Tyagi", "Ambrish", ""], ["Agrawal", "Amit", ""]]}, {"id": "1803.08908", "submitter": "Jan Bedna\\v{r}\\'ik", "authors": "Jan Bedna\\v{r}\\'ik, Pascal Fua, Mathieu Salzmann", "title": "Learning to Reconstruct Texture-less Deformable Surfaces from a Single\n  View", "comments": "Accepted to 3DV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have seen the development of mature solutions for reconstructing\ndeformable surfaces from a single image, provided that they are relatively\nwell-textured. By contrast, recovering the 3D shape of texture-less surfaces\nremains an open problem, and essentially relates to Shape-from-Shading. In this\npaper, we introduce a data-driven approach to this problem. We introduce a\ngeneral framework that can predict diverse 3D representations, such as meshes,\nnormals, and depth maps. Our experiments show that meshes are ill-suited to\nhandle texture-less 3D reconstruction in our context. Furthermore, we\ndemonstrate that our approach generalizes well to unseen objects, and that it\nyields higher-quality reconstructions than a state-of-the-art SfS technique,\nparticularly in terms of normal estimates. Our reconstructions accurately model\nthe fine details of the surfaces, such as the creases of a T-Shirt worn by a\nperson.\n", "versions": [{"version": "v1", "created": "Fri, 23 Mar 2018 17:46:20 GMT"}, {"version": "v2", "created": "Fri, 27 Jul 2018 14:11:25 GMT"}], "update_date": "2018-07-30", "authors_parsed": [["Bedna\u0159\u00edk", "Jan", ""], ["Fua", "Pascal", ""], ["Salzmann", "Mathieu", ""]]}, {"id": "1803.08943", "submitter": "Chao Yang Mr.", "authors": "Chao Yang, Yuhang Song, Xiaofeng Liu, Qingming Tang and C.-C. Jay Kuo", "title": "Image Inpainting using Block-wise Procedural Training with Annealed\n  Adversarial Counterpart", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in deep generative models have shown promising potential in\nimage inpanting, which refers to the task of predicting missing pixel values of\nan incomplete image using the known context. However, existing methods can be\nslow or generate unsatisfying results with easily detectable flaws. In\naddition, there is often perceivable discontinuity near the holes and require\nfurther post-processing to blend the results. We present a new approach to\naddress the difficulty of training a very deep generative model to synthesize\nhigh-quality photo-realistic inpainting. Our model uses conditional generative\nadversarial networks (conditional GANs) as the backbone, and we introduce a\nnovel block-wise procedural training scheme to stabilize the training while we\nincrease the network depth. We also propose a new strategy called adversarial\nloss annealing to reduce the artifacts. We further describe several losses\nspecifically designed for inpainting and show their effectiveness. Extensive\nexperiments and user-study show that our approach outperforms existing methods\nin several tasks such as inpainting, face completion and image harmonization.\nFinally, we show our framework can be easily used as a tool for interactive\nguided inpainting, demonstrating its practical value to solve common real-world\nchallenges.\n", "versions": [{"version": "v1", "created": "Fri, 23 Mar 2018 18:45:12 GMT"}, {"version": "v2", "created": "Wed, 28 Mar 2018 02:28:01 GMT"}], "update_date": "2018-03-29", "authors_parsed": [["Yang", "Chao", ""], ["Song", "Yuhang", ""], ["Liu", "Xiaofeng", ""], ["Tang", "Qingming", ""], ["Kuo", "C. -C. Jay", ""]]}, {"id": "1803.08949", "submitter": "Nathaniel Chodosh", "authors": "Nathaniel Chodosh, Chaoyang Wang, Simon Lucey", "title": "Deep Convolutional Compressed Sensing for LiDAR Depth Completion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider the problem of estimating a dense depth map from a\nset of sparse LiDAR points. We use techniques from compressed sensing and the\nrecently developed Alternating Direction Neural Networks (ADNNs) to create a\ndeep recurrent auto-encoder for this task. Our architecture internally performs\nan algorithm for extracting multi-level convolutional sparse codes from the\ninput which are then used to make a prediction. Our results demonstrate that\nwith only two layers and 1800 parameters we are able to out perform all\npreviously published results, including deep networks with orders of magnitude\nmore parameters.\n", "versions": [{"version": "v1", "created": "Fri, 23 Mar 2018 19:18:01 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Chodosh", "Nathaniel", ""], ["Wang", "Chaoyang", ""], ["Lucey", "Simon", ""]]}, {"id": "1803.08995", "submitter": "Maksym Kholiavchenko", "authors": "Maksym Kholiavchenko", "title": "Iterative Low-Rank Approximation for CNN Compression", "comments": "Updated paper: arXiv:1903.09973", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks contain tens of millions of parameters,\nmaking them impossible to work efficiently on embedded devices. We propose\niterative approach of applying low-rank approximation to compress deep\nconvolutional neural networks. Since classification and object detection are\nthe most favored tasks for embedded devices, we demonstrate the effectiveness\nof our approach by compressing AlexNet, VGG-16, YOLOv2 and Tiny YOLO networks.\nOur results show the superiority of the proposed method compared to\nnon-repetitive ones. We demonstrate higher compression ratio providing less\naccuracy loss.\n", "versions": [{"version": "v1", "created": "Fri, 23 Mar 2018 22:04:48 GMT"}, {"version": "v2", "created": "Fri, 15 Nov 2019 12:27:42 GMT"}], "update_date": "2019-11-18", "authors_parsed": [["Kholiavchenko", "Maksym", ""]]}, {"id": "1803.08996", "submitter": "David Friedlander", "authors": "David Friedlander", "title": "Pattern Analysis with Layered Self-Organizing Maps", "comments": "16 pages, 21 color figures, DRAFT", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper defines a new learning architecture, Layered Self-Organizing Maps\n(LSOMs), that uses the SOM and supervised-SOM learning algorithms. The\narchitecture is validated with the MNIST database of hand-written digit images.\nLSOMs are similar to convolutional neural nets (covnets) in the way they sample\ndata, but different in the way they represent features and learn. LSOMs analyze\n(or generate) image patches with maps of exemplars determined by the SOM\nlearning algorithm rather than feature maps from filter-banks learned via\nbackprop.\n  LSOMs provide an alternative to features derived from covnets. Multi-layer\nLSOMs are trained bottom-up, without the use of backprop and therefore may be\nof interest as a model of the visual cortex. The results show organization at\nmultiple levels. The algorithm appears to be resource efficient in learning,\nclassifying and generating images. Although LSOMs can be used for\nclassification, their validation accuracy for these exploratory runs was well\nbelow the state of the art. The goal of this article is to define the\narchitecture and display the structures resulting from its application to the\nMNIST images.\n", "versions": [{"version": "v1", "created": "Fri, 23 Mar 2018 22:07:52 GMT"}, {"version": "v2", "created": "Wed, 28 Mar 2018 17:07:18 GMT"}], "update_date": "2018-03-29", "authors_parsed": [["Friedlander", "David", ""]]}, {"id": "1803.08999", "submitter": "Chuhang Zou", "authors": "Chuhang Zou, Alex Colburn, Qi Shan, Derek Hoiem", "title": "LayoutNet: Reconstructing the 3D Room Layout from a Single RGB Image", "comments": "CVPR2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an algorithm to predict room layout from a single image that\ngeneralizes across panoramas and perspective images, cuboid layouts and more\ngeneral layouts (e.g. L-shape room). Our method operates directly on the\npanoramic image, rather than decomposing into perspective images as do recent\nworks. Our network architecture is similar to that of RoomNet, but we show\nimprovements due to aligning the image based on vanishing points, predicting\nmultiple layout elements (corners, boundaries, size and translation), and\nfitting a constrained Manhattan layout to the resulting predictions. Our method\ncompares well in speed and accuracy to other existing work on panoramas,\nachieves among the best accuracy for perspective images, and can handle both\ncuboid-shaped and more general Manhattan layouts.\n", "versions": [{"version": "v1", "created": "Fri, 23 Mar 2018 22:25:52 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Zou", "Chuhang", ""], ["Colburn", "Alex", ""], ["Shan", "Qi", ""], ["Hoiem", "Derek", ""]]}, {"id": "1803.09004", "submitter": "Xiaofan Zhang", "authors": "Chuanhao Zhuge, Xinheng Liu, Xiaofan Zhang, Sudeep Gummadi, Jinjun\n  Xiong, Deming Chen", "title": "Face Recognition with Hybrid Efficient Convolution Algorithms on FPGAs", "comments": "This paper is accepted in GLSVLSI'18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Convolutional Neural Networks have become a Swiss knife in solving\ncritical artificial intelligence tasks. However, deploying deep CNN models for\nlatency-critical tasks remains to be challenging because of the complex nature\nof CNNs. Recently, FPGA has become a favorable device to accelerate deep CNNs\nthanks to its high parallel processing capability and energy efficiency. In\nthis work, we explore different fast convolution algorithms including Winograd\nand Fast Fourier Transform (FFT), and find an optimal strategy to apply them\ntogether on different types of convolutions. We also propose an optimization\nscheme to exploit parallelism on novel CNN architectures such as Inception\nmodules in GoogLeNet. We implement a configurable IP-based face recognition\nacceleration system based on FaceNet using High-Level Synthesis. Our\nimplementation on a Xilinx Ultrascale device achieves 3.75x latency speedup\ncompared to a high-end NVIDIA GPU and surpasses previous FPGA results\nsignificantly.\n", "versions": [{"version": "v1", "created": "Fri, 23 Mar 2018 22:42:57 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Zhuge", "Chuanhao", ""], ["Liu", "Xinheng", ""], ["Zhang", "Xiaofan", ""], ["Gummadi", "Sudeep", ""], ["Xiong", "Jinjun", ""], ["Chen", "Deming", ""]]}, {"id": "1803.09014", "submitter": "Xi Yin", "authors": "Xi Yin, Xiang Yu, Kihyuk Sohn, Xiaoming Liu and Manmohan Chandraker", "title": "Feature Transfer Learning for Deep Face Recognition with\n  Under-Represented Data", "comments": "CVPR2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the large volume of face recognition datasets, there is a significant\nportion of subjects, of which the samples are insufficient and thus\nunder-represented. Ignoring such significant portion results in insufficient\ntraining data. Training with under-represented data leads to biased classifiers\nin conventionally-trained deep networks. In this paper, we propose a\ncenter-based feature transfer framework to augment the feature space of\nunder-represented subjects from the regular subjects that have sufficiently\ndiverse samples. A Gaussian prior of the variance is assumed across all\nsubjects and the variance from regular ones are transferred to the\nunder-represented ones. This encourages the under-represented distribution to\nbe closer to the regular distribution. Further, an alternating training regimen\nis proposed to simultaneously achieve less biased classifiers and a more\ndiscriminative feature representation. We conduct ablative study to mimic the\nunder-represented datasets by varying the portion of under-represented classes\non the MS-Celeb-1M dataset. Advantageous results on LFW, IJB-A and MS-Celeb-1M\ndemonstrate the effectiveness of our feature transfer and training strategy,\ncompared to both general baselines and state-of-the-art methods. Moreover, our\nfeature transfer successfully presents smooth visual interpolation, which\nconducts disentanglement to preserve identity of a class while augmenting its\nfeature space with non-identity variations such as pose and lighting.\n", "versions": [{"version": "v1", "created": "Fri, 23 Mar 2018 23:37:31 GMT"}, {"version": "v2", "created": "Mon, 19 Aug 2019 05:52:42 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Yin", "Xi", ""], ["Yu", "Xiang", ""], ["Sohn", "Kihyuk", ""], ["Liu", "Xiaoming", ""], ["Chandraker", "Manmohan", ""]]}, {"id": "1803.09025", "submitter": "Alex Zihao Zhu", "authors": "Alex Zihao Zhu, Yibo Chen, Kostas Daniilidis", "title": "Realtime Time Synchronized Event-based Stereo", "comments": "13 pages, 3 figures, 1 table. Video: https://youtu.be/4oa7e4hsrYo.\n  Updated with final version with additional experiments", "journal-ref": "European Conference on Computer Vision 2018", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a novel event based stereo method which addresses\nthe problem of motion blur for a moving event camera. Our method uses the\nvelocity of the camera and a range of disparities to synchronize the positions\nof the events, as if they were captured at a single point in time. We represent\nthese events using a pair of novel time synchronized event disparity volumes,\nwhich we show remove motion blur for pixels at the correct disparity in the\nvolume, while further blurring pixels at the wrong disparity. We then apply a\nnovel matching cost over these time synchronized event disparity volumes, which\nboth rewards similarity between the volumes while penalizing blurriness. We\nshow that our method outperforms more expensive, smoothing based event stereo\nmethods, by evaluating on the Multi Vehicle Stereo Event Camera dataset.\n", "versions": [{"version": "v1", "created": "Sat, 24 Mar 2018 01:00:49 GMT"}, {"version": "v2", "created": "Thu, 18 Oct 2018 19:01:43 GMT"}], "update_date": "2018-10-22", "authors_parsed": [["Zhu", "Alex Zihao", ""], ["Chen", "Yibo", ""], ["Daniilidis", "Kostas", ""]]}, {"id": "1803.09052", "submitter": "Jianjie Lu", "authors": "Meng Shengwei, Lu Jianjie", "title": "Design of a PCIe Interface Card Control Software Based on WDF", "comments": "4 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Based on a clear analysis of the latest Windows driver framework WDF, this\npaper has implemented a driver of the PCIe-SpaceWire interface card device and\nput forward a discussion about ensuring the stability of PCIe driver. At the\nsame time, Qt and OpenGL are used to design the upper application. Finally, a\nfunctional verification of the control software is provided.\n", "versions": [{"version": "v1", "created": "Sat, 24 Mar 2018 04:04:20 GMT"}, {"version": "v2", "created": "Fri, 6 Apr 2018 01:10:59 GMT"}, {"version": "v3", "created": "Wed, 11 Apr 2018 01:16:13 GMT"}], "update_date": "2018-04-12", "authors_parsed": [["Shengwei", "Meng", ""], ["Jianjie", "Lu", ""]]}, {"id": "1803.09058", "submitter": "Bingyao Huang", "authors": "Bingyao Huang, Samed Ozdemir, Ying Tang, Chunyuan Liao, Haibin Ling", "title": "A Single-shot-per-pose Camera-Projector Calibration System For Imperfect\n  Planar Targets", "comments": "Adjunct Proceedings of the IEEE International Symposium for Mixed and\n  Augmented Reality 2018. Source code:\n  https://github.com/BingyaoHuang/single-shot-pro-cam-calib/", "journal-ref": null, "doi": "10.1109/ISMAR-Adjunct.2018.00023", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing camera-projector calibration methods typically warp feature points\nfrom a camera image to a projector image using estimated homographies, and\noften suffer from errors in camera parameters and noise due to imperfect\nplanarity of the calibration target. In this paper we propose a simple yet\nrobust solution that explicitly deals with these challenges. Following the\nstructured light (SL) camera-project calibration framework, a carefully\ndesigned correspondence algorithm is built on top of the De Bruijn patterns.\nSuch correspondence is then used for initial camera-projector calibration.\nThen, to gain more robustness against noises, especially those from an\nimperfect planar calibration board, a bundle adjustment algorithm is developed\nto jointly optimize the estimated camera and projector models. Aside from the\nrobustness, our solution requires only one shot of SL pattern for each\ncalibration board pose, which is much more convenient than multi-shot solutions\nin practice. Data validations are conducted on both synthetic and real\ndatasets, and our method shows clear advantages over existing methods in all\nexperiments.\n", "versions": [{"version": "v1", "created": "Sat, 24 Mar 2018 05:25:23 GMT"}, {"version": "v2", "created": "Wed, 17 Oct 2018 19:43:56 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Huang", "Bingyao", ""], ["Ozdemir", "Samed", ""], ["Tang", "Ying", ""], ["Liao", "Chunyuan", ""], ["Ling", "Haibin", ""]]}, {"id": "1803.09092", "submitter": "Concetto Spampinato Dr", "authors": "C. Spampinato, S. Palazzo, P. D'Oro, D. Giordano, M. Shah", "title": "Adversarial Framework for Unsupervised Learning of Motion Dynamics in\n  Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human behavior understanding in videos is a complex, still unsolved problem\nand requires to accurately model motion at both the local (pixel-wise dense\nprediction) and global (aggregation of motion cues) levels. Current approaches\nbased on supervised learning require large amounts of annotated data, whose\nscarce availability is one of the main limiting factors to the development of\ngeneral solutions. Unsupervised learning can instead leverage the vast amount\nof videos available on the web and it is a promising solution for overcoming\nthe existing limitations. In this paper, we propose an adversarial GAN-based\nframework that learns video representations and dynamics through a\nself-supervision mechanism in order to perform dense and global prediction in\nvideos. Our approach synthesizes videos by 1) factorizing the process into the\ngeneration of static visual content and motion, 2) learning a suitable\nrepresentation of a motion latent space in order to enforce spatio-temporal\ncoherency of object trajectories, and 3) incorporating motion estimation and\npixel-wise dense prediction into the training procedure. Self-supervision is\nenforced by using motion masks produced by the generator, as a co-product of\nits generation process, to supervise the discriminator network in performing\ndense prediction. Performance evaluation, carried out on standard benchmarks,\nshows that our approach is able to learn, in an unsupervised way, both local\nand global video dynamics. The learned representations, then, support the\ntraining of video object segmentation methods with sensibly less (about 50%)\nannotations, giving performance comparable to the state of the art.\nFurthermore, the proposed method achieves promising performance in generating\nrealistic videos, outperforming state-of-the-art approaches especially on\nmotion-related metrics.\n", "versions": [{"version": "v1", "created": "Sat, 24 Mar 2018 11:17:41 GMT"}, {"version": "v2", "created": "Tue, 17 Sep 2019 20:42:07 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["Spampinato", "C.", ""], ["Palazzo", "S.", ""], ["D'Oro", "P.", ""], ["Giordano", "D.", ""], ["Shah", "M.", ""]]}, {"id": "1803.09093", "submitter": "Mathijs Pieters", "authors": "Mathijs Pieters and Marco Wiering", "title": "Comparing Generative Adversarial Network Techniques for Image Creation\n  and Modification", "comments": "20 pages, 23 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative adversarial networks (GANs) have demonstrated to be successful at\ngenerating realistic real-world images. In this paper we compare various GAN\ntechniques, both supervised and unsupervised. The effects on training stability\nof different objective functions are compared. We add an encoder to the\nnetwork, making it possible to encode images to the latent space of the GAN.\nThe generator, discriminator and encoder are parameterized by deep\nconvolutional neural networks. For the discriminator network we experimented\nwith using the novel Capsule Network, a state-of-the-art technique for\ndetecting global features in images. Experiments are performed using a digit\nand face dataset, with various visualizations illustrating the results. The\nresults show that using the encoder network it is possible to reconstruct\nimages. With the conditional GAN we can alter visual attributes of generated or\nencoded images. The experiments with the Capsule Network as discriminator\nresult in generated images of a lower quality, compared to a standard\nconvolutional neural network.\n", "versions": [{"version": "v1", "created": "Sat, 24 Mar 2018 11:19:07 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Pieters", "Mathijs", ""], ["Wiering", "Marco", ""]]}, {"id": "1803.09125", "submitter": "Yifei Huang", "authors": "Yifei Huang, Minjie Cai, Zhenqiang Li, Yoichi Sato", "title": "Predicting Gaze in Egocentric Video by Learning Task-dependent Attention\n  Transition", "comments": "Accepted as oral presentation in ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new computational model for gaze prediction in egocentric videos\nby exploring patterns in temporal shift of gaze fixations (attention\ntransition) that are dependent on egocentric manipulation tasks. Our assumption\nis that the high-level context of how a task is completed in a certain way has\na strong influence on attention transition and should be modeled for gaze\nprediction in natural dynamic scenes. Specifically, we propose a hybrid model\nbased on deep neural networks which integrates task-dependent attention\ntransition with bottom-up saliency prediction. In particular, the\ntask-dependent attention transition is learned with a recurrent neural network\nto exploit the temporal context of gaze fixations, e.g. looking at a cup after\nmoving gaze away from a grasped bottle. Experiments on public egocentric\nactivity datasets show that our model significantly outperforms\nstate-of-the-art gaze prediction methods and is able to learn meaningful\ntransition of human attention.\n", "versions": [{"version": "v1", "created": "Sat, 24 Mar 2018 15:31:55 GMT"}, {"version": "v2", "created": "Fri, 20 Jul 2018 06:05:03 GMT"}, {"version": "v3", "created": "Tue, 4 Dec 2018 12:23:07 GMT"}], "update_date": "2018-12-05", "authors_parsed": [["Huang", "Yifei", ""], ["Cai", "Minjie", ""], ["Li", "Zhenqiang", ""], ["Sato", "Yoichi", ""]]}, {"id": "1803.09127", "submitter": "Zheng Qin", "authors": "Zheng Qin, Zhaoning Zhang, Shiqing Zhang, Hao Yu, Yuxing Peng", "title": "Merging and Evolution: Improving Convolutional Neural Networks for\n  Mobile Applications", "comments": "8 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compact neural networks are inclined to exploit \"sparsely-connected\"\nconvolutions such as depthwise convolution and group convolution for employment\nin mobile applications. Compared with standard \"fully-connected\" convolutions,\nthese convolutions are more computationally economical. However,\n\"sparsely-connected\" convolutions block the inter-group information exchange,\nwhich induces severe performance degradation. To address this issue, we present\ntwo novel operations named merging and evolution to leverage the inter-group\ninformation. Our key idea is encoding the inter-group information with a narrow\nfeature map, then combining the generated features with the original network\nfor better representation. Taking advantage of the proposed operations, we then\nintroduce the Merging-and-Evolution (ME) module, an architectural unit\nspecifically designed for compact networks. Finally, we propose a family of\ncompact neural networks called MENet based on ME modules. Extensive experiments\non ILSVRC 2012 dataset and PASCAL VOC 2007 dataset demonstrate that MENet\nconsistently outperforms other state-of-the-art compact networks under\ndifferent computational budgets. For instance, under the computational budget\nof 140 MFLOPs, MENet surpasses ShuffleNet by 1% and MobileNet by 1.95% on\nILSVRC 2012 top-1 accuracy, while by 2.3% and 4.1% on PASCAL VOC 2007 mAP,\nrespectively.\n", "versions": [{"version": "v1", "created": "Sat, 24 Mar 2018 15:37:49 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Qin", "Zheng", ""], ["Zhang", "Zhaoning", ""], ["Zhang", "Shiqing", ""], ["Yu", "Hao", ""], ["Peng", "Yuxing", ""]]}, {"id": "1803.09132", "submitter": "Xiaobin Chang", "authors": "Xiaobin Chang, Timothy M. Hospedales, Tao Xiang", "title": "Multi-Level Factorisation Net for Person Re-Identification", "comments": "To Appear at CVPR2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Key to effective person re-identification (Re-ID) is modelling discriminative\nand view-invariant factors of person appearance at both high and low semantic\nlevels. Recently developed deep Re-ID models either learn a holistic single\nsemantic level feature representation and/or require laborious human annotation\nof these factors as attributes. We propose Multi-Level Factorisation Net\n(MLFN), a novel network architecture that factorises the visual appearance of a\nperson into latent discriminative factors at multiple semantic levels without\nmanual annotation. MLFN is composed of multiple stacked blocks. Each block\ncontains multiple factor modules to model latent factors at a specific level,\nand factor selection modules that dynamically select the factor modules to\ninterpret the content of each input image. The outputs of the factor selection\nmodules also provide a compact latent factor descriptor that is complementary\nto the conventional deeply learned features. MLFN achieves state-of-the-art\nresults on three Re-ID datasets, as well as compelling results on the general\nobject categorisation CIFAR-100 dataset.\n", "versions": [{"version": "v1", "created": "Sat, 24 Mar 2018 16:34:01 GMT"}, {"version": "v2", "created": "Tue, 17 Apr 2018 21:47:56 GMT"}], "update_date": "2018-04-19", "authors_parsed": [["Chang", "Xiaobin", ""], ["Hospedales", "Timothy M.", ""], ["Xiang", "Tao", ""]]}, {"id": "1803.09165", "submitter": "Renata Khasanova", "authors": "Renata Khasanova, Jan Wassenberg, Jyrki Alakuijala", "title": "Noise generation for compression algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In various Computer Vision and Signal Processing applications, noise is\ntypically perceived as a drawback of the image capturing system that ought to\nbe removed. We, on the other hand, claim that image noise, just as texture, is\nimportant for visual perception and, therefore, critical for lossy compression\nalgorithms that tend to make decompressed images look less realistic by\nremoving small image details. In this paper we propose a physically and\nbiologically inspired technique that learns a noise model at the encoding step\nof the compression algorithm and then generates the appropriate amount of\nadditive noise at the decoding step. Our method can significantly increase the\nrealism of the decompressed image at the cost of few bytes of additional memory\nspace regardless of the original image size. The implementation of our method\nis open-sourced and available at https://github.com/google/pik.\n", "versions": [{"version": "v1", "created": "Sat, 24 Mar 2018 21:19:29 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Khasanova", "Renata", ""], ["Wassenberg", "Jan", ""], ["Alakuijala", "Jyrki", ""]]}, {"id": "1803.09172", "submitter": "Snehashis Roy", "authors": "Snehashis Roy, John A. Butman, Daniel S. Reich, Peter A. Calabresi,\n  Dzung L. Pham", "title": "Multiple Sclerosis Lesion Segmentation from Brain MRI via Fully\n  Convolutional Neural Networks", "comments": "Submitted to NeuroImage", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple Sclerosis (MS) is an autoimmune disease that leads to lesions in the\ncentral nervous system. Magnetic resonance (MR) images provide sufficient\nimaging contrast to visualize and detect lesions, particularly those in the\nwhite matter. Quantitative measures based on various features of lesions have\nbeen shown to be useful in clinical trials for evaluating therapies. Therefore\nrobust and accurate segmentation of white matter lesions from MR images can\nprovide important information about the disease status and progression. In this\npaper, we propose a fully convolutional neural network (CNN) based method to\nsegment white matter lesions from multi-contrast MR images. The proposed CNN\nbased method contains two convolutional pathways. The first pathway consists of\nmultiple parallel convolutional filter banks catering to multiple MR\nmodalities. In the second pathway, the outputs of the first one are\nconcatenated and another set of convolutional filters are applied. The output\nof this last pathway produces a membership function for lesions that may be\nthresholded to obtain a binary segmentation. The proposed method is evaluated\non a dataset of 100 MS patients, as well as the ISBI 2015 challenge data\nconsisting of 14 patients. The comparison is performed against four publicly\navailable MS lesion segmentation methods. Significant improvement in\nsegmentation quality over the competing methods is demonstrated on various\nmetrics, such as Dice and false positive ratio. While evaluating on the ISBI\n2015 challenge data, our method produces a score of 90.48, where a score of 90\nis considered to be comparable to a human rater.\n", "versions": [{"version": "v1", "created": "Sat, 24 Mar 2018 22:43:16 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Roy", "Snehashis", ""], ["Butman", "John A.", ""], ["Reich", "Daniel S.", ""], ["Calabresi", "Peter A.", ""], ["Pham", "Dzung L.", ""]]}, {"id": "1803.09179", "submitter": "Matthias Nie{\\ss}ner", "authors": "Andreas R\\\"ossler, Davide Cozzolino, Luisa Verdoliva, Christian Riess,\n  Justus Thies, Matthias Nie{\\ss}ner", "title": "FaceForensics: A Large-scale Video Dataset for Forgery Detection in\n  Human Faces", "comments": "Video: https://youtu.be/Tle7YaPkO_k", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With recent advances in computer vision and graphics, it is now possible to\ngenerate videos with extremely realistic synthetic faces, even in real time.\nCountless applications are possible, some of which raise a legitimate alarm,\ncalling for reliable detectors of fake videos. In fact, distinguishing between\noriginal and manipulated video can be a challenge for humans and computers\nalike, especially when the videos are compressed or have low resolution, as it\noften happens on social networks. Research on the detection of face\nmanipulations has been seriously hampered by the lack of adequate datasets. To\nthis end, we introduce a novel face manipulation dataset of about half a\nmillion edited images (from over 1000 videos). The manipulations have been\ngenerated with a state-of-the-art face editing approach. It exceeds all\nexisting video manipulation datasets by at least an order of magnitude. Using\nour new dataset, we introduce benchmarks for classical image forensic tasks,\nincluding classification and segmentation, considering videos compressed at\nvarious quality levels. In addition, we introduce a benchmark evaluation for\ncreating indistinguishable forgeries with known ground truth; for instance with\ngenerative refinement models.\n", "versions": [{"version": "v1", "created": "Sat, 24 Mar 2018 23:12:44 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["R\u00f6ssler", "Andreas", ""], ["Cozzolino", "Davide", ""], ["Verdoliva", "Luisa", ""], ["Riess", "Christian", ""], ["Thies", "Justus", ""], ["Nie\u00dfner", "Matthias", ""]]}, {"id": "1803.09180", "submitter": "Sicheng Zhao Dr.", "authors": "Sicheng Zhao, Bichen Wu, Joseph Gonzalez, Sanjit A. Seshia, Kurt\n  Keutzer", "title": "Unsupervised Domain Adaptation: from Simulation Engine to the RealWorld", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale labeled training datasets have enabled deep neural networks to\nexcel on a wide range of benchmark vision tasks. However, in many applications\nit is prohibitively expensive or time-consuming to obtain large quantities of\nlabeled data. To cope with limited labeled training data, many have attempted\nto directly apply models trained on a large-scale labeled source domain to\nanother sparsely labeled target domain. Unfortunately, direct transfer across\ndomains often performs poorly due to domain shift and dataset bias. Domain\nadaptation is the machine learning paradigm that aims to learn a model from a\nsource domain that can perform well on a different (but related) target domain.\nIn this paper, we summarize and compare the latest unsupervised domain\nadaptation methods in computer vision applications. We classify the non-deep\napproaches into sample re-weighting and intermediate subspace transformation\ncategories, while the deep strategy includes discrepancy-based methods,\nadversarial generative models, adversarial discriminative models and\nreconstruction-based methods. We also discuss some potential directions.\n", "versions": [{"version": "v1", "created": "Sat, 24 Mar 2018 23:34:06 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Zhao", "Sicheng", ""], ["Wu", "Bichen", ""], ["Gonzalez", "Joseph", ""], ["Seshia", "Sanjit A.", ""], ["Keutzer", "Kurt", ""]]}, {"id": "1803.09189", "submitter": "Chenxi Liu", "authors": "Yu-Siang Wang, Chenxi Liu, Xiaohui Zeng, Alan Yuille", "title": "Scene Graph Parsing as Dependency Parsing", "comments": "To appear in NAACL 2018 as oral. Code is available at\n  https://github.com/Yusics/bist-parser/tree/sgparser", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the problem of parsing structured knowledge graphs\nfrom textual descriptions. In particular, we consider the scene graph\nrepresentation that considers objects together with their attributes and\nrelations: this representation has been proved useful across a variety of\nvision and language applications. We begin by introducing an alternative but\nequivalent edge-centric view of scene graphs that connect to dependency parses.\nTogether with a careful redesign of label and action space, we combine the\ntwo-stage pipeline used in prior work (generic dependency parsing followed by\nsimple post-processing) into one, enabling end-to-end training. The scene\ngraphs generated by our learned neural dependency parser achieve an F-score\nsimilarity of 49.67% to ground truth graphs on our evaluation set, surpassing\nbest previous approaches by 5%. We further demonstrate the effectiveness of our\nlearned parser on image retrieval applications.\n", "versions": [{"version": "v1", "created": "Sun, 25 Mar 2018 01:53:29 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Wang", "Yu-Siang", ""], ["Liu", "Chenxi", ""], ["Zeng", "Xiaohui", ""], ["Yuille", "Alan", ""]]}, {"id": "1803.09196", "submitter": "Mariya I. Vasileva", "authors": "Mariya I. Vasileva, Bryan A. Plummer, Krishna Dusad, Shreya Rajpal,\n  Ranjitha Kumar, and David Forsyth", "title": "Learning Type-Aware Embeddings for Fashion Compatibility", "comments": "Accepted at ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Outfits in online fashion data are composed of items of many different types\n(e.g. top, bottom, shoes) that share some stylistic relationship with one\nanother. A representation for building outfits requires a method that can learn\nboth notions of similarity (for example, when two tops are interchangeable) and\ncompatibility (items of possibly different type that can go together in an\noutfit). This paper presents an approach to learning an image embedding that\nrespects item type, and jointly learns notions of item similarity and\ncompatibility in an end-to-end model. To evaluate the learned representation,\nwe crawled 68,306 outfits created by users on the Polyvore website. Our\napproach obtains 3-5% improvement over the state-of-the-art on outfit\ncompatibility prediction and fill-in-the-blank tasks using our dataset, as well\nas an established smaller dataset, while supporting a variety of useful\nqueries.\n", "versions": [{"version": "v1", "created": "Sun, 25 Mar 2018 02:50:58 GMT"}, {"version": "v2", "created": "Fri, 27 Jul 2018 20:27:54 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Vasileva", "Mariya I.", ""], ["Plummer", "Bryan A.", ""], ["Dusad", "Krishna", ""], ["Rajpal", "Shreya", ""], ["Kumar", "Ranjitha", ""], ["Forsyth", "David", ""]]}, {"id": "1803.09202", "submitter": "Sina Honari", "authors": "Joel Ruben Antony Moniz, Christopher Beckham, Simon Rajotte, Sina\n  Honari, Christopher Pal", "title": "Unsupervised Depth Estimation, 3D Face Rotation and Replacement", "comments": "Depth Estimation, Face Rotation, Face Swap, 32nd Conference on Neural\n  Information Processing Systems (NIPS 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an unsupervised approach for learning to estimate three\ndimensional (3D) facial structure from a single image while also predicting 3D\nviewpoint transformations that match a desired pose and facial geometry. We\nachieve this by inferring the depth of facial keypoints of an input image in an\nunsupervised manner, without using any form of ground-truth depth information.\nWe show how it is possible to use these depths as intermediate computations\nwithin a new backpropable loss to predict the parameters of a 3D affine\ntransformation matrix that maps inferred 3D keypoints of an input face to the\ncorresponding 2D keypoints on a desired target facial geometry or pose. Our\nresulting approach, called DepthNets, can therefore be used to infer plausible\n3D transformations from one face pose to another, allowing faces to be\nfrontalized, transformed into 3D models or even warped to another pose and\nfacial geometry. Lastly, we identify certain shortcomings with our formulation,\nand explore adversarial image translation techniques as a post-processing step\nto re-synthesize complete head shots for faces re-targeted to different poses\nor identities.\n", "versions": [{"version": "v1", "created": "Sun, 25 Mar 2018 05:07:11 GMT"}, {"version": "v2", "created": "Wed, 30 May 2018 23:03:13 GMT"}, {"version": "v3", "created": "Mon, 1 Oct 2018 21:44:48 GMT"}, {"version": "v4", "created": "Tue, 6 Nov 2018 23:07:43 GMT"}, {"version": "v5", "created": "Mon, 24 Dec 2018 01:51:13 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Moniz", "Joel Ruben Antony", ""], ["Beckham", "Christopher", ""], ["Rajotte", "Simon", ""], ["Honari", "Sina", ""], ["Pal", "Christopher", ""]]}, {"id": "1803.09208", "submitter": "Jing Zhang", "authors": "Jing Zhang and Wanqing Li and Philip Ogunbona", "title": "Unsupervised Domain Adaptation: A Multi-task Learning-based Method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel multi-task learning-based method for unsupervised\ndomain adaptation. Specifically, the source and target domain classifiers are\njointly learned by considering the geometry of target domain and the divergence\nbetween the source and target domains based on the concept of multi-task\nlearning. Two novel algorithms are proposed upon the method using Regularized\nLeast Squares and Support Vector Machines respectively. Experiments on both\nsynthetic and real world cross domain recognition tasks have shown that the\nproposed methods outperform several state-of-the-art domain adaptation methods.\n", "versions": [{"version": "v1", "created": "Sun, 25 Mar 2018 06:20:00 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Zhang", "Jing", ""], ["Li", "Wanqing", ""], ["Ogunbona", "Philip", ""]]}, {"id": "1803.09210", "submitter": "Jing Zhang", "authors": "Jing Zhang and Zewei Ding and Wanqing Li and Philip Ogunbona", "title": "Importance Weighted Adversarial Nets for Partial Domain Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes an importance weighted adversarial nets-based method for\nunsupervised domain adaptation, specific for partial domain adaptation where\nthe target domain has less number of classes compared to the source domain.\nPrevious domain adaptation methods generally assume the identical label spaces,\nsuch that reducing the distribution divergence leads to feasible knowledge\ntransfer. However, such an assumption is no longer valid in a more realistic\nscenario that requires adaptation from a larger and more diverse source domain\nto a smaller target domain with less number of classes. This paper extends the\nadversarial nets-based domain adaptation and proposes a novel adversarial\nnets-based partial domain adaptation method to identify the source samples that\nare potentially from the outlier classes and, at the same time, reduce the\nshift of shared classes between domains.\n", "versions": [{"version": "v1", "created": "Sun, 25 Mar 2018 06:47:22 GMT"}, {"version": "v2", "created": "Wed, 28 Mar 2018 21:04:48 GMT"}], "update_date": "2018-03-30", "authors_parsed": [["Zhang", "Jing", ""], ["Ding", "Zewei", ""], ["Li", "Wanqing", ""], ["Ogunbona", "Philip", ""]]}, {"id": "1803.09218", "submitter": "Dong-Qing Zhang", "authors": "Dong-Qing Zhang", "title": "Image Recognition Using Scale Recurrent Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Network(CNN) has been widely used for image recognition\nwith great success. However, there are a number of limitations of the current\nCNN based image recognition paradigm. First, the receptive field of CNN is\ngenerally fixed, which limits its recognition capacity when the input image is\nvery large. Second, it lacks the computational scalability for dealing with\nimages with different sizes. Third, it is quite different from human visual\nsystem for image recognition, which involves both feadforward and recurrent\nproprocessing. This paper proposes a different paradigm of image recognition,\nwhich can take advantages of variable scales of the input images, has more\ncomputational scalabilities, and is more similar to image recognition by human\nvisual system. It is based on recurrent neural network (RNN) defined on image\nscale with an embeded base CNN, which is named Scale Recurrent Neural\nNetwork(SRNN). This RNN based approach makes it easier to deal with images with\nvariable sizes, and allows us to borrow existing RNN techniques, such as LSTM\nand GRU, to further enhance the recognition accuracy. Our experiments show that\nthe recognition accuracy of a base CNN can be significantly boosted using the\nproposed SRNN models. It also significantly outperforms the scale ensemble\nmethod, which integrate the results of performing CNN to the input image at\ndifferent scales, although the computational overhead of using SRNN is\nnegligible.\n", "versions": [{"version": "v1", "created": "Sun, 25 Mar 2018 09:16:55 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Zhang", "Dong-Qing", ""]]}, {"id": "1803.09256", "submitter": "Dezhi Peng", "authors": "Dezhi Peng, Zikai Sun, Zirong Chen, Zirui Cai, Lele Xie, Lianwen Jin", "title": "Detecting Heads using Feature Refine Net and Cascaded Multi-Scale\n  Architecture", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a method that can accurately detect heads especially\nsmall heads under the indoor scene. To achieve this, we propose a novel method,\nFeature Refine Net (FRN), and a cascaded multi-scale architecture. FRN exploits\nthe multi-scale hierarchical features created by deep convolutional neural\nnetworks. The proposed channel weighting method enables FRN to make use of\nfeatures alternatively and effectively. To improve the performance of small\nhead detection, we propose a cascaded multi-scale architecture which has two\ndetectors. One called global detector is responsible for detecting large\nobjects and acquiring the global distribution information. The other called\nlocal detector is designed for small objects detection and makes use of the\ninformation provided by global detector. Due to the lack of head detection\ndatasets, we have collected and labeled a new large dataset named SCUT-HEAD\nwhich includes 4405 images with 111251 heads annotated. Experiments show that\nour method has achieved state-of-the-art performance on SCUT-HEAD.\n", "versions": [{"version": "v1", "created": "Sun, 25 Mar 2018 14:02:51 GMT"}, {"version": "v2", "created": "Thu, 10 May 2018 08:46:20 GMT"}, {"version": "v3", "created": "Mon, 22 Oct 2018 13:42:20 GMT"}, {"version": "v4", "created": "Sat, 24 Nov 2018 01:53:48 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Peng", "Dezhi", ""], ["Sun", "Zikai", ""], ["Chen", "Zirong", ""], ["Cai", "Zirui", ""], ["Xie", "Lele", ""], ["Jin", "Lianwen", ""]]}, {"id": "1803.09263", "submitter": "Kangxue Yin", "authors": "Kangxue Yin, Hui Huang, Daniel Cohen-Or, Hao Zhang", "title": "P2P-NET: Bidirectional Point Displacement Net for Shape Transform", "comments": "siggraph revision is done. 13 pages", "journal-ref": "ACM Transactions on Graphics(Proc. of SIGGRAPH), 37(4),\n  152:1-152:13, 2018", "doi": "10.1145/3197517.3201288", "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce P2P-NET, a general-purpose deep neural network which learns\ngeometric transformations between point-based shape representations from two\ndomains, e.g., meso-skeletons and surfaces, partial and complete scans, etc.\nThe architecture of the P2P-NET is that of a bi-directional point displacement\nnetwork, which transforms a source point set to a target point set with the\nsame cardinality, and vice versa, by applying point-wise displacement vectors\nlearned from data. P2P-NET is trained on paired shapes from the source and\ntarget domains, but without relying on point-to-point correspondences between\nthe source and target point sets. The training loss combines two\nuni-directional geometric losses, each enforcing a shape-wise similarity\nbetween the predicted and the target point sets, and a cross-regularization\nterm to encourage consistency between displacement vectors going in opposite\ndirections. We develop and present several different applications enabled by\nour general-purpose bidirectional P2P-NET to highlight the effectiveness,\nversatility, and potential of our network in solving a variety of point-based\nshape transformation problems.\n", "versions": [{"version": "v1", "created": "Sun, 25 Mar 2018 14:30:51 GMT"}, {"version": "v2", "created": "Wed, 11 Apr 2018 07:52:48 GMT"}, {"version": "v3", "created": "Thu, 26 Apr 2018 10:12:56 GMT"}, {"version": "v4", "created": "Tue, 15 May 2018 08:14:30 GMT"}], "update_date": "2018-05-16", "authors_parsed": [["Yin", "Kangxue", ""], ["Huang", "Hui", ""], ["Cohen-Or", "Daniel", ""], ["Zhang", "Hao", ""]]}, {"id": "1803.09326", "submitter": "Yinda Zhang", "authors": "Yinda Zhang, Thomas Funkhouser", "title": "Deep Depth Completion of a Single RGB-D Image", "comments": "Accepted by CVPR2018 (Spotlight). Project webpage:\n  http://deepcompletion.cs.princeton.edu/ This version includes supplementary\n  materials which provide more implementation details, quantitative evaluation,\n  and qualitative results. Due to file size limit, please check project website\n  for high-res paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of our work is to complete the depth channel of an RGB-D image.\nCommodity-grade depth cameras often fail to sense depth for shiny, bright,\ntransparent, and distant surfaces. To address this problem, we train a deep\nnetwork that takes an RGB image as input and predicts dense surface normals and\nocclusion boundaries. Those predictions are then combined with raw depth\nobservations provided by the RGB-D camera to solve for depths for all pixels,\nincluding those missing in the original observation. This method was chosen\nover others (e.g., inpainting depths directly) as the result of extensive\nexperiments with a new depth completion benchmark dataset, where holes are\nfilled in training data through the rendering of surface reconstructions\ncreated from multiview RGB-D scans. Experiments with different network inputs,\ndepth representations, loss functions, optimization methods, inpainting\nmethods, and deep depth estimation networks show that our proposed approach\nprovides better depth completions than these alternatives.\n", "versions": [{"version": "v1", "created": "Sun, 25 Mar 2018 20:07:10 GMT"}, {"version": "v2", "created": "Wed, 2 May 2018 03:40:10 GMT"}], "update_date": "2018-05-03", "authors_parsed": [["Zhang", "Yinda", ""], ["Funkhouser", "Thomas", ""]]}, {"id": "1803.09331", "submitter": "Xingyi Zhou", "authors": "Xingyi Zhou, Arjun Karpur, Linjie Luo, Qixing Huang", "title": "StarMap for Category-Agnostic Keypoint and Viewpoint Estimation", "comments": "ECCV 2018. Supplementary material with more qualitative results and\n  higher resolution is available on the code page", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic keypoints provide concise abstractions for a variety of visual\nunderstanding tasks. Existing methods define semantic keypoints separately for\neach category with a fixed number of semantic labels in fixed indices. As a\nresult, this keypoint representation is in-feasible when objects have a varying\nnumber of parts, e.g. chairs with varying number of legs. We propose a\ncategory-agnostic keypoint representation, which combines a multi-peak heatmap\n(StarMap) for all the keypoints and their corresponding features as 3D\nlocations in the canonical viewpoint (CanViewFeature) defined for each\ninstance. Our intuition is that the 3D locations of the keypoints in canonical\nobject views contain rich semantic and compositional information. Using our\nflexible representation, we demonstrate competitive performance in keypoint\ndetection and localization compared to category-specific state-of-the-art\nmethods. Moreover, we show that when augmented with an additional depth channel\n(DepthMap) to lift the 2D keypoints to 3D, our representation can achieve\nstate-of-the-art results in viewpoint estimation. Finally, we show that our\ncategory-agnostic keypoint representation can be generalized to novel\ncategories.\n", "versions": [{"version": "v1", "created": "Sun, 25 Mar 2018 20:28:53 GMT"}, {"version": "v2", "created": "Thu, 26 Jul 2018 04:31:28 GMT"}], "update_date": "2018-07-27", "authors_parsed": [["Zhou", "Xingyi", ""], ["Karpur", "Arjun", ""], ["Luo", "Linjie", ""], ["Huang", "Qixing", ""]]}, {"id": "1803.09340", "submitter": "Giles Tetteh", "authors": "Giles Tetteh and Velizar Efremov and Nils D. Forkert and Matthias\n  Schneider and Jan Kirschke and Bruno Weber and Claus Zimmer and Marie Piraud\n  and Bjoern H. Menze", "title": "DeepVesselNet: Vessel Segmentation, Centerline Prediction, and\n  Bifurcation Detection in 3-D Angiographic Volumes", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present DeepVesselNet, an architecture tailored to the challenges faced\nwhen extracting vessel networks or trees and corresponding features in 3-D\nangiographic volumes using deep learning. We discuss the problems of low\nexecution speed and high memory requirements associated with full 3-D\nconvolutional networks, high-class imbalance arising from the low percentage of\nvessel voxels, and unavailability of accurately annotated training data - and\noffer solutions as the building blocks of DeepVesselNet.\n  First, we formulate 2-D orthogonal cross-hair filters which make use of 3-D\ncontext information at a reduced computational burden. Second, we introduce a\nclass balancing cross-entropy loss function with false positive rate correction\nto handle the high-class imbalance and high false positive rate problems\nassociated with existing loss functions. Finally, we generate synthetic dataset\nusing a computational angiogenesis model capable of generating vascular trees\nunder physiological constraints on local network structure and topology and use\nthese data for transfer learning.\n  DeepVesselNet is optimized for segmenting and analyzing vessels, and we test\nthe performance on a range of angiographic volumes including clinical MRA data\nof the human brain, as well as X-ray tomographic microscopy scans of the rat\nbrain. Our experiments show that, by replacing 3-D filters with cross-hair\nfilters in our network, we achieve over 23% improvement in speed, lower memory\nfootprint, lower network complexity which prevents overfitting and comparable\naccuracy (with a Cox-Wilcoxon paired sample significance test p-value of 0.07\nwhen compared to full 3-D filters). Our class balancing metric is crucial for\ntraining the network and transfer learning with synthetic data is an efficient,\nrobust, and very generalizable approach leading to a network that excels in a\nvariety of angiography segmentation tasks.\n", "versions": [{"version": "v1", "created": "Sun, 25 Mar 2018 21:09:36 GMT"}, {"version": "v2", "created": "Mon, 19 Nov 2018 12:44:07 GMT"}, {"version": "v3", "created": "Tue, 13 Aug 2019 12:50:47 GMT"}], "update_date": "2019-08-14", "authors_parsed": [["Tetteh", "Giles", ""], ["Efremov", "Velizar", ""], ["Forkert", "Nils D.", ""], ["Schneider", "Matthias", ""], ["Kirschke", "Jan", ""], ["Weber", "Bruno", ""], ["Zimmer", "Claus", ""], ["Piraud", "Marie", ""], ["Menze", "Bjoern H.", ""]]}, {"id": "1803.09354", "submitter": "Giacomo Tarroni", "authors": "Giacomo Tarroni, Ozan Oktay, Wenjia Bai, Andreas Schuh, Hideaki\n  Suzuki, Jonathan Passerat-Palmbach, Antonio de Marvao, Declan P. O'Regan,\n  Stuart Cook, Ben Glocker, Paul M. Matthews, Daniel Rueckert", "title": "Learning-Based Quality Control for Cardiac MR Images", "comments": null, "journal-ref": "IEEE Transactions on Medical Imaging Nov 2018", "doi": "10.1109/TMI.2018.2878509", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The effectiveness of a cardiovascular magnetic resonance (CMR) scan depends\non the ability of the operator to correctly tune the acquisition parameters to\nthe subject being scanned and on the potential occurrence of imaging artefacts\nsuch as cardiac and respiratory motion. In the clinical practice, a quality\ncontrol step is performed by visual assessment of the acquired images: however,\nthis procedure is strongly operator-dependent, cumbersome and sometimes\nincompatible with the time constraints in clinical settings and large-scale\nstudies. We propose a fast, fully-automated, learning-based quality control\npipeline for CMR images, specifically for short-axis image stacks. Our pipeline\nperforms three important quality checks: 1) heart coverage estimation, 2)\ninter-slice motion detection, 3) image contrast estimation in the cardiac\nregion. The pipeline uses a hybrid decision forest method - integrating both\nregression and structured classification models - to extract landmarks as well\nas probabilistic segmentation maps from both long- and short-axis images as a\nbasis to perform the quality checks. The technique was tested on up to 3000\ncases from the UK Biobank as well as on 100 cases from the UK Digital Heart\nProject, and validated against manual annotations and visual inspections\nperformed by expert interpreters. The results show the capability of the\nproposed pipeline to correctly detect incomplete or corrupted scans (e.g. on UK\nBiobank, sensitivity and specificity respectively 88% and 99% for heart\ncoverage estimation, 85% and 95% for motion detection), allowing their\nexclusion from the analysed dataset or the triggering of a new acquisition.\n", "versions": [{"version": "v1", "created": "Sun, 25 Mar 2018 21:49:55 GMT"}, {"version": "v2", "created": "Sat, 15 Sep 2018 09:36:00 GMT"}], "update_date": "2019-02-08", "authors_parsed": [["Tarroni", "Giacomo", ""], ["Oktay", "Ozan", ""], ["Bai", "Wenjia", ""], ["Schuh", "Andreas", ""], ["Suzuki", "Hideaki", ""], ["Passerat-Palmbach", "Jonathan", ""], ["de Marvao", "Antonio", ""], ["O'Regan", "Declan P.", ""], ["Cook", "Stuart", ""], ["Glocker", "Ben", ""], ["Matthews", "Paul M.", ""], ["Rueckert", "Daniel", ""]]}, {"id": "1803.09359", "submitter": "Lingfeng Zhang", "authors": "Lingfeng Zhang, Pengfei Dou, Ioannis A. Kakadiaris", "title": "A Face Recognition Signature Combining Patch-based Features with Soft\n  Facial Attributes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on improving face recognition performance with a new\nsignature combining implicit facial features with explicit soft facial\nattributes. This signature has two components: the existing patch-based\nfeatures and the soft facial attributes. A deep convolutional neural network\nadapted from state-of-the-art networks is used to learn the soft facial\nattributes. Then, a signature matcher is introduced that merges the\ncontributions of both patch-based features and the facial attributes. In this\nmatcher, the matching scores computed from patch-based features and the facial\nattributes are combined to obtain a final matching score. The matcher is also\nextended so that different weights are assigned to different facial attributes.\nThe proposed signature and matcher have been evaluated with the UR2D system on\nthe UHDB31 and IJB-A datasets. The experimental results indicate that the\nproposed signature achieve better performance than using only patch-based\nfeatures. The Rank-1 accuracy is improved significantly by 4% and 0.37% on the\ntwo datasets when compared with the UR2D system.\n", "versions": [{"version": "v1", "created": "Sun, 25 Mar 2018 22:32:18 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Zhang", "Lingfeng", ""], ["Dou", "Pengfei", ""], ["Kakadiaris", "Ioannis A.", ""]]}, {"id": "1803.09374", "submitter": "Brendan Duke", "authors": "Brendan Duke and Graham W. Taylor", "title": "Generalized Hadamard-Product Fusion Operators for Visual Question\n  Answering", "comments": "8 pages, 3 figures. To appear in CRV, 2018, 15th Canadian Conference\n  on Computer and Robot Vision", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a generalized class of multimodal fusion operators for the task of\nvisual question answering (VQA). We identify generalizations of existing\nmultimodal fusion operators based on the Hadamard product, and show that\nspecific non-trivial instantiations of this generalized fusion operator exhibit\nsuperior performance in terms of OpenEnded accuracy on the VQA task. In\nparticular, we introduce Nonlinearity Ensembling, Feature Gating, and\npost-fusion neural network layers as fusion operator components, culminating in\nan absolute percentage point improvement of $1.1\\%$ on the VQA 2.0 test-dev set\nover baseline fusion operators, which use the same features as input. We use\nour findings as evidence that our generalized class of fusion operators could\nlead to the discovery of even superior task-specific operators when used as a\nsearch space in an architecture search over fusion operators.\n", "versions": [{"version": "v1", "created": "Mon, 26 Mar 2018 00:30:34 GMT"}, {"version": "v2", "created": "Fri, 6 Apr 2018 15:18:26 GMT"}], "update_date": "2018-04-09", "authors_parsed": [["Duke", "Brendan", ""], ["Taylor", "Graham W.", ""]]}, {"id": "1803.09375", "submitter": "Harrison Nguyen", "authors": "Harrison Nguyen, Richard W. Morris, Anthony W. Harris, Mayuresh S.\n  Korgoankar and Fabio Ramos", "title": "Correcting differences in multi-site neuroimaging data using Generative\n  Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Magnetic Resonance Imaging (MRI) of the brain has been used to investigate a\nwide range of neurological disorders, but data acquisition can be expensive,\ntime-consuming, and inconvenient. Multi-site studies present a valuable\nopportunity to advance research by pooling data in order to increase\nsensitivity and statistical power. However images derived from MRI are\nsusceptible to both obvious and non-obvious differences between sites which can\nintroduce bias and subject variance, and so reduce statistical power. To\nrectify these differences, we propose a data driven approach using a deep\nlearning architecture known as generative adversarial networks (GANs). GANs\nlearn to estimate two distributions, and can then be used to transform examples\nfrom one distribution into the other distribution. Here we transform\nT1-weighted brain images collected from two different sites into MR images from\nthe same site. We evaluate whether our model can reduce site-specific\ndifferences without loss of information related to gender (male, female) or\nclinical diagnosis (schizophrenia, bipolar disorder, healthy). When trained\nappropriately, our model is able to normalise imaging sets to a common scanner\nset with less information loss compared to current approaches. An important\nadvantage is our method can be treated as a black box that does not require any\nknowledge of the sources of bias but only needs at least two distinct imaging\nsets.\n", "versions": [{"version": "v1", "created": "Mon, 26 Mar 2018 00:49:50 GMT"}, {"version": "v2", "created": "Thu, 12 Apr 2018 06:01:50 GMT"}], "update_date": "2018-04-13", "authors_parsed": [["Nguyen", "Harrison", ""], ["Morris", "Richard W.", ""], ["Harris", "Anthony W.", ""], ["Korgoankar", "Mayuresh S.", ""], ["Ramos", "Fabio", ""]]}, {"id": "1803.09386", "submitter": "Michael Teti", "authors": "Michael Teti, William Edward Hahn, Shawn Martin, Christopher Teti, and\n  Elan Barenholtz", "title": "A Systematic Comparison of Deep Learning Architectures in an Autonomous\n  Vehicle", "comments": "16 pages, 14 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-driving technology is advancing rapidly --- albeit with significant\nchallenges and limitations. This progress is largely due to recent developments\nin deep learning algorithms. To date, however, there has been no systematic\ncomparison of how different deep learning architectures perform at such tasks,\nor an attempt to determine a correlation between classification performance and\nperformance in an actual vehicle, a potentially critical factor in developing\nself-driving systems. Here, we introduce the first controlled comparison of\nmultiple deep-learning architectures in an end-to-end autonomous driving task\nacross multiple testing conditions. We compared performance, under identical\ndriving conditions, across seven architectures including a fully-connected\nnetwork, a simple 2 layer CNN, AlexNet, VGG-16, Inception-V3, ResNet, and an\nLSTM by assessing the number of laps each model was able to successfully\ncomplete without crashing while traversing an indoor racetrack. We compared\nperformance across models when the conditions exactly matched those in training\nas well as when the local environment and track were configured differently and\nobjects that were not included in the training dataset were placed on the track\nin various positions. In addition, we considered performance using several\ndifferent data types for training and testing including single grayscale and\ncolor frames, and multiple grayscale frames stacked together in sequence. With\nthe exception of a fully-connected network, all models performed reasonably\nwell (around or above 80\\%) and most very well (~95\\%) on at least one input\ntype but with considerable variation across models and inputs. Overall,\nAlexNet, operating on single color frames as input, achieved the best level of\nperformance (100\\% success rate in phase one and 55\\% in phase two) while\nVGG-16 performed well most consistently across image types.\n", "versions": [{"version": "v1", "created": "Mon, 26 Mar 2018 01:58:07 GMT"}, {"version": "v2", "created": "Sat, 13 Oct 2018 00:04:29 GMT"}], "update_date": "2018-10-16", "authors_parsed": [["Teti", "Michael", ""], ["Hahn", "William Edward", ""], ["Martin", "Shawn", ""], ["Teti", "Christopher", ""], ["Barenholtz", "Elan", ""]]}, {"id": "1803.09413", "submitter": "Sumita Mishra", "authors": "Sachin Kumar, Sumita Mishra, Pooja Khanna, Pragya", "title": "Precision Sugarcane Monitoring Using SVM Classifier", "comments": "This is a pre-print of an article published in [Procedia Computer\n  Science 2017]", "journal-ref": "Procedia Computer Science,2017,vol.122,pp. 881-887", "doi": "10.1016/j.procs.2017.11.450", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  India is agriculture based economy and sugarcane is one of the major crops\nproduced in northern India. Productivity of sugarcane decreases due to\ninappropriate soil conditions and infections caused by various types of\ndiseases , timely and accurate disease diagnosis, plays an important role\ntowards optimizing crop yield. This paper presents a system model for\nmonitoring of sugarcane crop, the proposed model continuously monitor\nparameters (temperature, humidity and moisture) responsible for healthy growth\nof the crop in addition KNN clustering along with SVM classifier is utilized\nfor infection identification if any through images obtained at regular\nintervals. The data has been transmitted wirelessly from the site to the\ncontrol unit. Model achieves an accuracy of 96% on a sample of 200 images, the\nmodel was tested at Lolai, near Malhaur, Gomti Nagar Extension.\n", "versions": [{"version": "v1", "created": "Mon, 26 Mar 2018 05:05:25 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Kumar", "Sachin", ""], ["Mishra", "Sumita", ""], ["Khanna", "Pooja", ""], ["Pragya", "", ""]]}, {"id": "1803.09420", "submitter": "Nati Ofir", "authors": "Nati Ofir and Yosi Keller", "title": "Multi-scale Processing of Noisy Images using Edge Preservation Losses", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Noisy images processing is a fundamental task of computer vision. The first\nexample is the detection of faint edges in noisy images, a challenging problem\nstudied in the last decades. A recent study introduced a fast method to detect\nfaint edges in the highest accuracy among all the existing approaches. Their\ncomplexity is nearly linear in the image's pixels and their runtime is seconds\nfor a noisy image. Their approach utilizes a multi-scale binary partitioning of\nthe image. By utilizing the multi-scale U-net architecture, we show in this\npaper that their method can be dramatically improved in both aspects of run\ntime and accuracy. By training the network on a dataset of binary images, we\ndeveloped an approach for faint edge detection that works in a linear\ncomplexity. Our runtime of a noisy image is milliseconds on a GPU. Even though\nour method is orders of magnitude faster, we still achieve higher accuracy of\ndetection under many challenging scenarios. In addition, we show that our\napproach to performing multi-scale preprocessing of noisy images using U-net\nimproves the ability to perform other vision tasks under the presence of noise.\nWe prove it on the problems of noisy objects classification and classical image\ndenoising. We show that multi-scale denoising can be carried out by a novel\nedge preservation loss. As our experiments show, we achieve high-quality\nresults in the three aspects of faint edge detection, noisy image\nclassification and natural image denoising.\n", "versions": [{"version": "v1", "created": "Mon, 26 Mar 2018 05:39:31 GMT"}, {"version": "v2", "created": "Mon, 7 Jan 2019 06:12:52 GMT"}, {"version": "v3", "created": "Sun, 3 Feb 2019 10:16:28 GMT"}, {"version": "v4", "created": "Sun, 10 Mar 2019 08:44:39 GMT"}, {"version": "v5", "created": "Thu, 21 Mar 2019 14:40:45 GMT"}], "update_date": "2019-03-22", "authors_parsed": [["Ofir", "Nati", ""], ["Keller", "Yosi", ""]]}, {"id": "1803.09437", "submitter": "Haihua Lu", "authors": "Haihua Lu, Hai Xu, Li Zhang, Yong Zhao", "title": "Cascaded multi-scale and multi-dimension convolutional neural network\n  for stereo matching", "comments": "13 pages, 2 figures, 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks(CNN) have been shown to perform better than the\nconventional stereo algorithms for stereo estimation. Numerous efforts focus on\nthe pixel-wise matching cost computation, which is the important building block\nfor many start-of-the-art algorithms. However, those architectures are limited\nto small and single scale receptive fields and use traditional methods for cost\naggregation or even ignore cost aggregation. Differently we take them both into\nconsideration. Firstly, we propose a new multi-scale matching cost computation\nsub-network, in which two different sizes of receptive fields are implemented\nparallelly. In this way, the network can make the best use of both variants and\nbalance the trade-off between the increase of receptive field and the loss of\ndetail. Furthermore, we show that our multi-dimension aggregation sub-network\nwhich containing 2D convolution and 3D convolution operations can provide rich\ncontext and semantic information for estimating an accurate initial disparity.\nFinally, experiments on challenging stereo benchmark KITTI demonstrate that the\nproposed method can achieve competitive results even without any additional\npost-processing.\n", "versions": [{"version": "v1", "created": "Mon, 26 Mar 2018 06:51:46 GMT"}, {"version": "v2", "created": "Tue, 17 Apr 2018 06:35:42 GMT"}], "update_date": "2018-04-18", "authors_parsed": [["Lu", "Haihua", ""], ["Xu", "Hai", ""], ["Zhang", "Li", ""], ["Zhao", "Yong", ""]]}, {"id": "1803.09448", "submitter": "Sota Shoman", "authors": "Sota Shoman, Tomohiro Mashita, Alexander Plopski, Photchara Ratsamee,\n  Yuki Uranishi, and Haruo Takemura", "title": "REST: Real-to-Synthetic Transform for Illumination Invariant Camera\n  Localization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate camera localization is an essential part of tracking systems.\nHowever, localization results are greatly affected by illumination. Including\ndata collected under various lighting conditions can improve the robustness of\nthe localization algorithm to lighting variation. However, this is very tedious\nand time consuming. By using synthesized images it is possible to easily\naccumulate a large variety of views under varying illumination and weather\nconditions. Despite continuously improving processing power and rendering\nalgorithms, synthesized images do not perfectly match real images of the same\nscene, i.e. there exists a gap between real and synthesized images that also\naffects the accuracy of camera localization. To reduce the impact of this gap,\nwe introduce \"REal-to-Synthetic Transform (REST).\" REST is an autoencoder-like\nnetwork that converts real features to their synthetic counterpart. The\nconverted features can then be matched against the accumulated database for\nrobust camera localization. In our experiments REST improved feature matching\naccuracy under variable lighting conditions by approximately 30%. Moreover, our\nsystem outperforms state of the art CNN-based camera localization methods\ntrained with synthetic images. We believe our method could be used to\ninitialize local tracking and to simplify data accumulation for lighting robust\nlocalization.\n", "versions": [{"version": "v1", "created": "Mon, 26 Mar 2018 07:36:11 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Shoman", "Sota", ""], ["Mashita", "Tomohiro", ""], ["Plopski", "Alexander", ""], ["Ratsamee", "Photchara", ""], ["Uranishi", "Yuki", ""], ["Takemura", "Haruo", ""]]}, {"id": "1803.09453", "submitter": "Linchao Bao", "authors": "Linchao Bao, Baoyuan Wu, Wei Liu", "title": "CNN in MRF: Video Object Segmentation via Inference in A CNN-Based\n  Higher-Order Spatio-Temporal MRF", "comments": "CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of video object segmentation, where the\ninitial object mask is given in the first frame of an input video. We propose a\nnovel spatio-temporal Markov Random Field (MRF) model defined over pixels to\nhandle this problem. Unlike conventional MRF models, the spatial dependencies\namong pixels in our model are encoded by a Convolutional Neural Network (CNN).\nSpecifically, for a given object, the probability of a labeling to a set of\nspatially neighboring pixels can be predicted by a CNN trained for this\nspecific object. As a result, higher-order, richer dependencies among pixels in\nthe set can be implicitly modeled by the CNN. With temporal dependencies\nestablished by optical flow, the resulting MRF model combines both spatial and\ntemporal cues for tackling video object segmentation. However, performing\ninference in the MRF model is very difficult due to the very high-order\ndependencies. To this end, we propose a novel CNN-embedded algorithm to perform\napproximate inference in the MRF. This algorithm proceeds by alternating\nbetween a temporal fusion step and a feed-forward CNN step. When initialized\nwith an appearance-based one-shot segmentation CNN, our model outperforms the\nwinning entries of the DAVIS 2017 Challenge, without resorting to model\nensembling or any dedicated detectors.\n", "versions": [{"version": "v1", "created": "Mon, 26 Mar 2018 07:56:43 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Bao", "Linchao", ""], ["Wu", "Baoyuan", ""], ["Liu", "Wei", ""]]}, {"id": "1803.09454", "submitter": "Zheng Hui", "authors": "Zheng Hui, Xiumei Wang, Xinbo Gao", "title": "Fast and Accurate Single Image Super-Resolution via Information\n  Distillation Network", "comments": "To appear in CVPR2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, deep convolutional neural networks (CNNs) have been demonstrated\nremarkable progress on single image super-resolution. However, as the depth and\nwidth of the networks increase, CNN-based super-resolution methods have been\nfaced with the challenges of computational complexity and memory consumption in\npractice. In order to solve the above questions, we propose a deep but compact\nconvolutional network to directly reconstruct the high resolution image from\nthe original low resolution image. In general, the proposed model consists of\nthree parts, which are feature extraction block, stacked information\ndistillation blocks and reconstruction block respectively. By combining an\nenhancement unit with a compression unit into a distillation block, the local\nlong and short-path features can be effectively extracted. Specifically, the\nproposed enhancement unit mixes together two different types of features and\nthe compression unit distills more useful information for the sequential\nblocks. In addition, the proposed network has the advantage of fast execution\ndue to the comparatively few numbers of filters per layer and the use of group\nconvolution. Experimental results demonstrate that the proposed method is\nsuperior to the state-of-the-art methods, especially in terms of time\nperformance.\n", "versions": [{"version": "v1", "created": "Mon, 26 Mar 2018 07:56:54 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Hui", "Zheng", ""], ["Wang", "Xiumei", ""], ["Gao", "Xinbo", ""]]}, {"id": "1803.09466", "submitter": "Libing Geng", "authors": "Libing Geng, Yan Pan, Jikai Chen, Hanjiang Lai", "title": "Regularizing Deep Hashing Networks Using GAN Generated Fake Images", "comments": "I need to modify the experiments", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, deep-networks-based hashing (deep hashing) has become a leading\napproach for large-scale image retrieval. It aims to learn a compact bitwise\nrepresentation for images via deep networks, so that similar images are mapped\nto nearby hash codes. Since a deep network model usually has a large number of\nparameters, it may probably be too complicated for the training data we have,\nleading to model over-fitting. To address this issue, in this paper, we propose\na simple two-stage pipeline to learn deep hashing models, by regularizing the\ndeep hashing networks using fake images. The first stage is to generate fake\nimages from the original training set without extra data, via a generative\nadversarial network (GAN). In the second stage, we propose a deep architec-\nture to learn hash functions, in which we use a maximum-entropy based loss to\nincorporate the newly created fake images by the GAN. We show that this loss\nacts as a strong regularizer of the deep architecture, by penalizing\nlow-entropy output hash codes. This loss can also be interpreted as a model\nensemble by simultaneously training many network models with massive weight\nsharing but over different training sets. Empirical evaluation results on\nseveral benchmark datasets show that the proposed method has superior\nperformance gains over state-of-the-art hashing methods.\n", "versions": [{"version": "v1", "created": "Mon, 26 Mar 2018 08:30:18 GMT"}, {"version": "v2", "created": "Sun, 2 Sep 2018 16:24:44 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Geng", "Libing", ""], ["Pan", "Yan", ""], ["Chen", "Jikai", ""], ["Lai", "Hanjiang", ""]]}, {"id": "1803.09470", "submitter": "Uzair Nadeem", "authors": "Uzair Nadeem, Syed Afaq Ali Shah, Mohammed Bennamoun, Roberto Togneri,\n  Ferdous Sohel", "title": "Real Time Surveillance for Low Resolution and Limited-Data Scenarios: An\n  Image Set Classification Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel image set classification technique based on the\nconcept of linear regression. Unlike most other approaches, the proposed\ntechnique does not involve any training or feature extraction. The gallery\nimage sets are represented as subspaces in a high dimensional space. Class\nspecific gallery subspaces are used to estimate regression models for each\nimage of the test image set. Images of the test set are then projected on the\ngallery subspaces. Residuals, calculated using the Euclidean distance between\nthe original and the projected test images, are used as the distance metric.\nThree different strategies are devised to decide on the final class of the test\nimage set. We performed extensive evaluations of the proposed technique under\nthe challenges of low resolution, noise and less gallery data for the tasks of\nsurveillance, video-based face recognition and object recognition. Experiments\nshow that the proposed technique achieves a better classification accuracy and\na faster execution time compared to existing techniques especially under the\nchallenging conditions of low resolution and small gallery and test data.\n", "versions": [{"version": "v1", "created": "Mon, 26 Mar 2018 08:52:26 GMT"}, {"version": "v2", "created": "Sun, 3 Mar 2019 18:09:17 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Nadeem", "Uzair", ""], ["Shah", "Syed Afaq Ali", ""], ["Bennamoun", "Mohammed", ""], ["Togneri", "Roberto", ""], ["Sohel", "Ferdous", ""]]}, {"id": "1803.09474", "submitter": "Huangjie Yu", "authors": "Huangjie Yu, Guli Zhang, Yuanxi Ma, Yingliang Zhang, Jingyi Yu", "title": "Semantic See-Through Rendering on Light Fields", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel semantic light field (LF) refocusing technique that can\nachieve unprecedented see-through quality. Different from prior art, our\nsemantic see-through (SST) differentiates rays in their semantic meaning and\ndepth. Specifically, we combine deep learning and stereo matching to provide\neach ray a semantic label. We then design tailored weighting schemes for\nblending the rays. Although simple, our solution can effectively remove\nforeground residues when focusing on the background. At the same time, SST\nmaintains smooth transitions in varying focal depths. Comprehensive experiments\non synthetic and new real indoor and outdoor datasets demonstrate the\neffectiveness and usefulness of our technique.\n", "versions": [{"version": "v1", "created": "Mon, 26 Mar 2018 09:06:12 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Yu", "Huangjie", ""], ["Zhang", "Guli", ""], ["Ma", "Yuanxi", ""], ["Zhang", "Yingliang", ""], ["Yu", "Jingyi", ""]]}, {"id": "1803.09490", "submitter": "Fadime Sener", "authors": "Fadime Sener and Angela Yao", "title": "Unsupervised Learning and Segmentation of Complex Activities from Video", "comments": "CVPR 2018 Accepted Manuscript", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new method for unsupervised segmentation of complex\nactivities from video into multiple steps, or sub-activities, without any\ntextual input. We propose an iterative discriminative-generative approach which\nalternates between discriminatively learning the appearance of sub-activities\nfrom the videos' visual features to sub-activity labels and generatively\nmodelling the temporal structure of sub-activities using a Generalized Mallows\nModel. In addition, we introduce a model for background to account for frames\nunrelated to the actual activities. Our approach is validated on the\nchallenging Breakfast Actions and Inria Instructional Videos datasets and\noutperforms both unsupervised and weakly-supervised state of the art.\n", "versions": [{"version": "v1", "created": "Mon, 26 Mar 2018 09:47:26 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Sener", "Fadime", ""], ["Yao", "Angela", ""]]}, {"id": "1803.09492", "submitter": "Jussi Hanhirova", "authors": "Jussi Hanhirova, Teemu K\\\"am\\\"ar\\\"ainen, Sipi Sepp\\\"al\\\"a, Matti\n  Siekkinen, Vesa Hirvisalo, Antti Yl\\\"a-J\\\"a\\\"aski", "title": "Latency and Throughput Characterization of Convolutional Neural Networks\n  for Mobile Computer Vision", "comments": "13 pages, 18 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study performance characteristics of convolutional neural networks (CNN)\nfor mobile computer vision systems. CNNs have proven to be a powerful and\nefficient approach to implement such systems. However, the system performance\ndepends largely on the utilization of hardware accelerators, which are able to\nspeed up the execution of the underlying mathematical operations tremendously\nthrough massive parallelism. Our contribution is performance characterization\nof multiple CNN-based models for object recognition and detection with several\ndifferent hardware platforms and software frameworks, using both local\n(on-device) and remote (network-side server) computation. The measurements are\nconducted using real workloads and real processing platforms. On the platform\nside, we concentrate especially on TensorFlow and TensorRT. Our measurements\ninclude embedded processors found on mobile devices and high-performance\nprocessors that can be used on the network side of mobile systems. We show that\nthere exists significant latency--throughput trade-offs but the behavior is\nvery complex. We demonstrate and discuss several factors that affect the\nperformance and yield this complex behavior.\n", "versions": [{"version": "v1", "created": "Mon, 26 Mar 2018 09:49:03 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Hanhirova", "Jussi", ""], ["K\u00e4m\u00e4r\u00e4inen", "Teemu", ""], ["Sepp\u00e4l\u00e4", "Sipi", ""], ["Siekkinen", "Matti", ""], ["Hirvisalo", "Vesa", ""], ["Yl\u00e4-J\u00e4\u00e4ski", "Antti", ""]]}, {"id": "1803.09502", "submitter": "Luca Bertinetto", "authors": "Jack Valmadre, Luca Bertinetto, Jo\\~ao F. Henriques, Ran Tao, Andrea\n  Vedaldi, Arnold Smeulders, Philip Torr, Efstratios Gavves", "title": "Long-term Tracking in the Wild: A Benchmark", "comments": "To appear at ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce the OxUvA dataset and benchmark for evaluating single-object\ntracking algorithms. Benchmarks have enabled great strides in the field of\nobject tracking by defining standardized evaluations on large sets of diverse\nvideos. However, these works have focused exclusively on sequences that are\njust tens of seconds in length and in which the target is always visible.\nConsequently, most researchers have designed methods tailored to this\n\"short-term\" scenario, which is poorly representative of practitioners' needs.\nAiming to address this disparity, we compile a long-term, large-scale tracking\ndataset of sequences with average length greater than two minutes and with\nfrequent target object disappearance. The OxUvA dataset is much larger than the\nobject tracking datasets of recent years: it comprises 366 sequences spanning\n14 hours of video. We assess the performance of several algorithms, considering\nboth the ability to locate the target and to determine whether it is present or\nabsent. Our goal is to offer the community a large and diverse benchmark to\nenable the design and evaluation of tracking methods ready to be used \"in the\nwild\". The project website is http://oxuva.net\n", "versions": [{"version": "v1", "created": "Mon, 26 Mar 2018 10:43:45 GMT"}, {"version": "v2", "created": "Fri, 20 Apr 2018 14:08:21 GMT"}, {"version": "v3", "created": "Fri, 10 Aug 2018 16:03:38 GMT"}], "update_date": "2018-08-13", "authors_parsed": [["Valmadre", "Jack", ""], ["Bertinetto", "Luca", ""], ["Henriques", "Jo\u00e3o F.", ""], ["Tao", "Ran", ""], ["Vedaldi", "Andrea", ""], ["Smeulders", "Arnold", ""], ["Torr", "Philip", ""], ["Gavves", "Efstratios", ""]]}, {"id": "1803.09569", "submitter": "Yuri Boykov", "authors": "Meng Tang, Federico Perazzi, Abdelaziz Djelouah, Ismail Ben Ayed,\n  Christopher Schroers, Yuri Boykov", "title": "On Regularized Losses for Weakly-supervised CNN Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Minimization of regularized losses is a principled approach to weak\nsupervision well-established in deep learning, in general. However, it is\nlargely overlooked in semantic segmentation currently dominated by methods\nmimicking full supervision via \"fake\" fully-labeled training masks (proposals)\ngenerated from available partial input. To obtain such full masks the typical\nmethods explicitly use standard regularization techniques for \"shallow\"\nsegmentation, e.g. graph cuts or dense CRFs. In contrast, we integrate such\nstandard regularizers directly into the loss functions over partial input. This\napproach simplifies weakly-supervised training by avoiding extra MRF/CRF\ninference steps or layers explicitly generating full masks, while improving\nboth the quality and efficiency of training. This paper proposes and\nexperimentally compares different losses integrating MRF/CRF regularization\nterms. We juxtapose our regularized losses with earlier proposal-generation\nmethods using explicit regularization steps or layers. Our approach achieves\nstate-of-the-art accuracy in semantic segmentation with near full-supervision\nquality.\n", "versions": [{"version": "v1", "created": "Mon, 26 Mar 2018 13:14:58 GMT"}, {"version": "v2", "created": "Tue, 10 Apr 2018 21:59:43 GMT"}], "update_date": "2018-04-12", "authors_parsed": [["Tang", "Meng", ""], ["Perazzi", "Federico", ""], ["Djelouah", "Abdelaziz", ""], ["Ayed", "Ismail Ben", ""], ["Schroers", "Christopher", ""], ["Boykov", "Yuri", ""]]}, {"id": "1803.09588", "submitter": "Florian Scheidegger", "authors": "Florian Scheidegger, Roxana Istrate, Giovanni Mariani, Luca Benini,\n  Costas Bekas, Cristiano Malossi", "title": "Efficient Image Dataset Classification Difficulty Estimation for\n  Predicting Deep-Learning Accuracy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the deep-learning community new algorithms are published at an incredible\npace. Therefore, solving an image classification problem for new datasets\nbecomes a challenging task, as it requires to re-evaluate published algorithms\nand their different configurations in order to find a close to optimal\nclassifier. To facilitate this process, before biasing our decision towards a\nclass of neural networks or running an expensive search over the network space,\nwe propose to estimate the classification difficulty of the dataset. Our method\ncomputes a single number that characterizes the dataset difficulty 27x faster\nthan training state-of-the-art networks. The proposed method can be used in\ncombination with network topology and hyper-parameter search optimizers to\nefficiently drive the search towards promising neural-network configurations.\n", "versions": [{"version": "v1", "created": "Mon, 26 Mar 2018 13:46:54 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Scheidegger", "Florian", ""], ["Istrate", "Roxana", ""], ["Mariani", "Giovanni", ""], ["Benini", "Luca", ""], ["Bekas", "Costas", ""], ["Malossi", "Cristiano", ""]]}, {"id": "1803.09597", "submitter": "Claudio Michaelis", "authors": "Claudio Michaelis, Matthias Bethge, Alexander S. Ecker", "title": "One-Shot Segmentation in Clutter", "comments": "To appaer in: $\\textit{Proceedings of the $\\mathit{35}^{th}$\n  International Conference on Machine Learning}$, Stockholm, Sweden, PMLR 80,\n  2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We tackle the problem of one-shot segmentation: finding and segmenting a\npreviously unseen object in a cluttered scene based on a single instruction\nexample. We propose a novel dataset, which we call $\\textit{cluttered\nOmniglot}$. Using a baseline architecture combining a Siamese embedding for\ndetection with a U-net for segmentation we show that increasing levels of\nclutter make the task progressively harder. Using oracle models with access to\nvarious amounts of ground-truth information, we evaluate different aspects of\nthe problem and show that in this kind of visual search task, detection and\nsegmentation are two intertwined problems, the solution to each of which helps\nsolving the other. We therefore introduce $\\textit{MaskNet}$, an improved model\nthat attends to multiple candidate locations, generates segmentation proposals\nto mask out background clutter and selects among the segmented objects. Our\nfindings suggest that such image recognition models based on an iterative\nrefinement of object detection and foreground segmentation may provide a way to\ndeal with highly cluttered scenes.\n", "versions": [{"version": "v1", "created": "Mon, 26 Mar 2018 14:07:20 GMT"}, {"version": "v2", "created": "Wed, 13 Jun 2018 12:01:10 GMT"}], "update_date": "2018-06-14", "authors_parsed": [["Michaelis", "Claudio", ""], ["Bethge", "Matthias", ""], ["Ecker", "Alexander S.", ""]]}, {"id": "1803.09630", "submitter": "Ibrahim Omara", "authors": "Ibrahim Omara, Hongzhi Zhang, Faqiang Wang, and Wangmeng Zuo", "title": "Metric Learning with Dynamically Generated Pairwise Constraints for Ear\n  Recognition", "comments": "17 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ear recognition task is known as predicting whether two ear images belong to\nthe same person or not. In this paper, we present a novel metric learning\nmethod for ear recognition. This method is formulated as a pairwise constrained\noptimization problem. In each training cycle, this method selects the nearest\nsimilar and dissimilar neighbors of each sample to construct the pairwise\nconstraints, and then solve the optimization problem by the iterated Bregman\nprojections. Experiments are conducted on AMI, USTB II and WPUT databases. The\nresults show that the proposed approach can achieve promising recognition rates\nin ear recognition, and its training process is much more efficient than the\nother competing metric learning methods.\n", "versions": [{"version": "v1", "created": "Mon, 26 Mar 2018 14:45:36 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Omara", "Ibrahim", ""], ["Zhang", "Hongzhi", ""], ["Wang", "Faqiang", ""], ["Zuo", "Wangmeng", ""]]}, {"id": "1803.09638", "submitter": "Pin-Yu Chen", "authors": "Pei-Hsuan Lu, Pin-Yu Chen, Chia-Mu Yu", "title": "On the Limitation of Local Intrinsic Dimensionality for Characterizing\n  the Subspaces of Adversarial Examples", "comments": "Accepted to ICLR 2018 Worshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding and characterizing the subspaces of adversarial examples aid in\nstudying the robustness of deep neural networks (DNNs) to adversarial\nperturbations. Very recently, Ma et al. (ICLR 2018) proposed to use local\nintrinsic dimensionality (LID) in layer-wise hidden representations of DNNs to\nstudy adversarial subspaces. It was demonstrated that LID can be used to\ncharacterize the adversarial subspaces associated with different attack\nmethods, e.g., the Carlini and Wagner's (C&W) attack and the fast gradient sign\nattack.\n  In this paper, we use MNIST and CIFAR-10 to conduct two new sets of\nexperiments that are absent in existing LID analysis and report the limitation\nof LID in characterizing the corresponding adversarial subspaces, which are (i)\noblivious attacks and LID analysis using adversarial examples with different\nconfidence levels; and (ii) black-box transfer attacks. For (i), we find that\nthe performance of LID is very sensitive to the confidence parameter deployed\nby an attack, and the LID learned from ensembles of adversarial examples with\nvarying confidence levels surprisingly gives poor performance. For (ii), we\nfind that when adversarial examples are crafted from another DNN model, LID is\nineffective in characterizing their adversarial subspaces. These two findings\ntogether suggest the limited capability of LID in characterizing the subspaces\nof adversarial examples.\n", "versions": [{"version": "v1", "created": "Mon, 26 Mar 2018 14:56:28 GMT"}], "update_date": "2018-04-11", "authors_parsed": [["Lu", "Pei-Hsuan", ""], ["Chen", "Pin-Yu", ""], ["Yu", "Chia-Mu", ""]]}, {"id": "1803.09655", "submitter": "Giovanni Mariani", "authors": "Giovanni Mariani, Florian Scheidegger, Roxana Istrate, Costas Bekas,\n  Cristiano Malossi", "title": "BAGAN: Data Augmentation with Balancing GAN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image classification datasets are often imbalanced, characteristic that\nnegatively affects the accuracy of deep-learning classifiers. In this work we\npropose balancing GAN (BAGAN) as an augmentation tool to restore balance in\nimbalanced datasets. This is challenging because the few minority-class images\nmay not be enough to train a GAN. We overcome this issue by including during\nthe adversarial training all available images of majority and minority classes.\nThe generative model learns useful features from majority classes and uses\nthese to generate images for minority classes. We apply class conditioning in\nthe latent space to drive the generation process towards a target class. The\ngenerator in the GAN is initialized with the encoder module of an autoencoder\nthat enables us to learn an accurate class-conditioning in the latent space. We\ncompare the proposed methodology with state-of-the-art GANs and demonstrate\nthat BAGAN generates images of superior quality when trained with an imbalanced\ndataset.\n", "versions": [{"version": "v1", "created": "Mon, 26 Mar 2018 15:20:56 GMT"}, {"version": "v2", "created": "Tue, 5 Jun 2018 08:07:30 GMT"}], "update_date": "2018-06-06", "authors_parsed": [["Mariani", "Giovanni", ""], ["Scheidegger", "Florian", ""], ["Istrate", "Roxana", ""], ["Bekas", "Costas", ""], ["Malossi", "Cristiano", ""]]}, {"id": "1803.09659", "submitter": "Chunbiao Zhu", "authors": "Chunbiao Zhu and Ge Li", "title": "A multilayer backpropagation saliency detection algorithm and its\n  applications", "comments": "Publish version can be downloaded in\n  https://link.springer.com/article/10.1007/s11042-018-5780-4 . Source code can\n  be downloaded in https://github.com/ChunbiaoZhu/CAIP2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Saliency detection is an active topic in the multimedia field. Most previous\nworks on saliency detection focus on 2D images. However, these methods are not\nrobust against complex scenes which contain multiple objects or complex\nbackgrounds. Recently, depth information supplies a powerful cue for saliency\ndetection. In this paper, we propose a multilayer backpropagation saliency\ndetection algorithm based on depth mining by which we exploit depth cue from\nthree different layers of images. The proposed algorithm shows a good\nperformance and maintains the robustness in complex situations. Experiments'\nresults show that the proposed framework is superior to other existing saliency\napproaches. Besides, we give two innovative applications by this algorithm,\nsuch as scene reconstruction from multiple images and small target object\ndetection in video.\n", "versions": [{"version": "v1", "created": "Mon, 26 Mar 2018 15:26:21 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Zhu", "Chunbiao", ""], ["Li", "Ge", ""]]}, {"id": "1803.09672", "submitter": "Vishnu Naresh Boddeti", "authors": "Sixue Gong, Vishnu Naresh Boddeti and Anil K. Jain", "title": "On the Intrinsic Dimensionality of Image Representations", "comments": "Accepted for publication at CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the following questions pertaining to the intrinsic\ndimensionality of any given image representation: (i) estimate its intrinsic\ndimensionality, (ii) develop a deep neural network based non-linear mapping,\ndubbed DeepMDS, that transforms the ambient representation to the minimal\nintrinsic space, and (iii) validate the veracity of the mapping through image\nmatching in the intrinsic space. Experiments on benchmark image datasets (LFW,\nIJB-C and ImageNet-100) reveal that the intrinsic dimensionality of deep neural\nnetwork representations is significantly lower than the dimensionality of the\nambient features. For instance, SphereFace's 512-dim face representation and\nResNet's 512-dim image representation have an intrinsic dimensionality of 16\nand 19 respectively. Further, the DeepMDS mapping is able to obtain a\nrepresentation of significantly lower dimensionality while maintaining\ndiscriminative ability to a large extent, 59.75% TAR @ 0.1% FAR in 16-dim vs\n71.26% TAR in 512-dim on IJB-C and a Top-1 accuracy of 77.0% at 19-dim vs 83.4%\nat 512-dim on ImageNet-100.\n", "versions": [{"version": "v1", "created": "Mon, 26 Mar 2018 15:38:27 GMT"}, {"version": "v2", "created": "Thu, 11 Apr 2019 01:04:41 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Gong", "Sixue", ""], ["Boddeti", "Vishnu Naresh", ""], ["Jain", "Anil K.", ""]]}, {"id": "1803.09693", "submitter": "David Acuna", "authors": "David Acuna, Huan Ling, Amlan Kar, Sanja Fidler", "title": "Efficient Interactive Annotation of Segmentation Datasets with\n  Polygon-RNN++", "comments": "Accepted to CVPR 2018 (http://www.cs.toronto.edu/polyrnn/)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Manually labeling datasets with object masks is extremely time consuming. In\nthis work, we follow the idea of Polygon-RNN to produce polygonal annotations\nof objects interactively using humans-in-the-loop. We introduce several\nimportant improvements to the model: 1) we design a new CNN encoder\narchitecture, 2) show how to effectively train the model with Reinforcement\nLearning, and 3) significantly increase the output resolution using a Graph\nNeural Network, allowing the model to accurately annotate high-resolution\nobjects in images. Extensive evaluation on the Cityscapes dataset shows that\nour model, which we refer to as Polygon-RNN++, significantly outperforms the\noriginal model in both automatic (10% absolute and 16% relative improvement in\nmean IoU) and interactive modes (requiring 50% fewer clicks by annotators). We\nfurther analyze the cross-domain scenario in which our model is trained on one\ndataset, and used out of the box on datasets from varying domains. The results\nshow that Polygon-RNN++ exhibits powerful generalization capabilities,\nachieving significant improvements over existing pixel-wise methods. Using\nsimple online fine-tuning we further achieve a high reduction in annotation\ntime for new datasets, moving a step closer towards an interactive annotation\ntool to be used in practice.\n", "versions": [{"version": "v1", "created": "Mon, 26 Mar 2018 16:14:36 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Acuna", "David", ""], ["Ling", "Huan", ""], ["Kar", "Amlan", ""], ["Fidler", "Sanja", ""]]}, {"id": "1803.09719", "submitter": "Stan Birchfield", "authors": "Nikolai Smolyanskiy, Alexey Kamenev, Stan Birchfield", "title": "On the Importance of Stereo for Accurate Depth Estimation: An Efficient\n  Semi-Supervised Deep Neural Network Approach", "comments": "CVPR 2018 Workshop on Autonomous Driving. For video, see\n  https://youtu.be/0FPQdVOYoAU", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit the problem of visual depth estimation in the context of\nautonomous vehicles. Despite the progress on monocular depth estimation in\nrecent years, we show that the gap between monocular and stereo depth accuracy\nremains large$-$a particularly relevant result due to the prevalent reliance\nupon monocular cameras by vehicles that are expected to be self-driving. We\nargue that the challenges of removing this gap are significant, owing to\nfundamental limitations of monocular vision. As a result, we focus our efforts\non depth estimation by stereo. We propose a novel semi-supervised learning\napproach to training a deep stereo neural network, along with a novel\narchitecture containing a machine-learned argmax layer and a custom runtime\n(that will be shared publicly) that enables a smaller version of our stereo DNN\nto run on an embedded GPU. Competitive results are shown on the KITTI 2015\nstereo dataset. We also evaluate the recent progress of stereo algorithms by\nmeasuring the impact upon accuracy of various design criteria.\n", "versions": [{"version": "v1", "created": "Mon, 26 Mar 2018 17:19:40 GMT"}, {"version": "v2", "created": "Wed, 11 Apr 2018 22:44:59 GMT"}, {"version": "v3", "created": "Fri, 20 Apr 2018 00:22:20 GMT"}, {"version": "v4", "created": "Wed, 8 Jul 2020 02:14:14 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Smolyanskiy", "Nikolai", ""], ["Kamenev", "Alexey", ""], ["Birchfield", "Stan", ""]]}, {"id": "1803.09722", "submitter": "Wei Yang", "authors": "Wei Yang, Wanli Ouyang, Xiaolong Wang, Jimmy Ren, Hongsheng Li,\n  Xiaogang Wang", "title": "3D Human Pose Estimation in the Wild by Adversarial Learning", "comments": "CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, remarkable advances have been achieved in 3D human pose estimation\nfrom monocular images because of the powerful Deep Convolutional Neural\nNetworks (DCNNs). Despite their success on large-scale datasets collected in\nthe constrained lab environment, it is difficult to obtain the 3D pose\nannotations for in-the-wild images. Therefore, 3D human pose estimation in the\nwild is still a challenge. In this paper, we propose an adversarial learning\nframework, which distills the 3D human pose structures learned from the fully\nannotated dataset to in-the-wild images with only 2D pose annotations. Instead\nof defining hard-coded rules to constrain the pose estimation results, we\ndesign a novel multi-source discriminator to distinguish the predicted 3D poses\nfrom the ground-truth, which helps to enforce the pose estimator to generate\nanthropometrically valid poses even with images in the wild. We also observe\nthat a carefully designed information source for the discriminator is essential\nto boost the performance. Thus, we design a geometric descriptor, which\ncomputes the pairwise relative locations and distances between body joints, as\na new information source for the discriminator. The efficacy of our adversarial\nlearning framework with the new geometric descriptor has been demonstrated\nthrough extensive experiments on widely used public benchmarks. Our approach\nsignificantly improves the performance compared with previous state-of-the-art\napproaches.\n", "versions": [{"version": "v1", "created": "Mon, 26 Mar 2018 17:24:40 GMT"}, {"version": "v2", "created": "Mon, 16 Apr 2018 14:20:04 GMT"}], "update_date": "2018-04-17", "authors_parsed": [["Yang", "Wei", ""], ["Ouyang", "Wanli", ""], ["Wang", "Xiaolong", ""], ["Ren", "Jimmy", ""], ["Li", "Hongsheng", ""], ["Wang", "Xiaogang", ""]]}, {"id": "1803.09760", "submitter": "Andrew Jaegle", "authors": "Andrew Jaegle, Oleh Rybkin, Konstantinos G. Derpanis, Kostas\n  Daniilidis", "title": "Predicting the Future with Transformational States", "comments": "24 pages, including supplement", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An intelligent observer looks at the world and sees not only what is, but\nwhat is moving and what can be moved. In other words, the observer sees how the\npresent state of the world can transform in the future. We propose a model that\npredicts future images by learning to represent the present state and its\ntransformation given only a sequence of images. To do so, we introduce an\narchitecture with a latent state composed of two components designed to capture\n(i) the present image state and (ii) the transformation between present and\nfuture states, respectively. We couple this latent state with a recurrent\nneural network (RNN) core that predicts future frames by transforming past\nstates into future states by applying the accumulated state transformation with\na learned operator. We describe how this model can be integrated into an\nencoder-decoder convolutional neural network (CNN) architecture that uses\nweighted residual connections to integrate representations of the past with\nrepresentations of the future. Qualitatively, our approach generates image\nsequences that are stable and capture realistic motion over multiple predicted\nframes, without requiring adversarial training. Quantitatively, our method\nachieves prediction results comparable to state-of-the-art results on standard\nimage prediction benchmarks (Moving MNIST, KTH, and UCF101).\n", "versions": [{"version": "v1", "created": "Mon, 26 Mar 2018 18:00:07 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Jaegle", "Andrew", ""], ["Rybkin", "Oleh", ""], ["Derpanis", "Konstantinos G.", ""], ["Daniilidis", "Kostas", ""]]}, {"id": "1803.09786", "submitter": "Jingya Wang", "authors": "Jingya Wang, Xiatian Zhu, Shaogang Gong, Wei Li", "title": "Transferable Joint Attribute-Identity Deep Learning for Unsupervised\n  Person Re-Identification", "comments": "Accepted at CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most existing person re-identification (re-id) methods require supervised\nmodel learning from a separate large set of pairwise labelled training data for\nevery single camera pair. This significantly limits their scalability and\nusability in real-world large scale deployments with the need for performing\nre-id across many camera views. To address this scalability problem, we develop\na novel deep learning method for transferring the labelled information of an\nexisting dataset to a new unseen (unlabelled) target domain for person re-id\nwithout any supervised learning in the target domain. Specifically, we\nintroduce an Transferable Joint Attribute-Identity Deep Learning (TJ-AIDL) for\nsimultaneously learning an attribute-semantic and identitydiscriminative\nfeature representation space transferrable to any new (unseen) target domain\nfor re-id tasks without the need for collecting new labelled training data from\nthe target domain (i.e. unsupervised learning in the target domain). Extensive\ncomparative evaluations validate the superiority of this new TJ-AIDL model for\nunsupervised person re-id over a wide range of state-of-the-art methods on four\nchallenging benchmarks including VIPeR, PRID, Market-1501, and DukeMTMC-ReID.\n", "versions": [{"version": "v1", "created": "Mon, 26 Mar 2018 18:47:55 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Wang", "Jingya", ""], ["Zhu", "Xiatian", ""], ["Gong", "Shaogang", ""], ["Li", "Wei", ""]]}, {"id": "1803.09797", "submitter": "Lisa Anne Hendricks", "authors": "Kaylee Burns, Lisa Anne Hendricks, Kate Saenko, Trevor Darrell, Anna\n  Rohrbach", "title": "Women also Snowboard: Overcoming Bias in Captioning Models", "comments": "22 pages, 6 figures, Burns and Hendricks contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most machine learning methods are known to capture and exploit biases of the\ntraining data. While some biases are beneficial for learning, others are\nharmful. Specifically, image captioning models tend to exaggerate biases\npresent in training data (e.g., if a word is present in 60% of training\nsentences, it might be predicted in 70% of sentences at test time). This can\nlead to incorrect captions in domains where unbiased captions are desired, or\nrequired, due to over-reliance on the learned prior and image context. In this\nwork we investigate generation of gender-specific caption words (e.g. man,\nwoman) based on the person's appearance or the image context. We introduce a\nnew Equalizer model that ensures equal gender probability when gender evidence\nis occluded in a scene and confident predictions when gender evidence is\npresent. The resulting model is forced to look at a person rather than use\ncontextual cues to make a gender-specific predictions. The losses that comprise\nour model, the Appearance Confusion Loss and the Confident Loss, are general,\nand can be added to any description model in order to mitigate impacts of\nunwanted bias in a description dataset. Our proposed model has lower error than\nprior work when describing images with people and mentioning their gender and\nmore closely matches the ground truth ratio of sentences including women to\nsentences including men. We also show that unlike other approaches, our model\nis indeed more often looking at people when predicting their gender.\n", "versions": [{"version": "v1", "created": "Mon, 26 Mar 2018 19:07:08 GMT"}, {"version": "v2", "created": "Fri, 15 Jun 2018 16:49:18 GMT"}, {"version": "v3", "created": "Mon, 18 Jun 2018 03:47:34 GMT"}, {"version": "v4", "created": "Wed, 13 Mar 2019 21:32:00 GMT"}], "update_date": "2019-03-15", "authors_parsed": [["Burns", "Kaylee", ""], ["Hendricks", "Lisa Anne", ""], ["Saenko", "Kate", ""], ["Darrell", "Trevor", ""], ["Rohrbach", "Anna", ""]]}, {"id": "1803.09803", "submitter": "Sefik Emre Eskimez", "authors": "Sefik Emre Eskimez, Ross K Maddox, Chenliang Xu, Zhiyao Duan", "title": "Generating Talking Face Landmarks from Speech", "comments": "To Appear in LVA ICA 2018. Please see the following link:\n  http://www2.ece.rochester.edu/projects/air/projects/talkingface.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The presence of a corresponding talking face has been shown to significantly\nimprove speech intelligibility in noisy conditions and for hearing impaired\npopulation. In this paper, we present a system that can generate landmark\npoints of a talking face from an acoustic speech in real time. The system uses\na long short-term memory (LSTM) network and is trained on frontal videos of 27\ndifferent speakers with automatically extracted face landmarks. After training,\nit can produce talking face landmarks from the acoustic speech of unseen\nspeakers and utterances. The training phase contains three key steps. We first\ntransform landmarks of the first video frame to pin the two eye points into two\npredefined locations and apply the same transformation on all of the following\nvideo frames. We then remove the identity information by transforming the\nlandmarks into a mean face shape across the entire training dataset. Finally,\nwe train an LSTM network that takes the first- and second-order temporal\ndifferences of the log-mel spectrogram as input to predict face landmarks in\neach frame. We evaluate our system using the mean-squared error (MSE) loss of\nlandmarks of lips between predicted and ground-truth landmarks as well as their\nfirst- and second-order temporal differences. We further evaluate our system by\nconducting subjective tests, where the subjects try to distinguish the real and\nfake videos of talking face landmarks. Both tests show promising results.\n", "versions": [{"version": "v1", "created": "Mon, 26 Mar 2018 19:25:02 GMT"}, {"version": "v2", "created": "Mon, 23 Apr 2018 15:08:51 GMT"}], "update_date": "2018-04-24", "authors_parsed": [["Eskimez", "Sefik Emre", ""], ["Maddox", "Ross K", ""], ["Xu", "Chenliang", ""], ["Duan", "Zhiyao", ""]]}, {"id": "1803.09820", "submitter": "Leslie Smith", "authors": "Leslie N. Smith", "title": "A disciplined approach to neural network hyper-parameters: Part 1 --\n  learning rate, batch size, momentum, and weight decay", "comments": "Files to help replicate the results reported here are available on\n  Github", "journal-ref": null, "doi": null, "report-no": "US Naval Research Laboratory Technical Report 5510-026", "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although deep learning has produced dazzling successes for applications of\nimage, speech, and video processing in the past few years, most trainings are\nwith suboptimal hyper-parameters, requiring unnecessarily long training times.\nSetting the hyper-parameters remains a black art that requires years of\nexperience to acquire. This report proposes several efficient ways to set the\nhyper-parameters that significantly reduce training time and improves\nperformance. Specifically, this report shows how to examine the training\nvalidation/test loss function for subtle clues of underfitting and overfitting\nand suggests guidelines for moving toward the optimal balance point. Then it\ndiscusses how to increase/decrease the learning rate/momentum to speed up\ntraining. Our experiments show that it is crucial to balance every manner of\nregularization for each dataset and architecture. Weight decay is used as a\nsample regularizer to show how its optimal value is tightly coupled with the\nlearning rates and momentums. Files to help replicate the results reported here\nare available.\n", "versions": [{"version": "v1", "created": "Mon, 26 Mar 2018 20:05:59 GMT"}, {"version": "v2", "created": "Tue, 24 Apr 2018 17:43:51 GMT"}], "update_date": "2018-04-25", "authors_parsed": [["Smith", "Leslie N.", ""]]}, {"id": "1803.09824", "submitter": "Ronald Kemker", "authors": "Ronald Kemker and Ryan Luu and Christopher Kanan", "title": "Low-Shot Learning for the Semantic Segmentation of Remote Sensing\n  Imagery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in computer vision using deep learning with RGB imagery\n(e.g., object recognition and detection) have been made possible thanks to the\ndevelopment of large annotated RGB image datasets. In contrast, multispectral\nimage (MSI) and hyperspectral image (HSI) datasets contain far fewer labeled\nimages, in part due to the wide variety of sensors used. These annotations are\nespecially limited for semantic segmentation, or pixel-wise classification, of\nremote sensing imagery because it is labor intensive to generate image\nannotations. Low-shot learning algorithms can make effective inferences despite\nsmaller amounts of annotated data. In this paper, we study low-shot learning\nusing self-taught feature learning for semantic segmentation. We introduce 1)\nan improved self-taught feature learning framework for HSI and MSI data and 2)\na semi-supervised classification algorithm. When these are combined, they\nachieve state-of-the-art performance on remote sensing datasets that have\nlittle annotated training data available. These low-shot learning frameworks\nwill reduce the manual image annotation burden and improve semantic\nsegmentation performance for remote sensing imagery.\n", "versions": [{"version": "v1", "created": "Mon, 26 Mar 2018 20:17:19 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Kemker", "Ronald", ""], ["Luu", "Ryan", ""], ["Kanan", "Christopher", ""]]}, {"id": "1803.09845", "submitter": "Jiasen Lu", "authors": "Jiasen Lu, Jianwei Yang, Dhruv Batra, Devi Parikh", "title": "Neural Baby Talk", "comments": "12 pages, 7 figures, CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel framework for image captioning that can produce natural\nlanguage explicitly grounded in entities that object detectors find in the\nimage. Our approach reconciles classical slot filling approaches (that are\ngenerally better grounded in images) with modern neural captioning approaches\n(that are generally more natural sounding and accurate). Our approach first\ngenerates a sentence `template' with slot locations explicitly tied to specific\nimage regions. These slots are then filled in by visual concepts identified in\nthe regions by object detectors. The entire architecture (sentence template\ngeneration and slot filling with object detectors) is end-to-end\ndifferentiable. We verify the effectiveness of our proposed model on different\nimage captioning tasks. On standard image captioning and novel object\ncaptioning, our model reaches state-of-the-art on both COCO and Flickr30k\ndatasets. We also demonstrate that our model has unique advantages when the\ntrain and test distributions of scene compositions -- and hence language priors\nof associated captions -- are different. Code has been made available at:\nhttps://github.com/jiasenlu/NeuralBabyTalk\n", "versions": [{"version": "v1", "created": "Tue, 27 Mar 2018 01:59:56 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Lu", "Jiasen", ""], ["Yang", "Jianwei", ""], ["Batra", "Dhruv", ""], ["Parikh", "Devi", ""]]}, {"id": "1803.09851", "submitter": "Tushar Nagarajan", "authors": "Tushar Nagarajan and Kristen Grauman", "title": "Attributes as Operators: Factorizing Unseen Attribute-Object\n  Compositions", "comments": "European Conference on Computer Vision (ECCV) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new approach to modeling visual attributes. Prior work casts\nattributes in a similar role as objects, learning a latent representation where\nproperties (e.g., sliced) are recognized by classifiers much in the way objects\n(e.g., apple) are. However, this common approach fails to separate the\nattributes observed during training from the objects with which they are\ncomposed, making it ineffectual when encountering new attribute-object\ncompositions. Instead, we propose to model attributes as operators. Our\napproach learns a semantic embedding that explicitly factors out attributes\nfrom their accompanying objects, and also benefits from novel regularizers\nexpressing attribute operators' effects (e.g., blunt should undo the effects of\nsharp). Not only does our approach align conceptually with the linguistic role\nof attributes as modifiers, but it also generalizes to recognize unseen\ncompositions of objects and attributes. We validate our approach on two\nchallenging datasets and demonstrate significant improvements over the\nstate-of-the-art. In addition, we show that not only can our model recognize\nunseen compositions robustly in an open-world setting, it can also generalize\nto compositions where objects themselves were unseen during training.\n", "versions": [{"version": "v1", "created": "Tue, 27 Mar 2018 02:30:56 GMT"}, {"version": "v2", "created": "Mon, 27 Aug 2018 19:33:30 GMT"}], "update_date": "2018-08-29", "authors_parsed": [["Nagarajan", "Tushar", ""], ["Grauman", "Kristen", ""]]}, {"id": "1803.09859", "submitter": "Qibin Hou", "authors": "Qibin Hou, Ming-Ming Cheng, Jiangjiang Liu, Philip H.S. Torr", "title": "WebSeg: Learning Semantic Segmentation from Web Searches", "comments": "Submitted to ECCV2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we improve semantic segmentation by automatically learning\nfrom Flickr images associated with a particular keyword, without relying on any\nexplicit user annotations, thus substantially alleviating the dependence on\naccurate annotations when compared to previous weakly supervised methods.\n  To solve such a challenging problem, we leverage several low-level cues (such\nas saliency, edges, etc.) to help generate a proxy ground truth. Due to the\ndiversity of web-crawled images, we anticipate a large amount of 'label noise'\nin which other objects might be present. We design an online noise filtering\nscheme which is able to deal with this label noise, especially in cluttered\nimages. We use this filtering strategy as an auxiliary module to help assist\nthe segmentation network in learning cleaner proxy annotations. Extensive\nexperiments on the popular PASCAL VOC 2012 semantic segmentation benchmark show\nsurprising good results in both our WebSeg (mIoU = 57.0%) and weakly supervised\n(mIoU = 63.3%) settings.\n", "versions": [{"version": "v1", "created": "Tue, 27 Mar 2018 02:59:13 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Hou", "Qibin", ""], ["Cheng", "Ming-Ming", ""], ["Liu", "Jiangjiang", ""], ["Torr", "Philip H. S.", ""]]}, {"id": "1803.09860", "submitter": "Qibin Hou", "authors": "Qibin Hou, Jiang-Jiang Liu, Ming-Ming Cheng, Ali Borji, Philip H.S.\n  Torr", "title": "Three Birds One Stone: A General Architecture for Salient Object\n  Segmentation, Edge Detection and Skeleton Extraction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we aim at solving pixel-wise binary problems, including\nsalient object segmentation, skeleton extraction, and edge detection, by\nintroducing a unified architecture. Previous works have proposed tailored\nmethods for solving each of the three tasks independently. Here, we show that\nthese tasks share some similarities that can be exploited for developing a\nunified framework. In particular, we introduce a horizontal cascade, each\ncomponent of which is densely connected to the outputs of previous component.\nStringing these components together allows us to effectively exploit features\nacross different levels hierarchically to effectively address the multiple\npixel-wise binary regression tasks. To assess the performance of our proposed\nnetwork on these tasks, we carry out exhaustive evaluations on multiple\nrepresentative datasets. Although these tasks are inherently very different, we\nshow that our unified approach performs very well on all of them and works far\nbetter than current single-purpose state-of-the-art methods. All the code in\nthis paper will be publicly available.\n", "versions": [{"version": "v1", "created": "Tue, 27 Mar 2018 03:00:44 GMT"}, {"version": "v2", "created": "Sat, 6 Apr 2019 02:31:04 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Hou", "Qibin", ""], ["Liu", "Jiang-Jiang", ""], ["Cheng", "Ming-Ming", ""], ["Borji", "Ali", ""], ["Torr", "Philip H. S.", ""]]}, {"id": "1803.09867", "submitter": "Liang Lin", "authors": "Keze Wang and Xiaopeng Yan and Dongyu Zhang and Lei Zhang and Liang\n  Lin", "title": "Towards Human-Machine Cooperation: Self-supervised Sample Mining for\n  Object Detection", "comments": "We enabled to mine from unlabeled or partially labeled data to boost\n  object detection (Accepted by CVPR 2018) The source code is available at\n  http://kezewang.com/codes/SSM_CVPR.zip", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Though quite challenging, leveraging large-scale unlabeled or partially\nlabeled images in a cost-effective way has increasingly attracted interests for\nits great importance to computer vision. To tackle this problem, many Active\nLearning (AL) methods have been developed. However, these methods mainly define\ntheir sample selection criteria within a single image context, leading to the\nsuboptimal robustness and impractical solution for large-scale object\ndetection. In this paper, aiming to remedy the drawbacks of existing AL\nmethods, we present a principled Self-supervised Sample Mining (SSM) process\naccounting for the real challenges in object detection. Specifically, our SSM\nprocess concentrates on automatically discovering and pseudo-labeling reliable\nregion proposals for enhancing the object detector via the introduced cross\nimage validation, i.e., pasting these proposals into different labeled images\nto comprehensively measure their values under different image contexts. By\nresorting to the SSM process, we propose a new AL framework for gradually\nincorporating unlabeled or partially labeled data into the model learning while\nminimizing the annotating effort of users. Extensive experiments on two public\nbenchmarks clearly demonstrate our proposed framework can achieve the\ncomparable performance to the state-of-the-art methods with significantly fewer\nannotations.\n", "versions": [{"version": "v1", "created": "Tue, 27 Mar 2018 03:06:51 GMT"}, {"version": "v2", "created": "Thu, 24 May 2018 11:59:38 GMT"}], "update_date": "2018-05-25", "authors_parsed": [["Wang", "Keze", ""], ["Yan", "Xiaopeng", ""], ["Zhang", "Dongyu", ""], ["Zhang", "Lei", ""], ["Lin", "Liang", ""]]}, {"id": "1803.09882", "submitter": "Shuang Li", "authors": "Shuang Li, Slawomir Bak, Peter Carr, Xiaogang Wang", "title": "Diversity Regularized Spatiotemporal Attention for Video-based Person\n  Re-identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video-based person re-identification matches video clips of people across\nnon-overlapping cameras. Most existing methods tackle this problem by encoding\neach video frame in its entirety and computing an aggregate representation\nacross all frames. In practice, people are often partially occluded, which can\ncorrupt the extracted features. Instead, we propose a new spatiotemporal\nattention model that automatically discovers a diverse set of distinctive body\nparts. This allows useful information to be extracted from all frames without\nsuccumbing to occlusions and misalignments. The network learns multiple spatial\nattention models and employs a diversity regularization term to ensure multiple\nmodels do not discover the same body part. Features extracted from local image\nregions are organized by spatial attention model and are combined using\ntemporal attention. As a result, the network learns latent representations of\nthe face, torso and other body parts using the best available image patches\nfrom the entire video sequence. Extensive evaluations on three datasets show\nthat our framework outperforms the state-of-the-art approaches by large margins\non multiple metrics.\n", "versions": [{"version": "v1", "created": "Tue, 27 Mar 2018 03:47:53 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Li", "Shuang", ""], ["Bak", "Slawomir", ""], ["Carr", "Peter", ""], ["Wang", "Xiaogang", ""]]}, {"id": "1803.09894", "submitter": "Lipeng Ke", "authors": "Lipeng Ke, Ming-Ching Chang, Honggang Qi, Siwei Lyu", "title": "Multi-Scale Structure-Aware Network for Human Pose Estimation", "comments": "Accepted by ECCV2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a robust multi-scale structure-aware neural network for human pose\nestimation. This method improves the recent deep conv-deconv hourglass models\nwith four key improvements: (1) multi-scale supervision to strengthen\ncontextual feature learning in matching body keypoints by combining feature\nheatmaps across scales, (2) multi-scale regression network at the end to\nglobally optimize the structural matching of the multi-scale features, (3)\nstructure-aware loss used in the intermediate supervision and at the regression\nto improve the matching of keypoints and respective neighbors to infer a\nhigher-order matching configurations, and (4) a keypoint masking training\nscheme that can effectively fine-tune our network to robustly localize occluded\nkeypoints via adjacent matches. Our method can effectively improve\nstate-of-the-art pose estimation methods that suffer from difficulties in scale\nvarieties, occlusions, and complex multi-person scenarios. This multi-scale\nsupervision tightly integrates with the regression network to effectively (i)\nlocalize keypoints using the ensemble of multi-scale features, and (ii) infer\nglobal pose configuration by maximizing structural consistencies across\nmultiple keypoints and scales. The keypoint masking training enhances these\nadvantages to focus learning on hard occlusion samples. Our method achieves the\nleading position in the MPII challenge leaderboard among the state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Tue, 27 Mar 2018 04:37:20 GMT"}, {"version": "v2", "created": "Thu, 29 Mar 2018 01:35:59 GMT"}, {"version": "v3", "created": "Sun, 16 Sep 2018 21:55:51 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Ke", "Lipeng", ""], ["Chang", "Ming-Ching", ""], ["Qi", "Honggang", ""], ["Lyu", "Siwei", ""]]}, {"id": "1803.09903", "submitter": "Nathan Cahill", "authors": "Nathan D. Cahill, Tyler L. Hayes, Renee T. Meinhold, John F. Hamilton", "title": "Compassionately Conservative Balanced Cuts for Image Segmentation", "comments": "Long version of paper accepted at CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Normalized Cut (NCut) objective function, widely used in data clustering\nand image segmentation, quantifies the cost of graph partitioning in a way that\nbiases clusters or segments that are balanced towards having lower values than\nunbalanced partitionings. However, this bias is so strong that it avoids any\nsingleton partitions, even when vertices are very weakly connected to the rest\nof the graph. Motivated by the B\\\"uhler-Hein family of balanced cut costs, we\npropose the family of Compassionately Conservative Balanced (CCB) Cut costs,\nwhich are indexed by a parameter that can be used to strike a compromise\nbetween the desire to avoid too many singleton partitions and the notion that\nall partitions should be balanced. We show that CCB-Cut minimization can be\nrelaxed into an orthogonally constrained $\\ell_{\\tau}$-minimization problem\nthat coincides with the problem of computing Piecewise Flat Embeddings (PFE)\nfor one particular index value, and we present an algorithm for solving the\nrelaxed problem by iteratively minimizing a sequence of reweighted Rayleigh\nquotients (IRRQ). Using images from the BSDS500 database, we show that image\nsegmentation based on CCB-Cut minimization provides better accuracy with\nrespect to ground truth and greater variability in region size than NCut-based\nimage segmentation.\n", "versions": [{"version": "v1", "created": "Tue, 27 Mar 2018 05:42:47 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Cahill", "Nathan D.", ""], ["Hayes", "Tyler L.", ""], ["Meinhold", "Renee T.", ""], ["Hamilton", "John F.", ""]]}, {"id": "1803.09909", "submitter": "Xinghao Ding", "authors": "Liyan Sun, Zhiwen Fan, Xinghao Ding, Congbo Cai, Yue Huang, John\n  Paisley", "title": "A Divide-and-Conquer Approach to Compressed Sensing MRI", "comments": "37 pages, 20 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compressed sensing (CS) theory assures us that we can accurately reconstruct\nmagnetic resonance images using fewer k-space measurements than the Nyquist\nsampling rate requires. In traditional CS-MRI inversion methods, the fact that\nthe energy within the Fourier measurement domain is distributed non-uniformly\nis often neglected during reconstruction. As a result, more densely sampled\nlow-frequency information tends to dominate penalization schemes for\nreconstructing MRI at the expense of high-frequency details. In this paper, we\npropose a new framework for CS-MRI inversion in which we decompose the observed\nk-space data into \"subspaces\" via sets of filters in a lossless way, and\nreconstruct the images in these various spaces individually using off-the-shelf\nalgorithms. We then fuse the results to obtain the final reconstruction. In\nthis way we are able to focus reconstruction on frequency information within\nthe entire k-space more equally, preserving both high and low frequency\ndetails. We demonstrate that the proposed framework is competitive with\nstate-of-the-art methods in CS-MRI in terms of quantitative performance, and\noften improves an algorithm's results qualitatively compared with it's direct\napplication to k-space.\n", "versions": [{"version": "v1", "created": "Tue, 27 Mar 2018 06:07:17 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Sun", "Liyan", ""], ["Fan", "Zhiwen", ""], ["Ding", "Xinghao", ""], ["Cai", "Congbo", ""], ["Huang", "Yue", ""], ["Paisley", "John", ""]]}, {"id": "1803.09926", "submitter": "Zheng Qin", "authors": "Zheng Qin, Zhaoning Zhang, Dongsheng Li, Yiming Zhang, Yuxing Peng", "title": "Diagonalwise Refactorization: An Efficient Training Method for Depthwise\n  Convolutions", "comments": "8 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Depthwise convolutions provide significant performance benefits owing to the\nreduction in both parameters and mult-adds. However, training depthwise\nconvolution layers with GPUs is slow in current deep learning frameworks\nbecause their implementations cannot fully utilize the GPU capacity. To address\nthis problem, in this paper we present an efficient method (called diagonalwise\nrefactorization) for accelerating the training of depthwise convolution layers.\nOur key idea is to rearrange the weight vectors of a depthwise convolution into\na large diagonal weight matrix so as to convert the depthwise convolution into\none single standard convolution, which is well supported by the cuDNN library\nthat is highly-optimized for GPU computations. We have implemented our training\nmethod in five popular deep learning frameworks. Evaluation results show that\nour proposed method gains $15.4\\times$ training speedup on Darknet, $8.4\\times$\non Caffe, $5.4\\times$ on PyTorch, $3.5\\times$ on MXNet, and $1.4\\times$ on\nTensorFlow, compared to their original implementations of depthwise\nconvolutions.\n", "versions": [{"version": "v1", "created": "Tue, 27 Mar 2018 07:06:54 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Qin", "Zheng", ""], ["Zhang", "Zhaoning", ""], ["Li", "Dongsheng", ""], ["Zhang", "Yiming", ""], ["Peng", "Yuxing", ""]]}, {"id": "1803.09932", "submitter": "Jianbo Wang", "authors": "Dasong Li, Jianbo Wang", "title": "Image Semantic Transformation: Faster, Lighter and Stronger", "comments": "ECCV 2018 submission, 14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We propose Image-Semantic-Transformation-Reconstruction-Circle(ISTRC) model,\na novel and powerful method using facenet's Euclidean latent space to\nunderstand the images. As the name suggests, ISTRC construct the circle, able\nto perfectly reconstruct images. One powerful Euclidean latent space embedded\nin ISTRC is FaceNet's last layer with the power of distinguishing and\nunderstanding images. Our model will reconstruct the images and manipulate\nEuclidean latent vectors to achieve semantic transformations and semantic\nimages arthimetic calculations. In this paper, we show that ISTRC performs 10\nhigh-level semantic transformations like \"Male and female\",\"add smile\",\"open\nmouth\", \"deduct beard or add mustache\", \"bigger/smaller nose\", \"make older and\nyounger\", \"bigger lips\", \"bigger eyes\", \"bigger/smaller mouths\" and \"more\nattractive\". It just takes 3 hours(GTX 1080) to train the models of 10 semantic\ntransformations.\n", "versions": [{"version": "v1", "created": "Tue, 27 Mar 2018 07:20:46 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Li", "Dasong", ""], ["Wang", "Jianbo", ""]]}, {"id": "1803.09933", "submitter": "Massimiliano Razzano", "authors": "Massimiliano Razzano, Elena Cuoco", "title": "Image-based deep learning for classification of noise transients in\n  gravitational wave detectors", "comments": "25 pages, 8 figures, accepted for publication in Classical and\n  Quantum Gravity", "journal-ref": "Razzano, M., and Cuoco, E., 2018, Classical and Quantum Gravity,\n  Volume 35, Number 9, 2018", "doi": "10.1088/1361-6382/aab793", "report-no": null, "categories": "gr-qc astro-ph.IM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The detection of gravitational waves has inaugurated the era of gravitational\nastronomy and opened new avenues for the multimessenger study of cosmic\nsources. Thanks to their sensitivity, the Advanced LIGO and Advanced Virgo\ninterferometers will probe a much larger volume of space and expand the\ncapability of discovering new gravitational wave emitters. The characterization\nof these detectors is a primary task in order to recognize the main sources of\nnoise and optimize the sensitivity of interferometers. Glitches are transient\nnoise events that can impact the data quality of the interferometers and their\nclassification is an important task for detector characterization. Deep\nlearning techniques are a promising tool for the recognition and classification\nof glitches. We present a classification pipeline that exploits convolutional\nneural networks to classify glitches starting from their time-frequency\nevolution represented as images. We evaluated the classification accuracy on\nsimulated glitches, showing that the proposed algorithm can automatically\nclassify glitches on very fast timescales and with high accuracy, thus\nproviding a promising tool for online detector characterization.\n", "versions": [{"version": "v1", "created": "Tue, 27 Mar 2018 07:23:13 GMT"}], "update_date": "2018-04-05", "authors_parsed": [["Razzano", "Massimiliano", ""], ["Cuoco", "Elena", ""]]}, {"id": "1803.09937", "submitter": "Chun-Guang Li", "authors": "Jianlou Si, Honggang Zhang, Chun-Guang Li, Jason Kuen, Xiangfei Kong,\n  Alex C. Kot, and Gang Wang", "title": "Dual Attention Matching Network for Context-Aware Feature Sequence based\n  Person Re-Identification", "comments": "10 pages, 8 figures, 7 tables, accepted by CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Typical person re-identification (ReID) methods usually describe each\npedestrian with a single feature vector and match them in a task-specific\nmetric space. However, the methods based on a single feature vector are not\nsufficient enough to overcome visual ambiguity, which frequently occurs in real\nscenario. In this paper, we propose a novel end-to-end trainable framework,\ncalled Dual ATtention Matching network (DuATM), to learn context-aware feature\nsequences and perform attentive sequence comparison simultaneously. The core\ncomponent of our DuATM framework is a dual attention mechanism, in which both\nintra-sequence and inter-sequence attention strategies are used for feature\nrefinement and feature-pair alignment, respectively. Thus, detailed visual cues\ncontained in the intermediate feature sequences can be automatically exploited\nand properly compared. We train the proposed DuATM network as a siamese network\nvia a triplet loss assisted with a de-correlation loss and a cross-entropy\nloss. We conduct extensive experiments on both image and video based ReID\nbenchmark datasets. Experimental results demonstrate the significant advantages\nof our approach compared to the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 27 Mar 2018 07:41:18 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Si", "Jianlou", ""], ["Zhang", "Honggang", ""], ["Li", "Chun-Guang", ""], ["Kuen", "Jason", ""], ["Kong", "Xiangfei", ""], ["Kot", "Alex C.", ""], ["Wang", "Gang", ""]]}, {"id": "1803.09956", "submitter": "Andy Zeng", "authors": "Andy Zeng, Shuran Song, Stefan Welker, Johnny Lee, Alberto Rodriguez,\n  Thomas Funkhouser", "title": "Learning Synergies between Pushing and Grasping with Self-supervised\n  Deep Reinforcement Learning", "comments": "To appear at the International Conference On Intelligent Robots and\n  Systems (IROS) 2018. Project webpage: http://vpg.cs.princeton.edu Summary\n  video: https://youtu.be/-OkyX7ZlhiU", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Skilled robotic manipulation benefits from complex synergies between\nnon-prehensile (e.g. pushing) and prehensile (e.g. grasping) actions: pushing\ncan help rearrange cluttered objects to make space for arms and fingers;\nlikewise, grasping can help displace objects to make pushing movements more\nprecise and collision-free. In this work, we demonstrate that it is possible to\ndiscover and learn these synergies from scratch through model-free deep\nreinforcement learning. Our method involves training two fully convolutional\nnetworks that map from visual observations to actions: one infers the utility\nof pushes for a dense pixel-wise sampling of end effector orientations and\nlocations, while the other does the same for grasping. Both networks are\ntrained jointly in a Q-learning framework and are entirely self-supervised by\ntrial and error, where rewards are provided from successful grasps. In this\nway, our policy learns pushing motions that enable future grasps, while\nlearning grasps that can leverage past pushes. During picking experiments in\nboth simulation and real-world scenarios, we find that our system quickly\nlearns complex behaviors amid challenging cases of clutter, and achieves better\ngrasping success rates and picking efficiencies than baseline alternatives\nafter only a few hours of training. We further demonstrate that our method is\ncapable of generalizing to novel objects. Qualitative results (videos), code,\npre-trained models, and simulation environments are available at\nhttp://vpg.cs.princeton.edu\n", "versions": [{"version": "v1", "created": "Tue, 27 Mar 2018 08:31:28 GMT"}, {"version": "v2", "created": "Mon, 16 Apr 2018 03:39:11 GMT"}, {"version": "v3", "created": "Sun, 30 Sep 2018 20:34:49 GMT"}], "update_date": "2018-10-02", "authors_parsed": [["Zeng", "Andy", ""], ["Song", "Shuran", ""], ["Welker", "Stefan", ""], ["Lee", "Johnny", ""], ["Rodriguez", "Alberto", ""], ["Funkhouser", "Thomas", ""]]}, {"id": "1803.10036", "submitter": "Minh-Tan Pham", "authors": "Minh-Tan Pham, S\\'ebastien Lef\\`evre, Erchan Aptoula, Lorenzo Bruzzone", "title": "Recent Developments from Attribute Profiles for Remote Sensing Image\n  Classification", "comments": "6 pages; to appear in ICPRAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Morphological attribute profiles (APs) are among the most effective methods\nto model the spatial and contextual information for the analysis of remote\nsensing images, especially for classification task. Since their first\nintroduction to this field in early 2010's, many research studies have been\ncontributed not only to exploit and adapt their use to different applications,\nbut also to extend and improve their performance for better dealing with more\ncomplex data. In this paper, we revisit and discuss different developments and\nextensions from APs which have drawn significant attention from researchers in\nthe past few years. These studies are analyzed and gathered based on the\nconcept of multi-stage AP construction. In our experiments, a comparative study\non classification results of two remote sensing data is provided in order to\nshow their significant improvements compared to the originally proposed APs.\n", "versions": [{"version": "v1", "created": "Tue, 27 Mar 2018 12:22:49 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Pham", "Minh-Tan", ""], ["Lef\u00e8vre", "S\u00e9bastien", ""], ["Aptoula", "Erchan", ""], ["Bruzzone", "Lorenzo", ""]]}, {"id": "1803.10039", "submitter": "Lei He", "authors": "Lei He, Guanghui Wang and Zhanyi Hu", "title": "Learning Depth from Single Images with Deep Neural Network Embedding\n  Focal Length", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2018.2832296", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning depth from a single image, as an important issue in scene\nunderstanding, has attracted a lot of attention in the past decade. The\naccuracy of the depth estimation has been improved from conditional Markov\nrandom fields, non-parametric methods, to deep convolutional neural networks\nmost recently. However, there exist inherent ambiguities in recovering 3D from\na single 2D image. In this paper, we first prove the ambiguity between the\nfocal length and monocular depth learning, and verify the result using\nexperiments, showing that the focal length has a great influence on accurate\ndepth recovery. In order to learn monocular depth by embedding the focal\nlength, we propose a method to generate synthetic varying-focal-length dataset\nfrom fixed-focal-length datasets, and a simple and effective method is\nimplemented to fill the holes in the newly generated images. For the sake of\naccurate depth recovery, we propose a novel deep neural network to infer depth\nthrough effectively fusing the middle-level information on the\nfixed-focal-length dataset, which outperforms the state-of-the-art methods\nbuilt on pre-trained VGG. Furthermore, the newly generated varying-focal-length\ndataset is taken as input to the proposed network in both learning and\ninference phases. Extensive experiments on the fixed- and varying-focal-length\ndatasets demonstrate that the learned monocular depth with embedded focal\nlength is significantly improved compared to that without embedding the focal\nlength information.\n", "versions": [{"version": "v1", "created": "Tue, 27 Mar 2018 12:26:15 GMT"}], "update_date": "2018-08-01", "authors_parsed": [["He", "Lei", ""], ["Wang", "Guanghui", ""], ["Hu", "Zhanyi", ""]]}, {"id": "1803.10045", "submitter": "Michele Scipioni", "authors": "Michele Scipioni (1 and 3), Maria F. Santarelli (2), Luigi Landini (1\n  and 2), Ciprian Catana (3 and 4), Douglas N. Greve (3 and 4), Julie C. Price\n  (3 and 4) and Stefano Pedemonte (3, 4 and 5) ((1) DII, University of Pisa,\n  (2) IFC-CNR, Pisa, (3) Martinos Center for Biomedical Imaging, Boston, (4)\n  Harvard Medical School, Boston, (5) MGH Center for Clinical Data Science,\n  Boston)", "title": "Kinetic Compressive Sensing", "comments": "5 pages, 6 figures, Submitted to the Conference Record of \"IEEE\n  Nuclear Science Symposium and Medical Imaging Conference (IEEE NSS-MIC) 2017\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.med-ph cs.CV physics.data-an stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parametric images provide insight into the spatial distribution of\nphysiological parameters, but they are often extremely noisy, due to low SNR of\ntomographic data. Direct estimation from projections allows accurate noise\nmodeling, improving the results of post-reconstruction fitting. We propose a\nmethod, which we name kinetic compressive sensing (KCS), based on a\nhierarchical Bayesian model and on a novel reconstruction algorithm, that\nencodes sparsity of kinetic parameters. Parametric maps are reconstructed by\nmaximizing the joint probability, with an Iterated Conditional Modes (ICM)\napproach, alternating the optimization of activity time series (OS-MAP-OSL),\nand kinetic parameters (MAP-LM). We evaluated the proposed algorithm on a\nsimulated dynamic phantom: a bias/variance study confirmed how direct estimates\ncan improve the quality of parametric maps over a post-reconstruction fitting,\nand showed how the novel sparsity prior can further reduce their variance,\nwithout affecting bias. Real FDG PET human brain data (Siemens mMR, 40min)\nimages were also processed. Results enforced how the proposed KCS-regularized\ndirect method can produce spatially coherent images and parametric maps, with\nlower spatial noise and better tissue contrast. A GPU-based open source\nimplementation of the algorithm is provided.\n", "versions": [{"version": "v1", "created": "Tue, 27 Mar 2018 12:46:57 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Scipioni", "Michele", "", "1 and 3"], ["Santarelli", "Maria F.", "", "1\n  and 2"], ["Landini", "Luigi", "", "1\n  and 2"], ["Catana", "Ciprian", "", "3 and 4"], ["Greve", "Douglas N.", "", "3 and 4"], ["Price", "Julie C.", "", "3 and 4"], ["Pedemonte", "Stefano", "", "3, 4 and 5"]]}, {"id": "1803.10071", "submitter": "Tong Zhang", "authors": "Tong Zhang (1 and 2), Wenming Zheng (2), Zhen Cui (3), Yang Li (1 and\n  2) ((1) the Department of Information Science and Engineering, Southeast\n  University, Nanjing, China (2) the Key Laboratory of Child Development and\n  Learning Science of Ministry of Education, Research Center for Learning\n  Science, Southeast University, Nanjing, China (3) the School of Computer\n  Science and Engineering, Nanjing University of Science and Technology,\n  Nanjing, China)", "title": "Tensor graph convolutional neural network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel tensor graph convolutional neural network\n(TGCNN) to conduct convolution on factorizable graphs, for which here two types\nof problems are focused, one is sequential dynamic graphs and the other is\ncross-attribute graphs. Especially, we propose a graph preserving layer to\nmemorize salient nodes of those factorized subgraphs, i.e. cross graph\nconvolution and graph pooling. For cross graph convolution, a parameterized\nKronecker sum operation is proposed to generate a conjunctive adjacency matrix\ncharacterizing the relationship between every pair of nodes across two\nsubgraphs. Taking this operation, then general graph convolution may be\nefficiently performed followed by the composition of small matrices, which thus\nreduces high memory and computational burden. Encapsuling sequence graphs into\na recursive learning, the dynamics of graphs can be efficiently encoded as well\nas the spatial layout of graphs. To validate the proposed TGCNN, experiments\nare conducted on skeleton action datasets as well as matrix completion dataset.\nThe experiment results demonstrate that our method can achieve more competitive\nperformance with the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 27 Mar 2018 13:34:05 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Zhang", "Tong", "", "1 and 2"], ["Zheng", "Wenming", "", "1 and\n  2"], ["Cui", "Zhen", "", "1 and\n  2"], ["Li", "Yang", "", "1 and\n  2"]]}, {"id": "1803.10075", "submitter": "Mathieu Garon", "authors": "Mathieu Garon, Denis Laurendeau and Jean-Fran\\c{c}ois Lalonde", "title": "A Framework for Evaluating 6-DOF Object Trackers", "comments": "Project website :\n  http://vision.gel.ulaval.ca/~jflalonde/projects/6dofObjectTracking/index.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a challenging and realistic novel dataset for evaluating 6-DOF\nobject tracking algorithms. Existing datasets show serious\nlimitations---notably, unrealistic synthetic data, or real data with large\nfiducial markers---preventing the community from obtaining an accurate picture\nof the state-of-the-art. Using a data acquisition pipeline based on a\ncommercial motion capture system for acquiring accurate ground truth poses of\nreal objects with respect to a Kinect V2 camera, we build a dataset which\ncontains a total of 297 calibrated sequences. They are acquired in three\ndifferent scenarios to evaluate the performance of trackers: stability,\nrobustness to occlusion and accuracy during challenging interactions between a\nperson and the object. We conduct an extensive study of a deep 6-DOF tracking\narchitecture and determine a set of optimal parameters. We enhance the\narchitecture and the training methodology to train a 6-DOF tracker that can\nrobustly generalize to objects never seen during training, and demonstrate\nfavorable performance compared to previous approaches trained specifically on\nthe objects to track.\n", "versions": [{"version": "v1", "created": "Tue, 27 Mar 2018 13:42:03 GMT"}, {"version": "v2", "created": "Wed, 28 Mar 2018 01:49:37 GMT"}, {"version": "v3", "created": "Thu, 6 Sep 2018 20:21:41 GMT"}], "update_date": "2018-09-10", "authors_parsed": [["Garon", "Mathieu", ""], ["Laurendeau", "Denis", ""], ["Lalonde", "Jean-Fran\u00e7ois", ""]]}, {"id": "1803.10081", "submitter": "Bharath Bhushan Damodaran", "authors": "Bharath Bhushan Damodaran, Benjamin Kellenberger, R\\'emi Flamary,\n  Devis Tuia, Nicolas Courty", "title": "DeepJDOT: Deep Joint Distribution Optimal Transport for Unsupervised\n  Domain Adaptation", "comments": "European Conference on Computer Vision 2018 (ECCV-2018)", "journal-ref": "in Proceedings of European Conference on Computer Vision 2018\n  (ECCV-2018)", "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In computer vision, one is often confronted with problems of domain shifts,\nwhich occur when one applies a classifier trained on a source dataset to target\ndata sharing similar characteristics (e.g. same classes), but also different\nlatent data structures (e.g. different acquisition conditions). In such a\nsituation, the model will perform poorly on the new data, since the classifier\nis specialized to recognize visual cues specific to the source domain. In this\nwork we explore a solution, named DeepJDOT, to tackle this problem: through a\nmeasure of discrepancy on joint deep representations/labels based on optimal\ntransport, we not only learn new data representations aligned between the\nsource and target domain, but also simultaneously preserve the discriminative\ninformation used by the classifier. We applied DeepJDOT to a series of visual\nrecognition tasks, where it compares favorably against state-of-the-art deep\ndomain adaptation methods.\n", "versions": [{"version": "v1", "created": "Tue, 27 Mar 2018 13:54:05 GMT"}, {"version": "v2", "created": "Fri, 27 Jul 2018 07:32:54 GMT"}, {"version": "v3", "created": "Wed, 5 Sep 2018 09:57:46 GMT"}], "update_date": "2018-09-06", "authors_parsed": [["Damodaran", "Bharath Bhushan", ""], ["Kellenberger", "Benjamin", ""], ["Flamary", "R\u00e9mi", ""], ["Tuia", "Devis", ""], ["Courty", "Nicolas", ""]]}, {"id": "1803.10082", "submitter": "Sylvestre-Alvise Rebuffi", "authors": "Sylvestre-Alvise Rebuffi, Hakan Bilen, Andrea Vedaldi", "title": "Efficient parametrization of multi-domain deep neural networks", "comments": "CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A practical limitation of deep neural networks is their high degree of\nspecialization to a single task and visual domain. Recently, inspired by the\nsuccesses of transfer learning, several authors have proposed to learn instead\nuniversal, fixed feature extractors that, used as the first stage of any deep\nnetwork, work well for several tasks and domains simultaneously. Nevertheless,\nsuch universal features are still somewhat inferior to specialized networks.\n  To overcome this limitation, in this paper we propose to consider instead\nuniversal parametric families of neural networks, which still contain\nspecialized problem-specific models, but differing only by a small number of\nparameters. We study different designs for such parametrizations, including\nseries and parallel residual adapters, joint adapter compression, and parameter\nallocations, and empirically identify the ones that yield the highest\ncompression. We show that, in order to maximize performance, it is necessary to\nadapt both shallow and deep layers of a deep network, but the required changes\nare very small. We also show that these universal parametrization are very\neffective for transfer learning, where they outperform traditional fine-tuning\ntechniques.\n", "versions": [{"version": "v1", "created": "Tue, 27 Mar 2018 13:55:56 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Rebuffi", "Sylvestre-Alvise", ""], ["Bilen", "Hakan", ""], ["Vedaldi", "Andrea", ""]]}, {"id": "1803.10091", "submitter": "Matan Atzmon", "authors": "Matan Atzmon, Haggai Maron, Yaron Lipman", "title": "Point Convolutional Neural Networks by Extension Operators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents Point Convolutional Neural Networks (PCNN): a novel\nframework for applying convolutional neural networks to point clouds. The\nframework consists of two operators: extension and restriction, mapping point\ncloud functions to volumetric functions and vise-versa. A point cloud\nconvolution is defined by pull-back of the Euclidean volumetric convolution via\nan extension-restriction mechanism.\n  The point cloud convolution is computationally efficient, invariant to the\norder of points in the point cloud, robust to different samplings and varying\ndensities, and translation invariant, that is the same convolution kernel is\nused at all points. PCNN generalizes image CNNs and allows readily adapting\ntheir architectures to the point cloud setting.\n  Evaluation of PCNN on three central point cloud learning benchmarks\nconvincingly outperform competing point cloud learning methods, and the vast\nmajority of methods working with more informative shape representations such as\nsurfaces and/or normals.\n", "versions": [{"version": "v1", "created": "Tue, 27 Mar 2018 14:06:16 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Atzmon", "Matan", ""], ["Maron", "Haggai", ""], ["Lipman", "Yaron", ""]]}, {"id": "1803.10098", "submitter": "Guanjun Guo", "authors": "Guanjun Guo, Hanzi Wang, Yan Yan, Hong-Yuan Mark Liao, Bo Li", "title": "A New Target-specific Object Proposal Generation Method for Visual\n  Tracking", "comments": "14pages,11figures, Submited to IEEE Transactions on Cybernetisc", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object proposal generation methods have been widely applied to many computer\nvision tasks. However, existing object proposal generation methods often suffer\nfrom the problems of motion blur, low contrast, deformation, etc., when they\nare applied to video related tasks. In this paper, we propose an effective and\nhighly accurate target-specific object proposal generation (TOPG) method, which\ntakes full advantage of the context information of a video to alleviate these\nproblems. Specifically, we propose to generate target-specific object proposals\nby integrating the information of two important objectness cues: colors and\nedges, which are complementary to each other for different challenging\nenvironments in the process of generating object proposals. As a result, the\nrecall of the proposed TOPG method is significantly increased. Furthermore, we\npropose an object proposal ranking strategy to increase the rank accuracy of\nthe generated object proposals. The proposed TOPG method has yielded\nsignificant recall gain (about 20%-60% higher) compared with several\nstate-of-the-art object proposal methods on several challenging visual tracking\ndatasets. Then, we apply the proposed TOPG method to the task of visual\ntracking and propose a TOPG-based tracker (called as TOPGT), where TOPG is used\nas a sample selection strategy to select a small number of high-quality target\ncandidates from the generated object proposals. Since the object proposals\ngenerated by the proposed TOPG cover many hard negative samples and positive\nsamples, these object proposals can not only be used for training an effective\nclassifier, but also be used as target candidates for visual tracking.\nExperimental results show the superior performance of TOPGT for visual tracking\ncompared with several other state-of-the-art visual trackers (about 3%-11%\nhigher than the winner of the VOT2015 challenge in term of distance precision).\n", "versions": [{"version": "v1", "created": "Tue, 27 Mar 2018 14:20:25 GMT"}], "update_date": "2018-03-30", "authors_parsed": [["Guo", "Guanjun", ""], ["Wang", "Hanzi", ""], ["Yan", "Yan", ""], ["Liao", "Hong-Yuan Mark", ""], ["Li", "Bo", ""]]}, {"id": "1803.10100", "submitter": "Markus D. Solbach", "authors": "Markus D. Solbach, Stephen Voland, Jeff Edmonds and John K. Tsotsos", "title": "Random Polyhedral Scenes: An Image Generator for Active Vision System\n  Experiments", "comments": "18 pages, 9 Figures, 2 Listings, Technical Report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a Polyhedral Scene Generator system which creates a random scene\nbased on a few user parameters, renders the scene from random view points and\ncreates a dataset containing the renderings and corresponding annotation files.\nWe hope that this generator will enable research on how a program could parse a\nscene if it had multiple viewpoints to consider. For ambiguous scenes,\ntypically people move their head or change their position to see the scene from\ndifferent angles as well as seeing how it changes while they move; this\nresearch field is called active perception. The random scene generator\npresented is designed to support research in this field by generating images of\nscenes with known complexity characteristics and with verifiable properties\nwith respect to the distribution of features across a population. Thus, it is\nwell-suited for research in active perception without the requirement of a live\n3D environment and mobile sensing agent, including comparative performance\nevaluations. The system is publicly available at\nhttps://polyhedral.eecs.yorku.ca.\n", "versions": [{"version": "v1", "created": "Tue, 27 Mar 2018 14:22:42 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Solbach", "Markus D.", ""], ["Voland", "Stephen", ""], ["Edmonds", "Jeff", ""], ["Tsotsos", "John K.", ""]]}, {"id": "1803.10103", "submitter": "Guanjun Guo", "authors": "Guanjun Guo, Hanzi Wang, Yan Yan, Jin Zheng, Bo Li", "title": "A Fast Face Detection Method via Convolutional Neural Network", "comments": "11 figures, 30 pages, To appear in Neurocomputing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current face or object detection methods via convolutional neural network\n(such as OverFeat, R-CNN and DenseNet) explicitly extract multi-scale features\nbased on an image pyramid. However, such a strategy increases the computational\nburden for face detection. In this paper, we propose a fast face detection\nmethod based on discriminative complete features (DCFs) extracted by an\nelaborately designed convolutional neural network, where face detection is\ndirectly performed on the complete feature maps. DCFs have shown the ability of\nscale invariance, which is beneficial for face detection with high speed and\npromising performance. Therefore, extracting multi-scale features on an image\npyramid employed in the conventional methods is not required in the proposed\nmethod, which can greatly improve its efficiency for face detection.\nExperimental results on several popular face detection datasets show the\nefficiency and the effectiveness of the proposed method for face detection.\n", "versions": [{"version": "v1", "created": "Tue, 27 Mar 2018 14:25:04 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Guo", "Guanjun", ""], ["Wang", "Hanzi", ""], ["Yan", "Yan", ""], ["Zheng", "Jin", ""], ["Li", "Bo", ""]]}, {"id": "1803.10106", "submitter": "Gregor Lenz", "authors": "Gregor Lenz, Sio-Hoi Ieng, Ryad Benosman", "title": "Event-based Face Detection and Tracking in the Blink of an Eye", "comments": null, "journal-ref": "Frontiers in Neuroscience 2020 volume 14", "doi": "10.3389/fnins.2020.00587", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the first purely event-based method for face detection using the\nhigh temporal resolution of an event-based camera. We will rely on a new\nfeature that has never been used for such a task that relies on detecting eye\nblinks. Eye blinks are a unique natural dynamic signature of human faces that\nis captured well by event-based sensors that rely on relative changes of\nluminance. Although an eye blink can be captured with conventional cameras, we\nwill show that the dynamics of eye blinks combined with the fact that two eyes\nact simultaneously allows to derive a robust methodology for face detection at\na low computational cost and high temporal resolution. We show that eye blinks\nhave a unique temporal signature over time that can be easily detected by\ncorrelating the acquired local activity with a generic temporal model of eye\nblinks that has been generated from a wide population of users. We furthermore\nshow that once the face is reliably detected it is possible to apply a\nprobabilistic framework to track the spatial position of a face for each\nincoming event while updating the position of trackers. Results are shown for\nseveral indoor and outdoor experiments. We will also release an annotated data\nset that can be used for future work on the topic.\n", "versions": [{"version": "v1", "created": "Tue, 27 Mar 2018 14:27:26 GMT"}, {"version": "v2", "created": "Mon, 19 Nov 2018 16:53:22 GMT"}, {"version": "v3", "created": "Tue, 2 Apr 2019 19:05:55 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Lenz", "Gregor", ""], ["Ieng", "Sio-Hoi", ""], ["Benosman", "Ryad", ""]]}, {"id": "1803.10119", "submitter": "Alexandre B\\^one", "authors": "Alexandre B\\^one, Olivier Colliot, Stanley Durrleman", "title": "Learning distributions of shape trajectories from longitudinal datasets:\n  a hierarchical model on a manifold of diffeomorphisms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV math.DG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method to learn a distribution of shape trajectories from\nlongitudinal data, i.e. the collection of individual objects repeatedly\nobserved at multiple time-points. The method allows to compute an average\nspatiotemporal trajectory of shape changes at the group level, and the\nindividual variations of this trajectory both in terms of geometry and time\ndynamics. First, we formulate a non-linear mixed-effects statistical model as\nthe combination of a generic statistical model for manifold-valued longitudinal\ndata, a deformation model defining shape trajectories via the action of a\nfinite-dimensional set of diffeomorphisms with a manifold structure, and an\nefficient numerical scheme to compute parallel transport on this manifold.\nSecond, we introduce a MCMC-SAEM algorithm with a specific approach to shape\nsampling, an adaptive scheme for proposal variances, and a log-likelihood\ntempering strategy to estimate our model. Third, we validate our algorithm on\n2D simulated data, and then estimate a scenario of alteration of the shape of\nthe hippocampus 3D brain structure during the course of Alzheimer's disease.\nThe method shows for instance that hippocampal atrophy progresses more quickly\nin female subjects, and occurs earlier in APOE4 mutation carriers. We finally\nillustrate the potential of our method for classifying pathological\ntrajectories versus normal ageing.\n", "versions": [{"version": "v1", "created": "Tue, 27 Mar 2018 15:03:51 GMT"}, {"version": "v2", "created": "Wed, 13 Jun 2018 14:35:22 GMT"}], "update_date": "2018-06-14", "authors_parsed": [["B\u00f4ne", "Alexandre", ""], ["Colliot", "Olivier", ""], ["Durrleman", "Stanley", ""]]}, {"id": "1803.10158", "submitter": "Simon Hecker", "authors": "Simon Hecker, Dengxin Dai, Luc Van Gool", "title": "End-to-End Learning of Driving Models with Surround-View Cameras and\n  Route Planners", "comments": "to be published at ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For human drivers, having rear and side-view mirrors is vital for safe\ndriving. They deliver a more complete view of what is happening around the car.\nHuman drivers also heavily exploit their mental map for navigation.\nNonetheless, several methods have been published that learn driving models with\nonly a front-facing camera and without a route planner. This lack of\ninformation renders the self-driving task quite intractable. We investigate the\nproblem in a more realistic setting, which consists of a surround-view camera\nsystem with eight cameras, a route planner, and a CAN bus reader. In\nparticular, we develop a sensor setup that provides data for a 360-degree view\nof the area surrounding the vehicle, the driving route to the destination, and\nlow-level driving maneuvers (e.g. steering angle and speed) by human drivers.\nWith such a sensor setup we collect a new driving dataset, covering diverse\ndriving scenarios and varying weather/illumination conditions. Finally, we\nlearn a novel driving model by integrating information from the surround-view\ncameras and the route planner. Two route planners are exploited: 1) by\nrepresenting the planned routes on OpenStreetMap as a stack of GPS coordinates,\nand 2) by rendering the planned routes on TomTom Go Mobile and recording the\nprogression into a video. Our experiments show that: 1) 360-degree\nsurround-view cameras help avoid failures made with a single front-view camera,\nin particular for city driving and intersection scenarios; and 2) route\nplanners help the driving task significantly, especially for steering angle\nprediction.\n", "versions": [{"version": "v1", "created": "Tue, 27 Mar 2018 16:07:45 GMT"}, {"version": "v2", "created": "Mon, 6 Aug 2018 14:54:01 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Hecker", "Simon", ""], ["Dai", "Dengxin", ""], ["Van Gool", "Luc", ""]]}, {"id": "1803.10193", "submitter": "Soshi Shimada", "authors": "Vladislav Golyanik and Soshi Shimada and Kiran Varanasi and Didier\n  Stricker", "title": "HDM-Net: Monocular Non-Rigid 3D Reconstruction with Learned Deformation\n  Model", "comments": "9 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monocular dense 3D reconstruction of deformable objects is a hard ill-posed\nproblem in computer vision. Current techniques either require dense\ncorrespondences and rely on motion and deformation cues, or assume a highly\naccurate reconstruction (referred to as a template) of at least a single frame\ngiven in advance and operate in the manner of non-rigid tracking. Accurate\ncomputation of dense point tracks often requires multiple frames and might be\ncomputationally expensive. Availability of a template is a very strong prior\nwhich restricts system operation to a pre-defined environment and scenarios. In\nthis work, we propose a new hybrid approach for monocular non-rigid\nreconstruction which we call Hybrid Deformation Model Network (HDM-Net). In our\napproach, deformation model is learned by a deep neural network, with a\ncombination of domain-specific loss functions. We train the network with\nmultiple states of a non-rigidly deforming structure with a known shape at\nrest. HDM-Net learns different reconstruction cues including texture-dependent\nsurface deformations, shading and contours. We show generalisability of HDM-Net\nto states not presented in the training dataset, with unseen textures and under\nnew illumination conditions. Experiments with noisy data and a comparison with\nother methods demonstrate robustness and accuracy of the proposed approach and\nsuggest possible application scenarios of the new technique in interventional\ndiagnostics and augmented reality.\n", "versions": [{"version": "v1", "created": "Tue, 27 Mar 2018 17:31:47 GMT"}, {"version": "v2", "created": "Mon, 5 Aug 2019 18:56:15 GMT"}], "update_date": "2019-08-07", "authors_parsed": [["Golyanik", "Vladislav", ""], ["Shimada", "Soshi", ""], ["Varanasi", "Kiran", ""], ["Stricker", "Didier", ""]]}, {"id": "1803.10335", "submitter": "Jyh-Jing Hwang", "authors": "Tsung-Wei Ke, Jyh-Jing Hwang, Ziwei Liu, and Stella X. Yu", "title": "Adaptive Affinity Fields for Semantic Segmentation", "comments": "To appear in European Conference on Computer Vision (ECCV) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic segmentation has made much progress with increasingly powerful\npixel-wise classifiers and incorporating structural priors via Conditional\nRandom Fields (CRF) or Generative Adversarial Networks (GAN). We propose a\nsimpler alternative that learns to verify the spatial structure of segmentation\nduring training only. Unlike existing approaches that enforce semantic labels\non individual pixels and match labels between neighbouring pixels, we propose\nthe concept of Adaptive Affinity Fields (AAF) to capture and match the semantic\nrelations between neighbouring pixels in the label space. We use adversarial\nlearning to select the optimal affinity field size for each semantic category.\nIt is formulated as a minimax problem, optimizing our segmentation neural\nnetwork in a best worst-case learning scenario. AAF is versatile for\nrepresenting structures as a collection of pixel-centric relations, easier to\ntrain than GAN and more efficient than CRF without run-time inference. Our\nextensive evaluations on PASCAL VOC 2012, Cityscapes, and GTA5 datasets\ndemonstrate its above-par segmentation performance and robust generalization\nacross domains.\n", "versions": [{"version": "v1", "created": "Tue, 27 Mar 2018 21:21:57 GMT"}, {"version": "v2", "created": "Tue, 14 Aug 2018 06:32:58 GMT"}, {"version": "v3", "created": "Tue, 21 Aug 2018 16:20:16 GMT"}], "update_date": "2018-08-22", "authors_parsed": [["Ke", "Tsung-Wei", ""], ["Hwang", "Jyh-Jing", ""], ["Liu", "Ziwei", ""], ["Yu", "Stella X.", ""]]}, {"id": "1803.10336", "submitter": "Karthik Gopinath", "authors": "Karthik Gopinath, Christian Desrosiers, and Herve Lombaert", "title": "Graph Convolutions on Spectral Embeddings: Learning of Cortical Surface\n  Data", "comments": "8 pages, Under review in MICCAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neuronal cell bodies mostly reside in the cerebral cortex. The study of this\nthin and highly convoluted surface is essential for understanding how the brain\nworks. The analysis of surface data is, however, challenging due to the high\nvariability of the cortical geometry. This paper presents a novel approach for\nlearning and exploiting surface data directly across surface domains. Current\napproaches rely on geometrical simplifications, such as spherical inflations, a\npopular but costly process. For instance, the widely used FreeSurfer takes\nabout 3 hours to parcellate brain surfaces on a standard machine. Direct\nlearning of surface data via graph convolutions would provide a new family of\nfast algorithms for processing brain surfaces. However, the current limitation\nof existing state-of-the-art approaches is their inability to compare surface\ndata across different surface domains. Surface bases are indeed incompatible\nbetween brain geometries. This paper leverages recent advances in spectral\ngraph matching to transfer surface data across aligned spectral domains. This\nnovel approach enables a direct learning of surface data across compatible\nsurface bases. It exploits spectral filters over intrinsic representations of\nsurface neighborhoods. We illustrate the benefits of this approach with an\napplication to brain parcellation. We validate the algorithm over 101 manually\nlabeled brain surfaces. The results show a significant improvement in labeling\naccuracy over recent Euclidean approaches, while gaining a drastic speed\nimprovement over conventional methods.\n", "versions": [{"version": "v1", "created": "Tue, 27 Mar 2018 21:25:12 GMT"}], "update_date": "2018-03-29", "authors_parsed": [["Gopinath", "Karthik", ""], ["Desrosiers", "Christian", ""], ["Lombaert", "Herve", ""]]}, {"id": "1803.10348", "submitter": "Patrick Perez", "authors": "Huy V. Vo, Ngoc Q. K. Duong, Patrick Perez", "title": "Structural inpainting", "comments": null, "journal-ref": null, "doi": "10.1145/3240508.3240678", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scene-agnostic visual inpainting remains very challenging despite progress in\npatch-based methods. Recently, Pathak et al. 2016 have introduced convolutional\n\"context encoders\" (CEs) for unsupervised feature learning through image\ncompletion tasks. With the additional help of adversarial training, CEs turned\nout to be a promising tool to complete complex structures in real inpainting\nproblems. In the present paper we propose to push further this key ability by\nrelying on perceptual reconstruction losses at training time. We show on a wide\nvariety of visual scenes the merit of the approach for structural inpainting,\nand confirm it through a user study. Combined with the optimization-based\nrefinement of Yang et al. 2016 with neural patches, our context encoder opens\nup new opportunities for prior-free visual inpainting.\n", "versions": [{"version": "v1", "created": "Tue, 27 Mar 2018 22:36:55 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["Vo", "Huy V.", ""], ["Duong", "Ngoc Q. K.", ""], ["Perez", "Patrick", ""]]}, {"id": "1803.10358", "submitter": "Ervin Teng", "authors": "Ervin Teng, Rui Huang, Bob Iannucci", "title": "ClickBAIT-v2: Training an Object Detector in Real-Time", "comments": "8 pages, 13 figures. For ClickBAIT-v1, see arXiv:1709.05021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern deep convolutional neural networks (CNNs) for image classification and\nobject detection are often trained offline on large static datasets. Some\napplications, however, will require training in real-time on live video streams\nwith a human-in-the-loop. We refer to this class of problem as time-ordered\nonline training (ToOT). These problems will require a consideration of not only\nthe quantity of incoming training data, but the human effort required to\nannotate and use it. We demonstrate and evaluate a system tailored to training\nan object detector on a live video stream with minimal input from a human\noperator. We show that we can obtain bounding box annotation from\nweakly-supervised single-point clicks through interactive segmentation.\nFurthermore, by exploiting the time-ordered nature of the video stream through\nobject tracking, we can increase the average training benefit of human\ninteractions by 3-4 times.\n", "versions": [{"version": "v1", "created": "Tue, 27 Mar 2018 23:30:08 GMT"}], "update_date": "2018-03-29", "authors_parsed": [["Teng", "Ervin", ""], ["Huang", "Rui", ""], ["Iannucci", "Bob", ""]]}, {"id": "1803.10362", "submitter": "Ranjay Krishna", "authors": "Ranjay Krishna, Ines Chami, Michael Bernstein, Li Fei-Fei", "title": "Referring Relationships", "comments": "CVPR 2018, 19 pages, 12 figures, includes supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Images are not simply sets of objects: each image represents a web of\ninterconnected relationships. These relationships between entities carry\nsemantic meaning and help a viewer differentiate between instances of an\nentity. For example, in an image of a soccer match, there may be multiple\npersons present, but each participates in different relationships: one is\nkicking the ball, and the other is guarding the goal. In this paper, we\nformulate the task of utilizing these \"referring relationships\" to disambiguate\nbetween entities of the same category. We introduce an iterative model that\nlocalizes the two entities in the referring relationship, conditioned on one\nanother. We formulate the cyclic condition between the entities in a\nrelationship by modelling predicates that connect the entities as shifts in\nattention from one entity to another. We demonstrate that our model can not\nonly outperform existing approaches on three datasets --- CLEVR, VRD and Visual\nGenome --- but also that it produces visually meaningful predicate shifts, as\nan instance of interpretable neural networks. Finally, we show that by\nmodelling predicates as attention shifts, we can even localize entities in the\nabsence of their category, allowing our model to find completely unseen\ncategories.\n", "versions": [{"version": "v1", "created": "Wed, 28 Mar 2018 00:32:57 GMT"}, {"version": "v2", "created": "Thu, 29 Mar 2018 05:37:25 GMT"}], "update_date": "2018-03-30", "authors_parsed": [["Krishna", "Ranjay", ""], ["Chami", "Ines", ""], ["Bernstein", "Michael", ""], ["Fei-Fei", "Li", ""]]}, {"id": "1803.10368", "submitter": "Hajime Taira", "authors": "Hajime Taira, Masatoshi Okutomi, Torsten Sattler, Mircea Cimpoi, Marc\n  Pollefeys, Josef Sivic, Tomas Pajdla, Akihiko Torii", "title": "InLoc: Indoor Visual Localization with Dense Matching and View Synthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We seek to predict the 6 degree-of-freedom (6DoF) pose of a query photograph\nwith respect to a large indoor 3D map. The contributions of this work are\nthree-fold. First, we develop a new large-scale visual localization method\ntargeted for indoor environments. The method proceeds along three steps: (i)\nefficient retrieval of candidate poses that ensures scalability to large-scale\nenvironments, (ii) pose estimation using dense matching rather than local\nfeatures to deal with textureless indoor scenes, and (iii) pose verification by\nvirtual view synthesis to cope with significant changes in viewpoint, scene\nlayout, and occluders. Second, we collect a new dataset with reference 6DoF\nposes for large-scale indoor localization. Query photographs are captured by\nmobile phones at a different time than the reference 3D map, thus presenting a\nrealistic indoor localization scenario. Third, we demonstrate that our method\nsignificantly outperforms current state-of-the-art indoor localization\napproaches on this new challenging data.\n", "versions": [{"version": "v1", "created": "Wed, 28 Mar 2018 00:50:30 GMT"}, {"version": "v2", "created": "Sun, 8 Apr 2018 15:08:20 GMT"}], "update_date": "2018-04-10", "authors_parsed": [["Taira", "Hajime", ""], ["Okutomi", "Masatoshi", ""], ["Sattler", "Torsten", ""], ["Cimpoi", "Mircea", ""], ["Pollefeys", "Marc", ""], ["Sivic", "Josef", ""], ["Pajdla", "Tomas", ""], ["Torii", "Akihiko", ""]]}, {"id": "1803.10385", "submitter": "Noranart Vesdapunt", "authors": "Noranart Vesdapunt, Nongluk Covavisaruch", "title": "Automatic Stroke Lesions Segmentation in Diffusion-Weighted MRI", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diffusion-Weighted Magnetic Resonance Imaging (DWI) is widely used for early\ncerebral infarct detection caused by ischemic stroke. Manual segmentation is\ndone by a radiologist as a common clinical process, nonetheless, challenges of\ncerebral infarct segmentation come from low resolution and uncertain\nboundaries. Many segmentation techniques have been proposed and proved by\nmanual segmentation as gold standard. In order to reduce human error in\nresearch operation and clinical process, we adopt a semi-automatic segmentation\nas gold standard using Fluid-Attenuated Inversion-Recovery (FLAIR) Magnetic\nResonance Image (MRI) from the same patient under controlled environment.\nExtensive testing is performed on popular segmentation algorithms including\nOtsu method, Fuzzy C-means, Hill-climbing based segmentation, and Growcut. The\nselected segmentation techniques have been validated by accuracy, sensitivity,\nand specificity using leave-one-out cross-validation to determine the\npossibility of each techniques first then maximizes the accuracy from the\ntraining set. Our experimental results demonstrate the effectiveness of\nselected methods.\n", "versions": [{"version": "v1", "created": "Wed, 28 Mar 2018 02:13:11 GMT"}], "update_date": "2018-03-29", "authors_parsed": [["Vesdapunt", "Noranart", ""], ["Covavisaruch", "Nongluk", ""]]}, {"id": "1803.10404", "submitter": "Lele Chen", "authors": "Lele Chen, Zhiheng Li, Ross K. Maddox, Zhiyao Duan, Chenliang Xu", "title": "Lip Movements Generation at a Glance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-modality generation is an emerging topic that aims to synthesize data\nin one modality based on information in a different modality. In this paper, we\nconsider a task of such: given an arbitrary audio speech and one lip image of\narbitrary target identity, generate synthesized lip movements of the target\nidentity saying the speech. To perform well in this task, it inevitably\nrequires a model to not only consider the retention of target identity,\nphoto-realistic of synthesized images, consistency and smoothness of lip images\nin a sequence, but more importantly, learn the correlations between audio\nspeech and lip movements. To solve the collective problems, we explore the best\nmodeling of the audio-visual correlations in building and training a\nlip-movement generator network. Specifically, we devise a method to fuse audio\nand image embeddings to generate multiple lip images at once and propose a\nnovel correlation loss to synchronize lip changes and speech changes. Our final\nmodel utilizes a combination of four losses for a comprehensive consideration\nin generating lip movements; it is trained in an end-to-end fashion and is\nrobust to lip shapes, view angles and different facial characteristics.\nThoughtful experiments on three datasets ranging from lab-recorded to lips\nin-the-wild show that our model significantly outperforms other\nstate-of-the-art methods extended to this task.\n", "versions": [{"version": "v1", "created": "Wed, 28 Mar 2018 04:02:33 GMT"}, {"version": "v2", "created": "Thu, 29 Mar 2018 16:07:21 GMT"}, {"version": "v3", "created": "Mon, 21 May 2018 22:23:50 GMT"}], "update_date": "2018-05-23", "authors_parsed": [["Chen", "Lele", ""], ["Li", "Zhiheng", ""], ["Maddox", "Ross K.", ""], ["Duan", "Zhiyao", ""], ["Xu", "Chenliang", ""]]}, {"id": "1803.10409", "submitter": "Angela Dai", "authors": "Angela Dai, Matthias Nie{\\ss}ner", "title": "3DMV: Joint 3D-Multi-View Prediction for 3D Semantic Scene Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present 3DMV, a novel method for 3D semantic scene segmentation of RGB-D\nscans in indoor environments using a joint 3D-multi-view prediction network. In\ncontrast to existing methods that either use geometry or RGB data as input for\nthis task, we combine both data modalities in a joint, end-to-end network\narchitecture. Rather than simply projecting color data into a volumetric grid\nand operating solely in 3D -- which would result in insufficient detail -- we\nfirst extract feature maps from associated RGB images. These features are then\nmapped into the volumetric feature grid of a 3D network using a differentiable\nbackprojection layer. Since our target is 3D scanning scenarios with possibly\nmany frames, we use a multi-view pooling approach in order to handle a varying\nnumber of RGB input views. This learned combination of RGB and geometric\nfeatures with our joint 2D-3D architecture achieves significantly better\nresults than existing baselines. For instance, our final result on the ScanNet\n3D segmentation benchmark increases from 52.8\\% to 75\\% accuracy compared to\nexisting volumetric architectures.\n", "versions": [{"version": "v1", "created": "Wed, 28 Mar 2018 04:22:13 GMT"}], "update_date": "2018-03-29", "authors_parsed": [["Dai", "Angela", ""], ["Nie\u00dfner", "Matthias", ""]]}, {"id": "1803.10417", "submitter": "Philipp Tschandl MD PhD", "authors": "Philipp Tschandl, Cliff Rosendahl, Harald Kittler", "title": "The HAM10000 dataset, a large collection of multi-source dermatoscopic\n  images of common pigmented skin lesions", "comments": null, "journal-ref": "Sci. Data 5, 180161 (2018)", "doi": "10.1038/sdata.2018.161", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Training of neural networks for automated diagnosis of pigmented skin lesions\nis hampered by the small size and lack of diversity of available datasets of\ndermatoscopic images. We tackle this problem by releasing the HAM10000 (\"Human\nAgainst Machine with 10000 training images\") dataset. We collected\ndermatoscopic images from different populations acquired and stored by\ndifferent modalities. Given this diversity we had to apply different\nacquisition and cleaning methods and developed semi-automatic workflows\nutilizing specifically trained neural networks. The final dataset consists of\n10015 dermatoscopic images which are released as a training set for academic\nmachine learning purposes and are publicly available through the ISIC archive.\nThis benchmark dataset can be used for machine learning and for comparisons\nwith human experts. Cases include a representative collection of all important\ndiagnostic categories in the realm of pigmented lesions. More than 50% of\nlesions have been confirmed by pathology, while the ground truth for the rest\nof the cases was either follow-up, expert consensus, or confirmation by in-vivo\nconfocal microscopy.\n", "versions": [{"version": "v1", "created": "Wed, 28 Mar 2018 05:18:15 GMT"}, {"version": "v2", "created": "Mon, 2 Apr 2018 16:29:20 GMT"}, {"version": "v3", "created": "Sun, 25 Nov 2018 10:18:03 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Tschandl", "Philipp", ""], ["Rosendahl", "Cliff", ""], ["Kittler", "Harald", ""]]}, {"id": "1803.10418", "submitter": "Alptekin Temizel", "authors": "Ayse Elvan Aydemir, Alptekin Temizel, Tugba Taskaya Temizel", "title": "The Effects of JPEG and JPEG2000 Compression on Attacks using\n  Adversarial Examples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial examples are known to have a negative effect on the performance\nof classifiers which have otherwise good performance on undisturbed images.\nThese examples are generated by adding non-random noise to the testing samples\nin order to make classifier misclassify the given data. Adversarial attacks use\nthese intentionally generated examples and they pose a security risk to the\nmachine learning based systems. To be immune to such attacks, it is desirable\nto have a pre-processing mechanism which removes these effects causing\nmisclassification while keeping the content of the image. JPEG and JPEG2000 are\nwell-known image compression techniques which suppress the high-frequency\ncontent taking the human visual system into account. JPEG has been also shown\nto be an effective method for reducing adversarial noise. In this paper, we\npropose applying JPEG2000 compression as an alternative and systematically\ncompare the classification performance of adversarial images compressed using\nJPEG and JPEG2000 at different target PSNR values and maximum compression\nlevels. Our experiments show that JPEG2000 is more effective in reducing\nadversarial noise as it allows higher compression rates with less distortion\nand it does not introduce blocking artifacts.\n", "versions": [{"version": "v1", "created": "Wed, 28 Mar 2018 05:20:46 GMT"}, {"version": "v2", "created": "Sat, 31 Mar 2018 14:42:27 GMT"}], "update_date": "2018-04-03", "authors_parsed": [["Aydemir", "Ayse Elvan", ""], ["Temizel", "Alptekin", ""], ["Temizel", "Tugba Taskaya", ""]]}, {"id": "1803.10433", "submitter": "Jie Chen", "authors": "Jie Chen, Cheen-Hau Tan, Junhui Hou, Lap-Pui Chau, and He Li", "title": "Robust Video Content Alignment and Compensation for Rain Removal in a\n  CNN Framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rain removal is important for improving the robustness of outdoor vision\nbased systems. Current rain removal methods show limitations either for complex\ndynamic scenes shot from fast moving cameras, or under torrential rain fall\nwith opaque occlusions. We propose a novel derain algorithm, which applies\nsuperpixel (SP) segmentation to decompose the scene into depth consistent\nunits. Alignment of scene contents are done at the SP level, which proves to be\nrobust towards rain occlusion and fast camera motion. Two alignment output\ntensors, i.e., optimal temporal match tensor and sorted spatial-temporal match\ntensor, provide informative clues for rain streak location and occluded\nbackground contents to generate an intermediate derain output. These tensors\nwill be subsequently prepared as input features for a convolutional neural\nnetwork to restore high frequency details to the intermediate output for\ncompensation of mis-alignment blur. Extensive evaluations show that up to 5 dB\nreconstruction PSNR advantage is achieved over state-of-the-art methods. Visual\ninspection shows that much cleaner rain removal is achieved especially for\nhighly dynamic scenes with heavy and opaque rainfall from a fast moving camera.\n", "versions": [{"version": "v1", "created": "Wed, 28 Mar 2018 07:22:10 GMT"}], "update_date": "2018-03-29", "authors_parsed": [["Chen", "Jie", ""], ["Tan", "Cheen-Hau", ""], ["Hou", "Junhui", ""], ["Chau", "Lap-Pui", ""], ["Li", "He", ""]]}, {"id": "1803.10435", "submitter": "Cristiano Massaroni", "authors": "Danilo Avola, Marco Bernardi, Luigi Cinque, Gian Luca Foresti,\n  Cristiano Massaroni", "title": "Exploiting Recurrent Neural Networks and Leap Motion Controller for Sign\n  Language and Semaphoric Gesture Recognition", "comments": null, "journal-ref": "IEEE Transactions on Multimedia 21 (2019) 234-245", "doi": "10.1109/TMM.2018.2856094", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In human interactions, hands are a powerful way of expressing information\nthat, in some cases, can be used as a valid substitute for voice, as it happens\nin Sign Language. Hand gesture recognition has always been an interesting topic\nin the areas of computer vision and multimedia. These gestures can be\nrepresented as sets of feature vectors that change over time. Recurrent Neural\nNetworks (RNNs) are suited to analyse this type of sets thanks to their ability\nto model the long term contextual information of temporal sequences. In this\npaper, a RNN is trained by using as features the angles formed by the finger\nbones of human hands. The selected features, acquired by a Leap Motion\nController (LMC) sensor, have been chosen because the majority of human\ngestures produce joint movements that generate truly characteristic corners. A\nchallenging subset composed by a large number of gestures defined by the\nAmerican Sign Language (ASL) is used to test the proposed solution and the\neffectiveness of the selected angles. Moreover, the proposed method has been\ncompared to other state of the art works on the SHREC dataset, thus\ndemonstrating its superiority in hand gesture recognition accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 28 Mar 2018 07:25:51 GMT"}], "update_date": "2019-07-26", "authors_parsed": [["Avola", "Danilo", ""], ["Bernardi", "Marco", ""], ["Cinque", "Luigi", ""], ["Foresti", "Gian Luca", ""], ["Massaroni", "Cristiano", ""]]}, {"id": "1803.10464", "submitter": "Jiwoon Ahn", "authors": "Jiwoon Ahn and Suha Kwak", "title": "Learning Pixel-level Semantic Affinity with Image-level Supervision for\n  Weakly Supervised Semantic Segmentation", "comments": "Accepted by CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The deficiency of segmentation labels is one of the main obstacles to\nsemantic segmentation in the wild. To alleviate this issue, we present a novel\nframework that generates segmentation labels of images given their image-level\nclass labels. In this weakly supervised setting, trained models have been known\nto segment local discriminative parts rather than the entire object area. Our\nsolution is to propagate such local responses to nearby areas which belong to\nthe same semantic entity. To this end, we propose a Deep Neural Network (DNN)\ncalled AffinityNet that predicts semantic affinity between a pair of adjacent\nimage coordinates. The semantic propagation is then realized by random walk\nwith the affinities predicted by AffinityNet. More importantly, the supervision\nemployed to train AffinityNet is given by the initial discriminative part\nsegmentation, which is incomplete as a segmentation annotation but sufficient\nfor learning semantic affinities within small image areas. Thus the entire\nframework relies only on image-level class labels and does not require any\nextra data or annotations. On the PASCAL VOC 2012 dataset, a DNN learned with\nsegmentation labels generated by our method outperforms previous models trained\nwith the same level of supervision, and is even as competitive as those relying\non stronger supervision.\n", "versions": [{"version": "v1", "created": "Wed, 28 Mar 2018 08:40:54 GMT"}, {"version": "v2", "created": "Mon, 9 Apr 2018 05:13:41 GMT"}], "update_date": "2018-04-10", "authors_parsed": [["Ahn", "Jiwoon", ""], ["Kwak", "Suha", ""]]}, {"id": "1803.10474", "submitter": "Paul Gay", "authors": "Paul Gay and Alessio Del Bue", "title": "Objects Localisation from Motion with Constraints", "comments": "4 pages. This paper has been withdrawn as it has been merged with\n  another submission: arXiv:1807.05933", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a method to estimate the 3D object position and occupancy\ngiven a set of object detections in multiple images and calibrated cameras.\nThis problem is modelled as the estimation of a set of quadrics given 2D conics\nfit to the object bounding boxes. Although a closed form solution has been\nrecently proposed, the resulting quadrics can be inaccurate or even be non\nvalid ellipsoids in presence of noisy and inaccurate detections. This effect is\nespecially important in case of small baselines, resulting in dramatic\nfailures. To cope with this problem, we propose a set of linear constraints by\nmatching the centres of the reprojected quadrics with the centres of the\nobserved conics. These constraints can be solved with a linear system thus\nproviding a more computationally efficient solution with respect to a\nnon-linear alternative. Experiments on real data show that the proposed\napproach improves significantly the accuracy and the validity of the\nellipsoids.\n", "versions": [{"version": "v1", "created": "Wed, 28 Mar 2018 09:14:33 GMT"}, {"version": "v2", "created": "Sun, 28 Oct 2018 11:24:42 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Gay", "Paul", ""], ["Del Bue", "Alessio", ""]]}, {"id": "1803.10537", "submitter": "Jongwon Choi", "authors": "Jongwon Choi, Hyung Jin Chang, Tobias Fischer, Sangdoo Yun, Kyuewang\n  Lee, Jiyeoup Jeong, Yiannis Demiris, Jin Young Choi", "title": "Context-aware Deep Feature Compression for High-speed Visual Tracking", "comments": "9 pages, 6 figures, Accepted in CVPR2018 (IEEE conference on Computer\n  Vision and Pattern Recognition)", "journal-ref": "IEEE Conference on Computer Vision and Pattern Recognition\n  (CVPR2018)", "doi": "10.1109/CVPR.2018.00057", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new context-aware correlation filter based tracking framework to\nachieve both high computational speed and state-of-the-art performance among\nreal-time trackers. The major contribution to the high computational speed lies\nin the proposed deep feature compression that is achieved by a context-aware\nscheme utilizing multiple expert auto-encoders; a context in our framework\nrefers to the coarse category of the tracking target according to appearance\npatterns. In the pre-training phase, one expert auto-encoder is trained per\ncategory. In the tracking phase, the best expert auto-encoder is selected for a\ngiven target, and only this auto-encoder is used. To achieve high tracking\nperformance with the compressed feature map, we introduce extrinsic denoising\nprocesses and a new orthogonality loss term for pre-training and fine-tuning of\nthe expert auto-encoders. We validate the proposed context-aware framework\nthrough a number of experiments, where our method achieves a comparable\nperformance to state-of-the-art trackers which cannot run in real-time, while\nrunning at a significantly fast speed of over 100 fps.\n", "versions": [{"version": "v1", "created": "Wed, 28 Mar 2018 11:38:12 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Choi", "Jongwon", ""], ["Chang", "Hyung Jin", ""], ["Fischer", "Tobias", ""], ["Yun", "Sangdoo", ""], ["Lee", "Kyuewang", ""], ["Jeong", "Jiyeoup", ""], ["Demiris", "Yiannis", ""], ["Choi", "Jin Young", ""]]}, {"id": "1803.10548", "submitter": "Yuechao Gao", "authors": "Yuechao Gao, Nianhong Liu and Sheng Zhang", "title": "FPGA Implementations of 3D-SIMD Processor Architecture for Deep Neural\n  Networks Using Relative Indexed Compressed Sparse Filter Encoding Format and\n  Stacked Filters Stationary Flow", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is a challenging task to deploy computationally and memory intensive\nState-of-the-art deep neural networks (DNNs) on embedded systems with limited\nhardware resources and power budgets. Recently developed techniques like Deep\nCompression make it possible to fit large DNNs, such as AlexNet and VGGNet,\nfully in on-chip SRAM. But sparse networks compressed using existing encoding\nformats, like CSR or CSC, complex the computation at runtime due to their\nirregular memory access characteristics. In [1], we introduce a computation\ndataflow, stacked filters stationary dataflow (SFS), and a corresponding data\nencoding format, relative indexed compressed sparse filter format (CSF), to\nmake the best of data sparsity, and simplify data handling at execution time.\nIn this paper we present FPGA implementations of these methods. We implement\nseveral compact streaming fully connected (FC) and Convolutional (CONV) neural\nnetwork processors to show their efficiency. Comparing with the\nstate-of-the-art results [2,3,4], our methods achieve at least 2x improvement\nfor computation efficiency per PE on most layers. Especially, our methods\nachieve 8x improvement on AlexNet layer CONV4 with 384 filters, and 11x\nimprovement on VGG16 layer CONV5-3 with 512 filters.\n", "versions": [{"version": "v1", "created": "Wed, 28 Mar 2018 11:56:20 GMT"}, {"version": "v2", "created": "Sat, 7 Apr 2018 08:23:05 GMT"}, {"version": "v3", "created": "Thu, 12 Apr 2018 07:42:42 GMT"}], "update_date": "2018-04-13", "authors_parsed": [["Gao", "Yuechao", ""], ["Liu", "Nianhong", ""], ["Zhang", "Sheng", ""]]}, {"id": "1803.10560", "submitter": "Alexander Shekhovtsov", "authors": "Alexander Shekhovtsov, Boris Flach", "title": "Normalization of Neural Networks using Analytic Variance Propagation", "comments": null, "journal-ref": "In Proceedings of Computer Vision Winter Workshop 2018", "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of estimating statistics of hidden units in a neural\nnetwork using a method of analytic moment propagation. These statistics are\nuseful for approximate whitening of the inputs in front of saturating\nnon-linearities such as a sigmoid function. This is important for\ninitialization of training and for reducing the accumulated scale and bias\ndependencies (compensating covariate shift), which presumably eases the\nlearning. In batch normalization, which is currently a very widely applied\ntechnique, sample estimates of statistics of hidden units over a batch are\nused. The proposed estimation uses an analytic propagation of mean and variance\nof the training set through the network. The result depends on the network\nstructure and its current weights but not on the specific batch input. The\nestimates are suitable for initialization and normalization, efficient to\ncompute and independent of the batch size. The experimental verification well\nsupports these claims. However, the method does not share the generalization\nproperties of BN, to which our experiments give some additional insight.\n", "versions": [{"version": "v1", "created": "Wed, 28 Mar 2018 12:37:27 GMT"}], "update_date": "2018-03-29", "authors_parsed": [["Shekhovtsov", "Alexander", ""], ["Flach", "Boris", ""]]}, {"id": "1803.10562", "submitter": "Taihong Xiao", "authors": "Taihong Xiao, Jiapeng Hong, Jinwen Ma", "title": "ELEGANT: Exchanging Latent Encodings with GAN for Transferring Multiple\n  Face Attributes", "comments": "Github: https://github.com/Prinsphield/ELEGANT", "journal-ref": "European Conference on Computer Vision 2018", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Recent studies on face attribute transfer have achieved great success. A lot\nof models are able to transfer face attributes with an input image. However,\nthey suffer from three limitations: (1) incapability of generating image by\nexemplars; (2) being unable to transfer multiple face attributes\nsimultaneously; (3) low quality of generated images, such as low-resolution or\nartifacts. To address these limitations, we propose a novel model which\nreceives two images of opposite attributes as inputs. Our model can transfer\nexactly the same type of attributes from one image to another by exchanging\ncertain part of their encodings. All the attributes are encoded in a\ndisentangled manner in the latent space, which enables us to manipulate several\nattributes simultaneously. Besides, our model learns the residual images so as\nto facilitate training on higher resolution images. With the help of\nmulti-scale discriminators for adversarial training, it can even generate\nhigh-quality images with finer details and less artifacts. We demonstrate the\neffectiveness of our model on overcoming the above three limitations by\ncomparing with other methods on the CelebA face database. A pytorch\nimplementation is available at https://github.com/Prinsphield/ELEGANT.\n", "versions": [{"version": "v1", "created": "Wed, 28 Mar 2018 12:39:03 GMT"}, {"version": "v2", "created": "Wed, 25 Jul 2018 06:13:06 GMT"}], "update_date": "2018-07-26", "authors_parsed": [["Xiao", "Taihong", ""], ["Hong", "Jiapeng", ""], ["Ma", "Jinwen", ""]]}, {"id": "1803.10567", "submitter": "Tobias Hinz", "authors": "Tobias Hinz, Stefan Wermter", "title": "Image Generation and Translation with Disentangled Representations", "comments": "Accepted as a conference paper at the International Joint Conference\n  on Neural Networks (IJCNN) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative models have made significant progress in the tasks of modeling\ncomplex data distributions such as natural images. The introduction of\nGenerative Adversarial Networks (GANs) and auto-encoders lead to the\npossibility of training on big data sets in an unsupervised manner. However,\nfor many generative models it is not possible to specify what kind of image\nshould be generated and it is not possible to translate existing images into\nnew images of similar domains. Furthermore, models that can perform\nimage-to-image translation often need distinct models for each domain, making\nit hard to scale these systems to multiple domain image-to-image translation.\nWe introduce a model that can do both, controllable image generation and\nimage-to-image translation between multiple domains. We split our image\nrepresentation into two parts encoding unstructured and structured information\nrespectively. The latter is designed in a disentangled manner, so that\ndifferent parts encode different image characteristics. We train an encoder to\nencode images into these representations and use a small amount of labeled data\nto specify what kind of information should be encoded in the disentangled part.\nA generator is trained to generate images from these representations using the\ncharacteristics provided by the disentangled part of the representation.\nThrough this we can control what kind of images the generator generates,\ntranslate images between different domains, and even learn unknown\ndata-generating factors while only using one single model.\n", "versions": [{"version": "v1", "created": "Wed, 28 Mar 2018 12:53:01 GMT"}], "update_date": "2018-03-29", "authors_parsed": [["Hinz", "Tobias", ""], ["Wermter", "Stefan", ""]]}, {"id": "1803.10586", "submitter": "Tobias Pl\\\"otz", "authors": "Tobias Pl\\\"otz, Anne S. Wannenwetsch, Stefan Roth", "title": "Stochastic Variational Inference with Gradient Linearization", "comments": "To appear at CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational inference has experienced a recent surge in popularity owing to\nstochastic approaches, which have yielded practical tools for a wide range of\nmodel classes. A key benefit is that stochastic variational inference obviates\nthe tedious process of deriving analytical expressions for closed-form variable\nupdates. Instead, one simply needs to derive the gradient of the log-posterior,\nwhich is often much easier. Yet for certain model classes, the log-posterior\nitself is difficult to optimize using standard gradient techniques. One such\nexample are random field models, where optimization based on gradient\nlinearization has proven popular, since it speeds up convergence significantly\nand can avoid poor local optima. In this paper we propose stochastic\nvariational inference with gradient linearization (SVIGL). It is similarly\nconvenient as standard stochastic variational inference - all that is required\nis a local linearization of the energy gradient. Its benefit over stochastic\nvariational inference with conventional gradient methods is a clear improvement\nin convergence speed, while yielding comparable or even better variational\napproximations in terms of KL divergence. We demonstrate the benefits of SVIGL\nin three applications: Optical flow estimation, Poisson-Gaussian denoising, and\n3D surface reconstruction.\n", "versions": [{"version": "v1", "created": "Wed, 28 Mar 2018 13:22:57 GMT"}], "update_date": "2018-03-29", "authors_parsed": [["Pl\u00f6tz", "Tobias", ""], ["Wannenwetsch", "Anne S.", ""], ["Roth", "Stefan", ""]]}, {"id": "1803.10590", "submitter": "Alexander Shekhovtsov", "authors": "Alexander Shekhovtsov and Boris Flach and Michal Busta", "title": "Feed-forward Uncertainty Propagation in Belief and Neural Networks", "comments": "error corrections", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a feed-forward inference method applicable to belief and neural\nnetworks. In a belief network, the method estimates an approximate factorized\nposterior of all hidden units given the input. In neural networks the method\npropagates uncertainty of the input through all the layers. In neural networks\nwith injected noise, the method analytically takes into account uncertainties\nresulting from this noise. Such feed-forward analytic propagation is\ndifferentiable in parameters and can be trained end-to-end. Compared to\nstandard NN, which can be viewed as propagating only the means, we propagate\nthe mean and variance. The method can be useful in all scenarios that require\nknowledge of the neuron statistics, e.g. when dealing with uncertain inputs,\nconsidering sigmoid activations as probabilities of Bernoulli units, training\nthe models regularized by injected noise (dropout) or estimating activation\nstatistics over the dataset (as needed for normalization methods). In the\nexperiments we show the possible utility of the method in all these tasks as\nwell as its current limitations.\n", "versions": [{"version": "v1", "created": "Wed, 28 Mar 2018 13:26:47 GMT"}, {"version": "v2", "created": "Thu, 1 Nov 2018 17:02:02 GMT"}], "update_date": "2018-11-02", "authors_parsed": [["Shekhovtsov", "Alexander", ""], ["Flach", "Boris", ""], ["Busta", "Michal", ""]]}, {"id": "1803.10628", "submitter": "Jue Wang", "authors": "Jue Wang, Anoop Cherian, Fatih Porikli, Stephen Gould", "title": "Video Representation Learning Using Discriminative Pooling", "comments": "8 pages, 7 figures, Accepted in CVPR2018. arXiv admin note:\n  substantial text overlap with arXiv:1704.01716", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Popular deep models for action recognition in videos generate independent\npredictions for short clips, which are then pooled heuristically to assign an\naction label to the full video segment. As not all frames may characterize the\nunderlying action---indeed, many are common across multiple actions---pooling\nschemes that impose equal importance on all frames might be unfavorable. In an\nattempt to tackle this problem, we propose discriminative pooling, based on the\nnotion that among the deep features generated on all short clips, there is at\nleast one that characterizes the action. To this end, we learn a (nonlinear)\nhyperplane that separates this unknown, yet discriminative, feature from the\nrest. Applying multiple instance learning in a large-margin setup, we use the\nparameters of this separating hyperplane as a descriptor for the full video\nsegment. Since these parameters are directly related to the support vectors in\na max-margin framework, they serve as robust representations for pooling of the\nfeatures. We formulate a joint objective and an efficient solver that learns\nthese hyperplanes per video and the corresponding action classifiers over the\nhyperplanes. Our pooling scheme is end-to-end trainable within a deep\nframework. We report results from experiments on three benchmark datasets\nspanning a variety of challenges and demonstrate state-of-the-art performance\nacross these tasks.\n", "versions": [{"version": "v1", "created": "Mon, 26 Mar 2018 18:32:04 GMT"}, {"version": "v2", "created": "Thu, 29 Mar 2018 15:38:14 GMT"}], "update_date": "2018-04-02", "authors_parsed": [["Wang", "Jue", ""], ["Cherian", "Anoop", ""], ["Porikli", "Fatih", ""], ["Gould", "Stephen", ""]]}, {"id": "1803.10630", "submitter": "Jubin Johnson Dr", "authors": "Jubin Johnson, Shunsuke Yasugi, Yoichi Sugino, Sugiri Pranata and\n  Shengmei Shen", "title": "Person re-identification with fusion of hand-crafted and deep pose-based\n  body region features", "comments": "arXiv admin note: text overlap with arXiv:1711.08184,\n  arXiv:1707.00798 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification (re-ID) aims to accurately re- trieve a person from\na large-scale database of images cap- tured across multiple cameras. Existing\nworks learn deep representations using a large training subset of unique per-\nsons. However, identifying unseen persons is critical for a good re-ID\nalgorithm. Moreover, the misalignment be- tween person crops to detection\nerrors or pose variations leads to poor feature matching. In this work, we\npresent a fusion of handcrafted features and deep feature representa- tion\nlearned using multiple body parts to complement the global body features that\nachieves high performance on un- seen test images. Pose information is used to\ndetect body regions that are passed through Convolutional Neural Net- works\n(CNN) to guide feature learning. Finally, a metric learning step enables robust\ndistance matching on a dis- criminative subspace. Experimental results on 4\npopular re-ID benchmark datasets namely VIPer, DukeMTMC-reID, Market-1501 and\nCUHK03 show that the proposed method achieves state-of-the-art performance in\nimage-based per- son re-identification.\n", "versions": [{"version": "v1", "created": "Tue, 27 Mar 2018 06:09:36 GMT"}], "update_date": "2018-03-29", "authors_parsed": [["Johnson", "Jubin", ""], ["Yasugi", "Shunsuke", ""], ["Sugino", "Yoichi", ""], ["Pranata", "Sugiri", ""], ["Shen", "Shengmei", ""]]}, {"id": "1803.10681", "submitter": "Juan Castorena", "authors": "Juan Castorena and Gint Puskorius and Gaurav Pandey", "title": "Motion Guided LIDAR-camera Self-calibration and Accelerated Depth\n  Upsampling for Autonomous Vehicles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work proposes a novel motion guided method for target-less\nself-calibration of a LiDAR and camera and use the re-projection of LiDAR\npoints onto the image reference frame for real-time depth upsampling. The\ncalibration parameters are estimated by optimizing an objective function that\npenalizes distances between 2D and re-projected 3D motion vectors obtained from\ntime-synchronized image and point cloud sequences. For upsampling, a simple,\nyet effective and time efficient formulation that minimizes depth gradients\nsubject to an equality constraint involving the LiDAR measurements is proposed.\nValidation is performed on recorded real data from urban environments and\ndemonstrations that our two methods are effective and suitable to mobile\nrobotics and autonomous vehicle applications imposing real-time requirements is\nshown.\n", "versions": [{"version": "v1", "created": "Wed, 28 Mar 2018 15:26:23 GMT"}, {"version": "v2", "created": "Fri, 29 Mar 2019 14:34:57 GMT"}, {"version": "v3", "created": "Thu, 2 Jul 2020 19:53:42 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Castorena", "Juan", ""], ["Puskorius", "Gint", ""], ["Pandey", "Gaurav", ""]]}, {"id": "1803.10683", "submitter": "Ruilong Li", "authors": "Song-Hai Zhang, Ruilong Li, Xin Dong, Paul L. Rosin, Zixi Cai, Han Xi,\n  Dingcheng Yang, Hao-Zhi Huang, Shi-Min Hu", "title": "Pose2Seg: Detection Free Human Instance Segmentation", "comments": "8 pages", "journal-ref": "CVPR 2019", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The standard approach to image instance segmentation is to perform the object\ndetection first, and then segment the object from the detection bounding-box.\nMore recently, deep learning methods like Mask R-CNN perform them jointly.\nHowever, little research takes into account the uniqueness of the \"human\"\ncategory, which can be well defined by the pose skeleton. Moreover, the human\npose skeleton can be used to better distinguish instances with heavy occlusion\nthan using bounding-boxes. In this paper, we present a brand new pose-based\ninstance segmentation framework for humans which separates instances based on\nhuman pose, rather than proposal region detection. We demonstrate that our\npose-based framework can achieve better accuracy than the state-of-art\ndetection-based approach on the human instance segmentation problem, and can\nmoreover better handle occlusion. Furthermore, there are few public datasets\ncontaining many heavily occluded humans along with comprehensive annotations,\nwhich makes this a challenging problem seldom noticed by researchers.\nTherefore, in this paper we introduce a new benchmark \"Occluded Human\n(OCHuman)\", which focuses on occluded humans with comprehensive annotations\nincluding bounding-box, human pose and instance masks. This dataset contains\n8110 detailed annotated human instances within 4731 images. With an average\n0.67 MaxIoU for each person, OCHuman is the most complex and challenging\ndataset related to human instance segmentation. Through this dataset, we want\nto emphasize occlusion as a challenging problem for researchers to study.\n", "versions": [{"version": "v1", "created": "Wed, 28 Mar 2018 15:33:56 GMT"}, {"version": "v2", "created": "Thu, 15 Nov 2018 10:24:27 GMT"}, {"version": "v3", "created": "Mon, 8 Apr 2019 12:28:50 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Zhang", "Song-Hai", ""], ["Li", "Ruilong", ""], ["Dong", "Xin", ""], ["Rosin", "Paul L.", ""], ["Cai", "Zixi", ""], ["Xi", "Han", ""], ["Yang", "Dingcheng", ""], ["Huang", "Hao-Zhi", ""], ["Hu", "Shi-Min", ""]]}, {"id": "1803.10699", "submitter": "Li Ding", "authors": "Li Ding, Chenliang Xu", "title": "Weakly-Supervised Action Segmentation with Iterative Soft Boundary\n  Assignment", "comments": "CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we address the task of weakly-supervised human action\nsegmentation in long, untrimmed videos. Recent methods have relied on expensive\nlearning models, such as Recurrent Neural Networks (RNN) and Hidden Markov\nModels (HMM). However, these methods suffer from expensive computational cost,\nthus are unable to be deployed in large scale. To overcome the limitations, the\nkeys to our design are efficiency and scalability. We propose a novel action\nmodeling framework, which consists of a new temporal convolutional network,\nnamed Temporal Convolutional Feature Pyramid Network (TCFPN), for predicting\nframe-wise action labels, and a novel training strategy for weakly-supervised\nsequence modeling, named Iterative Soft Boundary Assignment (ISBA), to align\naction sequences and update the network in an iterative fashion. The proposed\nframework is evaluated on two benchmark datasets, Breakfast and Hollywood\nExtended, with four different evaluation metrics. Extensive experimental\nresults show that our methods achieve competitive or superior performance to\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 28 Mar 2018 15:58:23 GMT"}], "update_date": "2018-03-29", "authors_parsed": [["Ding", "Li", ""], ["Xu", "Chenliang", ""]]}, {"id": "1803.10704", "submitter": "Shikun Liu", "authors": "Shikun Liu, Edward Johns and Andrew J. Davison", "title": "End-to-End Multi-Task Learning with Attention", "comments": "Accepted at Computer Vision and Pattern Recognition (CVPR), 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel multi-task learning architecture, which allows learning of\ntask-specific feature-level attention. Our design, the Multi-Task Attention\nNetwork (MTAN), consists of a single shared network containing a global feature\npool, together with a soft-attention module for each task. These modules allow\nfor learning of task-specific features from the global features, whilst\nsimultaneously allowing for features to be shared across different tasks. The\narchitecture can be trained end-to-end and can be built upon any feed-forward\nneural network, is simple to implement, and is parameter efficient. We evaluate\nour approach on a variety of datasets, across both image-to-image predictions\nand image classification tasks. We show that our architecture is\nstate-of-the-art in multi-task learning compared to existing methods, and is\nalso less sensitive to various weighting schemes in the multi-task loss\nfunction. Code is available at https://github.com/lorenmt/mtan.\n", "versions": [{"version": "v1", "created": "Wed, 28 Mar 2018 16:15:45 GMT"}, {"version": "v2", "created": "Fri, 5 Apr 2019 05:57:50 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["Liu", "Shikun", ""], ["Johns", "Edward", ""], ["Davison", "Andrew J.", ""]]}, {"id": "1803.10743", "submitter": "Taco Cohen", "authors": "Taco S. Cohen and Mario Geiger and Maurice Weiler", "title": "Intertwiners between Induced Representations (with Applications to the\n  Theory of Equivariant Neural Networks)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Group equivariant and steerable convolutional neural networks (regular and\nsteerable G-CNNs) have recently emerged as a very effective model class for\nlearning from signal data such as 2D and 3D images, video, and other data where\nsymmetries are present. In geometrical terms, regular G-CNNs represent data in\nterms of scalar fields (\"feature channels\"), whereas the steerable G-CNN can\nalso use vector or tensor fields (\"capsules\") to represent data. In algebraic\nterms, the feature spaces in regular G-CNNs transform according to a regular\nrepresentation of the group G, whereas the feature spaces in Steerable G-CNNs\ntransform according to the more general induced representations of G. In order\nto make the network equivariant, each layer in a G-CNN is required to\nintertwine between the induced representations associated with its input and\noutput space.\n  In this paper we present a general mathematical framework for G-CNNs on\nhomogeneous spaces like Euclidean space or the sphere. We show, using\nelementary methods, that the layers of an equivariant network are convolutional\nif and only if the input and output feature spaces transform according to an\ninduced representation. This result, which follows from G.W. Mackey's abstract\ntheory on induced representations, establishes G-CNNs as a universal class of\nequivariant network architectures, and generalizes the important recent work of\nKondor & Trivedi on the intertwiners between regular representations.\n", "versions": [{"version": "v1", "created": "Wed, 28 Mar 2018 17:30:26 GMT"}, {"version": "v2", "created": "Fri, 30 Mar 2018 09:27:16 GMT"}], "update_date": "2018-04-02", "authors_parsed": [["Cohen", "Taco S.", ""], ["Geiger", "Mario", ""], ["Weiler", "Maurice", ""]]}, {"id": "1803.10750", "submitter": "Vasileios Belagiannis", "authors": "Vasileios Belagiannis, Azade Farshad, Fabio Galasso", "title": "Adversarial Network Compression", "comments": "18 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural network compression has recently received much attention due to the\ncomputational requirements of modern deep models. In this work, our objective\nis to transfer knowledge from a deep and accurate model to a smaller one. Our\ncontributions are threefold: (i) we propose an adversarial network compression\napproach to train the small student network to mimic the large teacher, without\nthe need for labels during training; (ii) we introduce a regularization scheme\nto prevent a trivially-strong discriminator without reducing the network\ncapacity and (iii) our approach generalizes on different teacher-student\nmodels.\n  In an extensive evaluation on five standard datasets, we show that our\nstudent has small accuracy drop, achieves better performance than other\nknowledge transfer approaches and it surpasses the performance of the same\nnetwork trained with labels. In addition, we demonstrate state-of-the-art\nresults compared to other compression strategies.\n", "versions": [{"version": "v1", "created": "Wed, 28 Mar 2018 17:38:20 GMT"}, {"version": "v2", "created": "Wed, 14 Nov 2018 17:25:53 GMT"}], "update_date": "2018-11-15", "authors_parsed": [["Belagiannis", "Vasileios", ""], ["Farshad", "Azade", ""], ["Galasso", "Fabio", ""]]}, {"id": "1803.10794", "submitter": "Matthias M\\\"uller", "authors": "Matthias M\\\"uller, Adel Bibi, Silvio Giancola, Salman Al-Subaihi,\n  Bernard Ghanem", "title": "TrackingNet: A Large-Scale Dataset and Benchmark for Object Tracking in\n  the Wild", "comments": "preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the numerous developments in object tracking, further development of\ncurrent tracking algorithms is limited by small and mostly saturated datasets.\nAs a matter of fact, data-hungry trackers based on deep-learning currently rely\non object detection datasets due to the scarcity of dedicated large-scale\ntracking datasets. In this work, we present TrackingNet, the first large-scale\ndataset and benchmark for object tracking in the wild. We provide more than 30K\nvideos with more than 14 million dense bounding box annotations. Our dataset\ncovers a wide selection of object classes in broad and diverse context. By\nreleasing such a large-scale dataset, we expect deep trackers to further\nimprove and generalize. In addition, we introduce a new benchmark composed of\n500 novel videos, modeled with a distribution similar to our training dataset.\nBy sequestering the annotation of the test set and providing an online\nevaluation server, we provide a fair benchmark for future development of object\ntrackers. Deep trackers fine-tuned on a fraction of our dataset improve their\nperformance by up to 1.6% on OTB100 and up to 1.7% on TrackingNet Test. We\nprovide an extensive benchmark on TrackingNet by evaluating more than 20\ntrackers. Our results suggest that object tracking in the wild is far from\nbeing solved.\n", "versions": [{"version": "v1", "created": "Wed, 28 Mar 2018 18:30:17 GMT"}], "update_date": "2018-03-30", "authors_parsed": [["M\u00fcller", "Matthias", ""], ["Bibi", "Adel", ""], ["Giancola", "Silvio", ""], ["Al-Subaihi", "Salman", ""], ["Ghanem", "Bernard", ""]]}, {"id": "1803.10806", "submitter": "Louis-\\'Emile Robitaille", "authors": "Louis-\\'Emile Robitaille, Audrey Durand, Marc-Andr\\'e Gardner,\n  Christian Gagn\\'e, Paul De Koninck, Flavie Lavoie-Cardinal", "title": "Learning to Become an Expert: Deep Networks Applied To Super-Resolution\n  Microscopy", "comments": "Accepted to the Thirtieth Innovative Applications of Artificial\n  Intelligence Conference (IAAI), 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With super-resolution optical microscopy, it is now possible to observe\nmolecular interactions in living cells. The obtained images have a very high\nspatial precision but their overall quality can vary a lot depending on the\nstructure of interest and the imaging parameters. Moreover, evaluating this\nquality is often difficult for non-expert users. In this work, we tackle the\nproblem of learning the quality function of super- resolution images from\nscores provided by experts. More specifically, we are proposing a system based\non a deep neural network that can provide a quantitative quality measure of a\nSTED image of neuronal structures given as input. We conduct a user study in\norder to evaluate the quality of the predictions of the neural network against\nthose of a human expert. Results show the potential while highlighting some of\nthe limits of the proposed approach.\n", "versions": [{"version": "v1", "created": "Wed, 28 Mar 2018 19:01:45 GMT"}], "update_date": "2018-03-30", "authors_parsed": [["Robitaille", "Louis-\u00c9mile", ""], ["Durand", "Audrey", ""], ["Gardner", "Marc-Andr\u00e9", ""], ["Gagn\u00e9", "Christian", ""], ["De Koninck", "Paul", ""], ["Lavoie-Cardinal", "Flavie", ""]]}, {"id": "1803.10827", "submitter": "Kiana Ehsani", "authors": "Kiana Ehsani, Hessam Bagherinezhad, Joseph Redmon, Roozbeh Mottaghi,\n  Ali Farhadi", "title": "Who Let The Dogs Out? Modeling Dog Behavior From Visual Data", "comments": "Accepted to CVPR18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the task of directly modeling a visually intelligent agent.\nComputer vision typically focuses on solving various subtasks related to visual\nintelligence. We depart from this standard approach to computer vision; instead\nwe directly model a visually intelligent agent. Our model takes visual\ninformation as input and directly predicts the actions of the agent. Toward\nthis end we introduce DECADE, a large-scale dataset of ego-centric videos from\na dog's perspective as well as her corresponding movements. Using this data we\nmodel how the dog acts and how the dog plans her movements. We show under a\nvariety of metrics that given just visual input we can successfully model this\nintelligent agent in many situations. Moreover, the representation learned by\nour model encodes distinct information compared to representations trained on\nimage classification, and our learned representation can generalize to other\ndomains. In particular, we show strong results on the task of walkable surface\nestimation by using this dog modeling task as representation learning.\n", "versions": [{"version": "v1", "created": "Wed, 28 Mar 2018 19:43:33 GMT"}, {"version": "v2", "created": "Thu, 17 May 2018 20:00:03 GMT"}], "update_date": "2018-05-21", "authors_parsed": [["Ehsani", "Kiana", ""], ["Bagherinezhad", "Hessam", ""], ["Redmon", "Joseph", ""], ["Mottaghi", "Roozbeh", ""], ["Farhadi", "Ali", ""]]}, {"id": "1803.10842", "submitter": "Stefan Schneider Mr", "authors": "Stefan Schneider, Graham W. Taylor, Stefan C. Kremer", "title": "Deep Learning Object Detection Methods for Ecological Camera Trap Data", "comments": "8 pages, 6 figures, Conference of Computer and Robot Vision", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep learning methods for computer vision tasks show promise for automating\nthe data analysis of camera trap images. Ecological camera traps are a common\napproach for monitoring an ecosystem's animal population, as they provide\ncontinual insight into an environment without being intrusive. However, the\nanalysis of camera trap images is expensive, labour intensive, and time\nconsuming. Recent advances in the field of deep learning for object detection\nshow promise towards automating the analysis of camera trap images. Here, we\ndemonstrate their capabilities by training and comparing two deep learning\nobject detection classifiers, Faster R-CNN and YOLO v2.0, to identify,\nquantify, and localize animal species within camera trap images using the\nReconyx Camera Trap and the self-labeled Gold Standard Snapshot Serengeti data\nsets. When trained on large labeled datasets, object recognition methods have\nshown success. We demonstrate their use, in the context of realistically sized\necological data sets, by testing if object detection methods are applicable for\necological research scenarios when utilizing transfer learning. Faster R-CNN\noutperformed YOLO v2.0 with average accuracies of 93.0\\% and 76.7\\% on the two\ndata sets, respectively. Our findings show promising steps towards the\nautomation of the labourious task of labeling camera trap images, which can be\nused to improve our understanding of the population dynamics of ecosystems\nacross the planet.\n", "versions": [{"version": "v1", "created": "Wed, 28 Mar 2018 20:30:39 GMT"}], "update_date": "2018-03-30", "authors_parsed": [["Schneider", "Stefan", ""], ["Taylor", "Graham W.", ""], ["Kremer", "Stefan C.", ""]]}, {"id": "1803.10850", "submitter": "Yannick Hold-Geoffroy", "authors": "Yannick Hold-Geoffroy, Paulo F.U. Gotardo, Jean-Fran\\c{c}ois Lalonde", "title": "Single Day Outdoor Photometric Stereo", "comments": "To appear in IEEE Transactions on Pattern Analysis and Machine\n  Intelligence 2019, 0162-8828", "journal-ref": null, "doi": "10.1109/TPAMI.2019.2962693", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Photometric Stereo (PS) under outdoor illumination remains a challenging,\nill-posed problem due to insufficient variability in illumination. Months-long\ncapture sessions are typically used in this setup, with little success on\nshorter, single-day time intervals. In this paper, we investigate the solution\nof outdoor PS over a single day, under different weather conditions. First, we\ninvestigate the relationship between weather and surface reconstructability in\norder to understand when natural lighting allows existing PS algorithms to\nwork. Our analysis reveals that partially cloudy days improve the conditioning\nof the outdoor PS problem while sunny days do not allow the unambiguous\nrecovery of surface normals from photometric cues alone. We demonstrate that\ncalibrated PS algorithms can thus be employed to reconstruct Lambertian\nsurfaces accurately under partially cloudy days. Second, we solve the ambiguity\narising in clear days by combining photometric cues with prior knowledge on\nmaterial properties, local surface geometry and the natural variations in\noutdoor lighting through a CNN-based, weakly-calibrated PS technique. Given a\nsequence of outdoor images captured during a single sunny day, our method\nrobustly estimates the scene surface normals with unprecedented quality for the\nconsidered scenario. Our approach does not require precise geolocation and\nsignificantly outperforms several state-of-the-art methods on images with real\nlighting, showing that our CNN can combine efficiently learned priors and\nphotometric cues available during a single sunny day.\n", "versions": [{"version": "v1", "created": "Wed, 28 Mar 2018 20:55:42 GMT"}, {"version": "v2", "created": "Thu, 10 May 2018 04:21:35 GMT"}, {"version": "v3", "created": "Mon, 30 Dec 2019 01:19:32 GMT"}, {"version": "v4", "created": "Fri, 3 Jan 2020 02:05:08 GMT"}], "update_date": "2020-01-06", "authors_parsed": [["Hold-Geoffroy", "Yannick", ""], ["Gotardo", "Paulo F. U.", ""], ["Lalonde", "Jean-Fran\u00e7ois", ""]]}, {"id": "1803.10859", "submitter": "Ergys Ristani", "authors": "Ergys Ristani, Carlo Tomasi", "title": "Features for Multi-Target Multi-Camera Tracking and Re-Identification", "comments": "Accepted as spotlight at CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Multi-Target Multi-Camera Tracking (MTMCT) tracks many people through video\ntaken from several cameras. Person Re-Identification (Re-ID) retrieves from a\ngallery images of people similar to a person query image. We learn good\nfeatures for both MTMCT and Re-ID with a convolutional neural network. Our\ncontributions include an adaptive weighted triplet loss for training and a new\ntechnique for hard-identity mining. Our method outperforms the state of the art\nboth on the DukeMTMC benchmarks for tracking, and on the Market-1501 and\nDukeMTMC-ReID benchmarks for Re-ID. We examine the correlation between good\nRe-ID and good MTMCT scores, and perform ablation studies to elucidate the\ncontributions of the main components of our system. Code is available.\n", "versions": [{"version": "v1", "created": "Wed, 28 Mar 2018 21:23:23 GMT"}], "update_date": "2018-03-30", "authors_parsed": [["Ristani", "Ergys", ""], ["Tomasi", "Carlo", ""]]}, {"id": "1803.10861", "submitter": "Samuel Schulter", "authors": "Tuan-Hung Vu, Wongun Choi, Samuel Schulter, Manmohan Chandraker", "title": "Memory Warps for Learning Long-Term Online Video Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel memory-based online video representation that is\nefficient, accurate and predictive. This is in contrast to prior works that\noften rely on computationally heavy 3D convolutions, ignore actual motion when\naligning features over time, or operate in an off-line mode to utilize future\nframes. In particular, our memory (i) holds the feature representation, (ii) is\nspatially warped over time to compensate for observer and scene motions, (iii)\ncan carry long-term information, and (iv) enables predicting feature\nrepresentations in future frames. By exploring a variant that operates at\nmultiple temporal scales, we efficiently learn across even longer time\nhorizons. We apply our online framework to object detection in videos,\nobtaining a large 2.3 times speed-up and losing only 0.9% mAP on ImageNet-VID\ndataset, compared to prior works that even use future frames. Finally, we\ndemonstrate the predictive property of our representation in two novel\ndetection setups, where features are propagated over time to (i) significantly\nenhance a real-time detector by more than 10% mAP in a multi-threaded online\nsetup and to (ii) anticipate objects in future frames.\n", "versions": [{"version": "v1", "created": "Wed, 28 Mar 2018 21:36:40 GMT"}], "update_date": "2018-03-30", "authors_parsed": [["Vu", "Tuan-Hung", ""], ["Choi", "Wongun", ""], ["Schulter", "Samuel", ""], ["Chandraker", "Manmohan", ""]]}, {"id": "1803.10862", "submitter": "Patricio Loncomilla", "authors": "Javier Ruiz-del-Solar, Patricio Loncomilla, Naiomi Soto", "title": "A Survey on Deep Learning Methods for Robot Vision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has allowed a paradigm shift in pattern recognition, from using\nhand-crafted features together with statistical classifiers to using\ngeneral-purpose learning procedures for learning data-driven representations,\nfeatures, and classifiers together. The application of this new paradigm has\nbeen particularly successful in computer vision, in which the development of\ndeep learning methods for vision applications has become a hot research topic.\nGiven that deep learning has already attracted the attention of the robot\nvision community, the main purpose of this survey is to address the use of deep\nlearning in robot vision. To achieve this, a comprehensive overview of deep\nlearning and its usage in computer vision is given, that includes a description\nof the most frequently used neural models and their main application areas.\nThen, the standard methodology and tools used for designing deep-learning based\nvision systems are presented. Afterwards, a review of the principal work using\ndeep learning in robot vision is presented, as well as current and future\ntrends related to the use of deep learning in robotics. This survey is intended\nto be a guide for the developers of robot vision systems.\n", "versions": [{"version": "v1", "created": "Wed, 28 Mar 2018 21:37:14 GMT"}], "update_date": "2018-03-30", "authors_parsed": [["Ruiz-del-Solar", "Javier", ""], ["Loncomilla", "Patricio", ""], ["Soto", "Naiomi", ""]]}, {"id": "1803.10864", "submitter": "Chendi Wang", "authors": "Chendi Wang", "title": "Human Emotional Facial Expression Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An automatic Facial Expression Recognition (FER) model with Adaboost face\ndetector, feature selection based on manifold learning and synergetic prototype\nbased classifier has been proposed. Improved feature selection method and\nproposed classifier can achieve favorable effectiveness to performance FER in\nreasonable processing time.\n", "versions": [{"version": "v1", "created": "Wed, 28 Mar 2018 21:44:21 GMT"}], "update_date": "2018-03-30", "authors_parsed": [["Wang", "Chendi", ""]]}, {"id": "1803.10870", "submitter": "Samuel Schulter", "authors": "Samuel Schulter, Menghua Zhai, Nathan Jacobs, Manmohan Chandraker", "title": "Learning to Look around Objects for Top-View Representations of Outdoor\n  Scenes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a single RGB image of a complex outdoor road scene in the perspective\nview, we address the novel problem of estimating an occlusion-reasoned semantic\nscene layout in the top-view. This challenging problem not only requires an\naccurate understanding of both the 3D geometry and the semantics of the visible\nscene, but also of occluded areas. We propose a convolutional neural network\nthat learns to predict occluded portions of the scene layout by looking around\nforeground objects like cars or pedestrians. But instead of hallucinating RGB\nvalues, we show that directly predicting the semantics and depths in the\noccluded areas enables a better transformation into the top-view. We further\nshow that this initial top-view representation can be significantly enhanced by\nlearning priors and rules about typical road layouts from simulated or, if\navailable, map data. Crucially, training our model does not require costly or\nsubjective human annotations for occluded areas or the top-view, but rather\nuses readily available annotations for standard semantic segmentation. We\nextensively evaluate and analyze our approach on the KITTI and Cityscapes data\nsets.\n", "versions": [{"version": "v1", "created": "Wed, 28 Mar 2018 22:33:17 GMT"}], "update_date": "2018-03-30", "authors_parsed": [["Schulter", "Samuel", ""], ["Zhai", "Menghua", ""], ["Jacobs", "Nathan", ""], ["Chandraker", "Manmohan", ""]]}, {"id": "1803.10892", "submitter": "Agrim Gupta", "authors": "Agrim Gupta, Justin Johnson, Li Fei-Fei, Silvio Savarese, Alexandre\n  Alahi", "title": "Social GAN: Socially Acceptable Trajectories with Generative Adversarial\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding human motion behavior is critical for autonomous moving\nplatforms (like self-driving cars and social robots) if they are to navigate\nhuman-centric environments. This is challenging because human motion is\ninherently multimodal: given a history of human motion paths, there are many\nsocially plausible ways that people could move in the future. We tackle this\nproblem by combining tools from sequence prediction and generative adversarial\nnetworks: a recurrent sequence-to-sequence model observes motion histories and\npredicts future behavior, using a novel pooling mechanism to aggregate\ninformation across people. We predict socially plausible futures by training\nadversarially against a recurrent discriminator, and encourage diverse\npredictions with a novel variety loss. Through experiments on several datasets\nwe demonstrate that our approach outperforms prior work in terms of accuracy,\nvariety, collision avoidance, and computational complexity.\n", "versions": [{"version": "v1", "created": "Thu, 29 Mar 2018 01:24:02 GMT"}], "update_date": "2018-03-30", "authors_parsed": [["Gupta", "Agrim", ""], ["Johnson", "Justin", ""], ["Fei-Fei", "Li", ""], ["Savarese", "Silvio", ""], ["Alahi", "Alexandre", ""]]}, {"id": "1803.10896", "submitter": "Jia Xue", "authors": "Jia Xue, Hang Zhang, Kristin Dana", "title": "Deep Texture Manifold for Ground Terrain Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a texture network called Deep Encoding Pooling Network (DEP) for\nthe task of ground terrain recognition. Recognition of ground terrain is an\nimportant task in establishing robot or vehicular control parameters, as well\nas for localization within an outdoor environment. The architecture of DEP\nintegrates orderless texture details and local spatial information and the\nperformance of DEP surpasses state-of-the-art methods for this task. The GTOS\ndatabase (comprised of over 30,000 images of 40 classes of ground terrain in\noutdoor scenes) enables supervised recognition. For evaluation under realistic\nconditions, we use test images that are not from the existing GTOS dataset, but\nare instead from hand-held mobile phone videos of similar terrain. This new\nevaluation dataset, GTOS-mobile, consists of 81 videos of 31 classes of ground\nterrain such as grass, gravel, asphalt and sand. The resultant network shows\nexcellent performance not only for GTOS-mobile, but also for more general\ndatabases (MINC and DTD). Leveraging the discriminant features learned from\nthis network, we build a new texture manifold called DEP-manifold. We learn a\nparametric distribution in feature space in a fully supervised manner, which\ngives the distance relationship among classes and provides a means to\nimplicitly represent ambiguous class boundaries. The source code and database\nare publicly available.\n", "versions": [{"version": "v1", "created": "Thu, 29 Mar 2018 01:56:54 GMT"}, {"version": "v2", "created": "Tue, 3 Apr 2018 01:09:04 GMT"}], "update_date": "2018-04-04", "authors_parsed": [["Xue", "Jia", ""], ["Zhang", "Hang", ""], ["Dana", "Kristin", ""]]}, {"id": "1803.10906", "submitter": "Jiyang Gao", "authors": "Jiyang Gao, Runzhou Ge, Kan Chen, Ram Nevatia", "title": "Motion-Appearance Co-Memory Networks for Video Question Answering", "comments": "CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video Question Answering (QA) is an important task in understanding video\ntemporal structure. We observe that there are three unique attributes of video\nQA compared with image QA: (1) it deals with long sequences of images\ncontaining richer information not only in quantity but also in variety; (2)\nmotion and appearance information are usually correlated with each other and\nable to provide useful attention cues to the other; (3) different questions\nrequire different number of frames to infer the answer. Based these\nobservations, we propose a motion-appearance comemory network for video QA. Our\nnetworks are built on concepts from Dynamic Memory Network (DMN) and introduces\nnew mechanisms for video QA. Specifically, there are three salient aspects: (1)\na co-memory attention mechanism that utilizes cues from both motion and\nappearance to generate attention; (2) a temporal conv-deconv network to\ngenerate multi-level contextual facts; (3) a dynamic fact ensemble method to\nconstruct temporal representation dynamically for different questions. We\nevaluate our method on TGIF-QA dataset, and the results outperform\nstate-of-the-art significantly on all four tasks of TGIF-QA.\n", "versions": [{"version": "v1", "created": "Thu, 29 Mar 2018 02:47:49 GMT"}], "update_date": "2018-03-30", "authors_parsed": [["Gao", "Jiyang", ""], ["Ge", "Runzhou", ""], ["Chen", "Kan", ""], ["Nevatia", "Ram", ""]]}, {"id": "1803.10910", "submitter": "Yuchao Dai Dr.", "authors": "Jing Zhang, Tong Zhang, Yuchao Dai, Mehrtash Harandi, Richard Hartley", "title": "Deep Unsupervised Saliency Detection: A Multiple Noisy Labeling\n  Perspective", "comments": "Accepted by IEEE/CVF CVPR 2018 as Spotlight", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The success of current deep saliency detection methods heavily depends on the\navailability of large-scale supervision in the form of per-pixel labeling. Such\nsupervision, while labor-intensive and not always possible, tends to hinder the\ngeneralization ability of the learned models. By contrast, traditional\nhandcrafted features based unsupervised saliency detection methods, even though\nhave been surpassed by the deep supervised methods, are generally\ndataset-independent and could be applied in the wild. This raises a natural\nquestion that \"Is it possible to learn saliency maps without using labeled data\nwhile improving the generalization ability?\". To this end, we present a novel\nperspective to unsupervised saliency detection through learning from multiple\nnoisy labeling generated by \"weak\" and \"noisy\" unsupervised handcrafted\nsaliency methods. Our end-to-end deep learning framework for unsupervised\nsaliency detection consists of a latent saliency prediction module and a noise\nmodeling module that work collaboratively and are optimized jointly. Explicit\nnoise modeling enables us to deal with noisy saliency maps in a probabilistic\nway. Extensive experimental results on various benchmarking datasets show that\nour model not only outperforms all the unsupervised saliency methods with a\nlarge margin but also achieves comparable performance with the recent\nstate-of-the-art supervised deep saliency methods.\n", "versions": [{"version": "v1", "created": "Thu, 29 Mar 2018 03:05:28 GMT"}], "update_date": "2018-03-30", "authors_parsed": [["Zhang", "Jing", ""], ["Zhang", "Tong", ""], ["Dai", "Yuchao", ""], ["Harandi", "Mehrtash", ""], ["Hartley", "Richard", ""]]}, {"id": "1803.10914", "submitter": "Zheng Liu", "authors": "Zheng Liu, Jie Qin, Annan Li, Yunhong Wang, and Luc Van Gool", "title": "Adversarial Binary Coding for Efficient Person Re-identification", "comments": "17 pages, 6 figures, 8 tables. Codes:\n  https://github.com/dongb5/AdversarialBinaryCoding4ReID", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification (ReID) aims at matching persons across different\nviews/scenes. In addition to accuracy, the matching efficiency has received\nmore and more attention because of demanding applications using large-scale\ndata. Several binary coding based methods have been proposed for efficient\nReID, which either learn projections to map high-dimensional features to\ncompact binary codes, or directly adopt deep neural networks by simply\ninserting an additional fully-connected layer with tanh-like activations.\nHowever, the former approach requires time-consuming hand-crafted feature\nextraction and complicated (discrete) optimizations; the latter lacks the\nnecessary discriminative information greatly due to the straightforward\nactivation functions. In this paper, we propose a simple yet effective\nframework for efficient ReID inspired by the recent advances in adversarial\nlearning. Specifically, instead of learning explicit projections or adding\nfully-connected mapping layers, the proposed Adversarial Binary Coding (ABC)\nframework guides the extraction of binary codes implicitly and effectively. The\ndiscriminability of the extracted codes is further enhanced by equipping the\nABC with a deep triplet network for the ReID task. More importantly, the ABC\nand triplet network are simultaneously optimized in an end-to-end manner.\nExtensive experiments on three large-scale ReID benchmarks demonstrate the\nsuperiority of our approach over the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 29 Mar 2018 03:24:06 GMT"}, {"version": "v2", "created": "Mon, 2 Apr 2018 04:14:16 GMT"}, {"version": "v3", "created": "Fri, 6 Apr 2018 03:24:35 GMT"}], "update_date": "2018-04-09", "authors_parsed": [["Liu", "Zheng", ""], ["Qin", "Jie", ""], ["Li", "Annan", ""], ["Wang", "Yunhong", ""], ["Van Gool", "Luc", ""]]}, {"id": "1803.10930", "submitter": "Hideo Terada", "authors": "Hideo Terada, Hayaru Shouno", "title": "B-DCGAN:Evaluation of Binarized DCGAN for FPGA", "comments": "10 pages", "journal-ref": null, "doi": "10.1007/978-3-030-36708-4_5", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are trying to implement deep neural networks in the edge computing\nenvironment for real-world applications such as the IoT(Internet of Things),\nthe FinTech etc., for the purpose of utilizing the significant achievement of\nDeep Learning in recent years. Especially, we now focus algorithm\nimplementation on FPGA, because FPGA is one of the promising devices for\nlow-cost and low-power implementation of the edge computer. In this work, we\nintroduce Binary-DCGAN(B-DCGAN) - Deep Convolutional GAN model with binary\nweights and activations, and with using integer-valued operations in forward\npass(train-time and run-time). And we show how to implement B-DCGAN on\nFPGA(Xilinx Zynq). Using the B-DCGAN, we do feasibility study of FPGA's\ncharacteristic and performance for Deep Learning. Because the binarization and\nusing integer-valued operation reduce the memory capacity and the number of the\ncircuit gates, it is very effective for FPGA implementation. On the other hand,\nthe quality of generated data from the model will be decreased by these\nreductions. So we investigate the influence of these reductions.\n", "versions": [{"version": "v1", "created": "Thu, 29 Mar 2018 05:43:23 GMT"}, {"version": "v2", "created": "Wed, 6 Nov 2019 08:57:41 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Terada", "Hideo", ""], ["Shouno", "Hayaru", ""]]}, {"id": "1803.10932", "submitter": "Jhony Kaesemodel Pontes", "authors": "Dominic Jack, Jhony K. Pontes, Sridha Sridharan, Clinton Fookes, Sareh\n  Shirazi, Frederic Maire, Anders Eriksson", "title": "Learning Free-Form Deformations for 3D Object Reconstruction", "comments": "16 pages, 7 figures, 3 tables", "journal-ref": "Asian Conference on Computer Vision (ACCV) 2018", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Representing 3D shape in deep learning frameworks in an accurate, efficient\nand compact manner still remains an open challenge. Most existing work\naddresses this issue by employing voxel-based representations. While these\napproaches benefit greatly from advances in computer vision by generalizing 2D\nconvolutions to the 3D setting, they also have several considerable drawbacks.\nThe computational complexity of voxel-encodings grows cubically with the\nresolution thus limiting such representations to low-resolution 3D\nreconstruction. In an attempt to solve this problem, point cloud\nrepresentations have been proposed. Although point clouds are more efficient\nthan voxel representations as they only cover surfaces rather than volumes,\nthey do not encode detailed geometric information about relationships between\npoints. In this paper we propose a method to learn free-form deformations (FFD)\nfor the task of 3D reconstruction from a single image. By learning to deform\npoints sampled from a high-quality mesh, our trained model can be used to\nproduce arbitrarily dense point clouds or meshes with fine-grained geometry. We\nevaluate our proposed framework on both synthetic and real-world data and\nachieve state-of-the-art results on point-cloud and volumetric metrics.\nAdditionally, we qualitatively demonstrate its applicability to label\ntransferring for 3D semantic segmentation.\n", "versions": [{"version": "v1", "created": "Thu, 29 Mar 2018 05:56:15 GMT"}], "update_date": "2019-01-25", "authors_parsed": [["Jack", "Dominic", ""], ["Pontes", "Jhony K.", ""], ["Sridharan", "Sridha", ""], ["Fookes", "Clinton", ""], ["Shirazi", "Sareh", ""], ["Maire", "Frederic", ""], ["Eriksson", "Anders", ""]]}, {"id": "1803.10967", "submitter": "Simon Niklaus", "authors": "Simon Niklaus, Feng Liu", "title": "Context-aware Synthesis for Video Frame Interpolation", "comments": "CVPR 2018, http://graphics.cs.pdx.edu/project/ctxsyn", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video frame interpolation algorithms typically estimate optical flow or its\nvariations and then use it to guide the synthesis of an intermediate frame\nbetween two consecutive original frames. To handle challenges like occlusion,\nbidirectional flow between the two input frames is often estimated and used to\nwarp and blend the input frames. However, how to effectively blend the two\nwarped frames still remains a challenging problem. This paper presents a\ncontext-aware synthesis approach that warps not only the input frames but also\ntheir pixel-wise contextual information and uses them to interpolate a\nhigh-quality intermediate frame. Specifically, we first use a pre-trained\nneural network to extract per-pixel contextual information for input frames. We\nthen employ a state-of-the-art optical flow algorithm to estimate bidirectional\nflow between them and pre-warp both input frames and their context maps.\nFinally, unlike common approaches that blend the pre-warped frames, our method\nfeeds them and their context maps to a video frame synthesis neural network to\nproduce the interpolated frame in a context-aware fashion. Our neural network\nis fully convolutional and is trained end to end. Our experiments show that our\nmethod can handle challenging scenarios such as occlusion and large motion and\noutperforms representative state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Thu, 29 Mar 2018 08:56:14 GMT"}], "update_date": "2018-03-30", "authors_parsed": [["Niklaus", "Simon", ""], ["Liu", "Feng", ""]]}, {"id": "1803.10988", "submitter": "Mehdi Ghatee Dr.", "authors": "Fateme Teimouri and Mehdi Ghatee", "title": "A real-time warning system for rear-end collision based on random forest\n  classifier", "comments": "26 Pages, 7 Tables, 16 Figures, Extracted from an MSc project with\n  Department of Computer Science, Amirkabir University of Technology, Tehran,\n  Iran", "journal-ref": "Journal of Soft Computing in Civil Engineering, 4(1), pp. 49-71,\n  2020", "doi": "10.22115/scce.2020.217605.1172", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rear-end collision warning system has a great role to enhance the driving\nsafety. In this system some measures are used to estimate the dangers and the\nsystem warns drivers to be more cautious. The real-time processes should be\nexecuted in such system, to remain enough time and distance to avoid collision\nwith the front vehicle. To this end, in this paper a new system is developed by\nusing random forest classifier. To evaluate the performance of the proposed\nsystem, vehicles trajectory data of 100 car's database from Virginia tech\ntransportation institute are used and the methods are compared based on their\naccuracy and their processing time. By using TOPSIS multi-criteria selection\nmethod, we show that the results of the implemented classifier is better than\nthe results of different classifiers including Bayesian network, naive Bayes,\nMLP neural network, support vector machine, nearest neighbor, rule-based\nmethods and decision tree. The presented experiments reveals that the random\nforest is an acceptable algorithm for the proposed driver assistant system with\n88.4% accuracy for detecting warning situations and 94.7% for detecting safe\nsituations.\n", "versions": [{"version": "v1", "created": "Thu, 29 Mar 2018 09:51:26 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Teimouri", "Fateme", ""], ["Ghatee", "Mehdi", ""]]}, {"id": "1803.11029", "submitter": "Dan Xu", "authors": "Dan Xu, Wei Wang, Hao Tang, Hong Liu, Nicu Sebe, Elisa Ricci", "title": "Structured Attention Guided Convolutional Neural Fields for Monocular\n  Depth Estimation", "comments": "Accepted at CVPR 2018 as a spotlight", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent works have shown the benefit of integrating Conditional Random Fields\n(CRFs) models into deep architectures for improving pixel-level prediction\ntasks. Following this line of research, in this paper we introduce a novel\napproach for monocular depth estimation. Similarly to previous works, our\nmethod employs a continuous CRF to fuse multi-scale information derived from\ndifferent layers of a front-end Convolutional Neural Network (CNN). Differently\nfrom past works, our approach benefits from a structured attention model which\nautomatically regulates the amount of information transferred between\ncorresponding features at different scales. Importantly, the proposed attention\nmodel is seamlessly integrated into the CRF, allowing end-to-end training of\nthe entire architecture. Our extensive experimental evaluation demonstrates the\neffectiveness of the proposed method which is competitive with previous methods\non the KITTI benchmark and outperforms the state of the art on the NYU Depth V2\ndataset.\n", "versions": [{"version": "v1", "created": "Thu, 29 Mar 2018 12:27:34 GMT"}], "update_date": "2018-03-30", "authors_parsed": [["Xu", "Dan", ""], ["Wang", "Wei", ""], ["Tang", "Hao", ""], ["Liu", "Hong", ""], ["Sebe", "Nicu", ""], ["Ricci", "Elisa", ""]]}, {"id": "1803.11064", "submitter": "Anoop Cherian", "authors": "Anoop Cherian, Suvrit Sra, Stephen Gould, Richard Hartley", "title": "Non-Linear Temporal Subspace Representations for Activity Recognition", "comments": "Accepted at the IEEE International Conference on Computer Vision and\n  Pattern Recognition, CVPR, 2018. arXiv admin note: substantial text overlap\n  with arXiv:1705.08583", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Representations that can compactly and effectively capture the temporal\nevolution of semantic content are important to computer vision and machine\nlearning algorithms that operate on multi-variate time-series data. We\ninvestigate such representations motivated by the task of human action\nrecognition. Here each data instance is encoded by a multivariate feature (such\nas via a deep CNN) where action dynamics are characterized by their variations\nin time. As these features are often non-linear, we propose a novel pooling\nmethod, kernelized rank pooling, that represents a given sequence compactly as\nthe pre-image of the parameters of a hyperplane in a reproducing kernel Hilbert\nspace, projections of data onto which captures their temporal order. We develop\nthis idea further and show that such a pooling scheme can be cast as an\norder-constrained kernelized PCA objective. We then propose to use the\nparameters of a kernelized low-rank feature subspace as the representation of\nthe sequences. We cast our formulation as an optimization problem on\ngeneralized Grassmann manifolds and then solve it efficiently using Riemannian\noptimization techniques. We present experiments on several action recognition\ndatasets using diverse feature modalities and demonstrate state-of-the-art\nresults.\n", "versions": [{"version": "v1", "created": "Tue, 27 Mar 2018 20:11:04 GMT"}], "update_date": "2018-03-30", "authors_parsed": [["Cherian", "Anoop", ""], ["Sra", "Suvrit", ""], ["Gould", "Stephen", ""], ["Hartley", "Richard", ""]]}, {"id": "1803.11078", "submitter": "Raein Hashemi", "authors": "Seyed Raein Hashemi, Seyed Sadegh Mohseni Salehi, Deniz Erdogmus,\n  Sanjay P. Prabhu, Simon K. Warfield, Ali Gholipour", "title": "Asymmetric Loss Functions and Deep Densely Connected Networks for Highly\n  Imbalanced Medical Image Segmentation: Application to Multiple Sclerosis\n  Lesion Detection", "comments": "IEEE Access", "journal-ref": null, "doi": "10.1109/ACCESS.2018.2886371", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fully convolutional deep neural networks have been asserted to be fast and\nprecise frameworks with great potential in image segmentation. One of the major\nchallenges in training such networks raises when data is unbalanced, which is\ncommon in many medical imaging applications such as lesion segmentation where\nlesion class voxels are often much lower in numbers than non-lesion voxels. A\ntrained network with unbalanced data may make predictions with high precision\nand low recall, being severely biased towards the non-lesion class which is\nparticularly undesired in most medical applications where FNs are more\nimportant than FPs. Various methods have been proposed to address this problem,\nmore recently similarity loss functions and focal loss. In this work we trained\nfully convolutional deep neural networks using an asymmetric similarity loss\nfunction to mitigate the issue of data imbalance and achieve much better\ntradeoff between precision and recall. To this end, we developed a 3D\nFC-DenseNet with large overlapping image patches as input and an asymmetric\nsimilarity loss layer based on Tversky index (using Fbeta scores). We used\nlarge overlapping image patches as inputs for intrinsic and extrinsic data\naugmentation, a patch selection algorithm, and a patch prediction fusion\nstrategy using B-spline weighted soft voting to account for the uncertainty of\nprediction in patch borders. We applied this method to MS lesion segmentation\nbased on two different datasets of MSSEG and ISBI longitudinal MS lesion\nsegmentation challenge, where we achieved top performance in both challenges.\nOur network trained with focal loss ranked first according to the ISBI\nchallenge overall score and resulted in the lowest reported lesion false\npositive rate among all submitted methods. Our network trained with the\nasymmetric similarity loss led to the lowest surface distance and the best\nlesion true positive rate.\n", "versions": [{"version": "v1", "created": "Wed, 28 Mar 2018 16:29:54 GMT"}, {"version": "v2", "created": "Tue, 17 Apr 2018 00:33:51 GMT"}, {"version": "v3", "created": "Fri, 29 Jun 2018 16:16:19 GMT"}, {"version": "v4", "created": "Thu, 13 Dec 2018 22:02:18 GMT"}], "update_date": "2018-12-17", "authors_parsed": [["Hashemi", "Seyed Raein", ""], ["Salehi", "Seyed Sadegh Mohseni", ""], ["Erdogmus", "Deniz", ""], ["Prabhu", "Sanjay P.", ""], ["Warfield", "Simon K.", ""], ["Gholipour", "Ali", ""]]}, {"id": "1803.11080", "submitter": "Qiao Zheng", "authors": "Qiao Zheng, Herv\\'e Delingette, Nicolas Duchateau, Nicholas Ayache", "title": "3D Consistent Biventricular Myocardial Segmentation Using Deep Learning\n  for Mesh Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel automated method to segment the myocardium of both left\nand right ventricles in MRI volumes. The segmentation is consistent in 3D\nacross the slices such that it can be directly used for mesh generation. Two\nspecific neural networks with multi-scale coarse-to-fine prediction structure\nare proposed to cope with the small training dataset and trained using an\noriginal loss function. The former segments a slice in the middle of the\nvolume. Then the latter iteratively propagates the slice segmentations towards\nthe base and the apex, in a spatially consistent way. We perform 5-fold\ncross-validation on the 15 cases from STACOM to validate the method. For\ntraining, we use real cases and their synthetic variants generated by combining\nmotion simulation and image synthesis. Accurate and consistent testing results\nare obtained.\n", "versions": [{"version": "v1", "created": "Thu, 29 Mar 2018 14:08:12 GMT"}], "update_date": "2018-03-30", "authors_parsed": [["Zheng", "Qiao", ""], ["Delingette", "Herv\u00e9", ""], ["Duchateau", "Nicolas", ""], ["Ayache", "Nicholas", ""]]}, {"id": "1803.11088", "submitter": "Kalin Stefanov", "authors": "Kalin Stefanov", "title": "Webcam-based Eye Gaze Tracking under Natural Head Movement", "comments": "MSc Thesis in Artificial Intelligence", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This manuscript investigates and proposes a visual gaze tracker that tackles\nthe problem using only an ordinary web camera and no prior knowledge in any\nsense (scene set-up, camera intrinsic and/or extrinsic parameters). The tracker\nwe propose is based on the observation that our desire to grant the freedom of\nnatural head movement to the user requires 3D modeling of the scene set-up.\nAlthough, using a single low resolution web camera bounds us in dimensions (no\ndepth can be recovered), we propose ways to cope with this drawback and model\nthe scene in front of the user. We tackle this three-dimensional problem by\nrealizing that it can be viewed as series of two-dimensional special cases.\nThen, we propose a procedure that treats each movement of the user's head as a\nspecial two-dimensional case, hence reducing the complexity of the problem back\nto two dimensions. Furthermore, the proposed tracker is calibration free and\ndiscards this tedious part of all previously mentioned trackers.\n  Experimental results show that the proposed tracker achieves good results,\ngiven the restrictions on it. We can report that the tracker commits a mean\nerror of (56.95, 70.82) pixels in x and y direction, respectively, when the\nuser's head is as static as possible (no chin-rests are used). Furthermore, we\ncan report that the proposed tracker commits a mean error of (87.18, 103.86)\npixels in x and y direction, respectively, under natural head movement.\n", "versions": [{"version": "v1", "created": "Thu, 29 Mar 2018 14:16:00 GMT"}], "update_date": "2018-03-30", "authors_parsed": [["Stefanov", "Kalin", ""]]}, {"id": "1803.11095", "submitter": "Ahmet Iscen", "authors": "Ahmet Iscen and Giorgos Tolias and Yannis Avrithis and Ondrej Chum", "title": "Mining on Manifolds: Metric Learning without Labels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we present a novel unsupervised framework for hard training\nexample mining. The only input to the method is a collection of images relevant\nto the target application and a meaningful initial representation, provided\ne.g. by pre-trained CNN. Positive examples are distant points on a single\nmanifold, while negative examples are nearby points on different manifolds.\nBoth types of examples are revealed by disagreements between Euclidean and\nmanifold similarities. The discovered examples can be used in training with any\ndiscriminative loss. The method is applied to unsupervised fine-tuning of\npre-trained networks for fine-grained classification and particular object\nretrieval. Our models are on par or are outperforming prior models that are\nfully or partially supervised.\n", "versions": [{"version": "v1", "created": "Thu, 29 Mar 2018 14:29:46 GMT"}], "update_date": "2018-03-30", "authors_parsed": [["Iscen", "Ahmet", ""], ["Tolias", "Giorgos", ""], ["Avrithis", "Yannis", ""], ["Chum", "Ondrej", ""]]}, {"id": "1803.11097", "submitter": "Yaojie Liu", "authors": "Yaojie Liu, Amin Jourabloo, Xiaoming Liu", "title": "Learning Deep Models for Face Anti-Spoofing: Binary or Auxiliary\n  Supervision", "comments": "CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face anti-spoofing is the crucial step to prevent face recognition systems\nfrom a security breach. Previous deep learning approaches formulate face\nanti-spoofing as a binary classification problem. Many of them struggle to\ngrasp adequate spoofing cues and generalize poorly. In this paper, we argue the\nimportance of auxiliary supervision to guide the learning toward discriminative\nand generalizable cues. A CNN-RNN model is learned to estimate the face depth\nwith pixel-wise supervision, and to estimate rPPG signals with sequence-wise\nsupervision. Then we fuse the estimated depth and rPPG to distinguish live vs.\nspoof faces. In addition, we introduce a new face anti-spoofing database that\ncovers a large range of illumination, subject, and pose variations.\nExperimental results show that our model achieves the state-of-the-art\nperformance on both intra-database and cross-database testing.\n", "versions": [{"version": "v1", "created": "Thu, 29 Mar 2018 14:33:21 GMT"}], "update_date": "2018-03-30", "authors_parsed": [["Liu", "Yaojie", ""], ["Jourabloo", "Amin", ""], ["Liu", "Xiaoming", ""]]}, {"id": "1803.11111", "submitter": "Nima Hatami", "authors": "Nima Hatami, Yann Gavet, Johan Debayle", "title": "Bag of Recurrence Patterns Representation for Time-Series Classification", "comments": null, "journal-ref": "Pattern Analysis and Applications Journal, 2018", "doi": "10.1007/s10044-018-0703-6", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time-Series Classification (TSC) has attracted a lot of attention in pattern\nrecognition, because wide range of applications from different domains such as\nfinance and health informatics deal with time-series signals. Bag of Features\n(BoF) model has achieved a great success in TSC task by summarizing signals\naccording to the frequencies of \"feature words\" of a data-learned dictionary.\nThis paper proposes embedding the Recurrence Plots (RP), a visualization\ntechnique for analysis of dynamic systems, in the BoF model for TSC. While the\ntraditional BoF approach extracts features from 1D signal segments, this paper\nuses the RP to transform time-series into 2D texture images and then applies\nthe BoF on them. Image representation of time-series enables us to explore\ndifferent visual descriptors that are not available for 1D signals and to\ntreats TSC task as a texture recognition problem. Experimental results on the\nUCI time-series classification archive demonstrates a significant accuracy\nboost by the proposed Bag of Recurrence patterns (BoR), compared not only to\nthe existing BoF models, but also to the state-of-the art algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 29 Mar 2018 15:16:45 GMT"}], "update_date": "2018-03-30", "authors_parsed": [["Hatami", "Nima", ""], ["Gavet", "Yann", ""], ["Debayle", "Johan", ""]]}, {"id": "1803.11125", "submitter": "Birgitta Dresp-Langley", "authors": "John M. Wandeto, Henry O. Nyongesa, Birgitta Dresp-Langley", "title": "Detection of Structural Change in Geographic Regions of Interest by Self\n  Organized Mapping: Las Vegas City and Lake Mead across the Years", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time-series of satellite images may reveal important data about changes in\nenvironmental conditions and natural or urban landscape structures that are of\npotential interest to citizens, historians, or policymakers. We applied a fast\nmethod of image analysis using Self Organized Maps (SOM) and, more\nspecifically, the quantization error (QE), for the visualization of critical\nchanges in satellite images of Las Vegas, generated across the years 1984-2008,\na period of major restructuration of the urban landscape. As shown in our\nprevious work, the QE from the SOM output is a reliable measure of variability\nin local image contents. In the present work, we use statistical trend analysis\nto show how the QE from SOM run on specific geographic regions of interest\nextracted from satellite images can be exploited to detect both the magnitude\nand the direction of structural change across time at a glance. Significantly\ncorrelated demographic data for the same reference time period are highlighted.\nThe approach is fast and reliable, and can be implemented for the rapid\ndetection of potentially critical changes in time series of large bodies of\nimage data.\n", "versions": [{"version": "v1", "created": "Thu, 29 Mar 2018 15:56:05 GMT"}], "update_date": "2018-03-30", "authors_parsed": [["Wandeto", "John M.", ""], ["Nyongesa", "Henry O.", ""], ["Dresp-Langley", "Birgitta", ""]]}, {"id": "1803.11147", "submitter": "Abhishek Venkataraman", "authors": "Abhishek Venkataraman, Brent Griffin, Jason J. Corso", "title": "Learning Kinematic Descriptions using SPARE: Simulated and Physical\n  ARticulated Extendable dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Next generation robots will need to understand intricate and articulated\nobjects as they cooperate in human environments. To do so, these robots will\nneed to move beyond their current abilities--- working with relatively simple\nobjects in a task-indifferent manner--- toward more sophisticated abilities\nthat dynamically estimate the properties of complex, articulated objects. To\nthat end, we make two compelling contributions toward general articulated\n(physical) object understanding in this paper. First, we introduce a new\ndataset, SPARE: Simulated and Physical ARticulated Extendable dataset. SPARE is\nan extendable open-source dataset providing equivalent simulated and physical\ninstances of articulated objects (kinematic chains), providing the greater\nresearch community with a training and evaluation tool for methods generating\nkinematic descriptions of articulated objects. To the best of our knowledge,\nthis is the first joint visual and physical (3D-printable) dataset for the\nVision community. Second, we present a deep neural network that can predit the\nnumber of links and the length of the links of an articulated object. These new\nideas outperform classical approaches to understanding kinematic chains, such\ntracking-based methods, which fail in the case of occlusion and do not leverage\nmultiple views when available.\n", "versions": [{"version": "v1", "created": "Thu, 29 Mar 2018 16:49:14 GMT"}], "update_date": "2018-03-30", "authors_parsed": [["Venkataraman", "Abhishek", ""], ["Griffin", "Brent", ""], ["Corso", "Jason J.", ""]]}, {"id": "1803.11157", "submitter": "Pengpeng Yang", "authors": "Wei Zhao and Pengpeng Yang and Rongrong Ni and Yao Zhao and Haorui Wu", "title": "Security Consideration For Deep Learning-Based Image Forensics", "comments": null, "journal-ref": null, "doi": "10.1587/transinf.2018EDL8091", "report-no": null, "categories": "cs.CV cs.LG cs.MM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, image forensics community has paied attention to the research on\nthe design of effective algorithms based on deep learning technology and facts\nproved that combining the domain knowledge of image forensics and deep learning\nwould achieve more robust and better performance than the traditional schemes.\nInstead of improving it, in this paper, the safety of deep learning based\nmethods in the field of image forensics is taken into account. To the best of\nour knowledge, this is a first work focusing on this topic. Specifically, we\nexperimentally find that the method using deep learning would fail when adding\nthe slight noise into the images (adversarial images). Furthermore, two kinds\nof strategys are proposed to enforce security of deep learning-based method.\nFirstly, an extra penalty term to the loss function is added, which is referred\nto the 2-norm of the gradient of the loss with respect to the input images, and\nthen an novel training method are adopt to train the model by fusing the normal\nand adversarial images. Experimental results show that the proposed algorithm\ncan achieve good performance even in the case of adversarial images and provide\na safety consideration for deep learning-based image forensics\n", "versions": [{"version": "v1", "created": "Thu, 29 Mar 2018 17:06:00 GMT"}, {"version": "v2", "created": "Tue, 3 Apr 2018 09:54:20 GMT"}], "update_date": "2018-12-26", "authors_parsed": [["Zhao", "Wei", ""], ["Yang", "Pengpeng", ""], ["Ni", "Rongrong", ""], ["Zhao", "Yao", ""], ["Wu", "Haorui", ""]]}, {"id": "1803.11182", "submitter": "Jianmin Bao", "authors": "Jianmin Bao, Dong Chen, Fang Wen, Houqiang Li, Gang Hua", "title": "Towards Open-Set Identity Preserving Face Synthesis", "comments": null, "journal-ref": "2018 IEEE Conference on Computer Vision and Pattern\n  Recognition(CVPR 2018)", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a framework based on Generative Adversarial Networks to\ndisentangle the identity and attributes of faces, such that we can conveniently\nrecombine different identities and attributes for identity preserving face\nsynthesis in open domains. Previous identity preserving face synthesis\nprocesses are largely confined to synthesizing faces with known identities that\nare already in the training dataset. To synthesize a face with identity outside\nthe training dataset, our framework requires one input image of that subject to\nproduce an identity vector, and any other input face image to extract an\nattribute vector capturing, e.g., pose, emotion, illumination, and even the\nbackground. We then recombine the identity vector and the attribute vector to\nsynthesize a new face of the subject with the extracted attribute. Our proposed\nframework does not need to annotate the attributes of faces in any way. It is\ntrained with an asymmetric loss function to better preserve the identity and\nstabilize the training process. It can also effectively leverage large amounts\nof unlabeled training face images to further improve the fidelity of the\nsynthesized faces for subjects that are not presented in the labeled training\nface dataset. Our experiments demonstrate the efficacy of the proposed\nframework. We also present its usage in a much broader set of applications\nincluding face frontalization, face attribute morphing, and face adversarial\nexample detection.\n", "versions": [{"version": "v1", "created": "Thu, 29 Mar 2018 17:54:51 GMT"}, {"version": "v2", "created": "Thu, 9 Aug 2018 06:55:19 GMT"}], "update_date": "2018-08-10", "authors_parsed": [["Bao", "Jianmin", ""], ["Chen", "Dong", ""], ["Wen", "Fang", ""], ["Li", "Houqiang", ""], ["Hua", "Gang", ""]]}, {"id": "1803.11185", "submitter": "Raymond A. Yeh", "authors": "Raymond A. Yeh, Minh N. Do, Alexander G. Schwing", "title": "Unsupervised Textual Grounding: Linking Words to Image Concepts", "comments": "Accepted to CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Textual grounding, i.e., linking words to objects in images, is a challenging\nbut important task for robotics and human-computer interaction. Existing\ntechniques benefit from recent progress in deep learning and generally\nformulate the task as a supervised learning problem, selecting a bounding box\nfrom a set of possible options. To train these deep net based approaches,\naccess to a large-scale datasets is required, however, constructing such a\ndataset is time-consuming and expensive. Therefore, we develop a completely\nunsupervised mechanism for textual grounding using hypothesis testing as a\nmechanism to link words to detected image concepts. We demonstrate our approach\non the ReferIt Game dataset and the Flickr30k data, outperforming baselines by\n7.98% and 6.96% respectively.\n", "versions": [{"version": "v1", "created": "Thu, 29 Mar 2018 17:58:43 GMT"}], "update_date": "2018-03-30", "authors_parsed": [["Yeh", "Raymond A.", ""], ["Do", "Minh N.", ""], ["Schwing", "Alexander G.", ""]]}, {"id": "1803.11186", "submitter": "Unnat Jain", "authors": "Unnat Jain, Svetlana Lazebnik, Alexander Schwing", "title": "Two can play this Game: Visual Dialog with Discriminative Question\n  Generation and Answering", "comments": "Accepted to CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human conversation is a complex mechanism with subtle nuances. It is hence an\nambitious goal to develop artificial intelligence agents that can participate\nfluently in a conversation. While we are still far from achieving this goal,\nrecent progress in visual question answering, image captioning, and visual\nquestion generation shows that dialog systems may be realizable in the not too\ndistant future. To this end, a novel dataset was introduced recently and\nencouraging results were demonstrated, particularly for question answering. In\nthis paper, we demonstrate a simple symmetric discriminative baseline, that can\nbe applied to both predicting an answer as well as predicting a question. We\nshow that this method performs on par with the state of the art, even memory\nnet based methods. In addition, for the first time on the visual dialog\ndataset, we assess the performance of a system asking questions, and\ndemonstrate how visual dialog can be generated from discriminative question\ngeneration and question answering.\n", "versions": [{"version": "v1", "created": "Thu, 29 Mar 2018 17:58:43 GMT"}], "update_date": "2018-03-30", "authors_parsed": [["Jain", "Unnat", ""], ["Lazebnik", "Svetlana", ""], ["Schwing", "Alexander", ""]]}, {"id": "1803.11187", "submitter": "Yuan-Ting Hu", "authors": "Yuan-Ting Hu and Jia-Bin Huang and Alexander G. Schwing", "title": "MaskRNN: Instance Level Video Object Segmentation", "comments": "Accepted to NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Instance level video object segmentation is an important technique for video\nediting and compression. To capture the temporal coherence, in this paper, we\ndevelop MaskRNN, a recurrent neural net approach which fuses in each frame the\noutput of two deep nets for each object instance -- a binary segmentation net\nproviding a mask and a localization net providing a bounding box. Due to the\nrecurrent component and the localization component, our method is able to take\nadvantage of long-term temporal structures of the video data as well as\nrejecting outliers. We validate the proposed algorithm on three challenging\nbenchmark datasets, the DAVIS-2016 dataset, the DAVIS-2017 dataset, and the\nSegtrack v2 dataset, achieving state-of-the-art performance on all of them.\n", "versions": [{"version": "v1", "created": "Thu, 29 Mar 2018 17:58:44 GMT"}], "update_date": "2018-03-30", "authors_parsed": [["Hu", "Yuan-Ting", ""], ["Huang", "Jia-Bin", ""], ["Schwing", "Alexander G.", ""]]}, {"id": "1803.11188", "submitter": "Ishan Deshpande", "authors": "Ishan Deshpande, Ziyu Zhang, Alexander Schwing", "title": "Generative Modeling using the Sliced Wasserstein Distance", "comments": "Accepted to CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Nets (GANs) are very successful at modeling\ndistributions from given samples, even in the high-dimensional case. However,\ntheir formulation is also known to be hard to optimize and often not stable.\nWhile this is particularly true for early GAN formulations, there has been\nsignificant empirically motivated and theoretically founded progress to improve\nstability, for instance, by using the Wasserstein distance rather than the\nJenson-Shannon divergence. Here, we consider an alternative formulation for\ngenerative modeling based on random projections which, in its simplest form,\nresults in a single objective rather than a saddle-point formulation. By\naugmenting this approach with a discriminator we improve its accuracy. We found\nour approach to be significantly more stable compared to even the improved\nWasserstein GAN. Further, unlike the traditional GAN loss, the loss formulated\nin our method is a good measure of the actual distance between the\ndistributions and, for the first time for GAN training, we are able to show\nestimates for the same.\n", "versions": [{"version": "v1", "created": "Thu, 29 Mar 2018 17:58:44 GMT"}], "update_date": "2018-03-30", "authors_parsed": [["Deshpande", "Ishan", ""], ["Zhang", "Ziyu", ""], ["Schwing", "Alexander", ""]]}, {"id": "1803.11189", "submitter": "Xinlei Chen", "authors": "Xinlei Chen, Li-Jia Li, Li Fei-Fei, Abhinav Gupta", "title": "Iterative Visual Reasoning Beyond Convolutions", "comments": "CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel framework for iterative visual reasoning. Our framework\ngoes beyond current recognition systems that lack the capability to reason\nbeyond stack of convolutions. The framework consists of two core modules: a\nlocal module that uses spatial memory to store previous beliefs with parallel\nupdates; and a global graph-reasoning module. Our graph module has three\ncomponents: a) a knowledge graph where we represent classes as nodes and build\nedges to encode different types of semantic relationships between them; b) a\nregion graph of the current image where regions in the image are nodes and\nspatial relationships between these regions are edges; c) an assignment graph\nthat assigns regions to classes. Both the local module and the global module\nroll-out iteratively and cross-feed predictions to each other to refine\nestimates. The final predictions are made by combining the best of both modules\nwith an attention mechanism. We show strong performance over plain ConvNets,\n\\eg achieving an $8.4\\%$ absolute improvement on ADE measured by per-class\naverage precision. Analysis also shows that the framework is resilient to\nmissing regions for reasoning.\n", "versions": [{"version": "v1", "created": "Thu, 29 Mar 2018 17:59:03 GMT"}], "update_date": "2018-03-30", "authors_parsed": [["Chen", "Xinlei", ""], ["Li", "Li-Jia", ""], ["Fei-Fei", "Li", ""], ["Gupta", "Abhinav", ""]]}, {"id": "1803.11209", "submitter": "Raymond A. Yeh", "authors": "Raymond A. Yeh, Jinjun Xiong, Wen-mei W. Hwu, Minh N. Do, Alexander G.\n  Schwing", "title": "Interpretable and Globally Optimal Prediction for Textual Grounding\n  using Image Concepts", "comments": "Accepted to NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Textual grounding is an important but challenging task for human-computer\ninteraction, robotics and knowledge mining. Existing algorithms generally\nformulate the task as selection from a set of bounding box proposals obtained\nfrom deep net based systems. In this work, we demonstrate that we can cast the\nproblem of textual grounding into a unified framework that permits efficient\nsearch over all possible bounding boxes. Hence, the method is able to consider\nsignificantly more proposals and doesn't rely on a successful first stage\nhypothesizing bounding box proposals. Beyond, we demonstrate that the trained\nparameters of our model can be used as word-embeddings which capture\nspatial-image relationships and provide interpretability. Lastly, at the time\nof submission, our approach outperformed the current state-of-the-art methods\non the Flickr 30k Entities and the ReferItGame dataset by 3.08% and 7.77%\nrespectively.\n", "versions": [{"version": "v1", "created": "Thu, 29 Mar 2018 18:14:19 GMT"}], "update_date": "2018-04-02", "authors_parsed": [["Yeh", "Raymond A.", ""], ["Xiong", "Jinjun", ""], ["Hwu", "Wen-mei W.", ""], ["Do", "Minh N.", ""], ["Schwing", "Alexander G.", ""]]}, {"id": "1803.11217", "submitter": "Mingze Xu", "authors": "Mingze Xu, Chenyou Fan, Yuchen Wang, Michael S Ryoo, David J Crandall", "title": "Joint Person Segmentation and Identification in Synchronized First- and\n  Third-person Videos", "comments": "To appear in ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a world of pervasive cameras, public spaces are often captured from\nmultiple perspectives by cameras of different types, both fixed and mobile. An\nimportant problem is to organize these heterogeneous collections of videos by\nfinding connections between them, such as identifying correspondences between\nthe people appearing in the videos and the people holding or wearing the\ncameras. In this paper, we wish to solve two specific problems: (1) given two\nor more synchronized third-person videos of a scene, produce a pixel-level\nsegmentation of each visible person and identify corresponding people across\ndifferent views (i.e., determine who in camera A corresponds with whom in\ncamera B), and (2) given one or more synchronized third-person videos as well\nas a first-person video taken by a mobile or wearable camera, segment and\nidentify the camera wearer in the third-person videos. Unlike previous work\nwhich requires ground truth bounding boxes to estimate the correspondences, we\nperform person segmentation and identification jointly. We find that solving\nthese two problems simultaneously is mutually beneficial, because better\nfine-grained segmentation allows us to better perform matching across views,\nand information from multiple views helps us perform more accurate\nsegmentation. We evaluate our approach on two challenging datasets of\ninteracting people captured from multiple wearable cameras, and show that our\nproposed method performs significantly better than the state-of-the-art on both\nperson segmentation and identification.\n", "versions": [{"version": "v1", "created": "Thu, 29 Mar 2018 18:46:03 GMT"}, {"version": "v2", "created": "Wed, 25 Jul 2018 21:53:37 GMT"}], "update_date": "2018-07-27", "authors_parsed": [["Xu", "Mingze", ""], ["Fan", "Chenyou", ""], ["Wang", "Yuchen", ""], ["Ryoo", "Michael S", ""], ["Crandall", "David J", ""]]}, {"id": "1803.11227", "submitter": "Richard Yang", "authors": "Richard R. Yang, Steven Chen, and Edward Chou", "title": "AI Blue Book: Vehicle Price Prediction using Visual Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we build a series of machine learning models to predict the\nprice of a product given its image, and visualize the features that result in\nhigher or lower price predictions. We collect two novel datasets of product\nimages and their MSRP prices for this purpose: a bicycle dataset and a car\ndataset. We set baselines for price regression using linear regression on\nhistogram of oriented gradients (HOG) and convolutional neural network (CNN)\nfeatures, and a baseline for price segment classification using a multiclass\nSVM. For our main models, we train several deep CNNs using both transfer\nlearning and our own architectures, for both regression and classification. We\nachieve strong results on both datasets, with deep CNNs significantly\noutperforming other models in a variety of metrics. Finally, we use several\nrecently-developed methods to visualize the image features that result in\nhigher or lower prices.\n", "versions": [{"version": "v1", "created": "Thu, 29 Mar 2018 19:22:23 GMT"}, {"version": "v2", "created": "Thu, 18 Oct 2018 17:04:30 GMT"}], "update_date": "2018-10-19", "authors_parsed": [["Yang", "Richard R.", ""], ["Chen", "Steven", ""], ["Chou", "Edward", ""]]}, {"id": "1803.11232", "submitter": "Yuhao Zhu", "authors": "Yuhao Zhu, Anand Samajdar, Matthew Mattina, Paul Whatmough", "title": "Euphrates: Algorithm-SoC Co-Design for Low-Power Mobile Continuous\n  Vision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continuous computer vision (CV) tasks increasingly rely on convolutional\nneural networks (CNN). However, CNNs have massive compute demands that far\nexceed the performance and energy constraints of mobile devices. In this paper,\nwe propose and develop an algorithm-architecture co-designed system, Euphrates,\nthat simultaneously improves the energy-efficiency and performance of\ncontinuous vision tasks.\n  Our key observation is that changes in pixel data between consecutive frames\nrepresents visual motion. We first propose an algorithm that leverages this\nmotion information to relax the number of expensive CNN inferences required by\ncontinuous vision applications. We co-design a mobile System-on-a-Chip (SoC)\narchitecture to maximize the efficiency of the new algorithm. The key to our\narchitectural augmentation is to co-optimize different SoC IP blocks in the\nvision pipeline collectively. Specifically, we propose to expose the motion\ndata that is naturally generated by the Image Signal Processor (ISP) early in\nthe vision pipeline to the CNN engine. Measurement and synthesis results show\nthat Euphrates achieves up to 66% SoC-level energy savings (4 times for the\nvision computations), with only 1% accuracy loss.\n", "versions": [{"version": "v1", "created": "Thu, 29 Mar 2018 19:34:17 GMT"}], "update_date": "2018-04-02", "authors_parsed": [["Zhu", "Yuhao", ""], ["Samajdar", "Anand", ""], ["Mattina", "Matthew", ""], ["Whatmough", "Paul", ""]]}, {"id": "1803.11241", "submitter": "Hongliu Cao", "authors": "Hongliu Cao, Simon Bernard, Laurent Heutte, Robert Sabourin", "title": "Improve the performance of transfer learning without fine-tuning using\n  dissimilarity-based multi-view learning for breast cancer histology images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Breast cancer is one of the most common types of cancer and leading\ncancer-related death causes for women. In the context of ICIAR 2018 Grand\nChallenge on Breast Cancer Histology Images, we compare one handcrafted feature\nextractor and five transfer learning feature extractors based on deep learning.\nWe find out that the deep learning networks pretrained on ImageNet have better\nperformance than the popular handcrafted features used for breast cancer\nhistology images. The best feature extractor achieves an average accuracy of\n79.30%. To improve the classification performance, a random forest\ndissimilarity based integration method is used to combine different feature\ngroups together. When the five deep learning feature groups are combined, the\naverage accuracy is improved to 82.90% (best accuracy 85.00%). When handcrafted\nfeatures are combined with the five deep learning feature groups, the average\naccuracy is improved to 87.10% (best accuracy 93.00%).\n", "versions": [{"version": "v1", "created": "Thu, 29 Mar 2018 20:14:07 GMT"}], "update_date": "2018-04-02", "authors_parsed": [["Cao", "Hongliu", ""], ["Bernard", "Simon", ""], ["Heutte", "Laurent", ""], ["Sabourin", "Robert", ""]]}, {"id": "1803.11254", "submitter": "Ihab Mohamed", "authors": "Ihab S. Mohamed, Alessio Capitanelli, Fulvio Mastrogiovanni, Stefano\n  Rovetta, Renato Zaccaria", "title": "Detection, localisation and tracking of pallets using machine learning\n  techniques and 2D range data", "comments": "This paper has been submitted to Neural Computing and Applications\n  (NCAA). 23 pages, 7 figures", "journal-ref": null, "doi": "10.1007/s00521-019-04352-0", "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of autonomous transportation in industrial scenarios is receiving\na renewed interest due to the way it can revolutionise internal logistics,\nespecially in unstructured environments. This paper presents a novel\narchitecture allowing a robot to detect, localise, and track (possibly\nmultiple) pallets using machine learning techniques based on an on-board 2D\nlaser rangefinder only. The architecture is composed of two main components:\nthe first stage is a pallet detector employing a Faster Region-based\nConvolutional Neural Network (Faster R-CNN) detector cascaded with a CNN-based\nclassifier; the second stage is a Kalman filter for localising and tracking\ndetected pallets, which we also use to defer commitment to a pallet detected in\nthe first stage until sufficient confidence has been acquired via a sequential\ndata acquisition process. For fine-tuning the CNNs, the architecture has been\nsystematically evaluated using a real-world dataset containing 340 labeled 2D\nscans, which have been made freely available in an online repository. Detection\nperformance has been assessed on the basis of the average accuracy over k-fold\ncross-validation, and it scored 99.58% in our tests. Concerning pallet\nlocalisation and tracking, experiments have been performed in a scenario where\nthe robot is approaching the pallet to fork. Although data have been originally\nacquired by considering only one pallet as per specification of the use case we\nconsider, artificial data have been generated as well to mimic the presence of\nmultiple pallets in the robot workspace. Our experimental results confirm that\nthe system is capable of identifying, localising and tracking pallets with a\nhigh success rate while being robust to false positives.\n", "versions": [{"version": "v1", "created": "Thu, 29 Mar 2018 21:00:56 GMT"}, {"version": "v2", "created": "Fri, 22 Feb 2019 12:03:31 GMT"}, {"version": "v3", "created": "Sun, 28 Apr 2019 16:28:15 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Mohamed", "Ihab S.", ""], ["Capitanelli", "Alessio", ""], ["Mastrogiovanni", "Fulvio", ""], ["Rovetta", "Stefano", ""], ["Zaccaria", "Renato", ""]]}, {"id": "1803.11264", "submitter": "Hamid Reza Vaezi Joze", "authors": "Mehran Khodabandeh, Hamid Reza Vaezi Joze, Ilya Zharkov and Vivek\n  Pradeep", "title": "DIY Human Action Data Set Generation", "comments": null, "journal-ref": "The IEEE Conference on Computer Vision and Pattern Recognition\n  Workshop (CVPRW), 2018", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent successes in applying deep learning techniques to solve standard\ncomputer vision problems has aspired researchers to propose new computer vision\nproblems in different domains. As previously established in the field, training\ndata itself plays a significant role in the machine learning process,\nespecially deep learning approaches which are data hungry. In order to solve\neach new problem and get a decent performance, a large amount of data needs to\nbe captured which may in many cases pose logistical difficulties. Therefore,\nthe ability to generate de novo data or expand an existing data set, however\nsmall, in order to satisfy data requirement of current networks may be\ninvaluable. Herein, we introduce a novel way to partition an action video clip\ninto action, subject and context. Each part is manipulated separately and\nreassembled with our proposed video generation technique. Furthermore, our\nnovel human skeleton trajectory generation along with our proposed video\ngeneration technique, enables us to generate unlimited action recognition\ntraining data. These techniques enables us to generate video action clips from\nan small set without costly and time-consuming data acquisition. Lastly, we\nprove through extensive set of experiments on two small human action\nrecognition data sets, that this new data generation technique can improve the\nperformance of current action recognition neural nets.\n", "versions": [{"version": "v1", "created": "Thu, 29 Mar 2018 21:30:19 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Khodabandeh", "Mehran", ""], ["Joze", "Hamid Reza Vaezi", ""], ["Zharkov", "Ilya", ""], ["Pradeep", "Vivek", ""]]}, {"id": "1803.11276", "submitter": "Peng Zhou", "authors": "Peng Zhou, Xintong Han, Vlad I. Morariu and Larry S. Davis", "title": "Two-Stream Neural Networks for Tampered Face Detection", "comments": null, "journal-ref": "2017 CVPR workshop", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a two-stream network for face tampering detection. We train\nGoogLeNet to detect tampering artifacts in a face classification stream, and\ntrain a patch based triplet network to leverage features capturing local noise\nresiduals and camera characteristics as a second stream. In addition, we use\ntwo different online face swapping applications to create a new dataset that\nconsists of 2010 tampered images, each of which contains a tampered face. We\nevaluate the proposed two-stream network on our newly collected dataset.\nExperimental results demonstrate the effectiveness of our method.\n", "versions": [{"version": "v1", "created": "Thu, 29 Mar 2018 22:36:11 GMT"}], "update_date": "2018-04-02", "authors_parsed": [["Zhou", "Peng", ""], ["Han", "Xintong", ""], ["Morariu", "Vlad I.", ""], ["Davis", "Larry S.", ""]]}, {"id": "1803.11285", "submitter": "Filip Radenovi\\'c", "authors": "Filip Radenovi\\'c, Ahmet Iscen, Giorgos Tolias, Yannis Avrithis,\n  Ond\\v{r}ej Chum", "title": "Revisiting Oxford and Paris: Large-Scale Image Retrieval Benchmarking", "comments": "CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we address issues with image retrieval benchmarking on standard\nand popular Oxford 5k and Paris 6k datasets. In particular, annotation errors,\nthe size of the dataset, and the level of challenge are addressed: new\nannotation for both datasets is created with an extra attention to the\nreliability of the ground truth. Three new protocols of varying difficulty are\nintroduced. The protocols allow fair comparison between different methods,\nincluding those using a dataset pre-processing stage. For each dataset, 15 new\nchallenging queries are introduced. Finally, a new set of 1M hard,\nsemi-automatically cleaned distractors is selected.\n  An extensive comparison of the state-of-the-art methods is performed on the\nnew benchmark. Different types of methods are evaluated, ranging from\nlocal-feature-based to modern CNN based methods. The best results are achieved\nby taking the best of the two worlds. Most importantly, image retrieval appears\nfar from being solved.\n", "versions": [{"version": "v1", "created": "Thu, 29 Mar 2018 23:22:31 GMT"}], "update_date": "2018-04-02", "authors_parsed": [["Radenovi\u0107", "Filip", ""], ["Iscen", "Ahmet", ""], ["Tolias", "Giorgos", ""], ["Avrithis", "Yannis", ""], ["Chum", "Ond\u0159ej", ""]]}, {"id": "1803.11288", "submitter": "Andrew Davison", "authors": "Andrew J. Davison", "title": "FutureMapping: The Computational Structure of Spatial AI Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss and predict the evolution of Simultaneous Localisation and Mapping\n(SLAM) into a general geometric and semantic `Spatial AI' perception capability\nfor intelligent embodied devices. A big gap remains between the visual\nperception performance that devices such as augmented reality eyewear or\ncomsumer robots will require and what is possible within the constraints\nimposed by real products. Co-design of algorithms, processors and sensors will\nbe needed. We explore the computational structure of current and future Spatial\nAI algorithms and consider this within the landscape of ongoing hardware\ndevelopments.\n", "versions": [{"version": "v1", "created": "Thu, 29 Mar 2018 23:46:34 GMT"}], "update_date": "2018-04-02", "authors_parsed": [["Davison", "Andrew J.", ""]]}, {"id": "1803.11293", "submitter": "Aydogan Ozcan", "authors": "Yair Rivenson, Hongda Wang, Zhensong Wei, Yibo Zhang, Harun Gunaydin,\n  Aydogan Ozcan", "title": "Deep learning-based virtual histology staining using auto-fluorescence\n  of label-free tissue", "comments": null, "journal-ref": "Nature Biomedical Engineering (2019)", "doi": "10.1038/s41551-019-0362-y", "report-no": null, "categories": "cs.CV cs.LG physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Histological analysis of tissue samples is one of the most widely used\nmethods for disease diagnosis. After taking a sample from a patient, it goes\nthrough a lengthy and laborious preparation, which stains the tissue to\nvisualize different histological features under a microscope. Here, we\ndemonstrate a label-free approach to create a virtually-stained microscopic\nimage using a single wide-field auto-fluorescence image of an unlabeled tissue\nsample, bypassing the standard histochemical staining process, saving time and\ncost. This method is based on deep learning, and uses a convolutional neural\nnetwork trained using a generative adversarial network model to transform an\nauto-fluorescence image of an unlabeled tissue section into an image that is\nequivalent to the bright-field image of the stained-version of the same sample.\nWe validated this method by successfully creating virtually-stained microscopic\nimages of human tissue samples, including sections of salivary gland, thyroid,\nkidney, liver and lung tissue, also covering three different stains. This\nlabel-free virtual-staining method eliminates cumbersome and costly\nhistochemical staining procedures, and would significantly simplify tissue\npreparation in pathology and histology fields.\n", "versions": [{"version": "v1", "created": "Fri, 30 Mar 2018 00:23:22 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Rivenson", "Yair", ""], ["Wang", "Hongda", ""], ["Wei", "Zhensong", ""], ["Zhang", "Yibo", ""], ["Gunaydin", "Harun", ""], ["Ozcan", "Aydogan", ""]]}, {"id": "1803.11303", "submitter": "Jinzheng Cai", "authors": "Jinzheng Cai, Le Lu, Fuyong Xing and Lin Yang", "title": "Pancreas Segmentation in CT and MRI Images via Domain Specific Network\n  Designing and Recurrent Neural Contextual Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic pancreas segmentation in radiology images, eg., computed tomography\n(CT) and magnetic resonance imaging (MRI), is frequently required by\ncomputer-aided screening, diagnosis, and quantitative assessment. Yet pancreas\nis a challenging abdominal organ to segment due to the high inter-patient\nanatomical variability in both shape and volume metrics. Recently,\nconvolutional neural networks (CNNs) have demonstrated promising performance on\naccurate segmentation of pancreas. However, the CNN-based method often suffers\nfrom segmentation discontinuity for reasons such as noisy image quality and\nblurry pancreatic boundary. From this point, we propose to introduce recurrent\nneural networks (RNNs) to address the problem of spatial non-smoothness of\ninter-slice pancreas segmentation across adjacent image slices. To inference\ninitial segmentation, we first train a 2D CNN sub-network, where we modify its\nnetwork architecture with deep-supervision and multi-scale feature map\naggregation so that it can be trained from scratch with small-sized training\ndata and presents superior performance than transferred models. Thereafter, the\nsuccessive CNN outputs are processed by another RNN sub-network, which refines\nthe consistency of segmented shapes. More specifically, the RNN sub-network\nconsists convolutional long short-term memory (CLSTM) units in both top-down\nand bottom-up directions, which regularizes the segmentation of an image by\nintegrating predictions of its neighboring slices. We train the stacked CNN-RNN\nmodel end-to-end and perform quantitative evaluations on both CT and MRI\nimages.\n", "versions": [{"version": "v1", "created": "Fri, 30 Mar 2018 01:31:53 GMT"}], "update_date": "2018-04-02", "authors_parsed": [["Cai", "Jinzheng", ""], ["Lu", "Le", ""], ["Xing", "Fuyong", ""], ["Yang", "Lin", ""]]}, {"id": "1803.11305", "submitter": "Guangcan Liu", "authors": "Guangcan Liu, Zhao Zhang, Qingshan Liu, Kongkai Xiong", "title": "Robust Subspace Clustering with Compressed Data", "comments": null, "journal-ref": "IEEE Transactions on Image Processing, 2019", "doi": "10.1109/TIP.2019.2917857", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dimension reduction is widely regarded as an effective way for decreasing the\ncomputation, storage and communication loads of data-driven intelligent\nsystems, leading to a growing demand for statistical methods that allow\nanalysis (e.g., clustering) of compressed data. We therefore study in this\npaper a novel problem called compressive robust subspace clustering, which is\nto perform robust subspace clustering with the compressed data, and which is\ngenerated by projecting the original high-dimensional data onto a\nlower-dimensional subspace chosen at random. Given only the compressed data and\nsensing matrix, the proposed method, row space pursuit (RSP), recovers the\nauthentic row space that gives correct clustering results under certain\nconditions. Extensive experiments show that RSP is distinctly better than the\ncompeting methods, in terms of both clustering accuracy and computational\nefficiency.\n", "versions": [{"version": "v1", "created": "Fri, 30 Mar 2018 01:34:55 GMT"}, {"version": "v2", "created": "Mon, 27 Aug 2018 05:31:53 GMT"}, {"version": "v3", "created": "Sat, 29 Dec 2018 03:27:43 GMT"}, {"version": "v4", "created": "Wed, 2 Jan 2019 01:33:57 GMT"}, {"version": "v5", "created": "Tue, 4 Jun 2019 08:14:55 GMT"}, {"version": "v6", "created": "Thu, 22 Aug 2019 09:36:46 GMT"}], "update_date": "2019-08-23", "authors_parsed": [["Liu", "Guangcan", ""], ["Zhang", "Zhao", ""], ["Liu", "Qingshan", ""], ["Xiong", "Kongkai", ""]]}, {"id": "1803.11316", "submitter": "Muhammad Haris", "authors": "Muhammad Haris, Greg Shakhnarovich, and Norimichi Ukita", "title": "Task-Driven Super Resolution: Object Detection in Low-resolution Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider how image super resolution (SR) can contribute to an object\ndetection task in low-resolution images. Intuitively, SR gives a positive\nimpact on the object detection task. While several previous works demonstrated\nthat this intuition is correct, SR and detector are optimized independently in\nthese works. This paper proposes a novel framework to train a deep neural\nnetwork where the SR sub-network explicitly incorporates a detection loss in\nits training objective, via a tradeoff with a traditional detection loss. This\nend-to-end training procedure allows us to train SR preprocessing for any\ndifferentiable detector. We demonstrate that our task-driven SR consistently\nand significantly improves accuracy of an object detector on low-resolution\nimages for a variety of conditions and scaling factors.\n", "versions": [{"version": "v1", "created": "Fri, 30 Mar 2018 02:15:24 GMT"}], "update_date": "2018-04-02", "authors_parsed": [["Haris", "Muhammad", ""], ["Shakhnarovich", "Greg", ""], ["Ukita", "Norimichi", ""]]}, {"id": "1803.11320", "submitter": "Jie Song", "authors": "Jie Song, Chengchao Shen, Yezhou Yang, Yang Liu, Mingli Song", "title": "Transductive Unbiased Embedding for Zero-Shot Learning", "comments": "Accepted to CVPR2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most existing Zero-Shot Learning (ZSL) methods have the strong bias problem,\nin which instances of unseen (target) classes tend to be categorized as one of\nthe seen (source) classes. So they yield poor performance after being deployed\nin the generalized ZSL settings. In this paper, we propose a straightforward\nyet effective method named Quasi-Fully Supervised Learning (QFSL) to alleviate\nthe bias problem. Our method follows the way of transductive learning, which\nassumes that both the labeled source images and unlabeled target images are\navailable for training. In the semantic embedding space, the labeled source\nimages are mapped to several fixed points specified by the source categories,\nand the unlabeled target images are forced to be mapped to other points\nspecified by the target categories. Experiments conducted on AwA2, CUB and SUN\ndatasets demonstrate that our method outperforms existing state-of-the-art\napproaches by a huge margin of 9.3~24.5% following generalized ZSL settings,\nand by a large margin of 0.2~16.2% following conventional ZSL settings.\n", "versions": [{"version": "v1", "created": "Fri, 30 Mar 2018 02:43:15 GMT"}], "update_date": "2018-04-02", "authors_parsed": [["Song", "Jie", ""], ["Shen", "Chengchao", ""], ["Yang", "Yezhou", ""], ["Liu", "Yang", ""], ["Song", "Mingli", ""]]}, {"id": "1803.11333", "submitter": "Zhanxiang Feng", "authors": "Zhanxiang Feng, Jianhuang Lai, and Xiaohua Xie", "title": "Learning View-Specific Deep Networks for Person Re-Identification", "comments": "12 pages, 8 figures, accepted by IEEE Transactions on image\n  processing", "journal-ref": "Feng Z, Lai J, Xie X. Learning View-Specific Deep Networks for\n  Person Re-Identification[J]. IEEE Transactions on Image Processing, 2018", "doi": "10.1109/TIP.2018.2818438", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, a growing body of research has focused on the problem of\nperson re-identification (re-id). The re-id techniques attempt to match the\nimages of pedestrians from disjoint non-overlapping camera views. A major\nchallenge of re-id is the serious intra-class variations caused by changing\nviewpoints. To overcome this challenge, we propose a deep neural network-based\nframework which utilizes the view information in the feature extraction stage.\nThe proposed framework learns a view-specific network for each camera view with\na cross-view Euclidean constraint (CV-EC) and a cross-view center loss (CV-CL).\nWe utilize CV-EC to decrease the margin of the features between diverse views\nand extend the center loss metric to a view-specific version to better adapt\nthe re-id problem. Moreover, we propose an iterative algorithm to optimize the\nparameters of the view-specific networks from coarse to fine. The experiments\ndemonstrate that our approach significantly improves the performance of the\nexisting deep networks and outperforms the state-of-the-art methods on the\nVIPeR, CUHK01, CUHK03, SYSU-mReId, and Market-1501 benchmarks.\n", "versions": [{"version": "v1", "created": "Fri, 30 Mar 2018 04:10:06 GMT"}], "update_date": "2018-04-02", "authors_parsed": [["Feng", "Zhanxiang", ""], ["Lai", "Jianhuang", ""], ["Xie", "Xiaohua", ""]]}, {"id": "1803.11353", "submitter": "Yiluan Guo", "authors": "Yiluan Guo, Ngai-Man Cheung", "title": "Efficient and Deep Person Re-Identification using Multi-Level Similarity", "comments": "To appear in CVPR2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person Re-Identification (ReID) requires comparing two images of person\ncaptured under different conditions. Existing work based on neural networks\noften computes the similarity of feature maps from one single convolutional\nlayer. In this work, we propose an efficient, end-to-end fully convolutional\nSiamese network that computes the similarities at multiple levels. We\ndemonstrate that multi-level similarity can improve the accuracy considerably\nusing low-complexity network structures in ReID problem. Specifically, first,\nwe use several convolutional layers to extract the features of two input\nimages. Then, we propose Convolution Similarity Network to compute the\nsimilarity score maps for the inputs. We use spatial transformer networks\n(STNs) to determine spatial attention. We propose to apply efficient depth-wise\nconvolution to compute the similarity. The proposed Convolution Similarity\nNetworks can be inserted into different convolutional layers to extract visual\nsimilarities at different levels. Furthermore, we use an improved ranking loss\nto further improve the performance. Our work is the first to propose to compute\nvisual similarities at low, middle and high levels for ReID. With extensive\nexperiments and analysis, we demonstrate that our system, compact yet\neffective, can achieve competitive results with much smaller model size and\ncomputational complexity.\n", "versions": [{"version": "v1", "created": "Fri, 30 Mar 2018 06:18:28 GMT"}, {"version": "v2", "created": "Mon, 2 Apr 2018 03:06:54 GMT"}], "update_date": "2018-04-03", "authors_parsed": [["Guo", "Yiluan", ""], ["Cheung", "Ngai-Man", ""]]}, {"id": "1803.11361", "submitter": "Joseph Suarez", "authors": "Joseph Suarez, Justin Johnson, Fei-Fei Li", "title": "DDRprog: A CLEVR Differentiable Dynamic Reasoning Programmer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel Dynamic Differentiable Reasoning (DDR) framework for\njointly learning branching programs and the functions composing them; this\nresolves a significant nondifferentiability inhibiting recent dynamic\narchitectures. We apply our framework to two settings in two highly compact and\ndata efficient architectures: DDRprog for CLEVR Visual Question Answering and\nDDRstack for reverse Polish notation expression evaluation. DDRprog uses a\nrecurrent controller to jointly predict and execute modular neural programs\nthat directly correspond to the underlying question logic; it explicitly forks\nsubprocesses to handle logical branching. By effectively leveraging additional\nstructural supervision, we achieve a large improvement over previous approaches\nin subtask consistency and a small improvement in overall accuracy. We further\ndemonstrate the benefits of structural supervision in the RPN setting: the\ninclusion of a stack assumption in DDRstack allows our approach to generalize\nto long expressions where an LSTM fails the task.\n", "versions": [{"version": "v1", "created": "Fri, 30 Mar 2018 06:49:30 GMT"}], "update_date": "2018-04-02", "authors_parsed": [["Suarez", "Joseph", ""], ["Johnson", "Justin", ""], ["Li", "Fei-Fei", ""]]}, {"id": "1803.11364", "submitter": "Daiki Tanaka", "authors": "Daiki Tanaka, Daiki Ikami, Toshihiko Yamasaki, Kiyoharu Aizawa", "title": "Joint Optimization Framework for Learning with Noisy Labels", "comments": "To appear at CVPR 2018 (poster), including supplementary material", "journal-ref": "CVPR 2018, pp.5552--5550", "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) trained on large-scale datasets have exhibited\nsignificant performance in image classification. Many large-scale datasets are\ncollected from websites, however they tend to contain inaccurate labels that\nare termed as noisy labels. Training on such noisy labeled datasets causes\nperformance degradation because DNNs easily overfit to noisy labels. To\novercome this problem, we propose a joint optimization framework of learning\nDNN parameters and estimating true labels. Our framework can correct labels\nduring training by alternating update of network parameters and labels. We\nconduct experiments on the noisy CIFAR-10 datasets and the Clothing1M dataset.\nThe results indicate that our approach significantly outperforms other\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 30 Mar 2018 06:53:40 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Tanaka", "Daiki", ""], ["Ikami", "Daiki", ""], ["Yamasaki", "Toshihiko", ""], ["Aizawa", "Kiyoharu", ""]]}, {"id": "1803.11365", "submitter": "Naoto Inoue", "authors": "Naoto Inoue, Ryosuke Furuta, Toshihiko Yamasaki, Kiyoharu Aizawa", "title": "Cross-Domain Weakly-Supervised Object Detection through Progressive\n  Domain Adaptation", "comments": "To appear at CVPR2018 (poster), including supplementary materials", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Can we detect common objects in a variety of image domains without\ninstance-level annotations? In this paper, we present a framework for a novel\ntask, cross-domain weakly supervised object detection, which addresses this\nquestion. For this paper, we have access to images with instance-level\nannotations in a source domain (e.g., natural image) and images with\nimage-level annotations in a target domain (e.g., watercolor). In addition, the\nclasses to be detected in the target domain are all or a subset of those in the\nsource domain. Starting from a fully supervised object detector, which is\npre-trained on the source domain, we propose a two-step progressive domain\nadaptation technique by fine-tuning the detector on two types of artificially\nand automatically generated samples. We test our methods on our newly collected\ndatasets containing three image domains, and achieve an improvement of\napproximately 5 to 20 percentage points in terms of mean average precision\n(mAP) compared to the best-performing baselines.\n", "versions": [{"version": "v1", "created": "Fri, 30 Mar 2018 06:53:43 GMT"}], "update_date": "2018-04-02", "authors_parsed": [["Inoue", "Naoto", ""], ["Furuta", "Ryosuke", ""], ["Yamasaki", "Toshihiko", ""], ["Aizawa", "Kiyoharu", ""]]}, {"id": "1803.11366", "submitter": "Qijun Zhao", "authors": "Feng Liu, Ronghang Zhu, Dan Zeng, Qijun Zhao and Xiaoming Liu", "title": "Disentangling Features in 3D Face Shapes for Joint Face Reconstruction\n  and Recognition", "comments": "CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper proposes an encoder-decoder network to disentangle shape features\nduring 3D face reconstruction from single 2D images, such that the tasks of\nreconstructing accurate 3D face shapes and learning discriminative shape\nfeatures for face recognition can be accomplished simultaneously. Unlike\nexisting 3D face reconstruction methods, our proposed method directly regresses\ndense 3D face shapes from single 2D images, and tackles identity and residual\n(i.e., non-identity) components in 3D face shapes explicitly and separately\nbased on a composite 3D face shape model with latent representations. We devise\na training process for the proposed network with a joint loss measuring both\nface identification error and 3D face shape reconstruction error. To construct\ntraining data we develop a method for fitting 3D morphable model (3DMM) to\nmultiple 2D images of a subject. Comprehensive experiments have been done on\nMICC, BU3DFE, LFW and YTF databases. The results show that our method expands\nthe capacity of 3DMM for capturing discriminative shape features and facial\ndetail, and thus outperforms existing methods both in 3D face reconstruction\naccuracy and in face recognition accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 30 Mar 2018 06:58:40 GMT"}], "update_date": "2018-04-02", "authors_parsed": [["Liu", "Feng", ""], ["Zhu", "Ronghang", ""], ["Zeng", "Dan", ""], ["Zhao", "Qijun", ""], ["Liu", "Xiaoming", ""]]}, {"id": "1803.11370", "submitter": "Akito Takeki", "authors": "Akito Takeki, Daiki Ikami, Go Irie, and Kiyoharu Aizawa", "title": "Parallel Grid Pooling for Data Augmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural network (CNN) architectures utilize downsampling layers,\nwhich restrict the subsequent layers to learn spatially invariant features\nwhile reducing computational costs. However, such a downsampling operation\nmakes it impossible to use the full spectrum of input features. Motivated by\nthis observation, we propose a novel layer called parallel grid pooling (PGP)\nwhich is applicable to various CNN models. PGP performs downsampling without\ndiscarding any intermediate feature. It works as data augmentation and is\ncomplementary to commonly used data augmentation techniques. Furthermore, we\ndemonstrate that a dilated convolution can naturally be represented using PGP\noperations, which suggests that the dilated convolution can also be regarded as\na type of data augmentation technique. Experimental results based on popular\nimage classification benchmarks demonstrate the effectiveness of the proposed\nmethod. Code is available at: https://github.com/akitotakeki\n", "versions": [{"version": "v1", "created": "Fri, 30 Mar 2018 07:25:00 GMT"}], "update_date": "2018-04-02", "authors_parsed": [["Takeki", "Akito", ""], ["Ikami", "Daiki", ""], ["Irie", "Go", ""], ["Aizawa", "Kiyoharu", ""]]}, {"id": "1803.11395", "submitter": "Guanbin Li", "authors": "Guanbin Li and Yizhou Yu", "title": "Contrast-Oriented Deep Neural Networks for Salient Object Detection", "comments": "Accept to TNNLS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks have become a key element in the recent\nbreakthrough of salient object detection. However, existing CNN-based methods\nare based on either patch-wise (region-wise) training and inference or fully\nconvolutional networks. Methods in the former category are generally\ntime-consuming due to severe storage and computational redundancies among\noverlapping patches. To overcome this deficiency, methods in the second\ncategory attempt to directly map a raw input image to a predicted dense\nsaliency map in a single network forward pass. Though being very efficient, it\nis arduous for these methods to detect salient objects of different scales or\nsalient regions with weak semantic information. In this paper, we develop\nhybrid contrast-oriented deep neural networks to overcome the aforementioned\nlimitations. Each of our deep networks is composed of two complementary\ncomponents, including a fully convolutional stream for dense prediction and a\nsegment-level spatial pooling stream for sparse saliency inference. We further\npropose an attentional module that learns weight maps for fusing the two\nsaliency predictions from these two streams. A tailored alternate scheme is\ndesigned to train these deep networks by fine-tuning pre-trained baseline\nmodels. Finally, a customized fully connected CRF model incorporating a salient\ncontour feature embedding can be optionally applied as a post-processing step\nto improve spatial coherence and contour positioning in the fused result from\nthese two streams. Extensive experiments on six benchmark datasets demonstrate\nthat our proposed model can significantly outperform the state of the art in\nterms of all popular evaluation metrics.\n", "versions": [{"version": "v1", "created": "Fri, 30 Mar 2018 09:51:04 GMT"}], "update_date": "2018-04-03", "authors_parsed": [["Li", "Guanbin", ""], ["Yu", "Yizhou", ""]]}, {"id": "1803.11404", "submitter": "Adrian Spurr", "authors": "Adrian Spurr, Jie Song, Seonwook Park, Otmar Hilliges", "title": "Cross-modal Deep Variational Hand Pose Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The human hand moves in complex and high-dimensional ways, making estimation\nof 3D hand pose configurations from images alone a challenging task. In this\nwork we propose a method to learn a statistical hand model represented by a\ncross-modal trained latent space via a generative deep neural network. We\nderive an objective function from the variational lower bound of the VAE\nframework and jointly optimize the resulting cross-modal KL-divergence and the\nposterior reconstruction objective, naturally admitting a training regime that\nleads to a coherent latent space across multiple modalities such as RGB images,\n2D keypoint detections or 3D hand configurations. Additionally, it grants a\nstraightforward way of using semi-supervision. This latent space can be\ndirectly used to estimate 3D hand poses from RGB images, outperforming the\nstate-of-the art in different settings. Furthermore, we show that our proposed\nmethod can be used without changes on depth images and performs comparably to\nspecialized methods. Finally, the model is fully generative and can synthesize\nconsistent pairs of hand configurations across modalities. We evaluate our\nmethod on both RGB and depth datasets and analyze the latent space\nqualitatively.\n", "versions": [{"version": "v1", "created": "Fri, 30 Mar 2018 10:27:06 GMT"}], "update_date": "2018-04-02", "authors_parsed": [["Spurr", "Adrian", ""], ["Song", "Jie", ""], ["Park", "Seonwook", ""], ["Hilliges", "Otmar", ""]]}, {"id": "1803.11405", "submitter": "Rohit Keshari", "authors": "Rohit Keshari, Mayank Vatsa, Richa Singh, Afzel Noore", "title": "Learning Structure and Strength of CNN Filters for Small Sample Size\n  Training", "comments": "10 pages, 9 figures, Accepted in CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks have provided state-of-the-art results in\nseveral computer vision problems. However, due to a large number of parameters\nin CNNs, they require a large number of training samples which is a limiting\nfactor for small sample size problems. To address this limitation, we propose\nSSF-CNN which focuses on learning the structure and strength of filters. The\nstructure of the filter is initialized using a dictionary-based filter learning\nalgorithm and the strength of the filter is learned using the small sample\ntraining data. The architecture provides the flexibility of training with both\nsmall and large training databases and yields good accuracies even with small\nsize training data. The effectiveness of the algorithm is first demonstrated on\nMNIST, CIFAR10, and NORB databases, with a varying number of training samples.\nThe results show that SSF-CNN significantly reduces the number of parameters\nrequired for training while providing high accuracies the test databases. On\nsmall sample size problems such as newborn face recognition and Omniglot, it\nyields state-of-the-art results. Specifically, on the IIITD Newborn Face\nDatabase, the results demonstrate improvement in rank-1 identification accuracy\nby at least 10%.\n", "versions": [{"version": "v1", "created": "Fri, 30 Mar 2018 10:34:33 GMT"}], "update_date": "2018-04-02", "authors_parsed": [["Keshari", "Rohit", ""], ["Vatsa", "Mayank", ""], ["Singh", "Richa", ""], ["Noore", "Afzel", ""]]}, {"id": "1803.11410", "submitter": "Amnon Drory", "authors": "Amnon Drory, Oria Ratzon, Shai Avidan, Raja Giryes", "title": "The Resistance to Label Noise in K-NN and DNN Depends on its\n  Concentration", "comments": "None", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the classification performance of K-nearest neighbors (K-NN)\nand deep neural networks (DNNs) in the presence of label noise. We first show\nempirically that a DNN's prediction for a given test example depends on the\nlabels of the training examples in its local neighborhood. This motivates us to\nderive a realizable analytic expression that approximates the multi-class K-NN\nclassification error in the presence of label noise, which is of independent\nimportance. We then suggest that the expression for K-NN may serve as a\nfirst-order approximation for the DNN error. Finally, we demonstrate\nempirically the proximity of the developed expression to the observed\nperformance of K-NN and DNN classifiers. Our result may explain the already\nobserved surprising resistance of DNN to some types of label noise. It also\ncharacterizes an important factor of it showing that the more concentrated the\nnoise the greater is the degradation in performance.\n", "versions": [{"version": "v1", "created": "Fri, 30 Mar 2018 11:06:43 GMT"}, {"version": "v2", "created": "Thu, 3 Oct 2019 13:49:57 GMT"}, {"version": "v3", "created": "Thu, 3 Dec 2020 09:18:17 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Drory", "Amnon", ""], ["Ratzon", "Oria", ""], ["Avidan", "Shai", ""], ["Giryes", "Raja", ""]]}, {"id": "1803.11417", "submitter": "Hang Su", "authors": "Hang Su, Shaogang Gong, Xiatian Zhu", "title": "Scalable Deep Learning Logo Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing logo detection methods usually consider a small number of logo\nclasses and limited images per class with a strong assumption of requiring\ntedious object bounding box annotations, therefore not scalable to real-world\ndynamic applications. In this work, we tackle these challenges by exploring the\nwebly data learning principle without the need for exhaustive manual labelling.\nSpecifically, we propose a novel incremental learning approach, called Scalable\nLogo Self-co-Learning (SL^2), capable of automatically self-discovering\ninformative training images from noisy web data for progressively improving\nmodel capability in a cross-model co-learning manner. Moreover, we introduce a\nvery large (2,190,757 images of 194 logo classes) logo dataset \"WebLogo-2M\" by\nan automatic web data collection and processing method. Extensive comparative\nevaluations demonstrate the superiority of the proposed SL^2 method over the\nstate-of-the-art strongly and weakly supervised detection models and\ncontemporary webly data learning approaches.\n", "versions": [{"version": "v1", "created": "Fri, 30 Mar 2018 11:22:16 GMT"}, {"version": "v2", "created": "Mon, 2 Apr 2018 19:37:23 GMT"}], "update_date": "2018-04-04", "authors_parsed": [["Su", "Hang", ""], ["Gong", "Shaogang", ""], ["Zhu", "Xiatian", ""]]}, {"id": "1803.11438", "submitter": "Lin Ma", "authors": "Bairui Wang and Lin Ma and Wei Zhang and Wei Liu", "title": "Reconstruction Network for Video Captioning", "comments": "Accepted by CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, the problem of describing visual contents of a video sequence\nwith natural language is addressed. Unlike previous video captioning work\nmainly exploiting the cues of video contents to make a language description, we\npropose a reconstruction network (RecNet) with a novel\nencoder-decoder-reconstructor architecture, which leverages both the forward\n(video to sentence) and backward (sentence to video) flows for video\ncaptioning. Specifically, the encoder-decoder makes use of the forward flow to\nproduce the sentence description based on the encoded video semantic features.\nTwo types of reconstructors are customized to employ the backward flow and\nreproduce the video features based on the hidden state sequence generated by\nthe decoder. The generation loss yielded by the encoder-decoder and the\nreconstruction loss introduced by the reconstructor are jointly drawn into\ntraining the proposed RecNet in an end-to-end fashion. Experimental results on\nbenchmark datasets demonstrate that the proposed reconstructor can boost the\nencoder-decoder models and leads to significant gains in video caption\naccuracy.\n", "versions": [{"version": "v1", "created": "Fri, 30 Mar 2018 13:09:30 GMT"}], "update_date": "2018-04-02", "authors_parsed": [["Wang", "Bairui", ""], ["Ma", "Lin", ""], ["Zhang", "Wei", ""], ["Liu", "Wei", ""]]}, {"id": "1803.11439", "submitter": "Xinpeng Chen", "authors": "Xinpeng Chen and Lin Ma and Wenhao Jiang and Jian Yao and Wei Liu", "title": "Regularizing RNNs for Caption Generation by Reconstructing The Past with\n  The Present", "comments": "Accepted by CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, caption generation with an encoder-decoder framework has been\nextensively studied and applied in different domains, such as image captioning,\ncode captioning, and so on. In this paper, we propose a novel architecture,\nnamely Auto-Reconstructor Network (ARNet), which, coupling with the\nconventional encoder-decoder framework, works in an end-to-end fashion to\ngenerate captions. ARNet aims at reconstructing the previous hidden state with\nthe present one, besides behaving as the input-dependent transition operator.\nTherefore, ARNet encourages the current hidden state to embed more information\nfrom the previous one, which can help regularize the transition dynamics of\nrecurrent neural networks (RNNs). Extensive experimental results show that our\nproposed ARNet boosts the performance over the existing encoder-decoder models\non both image captioning and source code captioning tasks. Additionally, ARNet\nremarkably reduces the discrepancy between training and inference processes for\ncaption generation. Furthermore, the performance on permuted sequential MNIST\ndemonstrates that ARNet can effectively regularize RNN, especially on modeling\nlong-term dependencies. Our code is available at:\nhttps://github.com/chenxinpeng/ARNet\n", "versions": [{"version": "v1", "created": "Fri, 30 Mar 2018 13:15:56 GMT"}, {"version": "v2", "created": "Sat, 7 Apr 2018 01:49:44 GMT"}], "update_date": "2018-04-10", "authors_parsed": [["Chen", "Xinpeng", ""], ["Ma", "Lin", ""], ["Jiang", "Wenhao", ""], ["Yao", "Jian", ""], ["Liu", "Wei", ""]]}, {"id": "1803.11493", "submitter": "Alexander Grabner", "authors": "Alexander Grabner, Peter M. Roth and Vincent Lepetit", "title": "3D Pose Estimation and 3D Model Retrieval for Objects in the Wild", "comments": "Accepted to Conference on Computer Vision and Pattern Recognition\n  (CVPR) 2018", "journal-ref": "Conference on Computer Vision and Pattern Recognition 2018", "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a scalable, efficient and accurate approach to retrieve 3D models\nfor objects in the wild. Our contribution is twofold. We first present a 3D\npose estimation approach for object categories which significantly outperforms\nthe state-of-the-art on Pascal3D+. Second, we use the estimated pose as a prior\nto retrieve 3D models which accurately represent the geometry of objects in RGB\nimages. For this purpose, we render depth images from 3D models under our\npredicted pose and match learned image descriptors of RGB images against those\nof rendered depth images using a CNN-based multi-view metric learning approach.\nIn this way, we are the first to report quantitative results for 3D model\nretrieval on Pascal3D+, where our method chooses the same models as human\nannotators for 50% of the validation images on average. In addition, we show\nthat our method, which was trained purely on Pascal3D+, retrieves rich and\naccurate 3D models from ShapeNet given RGB images of objects in the wild.\n", "versions": [{"version": "v1", "created": "Fri, 30 Mar 2018 14:47:49 GMT"}], "update_date": "2018-04-02", "authors_parsed": [["Grabner", "Alexander", ""], ["Roth", "Peter M.", ""], ["Lepetit", "Vincent", ""]]}, {"id": "1803.11496", "submitter": "Pauline Luc", "authors": "Pauline Luc, Camille Couprie, Yann LeCun, Jakob Verbeek", "title": "Predicting Future Instance Segmentation by Forecasting Convolutional\n  Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anticipating future events is an important prerequisite towards intelligent\nbehavior. Video forecasting has been studied as a proxy task towards this goal.\nRecent work has shown that to predict semantic segmentation of future frames,\nforecasting at the semantic level is more effective than forecasting RGB frames\nand then segmenting these. In this paper we consider the more challenging\nproblem of future instance segmentation, which additionally segments out\nindividual objects. To deal with a varying number of output labels per image,\nwe develop a predictive model in the space of fixed-sized convolutional\nfeatures of the Mask R-CNN instance segmentation model. We apply the \"detection\nhead'\" of Mask R-CNN on the predicted features to produce the instance\nsegmentation of future frames. Experiments show that this approach\nsignificantly improves over strong baselines based on optical flow and\nrepurposed instance segmentation architectures.\n", "versions": [{"version": "v1", "created": "Fri, 30 Mar 2018 14:55:32 GMT"}, {"version": "v2", "created": "Wed, 3 Oct 2018 10:12:48 GMT"}], "update_date": "2018-10-04", "authors_parsed": [["Luc", "Pauline", ""], ["Couprie", "Camille", ""], ["LeCun", "Yann", ""], ["Verbeek", "Jakob", ""]]}, {"id": "1803.11527", "submitter": "Yifan Xu", "authors": "Yifan Xu, Tianqi Fan, Mingye Xu, Long Zeng, Yu Qiao", "title": "SpiderCNN: Deep Learning on Point Sets with Parameterized Convolutional\n  Filters", "comments": "European Conference on Computer Vision 2018 (ECCV 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have enjoyed remarkable success for various vision\ntasks, however it remains challenging to apply CNNs to domains lacking a\nregular underlying structures such as 3D point clouds. Towards this we propose\na novel convolutional architecture, termed SpiderCNN, to efficiently extract\ngeometric features from point clouds. SpiderCNN is comprised of units called\nSpiderConv, which extend convolutional operations from regular grids to\nirregular point sets that can be embedded in R^n, by parametrizing a family of\nconvolutional filters. We design the filter as a product of a simple step\nfunction that captures local geodesic information and a Taylor polynomial that\nensures the expressiveness. SpiderCNN inherits the multi-scale hierarchical\narchitecture from classical CNNs, which allows it to extract semantic deep\nfeatures. Experiments on ModelNet40 demonstrate that SpiderCNN achieves\nstate-of-the-art accuracy 92.4% on standard benchmarks, and shows competitive\nperformance on segmentation task.\n", "versions": [{"version": "v1", "created": "Fri, 30 Mar 2018 16:10:21 GMT"}, {"version": "v2", "created": "Mon, 10 Sep 2018 14:16:21 GMT"}, {"version": "v3", "created": "Wed, 12 Sep 2018 15:42:21 GMT"}], "update_date": "2018-09-13", "authors_parsed": [["Xu", "Yifan", ""], ["Fan", "Tianqi", ""], ["Xu", "Mingye", ""], ["Zeng", "Long", ""], ["Qiao", "Yu", ""]]}, {"id": "1803.11544", "submitter": "Iro Laina", "authors": "Christian Rupprecht, Iro Laina, Nassir Navab, Gregory D. Hager,\n  Federico Tombari", "title": "Guide Me: Interacting with Deep Networks", "comments": "CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interaction and collaboration between humans and intelligent machines has\nbecome increasingly important as machine learning methods move into real-world\napplications that involve end users. While much prior work lies at the\nintersection of natural language and vision, such as image captioning or image\ngeneration from text descriptions, less focus has been placed on the use of\nlanguage to guide or improve the performance of a learned visual processing\nalgorithm. In this paper, we explore methods to flexibly guide a trained\nconvolutional neural network through user input to improve its performance\nduring inference. We do so by inserting a layer that acts as a spatio-semantic\nguide into the network. This guide is trained to modify the network's\nactivations, either directly via an energy minimization scheme or indirectly\nthrough a recurrent model that translates human language queries to interaction\nweights. Learning the verbal interaction is fully automatic and does not\nrequire manual text annotations. We evaluate the method on two datasets,\nshowing that guiding a pre-trained network can improve performance, and provide\nextensive insights into the interaction between the guide and the CNN.\n", "versions": [{"version": "v1", "created": "Fri, 30 Mar 2018 17:28:52 GMT"}], "update_date": "2018-04-02", "authors_parsed": [["Rupprecht", "Christian", ""], ["Laina", "Iro", ""], ["Navab", "Nassir", ""], ["Hager", "Gregory D.", ""], ["Tombari", "Federico", ""]]}, {"id": "1803.11550", "submitter": "Gerome Vivar", "authors": "Gerome Vivar, Andreas Zwergal, Nassir Navab, and Seyed-Ahmad Ahmadi", "title": "Multi-modal Disease Classification in Incomplete Datasets Using\n  Geometric Matrix Completion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In large population-based studies and in clinical routine, tasks like disease\ndiagnosis and progression prediction are inherently based on a rich set of\nmulti-modal data, including imaging and other sensor data, clinical scores,\nphenotypes, labels and demographics. However, missing features, rater bias and\ninaccurate measurements are typical ailments of real-life medical datasets.\nRecently, it has been shown that deep learning with graph convolution neural\nnetworks (GCN) can outperform traditional machine learning in disease\nclassification, but missing features remain an open problem. In this work, we\nfollow up on the idea of modeling multi-modal disease classification as a\nmatrix completion problem, with simultaneous classification and non-linear\nimputation of features. Compared to methods before, we arrange subjects in a\ngraph-structure and solve classification through geometric matrix completion,\nwhich simulates a heat diffusion process that is learned and solved with a\nrecurrent neural network. We demonstrate the potential of this method on the\nADNI-based TADPOLE dataset and on the task of predicting the transition from\nMCI to Alzheimer's disease. With an AUC of 0.950 and classification accuracy of\n87%, our approach outperforms standard linear and non-linear classifiers, as\nwell as several state-of-the-art results in related literature, including a\nrecently proposed GCN-based approach.\n", "versions": [{"version": "v1", "created": "Fri, 30 Mar 2018 17:40:02 GMT"}], "update_date": "2018-04-02", "authors_parsed": [["Vivar", "Gerome", ""], ["Zwergal", "Andreas", ""], ["Navab", "Nassir", ""], ["Ahmadi", "Seyed-Ahmad", ""]]}, {"id": "1803.11556", "submitter": "Zhongzheng Ren", "authors": "Zhongzheng Ren, Yong Jae Lee, Michael S. Ryoo", "title": "Learning to Anonymize Faces for Privacy Preserving Action Detection", "comments": "ECCV'18 camera ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is an increasing concern in computer vision devices invading users'\nprivacy by recording unwanted videos. On the one hand, we want the camera\nsystems to recognize important events and assist human daily lives by\nunderstanding its videos, but on the other hand we want to ensure that they do\nnot intrude people's privacy. In this paper, we propose a new principled\napproach for learning a video \\emph{face anonymizer}. We use an adversarial\ntraining setting in which two competing systems fight: (1) a video anonymizer\nthat modifies the original video to remove privacy-sensitive information while\nstill trying to maximize spatial action detection performance, and (2) a\ndiscriminator that tries to extract privacy-sensitive information from the\nanonymized videos. The end result is a video anonymizer that performs\npixel-level modifications to anonymize each person's face, with minimal effect\non action detection performance. We experimentally confirm the benefits of our\napproach compared to conventional hand-crafted anonymization methods including\nmasking, blurring, and noise adding. Code, demo, and more results can be found\non our project page https://jason718.github.io/project/privacy/main.html.\n", "versions": [{"version": "v1", "created": "Fri, 30 Mar 2018 17:55:04 GMT"}, {"version": "v2", "created": "Thu, 26 Jul 2018 18:40:52 GMT"}], "update_date": "2018-07-30", "authors_parsed": [["Ren", "Zhongzheng", ""], ["Lee", "Yong Jae", ""], ["Ryoo", "Michael S.", ""]]}]