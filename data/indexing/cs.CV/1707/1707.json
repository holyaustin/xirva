[{"id": "1707.00051", "submitter": "Manikandasriram Srinivasan Ramanagopal", "authors": "Manikandasriram Srinivasan Ramanagopal, Cyrus Anderson, Ram Vasudevan\n  and Matthew Johnson-Roberson", "title": "Failing to Learn: Autonomously Identifying Perception Failures for\n  Self-driving Cars", "comments": "8 pages, 4 figures and 4 tables. Accepted for publication in RA-L and\n  will be presented in IROS 2018 in Madrid, Spain", "journal-ref": null, "doi": "10.1109/LRA.2018.2857402", "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the major open challenges in self-driving cars is the ability to\ndetect cars and pedestrians to safely navigate in the world. Deep\nlearning-based object detector approaches have enabled great advances in using\ncamera imagery to detect and classify objects. But for a safety critical\napplication, such as autonomous driving, the error rates of the current state\nof the art are still too high to enable safe operation. Moreover, the\ncharacterization of object detector performance is primarily limited to testing\non prerecorded datasets. Errors that occur on novel data go undetected without\nadditional human labels. In this letter, we propose an automated method to\nidentify mistakes made by object detectors without ground truth labels. We show\nthat inconsistencies in the object detector output between a pair of similar\nimages can be used as hypotheses for false negatives (e.g., missed detections)\nand using a novel set of features for each hypothesis, an off-the-shelf binary\nclassifier can be used to find valid errors. In particular, we study two\ndistinct cues - temporal and stereo inconsistencies - using data that are\nreadily available on most autonomous vehicles. Our method can be used with any\ncamera-based object detector and we illustrate the technique on several sets of\nreal world data. We show that a state-of-the-art detector, tracker, and our\nclassifier trained only on synthetic data can identify valid errors on KITTI\ntracking dataset with an average precision of 0.94. We also release a new\ntracking dataset with 104 sequences totaling 80,655 labeled pairs of stereo\nimages along with ground truth disparity from a game engine to facilitate\nfurther research. The dataset and code are available at\nhttps://fcav.engin.umich.edu/research/failing-to-learn\n", "versions": [{"version": "v1", "created": "Fri, 30 Jun 2017 21:42:47 GMT"}, {"version": "v2", "created": "Wed, 12 Jul 2017 01:58:46 GMT"}, {"version": "v3", "created": "Mon, 26 Mar 2018 19:09:44 GMT"}, {"version": "v4", "created": "Thu, 26 Jul 2018 19:41:39 GMT"}], "update_date": "2018-07-30", "authors_parsed": [["Ramanagopal", "Manikandasriram Srinivasan", ""], ["Anderson", "Cyrus", ""], ["Vasudevan", "Ram", ""], ["Johnson-Roberson", "Matthew", ""]]}, {"id": "1707.00058", "submitter": "Qing Li", "authors": "Qing Li, Qiang Peng, Chuan Yan", "title": "Multiple VLAD encoding of CNNs for image classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Despite the effectiveness of convolutional neural networks (CNNs) especially\nin image classification tasks, the effect of convolution features on learned\nrepresentations is still limited. It mostly focuses on the salient object of\nthe images, but ignores the variation information on clutter and local. In this\npaper, we propose a special framework, which is the multiple VLAD encoding\nmethod with the CNNs features for image classification. Furthermore, in order\nto improve the performance of the VLAD coding method, we explore the\nmultiplicity of VLAD encoding with the extension of three kinds of encoding\nalgorithms, which are the VLAD-SA method, the VLAD-LSA and the VLAD-LLC method.\nFinally, we equip the spatial pyramid patch (SPM) on VLAD encoding to add the\nspatial information of CNNs feature. In particular, the power of SPM leads our\nframework to yield better performance compared to the existing method.\n", "versions": [{"version": "v1", "created": "Fri, 30 Jun 2017 22:22:00 GMT"}], "update_date": "2017-07-04", "authors_parsed": [["Li", "Qing", ""], ["Peng", "Qiang", ""], ["Yan", "Chuan", ""]]}, {"id": "1707.00067", "submitter": "Viren Jain", "authors": "Viren Jain", "title": "Adversarial Image Alignment and Interpolation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Volumetric (3d) images are acquired for many scientific and biomedical\npurposes using imaging methods such as serial section microscopy, CT scans, and\nMRI. A frequent step in the analysis and reconstruction of such data is the\nalignment and registration of images that were acquired in succession along a\nspatial or temporal dimension. For example, in serial section electron\nmicroscopy, individual 2d sections are imaged via electron microscopy and then\nmust be aligned to one another in order to produce a coherent 3d volume. State\nof the art approaches find image correspondences derived from patch matching\nand invariant feature detectors, and then solve optimization problems that\nrigidly or elastically deform series of images into an aligned volume. Here we\nshow how fully convolutional neural networks trained with an adversarial loss\nfunction can be used for two tasks: (1) synthesis of missing or damaged image\ndata from adjacent sections, and (2) fine-scale alignment of block-face\nelectron microscopy data. Finally, we show how these two capabilities can be\ncombined in order to produce artificial isotropic volumes from anisotropic\nimage volumes using a super-resolution adversarial alignment and interpolation\napproach.\n", "versions": [{"version": "v1", "created": "Fri, 30 Jun 2017 23:59:27 GMT"}], "update_date": "2017-07-04", "authors_parsed": [["Jain", "Viren", ""]]}, {"id": "1707.00070", "submitter": "Patrick Virtue", "authors": "Patrick Virtue, Stella X. Yu, Michael Lustig", "title": "Better than Real: Complex-valued Neural Nets for MRI Fingerprinting", "comments": "Accepted in Proc. IEEE International Conference on Image Processing\n  (ICIP), 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of MRI fingerprinting is to identify tissue parameters from\ncomplex-valued MRI signals. The prevalent approach is dictionary based, where a\ntest MRI signal is compared to stored MRI signals with known tissue parameters\nand the most similar signals and tissue parameters retrieved. Such an approach\ndoes not scale with the number of parameters and is rather slow when the tissue\nparameter space is large.\n  Our first novel contribution is to use deep learning as an efficient\nnonlinear inverse mapping approach. We generate synthetic (tissue, MRI) data\nfrom an MRI simulator, and use them to train a deep net to map the MRI signal\nto the tissue parameters directly.\n  Our second novel contribution is to develop a complex-valued neural network\nwith new cardioid activation functions. Our results demonstrate that\ncomplex-valued neural nets could be much more accurate than real-valued neural\nnets at complex-valued MRI fingerprinting.\n", "versions": [{"version": "v1", "created": "Sat, 1 Jul 2017 00:15:33 GMT"}], "update_date": "2017-07-04", "authors_parsed": [["Virtue", "Patrick", ""], ["Yu", "Stella X.", ""], ["Lustig", "Michael", ""]]}, {"id": "1707.00081", "submitter": "Alexander Wong", "authors": "A. H. Karimi, M. J. Shafiee, A. Ghodsi, and A. Wong", "title": "Synthesizing Deep Neural Network Architectures using Biological Synaptic\n  Strength Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we perform an exploratory study on synthesizing deep neural\nnetworks using biological synaptic strength distributions, and the potential\ninfluence of different distributions on modelling performance particularly for\nthe scenario associated with small data sets. Surprisingly, a CNN with\nconvolutional layer synaptic strengths drawn from biologically-inspired\ndistributions such as log-normal or correlated center-surround distributions\nperformed relatively well suggesting a possibility for designing deep neural\nnetwork architectures that do not require many data samples to learn, and can\nsidestep current training procedures while maintaining or boosting modelling\nperformance.\n", "versions": [{"version": "v1", "created": "Sat, 1 Jul 2017 01:30:21 GMT"}], "update_date": "2017-07-04", "authors_parsed": [["Karimi", "A. H.", ""], ["Shafiee", "M. J.", ""], ["Ghodsi", "A.", ""], ["Wong", "A.", ""]]}, {"id": "1707.00095", "submitter": "Alexander Wong", "authors": "Mohammad Javad Shafiee, Francis Li, and Alexander Wong", "title": "Exploring the Imposition of Synaptic Precision Restrictions For\n  Evolutionary Synthesis of Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key contributing factor to incredible success of deep neural networks has\nbeen the significant rise on massively parallel computing devices allowing\nresearchers to greatly increase the size and depth of deep neural networks,\nleading to significant improvements in modeling accuracy. Although deeper,\nlarger, or complex deep neural networks have shown considerable promise, the\ncomputational complexity of such networks is a major barrier to utilization in\nresource-starved scenarios. We explore the synaptogenesis of deep neural\nnetworks in the formation of efficient deep neural network architectures within\nan evolutionary deep intelligence framework, where a probabilistic generative\nmodeling strategy is introduced to stochastically synthesize increasingly\nefficient yet effective offspring deep neural networks over generations,\nmimicking evolutionary processes such as heredity, random mutation, and natural\nselection in a probabilistic manner. In this study, we primarily explore the\nimposition of synaptic precision restrictions and its impact on the\nevolutionary synthesis of deep neural networks to synthesize more efficient\nnetwork architectures tailored for resource-starved scenarios. Experimental\nresults show significant improvements in synaptic efficiency (~10X decrease for\nGoogLeNet-based DetectNet) and inference speed (>5X increase for\nGoogLeNet-based DetectNet) while preserving modeling accuracy.\n", "versions": [{"version": "v1", "created": "Sat, 1 Jul 2017 04:56:08 GMT"}], "update_date": "2017-07-04", "authors_parsed": [["Shafiee", "Mohammad Javad", ""], ["Li", "Francis", ""], ["Wong", "Alexander", ""]]}, {"id": "1707.00116", "submitter": "Xianxu Hou", "authors": "Xianxu Hou, Guoping Qiu", "title": "Image Companding and Inverse Halftoning using Deep Convolutional Neural\n  Networks", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce deep learning technology to tackle two\ntraditional low-level image processing problems, companding and inverse\nhalftoning. We make two main contributions. First, to the best knowledge of the\nauthors, this is the first work that has successfully developed deep learning\nbased solutions to these two traditional low-level image processing problems.\nThis not only introduces new methods to tackle well-known image processing\nproblems but also demonstrates the power of deep learning in solving\ntraditional signal processing problems. Second, we have developed an effective\ndeep learning algorithm based on insights into the properties of visual quality\nof images and the internal representation properties of a deep convolutional\nneural network (CNN). We train a deep CNN as a nonlinear transformation\nfunction to map a low bit depth image to higher bit depth or from a halftone\nimage to a continuous tone image. We also employ another pretrained deep CNN as\na feature extractor to derive visually important features to construct the\nobjective function for the training of the mapping CNN. We present experimental\nresults to demonstrate the effectiveness of the new deep learning based\nsolutions.\n", "versions": [{"version": "v1", "created": "Sat, 1 Jul 2017 08:42:48 GMT"}, {"version": "v2", "created": "Fri, 21 Jul 2017 14:01:29 GMT"}], "update_date": "2017-07-24", "authors_parsed": [["Hou", "Xianxu", ""], ["Qiu", "Guoping", ""]]}, {"id": "1707.00243", "submitter": "Ning Xu", "authors": "Ning Xu, Brian Price, Scott Cohen, Jimei Yang, Thomas Huang", "title": "Deep GrabCut for Object Selection", "comments": "BMVC 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most previous bounding-box-based segmentation methods assume the bounding box\ntightly covers the object of interest. However it is common that a rectangle\ninput could be too large or too small. In this paper, we propose a novel\nsegmentation approach that uses a rectangle as a soft constraint by\ntransforming it into an Euclidean distance map. A convolutional encoder-decoder\nnetwork is trained end-to-end by concatenating images with these distance maps\nas inputs and predicting the object masks as outputs. Our approach gets\naccurate segmentation results given sloppy rectangles while being general for\nboth interactive segmentation and instance segmentation. We show our network\nextends to curve-based input without retraining. We further apply our network\nto instance-level semantic segmentation and resolve any overlap using a\nconditional random field. Experiments on benchmark datasets demonstrate the\neffectiveness of the proposed approaches.\n", "versions": [{"version": "v1", "created": "Sun, 2 Jul 2017 05:53:49 GMT"}, {"version": "v2", "created": "Fri, 14 Jul 2017 18:52:26 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Xu", "Ning", ""], ["Price", "Brian", ""], ["Cohen", "Scott", ""], ["Yang", "Jimei", ""], ["Huang", "Thomas", ""]]}, {"id": "1707.00251", "submitter": "SangKuk Lee", "authors": "Sangkuk Lee, Daesik Kim, Myunggi Lee, Jihye Hwang, Nojun Kwak", "title": "Where to Play: Retrieval of Video Segments using Natural-Language\n  Queries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a new approach for retrieval of video segments\nusing natural language queries. Unlike most previous approaches such as\nconcept-based methods or rule-based structured models, the proposed method uses\nimage captioning model to construct sentential queries for visual information.\nIn detail, our approach exploits multiple captions generated by visual features\nin each image with `Densecap'. Then, the similarities between captions of\nadjacent images are calculated, which is used to track semantically similar\ncaptions over multiple frames. Besides introducing this novel idea of 'tracking\nby captioning', the proposed method is one of the first approaches that uses a\nlanguage generation model learned by neural networks to construct semantic\nquery describing the relations and properties of visual information. To\nevaluate the effectiveness of our approach, we have created a new evaluation\ndataset, which contains about 348 segments of scenes in 20 movie-trailers.\nThrough quantitative and qualitative evaluation, we show that our method is\neffective for retrieval of video segments using natural language queries.\n", "versions": [{"version": "v1", "created": "Sun, 2 Jul 2017 07:56:06 GMT"}], "update_date": "2017-07-04", "authors_parsed": [["Lee", "Sangkuk", ""], ["Kim", "Daesik", ""], ["Lee", "Myunggi", ""], ["Hwang", "Jihye", ""], ["Kwak", "Nojun", ""]]}, {"id": "1707.00281", "submitter": "Aritra Dutta", "authors": "Aritra Dutta, Xin Li, Peter Richt\\'arik", "title": "A Batch-Incremental Video Background Estimation Model using Weighted\n  Low-Rank Approximation of Matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV math.NA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Principal component pursuit (PCP) is a state-of-the-art approach for\nbackground estimation problems. Due to their higher computational cost, PCP\nalgorithms, such as robust principal component analysis (RPCA) and its\nvariants, are not feasible in processing high definition videos. To avoid the\ncurse of dimensionality in those algorithms, several methods have been proposed\nto solve the background estimation problem in an incremental manner. We propose\na batch-incremental background estimation model using a special weighted\nlow-rank approximation of matrices. Through experiments with real and synthetic\nvideo sequences, we demonstrate that our method is superior to the\nstate-of-the-art background estimation algorithms such as GRASTA, ReProCS,\nincPCP, and GFL.\n", "versions": [{"version": "v1", "created": "Sun, 2 Jul 2017 11:55:09 GMT"}], "update_date": "2017-07-04", "authors_parsed": [["Dutta", "Aritra", ""], ["Li", "Xin", ""], ["Richt\u00e1rik", "Peter", ""]]}, {"id": "1707.00333", "submitter": "Vikas Gupta", "authors": "Vikas Gupta and Shanmuganathan Raman", "title": "Automatic Trimap Generation for Image Matting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image matting is a longstanding problem in computational photography.\nAlthough, it has been studied for more than two decades, yet there is a\nchallenge of developing an automatic matting algorithm which does not require\nany human efforts. Most of the state-of-the-art matting algorithms require\nhuman intervention in the form of trimap or scribbles to generate the alpha\nmatte form the input image. In this paper, we present a simple and efficient\napproach to automatically generate the trimap from the input image and make the\nwhole matting process free from human-in-the-loop. We use learning based\nmatting method to generate the matte from the automatically generated trimap.\nExperimental results demonstrate that our method produces good quality trimap\nwhich results into accurate matte estimation. We validate our results by\nreplacing the automatically generated trimap by manually created trimap while\nusing the same image matting algorithm.\n", "versions": [{"version": "v1", "created": "Sun, 2 Jul 2017 18:37:53 GMT"}, {"version": "v2", "created": "Tue, 4 Jul 2017 06:44:18 GMT"}], "update_date": "2017-07-05", "authors_parsed": [["Gupta", "Vikas", ""], ["Raman", "Shanmuganathan", ""]]}, {"id": "1707.00372", "submitter": "Jong Chul Ye", "authors": "Jong Chul Ye, Yoseob Han, Eunju Cha", "title": "Deep Convolutional Framelets: A General Deep Learning Framework for\n  Inverse Problems", "comments": "This will appear in SIAM Journal on Imaging Sciences", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, deep learning approaches with various network architectures have\nachieved significant performance improvement over existing iterative\nreconstruction methods in various imaging problems. However, it is still\nunclear why these deep learning architectures work for specific inverse\nproblems. To address these issues, here we show that the long-searched-for\nmissing link is the convolution framelets for representing a signal by\nconvolving local and non-local bases. The convolution framelets was originally\ndeveloped to generalize the theory of low-rank Hankel matrix approaches for\ninverse problems, and this paper further extends the idea so that we can obtain\na deep neural network using multilayer convolution framelets with perfect\nreconstruction (PR) under rectilinear linear unit nonlinearity (ReLU). Our\nanalysis also shows that the popular deep network components such as residual\nblock, redundant filter channels, and concatenated ReLU (CReLU) do indeed help\nto achieve the PR, while the pooling and unpooling layers should be augmented\nwith high-pass branches to meet the PR condition. Moreover, by changing the\nnumber of filter channels and bias, we can control the shrinkage behaviors of\nthe neural network. This discovery leads us to propose a novel theory for deep\nconvolutional framelets neural network. Using numerical experiments with\nvarious inverse problems, we demonstrated that our deep convolution framelets\nnetwork shows consistent improvement over existing deep architectures.This\ndiscovery suggests that the success of deep learning is not from a magical\npower of a black-box, but rather comes from the power of a novel signal\nrepresentation using non-local basis combined with data-driven local basis,\nwhich is indeed a natural extension of classical signal processing theory.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jul 2017 00:16:04 GMT"}, {"version": "v2", "created": "Thu, 20 Jul 2017 08:20:27 GMT"}, {"version": "v3", "created": "Wed, 2 Aug 2017 13:27:07 GMT"}, {"version": "v4", "created": "Thu, 30 Nov 2017 07:12:01 GMT"}, {"version": "v5", "created": "Thu, 25 Jan 2018 09:37:10 GMT"}], "update_date": "2018-01-26", "authors_parsed": [["Ye", "Jong Chul", ""], ["Han", "Yoseob", ""], ["Cha", "Eunju", ""]]}, {"id": "1707.00380", "submitter": "Fujiao Ju", "authors": "Fujiao Ju, Yanfeng Sun, Junbin Gao, Yongli Hu and Baocai Yin", "title": "Vectorial Dimension Reduction for Tensors Based on Bayesian Inference", "comments": "Submiting to TNNLS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dimensionality reduction for high-order tensors is a challenging problem. In\nconventional approaches, higher order tensors are `vectorized` via Tucker\ndecomposition to obtain lower order tensors. This will destroy the inherent\nhigh-order structures or resulting in undesired tensors, respectively. This\npaper introduces a probabilistic vectorial dimensionality reduction model for\ntensorial data. The model represents a tensor by employing a linear combination\nof same order basis tensors, thus it offers a mechanism to directly reduce a\ntensor to a vector. Under this expression, the projection base of the model is\nbased on the tensor CandeComp/PARAFAC (CP) decomposition and the number of free\nparameters in the model only grows linearly with the number of modes rather\nthan exponentially. A Bayesian inference has been established via the\nvariational EM approach. A criterion to set the parameters (factor number of CP\ndecomposition and the number of extracted features) is empirically given. The\nmodel outperforms several existing PCA-based methods and CP decomposition on\nseveral publicly available databases in terms of classification and clustering\naccuracy.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jul 2017 02:35:21 GMT"}], "update_date": "2017-07-04", "authors_parsed": [["Ju", "Fujiao", ""], ["Sun", "Yanfeng", ""], ["Gao", "Junbin", ""], ["Hu", "Yongli", ""], ["Yin", "Baocai", ""]]}, {"id": "1707.00381", "submitter": "Andrew Spek", "authors": "Andrew Spek and Tom Drummond", "title": "Joint Pose and Principal Curvature Refinement Using Quadrics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a novel joint approach for optimising surface\ncurvature and pose alignment. We present two implementations of this joint\noptimisation strategy, including a fast implementation that uses two frames and\nan offline multi-frame approach. We demonstrate an order of magnitude\nimprovement in simulation over state of the art dense relative point-to-plane\nIterative Closest Point (ICP) pose alignment using our dense joint\nframe-to-frame approach and show comparable pose drift to dense point-to-plane\nICP bundle adjustment using low-cost depth sensors. Additionally our improved\njoint quadric based approach can be used to more accurately estimate surface\ncurvature on noisy point clouds than previous approaches.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jul 2017 02:35:59 GMT"}, {"version": "v2", "created": "Fri, 14 Jul 2017 02:56:57 GMT"}], "update_date": "2017-07-17", "authors_parsed": [["Spek", "Andrew", ""], ["Drummond", "Tom", ""]]}, {"id": "1707.00383", "submitter": "Anbang Yao", "authors": "Hao Zhao, Ming Lu, Anbang Yao, Yiwen Guo, Yurong Chen, Li Zhang", "title": "Physics Inspired Optimization on Semantic Transfer Features: An\n  Alternative Method for Room Layout Estimation", "comments": "To appear in CVPR 2017. Project Page:\n  https://sites.google.com/view/st-pio/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an alternative method to estimate room layouts of\ncluttered indoor scenes. This method enjoys the benefits of two novel\ntechniques. The first one is semantic transfer (ST), which is: (1) a\nformulation to integrate the relationship between scene clutter and room layout\ninto convolutional neural networks; (2) an architecture that can be end-to-end\ntrained; (3) a practical strategy to initialize weights for very deep networks\nunder unbalanced training data distribution. ST allows us to extract highly\nrobust features under various circumstances, and in order to address the\ncomputation redundance hidden in these features we develop a principled and\nefficient inference scheme named physics inspired optimization (PIO). PIO's\nbasic idea is to formulate some phenomena observed in ST features into\nmechanics concepts. Evaluations on public datasets LSUN and Hedau show that the\nproposed method is more accurate than state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jul 2017 02:46:02 GMT"}], "update_date": "2017-07-04", "authors_parsed": [["Zhao", "Hao", ""], ["Lu", "Ming", ""], ["Yao", "Anbang", ""], ["Guo", "Yiwen", ""], ["Chen", "Yurong", ""], ["Zhang", "Li", ""]]}, {"id": "1707.00385", "submitter": "Andrew Spek", "authors": "Andrew Spek, Wai Ho Li, Tom Drummond", "title": "A Fast Method For Computing Principal Curvatures From Range Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimation of surface curvature from range data is important for a range of\ntasks in computer vision and robotics, object segmentation, object recognition\nand robotic grasping estimation. This work presents a fast method of robustly\ncomputing accurate metric principal curvature values from noisy point clouds\nwhich was implemented on GPU. In contrast to existing readily available\nsolutions which first differentiate the surface to estimate surface normals and\nthen differentiate these to obtain curvature, amplifying noise, our method\niteratively fits parabolic quadric surface patches to the data. Additionally\nprevious methods with a similar formulation use less robust techniques less\napplicable to a high noise sensor. We demonstrate that our method is fast and\nprovides better curvature estimates than existing techniques. In particular we\ncompare our method to several alternatives to demonstrate the improvement.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jul 2017 03:01:44 GMT"}, {"version": "v2", "created": "Fri, 14 Jul 2017 02:35:35 GMT"}], "update_date": "2017-07-17", "authors_parsed": [["Spek", "Andrew", ""], ["Li", "Wai Ho", ""], ["Drummond", "Tom", ""]]}, {"id": "1707.00408", "submitter": "Zhedong Zheng", "authors": "Zhedong Zheng, Liang Zheng, Yi Yang", "title": "Pedestrian Alignment Network for Large-scale Person Re-identification", "comments": null, "journal-ref": null, "doi": "10.1109/TCSVT.2018.2873599", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification (person re-ID) is mostly viewed as an image\nretrieval problem. This task aims to search a query person in a large image\npool. In practice, person re-ID usually adopts automatic detectors to obtain\ncropped pedestrian images. However, this process suffers from two types of\ndetector errors: excessive background and part missing. Both errors deteriorate\nthe quality of pedestrian alignment and may compromise pedestrian matching due\nto the position and scale variances. To address the misalignment problem, we\npropose that alignment can be learned from an identification procedure. We\nintroduce the pedestrian alignment network (PAN) which allows discriminative\nembedding learning and pedestrian alignment without extra annotations. Our key\nobservation is that when the convolutional neural network (CNN) learns to\ndiscriminate between different identities, the learned feature maps usually\nexhibit strong activations on the human body rather than the background. The\nproposed network thus takes advantage of this attention mechanism to adaptively\nlocate and align pedestrians within a bounding box. Visual examples show that\npedestrians are better aligned with PAN. Experiments on three large-scale re-ID\ndatasets confirm that PAN improves the discriminative ability of the feature\nembeddings and yields competitive accuracy with the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jul 2017 05:48:54 GMT"}], "update_date": "2019-06-24", "authors_parsed": [["Zheng", "Zhedong", ""], ["Zheng", "Liang", ""], ["Yang", "Yi", ""]]}, {"id": "1707.00409", "submitter": "Jiayun Wang", "authors": "Jiayun Wang, Sanping Zhou, Jinjun Wang, Qiqi Hou", "title": "Deep Ranking Model by Large Adaptive Margin Learning for Person\n  Re-identification", "comments": "Accepted to Pattern Recognition", "journal-ref": null, "doi": "10.1016/j.patcog.2017.09.024", "report-no": "PR-D-17-00433", "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Person re-identification aims to match images of the same person across\ndisjoint camera views, which is a challenging problem in video surveillance.\nThe major challenge of this task lies in how to preserve the similarity of the\nsame person against large variations caused by complex backgrounds, mutual\nocclusions and different illuminations, while discriminating the different\nindividuals. In this paper, we present a novel deep ranking model with feature\nlearning and fusion by learning a large adaptive margin between the intra-class\ndistance and inter-class distance to solve the person re-identification\nproblem. Specifically, we organize the training images into a batch of pairwise\nsamples. Treating these pairwise samples as inputs, we build a novel part-based\ndeep convolutional neural network (CNN) to learn the layered feature\nrepresentations by preserving a large adaptive margin. As a result, the final\nlearned model can effectively find out the matched target to the anchor image\namong a number of candidates in the gallery image set by learning\ndiscriminative and stable feature representations. Overcoming the weaknesses of\nconventional fixed-margin loss functions, our adaptive margin loss function is\nmore appropriate for the dynamic feature space. On four benchmark datasets,\nPRID2011, Market1501, CUHK01 and 3DPeS, we extensively conduct comparative\nevaluations to demonstrate the advantages of the proposed method over the\nstate-of-the-art approaches in person re-identification.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jul 2017 05:58:46 GMT"}, {"version": "v2", "created": "Sun, 17 Sep 2017 05:38:21 GMT"}], "update_date": "2017-09-26", "authors_parsed": [["Wang", "Jiayun", ""], ["Zhou", "Sanping", ""], ["Wang", "Jinjun", ""], ["Hou", "Qiqi", ""]]}, {"id": "1707.00433", "submitter": "Lakshmanan Nataraj", "authors": "Jason Bunk, Jawadul H. Bappy, Tajuddin Manhar Mohammed, Lakshmanan\n  Nataraj, Arjuna Flenner, B.S. Manjunath, Shivkumar Chandrasekaran, Amit K.\n  Roy-Chowdhury, Lawrence Peterson", "title": "Detection and Localization of Image Forgeries using Resampling Features\n  and Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Resampling is an important signature of manipulated images. In this paper, we\npropose two methods to detect and localize image manipulations based on a\ncombination of resampling features and deep learning. In the first method, the\nRadon transform of resampling features are computed on overlapping image\npatches. Deep learning classifiers and a Gaussian conditional random field\nmodel are then used to create a heatmap. Tampered regions are located using a\nRandom Walker segmentation method. In the second method, resampling features\ncomputed on overlapping image patches are passed through a Long short-term\nmemory (LSTM) based network for classification and localization. We compare the\nperformance of detection/localization of both these methods. Our experimental\nresults show that both techniques are effective in detecting and localizing\ndigital image forgeries.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jul 2017 07:50:15 GMT"}], "update_date": "2017-07-04", "authors_parsed": [["Bunk", "Jason", ""], ["Bappy", "Jawadul H.", ""], ["Mohammed", "Tajuddin Manhar", ""], ["Nataraj", "Lakshmanan", ""], ["Flenner", "Arjuna", ""], ["Manjunath", "B. S.", ""], ["Chandrasekaran", "Shivkumar", ""], ["Roy-Chowdhury", "Amit K.", ""], ["Peterson", "Lawrence", ""]]}, {"id": "1707.00471", "submitter": "Eddy Ilg", "authors": "Osama Makansi, Eddy Ilg, and Thomas Brox", "title": "End-to-End Learning of Video Super-Resolution with Motion Compensation", "comments": "Accepted to GCPR2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning approaches have shown great success in the task of super-resolving\nan image given a low resolution input. Video super-resolution aims for\nexploiting additionally the information from multiple images. Typically, the\nimages are related via optical flow and consecutive image warping. In this\npaper, we provide an end-to-end video super-resolution network that, in\ncontrast to previous works, includes the estimation of optical flow in the\noverall network architecture. We analyze the usage of optical flow for video\nsuper-resolution and find that common off-the-shelf image warping does not\nallow video super-resolution to benefit much from optical flow. We rather\npropose an operation for motion compensation that performs warping from low to\nhigh resolution directly. We show that with this network configuration, video\nsuper-resolution can benefit from optical flow and we obtain state-of-the-art\nresults on the popular test sets. We also show that the processing of whole\nimages rather than independent patches is responsible for a large increase in\naccuracy.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jul 2017 10:16:10 GMT"}], "update_date": "2017-07-04", "authors_parsed": [["Makansi", "Osama", ""], ["Ilg", "Eddy", ""], ["Brox", "Thomas", ""]]}, {"id": "1707.00478", "submitter": "Lucas Fidon", "authors": "Lucas Fidon, Wenqi Li, Luis C. Garcia-Peraza-Herrera, Jinendra\n  Ekanayake, Neil Kitchen, Sebastien Ourselin, Tom Vercauteren", "title": "Generalised Wasserstein Dice Score for Imbalanced Multi-class\n  Segmentation using Holistic Convolutional Networks", "comments": "Accepted as an oral presentation at the MICCAI 2017 Brain Lesion\n  (BrainLes) Workshop", "journal-ref": null, "doi": "10.1007/978-3-319-75238-9_6", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Dice score is widely used for binary segmentation due to its robustness\nto class imbalance. Soft generalisations of the Dice score allow it to be used\nas a loss function for training convolutional neural networks (CNN). Although\nCNNs trained using mean-class Dice score achieve state-of-the-art results on\nmulti-class segmentation, this loss function does neither take advantage of\ninter-class relationships nor multi-scale information. We argue that an\nimproved loss function should balance misclassifications to favour predictions\nthat are semantically meaningful. This paper investigates these issues in the\ncontext of multi-class brain tumour segmentation. Our contribution is\nthreefold. 1) We propose a semantically-informed generalisation of the Dice\nscore for multi-class segmentation based on the Wasserstein distance on the\nprobabilistic label space. 2) We propose a holistic CNN that embeds spatial\ninformation at multiple scales with deep supervision. 3) We show that the joint\nuse of holistic CNNs and generalised Wasserstein Dice scores achieves\nsegmentations that are more semantically meaningful for brain tumour\nsegmentation.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jul 2017 10:53:03 GMT"}, {"version": "v2", "created": "Tue, 4 Jul 2017 11:38:46 GMT"}, {"version": "v3", "created": "Tue, 1 Aug 2017 18:07:49 GMT"}, {"version": "v4", "created": "Tue, 29 Aug 2017 12:26:16 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Fidon", "Lucas", ""], ["Li", "Wenqi", ""], ["Garcia-Peraza-Herrera", "Luis C.", ""], ["Ekanayake", "Jinendra", ""], ["Kitchen", "Neil", ""], ["Ourselin", "Sebastien", ""], ["Vercauteren", "Tom", ""]]}, {"id": "1707.00548", "submitter": "Chi Zhang", "authors": "Chi Zhang, Rui Yao, Jinpeng Cai", "title": "Efficient Eye Typing with 9-direction Gaze Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vision based text entry systems aim to help disabled people achieve text\ncommunication using eye movement. Most previous methods have employed an\nexisting eye tracker to predict gaze direction and design an input method based\nupon that. However, these methods can result in eye tracking quality becoming\neasily affected by various factors and lengthy amounts of time for calibration.\nOur paper presents a novel efficient gaze based text input method, which has\nthe advantage of low cost and robustness. Users can type in words by looking at\nan on-screen keyboard and blinking. Rather than estimate gaze angles directly\nto track eyes, we introduce a method that divides the human gaze into nine\ndirections. This method can effectively improve the accuracy of making a\nselection by gaze and blinks. We build a Convolutional Neural Network (CNN)\nmodel for 9-direction gaze estimation. On the basis of the 9-direction gaze, we\nuse a nine-key T9 input method which is widely used in candy bar phones. Bar\nphones were very popular in the world decades ago and have cultivated strong\nuser habits and language models. To train a robust gaze estimator, we created a\nlarge-scale dataset with images of eyes sourced from 25 people. According to\nthe results from our experiments, our CNN model is able to accurately estimate\ndifferent people's gaze under various lighting conditions by different devices.\nIn considering disable people's needs, we removed the complex calibration\nprocess. The input methods can run in screen mode and portable off-screen mode.\nMoreover, The datasets used in our experiments are made available to the\ncommunity to allow further experimentation.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jul 2017 13:51:13 GMT"}], "update_date": "2017-07-04", "authors_parsed": [["Zhang", "Chi", ""], ["Yao", "Rui", ""], ["Cai", "Jinpeng", ""]]}, {"id": "1707.00569", "submitter": "Roman Pflugfelder", "authors": "Roman Pflugfelder", "title": "An In-Depth Analysis of Visual Tracking with Siamese Neural Networks", "comments": "submitted to IEEE TPAMI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This survey presents a deep analysis of the learning and inference\ncapabilities in nine popular trackers. It is neither intended to study the\nwhole literature nor is it an attempt to review all kinds of neural networks\nproposed for visual tracking. We focus instead on Siamese neural networks which\nare a promising starting point for studying the challenging problem of\ntracking. These networks integrate efficiently feature learning and the\ntemporal matching and have so far shown state-of-the-art performance. In\nparticular, the branches of Siamese networks, their layers connecting these\nbranches, specific aspects of training and the embedding of these networks into\nthe tracker are highlighted. Quantitative results from existing papers are\ncompared with the conclusion that the current evaluation methodology shows\nproblems with the reproducibility and the comparability of results. The paper\nproposes a novel Lisp-like formalism for a better comparison of trackers. This\nassumes a certain functional design and functional decomposition of trackers.\nThe paper tries to give foundation for tracker design by a formulation of the\nproblem based on the theory of machine learning and by the interpretation of a\ntracker as a decision function. The work concludes with promising lines of\nresearch and suggests future work.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jul 2017 14:27:10 GMT"}, {"version": "v2", "created": "Thu, 2 Aug 2018 07:15:14 GMT"}], "update_date": "2018-08-03", "authors_parsed": [["Pflugfelder", "Roman", ""]]}, {"id": "1707.00587", "submitter": "Paul Jaeger", "authors": "Fabian Isensee, Paul Jaeger, Peter M. Full, Ivo Wolf, Sandy Engelhardt\n  and Klaus H. Maier-Hein", "title": "Automatic Cardiac Disease Assessment on cine-MRI via Time-Series\n  Segmentation and Domain Specific Features", "comments": "To appear in the STACOM 2017 proceedings", "journal-ref": null, "doi": "10.1007/978-3-319-75541-0", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cardiac magnetic resonance imaging improves on diagnosis of cardiovascular\ndiseases by providing images at high spatiotemporal resolution. Manual\nevaluation of these time-series, however, is expensive and prone to biased and\nnon-reproducible outcomes. In this paper, we present a method that addresses\nnamed limitations by integrating segmentation and disease classification into a\nfully automatic processing pipeline. We use an ensemble of UNet inspired\narchitectures for segmentation of cardiac structures such as the left and right\nventricular cavity (LVC, RVC) and the left ventricular myocardium (LVM) on each\ntime instance of the cardiac cycle. For the classification task, information is\nextracted from the segmented time-series in form of comprehensive features\nhandcrafted to reflect diagnostic clinical procedures. Based on these features\nwe train an ensemble of heavily regularized multilayer perceptrons (MLP) and a\nrandom forest classifier to predict the pathologic target class. We evaluated\nour method on the ACDC dataset (4 pathology groups, 1 healthy group) and\nachieve dice scores of 0.945 (LVC), 0.908 (RVC) and 0.905 (LVM) in a\ncross-validation over the training set (100 cases) and 0.950 (LVC), 0.923 (RVC)\nand 0.911 (LVM) on the test set (50 cases). We report a classification accuracy\nof 94% on a training set cross-validation and 92% on the test set. Our results\nunderpin the potential of machine learning methods for accurate, fast and\nreproducible segmentation and computer-assisted diagnosis (CAD).\n", "versions": [{"version": "v1", "created": "Mon, 3 Jul 2017 15:10:30 GMT"}, {"version": "v2", "created": "Thu, 25 Jan 2018 08:15:24 GMT"}], "update_date": "2018-04-06", "authors_parsed": [["Isensee", "Fabian", ""], ["Jaeger", "Paul", ""], ["Full", "Peter M.", ""], ["Wolf", "Ivo", ""], ["Engelhardt", "Sandy", ""], ["Maier-Hein", "Klaus H.", ""]]}, {"id": "1707.00600", "submitter": "Yongqin Xian", "authors": "Yongqin Xian, Christoph H. Lampert, Bernt Schiele, Zeynep Akata", "title": "Zero-Shot Learning -- A Comprehensive Evaluation of the Good, the Bad\n  and the Ugly", "comments": "Accepted by TPAMI in July, 2018. We introduce Proposed Split Version\n  2.0 (Please download it from our project webpage). arXiv admin note:\n  substantial text overlap with arXiv:1703.04394", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the importance of zero-shot learning, i.e. classifying images where\nthere is a lack of labeled training data, the number of proposed approaches has\nrecently increased steadily. We argue that it is time to take a step back and\nto analyze the status quo of the area. The purpose of this paper is three-fold.\nFirst, given the fact that there is no agreed upon zero-shot learning\nbenchmark, we first define a new benchmark by unifying both the evaluation\nprotocols and data splits of publicly available datasets used for this task.\nThis is an important contribution as published results are often not comparable\nand sometimes even flawed due to, e.g. pre-training on zero-shot test classes.\nMoreover, we propose a new zero-shot learning dataset, the Animals with\nAttributes 2 (AWA2) dataset which we make publicly available both in terms of\nimage features and the images themselves. Second, we compare and analyze a\nsignificant number of the state-of-the-art methods in depth, both in the\nclassic zero-shot setting but also in the more realistic generalized zero-shot\nsetting. Finally, we discuss in detail the limitations of the current status of\nthe area which can be taken as a basis for advancing it.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jul 2017 15:41:41 GMT"}, {"version": "v2", "created": "Thu, 12 Apr 2018 12:36:12 GMT"}, {"version": "v3", "created": "Thu, 9 Aug 2018 08:47:34 GMT"}, {"version": "v4", "created": "Wed, 23 Sep 2020 15:02:08 GMT"}], "update_date": "2020-09-25", "authors_parsed": [["Xian", "Yongqin", ""], ["Lampert", "Christoph H.", ""], ["Schiele", "Bernt", ""], ["Akata", "Zeynep", ""]]}, {"id": "1707.00606", "submitter": "Stepan Tulyakov", "authors": "Stepan Tulyakov, Anton Ivanov, Nicolas Thomas, Victoria Roloff,\n  Antoine Pommerol, Gabriele Cremonese, Thomas Weigel, Francois Fleuret", "title": "Geometric calibration of Colour and Stereo Surface Imaging System of\n  ESA's Trace Gas Orbiter", "comments": "Submitted to Advances in Space Research", "journal-ref": null, "doi": "10.1016/j.asr.2017.10.025", "report-no": null, "categories": "astro-ph.IM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are many geometric calibration methods for \"standard\" cameras. These\nmethods, however, cannot be used for the calibration of telescopes with large\nfocal lengths and complex off-axis optics. Moreover, specialized calibration\nmethods for the telescopes are scarce in literature. We describe the\ncalibration method that we developed for the Colour and Stereo Surface Imaging\nSystem (CaSSIS) telescope, on board of the ExoMars Trace Gas Orbiter (TGO).\nAlthough our method is described in the context of CaSSIS, with camera-specific\nexperiments, it is general and can be applied to other telescopes. We further\nencourage re-use of the proposed method by making our calibration code and data\navailable on-line.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jul 2017 15:45:21 GMT"}], "update_date": "2017-12-27", "authors_parsed": [["Tulyakov", "Stepan", ""], ["Ivanov", "Anton", ""], ["Thomas", "Nicolas", ""], ["Roloff", "Victoria", ""], ["Pommerol", "Antoine", ""], ["Cremonese", "Gabriele", ""], ["Weigel", "Thomas", ""], ["Fleuret", "Francois", ""]]}, {"id": "1707.00652", "submitter": "Guotai Wang", "authors": "Guotai Wang, Maria A. Zuluaga, Wenqi Li, Rosalind Pratt, Premal A.\n  Patel, Michael Aertsen, Tom Doel, Anna L. David, Jan Deprest, Sebastien\n  Ourselin and Tom Vercauteren", "title": "DeepIGeoS: A Deep Interactive Geodesic Framework for Medical Image\n  Segmentation", "comments": "14 pages, 15 figures", "journal-ref": null, "doi": "10.1109/TPAMI.2018.2840695", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate medical image segmentation is essential for diagnosis, surgical\nplanning and many other applications. Convolutional Neural Networks (CNNs) have\nbecome the state-of-the-art automatic segmentation methods. However, fully\nautomatic results may still need to be refined to become accurate and robust\nenough for clinical use. We propose a deep learning-based interactive\nsegmentation method to improve the results obtained by an automatic CNN and to\nreduce user interactions during refinement for higher accuracy. We use one CNN\nto obtain an initial automatic segmentation, on which user interactions are\nadded to indicate mis-segmentations. Another CNN takes as input the user\ninteractions with the initial segmentation and gives a refined result. We\npropose to combine user interactions with CNNs through geodesic distance\ntransforms, and propose a resolution-preserving network that gives a better\ndense prediction. In addition, we integrate user interactions as hard\nconstraints into a back-propagatable Conditional Random Field. We validated the\nproposed framework in the context of 2D placenta segmentation from fetal MRI\nand 3D brain tumor segmentation from FLAIR images. Experimental results show\nour method achieves a large improvement from automatic CNNs, and obtains\ncomparable and even higher accuracy with fewer user interventions and less time\ncompared with traditional interactive methods.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jul 2017 17:04:54 GMT"}, {"version": "v2", "created": "Tue, 4 Jul 2017 16:36:47 GMT"}, {"version": "v3", "created": "Tue, 19 Sep 2017 08:48:14 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Wang", "Guotai", ""], ["Zuluaga", "Maria A.", ""], ["Li", "Wenqi", ""], ["Pratt", "Rosalind", ""], ["Patel", "Premal A.", ""], ["Aertsen", "Michael", ""], ["Doel", "Tom", ""], ["David", "Anna L.", ""], ["Deprest", "Jan", ""], ["Ourselin", "Sebastien", ""], ["Vercauteren", "Tom", ""]]}, {"id": "1707.00665", "submitter": "Weilin Huang", "authors": "Weilin Huang, Christopher P. Bridge, J. Alison Noble, and Andrew\n  Zisserman", "title": "Temporal HeartNet: Towards Human-Level Automatic Analysis of Fetal\n  Cardiac Screening Video", "comments": "To appear in MICCAI, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an automatic method to describe clinically useful information\nabout scanning, and to guide image interpretation in ultrasound (US) videos of\nthe fetal heart. Our method is able to jointly predict the visibility, viewing\nplane, location and orientation of the fetal heart at the frame level. The\ncontributions of the paper are three-fold: (i) a convolutional neural network\narchitecture is developed for a multi-task prediction, which is computed by\nsliding a 3x3 window spatially through convolutional maps. (ii) an anchor\nmechanism and Intersection over Union (IoU) loss are applied for improving\nlocalization accuracy. (iii) a recurrent architecture is designed to\nrecursively compute regional convolutional features temporally over sequential\nframes, allowing each prediction to be conditioned on the whole video. This\nresults in a spatial-temporal model that precisely describes detailed heart\nparameters in challenging US videos. We report results on a real-world clinical\ndataset, where our method achieves performance on par with expert annotations.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jul 2017 17:31:16 GMT"}], "update_date": "2017-07-04", "authors_parsed": [["Huang", "Weilin", ""], ["Bridge", "Christopher P.", ""], ["Noble", "J. Alison", ""], ["Zisserman", "Andrew", ""]]}, {"id": "1707.00683", "submitter": "Florian Strub", "authors": "Harm de Vries, Florian Strub, J\\'er\\'emie Mary, Hugo Larochelle,\n  Olivier Pietquin, Aaron Courville", "title": "Modulating early visual processing by language", "comments": "Advances in Neural Information Processing Systems 30 (NIPS 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is commonly assumed that language refers to high-level visual concepts\nwhile leaving low-level visual processing unaffected. This view dominates the\ncurrent literature in computational models for language-vision tasks, where\nvisual and linguistic input are mostly processed independently before being\nfused into a single representation. In this paper, we deviate from this classic\npipeline and propose to modulate the \\emph{entire visual processing} by\nlinguistic input. Specifically, we condition the batch normalization parameters\nof a pretrained residual network (ResNet) on a language embedding. This\napproach, which we call MOdulated RESnet (\\MRN), significantly improves strong\nbaselines on two visual question answering tasks. Our ablation study shows that\nmodulating from the early stages of the visual processing is beneficial.\n", "versions": [{"version": "v1", "created": "Sun, 2 Jul 2017 04:06:01 GMT"}, {"version": "v2", "created": "Mon, 27 Nov 2017 02:58:44 GMT"}, {"version": "v3", "created": "Mon, 18 Dec 2017 20:04:53 GMT"}], "update_date": "2017-12-20", "authors_parsed": [["de Vries", "Harm", ""], ["Strub", "Florian", ""], ["Mary", "J\u00e9r\u00e9mie", ""], ["Larochelle", "Hugo", ""], ["Pietquin", "Olivier", ""], ["Courville", "Aaron", ""]]}, {"id": "1707.00684", "submitter": "Tomoyoshi Shimobaba Dr.", "authors": "Tomoyoshi Shimobaba, Naoki Kuwata, Mizuha Homma, Takayuki Takahashi,\n  Yuki Nagahama, Marie Sano, Satoki Hasegawa, Ryuji Hirayama, Takashi Kakue,\n  Atsushi Shiraki, Naoki Takada, Tomoyoshi Ito", "title": "Deep-learning-based data page classification for holographic memory", "comments": null, "journal-ref": null, "doi": "10.1364/ao.56.007327", "report-no": null, "categories": "cs.CV cs.LG cs.NE physics.optics", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a deep-learning-based classification of data pages used in\nholographic memory. We numerically investigated the classification performance\nof a conventional multi-layer perceptron (MLP) and a deep neural network, under\nthe condition that reconstructed page data are contaminated by some noise and\nare randomly laterally shifted. The MLP was found to have a classification\naccuracy of 91.58%, whereas the deep neural network was able to classify data\npages at an accuracy of 99.98%. The accuracy of the deep neural network is two\norders of magnitude better than the MLP.\n", "versions": [{"version": "v1", "created": "Sun, 2 Jul 2017 05:47:37 GMT"}], "update_date": "2018-05-23", "authors_parsed": [["Shimobaba", "Tomoyoshi", ""], ["Kuwata", "Naoki", ""], ["Homma", "Mizuha", ""], ["Takahashi", "Takayuki", ""], ["Nagahama", "Yuki", ""], ["Sano", "Marie", ""], ["Hasegawa", "Satoki", ""], ["Hirayama", "Ryuji", ""], ["Kakue", "Takashi", ""], ["Shiraki", "Atsushi", ""], ["Takada", "Naoki", ""], ["Ito", "Tomoyoshi", ""]]}, {"id": "1707.00737", "submitter": "Bin Huang", "authors": "Huang Bin, Chen Weihai, Wu Xingming and Lin Chun-Liang", "title": "High-Quality Face Image SR Using Conditional Generative Adversarial\n  Networks", "comments": "9 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We propose a novel single face image super-resolution method, which named\nFace Conditional Generative Adversarial Network(FCGAN), based on boundary\nequilibrium generative adversarial networks. Without taking any facial prior\ninformation, our method can generate a high-resolution face image from a\nlow-resolution one. Compared with existing studies, both our training and\ntesting phases are end-to-end pipeline with little pre/post-processing. To\nenhance the convergence speed and strengthen feature propagation, skip-layer\nconnection is further employed in the generative and discriminative networks.\nExtensive experiments demonstrate that our model achieves competitive\nperformance compared with state-of-the-art models.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jul 2017 03:39:43 GMT"}], "update_date": "2017-07-05", "authors_parsed": [["Bin", "Huang", ""], ["Weihai", "Chen", ""], ["Xingming", "Wu", ""], ["Chun-Liang", "Lin", ""]]}, {"id": "1707.00755", "submitter": "Tolga Tasdizen", "authors": "Tolga Tasdizen, Mehdi Sajjadi, Mehran Javanmardi, Nisha Ramesh", "title": "Appearance invariance in convolutional networks with neighborhood\n  similarity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a neighborhood similarity layer (NSL) which induces appearance\ninvariance in a network when used in conjunction with convolutional layers. We\nare motivated by the observation that, even though convolutional networks have\nlow generalization error, their generalization capability does not extend to\nsamples which are not represented by the training data. For instance, while\nnovel appearances of learned concepts pose no problem for the human visual\nsystem, feedforward convolutional networks are generally not successful in such\nsituations. Motivated by the Gestalt principle of grouping with respect to\nsimilarity, the proposed NSL transforms its input feature map using the feature\nvectors at each pixel as a frame of reference, i.e. center of attention, for\nits surrounding neighborhood. This transformation is spatially varying, hence\nnot a convolution. It is differentiable; therefore, networks including the\nproposed layer can be trained in an end-to-end manner. We analyze the\ninvariance of NSL to significant changes in appearance that are not represented\nin the training data. We also demonstrate its advantages for digit recognition,\nsemantic labeling and cell detection problems.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jul 2017 20:53:56 GMT"}], "update_date": "2017-07-05", "authors_parsed": [["Tasdizen", "Tolga", ""], ["Sajjadi", "Mehdi", ""], ["Javanmardi", "Mehran", ""], ["Ramesh", "Nisha", ""]]}, {"id": "1707.00785", "submitter": "Zhiwu Lu", "authors": "Aoxue Li, Zhiwu Lu, Liwei Wang, Tao Xiang, Xinqi Li, and Ji-Rong Wen", "title": "Zero-Shot Fine-Grained Classification by Deep Feature Learning with\n  Semantics", "comments": "This paper has been submitted to IEEE TIP for peer-review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fine-grained image classification, which aims to distinguish images with\nsubtle distinctions, is a challenging task due to two main issues: lack of\nsufficient training data for every class and difficulty in learning\ndiscriminative features for representation. In this paper, to address the two\nissues, we propose a two-phase framework for recognizing images from unseen\nfine-grained classes, i.e. zero-shot fine-grained classification. In the first\nfeature learning phase, we finetune deep convolutional neural networks using\nhierarchical semantic structure among fine-grained classes to extract\ndiscriminative deep visual features. Meanwhile, a domain adaptation structure\nis induced into deep convolutional neural networks to avoid domain shift from\ntraining data to test data. In the second label inference phase, a semantic\ndirected graph is constructed over attributes of fine-grained classes. Based on\nthis graph, we develop a label propagation algorithm to infer the labels of\nimages in the unseen classes. Experimental results on two benchmark datasets\ndemonstrate that our model outperforms the state-of-the-art zero-shot learning\nmodels. In addition, the features obtained by our feature learning model also\nyield significant gains when they are used by other zero-shot learning models,\nwhich shows the flexility of our model in zero-shot fine-grained\nclassification.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jul 2017 00:18:32 GMT"}], "update_date": "2017-07-05", "authors_parsed": [["Li", "Aoxue", ""], ["Lu", "Zhiwu", ""], ["Wang", "Liwei", ""], ["Xiang", "Tao", ""], ["Li", "Xinqi", ""], ["Wen", "Ji-Rong", ""]]}, {"id": "1707.00798", "submitter": "Hantao Yao", "authors": "Hantao Yao, Shiliang Zhang, Yongdong Zhang, Jintao Li, Qi Tian", "title": "Deep Representation Learning with Part Loss for Person Re-Identification", "comments": "9 pages, 9 figures", "journal-ref": null, "doi": "10.1109/TIP.2019.2891888", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning discriminative representations for unseen person images is critical\nfor person Re-Identification (ReID). Most of current approaches learn deep\nrepresentations in classification tasks, which essentially minimize the\nempirical classification risk on the training set. As shown in our experiments,\nsuch representations commonly focus on several body parts discriminative to the\ntraining set, rather than the entire human body. Inspired by the structural\nrisk minimization principle in SVM, we revise the traditional deep\nrepresentation learning procedure to minimize both the empirical classification\nrisk and the representation learning risk. The representation learning risk is\nevaluated by the proposed part loss, which automatically generates several\nparts for an image, and computes the person classification loss on each part\nseparately. Compared with traditional global classification loss,\nsimultaneously considering multiple part loss enforces the deep network to\nfocus on the entire human body and learn discriminative representations for\ndifferent parts. Experimental results on three datasets, i.e., Market1501,\nCUHK03, VIPeR, show that our representation outperforms the existing deep\nrepresentations.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jul 2017 02:07:00 GMT"}, {"version": "v2", "created": "Thu, 16 Nov 2017 13:00:23 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["Yao", "Hantao", ""], ["Zhang", "Shiliang", ""], ["Zhang", "Yongdong", ""], ["Li", "Jintao", ""], ["Tian", "Qi", ""]]}, {"id": "1707.00800", "submitter": "Mahmoud Mousa", "authors": "Mahmoud A. A. Mousa, Mohammed S. Sayed and Mahmoud I. Abdalla", "title": "Arabic Character Segmentation Using Projection Based Approach with\n  Profile's Amplitude Filter", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Arabic is one of the languages that present special challenges to Optical\ncharacter recognition (OCR). The main challenge in Arabic is that it is mostly\ncursive. Therefore, a segmentation process must be carried out to determine\nwhere the character begins and where it ends. This step is essential for\ncharacter recognition. This paper presents Arabic character segmentation\nalgorithm. The proposed algorithm uses the projection-based approach concepts\nto separate lines, words, and characters. This is done using profile's\namplitude filter and simple edge tool to find characters separations. Our\nalgorithm shows promising performance when applied on different machine printed\ndocuments with different Arabic fonts.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jul 2017 02:29:07 GMT"}], "update_date": "2017-07-05", "authors_parsed": [["Mousa", "Mahmoud A. A.", ""], ["Sayed", "Mohammed S.", ""], ["Abdalla", "Mahmoud I.", ""]]}, {"id": "1707.00803", "submitter": "Zuxuan Wu", "authors": "Shaoxiang Chen, Xi Wang, Yongyi Tang, Xinpeng Chen, Zuxuan Wu, Yu-Gang\n  Jiang", "title": "Aggregating Frame-level Features for Large-Scale Video Classification", "comments": "Youtube-8M Challenge, 4th place", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces the system we developed for the Google Cloud &\nYouTube-8M Video Understanding Challenge, which can be considered as a\nmulti-label classification problem defined on top of the large scale YouTube-8M\nDataset. We employ a large set of techniques to aggregate the provided\nframe-level feature representations and generate video-level predictions,\nincluding several variants of recurrent neural networks (RNN) and generalized\nVLAD. We also adopt several fusion strategies to explore the complementarity\namong the models. In terms of the official metric GAP@20 (global average\nprecision at 20), our best fusion model attains 0.84198 on the public 50\\% of\ntest data and 0.84193 on the private 50\\% of test data, ranking 4th out of 650\nteams worldwide in the competition.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jul 2017 02:55:48 GMT"}], "update_date": "2017-07-05", "authors_parsed": [["Chen", "Shaoxiang", ""], ["Wang", "Xi", ""], ["Tang", "Yongyi", ""], ["Chen", "Xinpeng", ""], ["Wu", "Zuxuan", ""], ["Jiang", "Yu-Gang", ""]]}, {"id": "1707.00809", "submitter": "Tuan N.A. Hoang", "authors": "Tuan Hoang, Thanh-Toan Do, Dang-Khoa Le Tan and Ngai-Man Cheung", "title": "Selective Deep Convolutional Features for Image Retrieval", "comments": "Accepted to ACM MM 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Network (CNN) is a very powerful approach to extract\ndiscriminative local descriptors for effective image search. Recent work adopts\nfine-tuned strategies to further improve the discriminative power of the\ndescriptors. Taking a different approach, in this paper, we propose a novel\nframework to achieve competitive retrieval performance. Firstly, we propose\nvarious masking schemes, namely SIFT-mask, SUM-mask, and MAX-mask, to select a\nrepresentative subset of local convolutional features and remove a large number\nof redundant features. We demonstrate that this can effectively address the\nburstiness issue and improve retrieval accuracy. Secondly, we propose to employ\nrecent embedding and aggregating methods to further enhance feature\ndiscriminability. Extensive experiments demonstrate that our proposed framework\nachieves state-of-the-art retrieval accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jul 2017 03:38:15 GMT"}, {"version": "v2", "created": "Mon, 27 Nov 2017 06:59:23 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Hoang", "Tuan", ""], ["Do", "Thanh-Toan", ""], ["Tan", "Dang-Khoa Le", ""], ["Cheung", "Ngai-Man", ""]]}, {"id": "1707.00811", "submitter": "Hantao Yao", "authors": "Hantao Yao, Shiliang Zhang, Yongdong Zhang, Jintao Li, Qi Tian", "title": "One-Shot Fine-Grained Instance Retrieval", "comments": "Accepted by MM2017, 9 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fine-Grained Visual Categorization (FGVC) has achieved significant progress\nrecently. However, the number of fine-grained species could be huge and\ndynamically increasing in real scenarios, making it difficult to recognize\nunseen objects under the current FGVC framework. This raises an open issue to\nperform large-scale fine-grained identification without a complete training\nset. Aiming to conquer this issue, we propose a retrieval task named One-Shot\nFine-Grained Instance Retrieval (OSFGIR). \"One-Shot\" denotes the ability of\nidentifying unseen objects through a fine-grained retrieval task assisted with\nan incomplete auxiliary training set. This paper first presents the detailed\ndescription to OSFGIR task and our collected OSFGIR-378K dataset. Next, we\npropose the Convolutional and Normalization Networks (CN-Nets) learned on the\nauxiliary dataset to generate a concise and discriminative representation.\nFinally, we present a coarse-to-fine retrieval framework consisting of three\ncomponents, i.e., coarse retrieval, fine-grained retrieval, and query\nexpansion, respectively. The framework progressively retrieves images with\nsimilar semantics, and performs fine-grained identification. Experiments show\nour OSFGIR framework achieves significantly better accuracy and efficiency than\nexisting FGVC and image retrieval methods, thus could be a better solution for\nlarge-scale fine-grained object identification.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jul 2017 03:51:32 GMT"}], "update_date": "2017-07-05", "authors_parsed": [["Yao", "Hantao", ""], ["Zhang", "Shiliang", ""], ["Zhang", "Yongdong", ""], ["Li", "Jintao", ""], ["Tian", "Qi", ""]]}, {"id": "1707.00815", "submitter": "Bahadir Gunturk", "authors": "M. Shahzeb Khan Gul and Bahadir K. Gunturk", "title": "Spatial and Angular Resolution Enhancement of Light Fields Using\n  Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2018.2794181", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Light field imaging extends the traditional photography by capturing both\nspatial and angular distribution of light, which enables new capabilities,\nincluding post-capture refocusing, post-capture aperture control, and depth\nestimation from a single shot. Micro-lens array (MLA) based light field cameras\noffer a cost-effective approach to capture light field. A major drawback of MLA\nbased light field cameras is low spatial resolution, which is due to the fact\nthat a single image sensor is shared to capture both spatial and angular\ninformation. In this paper, we present a learning based light field enhancement\napproach. Both spatial and angular resolution of captured light field is\nenhanced using convolutional neural networks. The proposed method is tested\nwith real light field data captured with a Lytro light field camera, clearly\ndemonstrating spatial and angular resolution improvement.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jul 2017 04:33:32 GMT"}, {"version": "v2", "created": "Tue, 13 Feb 2018 13:53:43 GMT"}], "update_date": "2018-03-14", "authors_parsed": [["Gul", "M. Shahzeb Khan", ""], ["Gunturk", "Bahadir K.", ""]]}, {"id": "1707.00823", "submitter": "Jian Liu", "authors": "Jian Liu, Naveed Akhtar, Ajmal Mian", "title": "Learning Human Pose Models from Synthesized Data for Robust RGB-D Action\n  Recognition", "comments": "Revision submitted to IJCV", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose Human Pose Models that represent RGB and depth images of human\nposes independent of clothing textures, backgrounds, lighting conditions, body\nshapes and camera viewpoints. Learning such universal models requires training\nimages where all factors are varied for every human pose. Capturing such data\nis prohibitively expensive. Therefore, we develop a framework for synthesizing\nthe training data. First, we learn representative human poses from a large\ncorpus of real motion captured human skeleton data. Next, we fit synthetic 3D\nhumans with different body shapes to each pose and render each from 180 camera\nviewpoints while randomly varying the clothing textures, background and\nlighting. Generative Adversarial Networks are employed to minimize the gap\nbetween synthetic and real image distributions. CNN models are then learned\nthat transfer human poses to a shared high-level invariant space. The learned\nCNN models are then used as invariant feature extractors from real RGB and\ndepth frames of human action videos and the temporal variations are modelled by\nFourier Temporal Pyramid. Finally, linear SVM is used for classification.\nExperiments on three benchmark cross-view human action datasets show that our\nalgorithm outperforms existing methods by significant margins for RGB only and\nRGB-D action recognition.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jul 2017 06:18:34 GMT"}, {"version": "v2", "created": "Tue, 1 May 2018 06:24:35 GMT"}], "update_date": "2018-05-02", "authors_parsed": [["Liu", "Jian", ""], ["Akhtar", "Naveed", ""], ["Mian", "Ajmal", ""]]}, {"id": "1707.00835", "submitter": "Johannes Reschke", "authors": "Johannes Reschke, Armin Sehr", "title": "Face Recognition with Machine Learning in OpenCV_ Fusion of the results\n  with the Localization Data of an Acoustic Camera for Speaker Identification", "comments": "Applied Research Conference 2017 (Munich)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This contribution gives an overview of face recogni-tion algorithms, their\nimplementation and practical uses. First, a training set of different persons'\nfaces has to be collected and used to train a face recognizer. The resulting\nface model can be utilized to classify people in specific individuals or\nunknowns. After tracking the recognized face and estimating the acoustic sound\nsource's position, both can be combined to give detailed information about\npossible speakers and if they are talking or not. This leads to a precise\nreal-time description of the situation, which can be used for further\napplications, e.g. for multi-channel speech enhancement by adaptive\nbeamformers.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jul 2017 07:42:00 GMT"}], "update_date": "2017-07-05", "authors_parsed": [["Reschke", "Johannes", ""], ["Sehr", "Armin", ""]]}, {"id": "1707.00836", "submitter": "Kyungmin Kim", "authors": "Kyung-Min Kim, Min-Oh Heo, Seong-Ho Choi, and Byoung-Tak Zhang", "title": "DeepStory: Video Story QA by Deep Embedded Memory Networks", "comments": "7 pages, accepted for IJCAI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Question-answering (QA) on video contents is a significant challenge for\nachieving human-level intelligence as it involves both vision and language in\nreal-world settings. Here we demonstrate the possibility of an AI agent\nperforming video story QA by learning from a large amount of cartoon videos. We\ndevelop a video-story learning model, i.e. Deep Embedded Memory Networks\n(DEMN), to reconstruct stories from a joint scene-dialogue video stream using a\nlatent embedding space of observed data. The video stories are stored in a\nlong-term memory component. For a given question, an LSTM-based attention model\nuses the long-term memory to recall the best question-story-answer triplet by\nfocusing on specific words containing key information. We trained the DEMN on a\nnovel QA dataset of children's cartoon video series, Pororo. The dataset\ncontains 16,066 scene-dialogue pairs of 20.5-hour videos, 27,328 fine-grained\nsentences for scene description, and 8,913 story-related QA pairs. Our\nexperimental results show that the DEMN outperforms other QA models. This is\nmainly due to 1) the reconstruction of video stories in a scene-dialogue\ncombined form that utilize the latent embedding and 2) attention. DEMN also\nachieved state-of-the-art results on the MovieQA benchmark.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jul 2017 07:42:05 GMT"}], "update_date": "2017-07-05", "authors_parsed": [["Kim", "Kyung-Min", ""], ["Heo", "Min-Oh", ""], ["Choi", "Seong-Ho", ""], ["Zhang", "Byoung-Tak", ""]]}, {"id": "1707.00860", "submitter": "Sakyasingha Dasgupta", "authors": "Subhajit Chaudhury, Sakyasingha Dasgupta, Asim Munawar, Md. A. Salam\n  Khan, Ryuki Tachibana", "title": "Conditional generation of multi-modal data using constrained embedding\n  space mapping", "comments": "7 pages, 4 figures, ICML 2017 Workshop on Implicit Models", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a conditional generative model that maps low-dimensional\nembeddings of multiple modalities of data to a common latent space hence\nextracting semantic relationships between them. The embedding specific to a\nmodality is first extracted and subsequently a constrained optimization\nprocedure is performed to project the two embedding spaces to a common\nmanifold. The individual embeddings are generated back from this common latent\nspace. However, in order to enable independent conditional inference for\nseparately extracting the corresponding embeddings from the common latent space\nrepresentation, we deploy a proxy variable trick - wherein, the single shared\nlatent space is replaced by the respective separate latent spaces of each\nmodality. We design an objective function, such that, during training we can\nforce these separate spaces to lie close to each other, by minimizing the\ndistance between their probability distribution functions. Experimental results\ndemonstrate that the learned joint model can generalize to learning concepts of\ndouble MNIST digits with additional attributes of colors,from both textual and\nspeech input.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jul 2017 09:00:38 GMT"}, {"version": "v2", "created": "Wed, 26 Jul 2017 00:51:04 GMT"}], "update_date": "2017-07-27", "authors_parsed": [["Chaudhury", "Subhajit", ""], ["Dasgupta", "Sakyasingha", ""], ["Munawar", "Asim", ""], ["Khan", "Md. A. Salam", ""], ["Tachibana", "Ryuki", ""]]}, {"id": "1707.00893", "submitter": "Philipp Jund", "authors": "Philipp Jund, Andreas Eitel, Nichola Abdo, Wolfram Burgard", "title": "Optimization Beyond the Convolution: Generalizing Spatial Relations with\n  End-to-End Metric Learning", "comments": "Accepted for publication at ICRA2018. Supplementary Video:\n  http://spatialrelations.cs.uni-freiburg.de/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To operate intelligently in domestic environments, robots require the ability\nto understand arbitrary spatial relations between objects and to generalize\nthem to objects of varying sizes and shapes. In this work, we present a novel\nend-to-end approach to generalize spatial relations based on distance metric\nlearning. We train a neural network to transform 3D point clouds of objects to\na metric space that captures the similarity of the depicted spatial relations,\nusing only geometric models of the objects. Our approach employs gradient-based\noptimization to compute object poses in order to imitate an arbitrary target\nrelation by reducing the distance to it under the learned metric. Our results\nbased on simulated and real-world experiments show that the proposed method\nenables robots to generalize spatial relations to unknown objects over a\ncontinuous spectrum.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jul 2017 10:19:34 GMT"}, {"version": "v2", "created": "Wed, 20 Sep 2017 09:30:21 GMT"}, {"version": "v3", "created": "Sat, 24 Mar 2018 14:13:14 GMT"}, {"version": "v4", "created": "Mon, 1 Jul 2019 13:44:40 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Jund", "Philipp", ""], ["Eitel", "Andreas", ""], ["Abdo", "Nichola", ""], ["Burgard", "Wolfram", ""]]}, {"id": "1707.00907", "submitter": "Jan Funke", "authors": "Jan Funke, Chong Zhang, Tobias Pietzsch, Stephan Saalfeld", "title": "The Candidate Multi-Cut for Cell Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two successful approaches for the segmentation of biomedical images are (1)\nthe selection of segment candidates from a merge-tree, and (2) the clustering\nof small superpixels by solving a Multi-Cut problem. In this paper, we\nintroduce a model that unifies both approaches. Our model, the Candidate\nMulti-Cut (CMC), allows joint selection and clustering of segment candidates\nfrom a merge-tree. This way, we overcome the respective limitations of the\nindividual methods: (1) the space of possible segmentations is not constrained\nto candidates of a merge-tree, and (2) the decision for clustering can be made\non candidates larger than superpixels, using features over larger contexts. We\nsolve the optimization problem of selecting and clustering of candidates using\nan integer linear program. On datasets of 2D light microscopy of cell\npopulations and 3D electron microscopy of neurons, we show that our method\ngeneralizes well and generates more accurate segmentations than merge-tree or\nMulti-Cut methods alone.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jul 2017 11:05:40 GMT"}], "update_date": "2017-07-05", "authors_parsed": [["Funke", "Jan", ""], ["Zhang", "Chong", ""], ["Pietzsch", "Tobias", ""], ["Saalfeld", "Stephan", ""]]}, {"id": "1707.01018", "submitter": "Yvain Qu\\'eau", "authors": "Yvain Qu\\'eau, Bastien Durix, Tao Wu, Daniel Cremers, Fran\\c{c}ois\n  Lauze and Jean-Denis Durou", "title": "LED-based Photometric Stereo: Modeling, Calibration and Numerical\n  Solution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We conduct a thorough study of photometric stereo under nearby point light\nsource illumination, from modeling to numerical solution, through calibration.\nIn the classical formulation of photometric stereo, the luminous fluxes are\nassumed to be directional, which is very difficult to achieve in practice.\nRather, we use light-emitting diodes (LEDs) to illuminate the scene to\nreconstruct. Such point light sources are very convenient to use, yet they\nyield a more complex photometric stereo model which is arduous to solve. We\nfirst derive in a physically sound manner this model, and show how to calibrate\nits parameters. Then, we discuss two state-of-the-art numerical solutions. The\nfirst one alternatingly estimates the albedo and the normals, and then\nintegrates the normals into a depth map. It is shown empirically to be\nindependent from the initialization, but convergence of this sequential\napproach is not established. The second one directly recovers the depth, by\nformulating photometric stereo as a system of PDEs which are partially\nlinearized using image ratios. Although the sequential approach is avoided,\ninitialization matters a lot and convergence is not established either.\nTherefore, we introduce a provably convergent alternating reweighted\nleast-squares scheme for solving the original system of PDEs, without resorting\nto image ratios for linearization. Finally, we extend this study to the case of\nRGB images.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jul 2017 14:39:27 GMT"}, {"version": "v2", "created": "Mon, 4 Sep 2017 17:38:30 GMT"}], "update_date": "2017-09-05", "authors_parsed": [["Qu\u00e9au", "Yvain", ""], ["Durix", "Bastien", ""], ["Wu", "Tao", ""], ["Cremers", "Daniel", ""], ["Lauze", "Fran\u00e7ois", ""], ["Durou", "Jean-Denis", ""]]}, {"id": "1707.01058", "submitter": "Yichao Yan", "authors": "Yichao Yan, Jingwei Xu, Bingbing Ni, Xiaokang Yang", "title": "Skeleton-aided Articulated Motion Generation", "comments": "ACM MM 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work make the first attempt to generate articulated human motion\nsequence from a single image. On the one hand, we utilize paired inputs\nincluding human skeleton information as motion embedding and a single human\nimage as appearance reference, to generate novel motion frames, based on the\nconditional GAN infrastructure. On the other hand, a triplet loss is employed\nto pursue appearance-smoothness between consecutive frames. As the proposed\nframework is capable of jointly exploiting the image appearance space and\narticulated/kinematic motion space, it generates realistic articulated motion\nsequence, in contrast to most previous video generation methods which yield\nblurred motion effects. We test our model on two human action datasets\nincluding KTH and Human3.6M, and the proposed framework generates very\npromising results on both datasets.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jul 2017 16:25:23 GMT"}, {"version": "v2", "created": "Thu, 14 Sep 2017 09:06:02 GMT"}], "update_date": "2017-09-15", "authors_parsed": [["Yan", "Yichao", ""], ["Xu", "Jingwei", ""], ["Ni", "Bingbing", ""], ["Yang", "Xiaokang", ""]]}, {"id": "1707.01083", "submitter": "Xiangyu Zhang", "authors": "Xiangyu Zhang, Xinyu Zhou, Mengxiao Lin, Jian Sun", "title": "ShuffleNet: An Extremely Efficient Convolutional Neural Network for\n  Mobile Devices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce an extremely computation-efficient CNN architecture named\nShuffleNet, which is designed specially for mobile devices with very limited\ncomputing power (e.g., 10-150 MFLOPs). The new architecture utilizes two new\noperations, pointwise group convolution and channel shuffle, to greatly reduce\ncomputation cost while maintaining accuracy. Experiments on ImageNet\nclassification and MS COCO object detection demonstrate the superior\nperformance of ShuffleNet over other structures, e.g. lower top-1 error\n(absolute 7.8%) than recent MobileNet on ImageNet classification task, under\nthe computation budget of 40 MFLOPs. On an ARM-based mobile device, ShuffleNet\nachieves ~13x actual speedup over AlexNet while maintaining comparable\naccuracy.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jul 2017 17:42:58 GMT"}, {"version": "v2", "created": "Thu, 7 Dec 2017 18:06:34 GMT"}], "update_date": "2017-12-08", "authors_parsed": [["Zhang", "Xiangyu", ""], ["Zhou", "Xinyu", ""], ["Lin", "Mengxiao", ""], ["Sun", "Jian", ""]]}, {"id": "1707.01086", "submitter": "Jie Yang", "authors": "Xinyang Feng, Jie Yang, Andrew F. Laine, and Elsa D. Angelini", "title": "Discriminative Localization in CNNs for Weakly-Supervised Segmentation\n  of Pulmonary Nodules", "comments": null, "journal-ref": "International Conference on Medical Image Computing and\n  Computer-Assisted Intervention (MICCAI) 2017", "doi": "10.1007/978-3-319-66179-7_65", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated detection and segmentation of pulmonary nodules on lung computed\ntomography (CT) scans can facilitate early lung cancer diagnosis. Existing\nsupervised approaches for automated nodule segmentation on CT scans require\nvoxel-based annotations for training, which are labor- and time-consuming to\nobtain. In this work, we propose a weakly-supervised method that generates\naccurate voxel-level nodule segmentation trained with image-level labels only.\nBy adapting a convolutional neural network (CNN) trained for image\nclassification, our proposed method learns discriminative regions from the\nactivation maps of convolution units at different scales, and identifies the\ntrue nodule location with a novel candidate-screening framework. Experimental\nresults on the public LIDC-IDRI dataset demonstrate that, our weakly-supervised\nnodule segmentation framework achieves competitive performance compared to a\nfully-supervised CNN-based segmentation method.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jul 2017 17:54:57 GMT"}, {"version": "v2", "created": "Thu, 22 Feb 2018 19:24:06 GMT"}], "update_date": "2018-02-26", "authors_parsed": [["Feng", "Xinyang", ""], ["Yang", "Jie", ""], ["Laine", "Andrew F.", ""], ["Angelini", "Elsa D.", ""]]}, {"id": "1707.01159", "submitter": "Sayantan Sarkar", "authors": "Sayantan Sarkar, Ankan Bansal, Upal Mahbub, Rama Chellappa", "title": "UPSET and ANGRI : Breaking High Performance Image Classifiers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, targeted fooling of high performance image classifiers is\nachieved by developing two novel attack methods. The first method generates\nuniversal perturbations for target classes and the second generates image\nspecific perturbations. Extensive experiments are conducted on MNIST and\nCIFAR10 datasets to provide insights about the proposed algorithms and show\ntheir effectiveness.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jul 2017 21:34:08 GMT"}], "update_date": "2017-07-06", "authors_parsed": [["Sarkar", "Sayantan", ""], ["Bansal", "Ankan", ""], ["Mahbub", "Upal", ""], ["Chellappa", "Rama", ""]]}, {"id": "1707.01202", "submitter": "Vishwanath Sindagi", "authors": "Vishwanath A. Sindagi and Vishal M. Patel", "title": "A Survey of Recent Advances in CNN-based Single Image Crowd Counting and\n  Density Estimation", "comments": "16 pages, 17 figures", "journal-ref": null, "doi": "10.1016/j.patrec.2017.07.007", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating count and density maps from crowd images has a wide range of\napplications such as video surveillance, traffic monitoring, public safety and\nurban planning. In addition, techniques developed for crowd counting can be\napplied to related tasks in other fields of study such as cell microscopy,\nvehicle counting and environmental survey. The task of crowd counting and\ndensity map estimation is riddled with many challenges such as occlusions,\nnon-uniform density, intra-scene and inter-scene variations in scale and\nperspective. Nevertheless, over the last few years, crowd count analysis has\nevolved from earlier methods that are often limited to small variations in\ncrowd density and scales to the current state-of-the-art methods that have\ndeveloped the ability to perform successfully on a wide range of scenarios. The\nsuccess of crowd counting methods in the recent years can be largely attributed\nto deep learning and publications of challenging datasets. In this paper, we\nprovide a comprehensive survey of recent Convolutional Neural Network (CNN)\nbased approaches that have demonstrated significant improvements over earlier\nmethods that rely largely on hand-crafted representations. First, we briefly\nreview the pioneering methods that use hand-crafted representations and then we\ndelve in detail into the deep learning-based approaches and recently published\ndatasets. Furthermore, we discuss the merits and drawbacks of existing\nCNN-based approaches and identify promising avenues of research in this rapidly\nevolving field.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jul 2017 03:05:17 GMT"}], "update_date": "2017-07-20", "authors_parsed": [["Sindagi", "Vishwanath A.", ""], ["Patel", "Vishal M.", ""]]}, {"id": "1707.01213", "submitter": "Naiyan Wang", "authors": "Zehao Huang, Naiyan Wang", "title": "Data-Driven Sparse Structure Selection for Deep Neural Networks", "comments": "ECCV Camera ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks have liberated its extraordinary power on\nvarious tasks. However, it is still very challenging to deploy state-of-the-art\nmodels into real-world applications due to their high computational complexity.\nHow can we design a compact and effective network without massive experiments\nand expert knowledge? In this paper, we propose a simple and effective\nframework to learn and prune deep models in an end-to-end manner. In our\nframework, a new type of parameter -- scaling factor is first introduced to\nscale the outputs of specific structures, such as neurons, groups or residual\nblocks. Then we add sparsity regularizations on these factors, and solve this\noptimization problem by a modified stochastic Accelerated Proximal Gradient\n(APG) method. By forcing some of the factors to zero, we can safely remove the\ncorresponding structures, thus prune the unimportant parts of a CNN. Comparing\nwith other structure selection methods that may need thousands of trials or\niterative fine-tuning, our method is trained fully end-to-end in one training\npass without bells and whistles. We evaluate our method, Sparse Structure\nSelection with several state-of-the-art CNNs, and demonstrate very promising\nresults with adaptive depth and width selection.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jul 2017 05:21:50 GMT"}, {"version": "v2", "created": "Mon, 18 Dec 2017 22:38:02 GMT"}, {"version": "v3", "created": "Wed, 5 Sep 2018 05:14:37 GMT"}], "update_date": "2018-09-06", "authors_parsed": [["Huang", "Zehao", ""], ["Wang", "Naiyan", ""]]}, {"id": "1707.01219", "submitter": "Naiyan Wang", "authors": "Zehao Huang, Naiyan Wang", "title": "Like What You Like: Knowledge Distill via Neuron Selectivity Transfer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite deep neural networks have demonstrated extraordinary power in various\napplications, their superior performances are at expense of high storage and\ncomputational costs. Consequently, the acceleration and compression of neural\nnetworks have attracted much attention recently. Knowledge Transfer (KT), which\naims at training a smaller student network by transferring knowledge from a\nlarger teacher model, is one of the popular solutions. In this paper, we\npropose a novel knowledge transfer method by treating it as a distribution\nmatching problem. Particularly, we match the distributions of neuron\nselectivity patterns between teacher and student networks. To achieve this\ngoal, we devise a new KT loss function by minimizing the Maximum Mean\nDiscrepancy (MMD) metric between these distributions. Combined with the\noriginal loss function, our method can significantly improve the performance of\nstudent networks. We validate the effectiveness of our method across several\ndatasets, and further combine it with other KT methods to explore the best\npossible results. Last but not least, we fine-tune the model to other tasks\nsuch as object detection. The results are also encouraging, which confirm the\ntransferability of the learned features.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jul 2017 05:44:02 GMT"}, {"version": "v2", "created": "Mon, 18 Dec 2017 22:35:22 GMT"}], "update_date": "2017-12-20", "authors_parsed": [["Huang", "Zehao", ""], ["Wang", "Naiyan", ""]]}, {"id": "1707.01220", "submitter": "Naiyan Wang", "authors": "Yuntao Chen, Naiyan Wang, Zhaoxiang Zhang", "title": "DarkRank: Accelerating Deep Metric Learning via Cross Sample\n  Similarities Transfer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We have witnessed rapid evolution of deep neural network architecture design\nin the past years. These latest progresses greatly facilitate the developments\nin various areas such as computer vision and natural language processing.\nHowever, along with the extraordinary performance, these state-of-the-art\nmodels also bring in expensive computational cost. Directly deploying these\nmodels into applications with real-time requirement is still infeasible.\nRecently, Hinton etal. have shown that the dark knowledge within a powerful\nteacher model can significantly help the training of a smaller and faster\nstudent network. These knowledge are vastly beneficial to improve the\ngeneralization ability of the student model. Inspired by their work, we\nintroduce a new type of knowledge -- cross sample similarities for model\ncompression and acceleration. This knowledge can be naturally derived from deep\nmetric learning model. To transfer them, we bring the \"learning to rank\"\ntechnique into deep metric learning formulation. We test our proposed DarkRank\nmethod on various metric learning tasks including pedestrian re-identification,\nimage retrieval and image clustering. The results are quite encouraging. Our\nmethod can improve over the baseline method by a large margin. Moreover, it is\nfully compatible with other existing methods. When combined, the performance\ncan be further boosted.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jul 2017 05:47:11 GMT"}, {"version": "v2", "created": "Mon, 18 Dec 2017 22:44:44 GMT"}], "update_date": "2017-12-20", "authors_parsed": [["Chen", "Yuntao", ""], ["Wang", "Naiyan", ""], ["Zhang", "Zhaoxiang", ""]]}, {"id": "1707.01221", "submitter": "Yaqi Liu", "authors": "Yaqi Liu, Qingxiao Guan and Xianfeng Zhao", "title": "Copy-move Forgery Detection based on Convolutional Kernel Network", "comments": "26 pages, 8 figures, submitted to Multimedia Tools and Applications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a copy-move forgery detection method based on Convolutional\nKernel Network is proposed. Different from methods based on conventional\nhand-crafted features, Convolutional Kernel Network is a kind of data-driven\nlocal descriptor with the deep convolutional structure. Thanks to the\ndevelopment of deep learning theories and widely available datasets, the\ndata-driven methods can achieve competitive performance on different conditions\nfor its excellent discriminative capability. Besides, our Convolutional Kernel\nNetwork is reformulated as a series of matrix computations and convolutional\noperations which are easy to parallelize and accelerate by GPU, leading to high\nefficiency. Then, appropriate preprocessing and postprocessing for\nConvolutional Kernel Network are adopted to achieve copy-move forgery\ndetection. Particularly, a segmentation-based keypoints distribution strategy\nis proposed and a GPU-based adaptive oversegmentation method is adopted.\nNumerous experiments are conducted to demonstrate the effectiveness and\nrobustness of the GPU version of Convolutional Kernel Network, and the\nstate-of-the-art performance of the proposed copy-move forgery detection method\nbased on Convolutional Kernel Network.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jul 2017 05:54:18 GMT"}], "update_date": "2017-07-06", "authors_parsed": [["Liu", "Yaqi", ""], ["Guan", "Qingxiao", ""], ["Zhao", "Xianfeng", ""]]}, {"id": "1707.01243", "submitter": "Lin Duan", "authors": "Lin Duan", "title": "Exploration of object recognition from 3D point cloud", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present our latest experiment results of object recognition from 3D point\ncloud data collected through moving car.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jul 2017 07:43:00 GMT"}], "update_date": "2017-07-06", "authors_parsed": [["Duan", "Lin", ""]]}, {"id": "1707.01253", "submitter": "Shaohua Li", "authors": "Shaohua Li, Xinxing Xu, Liqiang Nie, Tat-Seng Chua", "title": "Laplacian-Steered Neural Style Transfer", "comments": "Accepted by the ACM Multimedia Conference (MM) 2017. 9 pages, 65\n  figures", "journal-ref": null, "doi": "10.1145/3123266.3123425", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural Style Transfer based on Convolutional Neural Networks (CNN) aims to\nsynthesize a new image that retains the high-level structure of a content\nimage, rendered in the low-level texture of a style image. This is achieved by\nconstraining the new image to have high-level CNN features similar to the\ncontent image, and lower-level CNN features similar to the style image. However\nin the traditional optimization objective, low-level features of the content\nimage are absent, and the low-level features of the style image dominate the\nlow-level detail structures of the new image. Hence in the synthesized image,\nmany details of the content image are lost, and a lot of inconsistent and\nunpleasing artifacts appear. As a remedy, we propose to steer image synthesis\nwith a novel loss function: the Laplacian loss. The Laplacian matrix\n(\"Laplacian\" in short), produced by a Laplacian operator, is widely used in\ncomputer vision to detect edges and contours. The Laplacian loss measures the\ndifference of the Laplacians, and correspondingly the difference of the detail\nstructures, between the content image and a new image. It is flexible and\ncompatible with the traditional style transfer constraints. By incorporating\nthe Laplacian loss, we obtain a new optimization objective for neural style\ntransfer named Lapstyle. Minimizing this objective will produce a stylized\nimage that better preserves the detail structures of the content image and\neliminates the artifacts. Experiments show that Lapstyle produces more\nappealing stylized images with less artifacts, without compromising their\n\"stylishness\".\n", "versions": [{"version": "v1", "created": "Wed, 5 Jul 2017 08:10:41 GMT"}, {"version": "v2", "created": "Sun, 30 Jul 2017 15:46:17 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Li", "Shaohua", ""], ["Xu", "Xinxing", ""], ["Nie", "Liqiang", ""], ["Chua", "Tat-Seng", ""]]}, {"id": "1707.01274", "submitter": "Ruben Gomez-Ojeda", "authors": "Ruben Gomez-Ojeda, Zichao Zhang, Javier Gonzalez-Jimenez and Davide\n  Scaramuzza", "title": "Learning-based Image Enhancement for Visual Odometry in Challenging HDR\n  Environments", "comments": null, "journal-ref": "IEEE Conference on Robotics and Automation (ICRA), 2018", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the main open challenges in visual odometry (VO) is the robustness to\ndifficult illumination conditions or high dynamic range (HDR) environments. The\nmain difficulties in these situations come from both the limitations of the\nsensors and the inability to perform a successful tracking of interest points\nbecause of the bold assumptions in VO, such as brightness constancy. We address\nthis problem from a deep learning perspective, for which we first fine-tune a\nDeep Neural Network (DNN) with the purpose of obtaining enhanced\nrepresentations of the sequences for VO. Then, we demonstrate how the insertion\nof Long Short Term Memory (LSTM) allows us to obtain temporally consistent\nsequences, as the estimation depends on previous states. However, the use of\nvery deep networks does not allow the insertion into a real-time VO framework;\ntherefore, we also propose a Convolutional Neural Network (CNN) of reduced size\ncapable of performing faster. Finally, we validate the enhanced representations\nby evaluating the sequences produced by the two architectures in several\nstate-of-art VO algorithms, such as ORB-SLAM and DSO.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jul 2017 09:26:34 GMT"}, {"version": "v2", "created": "Tue, 11 Jul 2017 10:03:47 GMT"}, {"version": "v3", "created": "Mon, 9 Apr 2018 10:59:20 GMT"}], "update_date": "2018-04-11", "authors_parsed": [["Gomez-Ojeda", "Ruben", ""], ["Zhang", "Zichao", ""], ["Gonzalez-Jimenez", "Javier", ""], ["Scaramuzza", "Davide", ""]]}, {"id": "1707.01294", "submitter": "Suman Ghosh", "authors": "Suman Ghosh, Ernest Valveny", "title": "R-PHOC: Segmentation-Free Word Spotting using CNN", "comments": "Accepted in ICDAR'2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a region based convolutional neural network for\nsegmentation-free word spotting. Our net- work takes as input an image and a\nset of word candidate bound- ing boxes and embeds all bounding boxes into an\nembedding space, where word spotting can be casted as a simple nearest\nneighbour search between the query representation and each of the candidate\nbounding boxes. We make use of PHOC embedding as it has previously achieved\nsignificant success in segmentation- based word spotting. Word candidates are\ngenerated using a simple procedure based on grouping connected components using\nsome spatial constraints. Experiments show that R-PHOC which operates on images\ndirectly can improve the current state-of- the-art in the standard GW dataset\nand performs as good as PHOCNET in some cases designed for segmentation based\nword spotting.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jul 2017 10:08:51 GMT"}], "update_date": "2017-07-06", "authors_parsed": [["Ghosh", "Suman", ""], ["Valveny", "Ernest", ""]]}, {"id": "1707.01307", "submitter": "Tatsunori Taniai", "authors": "Tatsunori Taniai, Sudipta N. Sinha, Yoichi Sato", "title": "Fast Multi-frame Stereo Scene Flow with Motion Segmentation", "comments": "15 pages. To appear at IEEE Conference on Computer Vision and Pattern\n  Recognition (CVPR 2017). Our results were submitted to KITTI 2015 Stereo\n  Scene Flow Benchmark in November 2016", "journal-ref": "2017 IEEE Conference on Computer Vision and Pattern Recognition\n  (CVPR), Honolulu, HI, USA, 2017, pp. 6891-6900", "doi": "10.1109/CVPR.2017.729", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new multi-frame method for efficiently computing scene flow\n(dense depth and optical flow) and camera ego-motion for a dynamic scene\nobserved from a moving stereo camera rig. Our technique also segments out\nmoving objects from the rigid scene. In our method, we first estimate the\ndisparity map and the 6-DOF camera motion using stereo matching and visual\nodometry. We then identify regions inconsistent with the estimated camera\nmotion and compute per-pixel optical flow only at these regions. This flow\nproposal is fused with the camera motion-based flow proposal using fusion moves\nto obtain the final optical flow and motion segmentation. This unified\nframework benefits all four tasks - stereo, optical flow, visual odometry and\nmotion segmentation leading to overall higher accuracy and efficiency. Our\nmethod is currently ranked third on the KITTI 2015 scene flow benchmark.\nFurthermore, our CPU implementation runs in 2-3 seconds per frame which is 1-3\norders of magnitude faster than the top six methods. We also report a thorough\nevaluation on challenging Sintel sequences with fast camera and object motion,\nwhere our method consistently outperforms OSF [Menze and Geiger, 2015], which\nis currently ranked second on the KITTI benchmark.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jul 2017 10:42:34 GMT"}], "update_date": "2017-11-29", "authors_parsed": [["Taniai", "Tatsunori", ""], ["Sinha", "Sudipta N.", ""], ["Sato", "Yoichi", ""]]}, {"id": "1707.01313", "submitter": "Tobias Pl\\\"otz", "authors": "Tobias Pl\\\"otz, Stefan Roth", "title": "Benchmarking Denoising Algorithms with Real Photographs", "comments": "To appear at CVPR17. See our website (www.visinf.tu-darmstadt.de) for\n  a version with high-resolution images", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lacking realistic ground truth data, image denoising techniques are\ntraditionally evaluated on images corrupted by synthesized i.i.d. Gaussian\nnoise. We aim to obviate this unrealistic setting by developing a methodology\nfor benchmarking denoising techniques on real photographs. We capture pairs of\nimages with different ISO values and appropriately adjusted exposure times,\nwhere the nearly noise-free low-ISO image serves as reference. To derive the\nground truth, careful post-processing is needed. We correct spatial\nmisalignment, cope with inaccuracies in the exposure parameters through a\nlinear intensity transform based on a novel heteroscedastic Tobit regression\nmodel, and remove residual low-frequency bias that stems, e.g., from minor\nillumination changes. We then capture a novel benchmark dataset, the Darmstadt\nNoise Dataset (DND), with consumer cameras of differing sensor sizes. One\ninteresting finding is that various recent techniques that perform well on\nsynthetic noise are clearly outperformed by BM3D on photographs with real\nnoise. Our benchmark delineates realistic evaluation scenarios that deviate\nstrongly from those commonly used in the scientific literature.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jul 2017 10:51:59 GMT"}], "update_date": "2017-07-06", "authors_parsed": [["Pl\u00f6tz", "Tobias", ""], ["Roth", "Stefan", ""]]}, {"id": "1707.01317", "submitter": "Tobias Pl\\\"otz", "authors": "Florian Lang, Tobias Pl\\\"otz, Stefan Roth", "title": "Robust Multi-Image HDR Reconstruction for the Modulo Camera", "comments": "to appear at the 39th German Conference on Pattern Recognition (GCPR)\n  2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Photographing scenes with high dynamic range (HDR) poses great challenges to\nconsumer cameras with their limited sensor bit depth. To address this, Zhao et\nal. recently proposed a novel sensor concept - the modulo camera - which\ncaptures the least significant bits of the recorded scene instead of going into\nsaturation. Similar to conventional pipelines, HDR images can be reconstructed\nfrom multiple exposures, but significantly fewer images are needed than with a\ntypical saturating sensor. While the concept is appealing, we show that the\noriginal reconstruction approach assumes noise-free measurements and quickly\nbreaks down otherwise. To address this, we propose a novel reconstruction\nalgorithm that is robust to image noise and produces significantly fewer\nartifacts. We theoretically analyze correctness as well as limitations, and\nshow that our approach significantly outperforms the baseline on real data.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jul 2017 11:00:36 GMT"}], "update_date": "2017-07-06", "authors_parsed": [["Lang", "Florian", ""], ["Pl\u00f6tz", "Tobias", ""], ["Roth", "Stefan", ""]]}, {"id": "1707.01330", "submitter": "Mojtaba Masoudi", "authors": "Mojtaba Masoudi, Hamidreza Pourreza, Mahdi Saadatmand Tarzjan, Fateme\n  Shafiee Zargar, Masoud Pezeshki Rad, Noushin Eftekhari", "title": "A dataset for Computer-Aided Detection of Pulmonary Embolism in CTA\n  images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Todays, researchers in the field of Pulmonary Embolism (PE) analysis need to\nuse a publicly available dataset to assess and compare their methods. Different\nsystems have been designed for the detection of pulmonary embolism (PE), but\nnone of them have used any public datasets. All papers have used their own\nprivate dataset. In order to fill this gap, we have collected 5160 slices of\ncomputed tomography angiography (CTA) images acquired from 20 patients, and\nafter labeling the image by experts in this field, we provided a reliable\ndataset which is now publicly available. In some situation, PE detection can be\ndifficult, for example when it occurs in the peripheral branches or when\npatients have pulmonary diseases (such as parenchymal disease). Therefore, the\nefficiency of CAD systems highly depends on the dataset. In the given dataset,\n66% of PE are located in peripheral branches, and different pulmonary diseases\nare also included.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jul 2017 11:35:27 GMT"}], "update_date": "2017-07-06", "authors_parsed": [["Masoudi", "Mojtaba", ""], ["Pourreza", "Hamidreza", ""], ["Tarzjan", "Mahdi Saadatmand", ""], ["Zargar", "Fateme Shafiee", ""], ["Rad", "Masoud Pezeshki", ""], ["Eftekhari", "Noushin", ""]]}, {"id": "1707.01342", "submitter": "Claudia Blaiotta", "authors": "Claudia Blaiotta, Patrick Freund, M. Jorge Cardoso, John Ashburner", "title": "Generative diffeomorphic atlas construction from brain and spinal cord\n  MRI data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we will focus on the potential and on the challenges associated\nwith the development of an integrated brain and spinal cord modelling framework\nfor processing MR neuroimaging data. The aim of the work is to explore how a\nhierarchical generative model of imaging data, which captures simultaneously\nthe distribution of signal intensities and the variability of anatomical shapes\nacross a large population of subjects, can serve to quantitatively investigate,\nin vivo, the morphology of the central nervous system (CNS). In fact, the\ngenerality of the proposed Bayesian approach, which extends the hierarchical\nstructure of the segmentation method implemented in the SPM software, allows\nprocessing simultaneously information relative to different compartments of the\nCNS, namely the brain and the spinal cord, without having to resort to organ\nspecific solutions (e.g. tools optimised only for the brain, or only for the\nspinal cord), which are inevitably harder to integrate and generalise.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jul 2017 12:10:44 GMT"}], "update_date": "2017-07-06", "authors_parsed": [["Blaiotta", "Claudia", ""], ["Freund", "Patrick", ""], ["Cardoso", "M. Jorge", ""], ["Ashburner", "John", ""]]}, {"id": "1707.01357", "submitter": "Stefan Lattner", "authors": "Stefan Lattner and Maarten Grachten", "title": "Improving Content-Invariance in Gated Autoencoders for 2D and 3D Object\n  Rotation", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Content-invariance in mapping codes learned by GAEs is a useful feature for\nvarious relation learning tasks. In this paper we show that the\ncontent-invariance of mapping codes for images of 2D and 3D rotated objects can\nbe substantially improved by extending the standard GAE loss (symmetric\nreconstruction error) with a regularization term that penalizes the symmetric\ncross-reconstruction error. This error term involves reconstruction of pairs\nwith mapping codes obtained from other pairs exhibiting similar\ntransformations. Although this would principally require knowledge of the\ntransformations exhibited by training pairs, our experiments show that a\nbootstrapping approach can sidestep this issue, and that the regularization\nterm can effectively be used in an unsupervised setting.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jul 2017 12:28:43 GMT"}], "update_date": "2017-07-06", "authors_parsed": [["Lattner", "Stefan", ""], ["Grachten", "Maarten", ""]]}, {"id": "1707.01394", "submitter": "Lukas On Arnold", "authors": "Lukas On Arnold", "title": "Development & Implementation of the Trigger for a Short-baseline Reactor\n  Antineutrino Experiment (SoLid)", "comments": "University of Bristol, MSc 2017", "journal-ref": null, "doi": null, "report-no": "MSC-2017-ARNOLD", "categories": "physics.ins-det cs.CV hep-ex", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  SoLid, located at SCK-CEN in Mol, Belgium, is a reactor antineutrino\nexperiment at a very short baseline of 5.5 - 10m aiming at the search for\nsterile neutrinos and for high precision measurement of the neutrino energy\nspectrum of Uranium-235. It uses a novel approach using Lithium-6 sheets and\nPVT cubes as scintillators for tagging the Inverse Beta-Decay products (neutron\nand positron). Being located overground and close to the BR2 research reactor,\nthe experiment faces a large amount of backgrounds. Efficient real-time\nbackground and noise rejection is essential in order to increase the\nsignal-background ratio for precise oscillation measurement and decrease data\nproduction to a rate which can be handled by the online software. Therefore, a\nreliable distinction between the neutrons and background signals is crucial.\nThis can be performed online with a dedicated firmware trigger. A peak counting\nalgorithm and an algorithm measuring time over threshold have been identified\nas performing well both in terms of efficiency and fake rate, and have been\nimplemented onto an FPGA. After having introduced the experimental and\ntheoretical background of neutrino oscillation physics, as well as SoLid's\ndetector technology, read-out system and trigger scheme, the thesis presents\nthe design of the firmware neutron trigger implemented by applying machine\nlearning methods.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jul 2017 13:52:49 GMT"}], "update_date": "2017-07-10", "authors_parsed": [["Arnold", "Lukas On", ""]]}, {"id": "1707.01395", "submitter": "Dmitriy Anisimov", "authors": "Dmitriy Anisimov, Tatiana Khanova", "title": "Towards lightweight convolutional neural networks for object detection", "comments": "Submitted to the International Workshop on Traffic and Street\n  Surveillance for Safety and Security (IWT4S) in conjunction with the 14th\n  IEEE International Conference on Advanced Video and Signal based Surveillance\n  (AVSS 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose model with larger spatial size of feature maps and evaluate it on\nobject detection task. With the goal to choose the best feature extraction\nnetwork for our model we compare several popular lightweight networks. After\nthat we conduct a set of experiments with channels reduction algorithms in\norder to accelerate execution. Our vehicle detection models are accurate, fast\nand therefore suit for embedded visual applications. With only 1.5 GFLOPs our\nbest model gives 93.39 AP on validation subset of challenging DETRAC dataset.\nThe smallest of our models is the first to achieve real-time inference speed on\nCPU with reasonable accuracy drop to 91.43 AP.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jul 2017 13:53:00 GMT"}, {"version": "v2", "created": "Mon, 7 Aug 2017 12:49:46 GMT"}, {"version": "v3", "created": "Thu, 5 Oct 2017 12:08:49 GMT"}], "update_date": "2017-10-06", "authors_parsed": [["Anisimov", "Dmitriy", ""], ["Khanova", "Tatiana", ""]]}, {"id": "1707.01400", "submitter": "Xudong Mao", "authors": "Xudong Mao, Qing Li, Haoran Xie", "title": "AlignGAN: Learning to Align Cross-Domain Images with Conditional\n  Generative Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, several methods based on generative adversarial network (GAN) have\nbeen proposed for the task of aligning cross-domain images or learning a joint\ndistribution of cross-domain images. One of the methods is to use conditional\nGAN for alignment. However, previous attempts of adopting conditional GAN do\nnot perform as well as other methods. In this work we present an approach for\nimproving the capability of the methods which are based on conditional GAN. We\nevaluate the proposed method on numerous tasks and the experimental results\nshow that it is able to align the cross-domain images successfully in absence\nof paired samples. Furthermore, we also propose another model which conditions\non multiple information such as domain information and label information.\nConditioning on domain information and label information, we are able to\nconduct label propagation from the source domain to the target domain. A 2-step\nalternating training algorithm is proposed to learn this model.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jul 2017 14:02:59 GMT"}], "update_date": "2017-07-06", "authors_parsed": [["Mao", "Xudong", ""], ["Li", "Qing", ""], ["Xie", "Haoran", ""]]}, {"id": "1707.01408", "submitter": "Po-Yao Huang", "authors": "Po-Yao Huang, Ye Yuan, Zhenzhong Lan, Lu Jiang, Alexander G. Hauptmann", "title": "Video Representation Learning and Latent Concept Mining for Large-scale\n  Multi-label Video Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  We report on CMU Informedia Lab's system used in Google's YouTube 8 Million\nVideo Understanding Challenge. In this multi-label video classification task,\nour pipeline achieved 84.675% and 84.662% GAP on our evaluation split and the\nofficial test set. We attribute the good performance to three components: 1)\nRefined video representation learning with residual links and hypercolumns 2)\nLatent concept mining which captures interactions among concepts. 3) Learning\nwith temporal segments and weighted multi-model ensemble. We conduct\nexperiments to validate and analyze the contribution of our models. We also\nshare some unsuccessful trials leveraging conventional approaches such as\nrecurrent neural networks for video representation learning for this\nlarge-scale video dataset. All the codes to reproduce our results are publicly\navailable at https://github.com/Martini09/informedia-yt8m-release.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jul 2017 14:15:06 GMT"}, {"version": "v2", "created": "Thu, 6 Jul 2017 06:48:02 GMT"}, {"version": "v3", "created": "Tue, 25 Jul 2017 07:50:26 GMT"}], "update_date": "2017-07-26", "authors_parsed": [["Huang", "Po-Yao", ""], ["Yuan", "Ye", ""], ["Lan", "Zhenzhong", ""], ["Jiang", "Lu", ""], ["Hauptmann", "Alexander G.", ""]]}, {"id": "1707.01530", "submitter": "Eric Miller", "authors": "Hamideh Rezaee, Brian Tracey, Eric L. Miller", "title": "On the Fusion of Compton Scatter and Attenuation Data for Limited-view\n  X-ray Tomographic Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we demonstrate the utility of fusing energy-resolved\nobservations of Compton scattered photons with traditional attenuation data for\nthe joint recovery of mass density and photoelectric absorption in the context\nof limited view tomographic imaging applications. We begin with the development\nof a physical and associated numerical model for the Compton scatter process.\nUsing this model, we propose a variational approach recovering these two\nmaterial properties. In addition to the typical data-fidelity terms, the\noptimization functional includes regularization for both the mass density and\nphotoelectric coefficients. We consider a novel edge-preserving method in the\ncase of mass density. To aid in the recovery of the photoelectric information,\nwe draw on our recent method in \\cite{r15} and employ a non-local\nregularization scheme that builds on the fact that mass density is more stably\nimaged. Simulation results demonstrate clear advantages associated with the use\nof both scattered photon data and energy resolved information in mapping the\ntwo material properties of interest. Specifically, comparing images obtained\nusing only conventional attenuation data with those where we employ only\nCompton scatter photons and images formed from the combination of the two,\nshows that taking advantage of both types of data for reconstruction provides\nfar more accurate results.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jul 2017 18:39:18 GMT"}], "update_date": "2017-07-07", "authors_parsed": [["Rezaee", "Hamideh", ""], ["Tracey", "Brian", ""], ["Miller", "Eric L.", ""]]}, {"id": "1707.01606", "submitter": "Ekraam Sabir", "authors": "Ayush Jaiswal, Ekraam Sabir, Wael AbdAlmageed, Premkumar Natarajan", "title": "Multimedia Semantic Integrity Assessment Using Joint Embedding Of Images\n  And Text", "comments": "*Ayush Jaiswal and Ekraam Sabir contributed equally to the work in\n  this paper", "journal-ref": "In Proceedings of the 2017 ACM on Multimedia Conference, pp.\n  1465-1471. ACM, 2017", "doi": "10.1145/3123266.3123385", "report-no": null, "categories": "cs.MM cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real world multimedia data is often composed of multiple modalities such as\nan image or a video with associated text (e.g. captions, user comments, etc.)\nand metadata. Such multimodal data packages are prone to manipulations, where a\nsubset of these modalities can be altered to misrepresent or repurpose data\npackages, with possible malicious intent. It is, therefore, important to\ndevelop methods to assess or verify the integrity of these multimedia packages.\nUsing computer vision and natural language processing methods to directly\ncompare the image (or video) and the associated caption to verify the integrity\nof a media package is only possible for a limited set of objects and scenes. In\nthis paper, we present a novel deep learning-based approach for assessing the\nsemantic integrity of multimedia packages containing images and captions, using\na reference set of multimedia packages. We construct a joint embedding of\nimages and captions with deep multimodal representation learning on the\nreference dataset in a framework that also provides image-caption consistency\nscores (ICCSs). The integrity of query media packages is assessed as the\ninlierness of the query ICCSs with respect to the reference dataset. We present\nthe MultimodAl Information Manipulation dataset (MAIM), a new dataset of media\npackages from Flickr, which we make available to the research community. We use\nboth the newly created dataset as well as Flickr30K and MS COCO datasets to\nquantitatively evaluate our proposed approach. The reference dataset does not\ncontain unmanipulated versions of tampered query packages. Our method is able\nto achieve F1 scores of 0.75, 0.89 and 0.94 on MAIM, Flickr30K and MS COCO,\nrespectively, for detecting semantically incoherent media packages.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jul 2017 01:25:17 GMT"}, {"version": "v2", "created": "Fri, 7 Jul 2017 00:47:21 GMT"}, {"version": "v3", "created": "Tue, 11 Jul 2017 18:02:08 GMT"}, {"version": "v4", "created": "Fri, 29 Jun 2018 00:34:27 GMT"}], "update_date": "2018-07-02", "authors_parsed": [["Jaiswal", "Ayush", ""], ["Sabir", "Ekraam", ""], ["AbdAlmageed", "Wael", ""], ["Natarajan", "Premkumar", ""]]}, {"id": "1707.01613", "submitter": "Haichao Shi", "authors": "Haichao Shi, Jing Dong, Wei Wang, Yinlong Qian, Xiaoyu Zhang", "title": "SSGAN: Secure Steganography Based on Generative Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a novel strategy of Secure Steganograpy based on Generative\nAdversarial Networks is proposed to generate suitable and secure covers for\nsteganography. The proposed architecture has one generative network, and two\ndiscriminative networks. The generative network mainly evaluates the visual\nquality of the generated images for steganography, and the discriminative\nnetworks are utilized to assess their suitableness for information hiding.\nDifferent from the existing work which adopts Deep Convolutional Generative\nAdversarial Networks, we utilize another form of generative adversarial\nnetworks. By using this new form of generative adversarial networks,\nsignificant improvements are made on the convergence speed, the training\nstability and the image quality. Furthermore, a sophisticated steganalysis\nnetwork is reconstructed for the discriminative network, and the network can\nbetter evaluate the performance of the generated images. Numerous experiments\nare conducted on the publicly available datasets to demonstrate the\neffectiveness and robustness of the proposed method.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jul 2017 02:05:51 GMT"}, {"version": "v2", "created": "Tue, 11 Jul 2017 02:54:39 GMT"}, {"version": "v3", "created": "Sat, 29 Jul 2017 04:26:48 GMT"}, {"version": "v4", "created": "Sat, 24 Nov 2018 02:32:03 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Shi", "Haichao", ""], ["Dong", "Jing", ""], ["Wang", "Wei", ""], ["Qian", "Yinlong", ""], ["Zhang", "Xiaoyu", ""]]}, {"id": "1707.01629", "submitter": "Yunpeng Chen", "authors": "Yunpeng Chen, Jianan Li, Huaxin Xiao, Xiaojie Jin, Shuicheng Yan,\n  Jiashi Feng", "title": "Dual Path Networks", "comments": "for code and models, see https://github.com/cypw/DPNs", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present a simple, highly efficient and modularized Dual Path\nNetwork (DPN) for image classification which presents a new topology of\nconnection paths internally. By revealing the equivalence of the\nstate-of-the-art Residual Network (ResNet) and Densely Convolutional Network\n(DenseNet) within the HORNN framework, we find that ResNet enables feature\nre-usage while DenseNet enables new features exploration which are both\nimportant for learning good representations. To enjoy the benefits from both\npath topologies, our proposed Dual Path Network shares common features while\nmaintaining the flexibility to explore new features through dual path\narchitectures. Extensive experiments on three benchmark datasets, ImagNet-1k,\nPlaces365 and PASCAL VOC, clearly demonstrate superior performance of the\nproposed DPN over state-of-the-arts. In particular, on the ImagNet-1k dataset,\na shallow DPN surpasses the best ResNeXt-101(64x4d) with 26% smaller model\nsize, 25% less computational cost and 8% lower memory consumption, and a deeper\nDPN (DPN-131) further pushes the state-of-the-art single model performance with\nabout 2 times faster training speed. Experiments on the Places365 large-scale\nscene dataset, PASCAL VOC detection dataset, and PASCAL VOC segmentation\ndataset also demonstrate its consistently better performance than DenseNet,\nResNet and the latest ResNeXt model over various applications.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jul 2017 04:05:14 GMT"}, {"version": "v2", "created": "Tue, 1 Aug 2017 01:15:57 GMT"}], "update_date": "2017-08-02", "authors_parsed": [["Chen", "Yunpeng", ""], ["Li", "Jianan", ""], ["Xiao", "Huaxin", ""], ["Jin", "Xiaojie", ""], ["Yan", "Shuicheng", ""], ["Feng", "Jiashi", ""]]}, {"id": "1707.01691", "submitter": "Anbang Yao", "authors": "Tao Kong, Fuchun Sun, Anbang Yao, Huaping Liu, Ming Lu, Yurong Chen", "title": "RON: Reverse Connection with Objectness Prior Networks for Object\n  Detection", "comments": "Project page will be available at https://github.com/taokong/RON, and\n  formal paper will appear in CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present RON, an efficient and effective framework for generic object\ndetection. Our motivation is to smartly associate the best of the region-based\n(e.g., Faster R-CNN) and region-free (e.g., SSD) methodologies. Under fully\nconvolutional architecture, RON mainly focuses on two fundamental problems: (a)\nmulti-scale object localization and (b) negative sample mining. To address (a),\nwe design the reverse connection, which enables the network to detect objects\non multi-levels of CNNs. To deal with (b), we propose the objectness prior to\nsignificantly reduce the searching space of objects. We optimize the reverse\nconnection, objectness prior and object detector jointly by a multi-task loss\nfunction, thus RON can directly predict final detection results from all\nlocations of various feature maps. Extensive experiments on the challenging\nPASCAL VOC 2007, PASCAL VOC 2012 and MS COCO benchmarks demonstrate the\ncompetitive performance of RON. Specifically, with VGG-16 and low resolution\n384X384 input size, the network gets 81.3% mAP on PASCAL VOC 2007, 80.7% mAP on\nPASCAL VOC 2012 datasets. Its superiority increases when datasets become larger\nand more difficult, as demonstrated by the results on the MS COCO dataset. With\n1.5G GPU memory at test phase, the speed of the network is 15 FPS, 3X faster\nthan the Faster R-CNN counterpart.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jul 2017 08:53:33 GMT"}], "update_date": "2017-07-07", "authors_parsed": [["Kong", "Tao", ""], ["Sun", "Fuchun", ""], ["Yao", "Anbang", ""], ["Liu", "Huaping", ""], ["Lu", "Ming", ""], ["Chen", "Yurong", ""]]}, {"id": "1707.01698", "submitter": "Stijn Heldens", "authors": "Stijn Heldens, Claudio Martella, Nelly Litvak, Maarten van Steen", "title": "Automated Lane Detection in Crowds using Proximity Graphs", "comments": "Presented at the 6th International Workshop on Urban Computing\n  (UrbComp 2017) held in conjunction with the 23th ACM SIGKDD", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Studying the behavior of crowds is vital for understanding and predicting\nhuman interactions in public areas. Research has shown that, under certain\nconditions, large groups of people can form collective behavior patterns: local\ninteractions between individuals results in global movements patterns. To\ndetect these patterns in a crowd, we assume each person is carrying an on-body\ndevice that acts a local proximity sensor, e.g., smartphone or bluetooth badge,\nand represent the texture of the crowd as a proximity graph. Our goal is\nextract information about crowds from these proximity graphs. In this work, we\nfocus on one particular type of pattern: lane formation. We present a formal\ndefinition of a lane, proposed a simple probabilistic model that simulates\nlanes moving through a stationary crowd, and present an automated\nlane-detection method. Our preliminary results show that our method is able to\ndetect lanes of different shapes and sizes. We see our work as an initial step\ntowards rich pattern recognition using proximity graphs.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jul 2017 09:23:07 GMT"}], "update_date": "2017-07-07", "authors_parsed": [["Heldens", "Stijn", ""], ["Martella", "Claudio", ""], ["Litvak", "Nelly", ""], ["van Steen", "Maarten", ""]]}, {"id": "1707.01700", "submitter": "Joris Gu\\'erin", "authors": "Joris Gu\\'erin, Olivier Gibaru, St\\'ephane Thiery and Eric Nyiri", "title": "CNN features are also great at unsupervised classification", "comments": "10 pages, 2 figures, 4 tables. Proceedings of AIFU 2018, Melbourne,\n  Australia", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper aims at providing insight on the transferability of deep CNN\nfeatures to unsupervised problems. We study the impact of different pretrained\nCNN feature extractors on the problem of image set clustering for object\nclassification as well as fine-grained classification. We propose a rather\nstraightforward pipeline combining deep-feature extraction using a CNN\npretrained on ImageNet and a classic clustering algorithm to classify sets of\nimages. This approach is compared to state-of-the-art algorithms in\nimage-clustering and provides better results. These results strengthen the\nbelief that supervised training of deep CNN on large datasets, with a large\nvariability of classes, extracts better features than most carefully designed\nengineering approaches, even for unsupervised tasks. We also validate our\napproach on a robotic application, consisting in sorting and storing objects\nsmartly based on clustering.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jul 2017 09:24:35 GMT"}, {"version": "v2", "created": "Tue, 11 Sep 2018 11:11:15 GMT"}], "update_date": "2018-09-12", "authors_parsed": [["Gu\u00e9rin", "Joris", ""], ["Gibaru", "Olivier", ""], ["Thiery", "St\u00e9phane", ""], ["Nyiri", "Eric", ""]]}, {"id": "1707.01736", "submitter": "Emiel van Miltenburg", "authors": "Emiel van Miltenburg, Desmond Elliott, Piek Vossen", "title": "Cross-linguistic differences and similarities in image descriptions", "comments": "Accepted for INLG 2017, Santiago de Compostela, Spain, 4-7 September,\n  2017. Camera-ready version. See the ACL anthology for full bibliographic\n  information", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic image description systems are commonly trained and evaluated on\nlarge image description datasets. Recently, researchers have started to collect\nsuch datasets for languages other than English. An unexplored question is how\ndifferent these datasets are from English and, if there are any differences,\nwhat causes them to differ. This paper provides a cross-linguistic comparison\nof Dutch, English, and German image descriptions. We find that these\ndescriptions are similar in many respects, but the familiarity of crowd workers\nwith the subjects of the images has a noticeable influence on description\nspecificity.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jul 2017 11:53:41 GMT"}, {"version": "v2", "created": "Sun, 13 Aug 2017 10:18:44 GMT"}], "update_date": "2017-08-15", "authors_parsed": [["van Miltenburg", "Emiel", ""], ["Elliott", "Desmond", ""], ["Vossen", "Piek", ""]]}, {"id": "1707.01745", "submitter": "Bogus{\\l}aw Rymut", "authors": "Bogus{\\l}aw Rymut", "title": "Computer methods for 3D motion tracking in real-time", "comments": "PhD Thesis \"Komputerowe algorytmy ekstrakcji i \\'sledzenia obiekt\\'ow\n  w czasie rzeczywistym\" (in Polish) Supervisor: PhD Bogdan Kwolek, Prof. AGH\n  Key words: model-based 3D motion tracking, parallel computing, computer\n  vision", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This thesis is devoted to marker-less 3D human motion tracking in calibrated\nand synchronized multicamera systems. Pose estimation is based on a 3D model,\nwhich is transformed into the image plane and then rendered. Owing to\nelaborated techniques the tracking of the full body has been achieved in\nreal-time via dynamic optimization or dynamic Bayesian filtering. The objective\nfunction of a particle swarm optimization algorithm and the observation model\nof a particle filter are based on matching between the rendered 3D models in\nthe required poses and image features representing the extracted person. In\nsuch an approach the main part of the computational overload is associated with\nthe rendering of 3D models in hypothetical poses as well as determination of\nvalue of objective function. Effective methods for rendering of 3D models in\nreal-time with support of OpenGL as well as parallel methods for determining\nthe objective function on the GPU were developed. The elaborated solutions\npermit 3D tracking of full body motion in real-time.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jul 2017 10:07:19 GMT"}], "update_date": "2017-07-07", "authors_parsed": [["Rymut", "Bogus\u0142aw", ""]]}, {"id": "1707.01753", "submitter": "Aritra Dutta", "authors": "Aritra Dutta and Xin Li", "title": "Weighted Low Rank Approximation for Background Estimation Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classical principal component analysis (PCA) is not robust to the presence of\nsparse outliers in the data. The use of the $\\ell_1$ norm in the Robust PCA\n(RPCA) method successfully eliminates the weakness of PCA in separating the\nsparse outliers. In this paper, by sticking a simple weight to the Frobenius\nnorm, we propose a weighted low rank (WLR) method to avoid the often\ncomputationally expensive algorithms relying on the $\\ell_1$ norm. As a proof\nof concept, a background estimation model has been presented and compared with\ntwo $\\ell_1$ norm minimization algorithms. We illustrate that as long as a\nsimple weight matrix is inferred from the data, one can use the weighted\nFrobenius norm and achieve the same or better performance.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jul 2017 08:30:23 GMT"}], "update_date": "2017-07-07", "authors_parsed": [["Dutta", "Aritra", ""], ["Li", "Xin", ""]]}, {"id": "1707.01786", "submitter": "Yinchong Yang", "authors": "Yinchong Yang, Denis Krompass, Volker Tresp", "title": "Tensor-Train Recurrent Neural Networks for Video Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Recurrent Neural Networks and their variants have shown promising\nperformances in sequence modeling tasks such as Natural Language Processing.\nThese models, however, turn out to be impractical and difficult to train when\nexposed to very high-dimensional inputs due to the large input-to-hidden weight\nmatrix. This may have prevented RNNs' large-scale application in tasks that\ninvolve very high input dimensions such as video modeling; current approaches\nreduce the input dimensions using various feature extractors. To address this\nchallenge, we propose a new, more general and efficient approach by factorizing\nthe input-to-hidden weight matrix using Tensor-Train decomposition which is\ntrained simultaneously with the weights themselves. We test our model on\nclassification tasks using multiple real-world video datasets and achieve\ncompetitive performances with state-of-the-art models, even though our model\narchitecture is orders of magnitude less complex. We believe that the proposed\napproach provides a novel and fundamental building block for modeling\nhigh-dimensional sequential data with RNN architectures and opens up many\npossibilities to transfer the expressive and advanced architectures from other\ndomains such as NLP to modeling high-dimensional sequential data.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jul 2017 13:43:14 GMT"}], "update_date": "2017-07-07", "authors_parsed": [["Yang", "Yinchong", ""], ["Krompass", "Denis", ""], ["Tresp", "Volker", ""]]}, {"id": "1707.01836", "submitter": "Awni Hannun", "authors": "Pranav Rajpurkar, Awni Y. Hannun, Masoumeh Haghpanahi, Codie Bourn and\n  Andrew Y. Ng", "title": "Cardiologist-Level Arrhythmia Detection with Convolutional Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop an algorithm which exceeds the performance of board certified\ncardiologists in detecting a wide range of heart arrhythmias from\nelectrocardiograms recorded with a single-lead wearable monitor. We build a\ndataset with more than 500 times the number of unique patients than previously\nstudied corpora. On this dataset, we train a 34-layer convolutional neural\nnetwork which maps a sequence of ECG samples to a sequence of rhythm classes.\nCommittees of board-certified cardiologists annotate a gold standard test set\non which we compare the performance of our model to that of 6 other individual\ncardiologists. We exceed the average cardiologist performance in both recall\n(sensitivity) and precision (positive predictive value).\n", "versions": [{"version": "v1", "created": "Thu, 6 Jul 2017 15:42:46 GMT"}], "update_date": "2017-07-07", "authors_parsed": [["Rajpurkar", "Pranav", ""], ["Hannun", "Awni Y.", ""], ["Haghpanahi", "Masoumeh", ""], ["Bourn", "Codie", ""], ["Ng", "Andrew Y.", ""]]}, {"id": "1707.01877", "submitter": "Kathl\\'en Kohn", "authors": "Kathl\\'en Kohn, Bernd Sturmfels and Matthew Trager", "title": "Changing Views on Curves and Surfaces", "comments": "31 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.AG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual events in computer vision are studied from the perspective of\nalgebraic geometry. Given a sufficiently general curve or surface in 3-space,\nwe consider the image or contour curve that arises by projecting from a\nviewpoint. Qualitative changes in that curve occur when the viewpoint crosses\nthe visual event surface. We examine the components of this ruled surface, and\nobserve that these coincide with the iterated singular loci of the coisotropic\nhypersurfaces associated with the original curve or surface. We derive\nformulas, due to Salmon and Petitjean, for the degrees of these surfaces, and\nshow how to compute exact representations for all visual event surfaces using\nalgebraic methods.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jul 2017 17:34:27 GMT"}, {"version": "v2", "created": "Sat, 11 Nov 2017 10:57:40 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Kohn", "Kathl\u00e9n", ""], ["Sturmfels", "Bernd", ""], ["Trager", "Matthew", ""]]}, {"id": "1707.01922", "submitter": "Kuan-Chuan Peng", "authors": "Kuan-Chuan Peng, Ziyan Wu, Jan Ernst", "title": "Zero-Shot Deep Domain Adaptation", "comments": "This paper is accepted to the European Conference on Computer Vision\n  (ECCV), 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain adaptation is an important tool to transfer knowledge about a task\n(e.g. classification) learned in a source domain to a second, or target domain.\nCurrent approaches assume that task-relevant target-domain data is available\nduring training. We demonstrate how to perform domain adaptation when no such\ntask-relevant target-domain data is available. To tackle this issue, we propose\nzero-shot deep domain adaptation (ZDDA), which uses privileged information from\ntask-irrelevant dual-domain pairs. ZDDA learns a source-domain representation\nwhich is not only tailored for the task of interest but also close to the\ntarget-domain representation. Therefore, the source-domain task of interest\nsolution (e.g. a classifier for classification tasks) which is jointly trained\nwith the source-domain representation can be applicable to both the source and\ntarget representations. Using the MNIST, Fashion-MNIST, NIST, EMNIST, and SUN\nRGB-D datasets, we show that ZDDA can perform domain adaptation in\nclassification tasks without access to task-relevant target-domain training\ndata. We also extend ZDDA to perform sensor fusion in the SUN RGB-D scene\nclassification task by simulating task-relevant target-domain representations\nwith task-relevant source-domain data. To the best of our knowledge, ZDDA is\nthe first domain adaptation and sensor fusion method which requires no\ntask-relevant target-domain data. The underlying principle is not particular to\ncomputer vision data, but should be extensible to other domains.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jul 2017 18:09:36 GMT"}, {"version": "v2", "created": "Thu, 16 Nov 2017 15:06:43 GMT"}, {"version": "v3", "created": "Wed, 18 Apr 2018 14:00:17 GMT"}, {"version": "v4", "created": "Mon, 23 Jul 2018 15:31:54 GMT"}, {"version": "v5", "created": "Tue, 24 Jul 2018 13:45:23 GMT"}], "update_date": "2018-07-25", "authors_parsed": [["Peng", "Kuan-Chuan", ""], ["Wu", "Ziyan", ""], ["Ernst", "Jan", ""]]}, {"id": "1707.01976", "submitter": "Nhan Duy Truong", "authors": "Nhan Duy Truong, Anh Duy Nguyen, Levin Kuhlmann, Mohammad Reza\n  Bonyadi, Jiawei Yang, Omid Kavehei", "title": "A Generalised Seizure Prediction with Convolutional Neural Networks for\n  Intracranial and Scalp Electroencephalogram Data Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Seizure prediction has attracted a growing attention as one of the most\nchallenging predictive data analysis efforts in order to improve the life of\npatients living with drug-resistant epilepsy and tonic seizures. Many\noutstanding works have been reporting great results in providing a sensible\nindirect (warning systems) or direct (interactive neural-stimulation) control\nover refractory seizures, some of which achieved high performance. However,\nmany works put heavily handcraft feature extraction and/or carefully tailored\nfeature engineering to each patient to achieve very high sensitivity and low\nfalse prediction rate for a particular dataset. This limits the benefit of\ntheir approaches if a different dataset is used. In this paper we apply\nConvolutional Neural Networks (CNNs) on different intracranial and scalp\nelectroencephalogram (EEG) datasets and proposed a generalized retrospective\nand patient-specific seizure prediction method. We use Short-Time Fourier\nTransform (STFT) on 30-second EEG windows with 50% overlapping to extract\ninformation in both frequency and time domains. A standardization step is then\napplied on STFT components across the whole frequency range to prevent high\nfrequencies features being influenced by those at lower frequencies. A\nconvolutional neural network model is used for both feature extraction and\nclassification to separate preictal segments from interictal ones. The proposed\napproach achieves sensitivity of 81.4%, 81.2%, 82.3% and false prediction rate\n(FPR) of 0.06/h, 0.16/h, 0.22/h on Freiburg Hospital intracranial EEG (iEEG)\ndataset, Children's Hospital of Boston-MIT scalp EEG (sEEG) dataset, and Kaggle\nAmerican Epilepsy Society Seizure Prediction Challenge's dataset, respectively.\nOur prediction method is also statistically better than an unspecific random\npredictor for most of patients in all three datasets.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jul 2017 21:54:55 GMT"}, {"version": "v2", "created": "Wed, 6 Dec 2017 11:48:22 GMT"}], "update_date": "2017-12-07", "authors_parsed": [["Truong", "Nhan Duy", ""], ["Nguyen", "Anh Duy", ""], ["Kuhlmann", "Levin", ""], ["Bonyadi", "Mohammad Reza", ""], ["Yang", "Jiawei", ""], ["Kavehei", "Omid", ""]]}, {"id": "1707.01992", "submitter": "Wenqi Li", "authors": "Wenqi Li, Guotai Wang, Lucas Fidon, Sebastien Ourselin, M. Jorge\n  Cardoso, Tom Vercauteren", "title": "On the Compactness, Efficiency, and Representation of 3D Convolutional\n  Networks: Brain Parcellation as a Pretext Task", "comments": "Paper accepted at IPMI 2017", "journal-ref": null, "doi": "10.1007/978-3-319-59050-9_28", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks are powerful tools for learning visual\nrepresentations from images. However, designing efficient deep architectures to\nanalyse volumetric medical images remains challenging. This work investigates\nefficient and flexible elements of modern convolutional networks such as\ndilated convolution and residual connection. With these essential building\nblocks, we propose a high-resolution, compact convolutional network for\nvolumetric image segmentation. To illustrate its efficiency of learning 3D\nrepresentation from large-scale image data, the proposed network is validated\nwith the challenging task of parcellating 155 neuroanatomical structures from\nbrain MR images. Our experiments show that the proposed network architecture\ncompares favourably with state-of-the-art volumetric segmentation networks\nwhile being an order of magnitude more compact. We consider the brain\nparcellation task as a pretext task for volumetric image segmentation; our\ntrained network potentially provides a good starting point for transfer\nlearning. Additionally, we show the feasibility of voxel-level uncertainty\nestimation using a sampling approximation through dropout.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jul 2017 23:13:03 GMT"}], "update_date": "2017-07-10", "authors_parsed": [["Li", "Wenqi", ""], ["Wang", "Guotai", ""], ["Fidon", "Lucas", ""], ["Ourselin", "Sebastien", ""], ["Cardoso", "M. Jorge", ""], ["Vercauteren", "Tom", ""]]}, {"id": "1707.02022", "submitter": "Ibrahim Sadek", "authors": "Ibrahim Sadek, Mohamed Elawady, Abd El Rahman Shabayek", "title": "Automatic Classification of Bright Retinal Lesions via Deep Network\n  Features", "comments": "Preprint submitted to Journal of Medical Imaging | SPIE (Tue, Jul 28,\n  2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The diabetic retinopathy is timely diagonalized through color eye fundus\nimages by experienced ophthalmologists, in order to recognize potential retinal\nfeatures and identify early-blindness cases. In this paper, it is proposed to\nextract deep features from the last fully-connected layer of, four different,\npre-trained convolutional neural networks. These features are then feeded into\na non-linear classifier to discriminate three-class diabetic cases, i.e.,\nnormal, exudates, and drusen. Averaged across 1113 color retinal images\ncollected from six publicly available annotated datasets, the deep features\napproach perform better than the classical bag-of-words approach. The proposed\napproaches have an average accuracy between 91.23% and 92.00% with more than\n13% improvement over the traditional state of art methods.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jul 2017 02:56:41 GMT"}, {"version": "v2", "created": "Tue, 18 Jul 2017 08:40:26 GMT"}, {"version": "v3", "created": "Fri, 28 Jul 2017 06:32:15 GMT"}], "update_date": "2017-07-31", "authors_parsed": [["Sadek", "Ibrahim", ""], ["Elawady", "Mohamed", ""], ["Shabayek", "Abd El Rahman", ""]]}, {"id": "1707.02051", "submitter": "Hao Yan", "authors": "Song Yuheng, Yan Hao", "title": "Image Segmentation Algorithms Overview", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The technology of image segmentation is widely used in medical image\nprocessing, face recognition pedestrian detection, etc. The current image\nsegmentation techniques include region-based segmentation, edge detection\nsegmentation, segmentation based on clustering, segmentation based on\nweakly-supervised learning in CNN, etc. This paper analyzes and summarizes\nthese algorithms of image segmentation, and compares the advantages and\ndisadvantages of different algorithms. Finally, we make a prediction of the\ndevelopment trend of image segmentation with the combination of these\nalgorithms.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jul 2017 06:27:54 GMT"}], "update_date": "2017-07-10", "authors_parsed": [["Yuheng", "Song", ""], ["Hao", "Yan", ""]]}, {"id": "1707.02069", "submitter": "Mo Shan", "authors": "Mo Shan and Nikolay Atanasov", "title": "A spatiotemporal model with visual attention for video classification", "comments": "Accepted by Robotics: Science and Systems 2017 Workshop on\n  Articulated Model Tracking", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High level understanding of sequential visual input is important for safe and\nstable autonomy, especially in localization and object detection. While\ntraditional object classification and tracking approaches are specifically\ndesigned to handle variations in rotation and scale, current state-of-the-art\napproaches based on deep learning achieve better performance. This paper\nfocuses on developing a spatiotemporal model to handle videos containing moving\nobjects with rotation and scale changes. Built on models that combine\nConvolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs) to\nclassify sequential data, this work investigates the effectiveness of\nincorporating attention modules in the CNN stage for video classification. The\nsuperiority of the proposed spatiotemporal model is demonstrated on the Moving\nMNIST dataset augmented with rotation and scaling.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jul 2017 08:12:27 GMT"}, {"version": "v2", "created": "Mon, 24 Jul 2017 01:53:20 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Shan", "Mo", ""], ["Atanasov", "Nikolay", ""]]}, {"id": "1707.02112", "submitter": "Jingkuan Song Dr.", "authors": "Jingkuan Song, Tao He, Hangbo Fan, Lianli Gao", "title": "Deep Discrete Hashing with Self-supervised Pairwise Labels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hashing methods have been widely used for applications of large-scale image\nretrieval and classification. Non-deep hashing methods using handcrafted\nfeatures have been significantly outperformed by deep hashing methods due to\ntheir better feature representation and end-to-end learning framework. However,\nthe most striking successes in deep hashing have mostly involved discriminative\nmodels, which require labels. In this paper, we propose a novel unsupervised\ndeep hashing method, named Deep Discrete Hashing (DDH), for large-scale image\nretrieval and classification. In the proposed framework, we address two main\nproblems: 1) how to directly learn discrete binary codes? 2) how to equip the\nbinary representation with the ability of accurate image retrieval and\nclassification in an unsupervised way? We resolve these problems by introducing\nan intermediate variable and a loss function steering the learning process,\nwhich is based on the neighborhood structure in the original space.\nExperimental results on standard datasets (CIFAR-10, NUS-WIDE, and Oxford-17)\ndemonstrate that our DDH significantly outperforms existing hashing methods by\nlarge margin in terms of~mAP for image retrieval and object recognition. Code\nis available at \\url{https://github.com/htconquer/ddh}.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jul 2017 10:36:29 GMT"}], "update_date": "2017-07-10", "authors_parsed": [["Song", "Jingkuan", ""], ["He", "Tao", ""], ["Fan", "Hangbo", ""], ["Gao", "Lianli", ""]]}, {"id": "1707.02120", "submitter": "Yoni Choukroun", "authors": "Yoni Choukroun and Gautam Pai and Ron Kimmel", "title": "Sparse Approximation of 3D Meshes using the Spectral Geometry of the\n  Hamiltonian Operator", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The discrete Laplace operator is ubiquitous in spectral shape analysis, since\nits eigenfunctions are provably optimal in representing smooth functions\ndefined on the surface of the shape. Indeed, subspaces defined by its\neigenfunctions have been utilized for shape compression, treating the\ncoordinates as smooth functions defined on the given surface. However, surfaces\nof shapes in nature often contain geometric structures for which the general\nsmoothness assumption may fail to hold. At the other end, some explicit mesh\ncompression algorithms utilize the order by which vertices that represent the\nsurface are traversed, a property which has been ignored in spectral\napproaches. Here, we incorporate the order of vertices into an operator that\ndefines a novel spectral domain. We propose a method for representing 3D meshes\nusing the spectral geometry of the Hamiltonian operator, integrated within a\nsparse approximation framework. We adapt the concept of a potential function\nfrom quantum physics and incorporate vertex ordering information into the\npotential, yielding a novel data-dependent operator. The potential function\nmodifies the spectral geometry of the Laplacian to focus on regions with finer\ndetails of the given surface. By sparsely encoding the geometry of the shape\nusing the proposed data-dependent basis, we improve compression performance\ncompared to previous results that use the standard Laplacian basis and spectral\ngraph wavelets.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jul 2017 11:24:11 GMT"}, {"version": "v2", "created": "Sat, 12 May 2018 20:20:59 GMT"}], "update_date": "2018-05-15", "authors_parsed": [["Choukroun", "Yoni", ""], ["Pai", "Gautam", ""], ["Kimmel", "Ron", ""]]}, {"id": "1707.02131", "submitter": "Sounak Dey", "authors": "Sounak Dey, Anjan Dutta, J. Ignacio Toledo, Suman K. Ghosh, Josep\n  Llados and Umapada Pal", "title": "SigNet: Convolutional Siamese Network for Writer Independent Offline\n  Signature Verification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Offline signature verification is one of the most challenging tasks in\nbiometrics and document forensics. Unlike other verification problems, it needs\nto model minute but critical details between genuine and forged signatures,\nbecause a skilled falsification might often resembles the real signature with\nsmall deformation. This verification task is even harder in writer independent\nscenarios which is undeniably fiscal for realistic cases. In this paper, we\nmodel an offline writer independent signature verification task with a\nconvolutional Siamese network. Siamese networks are twin networks with shared\nweights, which can be trained to learn a feature space where similar\nobservations are placed in proximity. This is achieved by exposing the network\nto a pair of similar and dissimilar observations and minimizing the Euclidean\ndistance between similar pairs while simultaneously maximizing it between\ndissimilar pairs. Experiments conducted on cross-domain datasets emphasize the\ncapability of our network to model forgery in different languages (scripts) and\nhandwriting styles. Moreover, our designed Siamese network, named SigNet,\nexceeds the state-of-the-art results on most of the benchmark signature\ndatasets, which paves the way for further research in this direction.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jul 2017 11:56:51 GMT"}, {"version": "v2", "created": "Sat, 30 Sep 2017 17:13:46 GMT"}], "update_date": "2017-10-03", "authors_parsed": [["Dey", "Sounak", ""], ["Dutta", "Anjan", ""], ["Toledo", "J. Ignacio", ""], ["Ghosh", "Suman K.", ""], ["Llados", "Josep", ""], ["Pal", "Umapada", ""]]}, {"id": "1707.02191", "submitter": "Erik Bekkers", "authors": "M.H.J. Janssen, A.J.E.M. Janssen, E.J. Bekkers, J. Olivan Bescos and\n  R. Duits", "title": "Design and Processing of Invertible Orientation Scores of 3D Images for\n  Enhancement of Complex Vasculature", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The enhancement and detection of elongated structures in noisy image data is\nrelevant for many biomedical imaging applications. To handle complex crossing\nstructures in 2D images, 2D orientation scores $U: \\mathbb{R} ^ 2\\times S ^ 1\n\\rightarrow \\mathbb{C}$ were introduced, which already showed their use in a\nvariety of applications. Here we extend this work to 3D orientation scores $U:\n\\mathbb{R} ^ 3 \\times S ^ 2\\rightarrow \\mathbb{C}$. First, we construct the\norientation score from a given dataset, which is achieved by an invertible\ncoherent state type of transform. For this transformation we introduce 3D\nversions of the 2D cake-wavelets, which are complex wavelets that can\nsimultaneously detect oriented structures and oriented edges. Here we introduce\ntwo types of cake-wavelets, the first uses a discrete Fourier transform, the\nsecond is designed in the 3D generalized Zernike basis, allowing us to\ncalculate analytical expressions for the spatial filters. Finally, we show two\napplications of the orientation score transformation. In the first application\nwe propose an extension of crossing-preserving coherence enhancing diffusion\nvia our invertible orientation scores of 3D images which we apply to real\nmedical image data. In the second one we develop a new tubularity measure using\n3D orientation scores and apply the tubularity measure to both artificial and\nreal medical data.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jul 2017 14:22:26 GMT"}, {"version": "v2", "created": "Mon, 31 Jul 2017 11:56:27 GMT"}, {"version": "v3", "created": "Mon, 27 Nov 2017 07:58:53 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Janssen", "M. H. J.", ""], ["Janssen", "A. J. E. M.", ""], ["Bekkers", "E. J.", ""], ["Bescos", "J. Olivan", ""], ["Duits", "R.", ""]]}, {"id": "1707.02194", "submitter": "Sohrab Ferdowsi", "authors": "Sohrab Ferdowsi, Slava Voloshynovskiy, Dimche Kostadinov", "title": "A multi-layer image representation using Regularized Residual\n  Quantization: application to compression and denoising", "comments": "At the International Conference on Image Processing 2017 (ICIP'17),\n  Beijing, China", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A learning-based framework for representation of domain-specific images is\nproposed where joint compression and denoising can be done using a VQ-based\nmulti-layer network. While it learns to compress the images from a training\nset, the compression performance is very well generalized on images from a test\nset. Moreover, when fed with noisy versions of the test set, since it has\npriors from clean images, the network also efficiently denoises the test images\nduring the reconstruction. The proposed framework is a regularized version of\nthe Residual Quantization (RQ) where at each stage, the quantization error from\nthe previous stage is further quantized. Instead of codebook learning from the\nk-means which over-trains for high-dimensional vectors, we show that only\ngenerating the codewords from a random, but properly regularized distribution\nsuffices to compress the images globally and without the need to resort to\npatch-based division of images. The experiments are done on the\n\\textit{CroppedYale-B} set of facial images and the method is compared with the\nJPEG-2000 codec for compression and BM3D for denoising, showing promising\nresults.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jul 2017 14:28:21 GMT"}], "update_date": "2017-07-10", "authors_parsed": [["Ferdowsi", "Sohrab", ""], ["Voloshynovskiy", "Slava", ""], ["Kostadinov", "Dimche", ""]]}, {"id": "1707.02237", "submitter": "Shanxin Yuan", "authors": "Shanxin Yuan, Qi Ye, Guillermo Garcia-Hernando, Tae-Kyun Kim", "title": "The 2017 Hands in the Million Challenge on 3D Hand Pose Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the 2017 Hands in the Million Challenge, a public competition\ndesigned for the evaluation of the task of 3D hand pose estimation. The goal of\nthis challenge is to assess how far is the state of the art in terms of solving\nthe problem of 3D hand pose estimation as well as detect major failure and\nstrength modes of both systems and evaluation metrics that can help to identify\nfuture research directions. The challenge follows up the recent publication of\nBigHand2.2M and First-Person Hand Action datasets, which have been designed to\nexhaustively cover multiple hand, viewpoint, hand articulation, and occlusion.\nThe challenge consists of a standardized dataset, an evaluation protocol for\ntwo different tasks, and a public competition. In this document we describe the\ndifferent aspects of the challenge and, jointly with the results of the\nparticipants, it will be presented at the 3rd International Workshop on\nObserving and Understanding Hands in Action, HANDS 2017, with ICCV 2017.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jul 2017 15:46:58 GMT"}], "update_date": "2017-07-10", "authors_parsed": [["Yuan", "Shanxin", ""], ["Ye", "Qi", ""], ["Garcia-Hernando", "Guillermo", ""], ["Kim", "Tae-Kyun", ""]]}, {"id": "1707.02240", "submitter": "Matteo Fabbri", "authors": "Matteo Fabbri and Simone Calderara and Rita Cucchiara", "title": "Generative Adversarial Models for People Attribute Recognition in\n  Surveillance", "comments": "Accepted as oral presentation at AVSS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a deep architecture for detecting people attributes\n(e.g. gender, race, clothing ...) in surveillance contexts. Our proposal\nexplicitly deal with poor resolution and occlusion issues that often occur in\nsurveillance footages by enhancing the images by means of Deep Convolutional\nGenerative Adversarial Networks (DCGAN). Experiments show that by combining\nboth our Generative Reconstruction and Deep Attribute Classification Network we\ncan effectively extract attributes even when resolution is poor and in presence\nof strong occlusions up to 80\\% of the whole person figure.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jul 2017 15:51:51 GMT"}], "update_date": "2017-07-10", "authors_parsed": [["Fabbri", "Matteo", ""], ["Calderara", "Simone", ""], ["Cucchiara", "Rita", ""]]}, {"id": "1707.02244", "submitter": "Sophie Fosson", "authors": "Attilio Fiandrotti, Sophie M. Fosson, Chiara Ravazzi, and Enrico Magli", "title": "GPU-Accelerated Algorithms for Compressed Signals Recovery with\n  Application to Astronomical Imagery Deblurring", "comments": null, "journal-ref": null, "doi": "10.1080/01431161.2017.1356489", "report-no": null, "categories": "cs.DC astro-ph.IM cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Compressive sensing promises to enable bandwidth-efficient on-board\ncompression of astronomical data by lifting the encoding complexity from the\nsource to the receiver. The signal is recovered off-line, exploiting GPUs\nparallel computation capabilities to speedup the reconstruction process.\nHowever, inherent GPU hardware constraints limit the size of the recoverable\nsignal and the speedup practically achievable. In this work, we design parallel\nalgorithms that exploit the properties of circulant matrices for efficient\nGPU-accelerated sparse signals recovery. Our approach reduces the memory\nrequirements, allowing us to recover very large signals with limited memory. In\naddition, it achieves a tenfold signal recovery speedup thanks to ad-hoc\nparallelization of matrix-vector multiplications and matrix inversions.\nFinally, we practically demonstrate our algorithms in a typical application of\ncirculant matrices: deblurring a sparse astronomical image in the compressed\ndomain.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jul 2017 15:55:28 GMT"}], "update_date": "2018-02-14", "authors_parsed": [["Fiandrotti", "Attilio", ""], ["Fosson", "Sophie M.", ""], ["Ravazzi", "Chiara", ""], ["Magli", "Enrico", ""]]}, {"id": "1707.02290", "submitter": "Chunhua Shen", "authors": "Hao Lu, Zhiguo Cao, Yang Xiao, Bohan Zhuang, Chunhua Shen", "title": "TasselNet: Counting maize tassels in the wild via local counts\n  regression network", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurately counting maize tassels is important for monitoring the growth\nstatus of maize plants. This tedious task, however, is still mainly done by\nmanual efforts. In the context of modern plant phenotyping, automating this\ntask is required to meet the need of large-scale analysis of genotype and\nphenotype. In recent years, computer vision technologies have experienced a\nsignificant breakthrough due to the emergence of large-scale datasets and\nincreased computational resources. Naturally image-based approaches have also\nreceived much attention in plant-related studies. Yet a fact is that most\nimage-based systems for plant phenotyping are deployed under controlled\nlaboratory environment. When transferring the application scenario to\nunconstrained in-field conditions, intrinsic and extrinsic variations in the\nwild pose great challenges for accurate counting of maize tassels, which goes\nbeyond the ability of conventional image processing techniques. This calls for\nfurther robust computer vision approaches to address in-field variations. This\npaper studies the in-field counting problem of maize tassels. To our knowledge,\nthis is the first time that a plant-related counting problem is considered\nusing computer vision technologies under unconstrained field-based environment.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jul 2017 02:47:06 GMT"}], "update_date": "2017-07-11", "authors_parsed": [["Lu", "Hao", ""], ["Cao", "Zhiguo", ""], ["Xiao", "Yang", ""], ["Zhuang", "Bohan", ""], ["Shen", "Chunhua", ""]]}, {"id": "1707.02309", "submitter": "Chao Ma", "authors": "Chao Ma, Jia-Bin Huang, Xiaokang Yang, Ming-Hsuan Yang", "title": "Adaptive Correlation Filters with Long-Term and Short-Term Memory for\n  Object Tracking", "comments": "IJCV 2018, Project page:\n  https://sites.google.com/site/chaoma99/cf-lstm", "journal-ref": null, "doi": "10.1007/s11263-018-1076-4", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object tracking is challenging as target objects often undergo drastic\nappearance changes over time. Recently, adaptive correlation filters have been\nsuccessfully applied to object tracking. However, tracking algorithms relying\non highly adaptive correlation filters are prone to drift due to noisy updates.\nMoreover, as these algorithms do not maintain long-term memory of target\nappearance, they cannot recover from tracking failures caused by heavy\nocclusion or target disappearance in the camera view. In this paper, we propose\nto learn multiple adaptive correlation filters with both long-term and\nshort-term memory of target appearance for robust object tracking. First, we\nlearn a kernelized correlation filter with an aggressive learning rate for\nlocating target objects precisely. We take into account the appropriate size of\nsurrounding context and the feature representations. Second, we learn a\ncorrelation filter over a feature pyramid centered at the estimated target\nposition for predicting scale changes. Third, we learn a complementary\ncorrelation filter with a conservative learning rate to maintain long-term\nmemory of target appearance. We use the output responses of this long-term\nfilter to determine if tracking failure occurs. In the case of tracking\nfailures, we apply an incrementally learned detector to recover the target\nposition in a sliding window fashion. Extensive experimental results on\nlarge-scale benchmark datasets demonstrate that the proposed algorithm performs\nfavorably against the state-of-the-art methods in terms of efficiency,\naccuracy, and robustness.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jul 2017 18:00:34 GMT"}, {"version": "v2", "created": "Fri, 23 Mar 2018 09:39:10 GMT"}], "update_date": "2018-03-26", "authors_parsed": [["Ma", "Chao", ""], ["Huang", "Jia-Bin", ""], ["Yang", "Xiaokang", ""], ["Yang", "Ming-Hsuan", ""]]}, {"id": "1707.02319", "submitter": "Yang Yang", "authors": "Yang Yang, Shengcai Liao, Zhen Lei, Stan Z. Li", "title": "Learning Efficient Image Representation for Person Re-Identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Color names based image representation is successfully used in person\nre-identification, due to the advantages of being compact, intuitively\nunderstandable as well as being robust to photometric variance. However, there\nexists the diversity between underlying distribution of color names' RGB values\nand that of image pixels' RGB values, which may lead to inaccuracy when\ndirectly comparing them in Euclidean space. In this paper, we propose a new\nmethod named soft Gaussian mapping (SGM) to address this problem. We model the\ndiscrepancies between color names and pixels using a Gaussian and utilize the\ninverse of covariance matrix to bridge the gap between them. Based on SGM, an\nimage could be converted to several soft Gaussian maps. In each soft Gaussian\nmap, we further seek to establish stable and robust descriptors within a local\nregion through a max pooling operation. Then, a robust image representation\nbased on color names is obtained by concatenating the statistical descriptors\nin each stripe. When labeled data are available, one discriminative subspace\nprojection matrix is learned to build efficient representations of an image via\ncross-view coupling learning. Experiments on the public datasets - VIPeR,\nPRID450S and CUHK03, demonstrate the effectiveness of our method.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jul 2017 18:05:48 GMT"}], "update_date": "2017-07-11", "authors_parsed": [["Yang", "Yang", ""], ["Liao", "Shengcai", ""], ["Lei", "Zhen", ""], ["Li", "Stan Z.", ""]]}, {"id": "1707.02336", "submitter": "John McKay", "authors": "John McKay, Raghu G. Raj, Vishal Monga", "title": "Fast Stochastic Hierarchical Bayesian MAP for Tomographic Imaging", "comments": "5 Pages, 4 Figures, Conference (Accepted to Asilomar 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Any image recovery algorithm attempts to achieve the highest quality\nreconstruction in a timely manner. The former can be achieved in several ways,\namong which are by incorporating Bayesian priors that exploit natural image\ntendencies to cue in on relevant phenomena. The Hierarchical Bayesian MAP\n(HB-MAP) is one such approach which is known to produce compelling results\nalbeit at a substantial computational cost. We look to provide further analysis\nand insights into what makes the HB-MAP work. While retaining the proficient\nnature of HB-MAP's Type-I estimation, we propose a stochastic\napproximation-based approach to Type-II estimation. The resulting algorithm,\nfast stochastic HB-MAP (fsHBMAP), takes dramatically fewer operations while\nretaining high reconstruction quality. We employ our fsHBMAP scheme towards the\nproblem of tomographic imaging and demonstrate that fsHBMAP furnishes promising\nresults when compared to many competing methods.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jul 2017 18:55:57 GMT"}], "update_date": "2017-07-11", "authors_parsed": [["McKay", "John", ""], ["Raj", "Raghu G.", ""], ["Monga", "Vishal", ""]]}, {"id": "1707.02356", "submitter": "Pichao Wang", "authors": "Chuankun Li and Pichao Wang and Shuang Wang and Yonghong Hou and\n  Wanqing Li", "title": "Skeleton-based Action Recognition Using LSTM and CNN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent methods based on 3D skeleton data have achieved outstanding\nperformance due to its conciseness, robustness, and view-independent\nrepresentation. With the development of deep learning, Convolutional Neural\nNetworks (CNN) and Long Short Term Memory (LSTM)-based learning methods have\nachieved promising performance for action recognition. However, for CNN-based\nmethods, it is inevitable to loss temporal information when a sequence is\nencoded into images. In order to capture as much spatial-temporal information\nas possible, LSTM and CNN are adopted to conduct effective recognition with\nlater score fusion. In addition, experimental results show that the score\nfusion between CNN and LSTM performs better than that between LSTM and LSTM for\nthe same feature. Our method achieved state-of-the-art results on NTU RGB+D\ndatasets for 3D human action analysis. The proposed method achieved 87.40% in\nterms of accuracy and ranked $1^{st}$ place in Large Scale 3D Human Activity\nAnalysis Challenge in Depth Videos.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jul 2017 11:06:26 GMT"}], "update_date": "2017-07-11", "authors_parsed": [["Li", "Chuankun", ""], ["Wang", "Pichao", ""], ["Wang", "Shuang", ""], ["Hou", "Yonghong", ""], ["Li", "Wanqing", ""]]}, {"id": "1707.02392", "submitter": "Panos Achlioptas", "authors": "Panos Achlioptas, Olga Diamanti, Ioannis Mitliagkas, Leonidas Guibas", "title": "Learning Representations and Generative Models for 3D Point Clouds", "comments": null, "journal-ref": "35th International Conference on Machine Learning (ICML), 2018", "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Three-dimensional geometric data offer an excellent domain for studying\nrepresentation learning and generative modeling. In this paper, we look at\ngeometric data represented as point clouds. We introduce a deep AutoEncoder\n(AE) network with state-of-the-art reconstruction quality and generalization\nability. The learned representations outperform existing methods on 3D\nrecognition tasks and enable shape editing via simple algebraic manipulations,\nsuch as semantic part editing, shape analogies and shape interpolation, as well\nas shape completion. We perform a thorough study of different generative models\nincluding GANs operating on the raw point clouds, significantly improved GANs\ntrained in the fixed latent space of our AEs, and Gaussian Mixture Models\n(GMMs). To quantitatively evaluate generative models we introduce measures of\nsample fidelity and diversity based on matchings between sets of point clouds.\nInterestingly, our evaluation of generalization, fidelity and diversity reveals\nthat GMMs trained in the latent space of our AEs yield the best results\noverall.\n", "versions": [{"version": "v1", "created": "Sat, 8 Jul 2017 03:44:49 GMT"}, {"version": "v2", "created": "Tue, 5 Dec 2017 05:35:30 GMT"}, {"version": "v3", "created": "Tue, 12 Jun 2018 04:27:00 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Achlioptas", "Panos", ""], ["Diamanti", "Olga", ""], ["Mitliagkas", "Ioannis", ""], ["Guibas", "Leonidas", ""]]}, {"id": "1707.02402", "submitter": "Joseph Suarez", "authors": "Joseph Suarez and Clare Zhu", "title": "Effective Approaches to Batch Parallelization for Dynamic Neural Network\n  Architectures", "comments": "Code at https://github.com/jsuarez5341/Efficient-Dynamic-Batching", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a simple dynamic batching approach applicable to a large class of\ndynamic architectures that consistently yields speedups of over 10x. We provide\nperformance bounds when the architecture is not known a priori and a stronger\nbound in the special case where the architecture is a predetermined balanced\ntree. We evaluate our approach on Johnson et al.'s recent visual question\nanswering (VQA) result of his CLEVR dataset by Inferring and Executing Programs\n(IEP). We also evaluate on sparsely gated mixture of experts layers and achieve\nspeedups of up to 1000x over the naive implementation.\n", "versions": [{"version": "v1", "created": "Sat, 8 Jul 2017 06:04:31 GMT"}], "update_date": "2017-07-11", "authors_parsed": [["Suarez", "Joseph", ""], ["Zhu", "Clare", ""]]}, {"id": "1707.02406", "submitter": "Tianyi Zhao", "authors": "Tianyi Zhao, Baopeng Zhang, Wei Zhang, Ning Zhou, Jun Yu, Jianping Fan", "title": "Embedding Visual Hierarchy with Deep Networks for Large-Scale Visual\n  Recognition", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2018.2845118", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a level-wise mixture model (LMM) is developed by embedding\nvisual hierarchy with deep networks to support large-scale visual recognition\n(i.e., recognizing thousands or even tens of thousands of object classes), and\na Bayesian approach is used to adapt a pre-trained visual hierarchy\nautomatically to the improvements of deep features (that are used for image and\nobject class representation) when more representative deep networks are learned\nalong the time. Our LMM model can provide an end-to-end approach for jointly\nlearning: (a) the deep networks to extract more discriminative deep features\nfor image and object class representation; (b) the tree classifier for\nrecognizing large numbers of object classes hierarchically; and (c) the visual\nhierarchy adaptation for achieving more accurate indexing of large numbers of\nobject classes hierarchically. By supporting joint learning of the tree\nclassifier, the deep networks and the visual hierarchy adaptation, our LMM\nalgorithm can provide an effective approach for controlling inter-level error\npropagation effectively, thus it can achieve better accuracy rates on\nlarge-scale visual recognition. Our experiments are carried on ImageNet1K and\nImageNet10K image sets, and our LMM algorithm can achieve very competitive\nresults on both the accuracy rates and the computation efficiency as compared\nwith the baseline methods.\n", "versions": [{"version": "v1", "created": "Sat, 8 Jul 2017 07:08:14 GMT"}], "update_date": "2018-08-01", "authors_parsed": [["Zhao", "Tianyi", ""], ["Zhang", "Baopeng", ""], ["Zhang", "Wei", ""], ["Zhou", "Ning", ""], ["Yu", "Jun", ""], ["Fan", "Jianping", ""]]}, {"id": "1707.02427", "submitter": "Hanno Ackermann", "authors": "Florian Kluger, Hanno Ackermann, Michael Ying Yang, Bodo Rosenhahn", "title": "Deep Learning for Vanishing Point Detection Using an Inverse Gnomonic\n  Projection", "comments": "Accepted for publication at German Conference on Pattern Recognition\n  (GCPR) 2017. This research was supported by German Research Foundation DFG\n  within Priority Research Programme 1894 \"Volunteered Geographic Information:\n  Interpretation, Visualisation and Social Computing\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel approach for vanishing point detection from uncalibrated\nmonocular images. In contrast to state-of-the-art, we make no a priori\nassumptions about the observed scene. Our method is based on a convolutional\nneural network (CNN) which does not use natural images, but a Gaussian sphere\nrepresentation arising from an inverse gnomonic projection of lines detected in\nan image. This allows us to rely on synthetic data for training, eliminating\nthe need for labelled images. Our method achieves competitive performance on\nthree horizon estimation benchmark datasets. We further highlight some\nadditional use cases for which our vanishing point detection algorithm can be\nused.\n", "versions": [{"version": "v1", "created": "Sat, 8 Jul 2017 11:41:51 GMT"}, {"version": "v2", "created": "Thu, 16 Nov 2017 13:13:03 GMT"}], "update_date": "2017-11-17", "authors_parsed": [["Kluger", "Florian", ""], ["Ackermann", "Hanno", ""], ["Yang", "Michael Ying", ""], ["Rosenhahn", "Bodo", ""]]}, {"id": "1707.02432", "submitter": "Senthil Yogamani", "authors": "Mennatullah Siam, Sara Elkerdawy, Martin Jagersand, Senthil Yogamani", "title": "Deep Semantic Segmentation for Automated Driving: Taxonomy, Roadmap and\n  Challenges", "comments": "To appear in IEEE ITSC 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic segmentation was seen as a challenging computer vision problem few\nyears ago. Due to recent advancements in deep learning, relatively accurate\nsolutions are now possible for its use in automated driving. In this paper, the\nsemantic segmentation problem is explored from the perspective of automated\ndriving. Most of the current semantic segmentation algorithms are designed for\ngeneric images and do not incorporate prior structure and end goal for\nautomated driving. First, the paper begins with a generic taxonomic survey of\nsemantic segmentation algorithms and then discusses how it fits in the context\nof automated driving. Second, the particular challenges of deploying it into a\nsafety system which needs high level of accuracy and robustness are listed.\nThird, different alternatives instead of using an independent semantic\nsegmentation module are explored. Finally, an empirical evaluation of various\nsemantic segmentation architectures was performed on CamVid dataset in terms of\naccuracy and speed. This paper is a preliminary shorter version of a more\ndetailed survey which is work in progress.\n", "versions": [{"version": "v1", "created": "Sat, 8 Jul 2017 12:32:51 GMT"}, {"version": "v2", "created": "Thu, 3 Aug 2017 14:54:40 GMT"}], "update_date": "2017-08-04", "authors_parsed": [["Siam", "Mennatullah", ""], ["Elkerdawy", "Sara", ""], ["Jagersand", "Martin", ""], ["Yogamani", "Senthil", ""]]}, {"id": "1707.02439", "submitter": "Hwann-Tzong Chen", "authors": "Chia-Jung Chou, Jui-Ting Chien, Hwann-Tzong Chen", "title": "Self Adversarial Training for Human Pose Estimation", "comments": "CVPR 2017 Workshop on Visual Understanding of Humans in Crowd Scene\n  and the 1st Look Into Person (LIP) Challenge", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a deep learning based approach to the problem of human\npose estimation. We employ generative adversarial networks as our learning\nparadigm in which we set up two stacked hourglass networks with the same\narchitecture, one as the generator and the other as the discriminator. The\ngenerator is used as a human pose estimator after the training is done. The\ndiscriminator distinguishes ground-truth heatmaps from generated ones, and\nback-propagates the adversarial loss to the generator. This process enables the\ngenerator to learn plausible human body configurations and is shown to be\nuseful for improving the prediction accuracy.\n", "versions": [{"version": "v1", "created": "Sat, 8 Jul 2017 13:24:44 GMT"}, {"version": "v2", "created": "Tue, 15 Aug 2017 10:10:26 GMT"}], "update_date": "2017-08-16", "authors_parsed": [["Chou", "Chia-Jung", ""], ["Chien", "Jui-Ting", ""], ["Chen", "Hwann-Tzong", ""]]}, {"id": "1707.02477", "submitter": "Yao Wang", "authors": "Yao Wang, Jiangjun Peng, Qian Zhao, Deyu Meng, Yee Leung and Xi-Le\n  Zhao", "title": "Hyperspectral Image Restoration via Total Variation Regularized Low-rank\n  Tensor Decomposition", "comments": "15 pages, 20 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hyperspectral images (HSIs) are often corrupted by a mixture of several types\nof noise during the acquisition process, e.g., Gaussian noise, impulse noise,\ndead lines, stripes, and many others. Such complex noise could degrade the\nquality of the acquired HSIs, limiting the precision of the subsequent\nprocessing. In this paper, we present a novel tensor-based HSI restoration\napproach by fully identifying the intrinsic structures of the clean HSI part\nand the mixed noise part respectively. Specifically, for the clean HSI part, we\nuse tensor Tucker decomposition to describe the global correlation among all\nbands, and an anisotropic spatial-spectral total variation (SSTV)\nregularization to characterize the piecewise smooth structure in both spatial\nand spectral domains. For the mixed noise part, we adopt the $\\ell_1$ norm\nregularization to detect the sparse noise, including stripes, impulse noise,\nand dead pixels. Despite that TV regulariztion has the ability of removing\nGaussian noise, the Frobenius norm term is further used to model heavy Gaussian\nnoise for some real-world scenarios. Then, we develop an efficient algorithm\nfor solving the resulting optimization problem by using the augmented Lagrange\nmultiplier (ALM) method. Finally, extensive experiments on simulated and\nreal-world noise HSIs are carried out to demonstrate the superiority of the\nproposed method over the existing state-of-the-art ones.\n", "versions": [{"version": "v1", "created": "Sat, 8 Jul 2017 18:41:06 GMT"}], "update_date": "2017-07-11", "authors_parsed": [["Wang", "Yao", ""], ["Peng", "Jiangjun", ""], ["Zhao", "Qian", ""], ["Meng", "Deyu", ""], ["Leung", "Yee", ""], ["Zhao", "Xi-Le", ""]]}, {"id": "1707.02485", "submitter": "Zizhao Zhang", "authors": "Zizhao Zhang, Yuanpu Xie, Fuyong Xing, Mason McGough, Lin Yang", "title": "MDNet: A Semantically and Visually Interpretable Medical Image Diagnosis\n  Network", "comments": "CVPR2017 Oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The inability to interpret the model prediction in semantically and visually\nmeaningful ways is a well-known shortcoming of most existing computer-aided\ndiagnosis methods. In this paper, we propose MDNet to establish a direct\nmultimodal mapping between medical images and diagnostic reports that can read\nimages, generate diagnostic reports, retrieve images by symptom descriptions,\nand visualize attention, to provide justifications of the network diagnosis\nprocess. MDNet includes an image model and a language model. The image model is\nproposed to enhance multi-scale feature ensembles and utilization efficiency.\nThe language model, integrated with our improved attention mechanism, aims to\nread and explore discriminative image feature descriptions from reports to\nlearn a direct mapping from sentence words to image pixels. The overall network\nis trained end-to-end by using our developed optimization strategy. Based on a\npathology bladder cancer images and its diagnostic reports (BCIDR) dataset, we\nconduct sufficient experiments to demonstrate that MDNet outperforms\ncomparative baselines. The proposed image model obtains state-of-the-art\nperformance on two CIFAR datasets as well.\n", "versions": [{"version": "v1", "created": "Sat, 8 Jul 2017 19:48:30 GMT"}], "update_date": "2017-07-11", "authors_parsed": [["Zhang", "Zizhao", ""], ["Xie", "Yuanpu", ""], ["Xing", "Fuyong", ""], ["McGough", "Mason", ""], ["Yang", "Lin", ""]]}, {"id": "1707.02554", "submitter": "Zhenghao Chen Mr", "authors": "Zhenghao Chen, Jianlong Zhou, Xiuying Wang", "title": "Visual Analytics of Movement Pattern Based on Time-Spatial Data: A\n  Neural Net Approach", "comments": "submitted to ICONIP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Time-Spatial data plays a crucial role for different fields such as traffic\nmanagement. These data can be collected via devices such as surveillance\nsensors or tracking systems. However, how to efficiently an- alyze and\nvisualize these data to capture essential embedded pattern information is\nbecoming a big challenge today. Classic visualization ap- proaches focus on\nrevealing 2D and 3D spatial information and modeling statistical test. Those\nmethods would easily fail when data become mas- sive. Recent attempts concern\non how to simply cluster data and perform prediction with time-oriented\ninformation. However, those approaches could still be further enhanced as they\nalso have limitations for han- dling massive clusters and labels. In this\npaper, we propose a visualiza- tion methodology for mobility data using\nartificial neural net techniques. This method aggregates three main parts that\nare Back-end Data Model, Neural Net Algorithm including clustering method\nSelf-Organizing Map (SOM) and prediction approach Recurrent Neural Net (RNN)\nfor ex- tracting the features and lastly a solid front-end that displays the\nresults to users with an interactive system. SOM is able to cluster the\nvisiting patterns and detect the abnormal pattern. RNN can perform the predic-\ntion for time series analysis using its dynamic architecture. Furthermore, an\ninteractive system will enable user to interpret the result with graph- ics,\nanimation and 3D model for a close-loop feedback. This method can be\nparticularly applied in two tasks that Commercial-based Promotion and abnormal\ntraffic patterns detection.\n", "versions": [{"version": "v1", "created": "Sun, 9 Jul 2017 10:19:04 GMT"}], "update_date": "2017-07-11", "authors_parsed": [["Chen", "Zhenghao", ""], ["Zhou", "Jianlong", ""], ["Wang", "Xiuying", ""]]}, {"id": "1707.02581", "submitter": "Xavier Gir\\'o-i-Nieto", "authors": "Albert Jimenez, Jose M. Alvarez and Xavier Giro-i-Nieto", "title": "Class-Weighted Convolutional Features for Visual Instance Search", "comments": "To appear in the British Machine Vision Conference (BMVC), September\n  2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image retrieval in realistic scenarios targets large dynamic datasets of\nunlabeled images. In these cases, training or fine-tuning a model every time\nnew images are added to the database is neither efficient nor scalable.\nConvolutional neural networks trained for image classification over large\ndatasets have been proven effective feature extractors for image retrieval. The\nmost successful approaches are based on encoding the activations of\nconvolutional layers, as they convey the image spatial information. In this\npaper, we go beyond this spatial information and propose a local-aware encoding\nof convolutional features based on semantic information predicted in the target\nimage. To this end, we obtain the most discriminative regions of an image using\nClass Activation Maps (CAMs). CAMs are based on the knowledge contained in the\nnetwork and therefore, our approach, has the additional advantage of not\nrequiring external information. In addition, we use CAMs to generate object\nproposals during an unsupervised re-ranking stage after a first fast search.\nOur experiments on two public available datasets for instance retrieval,\nOxford5k and Paris6k, demonstrate the competitiveness of our approach\noutperforming the current state-of-the-art when using off-the-shelf models\ntrained on ImageNet. The source code and model used in this paper are publicly\navailable at http://imatge-upc.github.io/retrieval-2017-cam/.\n", "versions": [{"version": "v1", "created": "Sun, 9 Jul 2017 13:51:45 GMT"}], "update_date": "2017-07-11", "authors_parsed": [["Jimenez", "Albert", ""], ["Alvarez", "Jose M.", ""], ["Giro-i-Nieto", "Xavier", ""]]}, {"id": "1707.02605", "submitter": "Fulvio Mastrogiovanni", "authors": "Divya Shah, Ernesto Denicia, Tiago Pimentel, Barbara Bruno, Fulvio\n  Mastrogiovanni", "title": "Detection of bimanual gestures everywhere: why it matters, what we need\n  and what is missing", "comments": "Submitted to Robotics and Autonomous Systems (Elsevier)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bimanual gestures are of the utmost importance for the study of motor\ncoordination in humans and in everyday activities. A reliable detection of\nbimanual gestures in unconstrained environments is fundamental for their\nclinical study and to assess common activities of daily living. This paper\ninvestigates techniques for a reliable, unconstrained detection and\nclassification of bimanual gestures. It assumes the availability of inertial\ndata originating from the two hands/arms, builds upon a previously developed\ntechnique for gesture modelling based on Gaussian Mixture Modelling (GMM) and\nGaussian Mixture Regression (GMR), and compares different modelling and\nclassification techniques, which are based on a number of assumptions inspired\nby literature about how bimanual gestures are represented and modelled in the\nbrain. Experiments show results related to 5 everyday bimanual activities,\nwhich have been selected on the basis of three main parameters: (not)\nconstraining the two hands by a physical tool, (not) requiring a specific\nsequence of single-hand gestures, being recursive (or not). In the best\nperforming combination of modeling approach and classification technique, five\nout of five activities are recognized up to an accuracy of 97%, a precision of\n82% and a level of recall of 100%.\n", "versions": [{"version": "v1", "created": "Sun, 9 Jul 2017 17:30:35 GMT"}], "update_date": "2017-07-11", "authors_parsed": [["Shah", "Divya", ""], ["Denicia", "Ernesto", ""], ["Pimentel", "Tiago", ""], ["Bruno", "Barbara", ""], ["Mastrogiovanni", "Fulvio", ""]]}, {"id": "1707.02637", "submitter": "Lijun Zhao", "authors": "Lijun Zhao and Jie Liang and Huihui Bai and Lili Meng and Anhong Wang\n  and Yao Zhao", "title": "Local Activity-tuned Image Filtering for Noise Removal and Image\n  Smoothing", "comments": "13 papers, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  In this paper, two local activity-tuned filtering frameworks are proposed for\nnoise removal and image smoothing, where the local activity measurement is\ngiven by the clipped and normalized local variance or standard deviation. The\nfirst framework is a modified anisotropic diffusion for noise removal of\npiece-wise smooth image. The second framework is a local activity-tuned\nRelative Total Variation (LAT-RTV) method for image smoothing. Both frameworks\nemploy the division of gradient and the local activity measurement to achieve\nnoise removal. In addition, to better capture local information, the proposed\nLAT-RTV uses the product of gradient and local activity measurement to boost\nthe performance of image smoothing. Experimental results are presented to\ndemonstrate the efficiency of the proposed methods on various applications,\nincluding depth image filtering, clip-art compression artifact removal, image\nsmoothing, and image denoising.\n", "versions": [{"version": "v1", "created": "Sun, 9 Jul 2017 21:07:05 GMT"}, {"version": "v2", "created": "Mon, 24 Jul 2017 06:33:13 GMT"}, {"version": "v3", "created": "Thu, 31 Aug 2017 06:45:28 GMT"}, {"version": "v4", "created": "Sat, 18 Nov 2017 06:45:46 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Zhao", "Lijun", ""], ["Liang", "Jie", ""], ["Bai", "Huihui", ""], ["Meng", "Lili", ""], ["Wang", "Anhong", ""], ["Zhao", "Yao", ""]]}, {"id": "1707.02642", "submitter": "Pedram Ghamisi Dr.", "authors": "Pedram Ghamisi, Gabriele Cavallaro, Dan (Sabrina) Wu, Jon Atli\n  Benediktsson, and Antonio Plaza", "title": "Integration of LiDAR and Hyperspectral Data for Land-cover\n  Classification: A Case Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, an approach is proposed to fuse LiDAR and hyperspectral data,\nwhich considers both spectral and spatial information in a single framework.\nHere, an extended self-dual attribute profile (ESDAP) is investigated to\nextract spatial information from a hyperspectral data set. To extract spectral\ninformation, a few well-known classifiers have been used such as support vector\nmachines (SVMs), random forests (RFs), and artificial neural networks (ANNs).\nThe proposed method accurately classify the relatively volumetric data set in a\nfew CPU processing time in a real ill-posed situation where there is no balance\nbetween the number of training samples and the number of features. The\nclassification part of the proposed approach is fully-automatic.\n", "versions": [{"version": "v1", "created": "Sun, 9 Jul 2017 21:35:54 GMT"}], "update_date": "2017-07-11", "authors_parsed": [["Ghamisi", "Pedram", "", "Sabrina"], ["Cavallaro", "Gabriele", "", "Sabrina"], ["Dan", "", "", "Sabrina"], ["Wu", "", ""], ["Benediktsson", "Jon Atli", ""], ["Plaza", "Antonio", ""]]}, {"id": "1707.02655", "submitter": "Vasileios Argyriou", "authors": "Rob Dupre and Vasileios Argyriou", "title": "A Human and Group Behaviour Simulation Evaluation Framework utilising\n  Composition and Video Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we present the modular Crowd Simulation Evaluation through\nComposition framework (CSEC) which provides a quantitative comparison between\ndifferent pedestrian and crowd simulation approaches. Evaluation is made based\non the comparison of source footage against synthetic video created through\nnovel composition techniques. The proposed framework seeks to reduce the\ncomplexity of simulation evaluation and provide a platform from which the\ncomparison of differing simulation algorithms as well as parametric tuning can\nbe conducted to improve simulation accuracy or providing measures of similarity\nbetween crowd simulation algorithms and source data. Through the use of\nfeatures designed to mimic the Human Visual System (HVS), specific simulation\nproperties can be evaluated relative to sample footage. Validation was\nperformed on a number of popular crowd datasets and through comparisons of\nmultiple pedestrian and crowd simulation algorithms.\n", "versions": [{"version": "v1", "created": "Sun, 9 Jul 2017 22:58:39 GMT"}, {"version": "v2", "created": "Wed, 2 May 2018 15:15:05 GMT"}, {"version": "v3", "created": "Sat, 24 Nov 2018 02:35:22 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Dupre", "Rob", ""], ["Argyriou", "Vasileios", ""]]}, {"id": "1707.02683", "submitter": "Jingkuan Song Dr.", "authors": "Lianli Gao, Jingkuan Song, Xingyi Liu, Junming Shao, Jiajun Liu, Jie\n  Shao", "title": "Learning in High-Dimensional Multimedia Data: The State of the Art", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  During the last decade, the deluge of multimedia data has impacted a wide\nrange of research areas, including multimedia retrieval, 3D tracking, database\nmanagement, data mining, machine learning, social media analysis, medical\nimaging, and so on. Machine learning is largely involved in multimedia\napplications of building models for classification and regression tasks etc.,\nand the learning principle consists in designing the models based on the\ninformation contained in the multimedia dataset. While many paradigms exist and\nare widely used in the context of machine learning, most of them suffer from\nthe `curse of dimensionality', which means that some strange phenomena appears\nwhen data are represented in a high-dimensional space. Given the high\ndimensionality and the high complexity of multimedia data, it is important to\ninvestigate new machine learning algorithms to facilitate multimedia data\nanalysis. To deal with the impact of high dimensionality, an intuitive way is\nto reduce the dimensionality. On the other hand, some researchers devoted\nthemselves to designing some effective learning schemes for high-dimensional\ndata. In this survey, we cover feature transformation, feature selection and\nfeature encoding, three approaches fighting the consequences of the curse of\ndimensionality. Next, we briefly introduce some recent progress of effective\nlearning algorithms. Finally, promising future trends on multimedia learning\nare envisaged.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jul 2017 03:12:41 GMT"}], "update_date": "2017-07-11", "authors_parsed": [["Gao", "Lianli", ""], ["Song", "Jingkuan", ""], ["Liu", "Xingyi", ""], ["Shao", "Junming", ""], ["Liu", "Jiajun", ""], ["Shao", "Jie", ""]]}, {"id": "1707.02692", "submitter": "Changyong Yu", "authors": "Changyong Yu, Yunde Jia", "title": "Anisotropic Diffusion-based Kernel Matrix Model for Face Liveness\n  Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial recognition and verification is a widely used biometric technology in\nsecurity system. Unfortunately, face biometrics is vulnerable to spoofing\nattacks using photographs or videos. In this paper, we present an anisotropic\ndiffusion-based kernel matrix model (ADKMM) for face liveness detection to\nprevent face spoofing attacks. We use the anisotropic diffusion to enhance the\nedges and boundary locations of a face image, and the kernel matrix model to\nextract face image features which we call the diffusion-kernel (D-K) features.\nThe D-K features reflect the inner correlation of the face image sequence. We\nintroduce convolution neural networks to extract the deep features, and then,\nemploy a generalized multiple kernel learning method to fuse the D-K features\nand the deep features to achieve better performance. Our experimental\nevaluation on the two publicly available datasets shows that the proposed\nmethod outperforms the state-of-art face liveness detection methods.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jul 2017 04:26:07 GMT"}], "update_date": "2017-07-11", "authors_parsed": [["Yu", "Changyong", ""], ["Jia", "Yunde", ""]]}, {"id": "1707.02711", "submitter": "Thomas Wiatowski", "authors": "Thomas Wiatowski and Philipp Grohs and Helmut B\\\"olcskei", "title": "Topology Reduction in Deep Convolutional Feature Extraction Networks", "comments": "Corrected errors in arguments on spectral decay of Sobolev functions.\n  Replaced part of the decay results (Sections 5-7) by corresponding statements\n  for effectively band-limited functions", "journal-ref": "Proc. of SPIE (Wavelets and Sparsity XVII), San Diego, USA, Vol.\n  10394, pp. 1039418:1-1039418:12, Aug. 2017, (invited paper)", "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.IT cs.LG math.FA math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks (CNNs) used in practice employ potentially\nhundreds of layers and $10$,$000$s of nodes. Such network sizes entail\nsignificant computational complexity due to the large number of convolutions\nthat need to be carried out; in addition, a large number of parameters needs to\nbe learned and stored. Very deep and wide CNNs may therefore not be well suited\nto applications operating under severe resource constraints as is the case,\ne.g., in low-power embedded and mobile platforms. This paper aims at\nunderstanding the impact of CNN topology, specifically depth and width, on the\nnetwork's feature extraction capabilities. We address this question for the\nclass of scattering networks that employ either Weyl-Heisenberg filters or\nwavelets, the modulus non-linearity, and no pooling. The exponential feature\nmap energy decay results in Wiatowski et al., 2017, are generalized to\n$\\mathcal{O}(a^{-N})$, where an arbitrary decay factor $a>1$ can be realized\nthrough suitable choice of the Weyl-Heisenberg prototype function or the mother\nwavelet. We then show how networks of fixed (possibly small) depth $N$ can be\ndesigned to guarantee that $((1-\\varepsilon)\\cdot 100)\\%$ of the input signal's\nenergy are contained in the feature vector. Based on the notion of\noperationally significant nodes, we characterize, partly rigorously and partly\nheuristically, the topology-reducing effects of (effectively) band-limited\ninput signals, band-limited filters, and feature map symmetries. Finally, for\nnetworks based on Weyl-Heisenberg filters, we determine the prototype function\nbandwidth that minimizes---for fixed network depth $N$---the average number of\noperationally significant nodes per layer.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jul 2017 06:35:48 GMT"}, {"version": "v2", "created": "Wed, 14 Mar 2018 08:59:37 GMT"}], "update_date": "2018-03-15", "authors_parsed": [["Wiatowski", "Thomas", ""], ["Grohs", "Philipp", ""], ["B\u00f6lcskei", "Helmut", ""]]}, {"id": "1707.02725", "submitter": "Jingdong Wang", "authors": "Ting Zhang, Guo-Jun Qi, Bin Xiao, and Jingdong Wang", "title": "Interleaved Group Convolutions for Deep Neural Networks", "comments": "To appear in ICCV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a simple and modularized neural network\narchitecture, named interleaved group convolutional neural networks (IGCNets).\nThe main point lies in a novel building block, a pair of two successive\ninterleaved group convolutions: primary group convolution and secondary group\nconvolution. The two group convolutions are complementary: (i) the convolution\non each partition in primary group convolution is a spatial convolution, while\non each partition in secondary group convolution, the convolution is a\npoint-wise convolution; (ii) the channels in the same secondary partition come\nfrom different primary partitions. We discuss one representative advantage:\nWider than a regular convolution with the number of parameters and the\ncomputation complexity preserved. We also show that regular convolutions, group\nconvolution with summation fusion, and the Xception block are special cases of\ninterleaved group convolutions. Empirical results over standard benchmarks,\nCIFAR-$10$, CIFAR-$100$, SVHN and ImageNet demonstrate that our networks are\nmore efficient in using parameters and computation complexity with similar or\nhigher accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jul 2017 07:28:57 GMT"}, {"version": "v2", "created": "Tue, 18 Jul 2017 07:46:45 GMT"}], "update_date": "2017-07-19", "authors_parsed": [["Zhang", "Ting", ""], ["Qi", "Guo-Jun", ""], ["Xiao", "Bin", ""], ["Wang", "Jingdong", ""]]}, {"id": "1707.02733", "submitter": "Sumit Shekhar", "authors": "Sumit Shekhar, Vishal M. Patel, Rama Chellappa", "title": "Synthesis-based Robust Low Resolution Face Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recognition of low resolution face images is a challenging problem in many\npractical face recognition systems. Methods have been proposed in the face\nrecognition literature for the problem which assume that the probe is low\nresolution, but a high resolution gallery is available for recognition. These\nattempts have been aimed at modifying the probe image such that the resultant\nimage provides better discrimination. We formulate the problem differently by\nleveraging the information available in the high resolution gallery image and\npropose a dictionary learning approach for classifying the low-resolution probe\nimage. An important feature of our algorithm is that it can handle resolution\nchange along with illumination variations. Furthermore, we also kernelize the\nalgorithm to handle non-linearity in data and present a joint dictionary\nlearning technique for robust recognition at low resolutions. The effectiveness\nof the proposed method is demonstrated using standard datasets and a\nchallenging outdoor face dataset. It is shown that our method is efficient and\ncan perform significantly better than many competitive low resolution face\nrecognition algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jul 2017 08:12:07 GMT"}], "update_date": "2017-07-11", "authors_parsed": [["Shekhar", "Sumit", ""], ["Patel", "Vishal M.", ""], ["Chellappa", "Rama", ""]]}, {"id": "1707.02749", "submitter": "Nam Le", "authors": "Nam Le and Jean-Marc Odobez", "title": "Improving speaker turn embedding by crossmodal transfer learning from\n  face embedding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning speaker turn embeddings has shown considerable improvement in\nsituations where conventional speaker modeling approaches fail. However, this\nimprovement is relatively limited when compared to the gain observed in face\nembedding learning, which has been proven very successful for face verification\nand clustering tasks. Assuming that face and voices from the same identities\nshare some latent properties (like age, gender, ethnicity), we propose three\ntransfer learning approaches to leverage the knowledge from the face domain\n(learned from thousands of images and identities) for tasks in the speaker\ndomain. These approaches, namely target embedding transfer, relative distance\ntransfer, and clustering structure transfer, utilize the structure of the\nsource face embedding space at different granularities to regularize the target\nspeaker turn embedding space as optimizing terms. Our methods are evaluated on\ntwo public broadcast corpora and yield promising advances over competitive\nbaselines in verification and audio clustering tasks, especially when dealing\nwith short speaker utterances. The analysis of the results also gives insight\ninto characteristics of the embedding spaces and shows their potential\napplications.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jul 2017 08:51:53 GMT"}], "update_date": "2017-07-11", "authors_parsed": [["Le", "Nam", ""], ["Odobez", "Jean-Marc", ""]]}, {"id": "1707.02785", "submitter": "Xu Lan", "authors": "Xu Lan, Hanxiao Wang, Shaogang Gong, Xiatian Zhu", "title": "Deep Reinforcement Learning Attention Selection for Person\n  Re-Identification", "comments": "Additional revision is needed", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing person re-identification (re-id) methods assume the provision of\naccurately cropped person bounding boxes with minimum background noise, mostly\nby manually cropping. This is significantly breached in practice when person\nbounding boxes must be detected automatically given a very large number of\nimages and/or videos processed. Compared to carefully cropped manually,\nauto-detected bounding boxes are far less accurate with random amount of\nbackground clutter which can degrade notably person re-id matching accuracy. In\nthis work, we develop a joint learning deep model that optimises person re-id\nattention selection within any auto-detected person bounding boxes by\nreinforcement learning of background clutter minimisation subject to re-id\nlabel pairwise constraints. Specifically, we formulate a novel unified re-id\narchitecture called Identity DiscriminativE Attention reinforcement Learning\n(IDEAL) to accurately select re-id attention in auto-detected bounding boxes\nfor optimising re-id performance. Our model can improve re-id accuracy\ncomparable to that from exhaustive human manual cropping of bounding boxes with\nadditional advantages from identity discriminative attention selection that\nspecially benefits re-id tasks beyond human knowledge. Extensive comparative\nevaluations demonstrate the re-id advantages of the proposed IDEAL model over a\nwide range of state-of-the-art re-id methods on two auto-detected re-id\nbenchmarks CUHK03 and Market-1501.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jul 2017 10:17:55 GMT"}, {"version": "v2", "created": "Tue, 11 Jul 2017 13:51:16 GMT"}, {"version": "v3", "created": "Mon, 24 Jul 2017 11:55:47 GMT"}, {"version": "v4", "created": "Sat, 7 Jul 2018 13:24:42 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["Lan", "Xu", ""], ["Wang", "Hanxiao", ""], ["Gong", "Shaogang", ""], ["Zhu", "Xiatian", ""]]}, {"id": "1707.02812", "submitter": "Suranjana Samanta", "authors": "Suranjana Samanta, Sameep Mehta", "title": "Towards Crafting Text Adversarial Samples", "comments": "11 pages, 5 figues", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial samples are strategically modified samples, which are crafted\nwith the purpose of fooling a classifier at hand. An attacker introduces\nspecially crafted adversarial samples to a deployed classifier, which are being\nmis-classified by the classifier. However, the samples are perceived to be\ndrawn from entirely different classes and thus it becomes hard to detect the\nadversarial samples. Most of the prior works have been focused on synthesizing\nadversarial samples in the image domain. In this paper, we propose a new method\nof crafting adversarial text samples by modification of the original samples.\nModifications of the original text samples are done by deleting or replacing\nthe important or salient words in the text or by introducing new words in the\ntext sample. Our algorithm works best for the datasets which have\nsub-categories within each of the classes of examples. While crafting\nadversarial samples, one of the key constraint is to generate meaningful\nsentences which can at pass off as legitimate from language (English)\nviewpoint. Experimental results on IMDB movie review dataset for sentiment\nanalysis and Twitter dataset for gender detection show the efficiency of our\nproposed method.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jul 2017 11:58:08 GMT"}], "update_date": "2017-07-11", "authors_parsed": [["Samanta", "Suranjana", ""], ["Mehta", "Sameep", ""]]}, {"id": "1707.02813", "submitter": "Marco Loog", "authors": "Marco Loog and Fran\\c{c}ois Lauze", "title": "Scale-Regularized Filter Learning", "comments": "Original submission to SSVM 2015 with a few minor corrections.\n  Version with minor changes to appear in Proceedings of the BMVC 2017 as\n  \\emph{Supervised Scale-Regularized Linear Convolutionary Filters}", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We start out by demonstrating that an elementary learning task, corresponding\nto the training of a single linear neuron in a convolutional neural network,\ncan be solved for feature spaces of very high dimensionality. In a second step,\nacknowledging that such high-dimensional learning tasks typically benefit from\nsome form of regularization and arguing that the problem of scale has not been\ntaken care of in a very satisfactory manner, we come to a combined resolution\nof both of these shortcomings by proposing a form of scale regularization.\nMoreover, using variational method, this regularization problem can also be\nsolved rather efficiently and we demonstrate, on an artificial filter learning\nproblem, the capabilities of our basic linear neuron. From a more general\nstandpoint, we see this work as prime example of how learning and variational\nmethods could, or even should work to their mutual benefit.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jul 2017 12:01:30 GMT"}], "update_date": "2017-07-11", "authors_parsed": [["Loog", "Marco", ""], ["Lauze", "Fran\u00e7ois", ""]]}, {"id": "1707.02850", "submitter": "Johann Sawatzky", "authors": "Johann Sawatzky and Juergen Gall", "title": "Adaptive Binarization for Weakly Supervised Affordance Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The concept of affordance is important to understand the relevance of object\nparts for a certain functional interaction. Affordance types generalize across\nobject categories and are not mutually exclusive. This makes the segmentation\nof affordance regions of objects in images a difficult task. In this work, we\nbuild on an iterative approach that learns a convolutional neural network for\naffordance segmentation from sparse keypoints. During this process, the\npredictions of the network need to be binarized. In this work, we propose an\nadaptive approach for binarization and estimate the parameters for\ninitialization by approximated cross validation. We evaluate our approach on\ntwo affordance datasets where our approach outperforms the state-of-the-art for\nweakly supervised affordance segmentation.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jul 2017 13:36:25 GMT"}], "update_date": "2017-07-11", "authors_parsed": [["Sawatzky", "Johann", ""], ["Gall", "Juergen", ""]]}, {"id": "1707.02880", "submitter": "Michael Gharbi", "authors": "Micha\\\"el Gharbi, Jiawen Chen, Jonathan T. Barron, Samuel W. Hasinoff,\n  and Fr\\'edo Durand", "title": "Deep Bilateral Learning for Real-Time Image Enhancement", "comments": "12 pages, 14 figures, Siggraph 2017", "journal-ref": "ACM Trans. Graph. 36, 4, Article 118 (2017)", "doi": "10.1145/3072959.3073592", "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Performance is a critical challenge in mobile image processing. Given a\nreference imaging pipeline, or even human-adjusted pairs of images, we seek to\nreproduce the enhancements and enable real-time evaluation. For this, we\nintroduce a new neural network architecture inspired by bilateral grid\nprocessing and local affine color transforms. Using pairs of input/output\nimages, we train a convolutional neural network to predict the coefficients of\na locally-affine model in bilateral space. Our architecture learns to make\nlocal, global, and content-dependent decisions to approximate the desired image\ntransformation. At runtime, the neural network consumes a low-resolution\nversion of the input image, produces a set of affine transformations in\nbilateral space, upsamples those transformations in an edge-preserving fashion\nusing a new slicing node, and then applies those upsampled transformations to\nthe full-resolution image. Our algorithm processes high-resolution images on a\nsmartphone in milliseconds, provides a real-time viewfinder at 1080p\nresolution, and matches the quality of state-of-the-art approximation\ntechniques on a large class of image operators. Unlike previous work, our model\nis trained off-line from data and therefore does not require access to the\noriginal operator at runtime. This allows our model to learn complex,\nscene-dependent transformations for which no reference implementation is\navailable, such as the photographic edits of a human retoucher.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jul 2017 14:34:06 GMT"}, {"version": "v2", "created": "Tue, 22 Aug 2017 19:26:08 GMT"}], "update_date": "2017-08-24", "authors_parsed": [["Gharbi", "Micha\u00ebl", ""], ["Chen", "Jiawen", ""], ["Barron", "Jonathan T.", ""], ["Hasinoff", "Samuel W.", ""], ["Durand", "Fr\u00e9do", ""]]}, {"id": "1707.02905", "submitter": "Jose Oramas", "authors": "Kaili Wang, Yu-Hui Huang, Jose Oramas, Luc Van Gool, Tinne Tuytelaars", "title": "An Analysis of Human-centered Geolocation", "comments": "WACV'18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online social networks contain a constantly increasing amount of images -\nmost of them focusing on people. Due to cultural and climate factors, fashion\ntrends and physical appearance of individuals differ from city to city. In this\npaper we investigate to what extent such cues can be exploited in order to\ninfer the geographic location, i.e. the city, where a picture was taken. We\nconduct a user study, as well as an evaluation of automatic methods based on\nconvolutional neural networks. Experiments on the Fashion 144k and a\nPinterest-based dataset show that the automatic methods succeed at this task to\na reasonable extent. As a matter of fact, our empirical results suggest that\nautomatic methods can surpass human performance by a large margin. Further\ninspection of the trained models shows that human-centered characteristics,\nlike clothing style, physical features, and accessories, are informative for\nthe task at hand. Moreover, it reveals that also contextual features, e.g. wall\ntype, natural environment, etc., are taken into account by the automatic\nmethods.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jul 2017 15:25:02 GMT"}, {"version": "v2", "created": "Thu, 14 Dec 2017 09:11:52 GMT"}, {"version": "v3", "created": "Wed, 31 Jan 2018 16:17:48 GMT"}], "update_date": "2018-02-01", "authors_parsed": [["Wang", "Kaili", ""], ["Huang", "Yu-Hui", ""], ["Oramas", "Jose", ""], ["Van Gool", "Luc", ""], ["Tuytelaars", "Tinne", ""]]}, {"id": "1707.02921", "submitter": "Bee Lim", "authors": "Bee Lim, Sanghyun Son, Heewon Kim, Seungjun Nah, and Kyoung Mu Lee", "title": "Enhanced Deep Residual Networks for Single Image Super-Resolution", "comments": "To appear in CVPR 2017 workshop. Best paper award of the NTIRE2017\n  workshop, and the winners of the NTIRE2017 Challenge on Single Image\n  Super-Resolution", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent research on super-resolution has progressed with the development of\ndeep convolutional neural networks (DCNN). In particular, residual learning\ntechniques exhibit improved performance. In this paper, we develop an enhanced\ndeep super-resolution network (EDSR) with performance exceeding those of\ncurrent state-of-the-art SR methods. The significant performance improvement of\nour model is due to optimization by removing unnecessary modules in\nconventional residual networks. The performance is further improved by\nexpanding the model size while we stabilize the training procedure. We also\npropose a new multi-scale deep super-resolution system (MDSR) and training\nmethod, which can reconstruct high-resolution images of different upscaling\nfactors in a single model. The proposed methods show superior performance over\nthe state-of-the-art methods on benchmark datasets and prove its excellence by\nwinning the NTIRE2017 Super-Resolution Challenge.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jul 2017 16:07:30 GMT"}], "update_date": "2017-07-11", "authors_parsed": [["Lim", "Bee", ""], ["Son", "Sanghyun", ""], ["Kim", "Heewon", ""], ["Nah", "Seungjun", ""], ["Lee", "Kyoung Mu", ""]]}, {"id": "1707.02931", "submitter": "Mohamed Elawady", "authors": "Mohamed Elawady, Christophe Ducottet, Olivier Alata, Cecile Barat,\n  Philippe Colantoni", "title": "Wavelet-based Reflection Symmetry Detection via Textural and Color\n  Histograms", "comments": "Draft submission for ICCV 2017 Workshop (Detecting Symmetry in the\n  Wild) [Paper track]", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Symmetry is one of the significant visual properties inside an image plane,\nto identify the geometrically balanced structures through real-world objects.\nExisting symmetry detection methods rely on descriptors of the local image\nfeatures and their neighborhood behavior, resulting incomplete symmetrical axis\ncandidates to discover the mirror similarities on a global scale. In this\npaper, we propose a new reflection symmetry detection scheme, based on a\nreliable edge-based feature extraction using Log-Gabor filters, plus an\nefficient voting scheme parameterized by their corresponding textural and color\nneighborhood information. Experimental evaluation on four single-case and three\nmultiple-case symmetry detection datasets validates the superior achievement of\nthe proposed work to find global symmetries inside an image.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jul 2017 16:32:43 GMT"}, {"version": "v2", "created": "Tue, 11 Jul 2017 05:31:27 GMT"}, {"version": "v3", "created": "Mon, 17 Jul 2017 11:57:21 GMT"}, {"version": "v4", "created": "Sat, 22 Jul 2017 02:05:16 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Elawady", "Mohamed", ""], ["Ducottet", "Christophe", ""], ["Alata", "Olivier", ""], ["Barat", "Cecile", ""], ["Colantoni", "Philippe", ""]]}, {"id": "1707.02937", "submitter": "Wenzhe Shi", "authors": "Andrew Aitken, Christian Ledig, Lucas Theis, Jose Caballero, Zehan\n  Wang, Wenzhe Shi", "title": "Checkerboard artifact free sub-pixel convolution: A note on sub-pixel\n  convolution, resize convolution and convolution resize", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The most prominent problem associated with the deconvolution layer is the\npresence of checkerboard artifacts in output images and dense labels. To combat\nthis problem, smoothness constraints, post processing and different\narchitecture designs have been proposed. Odena et al. highlight three sources\nof checkerboard artifacts: deconvolution overlap, random initialization and\nloss functions. In this note, we proposed an initialization method for\nsub-pixel convolution known as convolution NN resize. Compared to sub-pixel\nconvolution initialized with schemes designed for standard convolution kernels,\nit is free from checkerboard artifacts immediately after initialization.\nCompared to resize convolution, at the same computational complexity, it has\nmore modelling power and converges to solutions with smaller test errors.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jul 2017 16:41:22 GMT"}], "update_date": "2017-07-11", "authors_parsed": [["Aitken", "Andrew", ""], ["Ledig", "Christian", ""], ["Theis", "Lucas", ""], ["Caballero", "Jose", ""], ["Wang", "Zehan", ""], ["Shi", "Wenzhe", ""]]}, {"id": "1707.02968", "submitter": "Chen Sun", "authors": "Chen Sun and Abhinav Shrivastava and Saurabh Singh and Abhinav Gupta", "title": "Revisiting Unreasonable Effectiveness of Data in Deep Learning Era", "comments": "ICCV 2017 camera ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The success of deep learning in vision can be attributed to: (a) models with\nhigh capacity; (b) increased computational power; and (c) availability of\nlarge-scale labeled data. Since 2012, there have been significant advances in\nrepresentation capabilities of the models and computational capabilities of\nGPUs. But the size of the biggest dataset has surprisingly remained constant.\nWhat will happen if we increase the dataset size by 10x or 100x? This paper\ntakes a step towards clearing the clouds of mystery surrounding the\nrelationship between `enormous data' and visual deep learning. By exploiting\nthe JFT-300M dataset which has more than 375M noisy labels for 300M images, we\ninvestigate how the performance of current vision tasks would change if this\ndata was used for representation learning. Our paper delivers some surprising\n(and some expected) findings. First, we find that the performance on vision\ntasks increases logarithmically based on volume of training data size. Second,\nwe show that representation learning (or pre-training) still holds a lot of\npromise. One can improve performance on many vision tasks by just training a\nbetter base model. Finally, as expected, we present new state-of-the-art\nresults for different vision tasks including image classification, object\ndetection, semantic segmentation and human pose estimation. Our sincere hope is\nthat this inspires vision community to not undervalue the data and develop\ncollective efforts in building larger datasets.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jul 2017 17:54:31 GMT"}, {"version": "v2", "created": "Fri, 4 Aug 2017 01:33:22 GMT"}], "update_date": "2017-08-07", "authors_parsed": [["Sun", "Chen", ""], ["Shrivastava", "Abhinav", ""], ["Singh", "Saurabh", ""], ["Gupta", "Abhinav", ""]]}, {"id": "1707.02973", "submitter": "Li Du", "authors": "Li Du, Yuan Du, Yilei Li, Mau-Chung Frank Chang", "title": "A Reconfigurable Streaming Deep Convolutional Neural Network Accelerator\n  for Internet of Things", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural network (CNN) offers significant accuracy in image\ndetection. To implement image detection using CNN in the internet of things\n(IoT) devices, a streaming hardware accelerator is proposed. The proposed\naccelerator optimizes the energy efficiency by avoiding unnecessary data\nmovement. With unique filter decomposition technique, the accelerator can\nsupport arbitrary convolution window size. In addition, max pooling function\ncan be computed in parallel with convolution by using separate pooling unit,\nthus achieving throughput improvement. A prototype accelerator was implemented\nin TSMC 65nm technology with a core size of 5mm2. The accelerator can support\nmajor CNNs and achieve 152GOPS peak throughput and 434GOPS/W energy efficiency\nat 350mW, making it a promising hardware accelerator for intelligent IoT\ndevices.\n", "versions": [{"version": "v1", "created": "Sat, 8 Jul 2017 19:31:38 GMT"}], "update_date": "2017-07-12", "authors_parsed": [["Du", "Li", ""], ["Du", "Yuan", ""], ["Li", "Yilei", ""], ["Chang", "Mau-Chung Frank", ""]]}, {"id": "1707.02975", "submitter": "Song Wang Dr.", "authors": "Song Wang, Jun Sun, Satoshi Naoi", "title": "On Study of the Reliable Fully Convolutional Networks with Tree Arranged\n  Outputs (TAO-FCN) for Handwritten String Recognition", "comments": "Rejected by ICDAR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The handwritten string recognition is still a challengeable task, though the\npowerful deep learning tools were introduced. In this paper, based on TAO-FCN,\nwe proposed an end-to-end system for handwritten string recognition. Compared\nwith the conventional methods, there is no preprocess nor manually designed\nrules employed. With enough labelled data, it is easy to apply the proposed\nmethod to different applications. Although the performance of the proposed\nmethod may not be comparable with the state-of-the-art approaches, it's\nusability and robustness are more meaningful for practical applications.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jul 2017 07:34:29 GMT"}], "update_date": "2017-07-12", "authors_parsed": [["Wang", "Song", ""], ["Sun", "Jun", ""], ["Naoi", "Satoshi", ""]]}, {"id": "1707.02978", "submitter": "Joris Gu\\'erin", "authors": "Joris Gu\\'erin, Olivier Gibaru, Eric Nyiri and St\\'ephane Thiery", "title": "Automatic Construction of Real-World Datasets for 3D Object Localization\n  using Two Cameras", "comments": "5 pages, 3 figures, to appear in the proceedings of IECON 2018\n  (Special session on Collaborative Robots in Smart Manufacturing), Washington\n  D.C., USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unlike classification, position labels cannot be assigned manually by humans.\nFor this reason, generating supervision for precise object localization is a\nhard task. This paper details a method to create large datasets for 3D object\nlocalization, with real world images, using an industrial robot to generate\nposition labels. By knowledge of the geometry of the robot, we are able to\nautomatically synchronize the images of the two cameras and the object 3D\nposition. We applied it to generate a screw-driver localization dataset with\nstereo images, using a KUKA LBR iiwa robot. This dataset could then be used to\ntrain a CNN regressor to learn end-to-end stereo object localization from a set\nof two standard uncalibrated cameras.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jul 2017 09:50:48 GMT"}, {"version": "v2", "created": "Wed, 12 Jul 2017 11:57:34 GMT"}, {"version": "v3", "created": "Tue, 11 Sep 2018 08:49:07 GMT"}], "update_date": "2018-09-12", "authors_parsed": [["Gu\u00e9rin", "Joris", ""], ["Gibaru", "Olivier", ""], ["Nyiri", "Eric", ""], ["Thiery", "St\u00e9phane", ""]]}, {"id": "1707.03004", "submitter": "Amir Mohammad Esmaieeli Sikaroudi", "authors": "Amir Mohammad Esmaieeli Sikaroudi, Sasan Ghaffari, Ali Yousefi, Hassan\n  Sadeghi Naeini", "title": "Foot anthropometry device and single object image thresholding", "comments": null, "journal-ref": "Signal & Image Processing : An International Journal (SIPIJ)\n  Vol.8, No.3, June 2017", "doi": "10.5121/sipij.2017.8301", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a device, algorithm and graphical user interface to\nobtain anthropometric measurements of foot. Presented device facilitates\nobtaining scale of image and image processing by taking one image from side\nfoot and underfoot simultaneously. Introduced image processing algorithm\nminimizes a noise criterion, which is suitable for object detection in single\nobject images and outperforms famous image thresholding methods when lighting\ncondition is poor. Performance of image-based method is compared to manual\nmethod. Image-based measurements of underfoot in average was 4mm less than\nactual measures. Mean absolute error of underfoot length was 1.6mm, however\nlength obtained from side foot had 4.4mm mean absolute error. Furthermore,\nbased on t-test and f-test results, no significant difference between manual\nand image-based anthropometry observed. In order to maintain anthropometry\nprocess performance in different situations user interface designed for\nhandling changes in light conditions and altering speed of the algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jul 2017 18:21:03 GMT"}], "update_date": "2017-07-12", "authors_parsed": [["Sikaroudi", "Amir Mohammad Esmaieeli", ""], ["Ghaffari", "Sasan", ""], ["Yousefi", "Ali", ""], ["Naeini", "Hassan Sadeghi", ""]]}, {"id": "1707.03017", "submitter": "Ethan Perez", "authors": "Ethan Perez, Harm de Vries, Florian Strub, Vincent Dumoulin, Aaron\n  Courville", "title": "Learning Visual Reasoning Without Strong Priors", "comments": "Full AAAI 2018 paper is at arXiv:1709.07871. Presented at ICML 2017's\n  Machine Learning in Speech and Language Processing Workshop. Code is at\n  http://github.com/ethanjperez/film", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Achieving artificial visual reasoning - the ability to answer image-related\nquestions which require a multi-step, high-level process - is an important step\ntowards artificial general intelligence. This multi-modal task requires\nlearning a question-dependent, structured reasoning process over images from\nlanguage. Standard deep learning approaches tend to exploit biases in the data\nrather than learn this underlying structure, while leading methods learn to\nvisually reason successfully but are hand-crafted for reasoning. We show that a\ngeneral-purpose, Conditional Batch Normalization approach achieves\nstate-of-the-art results on the CLEVR Visual Reasoning benchmark with a 2.4%\nerror rate. We outperform the next best end-to-end method (4.5%) and even\nmethods that use extra supervision (3.1%). We probe our model to shed light on\nhow it reasons, showing it has learned a question-dependent, multi-step\nprocess. Previous work has operated under the assumption that visual reasoning\ncalls for a specialized architecture, but we show that a general architecture\nwith proper conditioning can learn to visually reason effectively.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jul 2017 18:49:28 GMT"}, {"version": "v2", "created": "Wed, 12 Jul 2017 15:02:50 GMT"}, {"version": "v3", "created": "Sun, 20 Aug 2017 14:56:51 GMT"}, {"version": "v4", "created": "Wed, 4 Oct 2017 20:01:27 GMT"}, {"version": "v5", "created": "Mon, 18 Dec 2017 21:37:16 GMT"}], "update_date": "2017-12-20", "authors_parsed": [["Perez", "Ethan", ""], ["de Vries", "Harm", ""], ["Strub", "Florian", ""], ["Dumoulin", "Vincent", ""], ["Courville", "Aaron", ""]]}, {"id": "1707.03039", "submitter": "Guoan Zheng", "authors": "Jun Liao, Yutong Jiang, Zichao Bian, Bahareh Mahrou, Aparna Nambiar,\n  Alexander W. Magsam, Kaikai Guo, Yong Ku Cho, and Guoan Zheng", "title": "Rapid focus map surveying for whole slide imaging with continues sample\n  motion", "comments": null, "journal-ref": "Optics Letters, 42 (17), 3379-3382 (2017)", "doi": "10.1364/OL.42.003379", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Whole slide imaging (WSI) has recently been cleared for primary diagnosis in\nthe US. A critical challenge of WSI is to perform accurate focusing in high\nspeed. Traditional systems create a focus map prior to scanning. For each focus\npoint on the map, sample needs to be static in the x-y plane and axial scanning\nis needed to maximize the contrast. Here we report a novel focus map surveying\nmethod for WSI. The reported method requires no axial scanning, no additional\ncamera and lens, works for stained and transparent samples, and allows\ncontinuous sample motion in the surveying process. It can be used for both\nbrightfield and fluorescence WSI. By using a 20X, 0.75 NA objective lens, we\ndemonstrate a mean focusing error of ~0.08 microns in the static mode and ~0.17\nmicrons in the continuous motion mode. The reported method may provide a\nturnkey solution for most existing WSI systems for its simplicity, robustness,\naccuracy, and high-speed. It may also standardize the imaging performance of\nWSI systems for digital pathology and find other applications in high-content\nmicroscopy such as DNA sequencing and time-lapse live-cell imaging.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jul 2017 02:07:06 GMT"}], "update_date": "2019-04-26", "authors_parsed": [["Liao", "Jun", ""], ["Jiang", "Yutong", ""], ["Bian", "Zichao", ""], ["Mahrou", "Bahareh", ""], ["Nambiar", "Aparna", ""], ["Magsam", "Alexander W.", ""], ["Guo", "Kaikai", ""], ["Cho", "Yong Ku", ""], ["Zheng", "Guoan", ""]]}, {"id": "1707.03067", "submitter": "Mingda Zhang", "authors": "Zaeem Hussain, Mingda Zhang, Xiaozhong Zhang, Keren Ye, Christopher\n  Thomas, Zuha Agha, Nathan Ong, Adriana Kovashka", "title": "Automatic Understanding of Image and Video Advertisements", "comments": "To appear in CVPR 2017; data available on\n  http://cs.pitt.edu/~kovashka/ads", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is more to images than their objective physical content: for example,\nadvertisements are created to persuade a viewer to take a certain action. We\npropose the novel problem of automatic advertisement understanding. To enable\nresearch on this problem, we create two datasets: an image dataset of 64,832\nimage ads, and a video dataset of 3,477 ads. Our data contains rich annotations\nencompassing the topic and sentiment of the ads, questions and answers\ndescribing what actions the viewer is prompted to take and the reasoning that\nthe ad presents to persuade the viewer (\"What should I do according to this ad,\nand why should I do it?\"), and symbolic references ads make (e.g. a dove\nsymbolizes peace). We also analyze the most common persuasive strategies ads\nuse, and the capabilities that computer vision systems should have to\nunderstand these strategies. We present baseline classification results for\nseveral prediction tasks, including automatically answering questions about the\nmessages of the ads.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jul 2017 21:25:44 GMT"}], "update_date": "2017-07-12", "authors_parsed": [["Hussain", "Zaeem", ""], ["Zhang", "Mingda", ""], ["Zhang", "Xiaozhong", ""], ["Ye", "Keren", ""], ["Thomas", "Christopher", ""], ["Agha", "Zuha", ""], ["Ong", "Nathan", ""], ["Kovashka", "Adriana", ""]]}, {"id": "1707.03088", "submitter": "Edris Naderan", "authors": "E. Naderan", "title": "Online Handwritten Mathematical Expressions Recognition System Using\n  Fuzzy Neural Network", "comments": "in Russian", "journal-ref": "ITHEA, Information Content and Processing, 2014, 1 (3) , 262-268", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The article describes developed information technology for online recognition\nof handwritten mathematical expressions that based on proposed approaches to\nhandwritten symbols recognition and structural analysis.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jul 2017 00:28:23 GMT"}, {"version": "v2", "created": "Wed, 12 Jul 2017 04:38:13 GMT"}], "update_date": "2017-07-13", "authors_parsed": [["Naderan", "E.", ""]]}, {"id": "1707.03110", "submitter": "Sumarsih Purbarani", "authors": "Sumarsih Condroayu Purbarani, Hadaiq Rolis Sanabila, Wisnu Jatmiko", "title": "Distance-to-Mean Continuous Conditional Random Fields to Enhance\n  Prediction Problem in Traffic Flow Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increase of vehicle in highways may cause traffic congestion as well as\nin the normal roadways. Predicting the traffic flow in highways especially, is\ndemanded to solve this congestion problem. Predictions on time-series\nmultivariate data, such as in the traffic flow dataset, have been largely\naccomplished through various approaches. The approach with conventional\nprediction algorithms, such as with Support Vector Machine (SVM), is only\ncapable of accommodating predictions that are independent in each time unit.\nHence, the sequential relationships in this time series data is hardly\nexplored. Continuous Conditional Random Field (CCRF) is one of Probabilistic\nGraphical Model (PGM) algorithms which can accommodate this problem. The\nneighboring aspects of sequential data such as in the time series data can be\nexpressed by CCRF so that its predictions are more reliable. In this article, a\nnovel approach called DM-CCRF is adopted by modifying the CCRF prediction\nalgorithm to strengthen the probability of the predictions made by the baseline\nregressor. The result shows that DM-CCRF is superior in performance compared to\nCCRF. This is validated by the error decrease of the baseline up to 9%\nsignificance. This is twice the standard CCRF performance which can only\ndecrease baseline error by 4.582% at most.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jul 2017 02:36:28 GMT"}], "update_date": "2017-07-12", "authors_parsed": [["Purbarani", "Sumarsih Condroayu", ""], ["Sanabila", "Hadaiq Rolis", ""], ["Jatmiko", "Wisnu", ""]]}, {"id": "1707.03123", "submitter": "Xavier Gir\\'o-i-Nieto", "authors": "Marc Assens, Kevin McGuinness, Xavier Giro-i-Nieto, Noel E. O'Connor", "title": "SaltiNet: Scan-path Prediction on 360 Degree Images using Saliency\n  Volumes", "comments": "Winner of the Best Scan-path Award at the Salient360!: Visual\n  attention modeling for 360 degrees Images Grand Challenge of ICME 2017.\n  Presented at the ICCV 2017 Workshop on Egocentric Perception, Interaction and\n  Computing (EPIC)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce SaltiNet, a deep neural network for scanpath prediction trained\non 360-degree images. The model is based on a temporal-aware novel\nrepresentation of saliency information named the saliency volume. The first\npart of the network consists of a model trained to generate saliency volumes,\nwhose parameters are fit by back-propagation computed from a binary cross\nentropy (BCE) loss over downsampled versions of the saliency volumes. Sampling\nstrategies over these volumes are used to generate scanpaths over the\n360-degree images. Our experiments show the advantages of using saliency\nvolumes, and how they can be used for related tasks. Our source code and\ntrained models available at\nhttps://github.com/massens/saliency-360salient-2017.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jul 2017 04:04:09 GMT"}, {"version": "v2", "created": "Wed, 12 Jul 2017 06:07:57 GMT"}, {"version": "v3", "created": "Thu, 13 Jul 2017 14:10:40 GMT"}, {"version": "v4", "created": "Wed, 19 Jul 2017 10:11:25 GMT"}, {"version": "v5", "created": "Thu, 17 Aug 2017 10:48:16 GMT"}], "update_date": "2017-08-18", "authors_parsed": [["Assens", "Marc", ""], ["McGuinness", "Kevin", ""], ["Giro-i-Nieto", "Xavier", ""], ["O'Connor", "Noel E.", ""]]}, {"id": "1707.03124", "submitter": "Chunhua Shen", "authors": "Xinlong Wang, Zhipeng Man, Mingyu You, Chunhua Shen", "title": "Adversarial Generation of Training Examples: Applications to Moving\n  Vehicle License Plate Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Networks (GAN) have attracted much research attention\nrecently, leading to impressive results for natural image generation. However,\nto date little success was observed in using GAN generated images for improving\nclassification tasks. Here we attempt to explore, in the context of car license\nplate recognition, whether it is possible to generate synthetic training data\nusing GAN to improve recognition accuracy. With a carefully-designed pipeline,\nwe show that the answer is affirmative. First, a large-scale image set is\ngenerated using the generator of GAN, without manual annotation. Then, these\nimages are fed to a deep convolutional neural network (DCNN) followed by a\nbidirectional recurrent neural network (BRNN) with long short-term memory\n(LSTM), which performs the feature learning and sequence labelling. Finally,\nthe pre-trained model is fine-tuned on real images. Our experimental results on\na few data sets demonstrate the effectiveness of using GAN images: an\nimprovement of 7.5% over a strong baseline with moderate-sized real data being\navailable. We show that the proposed framework achieves competitive recognition\naccuracy on challenging test datasets. We also leverage the depthwise separate\nconvolution to construct a lightweight convolutional RNN, which is about half\nsize and 2x faster on CPU. Combining this framework and the proposed pipeline,\nwe make progress in performing accurate recognition on mobile and embedded\ndevices.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jul 2017 04:17:17 GMT"}, {"version": "v2", "created": "Sat, 22 Jul 2017 12:20:36 GMT"}, {"version": "v3", "created": "Fri, 10 Nov 2017 23:33:46 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Wang", "Xinlong", ""], ["Man", "Zhipeng", ""], ["You", "Mingyu", ""], ["Shen", "Chunhua", ""]]}, {"id": "1707.03126", "submitter": "Alexey Ruchay", "authors": "Alexey Ruchay and Vitaly Kober", "title": "Impulsive noise removal from color images with morphological filtering", "comments": "The 6th international conference on analysis of images, social\n  networks, and texts (AIST 2017), 27-29 July, 2017, Moscow, Russia", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with impulse noise removal from color images. The proposed\nnoise removal algorithm employs a novel approach with morphological filtering\nfor color image denoising; that is, detection of corrupted pixels and removal\nof the detected noise by means of morphological filtering. With the help of\ncomputer simulation we show that the proposed algorithm can effectively remove\nimpulse noise. The performance of the proposed algorithm is compared in terms\nof image restoration metrics and processing speed with that of common\nsuccessful algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jul 2017 04:48:24 GMT"}], "update_date": "2017-07-12", "authors_parsed": [["Ruchay", "Alexey", ""], ["Kober", "Vitaly", ""]]}, {"id": "1707.03133", "submitter": "David S Weber", "authors": "Naoki Saito and David S. Weber", "title": "Underwater object classification using scattering transform of sonar\n  signals", "comments": "13 pages, 12 figures, SPIE conference on Wavelets and Sparsity 17.\n  I've also included a compiled version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we apply the scattering transform (ST), a nonlinear map based\noff of a convolutional neural network (CNN), to classification of underwater\nobjects using sonar signals. The ST formalizes the observation that the filters\nlearned by a CNN have wavelet like structure. We achieve effective binary\nclassification both on a real dataset of Unexploded Ordinance (UXOs), as well\nas synthetically generated examples. We also explore the effects on the\nwaveforms with respect to changes in the object domain (e.g., translation,\nrotation, and acoustic impedance, etc.), and examine the consequences coming\nfrom theoretical results for the scattering transform. We show that the\nscattering transform is capable of excellent classification on both the\nsynthetic and real problems, thanks to having more quasi-invariance properties\nthat are well-suited to translation and rotation of the object.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jul 2017 05:21:20 GMT"}, {"version": "v2", "created": "Mon, 31 Jul 2017 20:49:51 GMT"}, {"version": "v3", "created": "Sun, 3 Sep 2017 22:03:33 GMT"}], "update_date": "2017-09-05", "authors_parsed": [["Saito", "Naoki", ""], ["Weber", "David S.", ""]]}, {"id": "1707.03164", "submitter": "Liheng Bian", "authors": "Liheng Bian, Jinli Suo, Qionghai Dai, Feng Chen", "title": "Experimental comparison of single-pixel imaging algorithms", "comments": null, "journal-ref": null, "doi": "10.1364/JOSAA.35.000078", "report-no": null, "categories": "cs.CV physics.optics", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single-pixel imaging (SPI) is a novel technique capturing 2D images using a\nphotodiode, instead of conventional 2D array sensors. SPI owns high\nsignal-to-noise ratio, wide spectrum range, low cost, and robustness to light\nscattering. Various algorithms have been proposed for SPI reconstruction,\nincluding the linear correlation methods, the alternating projection method\n(AP), and the compressive sensing based methods. However, there has been no\ncomprehensive review discussing respective advantages, which is important for\nSPI's further applications and development. In this paper, we reviewed and\ncompared these algorithms in a unified reconstruction framework. Besides, we\nproposed two other SPI algorithms including a conjugate gradient descent based\nmethod (CGD) and a Poisson maximum likelihood based method. Both simulations\nand experiments validate the following conclusions: to obtain comparable\nreconstruction accuracy, the compressive sensing based total variation\nregularization method (TV) requires the least measurements and consumes the\nleast running time for small-scale reconstruction; the CGD and AP methods run\nfastest in large-scale cases; the TV and AP methods are the most robust to\nmeasurement noise. In a word, there are trade-offs between capture efficiency,\ncomputational complexity and robustness to noise among different SPI\nalgorithms. We have released our source code for non-commercial use.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jul 2017 08:21:03 GMT"}, {"version": "v2", "created": "Tue, 24 Oct 2017 07:51:10 GMT"}], "update_date": "2017-12-12", "authors_parsed": [["Bian", "Liheng", ""], ["Suo", "Jinli", ""], ["Dai", "Qionghai", ""], ["Chen", "Feng", ""]]}, {"id": "1707.03166", "submitter": "Shuai Li", "authors": "Shuai Li, Dinei Florencio, Yaqin Zhao, Chris Cook, Wanqing Li", "title": "Foreground Detection in Camouflaged Scenes", "comments": "IEEE International Conference on Image Processing, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Foreground detection has been widely studied for decades due to its\nimportance in many practical applications. Most of the existing methods assume\nforeground and background show visually distinct characteristics and thus the\nforeground can be detected once a good background model is obtained. However,\nthere are many situations where this is not the case. Of particular interest in\nvideo surveillance is the camouflage case. For example, an active attacker\ncamouflages by intentionally wearing clothes that are visually similar to the\nbackground. In such cases, even given a decent background model, it is not\ntrivial to detect foreground objects. This paper proposes a texture guided\nweighted voting (TGWV) method which can efficiently detect foreground objects\nin camouflaged scenes. The proposed method employs the stationary wavelet\ntransform to decompose the image into frequency bands. We show that the small\nand hardly noticeable differences between foreground and background in the\nimage domain can be effectively captured in certain wavelet frequency bands. To\nmake the final foreground decision, a weighted voting scheme is developed based\non intensity and texture of all the wavelet bands with weights carefully\ndesigned. Experimental results demonstrate that the proposed method achieves\nsuperior performance compared to the current state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jul 2017 08:21:45 GMT"}], "update_date": "2017-07-12", "authors_parsed": [["Li", "Shuai", ""], ["Florencio", "Dinei", ""], ["Zhao", "Yaqin", ""], ["Cook", "Chris", ""], ["Li", "Wanqing", ""]]}, {"id": "1707.03167", "submitter": "Nick Schneider", "authors": "Nick Schneider, Florian Piewak, Christoph Stiller, Uwe Franke", "title": "RegNet: Multimodal Sensor Registration Using Deep Neural Networks", "comments": "published in IEEE Intelligent Vehicles Symposium, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present RegNet, the first deep convolutional neural network\n(CNN) to infer a 6 degrees of freedom (DOF) extrinsic calibration between\nmultimodal sensors, exemplified using a scanning LiDAR and a monocular camera.\nCompared to existing approaches, RegNet casts all three conventional\ncalibration steps (feature extraction, feature matching and global regression)\ninto a single real-time capable CNN. Our method does not require any human\ninteraction and bridges the gap between classical offline and target-less\nonline calibration approaches as it provides both a stable initial estimation\nas well as a continuous online correction of the extrinsic parameters. During\ntraining we randomly decalibrate our system in order to train RegNet to infer\nthe correspondence between projected depth measurements and RGB image and\nfinally regress the extrinsic calibration. Additionally, with an iterative\nexecution of multiple CNNs, that are trained on different magnitudes of\ndecalibration, our approach compares favorably to state-of-the-art methods in\nterms of a mean calibration error of 0.28 degrees for the rotational and 6 cm\nfor the translation components even for large decalibrations up to 1.5 m and 20\ndegrees.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jul 2017 08:21:58 GMT"}], "update_date": "2017-07-12", "authors_parsed": [["Schneider", "Nick", ""], ["Piewak", "Florian", ""], ["Stiller", "Christoph", ""], ["Franke", "Uwe", ""]]}, {"id": "1707.03194", "submitter": "Jalal Fadili", "authors": "Jalal Fadili, J\\'er\\^ome Malick and Gabriel Peyr\\'e", "title": "Sensitivity Analysis for Mirror-Stratifiable Convex Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper provides a set of sensitivity analysis and activity identification\nresults for a class of convex functions with a strong geometric structure, that\nwe coined \"mirror-stratifiable\". These functions are such that there is a\nbijection between a primal and a dual stratification of the space into\npartitioning sets, called strata. This pairing is crucial to track the strata\nthat are identifiable by solutions of parametrized optimization problems or by\niterates of optimization algorithms. This class of functions encompasses all\nregularizers routinely used in signal and image processing, machine learning,\nand statistics. We show that this \"mirror-stratifiable\" structure enjoys a nice\nsensitivity theory, allowing us to study stability of solutions of optimization\nproblems to small perturbations, as well as activity identification of\nfirst-order proximal splitting-type algorithms. Existing results in the\nliterature typically assume that, under a non-degeneracy condition, the active\nset associated to a minimizer is stable to small perturbations and is\nidentified in finite time by optimization schemes. In contrast, our results do\nnot require any non-degeneracy assumption: in consequence, the optimal active\nset is not necessarily stable anymore, but we are able to track precisely the\nset of identifiable strata.We show that these results have crucial implications\nwhen solving challenging ill-posed inverse problems via regularization, a\ntypical scenario where the non-degeneracy condition is not fulfilled. Our\ntheoretical results, illustrated by numerical simulations, allow to\ncharacterize the instability behaviour of the regularized solutions, by\nlocating the set of all low-dimensional strata that can be potentially\nidentified by these solutions.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jul 2017 09:35:14 GMT"}, {"version": "v2", "created": "Thu, 29 Mar 2018 09:35:30 GMT"}, {"version": "v3", "created": "Tue, 5 Jun 2018 08:25:18 GMT"}], "update_date": "2018-06-06", "authors_parsed": [["Fadili", "Jalal", ""], ["Malick", "J\u00e9r\u00f4me", ""], ["Peyr\u00e9", "Gabriel", ""]]}, {"id": "1707.03195", "submitter": "Pim Moeskops", "authors": "Pim Moeskops, Mitko Veta, Maxime W. Lafarge, Koen A.J. Eppenhof,\n  Josien P.W. Pluim", "title": "Adversarial training and dilated convolutions for brain MRI segmentation", "comments": "MICCAI 2017 Workshop on Deep Learning in Medical Image Analysis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) have been applied to various automatic\nimage segmentation tasks in medical image analysis, including brain MRI\nsegmentation. Generative adversarial networks have recently gained popularity\nbecause of their power in generating images that are difficult to distinguish\nfrom real images.\n  In this study we use an adversarial training approach to improve CNN-based\nbrain MRI segmentation. To this end, we include an additional loss function\nthat motivates the network to generate segmentations that are difficult to\ndistinguish from manual segmentations. During training, this loss function is\noptimised together with the conventional average per-voxel cross entropy loss.\n  The results show improved segmentation performance using this adversarial\ntraining procedure for segmentation of two different sets of images and using\ntwo different network architectures, both visually and in terms of Dice\ncoefficients.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jul 2017 09:37:11 GMT"}], "update_date": "2017-07-12", "authors_parsed": [["Moeskops", "Pim", ""], ["Veta", "Mitko", ""], ["Lafarge", "Maxime W.", ""], ["Eppenhof", "Koen A. J.", ""], ["Pluim", "Josien P. W.", ""]]}, {"id": "1707.03237", "submitter": "Carole Sudre", "authors": "Carole H Sudre, Wenqi Li, Tom Vercauteren, S\\'ebastien Ourselin, M.\n  Jorge Cardoso", "title": "Generalised Dice overlap as a deep learning loss function for highly\n  unbalanced segmentations", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-319-67558-9_28", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep-learning has proved in recent years to be a powerful tool for image\nanalysis and is now widely used to segment both 2D and 3D medical images.\nDeep-learning segmentation frameworks rely not only on the choice of network\narchitecture but also on the choice of loss function. When the segmentation\nprocess targets rare observations, a severe class imbalance is likely to occur\nbetween candidate labels, thus resulting in sub-optimal performance. In order\nto mitigate this issue, strategies such as the weighted cross-entropy function,\nthe sensitivity function or the Dice loss function, have been proposed. In this\nwork, we investigate the behavior of these loss functions and their sensitivity\nto learning rate tuning in the presence of different rates of label imbalance\nacross 2D and 3D segmentation tasks. We also propose to use the class\nre-balancing properties of the Generalized Dice overlap, a known metric for\nsegmentation assessment, as a robust and accurate deep-learning loss function\nfor unbalanced tasks.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jul 2017 12:07:52 GMT"}, {"version": "v2", "created": "Wed, 12 Jul 2017 05:50:55 GMT"}, {"version": "v3", "created": "Fri, 14 Jul 2017 16:57:58 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["Sudre", "Carole H", ""], ["Li", "Wenqi", ""], ["Vercauteren", "Tom", ""], ["Ourselin", "S\u00e9bastien", ""], ["Cardoso", "M. Jorge", ""]]}, {"id": "1707.03266", "submitter": "Xin Wang", "authors": "Xin Wang, Lejun Zou, Xiaohua Shen, Yupeng Ren, Yi Qin", "title": "A region-growing approach for automatic outcrop fracture extraction from\n  a three-dimensional point cloud", "comments": "25 pages, 7 figures", "journal-ref": "Computers & Geosciences, 99, 100-106", "doi": "10.1016/j.cageo.2016.11.002", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional manual surveys of rock mass fractures usually require large\namounts of time and labor; yet, they provide a relatively small set of data\nthat cannot be considered representative of the study region. Terrestrial laser\nscanners are increasingly used for fracture surveys because they can\nefficiently acquire large area, high-resolution, three-dimensional (3D) point\nclouds from outcrops. However, extracting fractures and other planar surfaces\nfrom 3D outcrop point clouds is still a challenging task. No method has been\nreported that can be used to automatically extract the full extent of every\nindividual fracture from a 3D outcrop point cloud. In this study, we propose a\nmethod using a region-growing approach to address this problem; the method also\nestimates the orientation of each fracture. In this method, criteria based on\nthe local surface normal and curvature of the point cloud are used to initiate\nand control the growth of the fracture region. In tests using outcrop point\ncloud data, the proposed method identified and extracted the full extent of\nindividual fractures with high accuracy. Compared with manually acquired field\nsurvey data, our method obtained better-quality fracture data, thereby\ndemonstrating the high potential utility of the proposed method.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jun 2017 02:51:43 GMT"}], "update_date": "2017-07-19", "authors_parsed": [["Wang", "Xin", ""], ["Zou", "Lejun", ""], ["Shen", "Xiaohua", ""], ["Ren", "Yupeng", ""], ["Qin", "Yi", ""]]}, {"id": "1707.03268", "submitter": "Denis Parkhomenko", "authors": "D. V. Parkhomenko, I. L. Mazurenko", "title": "Tensor-based approach to accelerate deformable part models", "comments": "5 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article provides next step towards solving speed bottleneck of any\nsystem that intensively uses convolutions operations (e.g. CNN). Method\ndescribed in the article is applied on deformable part models (DPM) algorithm.\nMethod described here is based on multidimensional tensors and provides\nefficient tradeoff between DPM performance and accuracy. Experiments on various\ndatabases, including Pascal VOC, show that the proposed method allows\ndecreasing a number of convolutions up to 4.5 times compared with DPM v.5,\nwhile maintaining similar accuracy. If insignificant accuracy degradation is\nallowable, higher computational gain can be achieved. The method consists of\nfilters tensor decomposition and convolutions shortening using the decomposed\nfilter. Mathematical overview of the proposed method as well as simulation\nresults are provided.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jun 2017 11:33:18 GMT"}], "update_date": "2017-07-12", "authors_parsed": [["Parkhomenko", "D. V.", ""], ["Mazurenko", "I. L.", ""]]}, {"id": "1707.03275", "submitter": "Javier Conte Alcaraz", "authors": "Javier Conte Alcaraz, Sanam Moghaddamnia and J\\\"urgen Peissig", "title": "Mobile Quantification and Therapy Course Tracking for Gait\n  Rehabilitation", "comments": "5 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel autonomous quality metric to quantify the\nrehabilitations progress of subjects with knee/hip operations. The presented\nmethod supports digital analysis of human gait patterns using smartphones. The\nalgorithm related to the autonomous metric utilizes calibrated acceleration,\ngyroscope and magnetometer signals from seven Inertial Measurement Unit\nattached on the lower body in order to classify and generate the grading system\nvalues. The developed Android application connects the seven Inertial\nMeasurement Units via Bluetooth and performs the data acquisition and\nprocessing in real-time. In total nine features per acceleration direction and\nlower body joint angle are calculated and extracted in real-time to achieve a\nfast feedback to the user. We compare the classification accuracy and\nquantification capabilities of Linear Discriminant Analysis, Principal\nComponent Analysis and Naive Bayes algorithms. The presented system is able to\nclassify patients and control subjects with an accuracy of up to 100\\%. The\noutcomes can be saved on the device or transmitted to treating physicians for\nlater control of the subject's improvements and the efficiency of physiotherapy\ntreatments in motor rehabilitation. The proposed autonomous quality metric\nsolution bears great potential to be used and deployed to support digital\nhealthcare and therapy.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jul 2017 11:54:40 GMT"}], "update_date": "2017-07-12", "authors_parsed": [["Alcaraz", "Javier Conte", ""], ["Moghaddamnia", "Sanam", ""], ["Peissig", "J\u00fcrgen", ""]]}, {"id": "1707.03296", "submitter": "Luming Tang", "authors": "Luming Tang, Boyang Deng, Haiyu Zhao, Shuai Yi", "title": "Hierarchical Deep Recurrent Architecture for Video Understanding", "comments": "Accepted as Classification Challenge Track paper in CVPR 2017\n  Workshop on YouTube-8M Large-Scale Video Understanding", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces the system we developed for the Youtube-8M Video\nUnderstanding Challenge, in which a large-scale benchmark dataset was used for\nmulti-label video classification. The proposed framework contains hierarchical\ndeep architecture, including the frame-level sequence modeling part and the\nvideo-level classification part. In the frame-level sequence modelling part, we\nexplore a set of methods including Pooling-LSTM (PLSTM), Hierarchical-LSTM\n(HLSTM), Random-LSTM (RLSTM) in order to address the problem of large amount of\nframes in a video. We also introduce two attention pooling methods, single\nattention pooling (ATT) and multiply attention pooling (Multi-ATT) so that we\ncan pay more attention to the informative frames in a video and ignore the\nuseless frames. In the video-level classification part, two methods are\nproposed to increase the classification performance, i.e.\nHierarchical-Mixture-of-Experts (HMoE) and Classifier Chains (CC). Our final\nsubmission is an ensemble consisting of 18 sub-models. In terms of the official\nevaluation metric Global Average Precision (GAP) at 20, our best submission\nachieves 0.84346 on the public 50% of test dataset and 0.84333 on the private\n50% of test data.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jul 2017 14:25:16 GMT"}], "update_date": "2017-07-12", "authors_parsed": [["Tang", "Luming", ""], ["Deng", "Boyang", ""], ["Zhao", "Haiyu", ""], ["Yi", "Shuai", ""]]}, {"id": "1707.03321", "submitter": "Stanislas Chambon", "authors": "Stanislas Chambon, Mathieu Galtier, Pierrick Arnal, Gilles Wainrib and\n  Alexandre Gramfort", "title": "A deep learning architecture for temporal sleep stage classification\n  using multivariate and multimodal time series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sleep stage classification constitutes an important preliminary exam in the\ndiagnosis of sleep disorders. It is traditionally performed by a sleep expert\nwho assigns to each 30s of signal a sleep stage, based on the visual inspection\nof signals such as electroencephalograms (EEG), electrooculograms (EOG),\nelectrocardiograms (ECG) and electromyograms (EMG). We introduce here the first\ndeep learning approach for sleep stage classification that learns end-to-end\nwithout computing spectrograms or extracting hand-crafted features, that\nexploits all multivariate and multimodal Polysomnography (PSG) signals (EEG,\nEMG and EOG), and that can exploit the temporal context of each 30s window of\ndata. For each modality the first layer learns linear spatial filters that\nexploit the array of sensors to increase the signal-to-noise ratio, and the\nlast layer feeds the learnt representation to a softmax classifier. Our model\nis compared to alternative automatic approaches based on convolutional networks\nor decisions trees. Results obtained on 61 publicly available PSG records with\nup to 20 EEG channels demonstrate that our network architecture yields\nstate-of-the-art performance. Our study reveals a number of insights on the\nspatio-temporal distribution of the signal of interest: a good trade-off for\noptimal classification performance measured with balanced accuracy is to use 6\nEEG with 2 EOG (left and right) and 3 EMG chin channels. Also exploiting one\nminute of data before and after each data segment offers the strongest\nimprovement when a limited number of channels is available. As sleep experts,\nour system exploits the multivariate and multimodal nature of PSG signals in\norder to deliver state-of-the-art classification performance with a small\ncomputational cost.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jul 2017 08:29:36 GMT"}, {"version": "v2", "created": "Mon, 27 Nov 2017 09:37:28 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Chambon", "Stanislas", ""], ["Galtier", "Mathieu", ""], ["Arnal", "Pierrick", ""], ["Wainrib", "Gilles", ""], ["Gramfort", "Alexandre", ""]]}, {"id": "1707.03374", "submitter": "Abhishek Gupta", "authors": "YuXuan Liu, Abhishek Gupta, Pieter Abbeel, Sergey Levine", "title": "Imitation from Observation: Learning to Imitate Behaviors from Raw Video\n  via Context Translation", "comments": "Accepted at ICRA 2018, Brisbane. YuXuan Liu and Abhishek Gupta had\n  equal contribution", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.NE cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Imitation learning is an effective approach for autonomous systems to acquire\ncontrol policies when an explicit reward function is unavailable, using\nsupervision provided as demonstrations from an expert, typically a human\noperator. However, standard imitation learning methods assume that the agent\nreceives examples of observation-action tuples that could be provided, for\ninstance, to a supervised learning algorithm. This stands in contrast to how\nhumans and animals imitate: we observe another person performing some behavior\nand then figure out which actions will realize that behavior, compensating for\nchanges in viewpoint, surroundings, object positions and types, and other\nfactors. We term this kind of imitation learning \"imitation-from-observation,\"\nand propose an imitation learning method based on video prediction with context\ntranslation and deep reinforcement learning. This lifts the assumption in\nimitation learning that the demonstration should consist of observations in the\nsame environment configuration, and enables a variety of interesting\napplications, including learning robotic skills that involve tool use simply by\nobserving videos of human tool use. Our experimental results show the\neffectiveness of our approach in learning a wide range of real-world robotic\ntasks modeled after common household chores from videos of a human\ndemonstrator, including sweeping, ladling almonds, pushing objects as well as a\nnumber of tasks in simulation.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jul 2017 17:23:53 GMT"}, {"version": "v2", "created": "Mon, 18 Jun 2018 21:00:13 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Liu", "YuXuan", ""], ["Gupta", "Abhishek", ""], ["Abbeel", "Pieter", ""], ["Levine", "Sergey", ""]]}, {"id": "1707.03376", "submitter": "Wei-Lin Hsiao", "authors": "Wei-Lin Hsiao, Kristen Grauman", "title": "Learning the Latent \"Look\": Unsupervised Discovery of a Style-Coherent\n  Embedding from Fashion Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  What defines a visual style? Fashion styles emerge organically from how\npeople assemble outfits of clothing, making them difficult to pin down with a\ncomputational model. Low-level visual similarity can be too specific to detect\nstylistically similar images, while manually crafted style categories can be\ntoo abstract to capture subtle style differences. We propose an unsupervised\napproach to learn a style-coherent representation. Our method leverages\nprobabilistic polylingual topic models based on visual attributes to discover a\nset of latent style factors. Given a collection of unlabeled fashion images,\nour approach mines for the latent styles, then summarizes outfits by how they\nmix those styles. Our approach can organize galleries of outfits by style\nwithout requiring any style labels. Experiments on over 100K images demonstrate\nits promise for retrieving, mixing, and summarizing fashion images by their\nstyle.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jul 2017 17:28:59 GMT"}, {"version": "v2", "created": "Thu, 3 Aug 2017 05:10:52 GMT"}], "update_date": "2017-08-04", "authors_parsed": [["Hsiao", "Wei-Lin", ""], ["Grauman", "Kristen", ""]]}, {"id": "1707.03383", "submitter": "Christopher Beckham", "authors": "Christopher Beckham and Christopher Pal", "title": "A step towards procedural terrain generation with GANs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Procedural terrain generation for video games has been traditionally been\ndone with smartly designed but handcrafted algorithms that generate heightmaps.\nWe propose a first step toward the learning and synthesis of these using recent\nadvances in deep generative modelling with openly available satellite imagery\nfrom NASA.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jul 2017 17:44:20 GMT"}], "update_date": "2017-07-12", "authors_parsed": [["Beckham", "Christopher", ""], ["Pal", "Christopher", ""]]}, {"id": "1707.03435", "submitter": "Gustavo Gil", "authors": "Giovanni Savino, Simone Piantini, Gustavo Gil, Marco Pierini", "title": "Obstacle detection test in real-word traffic contexts for the purposes\n  of motorcycle autonomous emergency braking (MAEB)", "comments": "25th International Technical Conference on the Enhanced Safety of\n  Vehicles (2017)", "journal-ref": null, "doi": null, "report-no": "Paper Number 17-0047", "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research suggests that a Motorcycle Autonomous Emergency Braking system\n(MAEB) could influence 25% of the crashes involving powered two wheelers\n(PTWs). By automatically slowing down a host PTW of up to 10 km/h in inevitable\ncollision scenarios, MAEB could potentially mitigate the crash severity for the\nriders. The feasibility of automatic decelerations of motorcycles was shown via\nfield trials in controlled environment. However, the feasibility of correct\nMAEB triggering in the real traffic context is still unclear. In particular,\nMAEB requires an accurate obstacle detection, the feasibility of which from a\nsingle track vehicle has not been confirmed yet. To address this issue, our\nstudy presents obstacle detection tests in a real-world MAEB-sensitive crash\nscenario.\n", "versions": [{"version": "v1", "created": "Sun, 25 Jun 2017 09:44:17 GMT"}, {"version": "v2", "created": "Mon, 2 Oct 2017 20:57:23 GMT"}], "update_date": "2017-10-04", "authors_parsed": [["Savino", "Giovanni", ""], ["Piantini", "Simone", ""], ["Gil", "Gustavo", ""], ["Pierini", "Marco", ""]]}, {"id": "1707.03467", "submitter": "Lei Chu", "authors": "Lei Chu, Robert Qiu, Haichun Liu, Zenan Ling, Tianhong Zhang and Jijun\n  Wang", "title": "Individual Recognition in Schizophrenia using Deep Learning Methods with\n  Random Forest and Voting Classifiers: Insights from Resting State EEG Streams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, there has been a growing interest in monitoring brain activity for\nindividual recognition system. So far these works are mainly focussing on\nsingle channel data or fragment data collected by some advanced brain\nmonitoring modalities. In this study we propose new individual recognition\nschemes based on spatio-temporal resting state Electroencephalography (EEG)\ndata. Besides, instead of using features derived from artificially-designed\nprocedures, modified deep learning architectures which aim to automatically\nextract an individual's unique features are developed to conduct\nclassification. Our designed deep learning frameworks are proved of a small but\nconsistent advantage of replacing the $softmax$ layer with Random Forest.\nAdditionally, a voting layer is added at the top of designed neural networks in\norder to tackle the classification problem arisen from EEG streams. Lastly,\nvarious experiments are implemented to evaluate the performance of the designed\ndeep learning architectures; Results indicate that the proposed EEG-based\nindividual recognition scheme yields a high degree of classification accuracy:\n$81.6\\%$ for characteristics in high risk (CHR) individuals, $96.7\\%$ for\nclinically stable first episode patients with schizophrenia (FES) and $99.2\\%$\nfor healthy controls (HC).\n", "versions": [{"version": "v1", "created": "Tue, 20 Jun 2017 01:23:24 GMT"}, {"version": "v2", "created": "Wed, 17 Jan 2018 13:06:39 GMT"}], "update_date": "2018-01-18", "authors_parsed": [["Chu", "Lei", ""], ["Qiu", "Robert", ""], ["Liu", "Haichun", ""], ["Ling", "Zenan", ""], ["Zhang", "Tianhong", ""], ["Wang", "Jijun", ""]]}, {"id": "1707.03468", "submitter": "Jianyu Lin", "authors": "Jianyu Lin, Neil T. Clancy, Daniel S. Elson", "title": "Recovering Dense Tissue Multispectral Signal from in vivo RGB Images", "comments": "accepted by Hamlyn Symposium 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hyperspectral/multispectral imaging (HSI/MSI) contains rich information\nclinical applications, such as 1) narrow band imaging for vascular\nvisualisation; 2) oxygen saturation for intraoperative perfusion monitoring and\nclinical decision making [1]; 3) tissue classification and identification of\npathology [2]. The current systems which provide pixel-level HSI/MSI signal can\nbe generally divided into two types: spatial scanning and spectral scanning.\nHowever, the trade-off between spatial/spectral resolution, the acquisition\ntime, and the hardware complexity hampers implementation in real-world\napplications, especially intra-operatively. Acquiring high resolution images in\nreal-time is important for HSI/MSI in intra-operative imaging, to alleviate the\nside effect caused by breathing, heartbeat, and other sources of motion.\nTherefore, we developed an algorithm to recover a pixel-level MSI stack using\nonly the captured snapshot RGB images from a normal camera. We refer to this\ntechnique as \"super-spectral-resolution\". The proposed method enables recovery\nof pixel-level-dense MSI signals with 24 spectral bands at ~11 frames per\nsecond (FPS) on a GPU. Multispectral data captured from porcine bowel and\nsheep/rabbit uteri in vivo has been used for training, and the algorithm has\nbeen validated using unseen in vivo animal experiments.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jun 2017 15:38:44 GMT"}], "update_date": "2017-07-13", "authors_parsed": [["Lin", "Jianyu", ""], ["Clancy", "Neil T.", ""], ["Elson", "Daniel S.", ""]]}, {"id": "1707.03469", "submitter": "Evgeny Burnaev", "authors": "Alexander Kuleshov, Alexander Bernstein, Evgeny Burnaev, Yury Yanovich", "title": "Machine Learning in Appearance-based Robot Self-localization", "comments": "7 pages, 3 figures, ICMLA 2017 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An appearance-based robot self-localization problem is considered in the\nmachine learning framework. The appearance space is composed of all possible\nimages, which can be captured by a robot's visual system under all robot\nlocalizations. Using recent manifold learning and deep learning techniques, we\npropose a new geometrically motivated solution based on training data\nconsisting of a finite set of images captured in known locations of the robot.\nThe solution includes estimation of the robot localization mapping from the\nappearance space to the robot localization space, as well as estimation of the\ninverse mapping for modeling visual image features. The latter allows solving\nthe robot localization problem as the Kalman filtering problem.\n", "versions": [{"version": "v1", "created": "Sat, 17 Jun 2017 15:32:53 GMT"}, {"version": "v2", "created": "Wed, 4 Oct 2017 22:17:24 GMT"}], "update_date": "2017-10-06", "authors_parsed": [["Kuleshov", "Alexander", ""], ["Bernstein", "Alexander", ""], ["Burnaev", "Evgeny", ""], ["Yanovich", "Yury", ""]]}, {"id": "1707.03470", "submitter": "Chaoyang Zhu", "authors": "Zhiqiang Zeng, Jian Zhang, Xiaodong Wang, Yuming Chen, Chaoyang Zhu", "title": "Place recognition: An Overview of Vision Perspective", "comments": "Applied Sciences (2018)", "journal-ref": null, "doi": "10.3390/app8112257", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Place recognition is one of the most fundamental topics in computer vision\nand robotics communities, where the task is to accurately and efficiently\nrecognize the location of a given query image. Despite years of wisdom\naccumulated in this field, place recognition still remains an open problem due\nto the various ways in which the appearance of real-world places may differ.\nThis paper presents an overview of the place recognition literature. Since\ncondition invariant and viewpoint invariant features are essential factors to\nlong-term robust visual place recognition system, We start with traditional\nimage description methodology developed in the past, which exploit techniques\nfrom image retrieval field. Recently, the rapid advances of related fields such\nas object detection and image classification have inspired a new technique to\nimprove visual place recognition system, i.e., convolutional neural networks\n(CNNs). Thus we then introduce recent progress of visual place recognition\nsystem based on CNNs to automatically learn better image representations for\nplaces. Eventually, we close with discussions and future work of place\nrecognition.\n", "versions": [{"version": "v1", "created": "Sat, 17 Jun 2017 11:34:34 GMT"}, {"version": "v2", "created": "Sun, 30 Dec 2018 14:26:37 GMT"}], "update_date": "2019-01-01", "authors_parsed": [["Zeng", "Zhiqiang", ""], ["Zhang", "Jian", ""], ["Wang", "Xiaodong", ""], ["Chen", "Yuming", ""], ["Zhu", "Chaoyang", ""]]}, {"id": "1707.03491", "submitter": "Hui Fang", "authors": "Hui Fang, Meng Zhang", "title": "Creatism: A deep-learning photographer capable of creating professional\n  work", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine-learning excels in many areas with well-defined goals. However, a\nclear goal is usually not available in art forms, such as photography. The\nsuccess of a photograph is measured by its aesthetic value, a very subjective\nconcept. This adds to the challenge for a machine learning approach.\n  We introduce Creatism, a deep-learning system for artistic content creation.\nIn our system, we break down aesthetics into multiple aspects, each can be\nlearned individually from a shared dataset of professional examples. Each\naspect corresponds to an image operation that can be optimized efficiently. A\nnovel editing tool, dramatic mask, is introduced as one operation that improves\ndramatic lighting for a photo. Our training does not require a dataset with\nbefore/after image pairs, or any additional labels to indicate different\naspects in aesthetics.\n  Using our system, we mimic the workflow of a landscape photographer, from\nframing for the best composition to carrying out various post-processing\noperations. The environment for our virtual photographer is simulated by a\ncollection of panorama images from Google Street View. We design a\n\"Turing-test\"-like experiment to objectively measure quality of its creations,\nwhere professional photographers rate a mixture of photographs from different\nsources blindly. Experiments show that a portion of our robot's creation can be\nconfused with professional work.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jul 2017 23:18:50 GMT"}], "update_date": "2017-07-13", "authors_parsed": [["Fang", "Hui", ""], ["Zhang", "Meng", ""]]}, {"id": "1707.03501", "submitter": "Jiajun Lu", "authors": "Jiajun Lu, Hussein Sibai, Evan Fabry, David Forsyth", "title": "NO Need to Worry about Adversarial Examples in Object Detection in\n  Autonomous Vehicles", "comments": "Accepted to CVPR 2017, Spotlight Oral Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has been shown that most machine learning algorithms are susceptible to\nadversarial perturbations. Slightly perturbing an image in a carefully chosen\ndirection in the image space may cause a trained neural network model to\nmisclassify it. Recently, it was shown that physical adversarial examples\nexist: printing perturbed images then taking pictures of them would still\nresult in misclassification. This raises security and safety concerns.\n  However, these experiments ignore a crucial property of physical objects: the\ncamera can view objects from different distances and at different angles. In\nthis paper, we show experiments that suggest that current constructions of\nphysical adversarial examples do not disrupt object detection from a moving\nplatform. Instead, a trained neural network classifies most of the pictures\ntaken from different distances and angles of a perturbed image correctly. We\nbelieve this is because the adversarial property of the perturbation is\nsensitive to the scale at which the perturbed picture is viewed, so (for\nexample) an autonomous car will misclassify a stop sign only from a small range\nof distances.\n  Our work raises an important question: can one construct examples that are\nadversarial for many or most viewing conditions? If so, the construction should\noffer very significant insights into the internal representation of patterns by\ndeep networks. If not, there is a good prospect that adversarial examples can\nbe reduced to a curiosity with little practical impact.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jul 2017 00:09:50 GMT"}], "update_date": "2017-07-13", "authors_parsed": [["Lu", "Jiajun", ""], ["Sibai", "Hussein", ""], ["Fabry", "Evan", ""], ["Forsyth", "David", ""]]}, {"id": "1707.03502", "submitter": "Jindong Wang", "authors": "Jindong Wang, Yiqiang Chen, Shuji Hao, Xiaohui Peng and Lisha Hu", "title": "Deep Learning for Sensor-based Activity Recognition: A Survey", "comments": "10 pages, 2 figures, and 5 tables; submitted to Pattern Recognition\n  Letters (second revision)", "journal-ref": null, "doi": "10.1016/j.patrec.2018.02.010", "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sensor-based activity recognition seeks the profound high-level knowledge\nabout human activities from multitudes of low-level sensor readings.\nConventional pattern recognition approaches have made tremendous progress in\nthe past years. However, those methods often heavily rely on heuristic\nhand-crafted feature extraction, which could hinder their generalization\nperformance. Additionally, existing methods are undermined for unsupervised and\nincremental learning tasks. Recently, the recent advancement of deep learning\nmakes it possible to perform automatic high-level feature extraction thus\nachieves promising performance in many areas. Since then, deep learning based\nmethods have been widely adopted for the sensor-based activity recognition\ntasks. This paper surveys the recent advance of deep learning based\nsensor-based activity recognition. We summarize existing literature from three\naspects: sensor modality, deep model, and application. We also present detailed\ninsights on existing work and propose grand challenges for future research.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jul 2017 00:21:04 GMT"}, {"version": "v2", "created": "Thu, 14 Dec 2017 03:11:15 GMT"}], "update_date": "2018-03-02", "authors_parsed": [["Wang", "Jindong", ""], ["Chen", "Yiqiang", ""], ["Hao", "Shuji", ""], ["Peng", "Xiaohui", ""], ["Hu", "Lisha", ""]]}, {"id": "1707.03548", "submitter": "Zheng Zhang", "authors": "Zheng Zhang, Yong Xu, Ling Shao, Jian Yang", "title": "Discriminative Block-Diagonal Representation Learning for Image\n  Recognition", "comments": "Accepted by TNNLS, and the matlab codes are available at\n  https://sites.google.com/site/darrenzz219/Home/publication", "journal-ref": null, "doi": "10.1109/TNNLS.2017.2712801", "report-no": null, "categories": "cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing block-diagonal representation researches mainly focuses on casting\nblock-diagonal regularization on training data, while only little attention is\ndedicated to concurrently learning both block-diagonal representations of\ntraining and test data. In this paper, we propose a discriminative\nblock-diagonal low-rank representation (BDLRR) method for recognition. In\nparticular, the elaborate BDLRR is formulated as a joint optimization problem\nof shrinking the unfavorable representation from off-block-diagonal elements\nand strengthening the compact block-diagonal representation under the\nsemi-supervised framework of low-rank representation. To this end, we first\nimpose penalty constraints on the negative representation to eliminate the\ncorrelation between different classes such that the incoherence criterion of\nthe extra-class representation is boosted. Moreover, a constructed subspace\nmodel is developed to enhance the self-expressive power of training samples and\nfurther build the representation bridge between the training and test samples,\nsuch that the coherence of the learned intra-class representation is\nconsistently heightened. Finally, the resulting optimization problem is solved\nelegantly by employing an alternative optimization strategy, and a simple\nrecognition algorithm on the learned representation is utilized for final\nprediction. Extensive experimental results demonstrate that the proposed method\nachieves superb recognition results on four face image datasets, three\ncharacter datasets, and the fifteen scene multi-categories dataset. It not only\nshows superior potential on image recognition but also outperforms\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jul 2017 05:33:57 GMT"}], "update_date": "2017-07-13", "authors_parsed": [["Zhang", "Zheng", ""], ["Xu", "Yong", ""], ["Shao", "Ling", ""], ["Yang", "Jian", ""]]}, {"id": "1707.03553", "submitter": "Burak Uzkent", "authors": "Burak Uzkent, Aneesh Rangnekar, M.J. Hoffman", "title": "Aerial Vehicle Tracking by Adaptive Fusion of Hyperspectral Likelihood\n  Maps", "comments": "Accepted at the International Conference on Computer Vision and\n  Pattern Recognition Workshops, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hyperspectral cameras can provide unique spectral signatures for consistently\ndistinguishing materials that can be used to solve surveillance tasks. In this\npaper, we propose a novel real-time hyperspectral likelihood maps-aided\ntracking method (HLT) inspired by an adaptive hyperspectral sensor. A moving\nobject tracking system generally consists of registration, object detection,\nand tracking modules. We focus on the target detection part and remove the\nnecessity to build any offline classifiers and tune a large amount of\nhyperparameters, instead learning a generative target model in an online manner\nfor hyperspectral channels ranging from visible to infrared wavelengths. The\nkey idea is that, our adaptive fusion method can combine likelihood maps from\nmultiple bands of hyperspectral imagery into one single more distinctive\nrepresentation increasing the margin between mean value of foreground and\nbackground pixels in the fused map. Experimental results show that the HLT not\nonly outperforms all established fusion methods but is on par with the current\nstate-of-the-art hyperspectral target tracking frameworks.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jul 2017 05:55:54 GMT"}], "update_date": "2017-07-13", "authors_parsed": [["Uzkent", "Burak", ""], ["Rangnekar", "Aneesh", ""], ["Hoffman", "M. J.", ""]]}, {"id": "1707.03574", "submitter": "Menghan Hu", "authors": "Menghan Hu, Xiongkuo Min, Guangtao Zhai, Wenhan Zhu, Yucheng Zhu,\n  Zhaodi Wang, Xiaokang Yang, Guang Tian", "title": "Terahertz Security Image Quality Assessment by No-reference Model\n  Observers", "comments": "13 pages, 8 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To provide the possibility of developing objective image quality assessment\n(IQA) algorithms for THz security images, we constructed the THz security image\ndatabase (THSID) including a total of 181 THz security images with the\nresolution of 127*380. The main distortion types in THz security images were\nfirst analyzed for the design of subjective evaluation criteria to acquire the\nmean opinion scores. Subsequently, the existing no-reference IQA algorithms,\nwhich were 5 opinion-aware approaches viz., NFERM, GMLF, DIIVINE, BRISQUE and\nBLIINDS2, and 8 opinion-unaware approaches viz., QAC, SISBLIM, NIQE, FISBLIM,\nCPBD, S3 and Fish_bb, were executed for the evaluation of the THz security\nimage quality. The statistical results demonstrated the superiority of Fish_bb\nover the other testing IQA approaches for assessing the THz image quality with\nPLCC (SROCC) values of 0.8925 (-0.8706), and with RMSE value of 0.3993. The\nlinear regression analysis and Bland-Altman plot further verified that the\nFish__bb could substitute for the subjective IQA. Nonetheless, for the\nclassification of THz security images, we tended to use S3 as a criterion for\nranking THz security image grades because of the relatively low false positive\nrate in classifying bad THz image quality into acceptable category (24.69%).\nInterestingly, due to the specific property of THz image, the average pixel\nintensity gave the best performance than the above complicated IQA algorithms,\nwith the PLCC, SROCC and RMSE of 0.9001, -0.8800 and 0.3857, respectively. This\nstudy will help the users such as researchers or security staffs to obtain the\nTHz security images of good quality. Currently, our research group is\nattempting to make this research more comprehensive.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jul 2017 07:26:06 GMT"}, {"version": "v2", "created": "Tue, 3 Oct 2017 14:01:53 GMT"}], "update_date": "2017-10-04", "authors_parsed": [["Hu", "Menghan", ""], ["Min", "Xiongkuo", ""], ["Zhai", "Guangtao", ""], ["Zhu", "Wenhan", ""], ["Zhu", "Yucheng", ""], ["Wang", "Zhaodi", ""], ["Yang", "Xiaokang", ""], ["Tian", "Guang", ""]]}, {"id": "1707.03628", "submitter": "Vincenzo Suriani", "authors": "Domenico Bloisi, Francesco Del Duchetto, Tiziano Manoni, Vincenzo\n  Suriani", "title": "Machine Learning for RealisticBall Detection in RoboCup SPL", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this technical report, we describe the use of a machine learning approach\nfor detecting the realistic black and white ball currently in use in the\nRoboCup Standard Platform League. Our aim is to provide a ready-to-use software\nmodule that can be useful for the RoboCup SPL community. To this end, the\napproach is integrated within the official B-Human code release 2016. The\ncomplete code for the approach presented in this work can be downloaded from\nthe SPQR Team homepage at http://spqr.diag.uniroma1.it and from the SPQR Team\nGitHub repository at https://github.com/SPQRTeam/SPQRBallPerceptor. The\napproach has been tested in multiple environments, both indoor and outdoor.\nFurthermore, the ball detector described in this technical report has been used\nby the SPQR Robot Soccer Team during the competitions of the Robocup German\nOpen 2017. To facilitate the use of our code by other teams, we have prepared a\nstep-by-step installation guide.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jul 2017 10:21:05 GMT"}], "update_date": "2017-07-13", "authors_parsed": [["Bloisi", "Domenico", ""], ["Del Duchetto", "Francesco", ""], ["Manoni", "Tiziano", ""], ["Suriani", "Vincenzo", ""]]}, {"id": "1707.03631", "submitter": "Sungrae Park", "authors": "Sungrae Park, Jun-Keon Park, Su-Jin Shin, Il-Chul Moon", "title": "Adversarial Dropout for Supervised and Semi-supervised Learning", "comments": "submitted to AAAI-18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the training with adversarial examples, which are generated by\nadding a small but worst-case perturbation on input examples, has been proved\nto improve generalization performance of neural networks. In contrast to the\nindividually biased inputs to enhance the generality, this paper introduces\nadversarial dropout, which is a minimal set of dropouts that maximize the\ndivergence between the outputs from the network with the dropouts and the\ntraining supervisions. The identified adversarial dropout are used to\nreconfigure the neural network to train, and we demonstrated that training on\nthe reconfigured sub-network improves the generalization performance of\nsupervised and semi-supervised learning tasks on MNIST and CIFAR-10. We\nanalyzed the trained model to reason the performance improvement, and we found\nthat adversarial dropout increases the sparsity of neural networks more than\nthe standard dropout does.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jul 2017 10:25:57 GMT"}, {"version": "v2", "created": "Mon, 18 Sep 2017 00:36:45 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["Park", "Sungrae", ""], ["Park", "Jun-Keon", ""], ["Shin", "Su-Jin", ""], ["Moon", "Il-Chul", ""]]}, {"id": "1707.03684", "submitter": "Yoonho Boo", "authors": "Yoonho Boo and Wonyong Sung", "title": "Structured Sparse Ternary Weight Coding of Deep Neural Networks for\n  Efficient Hardware Implementations", "comments": "This paper is accepted in SIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) usually demand a large amount of operations for\nreal-time inference. Especially, fully-connected layers contain a large number\nof weights, thus they usually need many off-chip memory accesses for inference.\nWe propose a weight compression method for deep neural networks, which allows\nvalues of +1 or -1 only at predetermined positions of the weights so that\ndecoding using a table can be conducted easily. For example, the structured\nsparse (8,2) coding allows at most two non-zero values among eight weights.\nThis method not only enables multiplication-free DNN implementations but also\ncompresses the weight storage by up to x32 compared to floating-point networks.\nWeight distribution normalization and gradual pruning techniques are applied to\nmitigate the performance degradation. The experiments are conducted with\nfully-connected deep neural networks and convolutional neural networks.\n", "versions": [{"version": "v1", "created": "Sat, 1 Jul 2017 05:38:55 GMT"}], "update_date": "2017-07-13", "authors_parsed": [["Boo", "Yoonho", ""], ["Sung", "Wonyong", ""]]}, {"id": "1707.03685", "submitter": "Wei Cui", "authors": "Wei Cui, Liang Gao", "title": "Optical Mapping Near-eye Three-dimensional Display with Correct Focus\n  Cues", "comments": "5 pages, 6 figures, 2 tables, short article for Optics Letters", "journal-ref": null, "doi": "10.1364/OL.42.002475", "report-no": null, "categories": "cs.CV physics.med-ph physics.optics", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an optical mapping near-eye (OMNI) three-dimensional display\nmethod for wearable devices. By dividing a display screen into different\nsub-panels and optically mapping them to various depths, we create a multiplane\nvolumetric image with correct focus cues for depth perception. The resultant\nsystem can drive the eye's accommodation to the distance that is consistent\nwith binocular stereopsis, thereby alleviating the vergence-accommodation\nconflict, the primary cause for eye fatigue and discomfort. Compared with the\nprevious methods, the OMNI display offers prominent advantages in adaptability,\nimage dynamic range, and refresh rate.\n", "versions": [{"version": "v1", "created": "Wed, 24 May 2017 19:13:44 GMT"}], "update_date": "2017-08-02", "authors_parsed": [["Cui", "Wei", ""], ["Gao", "Liang", ""]]}, {"id": "1707.03688", "submitter": "Shih-Gu Huang", "authors": "Soo-Chang Pei, Shih-Gu Huang", "title": "Two-dimensional nonseparable discrete linear canonical transform based\n  on CM-CC-CM-CC decomposition", "comments": "Accepted by Journal of the Optical Society of America A (JOSA A)", "journal-ref": "Journal of the Optical Society of America A, Vol. 33, Issue 2, pp.\n  214-227, 2016", "doi": "10.1364/JOSAA.33.000214", "report-no": null, "categories": "cs.CV physics.optics", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a generalization of the two-dimensional Fourier transform (2D FT) and 2D\nfractional Fourier transform, the 2D nonseparable linear canonical transform\n(2D NsLCT) is useful in optics, signal and image processing. To reduce the\ndigital implementation complexity of the 2D NsLCT, some previous works\ndecomposed the 2D NsLCT into several low-complexity operations, including 2D\nFT, 2D chirp multiplication (2D CM) and 2D affine transformations. However, 2D\naffine transformations will introduce interpolation error. In this paper, we\npropose a new decomposition called CM-CC-CM-CC decomposition, which decomposes\nthe 2D NsLCT into two 2D CMs and two 2D chirp convolutions (2D CCs). No 2D\naffine transforms are involved. Simulation results show that the proposed\nmethods have higher accuracy, lower computational complexity and smaller error\nin the additivity property compared with the previous works. Plus, the proposed\nmethods have perfect reversibility property that one can reconstruct the input\nsignal/image losslessly from the output.\n", "versions": [{"version": "v1", "created": "Fri, 26 May 2017 16:45:54 GMT"}], "update_date": "2017-07-13", "authors_parsed": [["Pei", "Soo-Chang", ""], ["Huang", "Shih-Gu", ""]]}, {"id": "1707.03689", "submitter": "Shih-Gu Huang", "authors": "Soo-Chang Pei, Shih-Gu Huang, Jian-Jiun Ding", "title": "Discrete Gyrator Transforms: Computational Algorithms and Applications", "comments": "Accepted by IEEE Transactions on Signal Processing", "journal-ref": "IEEE Transactions on Signal Processing, Volume 63, Issue 16,\n  Aug.15, 2015", "doi": "10.1109/TSP.2015.2437845", "report-no": null, "categories": "cs.CV physics.optics", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As an extension of the 2D fractional Fourier transform (FRFT) and a special\ncase of the 2D linear canonical transform (LCT), the gyrator transform was\nintroduced to produce rotations in twisted space/spatial-frequency planes. It\nis a useful tool in optics, signal processing and image processing. In this\npaper, we develop discrete gyrator transforms (DGTs) based on the 2D LCT.\nTaking the advantage of the additivity property of the 2D LCT, we propose three\nkinds of DGTs, each of which is a cascade of low-complexity operators. These\nDGTs have different constraints, characteristics, and properties, and are\nrealized by different computational algorithms. Besides, we propose a kind of\nDGT based on the eigenfunctions of the gyrator transform. This DGT is an\northonormal transform, and thus its comprehensive properties, especially the\nadditivity property, make it more useful in many applications. We also develop\nan efficient computational algorithm to significantly reduce the complexity of\nthis DGT. At the end, a brief review of some important applications of the DGTs\nis presented, including mode conversion, sampling and reconstruction,\nwatermarking, and image encryption.\n", "versions": [{"version": "v1", "created": "Sat, 3 Jun 2017 08:53:16 GMT"}], "update_date": "2017-07-13", "authors_parsed": [["Pei", "Soo-Chang", ""], ["Huang", "Shih-Gu", ""], ["Ding", "Jian-Jiun", ""]]}, {"id": "1707.03692", "submitter": "Chunyu Xie", "authors": "Chunyu Xie, Ce Li, Baochang Zhang, Chen Chen and Jungong Han", "title": "Deep Fisher Discriminant Learning for Mobile Hand Gesture Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gesture recognition is a challenging problem in the field of biometrics. In\nthis paper, we integrate Fisher criterion into Bidirectional Long-Short Term\nMemory (BLSTM) network and Bidirectional Gated Recurrent Unit (BGRU),thus\nleading to two new deep models termed as F-BLSTM and F-BGRU. BothFisher\ndiscriminative deep models can effectively classify the gesture based on\nanalyzing the acceleration and angular velocity data of the human gestures.\nMoreover, we collect a large Mobile Gesture Database (MGD) based on the\naccelerations and angular velocities containing 5547 sequences of 12 gestures.\nExtensive experiments are conducted to validate the superior performance of the\nproposed networks as compared to the state-of-the-art BLSTM and BGRU on MGD\ndatabase and two benchmark databases (i.e. BUAA mobile gesture and SmartWatch\ngesture).\n", "versions": [{"version": "v1", "created": "Wed, 12 Jul 2017 13:05:19 GMT"}], "update_date": "2017-07-13", "authors_parsed": [["Xie", "Chunyu", ""], ["Li", "Ce", ""], ["Zhang", "Baochang", ""], ["Chen", "Chen", ""], ["Han", "Jungong", ""]]}, {"id": "1707.03705", "submitter": "Julien Fade", "authors": "Julien Fade, Est\\'eban Perrotin, J\\'er\\^ome Bobin", "title": "Two-pixel polarimetric camera by compressive sensing", "comments": null, "journal-ref": null, "doi": "10.1364/ao.57.00b102", "report-no": null, "categories": "cs.CV physics.optics", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an original concept of compressive sensing (CS) polarimetric\nimaging based on a digital micro-mirror (DMD) array and two single-pixel\ndetectors. The polarimetric sensitivity of the proposed setup is due to an\nexperimental imperfection of reflecting mirrors which is exploited here to form\nan original reconstruction problem, including a CS problem and a source\nseparation task. We show that a two-step approach tackling each problem\nsuccessively is outperformed by a dedicated combined reconstruction method,\nwhich is explicited in this article and preferably implemented through a\nreweighted FISTA algorithm. The combined reconstruction approach is then\nfurther improved by including physical constraints specific to the polarimetric\nimaging context considered, which are implemented in an original constrained\nGFB algorithm. Numerical simulations demonstrate the efficiency of the 2-pixel\nCS polarimetric imaging setup to retrieve polarimetric contrast data with\nsignificant compression rate and good reconstruction quality. The influence of\nexperimental imperfections of the DMD are also analyzed through numerical\nsimulations, and 2D polarimetric imaging reconstruction results are finally\npresented.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jun 2017 09:11:05 GMT"}], "update_date": "2018-05-23", "authors_parsed": [["Fade", "Julien", ""], ["Perrotin", "Est\u00e9ban", ""], ["Bobin", "J\u00e9r\u00f4me", ""]]}, {"id": "1707.03710", "submitter": "Irina Andra Tache", "authors": "Tache Irina Andra", "title": "Contour and Centreline Tracking of Vessels from Angiograms using the\n  Classical Image Processing Techniques", "comments": "12 pages, 1 figures, 2 tables with figures, journal", "journal-ref": "Buletinul Institutului Politehnic din Iasi, Sectia Automatica si\n  Calculatoare, Vol. 62(66), No. 3, pp. 49-60, July 2016, ISSN 1220-2169", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article deals with the problem of vessel edge and centerline detection\nusing classical image processing techniques due to their simpleness and\neasiness to be implemented. The method is divided into four steps: the vessel\nenhancement which implies a non-linear filtering proposed by Frangi, the\nthresholding using Otsu method and the contour detection using the Canny edge\ndetector due to its good performances for the small vessels and the\nmorphological skeletonisation. The algorithms are tested on real data collected\nfrom a cardiac catheterism laboratory and it is accurate for images with good\nspatial resolution (512*512). The output image can be used for further\nprocessing in order to find the vessel length or its radius.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jun 2017 16:39:58 GMT"}], "update_date": "2017-07-13", "authors_parsed": [["Andra", "Tache Irina", ""]]}, {"id": "1707.03716", "submitter": "An Pan", "authors": "Yan Zhang, An Pan, Ming Lei, Baoli Yao", "title": "Data preprocessing methods for robust Fourier ptychographic microscopy", "comments": "7 pages, 8 figures", "journal-ref": "Optical Engineering 56(12), 123107 (2017)", "doi": "10.1117/1.OE.56.12.123107", "report-no": null, "categories": "cs.CV physics.optics", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fourier ptychographic microscopy (FPM) is a recently proposed computational\nimaging technique with both high resolution and wide field-of-view. In current\nFP experimental setup, the dark-field images with high-angle illuminations are\neasily submerged by stray light and background noise due to the low\nsignal-to-noise ratio, thus significantly degrading the reconstruction quality\nand also imposing a major restriction on the synthetic numerical aperture (NA)\nof the FP approach. To this end, an overall and systematic data preprocessing\nscheme for noise removal from FP's raw dataset is provided, which involves\nsampling analysis as well as underexposed/overexposed treatments, then followed\nby the elimination of unknown stray light and suppression of inevitable\nbackground noise, especially Gaussian noise and CCD dark current in our\nexperiments. The reported non-parametric scheme facilitates great enhancements\nof the FP's performance, which has been demonstrated experimentally that the\nbenefits of noise removal by these methods far outweigh its defects of\nconcomitant signal loss. In addition, it could be flexibly cooperated with the\nexisting state-of-the-art algorithms, producing a stronger robustness of the FP\napproach in various applications.\n", "versions": [{"version": "v1", "created": "Sun, 4 Jun 2017 16:13:59 GMT"}], "update_date": "2017-12-19", "authors_parsed": [["Zhang", "Yan", ""], ["Pan", "An", ""], ["Lei", "Ming", ""], ["Yao", "Baoli", ""]]}, {"id": "1707.03717", "submitter": "Amanda Ramcharan", "authors": "Amanda Ramcharan, Kelsee Baranowski, Peter McCloskey, Babuali Ahmed,\n  James Legg, and David Hughes", "title": "Using Transfer Learning for Image-Based Cassava Disease Detection", "comments": "10 pages, 4 figures", "journal-ref": "Frontiers in Plant Science 2017 vol. 8 p. 1852", "doi": "10.3389/fpls.2017.01852", "report-no": null, "categories": "cs.CV cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cassava is the third largest source of carbohydrates for human food in the\nworld but is vulnerable to virus diseases, which threaten to destabilize food\nsecurity in sub-Saharan Africa. Novel methods of cassava disease detection are\nneeded to support improved control which will prevent this crisis. Image\nrecognition offers both a cost effective and scalable technology for disease\ndetection. New transfer learning methods offer an avenue for this technology to\nbe easily deployed on mobile devices. Using a dataset of cassava disease images\ntaken in the field in Tanzania, we applied transfer learning to train a deep\nconvolutional neural network to identify three diseases and two types of pest\ndamage (or lack thereof). The best trained model accuracies were 98% for brown\nleaf spot (BLS), 96% for red mite damage (RMD), 95% for green mite damage\n(GMD), 98% for cassava brown streak disease (CBSD), and 96% for cassava mosaic\ndisease (CMD). The best model achieved an overall accuracy of 93% for data not\nused in the training process. Our results show that the transfer learning\napproach for image recognition of field images offers a fast, affordable, and\neasily deployable strategy for digital plant disease detection.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jun 2017 15:01:59 GMT"}, {"version": "v2", "created": "Tue, 1 Aug 2017 19:29:43 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Ramcharan", "Amanda", ""], ["Baranowski", "Kelsee", ""], ["McCloskey", "Peter", ""], ["Ahmed", "Babuali", ""], ["Legg", "James", ""], ["Hughes", "David", ""]]}, {"id": "1707.03718", "submitter": "Abhishek Chaurasia", "authors": "Abhishek Chaurasia and Eugenio Culurciello", "title": "LinkNet: Exploiting Encoder Representations for Efficient Semantic\n  Segmentation", "comments": "5 pages, 5 figures, GitHub: https://github.com/e-lab/LinkNet", "journal-ref": null, "doi": "10.1109/VCIP.2017.8305148", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pixel-wise semantic segmentation for visual scene understanding not only\nneeds to be accurate, but also efficient in order to find any use in real-time\napplication. Existing algorithms even though are accurate but they do not focus\non utilizing the parameters of neural network efficiently. As a result they are\nhuge in terms of parameters and number of operations; hence slow too. In this\npaper, we propose a novel deep neural network architecture which allows it to\nlearn without any significant increase in number of parameters. Our network\nuses only 11.5 million parameters and 21.2 GFLOPs for processing an image of\nresolution 3x640x360. It gives state-of-the-art performance on CamVid and\ncomparable results on Cityscapes dataset. We also compare our networks\nprocessing time on NVIDIA GPU and embedded system device with existing\nstate-of-the-art architectures for different image resolutions.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jun 2017 20:37:17 GMT"}], "update_date": "2018-04-03", "authors_parsed": [["Chaurasia", "Abhishek", ""], ["Culurciello", "Eugenio", ""]]}, {"id": "1707.03720", "submitter": "Y F Li", "authors": "F. Li, J. Du", "title": "Multiband NFC for High-Throughput Wireless Computer Vision Sensor\n  Network", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vision sensors lie in the heart of computer vision. In many computer vision\napplications, such as AR/VR, non-contacting near-field communication (NFC) with\nhigh throughput is required to transfer information to algorithms. In this\nwork, we proposed a novel NFC system which utilizes multiple frequency bands to\nachieve high throughput.\n", "versions": [{"version": "v1", "created": "Sun, 28 May 2017 06:43:29 GMT"}], "update_date": "2017-07-13", "authors_parsed": [["Li", "F.", ""], ["Du", "J.", ""]]}, {"id": "1707.03742", "submitter": "Francisco Gomez-Donoso", "authors": "Francisco Gomez-Donoso, Sergio Orts-Escolano and Miguel Cazorla", "title": "Large-scale Multiview 3D Hand Pose Dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate hand pose estimation at joint level has several uses on human-robot\ninteraction, user interfacing and virtual reality applications. Yet, it\ncurrently is not a solved problem. The novel deep learning techniques could\nmake a great improvement on this matter but they need a huge amount of\nannotated data. The hand pose datasets released so far present some issues that\nmake them impossible to use on deep learning methods such as the few number of\nsamples, high-level abstraction annotations or samples consisting in depth\nmaps. In this work, we introduce a multiview hand pose dataset in which we\nprovide color images of hands and different kind of annotations for each, i.e\nthe bounding box and the 2D and 3D location on the joints in the hand. Besides,\nwe introduce a simple yet accurate deep learning architecture for real-time\nrobust 2D hand pose estimation.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jul 2017 14:39:49 GMT"}, {"version": "v2", "created": "Thu, 13 Jul 2017 08:05:53 GMT"}, {"version": "v3", "created": "Tue, 18 Jul 2017 19:02:55 GMT"}], "update_date": "2017-07-20", "authors_parsed": [["Gomez-Donoso", "Francisco", ""], ["Orts-Escolano", "Sergio", ""], ["Cazorla", "Miguel", ""]]}, {"id": "1707.03775", "submitter": "Dingfu Zhou", "authors": "Dingfu Zhou, Yuchao Dai and Hongdong Li", "title": "Pixel-variant Local Homography for Fisheye Stereo Rectification\n  Minimizing Resampling Distortion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large field-of-view fisheye lens cameras have attracted more and more\nresearchers' attention in the field of robotics. However, there does not exist\na convenient off-the-shelf stereo rectification approach which can be applied\ndirectly to fisheye stereo rig. One obvious drawback of existing methods is\nthat the resampling distortion (which is defined as the loss of pixels due to\nunder-sampling and the creation of new pixels due to over-sampling during\nrectification process) is severe if we want to obtain a rectification with\nepipolar line (not epipolar circle) constraint. To overcome this weakness, we\npropose a novel pixel-wise local homography technique for stereo rectification.\nFirst, we prove that there indeed exist enough degrees of freedom to apply\npixel-wise local homography for stereo rectification. Then we present a method\nto exploit these freedoms and the solution via an optimization framework.\nFinally, the robustness and effectiveness of the proposed method have been\nverified on real fisheye lens images. The rectification results show that the\nproposed approach can effectively reduce the resampling distortion in\ncomparison with existing methods while satisfying the epipolar line constraint.\nBy employing the proposed method, dense stereo matching and 3D reconstruction\nfor fisheye lens camera become as easy as perspective lens cameras.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jul 2017 15:50:37 GMT"}], "update_date": "2017-07-13", "authors_parsed": [["Zhou", "Dingfu", ""], ["Dai", "Yuchao", ""], ["Li", "Hongdong", ""]]}, {"id": "1707.03816", "submitter": "Chao Ma", "authors": "Chao Ma, Jia-Bin Huang, Xiaokang Yang, Ming-Hsuan Yang", "title": "Robust Visual Tracking via Hierarchical Convolutional Features", "comments": "To appear in T-PAMI 2018, project page at\n  https://sites.google.com/site/chaoma99/hcft-tracking", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose to exploit the rich hierarchical features of deep\nconvolutional neural networks to improve the accuracy and robustness of visual\ntracking. Deep neural networks trained on object recognition datasets consist\nof multiple convolutional layers. These layers encode target appearance with\ndifferent levels of abstraction. For example, the outputs of the last\nconvolutional layers encode the semantic information of targets and such\nrepresentations are invariant to significant appearance variations. However,\ntheir spatial resolutions are too coarse to precisely localize the target. In\ncontrast, features from earlier convolutional layers provide more precise\nlocalization but are less invariant to appearance changes. We interpret the\nhierarchical features of convolutional layers as a nonlinear counterpart of an\nimage pyramid representation and explicitly exploit these multiple levels of\nabstraction to represent target objects. Specifically, we learn adaptive\ncorrelation filters on the outputs from each convolutional layer to encode the\ntarget appearance. We infer the maximum response of each layer to locate\ntargets in a coarse-to-fine manner. To further handle the issues with scale\nestimation and re-detecting target objects from tracking failures caused by\nheavy occlusion or out-of-the-view movement, we conservatively learn another\ncorrelation filter, that maintains a long-term memory of target appearance, as\na discriminative classifier. We apply the classifier to two types of object\nproposals: (1) proposals with a small step size and tightly around the\nestimated location for scale estimation; and (2) proposals with large step size\nand across the whole image for target re-detection. Extensive experimental\nresults on large-scale benchmark datasets show that the proposed algorithm\nperforms favorably against state-of-the-art tracking methods.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jul 2017 17:54:21 GMT"}, {"version": "v2", "created": "Sat, 11 Aug 2018 06:02:26 GMT"}], "update_date": "2018-08-14", "authors_parsed": [["Ma", "Chao", ""], ["Huang", "Jia-Bin", ""], ["Yang", "Xiaokang", ""], ["Yang", "Ming-Hsuan", ""]]}, {"id": "1707.03848", "submitter": "Yan Zhang", "authors": "Yan Zhang, G. M. Dilshan Godaliyadda, Nicola Ferrier, Emine B. Gulsoy,\n  Charles A. Bouman, Charudatta Phatak", "title": "Reduced Electron Exposure for Energy-Dispersive Spectroscopy using\n  Dynamic Sampling", "comments": "22 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analytical electron microscopy and spectroscopy of biological specimens,\npolymers, and other beam sensitive materials has been a challenging area due to\nirradiation damage. There is a pressing need to develop novel imaging and\nspectroscopic imaging methods that will minimize such sample damage as well as\nreduce the data acquisition time. The latter is useful for high-throughput\nanalysis of materials structure and chemistry. In this work, we present a novel\nmachine learning based method for dynamic sparse sampling of EDS data using a\nscanning electron microscope. Our method, based on the supervised learning\napproach for dynamic sampling algorithm and neural networks based\nclassification of EDS data, allows a dramatic reduction in the total sampling\nof up to 90%, while maintaining the fidelity of the reconstructed elemental\nmaps and spectroscopic data. We believe this approach will enable imaging and\nelemental mapping of materials that would otherwise be inaccessible to these\nanalysis techniques.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jun 2017 15:05:14 GMT"}], "update_date": "2017-07-14", "authors_parsed": [["Zhang", "Yan", ""], ["Godaliyadda", "G. M. Dilshan", ""], ["Ferrier", "Nicola", ""], ["Gulsoy", "Emine B.", ""], ["Bouman", "Charles A.", ""], ["Phatak", "Charudatta", ""]]}, {"id": "1707.03891", "submitter": "Ke Yan", "authors": "Ke Yan, Le Lu, Ronald M. Summers", "title": "Unsupervised Body Part Regression via Spatially Self-ordering\n  Convolutional Neural Networks", "comments": "Oral presentation in ISBI18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic body part recognition for CT slices can benefit various medical\nimage applications. Recent deep learning methods demonstrate promising\nperformance, with the requirement of large amounts of labeled images for\ntraining. The intrinsic structural or superior-inferior slice ordering\ninformation in CT volumes is not fully exploited. In this paper, we propose a\nconvolutional neural network (CNN) based Unsupervised Body part Regression\n(UBR) algorithm to address this problem. A novel unsupervised learning method\nand two inter-sample CNN loss functions are presented. Distinct from previous\nwork, UBR builds a coordinate system for the human body and outputs a\ncontinuous score for each axial slice, representing the normalized position of\nthe body part in the slice. The training process of UBR resembles a\nself-organization process: slice scores are learned from inter-slice\nrelationships. The training samples are unlabeled CT volumes that are abundant,\nthus no extra annotation effort is needed. UBR is simple, fast, and accurate.\nQuantitative and qualitative experiments validate its effectiveness. In\naddition, we show two applications of UBR in network initialization and anomaly\ndetection.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jul 2017 19:56:55 GMT"}, {"version": "v2", "created": "Wed, 7 Mar 2018 01:18:39 GMT"}], "update_date": "2018-03-08", "authors_parsed": [["Yan", "Ke", ""], ["Lu", "Le", ""], ["Summers", "Ronald M.", ""]]}, {"id": "1707.03946", "submitter": "Ricardo Fabbri", "authors": "Anil Usumezbas, Ricardo Fabbri, Benjamin Kimia", "title": "The Surfacing of Multiview 3D Drawings via Lofting and Occlusion\n  Reasoning", "comments": "CVPR 2017 expanded version with improvements over camera ready,\n  Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition\n  CVPR, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The three-dimensional reconstruction of scenes from multiple views has made\nimpressive strides in recent years, chiefly by methods correlating isolated\nfeature points, intensities, or curvilinear structure. In the general setting,\ni.e., without requiring controlled acquisition, limited number of objects,\nabundant patterns on objects, or object curves to follow particular models, the\nmajority of these methods produce unorganized point clouds, meshes, or voxel\nrepresentations of the reconstructed scene, with some exceptions producing 3D\ndrawings as networks of curves. Many applications, e.g., robotics, urban\nplanning, industrial design, and hard surface modeling, however, require\nstructured representations which make explicit 3D curves, surfaces, and their\nspatial relationships. Reconstructing surface representations can now be\nconstrained by the 3D drawing acting like a scaffold to hang on the computed\nrepresentations, leading to increased robustness and quality of reconstruction.\nThis paper presents one way of completing such 3D drawings with surface\nreconstructions, by exploring occlusion reasoning through lofting algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jul 2017 01:11:15 GMT"}], "update_date": "2017-07-14", "authors_parsed": [["Usumezbas", "Anil", ""], ["Fabbri", "Ricardo", ""], ["Kimia", "Benjamin", ""]]}, {"id": "1707.03981", "submitter": "Gautam Malu", "authors": "Gautam Malu, Raju S. Bapi, Bipin Indurkhya", "title": "Learning Photography Aesthetics with Deep CNNs", "comments": "Accepted in The 28th Modern Artificial Intelligence and Cognitive\n  Science Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic photo aesthetic assessment is a challenging artificial intelligence\ntask. Existing computational approaches have focused on modeling a single\naesthetic score or a class (good or bad), however these do not provide any\ndetails on why the photograph is good or bad, or which attributes contribute to\nthe quality of the photograph. To obtain both accuracy and human interpretation\nof the score, we advocate learning the aesthetic attributes along with the\nprediction of the overall score. For this purpose, we propose a novel multitask\ndeep convolution neural network, which jointly learns eight aesthetic\nattributes along with the overall aesthetic score. We report near human\nperformance in the prediction of the overall aesthetic score. To understand the\ninternal representation of these attributes in the learned model, we also\ndevelop the visualization technique using back propagation of gradients. These\nvisualizations highlight the important image regions for the corresponding\nattributes, thus providing insights about model's representation of these\nattributes. We showcase the diversity and complexity associated with different\nattributes through a qualitative analysis of the activation maps.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jul 2017 05:16:03 GMT"}], "update_date": "2017-07-14", "authors_parsed": [["Malu", "Gautam", ""], ["Bapi", "Raju S.", ""], ["Indurkhya", "Bipin", ""]]}, {"id": "1707.03985", "submitter": "Chunhua Shen", "authors": "Hui Li, Peng Wang, Chunhua Shen", "title": "Towards End-to-end Text Spotting with Convolutional Recurrent Neural\n  Networks", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we jointly address the problem of text detection and\nrecognition in natural scene images based on convolutional recurrent neural\nnetworks. We propose a unified network that simultaneously localizes and\nrecognizes text with a single forward pass, avoiding intermediate processes\nlike image cropping and feature re-calculation, word separation, or character\ngrouping. In contrast to existing approaches that consider text detection and\nrecognition as two distinct tasks and tackle them one by one, the proposed\nframework settles these two tasks concurrently. The whole framework can be\ntrained end-to-end, requiring only images, the ground-truth bounding boxes and\ntext labels. Through end-to-end training, the learned features can be more\ninformative, which improves the overall performance. The convolutional features\nare calculated only once and shared by both detection and recognition, which\nsaves processing time. Our proposed method has achieved competitive performance\non several benchmark datasets.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jul 2017 06:02:00 GMT"}], "update_date": "2017-07-14", "authors_parsed": [["Li", "Hui", ""], ["Wang", "Peng", ""], ["Shen", "Chunhua", ""]]}, {"id": "1707.03986", "submitter": "Cheng Li", "authors": "Yue He, Kaidi Cao, Cheng Li and Chen Change Loy", "title": "Merge or Not? Learning to Group Faces via Imitation Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Given a large number of unlabeled face images, face grouping aims at\nclustering the images into individual identities present in the data. This task\nremains a challenging problem despite the remarkable capability of deep\nlearning approaches in learning face representation. In particular, grouping\nresults can still be egregious given profile faces and a large number of\nuninteresting faces and noisy detections. Often, a user needs to correct the\nerroneous grouping manually. In this study, we formulate a novel face grouping\nframework that learns clustering strategy from ground-truth simulated behavior.\nThis is achieved through imitation learning (a.k.a apprenticeship learning or\nlearning by watching) via inverse reinforcement learning (IRL). In contrast to\nexisting clustering approaches that group instances by similarity, our\nframework makes sequential decision to dynamically decide when to merge two\nface instances/groups driven by short- and long-term rewards. Extensive\nexperiments on three benchmark datasets show that our framework outperforms\nunsupervised and supervised baselines.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jul 2017 06:16:43 GMT"}], "update_date": "2017-07-14", "authors_parsed": [["He", "Yue", ""], ["Cao", "Kaidi", ""], ["Li", "Cheng", ""], ["Loy", "Chen Change", ""]]}, {"id": "1707.03993", "submitter": "Weixin Yang", "authors": "Weixin Yang, Terry Lyons, Hao Ni, Cordelia Schmid, Lianwen Jin", "title": "Developing the Path Signature Methodology and its Application to\n  Landmark-based Human Action Recognition", "comments": "14 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Landmark-based human action recognition in videos is a challenging task in\ncomputer vision. One key step is to design a generic approach that generates\ndiscriminative features for the spatial structure and temporal dynamics. To\nthis end, we regard the evolving landmark data as a high-dimensional path and\napply non-linear path signature techniques to provide an expressive, robust,\nnon-linear, and interpretable representation for the sequential events. We do\nnot extract signature features from the raw path, rather we propose path\ndisintegrations and path transformations as preprocessing steps. Path\ndisintegrations turn a high-dimensional path linearly into a collection of\nlower-dimensional paths; some of these paths are in pose space while others are\ndefined over a multiscale collection of temporal intervals. Path\ntransformations decorate the paths with additional coordinates in standard ways\nto allow the truncated signatures of transformed paths to expose additional\nfeatures. For spatial representation, we apply the signature transform to\nvectorize the paths that arise out of pose disintegration, and for temporal\nrepresentation, we apply it again to describe this evolving vectorization.\nFinally, all the features are collected together to constitute the input vector\nof a linear single-hidden-layer fully-connected network for classification.\nExperimental results on four datasets demonstrated that the proposed feature\nset with only a linear shallow network and Dropconnect is effective and\nachieves comparable state-of-the-art results to the advanced deep networks, and\nmeanwhile, is capable of interpretation.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jul 2017 07:02:37 GMT"}, {"version": "v2", "created": "Thu, 12 Dec 2019 14:52:36 GMT"}], "update_date": "2019-12-13", "authors_parsed": [["Yang", "Weixin", ""], ["Lyons", "Terry", ""], ["Ni", "Hao", ""], ["Schmid", "Cordelia", ""], ["Jin", "Lianwen", ""]]}, {"id": "1707.04021", "submitter": "Zhong Ji", "authors": "Zhong Ji, Yaru Ma, Yanwei Pang, Xuelong Li", "title": "Query-Aware Sparse Coding for Multi-Video Summarization", "comments": "10 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given the explosive growth of online videos, it is becoming increasingly\nimportant to relieve the tedious work of browsing and managing the video\ncontent of interest. Video summarization aims at providing such a technique by\ntransforming one or multiple videos into a compact one. However, conventional\nmulti-video summarization methods often fail to produce satisfying results as\nthey ignore the user's search intent. To this end, this paper proposes a novel\nquery-aware approach by formulating the multi-video summarization in a sparse\ncoding framework, where the web images searched by the query are taken as the\nimportant preference information to reveal the query intent. To provide a\nuser-friendly summarization, this paper also develops an event-keyframe\npresentation structure to present keyframes in groups of specific events\nrelated to the query by using an unsupervised multi-graph fusion method. We\nrelease a new public dataset named MVS1K, which contains about 1, 000 videos\nfrom 10 queries and their video tags, manual annotations, and associated web\nimages. Extensive experiments on MVS1K dataset validate our approaches produce\nsuperior objective and subjective results against several recently proposed\napproaches.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jul 2017 08:18:21 GMT"}], "update_date": "2017-07-14", "authors_parsed": [["Ji", "Zhong", ""], ["Ma", "Yaru", ""], ["Pang", "Yanwei", ""], ["Li", "Xuelong", ""]]}, {"id": "1707.04025", "submitter": "Marco Loog", "authors": "Marco Loog, Jesse H. Krijthe, Are C. Jensen", "title": "On Measuring and Quantifying Performance: Error Rates, Surrogate Loss,\n  and an Example in SSL", "comments": null, "journal-ref": "In Handbook of Pattern Recognition and Computer Vision (pp. 53-68)\n  (2016)", "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In various approaches to learning, notably in domain adaptation, active\nlearning, learning under covariate shift, semi-supervised learning, learning\nwith concept drift, and the like, one often wants to compare a baseline\nclassifier to one or more advanced (or at least different) strategies. In this\nchapter, we basically argue that if such classifiers, in their respective\ntraining phases, optimize a so-called surrogate loss that it may also be\nvaluable to compare the behavior of this loss on the test set, next to the\nregular classification error rate. It can provide us with an additional view on\nthe classifiers' relative performances that error rates cannot capture. As an\nexample, limited but convincing empirical results demonstrates that we may be\nable to find semi-supervised learning strategies that can guarantee performance\nimprovements with increasing numbers of unlabeled data in terms of\nlog-likelihood. In contrast, the latter may be impossible to guarantee for the\nclassification error rate.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jul 2017 08:29:00 GMT"}], "update_date": "2017-07-14", "authors_parsed": [["Loog", "Marco", ""], ["Krijthe", "Jesse H.", ""], ["Jensen", "Are C.", ""]]}, {"id": "1707.04041", "submitter": "Christoph David Hofer M.Sc.", "authors": "Christoph Hofer and Roland Kwitt and Marc Niethammer and Andreas Uhl", "title": "Deep Learning with Topological Signatures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG math.AT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inferring topological and geometrical information from data can offer an\nalternative perspective on machine learning problems. Methods from topological\ndata analysis, e.g., persistent homology, enable us to obtain such information,\ntypically in the form of summary representations of topological features.\nHowever, such topological signatures often come with an unusual structure\n(e.g., multisets of intervals) that is highly impractical for most machine\nlearning techniques. While many strategies have been proposed to map these\ntopological signatures into machine learning compatible representations, they\nsuffer from being agnostic to the target learning task. In contrast, we propose\na technique that enables us to input topological signatures to deep neural\nnetworks and learn a task-optimal representation during training. Our approach\nis realized as a novel input layer with favorable theoretical properties.\nClassification experiments on 2D object shapes and social network graphs\ndemonstrate the versatility of the approach and, in case of the latter, we even\noutperform the state-of-the-art by a large margin.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jul 2017 09:36:05 GMT"}, {"version": "v2", "created": "Mon, 13 Nov 2017 11:38:53 GMT"}, {"version": "v3", "created": "Fri, 16 Feb 2018 07:12:19 GMT"}], "update_date": "2018-02-19", "authors_parsed": [["Hofer", "Christoph", ""], ["Kwitt", "Roland", ""], ["Niethammer", "Marc", ""], ["Uhl", "Andreas", ""]]}, {"id": "1707.04045", "submitter": "Jae Hyeon Yoo", "authors": "Jae Hyeon Yoo", "title": "Large-scale Video Classification guided by Batch Normalized LSTM\n  Translator", "comments": "This is accepted by CVPR2017 Workshop on Youtube-8M Large-scale Video\n  Understanding", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Youtube-8M dataset enhances the development of large-scale video recognition\ntechnology as ImageNet dataset has encouraged image classification, recognition\nand detection of artificial intelligence fields. For this large video dataset,\nit is a challenging task to classify a huge amount of multi-labels. By change\nof perspective, we propose a novel method by regarding labels as words. In\ndetails, we describe online learning approaches to multi-label video\nclassification that are guided by deep recurrent neural networks for video to\nsentence translator. We designed the translator based on LSTMs and found out\nthat a stochastic gating before the input of each LSTM cell can help us to\ndesign the structural details. In addition, we adopted batch normalizations\ninto our models to improve our LSTM models. Since our models are feature\nextractors, they can be used with other classifiers. Finally we report improved\nvalidation results of our models on large-scale Youtube-8M datasets and\ndiscussions for the further improvement.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jul 2017 09:48:03 GMT"}], "update_date": "2017-07-14", "authors_parsed": [["Yoo", "Jae Hyeon", ""]]}, {"id": "1707.04046", "submitter": "Ben Usman", "authors": "Ben Usman, Kate Saenko, Brian Kulis", "title": "Stable Distribution Alignment Using the Dual of the Adversarial Distance", "comments": "ICLR 2018 Conference Invite to Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Methods that align distributions by minimizing an adversarial distance\nbetween them have recently achieved impressive results. However, these\napproaches are difficult to optimize with gradient descent and they often do\nnot converge well without careful hyperparameter tuning and proper\ninitialization. We investigate whether turning the adversarial min-max problem\ninto an optimization problem by replacing the maximization part with its dual\nimproves the quality of the resulting alignment and explore its connections to\nMaximum Mean Discrepancy. Our empirical results suggest that using the dual\nformulation for the restricted family of linear discriminators results in a\nmore stable convergence to a desirable solution when compared with the\nperformance of a primal min-max GAN-like objective and an MMD objective under\nthe same restrictions. We test our hypothesis on the problem of aligning two\nsynthetic point clouds on a plane and on a real-image domain adaptation problem\non digits. In both cases, the dual formulation yields an iterative procedure\nthat gives more stable and monotonic improvement over time.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jul 2017 09:50:14 GMT"}, {"version": "v2", "created": "Sat, 28 Oct 2017 02:28:56 GMT"}, {"version": "v3", "created": "Mon, 8 Jan 2018 19:32:26 GMT"}, {"version": "v4", "created": "Tue, 30 Jan 2018 20:49:21 GMT"}], "update_date": "2018-02-01", "authors_parsed": [["Usman", "Ben", ""], ["Saenko", "Kate", ""], ["Kulis", "Brian", ""]]}, {"id": "1707.04047", "submitter": "Jingkuan Song Dr.", "authors": "Lei Zhu, Zi Huang, Xiaobai Liu, Xiangnan He, Jingkuan Song, Xiaofang\n  Zhou", "title": "Discrete Multi-modal Hashing with Canonical Views for Robust Mobile\n  Landmark Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mobile landmark search (MLS) recently receives increasing attention for its\ngreat practical values. However, it still remains unsolved due to two important\nchallenges. One is high bandwidth consumption of query transmission, and the\nother is the huge visual variations of query images sent from mobile devices.\nIn this paper, we propose a novel hashing scheme, named as canonical view based\ndiscrete multi-modal hashing (CV-DMH), to handle these problems via a novel\nthree-stage learning procedure. First, a submodular function is designed to\nmeasure visual representativeness and redundancy of a view set. With it,\ncanonical views, which capture key visual appearances of landmark with limited\nredundancy, are efficiently discovered with an iterative mining strategy.\nSecond, multi-modal sparse coding is applied to transform visual features from\nmultiple modalities into an intermediate representation. It can robustly and\nadaptively characterize visual contents of varied landmark images with certain\ncanonical views. Finally, compact binary codes are learned on intermediate\nrepresentation within a tailored discrete binary embedding model which\npreserves visual relations of images measured with canonical views and removes\nthe involved noises. In this part, we develop a new augmented Lagrangian\nmultiplier (ALM) based optimization method to directly solve the discrete\nbinary codes. We can not only explicitly deal with the discrete constraint, but\nalso consider the bit-uncorrelated constraint and balance constraint together.\nExperiments on real world landmark datasets demonstrate the superior\nperformance of CV-DMH over several state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jul 2017 09:54:26 GMT"}], "update_date": "2017-07-14", "authors_parsed": [["Zhu", "Lei", ""], ["Huang", "Zi", ""], ["Liu", "Xiaobai", ""], ["He", "Xiangnan", ""], ["Song", "Jingkuan", ""], ["Zhou", "Xiaofang", ""]]}, {"id": "1707.04061", "submitter": "Ciprian Corneanu", "authors": "Kaustubh Kulkarni, Ciprian Adrian Corneanu, Ikechukwu Ofodile, Sergio\n  Escalera, Xavier Baro, Sylwia Hyniewska, Juri Allik, and Gholamreza\n  Anbarjafari", "title": "Automatic Recognition of Facial Displays of Unfelt Emotions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans modify their facial expressions in order to communicate their internal\nstates and sometimes to mislead observers regarding their true emotional\nstates. Evidence in experimental psychology shows that discriminative facial\nresponses are short and subtle. This suggests that such behavior would be\neasier to distinguish when captured in high resolution at an increased frame\nrate. We are proposing SASE-FE, the first dataset of facial expressions that\nare either congruent or incongruent with underlying emotion states. We show\nthat overall the problem of recognizing whether facial movements are\nexpressions of authentic emotions or not can be successfully addressed by\nlearning spatio-temporal representations of the data. For this purpose, we\npropose a method that aggregates features along fiducial trajectories in a\ndeeply learnt space. Performance of the proposed model shows that on average it\nis easier to distinguish among genuine facial expressions of emotion than among\nunfelt facial expressions of emotion and that certain emotion pairs such as\ncontempt and disgust are more difficult to distinguish than the rest.\nFurthermore, the proposed methodology improves state of the art results on CK+\nand OULU-CASIA datasets for video emotion recognition, and achieves competitive\nresults when classifying facial action units on BP4D datase.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jul 2017 10:57:31 GMT"}, {"version": "v2", "created": "Tue, 9 Jan 2018 11:44:01 GMT"}], "update_date": "2018-01-10", "authors_parsed": [["Kulkarni", "Kaustubh", ""], ["Corneanu", "Ciprian Adrian", ""], ["Ofodile", "Ikechukwu", ""], ["Escalera", "Sergio", ""], ["Baro", "Xavier", ""], ["Hyniewska", "Sylwia", ""], ["Allik", "Juri", ""], ["Anbarjafari", "Gholamreza", ""]]}, {"id": "1707.04092", "submitter": "Xavier Gir\\'o-i-Nieto", "authors": "Xunyu Lin, Victor Campos, Xavier Giro-i-Nieto, Jordi Torres and\n  Cristian Canton Ferrer", "title": "Disentangling Motion, Foreground and Background Features in Videos", "comments": "Poster presented at the CVPR 2017 Workshop Brave New Ideas for Motion\n  Representations in Videos", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces an unsupervised framework to extract semantically rich\nfeatures for video representation. Inspired by how the human visual system\ngroups objects based on motion cues, we propose a deep convolutional neural\nnetwork that disentangles motion, foreground and background information. The\nproposed architecture consists of a 3D convolutional feature encoder for blocks\nof 16 frames, which is trained for reconstruction tasks over the first and last\nframes of the sequence. A preliminary supervised experiment was conducted to\nverify the feasibility of proposed method by training the model with a fraction\nof videos from the UCF-101 dataset taking as ground truth the bounding boxes\naround the activity regions. Qualitative results indicate that the network can\nsuccessfully segment foreground and background in videos as well as update the\nforeground appearance based on disentangled motion features. The benefits of\nthese learned features are shown in a discriminative classification task, where\ninitializing the network with the proposed pretraining method outperforms both\nrandom initialization and autoencoder pretraining. Our model and source code\nare publicly available at https://imatge-upc.github.io/unsupervised-2017-cvprw/ .\n", "versions": [{"version": "v1", "created": "Thu, 13 Jul 2017 12:40:28 GMT"}, {"version": "v2", "created": "Mon, 17 Jul 2017 13:50:01 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Lin", "Xunyu", ""], ["Campos", "Victor", ""], ["Giro-i-Nieto", "Xavier", ""], ["Torres", "Jordi", ""], ["Ferrer", "Cristian Canton", ""]]}, {"id": "1707.04131", "submitter": "Jonas Rauber", "authors": "Jonas Rauber, Wieland Brendel, Matthias Bethge", "title": "Foolbox: A Python toolbox to benchmark the robustness of machine\n  learning models", "comments": "Code and examples available at https://github.com/bethgelab/foolbox\n  and documentation available at http://foolbox.readthedocs.io", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Even todays most advanced machine learning models are easily fooled by almost\nimperceptible perturbations of their inputs. Foolbox is a new Python package to\ngenerate such adversarial perturbations and to quantify and compare the\nrobustness of machine learning models. It is build around the idea that the\nmost comparable robustness measure is the minimum perturbation needed to craft\nan adversarial example. To this end, Foolbox provides reference implementations\nof most published adversarial attack methods alongside some new ones, all of\nwhich perform internal hyperparameter tuning to find the minimum adversarial\nperturbation. Additionally, Foolbox interfaces with most popular deep learning\nframeworks such as PyTorch, Keras, TensorFlow, Theano and MXNet and allows\ndifferent adversarial criteria such as targeted misclassification and top-k\nmisclassification as well as different distance measures. The code is licensed\nunder the MIT license and is openly available at\nhttps://github.com/bethgelab/foolbox . The most up-to-date documentation can be\nfound at http://foolbox.readthedocs.io .\n", "versions": [{"version": "v1", "created": "Thu, 13 Jul 2017 13:59:15 GMT"}, {"version": "v2", "created": "Tue, 8 Aug 2017 03:22:01 GMT"}, {"version": "v3", "created": "Tue, 20 Mar 2018 10:10:10 GMT"}], "update_date": "2018-03-21", "authors_parsed": [["Rauber", "Jonas", ""], ["Brendel", "Wieland", ""], ["Bethge", "Matthias", ""]]}, {"id": "1707.04143", "submitter": "Linchao Zhu", "authors": "Linchao Zhu, Yanbin Liu, Yi Yang", "title": "UTS submission to Google YouTube-8M Challenge 2017", "comments": "CVPR'17 Workshop on YouTube-8M", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present our solution to Google YouTube-8M Video\nClassification Challenge 2017. We leveraged both video-level and frame-level\nfeatures in the submission. For video-level classification, we simply used a\n200-mixture Mixture of Experts (MoE) layer, which achieves GAP 0.802 on the\nvalidation set with a single model. For frame-level classification, we utilized\nseveral variants of recurrent neural networks, sequence aggregation with\nattention mechanism and 1D convolutional models. We achieved GAP 0.8408 on the\nprivate testing set with the ensemble model.\n  The source code of our models can be found in\n\\url{https://github.com/ffmpbgrnn/yt8m}.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jul 2017 14:24:39 GMT"}], "update_date": "2017-07-14", "authors_parsed": [["Zhu", "Linchao", ""], ["Liu", "Yanbin", ""], ["Yang", "Yi", ""]]}, {"id": "1707.04199", "submitter": "Anders Oland", "authors": "Anders Oland and Aayush Bansal and Roger B. Dannenberg and Bhiksha Raj", "title": "Be Careful What You Backpropagate: A Case For Linear Output Activations\n  & Gradient Boosting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we show that saturating output activation functions, such as\nthe softmax, impede learning on a number of standard classification tasks.\nMoreover, we present results showing that the utility of softmax does not stem\nfrom the normalization, as some have speculated. In fact, the normalization\nmakes things worse. Rather, the advantage is in the exponentiation of error\ngradients. This exponential gradient boosting is shown to speed up convergence\nand improve generalization. To this end, we demonstrate faster convergence and\nbetter performance on diverse classification tasks: image classification using\nCIFAR-10 and ImageNet, and semantic segmentation using PASCAL VOC 2012. In the\nlatter case, using the state-of-the-art neural network architecture, the model\nconverged 33% faster with our method (roughly two days of training less) than\nwith the standard softmax activation, and with a slightly better performance to\nboot.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jul 2017 16:19:09 GMT"}], "update_date": "2017-07-14", "authors_parsed": [["Oland", "Anders", ""], ["Bansal", "Aayush", ""], ["Dannenberg", "Roger B.", ""], ["Raj", "Bhiksha", ""]]}, {"id": "1707.04272", "submitter": "Mikel Bober-Irizar", "authors": "Mikel Bober-Irizar, Sameed Husain, Eng-Jon Ong, Miroslaw Bober", "title": "Cultivating DNN Diversity for Large Scale Video Labelling", "comments": "CVPR 2017 Youtube-8M Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate factors controlling DNN diversity in the context of the Google\nCloud and YouTube-8M Video Understanding Challenge. While it is well-known that\nensemble methods improve prediction performance, and that combining accurate\nbut diverse predictors helps, there is little knowledge on how to best promote\n& measure DNN diversity. We show that diversity can be cultivated by some\nunexpected means, such as model over-fitting or dropout variations. We also\npresent details of our solution to the video understanding problem, which\nranked #7 in the Kaggle competition (competing as the Yeti team).\n", "versions": [{"version": "v1", "created": "Thu, 13 Jul 2017 18:11:02 GMT"}], "update_date": "2017-07-17", "authors_parsed": [["Bober-Irizar", "Mikel", ""], ["Husain", "Sameed", ""], ["Ong", "Eng-Jon", ""], ["Bober", "Miroslaw", ""]]}, {"id": "1707.04318", "submitter": "Jayakorn Vongkulbhisal", "authors": "Jayakorn Vongkulbhisal, Fernando De la Torre, Jo\\~ao P. Costeira", "title": "Discriminative Optimization: Theory and Applications to Computer Vision\n  Problems", "comments": "26 pages, 28 figures", "journal-ref": "IEEE Transactions on Pattern Analysis and Machine Intelligence (\n  Volume: 41, Issue: 4, Apr 2019 )", "doi": "10.1109/TPAMI.2018.2826536", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many computer vision problems are formulated as the optimization of a cost\nfunction. This approach faces two main challenges: (i) designing a cost\nfunction with a local optimum at an acceptable solution, and (ii) developing an\nefficient numerical method to search for one (or multiple) of these local\noptima. While designing such functions is feasible in the noiseless case, the\nstability and location of local optima are mostly unknown under noise,\nocclusion, or missing data. In practice, this can result in undesirable local\noptima or not having a local optimum in the expected place. On the other hand,\nnumerical optimization algorithms in high-dimensional spaces are typically\nlocal and often rely on expensive first or second order information to guide\nthe search. To overcome these limitations, this paper proposes Discriminative\nOptimization (DO), a method that learns search directions from data without the\nneed of a cost function. Specifically, DO explicitly learns a sequence of\nupdates in the search space that leads to stationary points that correspond to\ndesired solutions. We provide a formal analysis of DO and illustrate its\nbenefits in the problem of 3D point cloud registration, camera pose estimation,\nand image denoising. We show that DO performed comparably or outperformed\nstate-of-the-art algorithms in terms of accuracy, robustness to perturbations,\nand computational efficiency.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jul 2017 20:58:32 GMT"}], "update_date": "2019-04-15", "authors_parsed": [["Vongkulbhisal", "Jayakorn", ""], ["De la Torre", "Fernando", ""], ["Costeira", "Jo\u00e3o P.", ""]]}, {"id": "1707.04406", "submitter": "Noa Arbel", "authors": "Noa Arbel, Tamar Avraham and Michael Lindenbaum", "title": "Inner-Scene Similarities as a Contextual Cue for Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using image context is an effective approach for improving object detection.\nPreviously proposed methods used contextual cues that rely on semantic or\nspatial information. In this work, we explore a different kind of contextual\ninformation: inner-scene similarity. We present the CISS (Context by Inner\nScene Similarity) algorithm, which is based on the observation that two\nvisually similar sub-image patches are likely to share semantic identities,\nespecially when both appear in the same image. CISS uses base-scores provided\nby a base detector and performs as a post-detection stage. For each candidate\nsub-image (denoted anchor), the CISS algorithm finds a few similar sub-images\n(denoted supporters), and, using them, calculates a new enhanced score for the\nanchor. This is done by utilizing the base-scores of the supporters and a\npre-trained dependency model. The new scores are modeled as a linear function\nof the base scores of the anchor and the supporters and is estimated using a\nminimum mean square error optimization. This approach results in: (a) improved\ndetection of partly occluded objects (when there are similar non-occluded\nobjects in the scene), and (b) fewer false alarms (when the base detector\nmistakenly classifies a background patch as an object). This work relates to\nDuncan and Humphreys' \"similarity theory,\" a psychophysical study. which\nsuggested that the human visual system perceptually groups similar image\nregions and that the classification of one region is affected by the estimated\nidentity of the other. Experimental results demonstrate the enhancement of a\nbase detector's scores on the PASCAL VOC dataset.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jul 2017 07:37:21 GMT"}], "update_date": "2017-07-17", "authors_parsed": [["Arbel", "Noa", ""], ["Avraham", "Tamar", ""], ["Lindenbaum", "Michael", ""]]}, {"id": "1707.04444", "submitter": "Riccardo Polvara", "authors": "George Terzakis, Riccardo Polvara, Sanjay Sharma, Phil Culverhouse and\n  Robert Sutton", "title": "Monocular Visual Odometry for an Unmanned Sea-Surface Vehicle", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We tackle the problem of localizing an autonomous sea-surface vehicle in\nriver estuarine areas using monocular camera and angular velocity input from an\ninertial sensor. Our method is challenged by two prominent drawbacks associated\nwith the environment, which are typically not present in standard visual\nsimultaneous localization and mapping (SLAM) applications on land (or air): a)\nScene depth varies significantly (from a few meters to several kilometers) and,\nb) In conjunction to the latter, there exists no ground plane to provide\nfeatures with enough disparity based on which to reliably detect motion. To\nthat end, we use the IMU orientation feedback in order to re-cast the problem\nof visual localization without the mapping component, although the map can be\nimplicitly obtained from the camera pose estimates. We find that our method\nproduces reliable odometry estimates for trajectories several hundred meters\nlong in the water. To compare the visual odometry estimates with GPS based\nground truth, we interpolate the trajectory with splines on a common parameter\nand obtain position error in meters recovering an optimal affine transformation\nbetween the two splines.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jul 2017 10:13:59 GMT"}, {"version": "v2", "created": "Wed, 19 Jul 2017 10:39:29 GMT"}], "update_date": "2017-07-20", "authors_parsed": [["Terzakis", "George", ""], ["Polvara", "Riccardo", ""], ["Sharma", "Sanjay", ""], ["Culverhouse", "Phil", ""], ["Sutton", "Robert", ""]]}, {"id": "1707.04487", "submitter": "Adrian Spurr", "authors": "Adrian Spurr, Emre Aksan, Otmar Hilliges", "title": "Guiding InfoGAN with Semi-Supervision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a new semi-supervised GAN architecture (ss-InfoGAN)\nfor image synthesis that leverages information from few labels (as little as\n0.22%, max. 10% of the dataset) to learn semantically meaningful and\ncontrollable data representations where latent variables correspond to label\ncategories. The architecture builds on Information Maximizing Generative\nAdversarial Networks (InfoGAN) and is shown to learn both continuous and\ncategorical codes and achieves higher quality of synthetic samples compared to\nfully unsupervised settings. Furthermore, we show that using small amounts of\nlabeled data speeds-up training convergence. The architecture maintains the\nability to disentangle latent variables for which no labels are available.\nFinally, we contribute an information-theoretic reasoning on how introducing\nsemi-supervision increases mutual information between synthetic and real data.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jul 2017 12:44:22 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Spurr", "Adrian", ""], ["Aksan", "Emre", ""], ["Hilliges", "Otmar", ""]]}, {"id": "1707.04555", "submitter": "Chuang Gan", "authors": "Fu Li, Chuang Gan, Xiao Liu, Yunlong Bian, Xiang Long, Yandong Li,\n  Zhichao Li, Jie Zhou, Shilei Wen", "title": "Temporal Modeling Approaches for Large-scale Youtube-8M Video\n  Understanding", "comments": "To appear on CVPR 2017 YouTube-8M Workshop(Rank 3rd out of 650 teams)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes our solution for the video recognition task of the\nGoogle Cloud and YouTube-8M Video Understanding Challenge that ranked the 3rd\nplace. Because the challenge provides pre-extracted visual and audio features\ninstead of the raw videos, we mainly investigate various temporal modeling\napproaches to aggregate the frame-level features for multi-label video\nrecognition. Our system contains three major components: two-stream sequence\nmodel, fast-forward sequence model and temporal residual neural networks.\nExperiment results on the challenging Youtube-8M dataset demonstrate that our\nproposed temporal modeling approaches can significantly improve existing\ntemporal modeling approaches in the large-scale video recognition tasks. To be\nnoted, our fast-forward LSTM with a depth of 7 layers achieves 82.75% in term\nof GAP@20 on the Kaggle Public test set.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jul 2017 16:11:14 GMT"}], "update_date": "2017-07-17", "authors_parsed": [["Li", "Fu", ""], ["Gan", "Chuang", ""], ["Liu", "Xiao", ""], ["Bian", "Yunlong", ""], ["Long", "Xiang", ""], ["Li", "Yandong", ""], ["Li", "Zhichao", ""], ["Zhou", "Jie", ""], ["Wen", "Shilei", ""]]}, {"id": "1707.04585", "submitter": "Aidan N. Gomez", "authors": "Aidan N. Gomez, Mengye Ren, Raquel Urtasun, Roger B. Grosse", "title": "The Reversible Residual Network: Backpropagation Without Storing\n  Activations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep residual networks (ResNets) have significantly pushed forward the\nstate-of-the-art on image classification, increasing in performance as networks\ngrow both deeper and wider. However, memory consumption becomes a bottleneck,\nas one needs to store the activations in order to calculate gradients using\nbackpropagation. We present the Reversible Residual Network (RevNet), a variant\nof ResNets where each layer's activations can be reconstructed exactly from the\nnext layer's. Therefore, the activations for most layers need not be stored in\nmemory during backpropagation. We demonstrate the effectiveness of RevNets on\nCIFAR-10, CIFAR-100, and ImageNet, establishing nearly identical classification\naccuracy to equally-sized ResNets, even though the activation storage\nrequirements are independent of depth.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jul 2017 03:05:43 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Gomez", "Aidan N.", ""], ["Ren", "Mengye", ""], ["Urtasun", "Raquel", ""], ["Grosse", "Roger B.", ""]]}, {"id": "1707.04610", "submitter": "Tian Guo", "authors": "Tian Guo", "title": "Cloud-based or On-device: An Empirical Study of Mobile Deep Inference", "comments": "Accepted at The IEEE International Conference on Cloud Engineering\n  (IC2E) conference 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern mobile applications are benefiting significantly from the advancement\nin deep learning, e.g., implementing real-time image recognition and\nconversational system. Given a trained deep learning model, applications\nusually need to perform a series of matrix operations based on the input data,\nin order to infer possible output values. Because of computational complexity\nand size constraints, these trained models are often hosted in the cloud. To\nutilize these cloud-based models, mobile apps will have to send input data over\nthe network. While cloud-based deep learning can provide reasonable response\ntime for mobile apps, it restricts the use case scenarios, e.g. mobile apps\nneed to have network access. With mobile specific deep learning optimizations,\nit is now possible to employ on-device inference. However, because mobile\nhardware, such as GPU and memory size, can be very limited when compared to its\ndesktop counterpart, it is important to understand the feasibility of this new\non-device deep learning inference architecture. In this paper, we empirically\nevaluate the inference performance of three Convolutional Neural Networks\n(CNNs) using a benchmark Android application we developed. Our measurement and\nanalysis suggest that on-device inference can cost up to two orders of\nmagnitude greater response time and energy when compared to cloud-based\ninference, and that loading model and computing probability are two performance\nbottlenecks for on-device deep inferences.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jul 2017 19:05:50 GMT"}, {"version": "v2", "created": "Sun, 15 Apr 2018 17:48:20 GMT"}], "update_date": "2019-03-01", "authors_parsed": [["Guo", "Tian", ""]]}, {"id": "1707.04642", "submitter": "Jonathan Rubin", "authors": "Jonathan Rubin, Rui Abreu, Anurag Ganguli, Saigopal Nelaturi, Ion\n  Matei, Kumar Sricharan", "title": "Recognizing Abnormal Heart Sounds Using Deep Learning", "comments": "IJCAI 2017 Knowledge Discovery in Healthcare Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The work presented here applies deep learning to the task of automated\ncardiac auscultation, i.e. recognizing abnormalities in heart sounds. We\ndescribe an automated heart sound classification algorithm that combines the\nuse of time-frequency heat map representations with a deep convolutional neural\nnetwork (CNN). Given the cost-sensitive nature of misclassification, our CNN\narchitecture is trained using a modified loss function that directly optimizes\nthe trade-off between sensitivity and specificity. We evaluated our algorithm\nat the 2016 PhysioNet Computing in Cardiology challenge where the objective was\nto accurately classify normal and abnormal heart sounds from single, short,\npotentially noisy recordings. Our entry to the challenge achieved a final\nspecificity of 0.95, sensitivity of 0.73 and overall score of 0.84. We achieved\nthe greatest specificity score out of all challenge entries and, using just a\nsingle CNN, our algorithm differed in overall score by only 0.02 compared to\nthe top place finisher, which used an ensemble approach.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jul 2017 21:17:24 GMT"}, {"version": "v2", "created": "Thu, 19 Oct 2017 08:27:01 GMT"}], "update_date": "2017-10-20", "authors_parsed": [["Rubin", "Jonathan", ""], ["Abreu", "Rui", ""], ["Ganguli", "Anurag", ""], ["Nelaturi", "Saigopal", ""], ["Matei", "Ion", ""], ["Sricharan", "Kumar", ""]]}, {"id": "1707.04677", "submitter": "Tianshui Chen", "authors": "Liang Lin, Lili Huang, Tianshui Chen, Yukang Gan, and Hui Cheng", "title": "Knowledge-Guided Recurrent Neural Network Learning for Task-Oriented\n  Action Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper aims at task-oriented action prediction, i.e., predicting a\nsequence of actions towards accomplishing a specific task under a certain\nscene, which is a new problem in computer vision research. The main challenges\nlie in how to model task-specific knowledge and integrate it in the learning\nprocedure. In this work, we propose to train a recurrent long-short term memory\n(LSTM) network for handling this problem, i.e., taking a scene image (including\npre-located objects) and the specified task as input and recurrently predicting\naction sequences. However, training such a network usually requires large\namounts of annotated samples for covering the semantic space (e.g., diverse\naction decomposition and ordering). To alleviate this issue, we introduce a\ntemporal And-Or graph (AOG) for task description, which hierarchically\nrepresents a task into atomic actions. With this AOG representation, we can\nproduce many valid samples (i.e., action sequences according with common sense)\nby training another auxiliary LSTM network with a small set of annotated\nsamples. And these generated samples (i.e., task-oriented action sequences)\neffectively facilitate training the model for task-oriented action prediction.\nIn the experiments, we create a new dataset containing diverse daily tasks and\nextensively evaluate the effectiveness of our approach.\n", "versions": [{"version": "v1", "created": "Sat, 15 Jul 2017 01:40:07 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Lin", "Liang", ""], ["Huang", "Lili", ""], ["Chen", "Tianshui", ""], ["Gan", "Yukang", ""], ["Cheng", "Hui", ""]]}, {"id": "1707.04682", "submitter": "Rui Zhu", "authors": "Rui Zhu, Hamed Kiani Galoogahi, Chaoyang Wang, Simon Lucey", "title": "Rethinking Reprojection: Closing the Loop for Pose-aware\n  ShapeReconstruction from a Single Image", "comments": "First sub", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An emerging problem in computer vision is the reconstruction of 3D shape and\npose of an object from a single image. Hitherto, the problem has been addressed\nthrough the application of canonical deep learning methods to regress from the\nimage directly to the 3D shape and pose labels. These approaches, however, are\nproblematic from two perspectives. First, they are minimizing the error between\n3D shapes and pose labels - with little thought about the nature of this label\nerror when reprojecting the shape back onto the image. Second, they rely on the\nonerous and ill-posed task of hand labeling natural images with respect to 3D\nshape and pose. In this paper we define the new task of pose-aware shape\nreconstruction from a single image, and we advocate that cheaper 2D annotations\nof objects silhouettes in natural images can be utilized. We design\narchitectures of pose-aware shape reconstruction which re-project the predicted\nshape back on to the image using the predicted pose. Our evaluation on several\nobject categories demonstrates the superiority of our method for predicting\npose-aware 3D shapes from natural images.\n", "versions": [{"version": "v1", "created": "Sat, 15 Jul 2017 02:54:11 GMT"}, {"version": "v2", "created": "Wed, 26 Jul 2017 17:08:57 GMT"}], "update_date": "2017-07-27", "authors_parsed": [["Zhu", "Rui", ""], ["Galoogahi", "Hamed Kiani", ""], ["Wang", "Chaoyang", ""], ["Lucey", "Simon", ""]]}, {"id": "1707.04693", "submitter": "Jeng-Hau Lin", "authors": "Jeng-Hau Lin, Tianwei Xing, Ritchie Zhao, Zhiru Zhang, Mani\n  Srivastava, Zhuowen Tu, Rajesh K. Gupta", "title": "Binarized Convolutional Neural Networks with Separable Filters for\n  Efficient Hardware Acceleration", "comments": "9 pages, 6 figures, accepted for Embedded Vision Workshop (CVPRW)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art convolutional neural networks are enormously costly in both\ncompute and memory, demanding massively parallel GPUs for execution. Such\nnetworks strain the computational capabilities and energy available to embedded\nand mobile processing platforms, restricting their use in many important\napplications. In this paper, we push the boundaries of hardware-effective CNN\ndesign by proposing BCNN with Separable Filters (BCNNw/SF), which applies\nSingular Value Decomposition (SVD) on BCNN kernels to further reduce\ncomputational and storage complexity. To enable its implementation, we provide\na closed form of the gradient over SVD to calculate the exact gradient with\nrespect to every binarized weight in backward propagation. We verify BCNNw/SF\non the MNIST, CIFAR-10, and SVHN datasets, and implement an accelerator for\nCIFAR-10 on FPGA hardware. Our BCNNw/SF accelerator realizes memory savings of\n17% and execution time reduction of 31.3% compared to BCNN with only minor\naccuracy sacrifices.\n", "versions": [{"version": "v1", "created": "Sat, 15 Jul 2017 06:17:07 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Lin", "Jeng-Hau", ""], ["Xing", "Tianwei", ""], ["Zhao", "Ritchie", ""], ["Zhang", "Zhiru", ""], ["Srivastava", "Mani", ""], ["Tu", "Zhuowen", ""], ["Gupta", "Rajesh K.", ""]]}, {"id": "1707.04771", "submitter": "Andrey Bokovoy", "authors": "Andrey Bokovoy and Konstantin Yakovlev", "title": "Original Loop-closure Detection Algorithm for Monocular vSLAM", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-319-73013-4_19", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vision-based simultaneous localization and mapping (vSLAM) is a\nwell-established problem in mobile robotics and monocular vSLAM is one of the\nmost challenging variations of that problem nowadays. In this work we study one\nof the core post-processing optimization mechanisms in vSLAM, e.g. loop-closure\ndetection. We analyze the existing methods and propose original algorithm for\nloop-closure detection, which is suitable for dense, semi-dense and\nfeature-based vSLAM methods. We evaluate the algorithm experimentally and show\nthat it contribute to more accurate mapping while speeding up the monocular\nvSLAM pipeline to the extent the latter can be used in real-time for\ncontrolling small multi-rotor vehicle (drone).\n", "versions": [{"version": "v1", "created": "Sat, 15 Jul 2017 18:12:11 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Bokovoy", "Andrey", ""], ["Yakovlev", "Konstantin", ""]]}, {"id": "1707.04781", "submitter": "Aparna John", "authors": "Artyom M. Grigoryan, Aparna John and Sos S. Agaian", "title": "Modified Alpha-Rooting Color Image Enhancement Method On The Two-Side\n  2-D Quaternion Discrete Fourier Transform And The 2-D Discrete Fourier\n  Transform", "comments": "16 pages, 53 figures (including sub-figures)", "journal-ref": "Applied Mathematics and Sciences: An International Journal\n  (MathSJ), Vol. 4, No. 1/2, June 2017", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Color in an image is resolved into 3 or 4 color components and 2-Dimages of\nthese components are stored in separate channels. Most of the color image\nenhancement algorithms are applied channel-by-channel on each image. But such a\nsystem of color image processing is not processing the original color. When a\ncolor image is represented as a quaternion image, processing is done in\noriginal colors. This paper proposes an implementation of the quaternion\napproach of enhancement algorithm for enhancing color images and is referred as\nthe modified alpha-rooting by the two-dimensional quaternion discrete Fourier\ntransform (2-D QDFT). Enhancement results of this proposed method are compared\nwith the channel-by-channel image enhancement by the 2-D DFT. Enhancements in\ncolor images are quantitatively measured by the color enhancement measure\nestimation (CEME), which allows for selecting optimum parameters for processing\nby the genetic algorithm. Enhancement of color images by the quaternion based\nmethod allows for obtaining images which are closer to the genuine\nrepresentation of the real original color.\n", "versions": [{"version": "v1", "created": "Sat, 15 Jul 2017 19:55:26 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Grigoryan", "Artyom M.", ""], ["John", "Aparna", ""], ["Agaian", "Sos S.", ""]]}, {"id": "1707.04796", "submitter": "Peter Florence", "authors": "Pat Marion, Peter R. Florence, Lucas Manuelli and Russ Tedrake", "title": "LabelFusion: A Pipeline for Generating Ground Truth Labels for Real RGBD\n  Data of Cluttered Scenes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural network (DNN) architectures have been shown to outperform\ntraditional pipelines for object segmentation and pose estimation using RGBD\ndata, but the performance of these DNN pipelines is directly tied to how\nrepresentative the training data is of the true data. Hence a key requirement\nfor employing these methods in practice is to have a large set of labeled data\nfor your specific robotic manipulation task, a requirement that is not\ngenerally satisfied by existing datasets. In this paper we develop a pipeline\nto rapidly generate high quality RGBD data with pixelwise labels and object\nposes. We use an RGBD camera to collect video of a scene from multiple\nviewpoints and leverage existing reconstruction techniques to produce a 3D\ndense reconstruction. We label the 3D reconstruction using a human assisted\nICP-fitting of object meshes. By reprojecting the results of labeling the 3D\nscene we can produce labels for each RGBD image of the scene. This pipeline\nenabled us to collect over 1,000,000 labeled object instances in just a few\ndays. We use this dataset to answer questions related to how much training data\nis required, and of what quality the data must be, to achieve high performance\nfrom a DNN architecture.\n", "versions": [{"version": "v1", "created": "Sat, 15 Jul 2017 23:17:11 GMT"}, {"version": "v2", "created": "Tue, 25 Jul 2017 19:48:42 GMT"}, {"version": "v3", "created": "Tue, 26 Sep 2017 15:16:52 GMT"}], "update_date": "2017-09-27", "authors_parsed": [["Marion", "Pat", ""], ["Florence", "Peter R.", ""], ["Manuelli", "Lucas", ""], ["Tedrake", "Russ", ""]]}, {"id": "1707.04818", "submitter": "Jiyang Gao", "authors": "Jiyang Gao, Zhenheng Yang, Ram Nevatia", "title": "RED: Reinforced Encoder-Decoder Networks for Action Anticipation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Action anticipation aims to detect an action before it happens. Many real\nworld applications in robotics and surveillance are related to this predictive\ncapability. Current methods address this problem by first anticipating visual\nrepresentations of future frames and then categorizing the anticipated\nrepresentations to actions. However, anticipation is based on a single past\nframe's representation, which ignores the history trend. Besides, it can only\nanticipate a fixed future time. We propose a Reinforced Encoder-Decoder (RED)\nnetwork for action anticipation. RED takes multiple history representations as\ninput and learns to anticipate a sequence of future representations. One\nsalient aspect of RED is that a reinforcement module is adopted to provide\nsequence-level supervision; the reward function is designed to encourage the\nsystem to make correct predictions as early as possible. We test RED on\nTVSeries, THUMOS-14 and TV-Human-Interaction datasets for action anticipation\nand achieve state-of-the-art performance on all datasets.\n", "versions": [{"version": "v1", "created": "Sun, 16 Jul 2017 04:23:57 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Gao", "Jiyang", ""], ["Yang", "Zhenheng", ""], ["Nevatia", "Ram", ""]]}, {"id": "1707.04877", "submitter": "Eelco Van Der Wel", "authors": "Eelco van der Wel, Karen Ullrich", "title": "Optical Music Recognition with Convolutional Sequence-to-Sequence Models", "comments": "ISMIR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.SD", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Optical Music Recognition (OMR) is an important technology within Music\nInformation Retrieval. Deep learning models show promising results on OMR\ntasks, but symbol-level annotated data sets of sufficient size to train such\nmodels are not available and difficult to develop. We present a deep learning\narchitecture called a Convolutional Sequence-to-Sequence model to both move\ntowards an end-to-end trainable OMR pipeline, and apply a learning process that\ntrains on full sentences of sheet music instead of individually labeled\nsymbols. The model is trained and evaluated on a human generated data set, with\nvarious image augmentations based on real-world scenarios. This data set is the\nfirst publicly available set in OMR research with sufficient size to train and\nevaluate deep learning models. With the introduced augmentations a pitch\nrecognition accuracy of 81% and a duration accuracy of 94% is achieved,\nresulting in a note level accuracy of 80%. Finally, the model is compared to\ncommercially available methods, showing a large improvements over these\napplications.\n", "versions": [{"version": "v1", "created": "Sun, 16 Jul 2017 13:11:22 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["van der Wel", "Eelco", ""], ["Ullrich", "Karen", ""]]}, {"id": "1707.04881", "submitter": "Meng Wang", "authors": "Meng Wang, Huafeng Li, Fang Li", "title": "Generative Adversarial Network based on Resnet for Conditional Image\n  Restoration", "comments": "6 pages, 15 figures, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The GANs promote an adversarive game to approximate complex and jointed\nexample probability. The networks driven by noise generate fake examples to\napproximate realistic data distributions. Later the conditional GAN merges\nprior-conditions as input in order to transfer attribute vectors to the\ncorresponding data. However, the CGAN is not designed to deal with the high\ndimension conditions since indirect guide of the learning is inefficiency. In\nthis paper, we proposed a network ResGAN to generate fine images in terms of\nextremely degenerated images. The coarse images aligned to attributes are\nembedded as the generator inputs and classifier labels. In generative network,\na straight path similar to the Resnet is cohered to directly transfer the\ncoarse images to the higher layers. And adversarial training is circularly\nimplemented to prevent degeneration of the generated images. Experimental\nresults of applying the ResGAN to datasets MNIST, CIFAR10/100 and CELEBA show\nits higher accuracy to the state-of-art GANs.\n", "versions": [{"version": "v1", "created": "Sun, 16 Jul 2017 13:38:36 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Wang", "Meng", ""], ["Li", "Huafeng", ""], ["Li", "Fang", ""]]}, {"id": "1707.04904", "submitter": "Jie Chang", "authors": "Jie Chang, Yujun Gu", "title": "Chinese Typography Transfer", "comments": "There is an error in Figure 5.(b) where the figure caption is\n  \"evaluation mse\" instead of \"Loss curve\". It can lead to the misunderstanding\n  of my performance under different configuration. So I request to withdraw", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a new network architecture for Chinese typography\ntransformation based on deep learning. The architecture consists of two\nsub-networks: (1)a fully convolutional network(FCN) aiming at transferring\nspecified typography style to another in condition of preserving structure\ninformation; (2)an adversarial network aiming at generating more realistic\nstrokes in some details. Unlike models proposed before 2012 relying on the\ncomplex segmentation of Chinese components or strokes, our model treats every\nChinese character as an inseparable image, so pre-processing or\npost-preprocessing are abandoned. Besides, our model adopts end-to-end training\nwithout pre-trained used in other deep models. The experiments demonstrates\nthat our model can synthesize realistic-looking target typography from any\nsource typography both on printed style and handwriting style.\n", "versions": [{"version": "v1", "created": "Sun, 16 Jul 2017 16:21:37 GMT"}, {"version": "v2", "created": "Wed, 2 Aug 2017 04:46:30 GMT"}], "update_date": "2017-08-03", "authors_parsed": [["Chang", "Jie", ""], ["Gu", "Yujun", ""]]}, {"id": "1707.04905", "submitter": "Raphael Sznitman", "authors": "Laurent Lejeune, Mario Christoudias, Raphael Sznitman", "title": "Expected exponential loss for gaze-based video and volume ground truth\n  annotation", "comments": "9 pages, 5 figues, MICCAI 2017 - LABELS Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many recent machine learning approaches used in medical imaging are highly\nreliant on large amounts of image and ground truth data. In the context of\nobject segmentation, pixel-wise annotations are extremely expensive to collect,\nespecially in video and 3D volumes. To reduce this annotation burden, we\npropose a novel framework to allow annotators to simply observe the object to\nsegment and record where they have looked at with a \\$200 eye gaze tracker. Our\nmethod then estimates pixel-wise probabilities for the presence of the object\nthroughout the sequence from which we train a classifier in semi-supervised\nsetting using a novel Expected Exponential loss function. We show that our\nframework provides superior performances on a wide range of medical image\nsettings compared to existing strategies and that our method can be combined\nwith current crowd-sourcing paradigms as well.\n", "versions": [{"version": "v1", "created": "Sun, 16 Jul 2017 16:22:12 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Lejeune", "Laurent", ""], ["Christoudias", "Mario", ""], ["Sznitman", "Raphael", ""]]}, {"id": "1707.04912", "submitter": "Jinzheng Cai", "authors": "Jinzheng Cai, Le Lu, Yuanpu Xie, Fuyong Xing, Lin Yang", "title": "Improving Deep Pancreas Segmentation in CT and MRI Images via Recurrent\n  Neural Contextual Learning and Direct Loss Function", "comments": "8 pages, 7 figures, accepted to Medical Image Computing and Computer\n  Assisted Interventions Conference (MICCAI) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have demonstrated very promising performance on accurate\nsegmentation of challenging organs (e.g., pancreas) in abdominal CT and MRI\nscans. The current deep learning approaches conduct pancreas segmentation by\nprocessing sequences of 2D image slices independently through deep, dense\nper-pixel masking for each image, without explicitly enforcing spatial\nconsistency constraint on segmentation of successive slices. We propose a new\nconvolutional/recurrent neural network architecture to address the contextual\nlearning and segmentation consistency problem. A deep convolutional sub-network\nis first designed and pre-trained from scratch. The output layer of this\nnetwork module is then connected to recurrent layers and can be fine-tuned for\ncontextual learning, in an end-to-end manner. Our recurrent sub-network is a\ntype of Long short-term memory (LSTM) network that performs segmentation on an\nimage by integrating its neighboring slice segmentation predictions, in the\nform of a dependent sequence processing. Additionally, a novel\nsegmentation-direct loss function (named Jaccard Loss) is proposed and deep\nnetworks are trained to optimize Jaccard Index (JI) directly. Extensive\nexperiments are conducted to validate our proposed deep models, on quantitative\npancreas segmentation using both CT and MRI scans. Our method outperforms the\nstate-of-the-art work on CT [11] and MRI pancreas segmentation [1],\nrespectively.\n", "versions": [{"version": "v1", "created": "Sun, 16 Jul 2017 16:57:25 GMT"}, {"version": "v2", "created": "Tue, 18 Jul 2017 01:33:08 GMT"}], "update_date": "2017-07-19", "authors_parsed": [["Cai", "Jinzheng", ""], ["Lu", "Le", ""], ["Xie", "Yuanpu", ""], ["Xing", "Fuyong", ""], ["Yang", "Lin", ""]]}, {"id": "1707.04931", "submitter": "Raphael Sznitman", "authors": "Stefanos Apostolopoulos, Sandro De Zanet, Carlos Ciller, Sebastian\n  Wolf, Raphael Sznitman", "title": "Pathological OCT Retinal Layer Segmentation using Branch Residual\n  U-shape Networks", "comments": "9 pages, 5 figures, MICCAI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The automatic segmentation of retinal layer structures enables\nclinically-relevant quantification and monitoring of eye disorders over time in\nOCT imaging. Eyes with late-stage diseases are particularly challenging to\nsegment, as their shape is highly warped due to pathological biomarkers. In\nthis context, we propose a novel fully Convolutional Neural Network (CNN)\narchitecture which combines dilated residual blocks in an asymmetric U-shape\nconfiguration, and can segment multiple layers of highly pathological eyes in\none shot. We validate our approach on a dataset of late-stage AMD patients and\ndemonstrate lower computational costs and higher performance compared to other\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sun, 16 Jul 2017 19:17:16 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Apostolopoulos", "Stefanos", ""], ["De Zanet", "Sandro", ""], ["Ciller", "Carlos", ""], ["Wolf", "Sebastian", ""], ["Sznitman", "Raphael", ""]]}, {"id": "1707.04940", "submitter": "Yuri G. Gordienko", "authors": "Yuriy Kochura, Sergii Stirenko, Yuri Gordienko", "title": "Comparative Performance Analysis of Neural Networks Architectures on H2O\n  Platform for Various Activation Functions", "comments": "4 pages, 6 figures, 6 tables; 2017 IEEE International Young\n  Scientists Forum on Applied Physics and Engineering (YSF-2017) (Lviv,\n  Ukraine)", "journal-ref": null, "doi": "10.1109/YSF.2017.8126654", "report-no": null, "categories": "cs.LG cs.CV cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning (deep structured learning, hierarchi- cal learning or deep\nmachine learning) is a branch of machine learning based on a set of algorithms\nthat attempt to model high- level abstractions in data by using multiple\nprocessing layers with complex structures or otherwise composed of multiple\nnon-linear transformations. In this paper, we present the results of testing\nneural networks architectures on H2O platform for various activation functions,\nstopping metrics, and other parameters of machine learning algorithm. It was\ndemonstrated for the use case of MNIST database of handwritten digits in\nsingle-threaded mode that blind selection of these parameters can hugely\nincrease (by 2-3 orders) the runtime without the significant increase of\nprecision. This result can have crucial influence for opitmization of available\nand new machine learning methods, especially for image recognition problems.\n", "versions": [{"version": "v1", "created": "Sun, 16 Jul 2017 19:57:28 GMT"}], "update_date": "2018-01-30", "authors_parsed": [["Kochura", "Yuriy", ""], ["Stirenko", "Sergii", ""], ["Gordienko", "Yuri", ""]]}, {"id": "1707.04960", "submitter": "Aidean Sharghi", "authors": "Aidean Sharghi, Jacob S. Laurel, Boqing Gong", "title": "Query-Focused Video Summarization: Dataset, Evaluation, and A Memory\n  Network Based Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have witnessed a resurgence of interest in video summarization.\nHowever, one of the main obstacles to the research on video summarization is\nthe user subjectivity - users have various preferences over the summaries. The\nsubjectiveness causes at least two problems. First, no single video summarizer\nfits all users unless it interacts with and adapts to the individual users.\nSecond, it is very challenging to evaluate the performance of a video\nsummarizer.\n  To tackle the first problem, we explore the recently proposed query-focused\nvideo summarization which introduces user preferences in the form of text\nqueries about the video into the summarization process. We propose a memory\nnetwork parameterized sequential determinantal point process in order to attend\nthe user query onto different video frames and shots. To address the second\nchallenge, we contend that a good evaluation metric for video summarization\nshould focus on the semantic information that humans can perceive rather than\nthe visual features or temporal overlaps. To this end, we collect dense\nper-video-shot concept annotations, compile a new dataset, and suggest an\nefficient evaluation method defined upon the concept annotations. We conduct\nextensive experiments contrasting our video summarizer to existing ones and\npresent detailed analyses about the dataset and the new evaluation method.\n", "versions": [{"version": "v1", "created": "Sun, 16 Jul 2017 23:11:28 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Sharghi", "Aidean", ""], ["Laurel", "Jacob S.", ""], ["Gong", "Boqing", ""]]}, {"id": "1707.04968", "submitter": "Chao Ma", "authors": "Chao Ma, Chunhua Shen, Anthony Dick, Qi Wu, Peng Wang, Anton van den\n  Hengel, Ian Reid", "title": "Visual Question Answering with Memory-Augmented Networks", "comments": "CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we exploit a memory-augmented neural network to predict\naccurate answers to visual questions, even when those answers occur rarely in\nthe training set. The memory network incorporates both internal and external\nmemory blocks and selectively pays attention to each training exemplar. We show\nthat memory-augmented neural networks are able to maintain a relatively\nlong-term memory of scarce training exemplars, which is important for visual\nquestion answering due to the heavy-tailed distribution of answers in a general\nVQA setting. Experimental results on two large-scale benchmark datasets show\nthe favorable performance of the proposed algorithm with a comparison to state\nof the art.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jul 2017 00:42:56 GMT"}, {"version": "v2", "created": "Sun, 25 Mar 2018 09:46:14 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Ma", "Chao", ""], ["Shen", "Chunhua", ""], ["Dick", "Anthony", ""], ["Wu", "Qi", ""], ["Wang", "Peng", ""], ["Hengel", "Anton van den", ""], ["Reid", "Ian", ""]]}, {"id": "1707.04974", "submitter": "Pan Ji", "authors": "Pan Ji, Ian Reid, Ravi Garg, Hongdong Li, Mathieu Salzmann", "title": "Adaptive Low-Rank Kernel Subspace Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a kernel subspace clustering method that can handle\nnon-linear models. In contrast to recent kernel subspace clustering methods\nwhich use predefined kernels, we propose to learn a low-rank kernel matrix,\nwith which mapped data in feature space are not only low-rank but also\nself-expressive. In this manner, the low-dimensional subspace structures of the\n(implicitly) mapped data are retained and manifested in the high-dimensional\nfeature space. We evaluate the proposed method extensively on both motion\nsegmentation and image clustering benchmarks, and obtain superior results,\noutperforming the kernel subspace clustering method that uses standard\nkernels[Patel 2014] and other state-of-the-art linear subspace clustering\nmethods.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jul 2017 01:25:07 GMT"}, {"version": "v2", "created": "Sat, 16 Sep 2017 14:27:05 GMT"}, {"version": "v3", "created": "Wed, 20 Sep 2017 02:33:55 GMT"}, {"version": "v4", "created": "Fri, 25 Jan 2019 18:02:31 GMT"}], "update_date": "2019-01-28", "authors_parsed": [["Ji", "Pan", ""], ["Reid", "Ian", ""], ["Garg", "Ravi", ""], ["Li", "Hongdong", ""], ["Salzmann", "Mathieu", ""]]}, {"id": "1707.04991", "submitter": "James Supancic III", "authors": "James Steven Supancic III and Deva Ramanan", "title": "Tracking as Online Decision-Making: Learning a Policy from Streaming\n  Videos with Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We formulate tracking as an online decision-making process, where a tracking\nagent must follow an object despite ambiguous image frames and a limited\ncomputational budget. Crucially, the agent must decide where to look in the\nupcoming frames, when to reinitialize because it believes the target has been\nlost, and when to update its appearance model for the tracked object. Such\ndecisions are typically made heuristically. Instead, we propose to learn an\noptimal decision-making policy by formulating tracking as a partially\nobservable decision-making process (POMDP). We learn policies with deep\nreinforcement learning algorithms that need supervision (a reward signal) only\nwhen the track has gone awry. We demonstrate that sparse rewards allow us to\nquickly train on massive datasets, several orders of magnitude more than past\nwork. Interestingly, by treating the data source of Internet videos as\nunlimited streams, we both learn and evaluate our trackers in a single, unified\ncomputational stream.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jul 2017 03:38:35 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Supancic", "James Steven", "III"], ["Ramanan", "Deva", ""]]}, {"id": "1707.04993", "submitter": "Sergey Tulyakov", "authors": "Sergey Tulyakov, Ming-Yu Liu, Xiaodong Yang, Jan Kautz", "title": "MoCoGAN: Decomposing Motion and Content for Video Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual signals in a video can be divided into content and motion. While\ncontent specifies which objects are in the video, motion describes their\ndynamics. Based on this prior, we propose the Motion and Content decomposed\nGenerative Adversarial Network (MoCoGAN) framework for video generation. The\nproposed framework generates a video by mapping a sequence of random vectors to\na sequence of video frames. Each random vector consists of a content part and a\nmotion part. While the content part is kept fixed, the motion part is realized\nas a stochastic process. To learn motion and content decomposition in an\nunsupervised manner, we introduce a novel adversarial learning scheme utilizing\nboth image and video discriminators. Extensive experimental results on several\nchallenging datasets with qualitative and quantitative comparison to the\nstate-of-the-art approaches, verify effectiveness of the proposed framework. In\naddition, we show that MoCoGAN allows one to generate videos with same content\nbut different motion as well as videos with different content and same motion.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jul 2017 03:42:17 GMT"}, {"version": "v2", "created": "Thu, 14 Dec 2017 00:04:02 GMT"}], "update_date": "2017-12-15", "authors_parsed": [["Tulyakov", "Sergey", ""], ["Liu", "Ming-Yu", ""], ["Yang", "Xiaodong", ""], ["Kautz", "Jan", ""]]}, {"id": "1707.05009", "submitter": "Pan Ji", "authors": "Pan Ji, Hongdong Li, Yuchao Dai, Ian Reid", "title": "\"Maximizing rigidity\" revisited: a convex programming approach for\n  generic 3D shape reconstruction from multiple perspective views", "comments": "to appear in ICCV'17", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rigid structure-from-motion (RSfM) and non-rigid structure-from-motion\n(NRSfM) have long been treated in the literature as separate (different)\nproblems. Inspired by a previous work which solved directly for 3D scene\nstructure by factoring the relative camera poses out, we revisit the principle\nof \"maximizing rigidity\" in structure-from-motion literature, and develop a\nunified theory which is applicable to both rigid and non-rigid structure\nreconstruction in a rigidity-agnostic way. We formulate these problems as a\nconvex semi-definite program, imposing constraints that seek to apply the\nprinciple of minimizing non-rigidity. Our results demonstrate the efficacy of\nthe approach, with state-of-the-art accuracy on various 3D reconstruction\nproblems.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jul 2017 06:18:28 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Ji", "Pan", ""], ["Li", "Hongdong", ""], ["Dai", "Yuchao", ""], ["Reid", "Ian", ""]]}, {"id": "1707.05031", "submitter": "Kyoungmin Lee", "authors": "Kyoungmin Lee, Jaeseok Choi, Jisoo Jeong, Nojun Kwak", "title": "Residual Features and Unified Prediction Network for Single Stage\n  Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, a lot of single stage detectors using multi-scale features have\nbeen actively proposed. They are much faster than two stage detectors that use\nregion proposal networks (RPN) without much degradation in the detection\nperformances. However, the feature maps in the lower layers close to the input\nwhich are responsible for detecting small objects in a single stage detector\nhave a problem of insufficient representation power because they are too\nshallow. There is also a structural contradiction that the feature maps have to\ndeliver low-level information to next layers as well as contain high-level\nabstraction for prediction. In this paper, we propose a method to enrich the\nrepresentation power of feature maps using Resblock and deconvolution layers.\nIn addition, a unified prediction module is applied to generalize output\nresults and boost earlier layers' representation power for prediction. The\nproposed method enables more precise prediction, which achieved higher score\nthan SSD on PASCAL VOC and MS COCO. In addition, it maintains the advantage of\nfast computation of a single stage detector, which requires much less\ncomputation than other detectors with similar performance. Code is available at\nhttps://github.com/kmlee-snu/run\n", "versions": [{"version": "v1", "created": "Mon, 17 Jul 2017 07:54:44 GMT"}, {"version": "v2", "created": "Tue, 18 Jul 2017 07:00:19 GMT"}, {"version": "v3", "created": "Wed, 29 Nov 2017 07:17:42 GMT"}, {"version": "v4", "created": "Fri, 5 Jan 2018 04:32:45 GMT"}], "update_date": "2018-01-08", "authors_parsed": [["Lee", "Kyoungmin", ""], ["Choi", "Jaeseok", ""], ["Jeong", "Jisoo", ""], ["Kwak", "Nojun", ""]]}, {"id": "1707.05055", "submitter": "Ya\\u{g}{\\i}z Aksoy", "authors": "Ya\\u{g}{\\i}z Aksoy, Tun\\c{c} Ozan Ayd{\\i}n and Marc Pollefeys", "title": "Information-Flow Matting", "comments": "16 pages, 13 figures, extended version of CVPR 2017 publication\n  titled \"Designing Effective Inter-pixel Information Flow for Natural Image\n  Matting\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel, purely affinity-based natural image matting algorithm.\nOur method relies on carefully defined pixel-to-pixel connections that enable\neffective use of information available in the image. We control the information\nflow from the known-opacity regions into the unknown region, as well as within\nthe unknown region itself, by utilizing multiple definitions of pixel\naffinities. Among other forms of information flow, we introduce color-mixture\nflow, which builds upon local linear embedding and effectively encapsulates the\nrelation between different pixel opacities. Our resulting novel linear system\nformulation can be solved in closed-form and is robust against several\nfundamental challenges of natural matting such as holes and remote intricate\nstructures. While our method is primarily designed as a standalone matting\ntool, we show that it can also be used for regularizing mattes obtained by\nsampling-based methods. The formulation is also extended to layer color\nestimation and we show that the use of multiple channels of flow increases the\nlayer color quality. We also demonstrate our performance in green-screen keying\nand analyze the characteristics of the utilized affinities.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jul 2017 09:35:14 GMT"}, {"version": "v2", "created": "Fri, 12 Apr 2019 13:08:18 GMT"}], "update_date": "2019-04-15", "authors_parsed": [["Aksoy", "Ya\u011f\u0131z", ""], ["Ayd\u0131n", "Tun\u00e7 Ozan", ""], ["Pollefeys", "Marc", ""]]}, {"id": "1707.05062", "submitter": "Guillaume Noyel", "authors": "Guillaume Noyel (IPRI, SIGPH@iPRI)", "title": "Speeding up the K\\\"ohler's method of contrast thresholding", "comments": "IEEE CopyrightProceedings of the IEEE International Conference on\n  Image Processing ICIP 2017", "journal-ref": "IEEE. IEEE International Conference on Image Processing ICIP 2017,\n  Sep 2017, Beijing, China. IEEE, 2017, http://2017.ieeeicip.org", "doi": "10.1109/ICIP.2017.8296295", "report-no": null, "categories": "cs.CV cs.CC cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  K{\\\"o}hler's method is a useful multi-thresholding technique based on\nboundary contrast. However, the direct algorithm has a too high complexity-O(N\n2) i.e. quadratic with the pixel numbers N-to process images at a sufficient\nspeed for practical applications. In this paper, a new algorithm to speed up\nK{\\\"o}hler's method is introduced with a complexity in O(N M), M is the number\nof grey levels. The proposed algorithm is designed for parallelisation and\nvector processing , which are available in current processors, using OpenMP\n(Open Multi-Processing) and SIMD instructions (Single Instruction on Multiple\nData). A fast implementation allows a gain factor of 405 in an image of 18\nmillion pixels and a video processing in real time (gain factor of 96).\n", "versions": [{"version": "v1", "created": "Mon, 17 Jul 2017 09:41:04 GMT"}, {"version": "v2", "created": "Fri, 2 Mar 2018 10:43:48 GMT"}], "update_date": "2019-04-25", "authors_parsed": [["Noyel", "Guillaume", "", "IPRI, SIGPH@iPRI"]]}, {"id": "1707.05137", "submitter": "Pierre Ambrosini", "authors": "Pierre Ambrosini, Daniel Ruijters, Wiro J. Niessen, Adriaan Moelker,\n  Theo van Walsum", "title": "Fully Automatic and Real-Time Catheter Segmentation in X-Ray Fluoroscopy", "comments": "Accepted to MICCAI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Augmenting X-ray imaging with 3D roadmap to improve guidance is a common\nstrategy. Such approaches benefit from automated analysis of the X-ray images,\nsuch as the automatic detection and tracking of instruments. In this paper, we\npropose a real-time method to segment the catheter and guidewire in 2D X-ray\nfluoroscopic sequences. The method is based on deep convolutional neural\nnetworks. The network takes as input the current image and the three previous\nones, and segments the catheter and guidewire in the current image.\nSubsequently, a centerline model of the catheter is constructed from the\nsegmented image. A small set of annotated data combined with data augmentation\nis used to train the network. We trained the method on images from 182 X-ray\nsequences from 23 different interventions. On a testing set with images of 55\nX-ray sequences from 5 other interventions, a median centerline distance error\nof 0.2 mm and a median tip distance error of 0.9 mm was obtained. The\nsegmentation of the instruments in 2D X-ray sequences is performed in a\nreal-time fully-automatic manner.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jul 2017 13:18:37 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Ambrosini", "Pierre", ""], ["Ruijters", "Daniel", ""], ["Niessen", "Wiro J.", ""], ["Moelker", "Adriaan", ""], ["van Walsum", "Theo", ""]]}, {"id": "1707.05224", "submitter": "Kumar Sankar Ray", "authors": "Kumar S. Ray, Anit Chakraborty, Sayandip Dutta", "title": "Detection, Recognition and Tracking of Moving Objects from Real-time\n  Video via Visual Vocabulary Model and Species Inspired PSO", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the basic problem of recognizing moving objects in\nvideo images using Visual Vocabulary model and Bag of Words and track our\nobject of interest in the subsequent video frames using species inspired PSO.\nInitially, the shadow free images are obtained by background modelling followed\nby foreground modeling to extract the blobs of our object of interest.\nSubsequently, we train a cubic SVM with human body datasets in accordance with\nour domain of interest for recognition and tracking. During training, using the\nprinciple of Bag of Words we extract necessary features of certain domains and\nobjects for classification. Subsequently, matching these feature sets with\nthose of the extracted object blobs that are obtained by subtracting the shadow\nfree background from the foreground, we detect successfully our object of\ninterest from the test domain. The performance of the classification by cubic\nSVM is satisfactorily represented by confusion matrix and ROC curve reflecting\nthe accuracy of each module. After classification, our object of interest is\ntracked in the test domain using species inspired PSO. By combining the\nadaptive learning tools with the efficient classification of description, we\nachieve optimum accuracy in recognition of the moving objects. We evaluate our\nalgorithm benchmark datasets: iLIDS, VIVID, Walking2, Woman. Comparative\nanalysis of our algorithm against the existing state-of-the-art trackers shows\nvery satisfactory and competitive results.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jun 2017 11:09:10 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Ray", "Kumar S.", ""], ["Chakraborty", "Anit", ""], ["Dutta", "Sayandip", ""]]}, {"id": "1707.05228", "submitter": "Kumar Sankar Ray", "authors": "Rajesh Misra, Kumar S. Ray", "title": "Object Tracking based on Quantum Particle Swarm Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Computer Vision domain, moving Object Tracking considered as one of the\ntoughest problem.As there so many factors associated like illumination of\nlight, noise, occlusion, sudden start and stop of moving object, shading which\nmakes tracking even harder problem not only for dynamic background but also for\nstatic background.In this paper we present a new object tracking algorithm\nbased on Dominant points on tracked object using Quantum particle swarm\noptimization (QPSO) which is a new different version of PSO based on Quantum\ntheory. The novelty in our approach is that it can be successfully applicable\nin variable background as well as static background and application of quantum\nPSO makes the algorithm runs lot faster where other basic PSO algorithm failed\nto do so due to heavy computation.In our approach firstly dominants points of\ntracked objects detected, then a group of particles form a swarm are\ninitialized randomly over the image search space and then start searching the\ncurvature connected between two consecutive dominant points until they satisfy\nfitness criteria. Obviously it is a Multi-Swarm approach as there are multiple\ndominant points, as they moves, the curvature moves and the curvature movement\nis tracked by the swarm throughout the video and eventually when the swarm\nreaches optimal solution , a bounding box drawn based on particles final\nposition.Experimental results demonstrate this proposed QPSO based method work\nefficiently and effectively in visual object tracking in both dynamic and\nstatic environments and run time shows that it runs closely 90% faster than\nbasic PSO.in our approach we also apply parallelism using MatLab Parfor command\nto show how very less number of iteration and swarm size will enable us to\nsuccessfully track object.\n", "versions": [{"version": "v1", "created": "Wed, 24 May 2017 10:29:45 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Misra", "Rajesh", ""], ["Ray", "Kumar S.", ""]]}, {"id": "1707.05251", "submitter": "Yubin Deng", "authors": "Yubin Deng, Chen Change Loy, Xiaoou Tang", "title": "Aesthetic-Driven Image Enhancement by Adversarial Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce EnhanceGAN, an adversarial learning based model that performs\nautomatic image enhancement. Traditional image enhancement frameworks typically\ninvolve training models in a fully-supervised manner, which require expensive\nannotations in the form of aligned image pairs. In contrast to these\napproaches, our proposed EnhanceGAN only requires weak supervision (binary\nlabels on image aesthetic quality) and is able to learn enhancement operators\nfor the task of aesthetic-based image enhancement. In particular, we show the\neffectiveness of a piecewise color enhancement module trained with weak\nsupervision, and extend the proposed EnhanceGAN framework to learning a deep\nfiltering-based aesthetic enhancer. The full differentiability of our image\nenhancement operators enables the training of EnhanceGAN in an end-to-end\nmanner. We further demonstrate the capability of EnhanceGAN in learning\naesthetic-based image cropping without any groundtruth cropping pairs. Our\nweakly-supervised EnhanceGAN reports competitive quantitative results on\naesthetic-based color enhancement as well as automatic image cropping, and a\nuser study confirms that our image enhancement results are on par with or even\npreferred over professional enhancement.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jul 2017 15:58:49 GMT"}, {"version": "v2", "created": "Mon, 2 Jul 2018 06:25:18 GMT"}], "update_date": "2018-07-03", "authors_parsed": [["Deng", "Yubin", ""], ["Loy", "Chen Change", ""], ["Tang", "Xiaoou", ""]]}, {"id": "1707.05309", "submitter": "Eyasu Zemene", "authors": "Eyasu Zemene, Leulseged Tesfaye Alemu and Marcello Pelillo", "title": "Dominant Sets for \"Constrained\" Image Segmentation", "comments": "arXiv admin note: text overlap with arXiv:1608.00641", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image segmentation has come a long way since the early days of computer\nvision, and still remains a challenging task. Modern variations of the\nclassical (purely bottom-up) approach, involve, e.g., some form of user\nassistance (interactive segmentation) or ask for the simultaneous segmentation\nof two or more images (co-segmentation). At an abstract level, all these\nvariants can be thought of as \"constrained\" versions of the original\nformulation, whereby the segmentation process is guided by some external source\nof information. In this paper, we propose a new approach to tackle this kind of\nproblems in a unified way. Our work is based on some properties of a family of\nquadratic optimization problems related to dominant sets, a well-known\ngraph-theoretic notion of a cluster which generalizes the concept of a maximal\nclique to edge-weighted graphs. In particular, we show that by properly\ncontrolling a regularization parameter which determines the structure and the\nscale of the underlying problem, we are in a position to extract groups of\ndominant-set clusters that are constrained to contain predefined elements. In\nparticular, we shall focus on interactive segmentation and co-segmentation (in\nboth the unsupervised and the interactive versions). The proposed algorithm can\ndeal naturally with several type of constraints and input modality, including\nscribbles, sloppy contours, and bounding boxes, and is able to robustly handle\nnoisy annotations on the part of the user. Experiments on standard benchmark\ndatasets show the effectiveness of our approach as compared to state-of-the-art\nalgorithms on a variety of natural images under several input conditions and\nconstraints.\n", "versions": [{"version": "v1", "created": "Sat, 15 Jul 2017 03:32:42 GMT"}], "update_date": "2017-07-19", "authors_parsed": [["Zemene", "Eyasu", ""], ["Alemu", "Leulseged Tesfaye", ""], ["Pelillo", "Marcello", ""]]}, {"id": "1707.05357", "submitter": "Harvineet Singh", "authors": "Sumit Shekhar, Dhruv Singal, Harvineet Singh, Manav Kedia, Akhil\n  Shetty", "title": "Show and Recall: Learning What Makes Videos Memorable", "comments": "10 pages, updated abstract, added few references, project page link\n  and acknowledgements. Accepted at ICCV 2017 Workshop on Mutual Benefits of\n  Cognitive and Computer Vision (MBCC)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  With the explosion of video content on the Internet, there is a need for\nresearch on methods for video analysis which take human cognition into account.\nOne such cognitive measure is memorability, or the ability to recall visual\ncontent after watching it. Prior research has looked into image memorability\nand shown that it is intrinsic to visual content, but the problem of modeling\nvideo memorability has not been addressed sufficiently. In this work, we\ndevelop a prediction model for video memorability, including complexities of\nvideo content in it. Detailed feature analysis reveals that the proposed method\ncorrelates well with existing findings on memorability. We also describe a\nnovel experiment of predicting video sub-shot memorability and show that our\napproach improves over current memorability methods in this task. Experiments\non standard datasets demonstrate that the proposed metric can achieve results\non par or better than the state-of-the art methods for video summarization.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jul 2017 18:34:37 GMT"}, {"version": "v2", "created": "Fri, 28 Jul 2017 10:59:57 GMT"}, {"version": "v3", "created": "Mon, 28 Aug 2017 07:43:28 GMT"}], "update_date": "2017-08-29", "authors_parsed": [["Shekhar", "Sumit", ""], ["Singal", "Dhruv", ""], ["Singh", "Harvineet", ""], ["Kedia", "Manav", ""], ["Shetty", "Akhil", ""]]}, {"id": "1707.05373", "submitter": "Moustapha Cisse", "authors": "Moustapha Cisse, Yossi Adi, Natalia Neverova and Joseph Keshet", "title": "Houdini: Fooling Deep Structured Prediction Models", "comments": "12 pages, 8 figures, under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating adversarial examples is a critical step for evaluating and\nimproving the robustness of learning machines. So far, most existing methods\nonly work for classification and are not designed to alter the true performance\nmeasure of the problem at hand. We introduce a novel flexible approach named\nHoudini for generating adversarial examples specifically tailored for the final\nperformance measure of the task considered, be it combinatorial and\nnon-decomposable. We successfully apply Houdini to a range of applications such\nas speech recognition, pose estimation and semantic segmentation. In all cases,\nthe attacks based on Houdini achieve higher success rate than those based on\nthe traditional surrogates used to train the models while using a less\nperceptible adversarial perturbation.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jul 2017 19:11:08 GMT"}], "update_date": "2017-07-19", "authors_parsed": [["Cisse", "Moustapha", ""], ["Adi", "Yossi", ""], ["Neverova", "Natalia", ""], ["Keshet", "Joseph", ""]]}, {"id": "1707.05385", "submitter": "Rahul Paul", "authors": "Rahul Paul, Saeed Alahamri, Sulav Malla, and Ghulam Jilani Quadri", "title": "Make Your Bone Great Again : A study on Osteoporosis Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Osteoporosis can be identified by looking at 2D x-ray images of the bone. The\nhigh degree of similarity between images of a healthy bone and a diseased one\nmakes classification a challenge. A good bone texture characterization\ntechnique is essential for identifying osteoporosis cases. Standard texture\nfeature extraction techniques like Local Binary Pattern (LBP), Gray Level\nCo-occurrence Matrix (GLCM) have been used for this purpose. In this paper, we\ndraw a comparison between deep features extracted from convolution neural\nnetwork against these traditional features. Our results show that deep features\nhave more discriminative power as classifiers trained on them always outperform\nthe ones trained on traditional features.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jul 2017 20:22:20 GMT"}], "update_date": "2017-07-19", "authors_parsed": [["Paul", "Rahul", ""], ["Alahamri", "Saeed", ""], ["Malla", "Sulav", ""], ["Quadri", "Ghulam Jilani", ""]]}, {"id": "1707.05388", "submitter": "Matteo Ruggero Ronchi", "authors": "Matteo Ruggero Ronchi and Pietro Perona", "title": "Benchmarking and Error Diagnosis in Multi-Instance Pose Estimation", "comments": "Project page available at\n  http://www.vision.caltech.edu/~mronchi/projects/PoseErrorDiagnosis/; Code\n  available at https://github.com/matteorr/coco-analyze; published at ICCV 17", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new method to analyze the impact of errors in algorithms for\nmulti-instance pose estimation and a principled benchmark that can be used to\ncompare them. We define and characterize three classes of errors -\nlocalization, scoring, and background - study how they are influenced by\ninstance attributes and their impact on an algorithm's performance. Our\ntechnique is applied to compare the two leading methods for human pose\nestimation on the COCO Dataset, measure the sensitivity of pose estimation with\nrespect to instance size, type and number of visible keypoints, clutter due to\nmultiple instances, and the relative score of instances. The performance of\nalgorithms, and the types of error they make, are highly dependent on all these\nvariables, but mostly on the number of keypoints and the clutter. The analysis\nand software tools we propose offer a novel and insightful approach for\nunderstanding the behavior of pose estimation algorithms and an effective\nmethod for measuring their strengths and weaknesses.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jul 2017 20:32:37 GMT"}, {"version": "v2", "created": "Sat, 5 Aug 2017 00:55:29 GMT"}], "update_date": "2017-08-08", "authors_parsed": [["Ronchi", "Matteo Ruggero", ""], ["Perona", "Pietro", ""]]}, {"id": "1707.05392", "submitter": "Yipeng Hu", "authors": "Yipeng Hu, Eli Gibson, Li-Lin Lee, Weidi Xie, Dean C. Barratt, Tom\n  Vercauteren, J. Alison Noble", "title": "Freehand Ultrasound Image Simulation with Spatially-Conditioned\n  Generative Adversarial Networks", "comments": "Accepted to MICCAI RAMBO 2017", "journal-ref": null, "doi": "10.1007/978-3-319-67564-0_11", "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sonography synthesis has a wide range of applications, including medical\nprocedure simulation, clinical training and multimodality image registration.\nIn this paper, we propose a machine learning approach to simulate ultrasound\nimages at given 3D spatial locations (relative to the patient anatomy), based\non conditional generative adversarial networks (GANs). In particular, we\nintroduce a novel neural network architecture that can sample anatomically\naccurate images conditionally on spatial position of the (real or mock)\nfreehand ultrasound probe. To ensure an effective and efficient spatial\ninformation assimilation, the proposed spatially-conditioned GANs take\ncalibrated pixel coordinates in global physical space as conditioning input,\nand utilise residual network units and shortcuts of conditioning data in the\nGANs' discriminator and generator, respectively. Using optically tracked B-mode\nultrasound images, acquired by an experienced sonographer on a fetus phantom,\nwe demonstrate the feasibility of the proposed method by two sets of\nquantitative results: distances were calculated between corresponding\nanatomical landmarks identified in the held-out ultrasound images and the\nsimulated data at the same locations unseen to the networks; a usability study\nwas carried out to distinguish the simulated data from the real images. In\nsummary, we present what we believe are state-of-the-art visually realistic\nultrasound images, simulated by the proposed GAN architecture that is stable to\ntrain and capable of generating plausibly diverse image samples.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jul 2017 20:48:28 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["Hu", "Yipeng", ""], ["Gibson", "Eli", ""], ["Lee", "Li-Lin", ""], ["Xie", "Weidi", ""], ["Barratt", "Dean C.", ""], ["Vercauteren", "Tom", ""], ["Noble", "J. Alison", ""]]}, {"id": "1707.05395", "submitter": "Shizhong Han", "authors": "Shizhong Han, Zibo Meng, Ahmed Shehab Khan, Yan Tong", "title": "Incremental Boosting Convolutional Neural Network for Facial Action Unit\n  Recognition", "comments": "NIPS2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recognizing facial action units (AUs) from spontaneous facial expressions is\nstill a challenging problem. Most recently, CNNs have shown promise on facial\nAU recognition. However, the learned CNNs are often overfitted and do not\ngeneralize well to unseen subjects due to limited AU-coded training images. We\nproposed a novel Incremental Boosting CNN (IB-CNN) to integrate boosting into\nthe CNN via an incremental boosting layer that selects discriminative neurons\nfrom the lower layer and is incrementally updated on successive mini-batches.\nIn addition, a novel loss function that accounts for errors from both the\nincremental boosted classifier and individual weak classifiers was proposed to\nfine-tune the IB-CNN. Experimental results on four benchmark AU databases have\ndemonstrated that the IB-CNN yields significant improvement over the\ntraditional CNN and the boosting CNN without incremental learning, as well as\noutperforming the state-of-the-art CNN-based methods in AU recognition. The\nimprovement is more impressive for the AUs that have the lowest frequencies in\nthe databases.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jul 2017 21:04:13 GMT"}], "update_date": "2017-07-19", "authors_parsed": [["Han", "Shizhong", ""], ["Meng", "Zibo", ""], ["Khan", "Ahmed Shehab", ""], ["Tong", "Yan", ""]]}, {"id": "1707.05397", "submitter": "Daniel Hernandez-Juarez", "authors": "Daniel Hernandez-Juarez, Lukas Schneider, Antonio Espinosa, David\n  V\\'azquez, Antonio M. L\\'opez, Uwe Franke, Marc Pollefeys, Juan C. Moure", "title": "Slanted Stixels: Representing San Francisco's Steepest Streets", "comments": "Accepted to BMVC 2017 as oral presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we present a novel compact scene representation based on Stixels\nthat infers geometric and semantic information. Our approach overcomes the\nprevious rather restrictive geometric assumptions for Stixels by introducing a\nnovel depth model to account for non-flat roads and slanted objects. Both\nsemantic and depth cues are used jointly to infer the scene representation in a\nsound global energy minimization formulation. Furthermore, a novel\napproximation scheme is introduced that uses an extremely efficient\nover-segmentation. In doing so, the computational complexity of the Stixel\ninference algorithm is reduced significantly, achieving real-time computation\ncapabilities with only a slight drop in accuracy. We evaluate the proposed\napproach in terms of semantic and geometric accuracy as well as run-time on\nfour publicly available benchmark datasets. Our approach maintains accuracy on\nflat road scene datasets while improving substantially on a novel non-flat road\ndataset.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jul 2017 21:14:26 GMT"}], "update_date": "2017-07-19", "authors_parsed": [["Hernandez-Juarez", "Daniel", ""], ["Schneider", "Lukas", ""], ["Espinosa", "Antonio", ""], ["V\u00e1zquez", "David", ""], ["L\u00f3pez", "Antonio M.", ""], ["Franke", "Uwe", ""], ["Pollefeys", "Marc", ""], ["Moure", "Juan C.", ""]]}, {"id": "1707.05411", "submitter": "Ioannis Rigas", "authors": "Ioannis Rigas, Hayes Raffle, and Oleg V. Komogortsev", "title": "Hybrid PS-V Technique: A Novel Sensor Fusion Approach for Fast Mobile\n  Eye-Tracking with Sensor-Shift Aware Correction", "comments": "11 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces and evaluates a hybrid technique that fuses efficiently\nthe eye-tracking principles of photosensor oculography (PSOG) and video\noculography (VOG). The main concept of this novel approach is to use a few fast\nand power-economic photosensors as the core mechanism for performing high speed\neye-tracking, whereas in parallel, use a video sensor operating at low\nsampling-rate (snapshot mode) to perform dead-reckoning error correction when\nsensor movements occur. In order to evaluate the proposed method, we simulate\nthe functional components of the technique and present our results in\nexperimental scenarios involving various combinations of horizontal and\nvertical eye and sensor movements. Our evaluation shows that the developed\ntechnique can be used to provide robustness to sensor shifts that otherwise\ncould induce error larger than 5 deg. Our analysis suggests that the technique\ncan potentially enable high speed eye-tracking at low power profiles, making it\nsuitable to be used in emerging head-mounted devices, e.g. AR/VR headsets.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jul 2017 23:26:09 GMT"}, {"version": "v2", "created": "Wed, 19 Jul 2017 04:28:11 GMT"}], "update_date": "2017-07-20", "authors_parsed": [["Rigas", "Ioannis", ""], ["Raffle", "Hayes", ""], ["Komogortsev", "Oleg V.", ""]]}, {"id": "1707.05413", "submitter": "Ioannis Rigas", "authors": "Ioannis Rigas, Hayes Raffle, and Oleg V. Komogortsev", "title": "Photosensor Oculography: Survey and Parametric Analysis of Designs using\n  Model-Based Simulation", "comments": "12 pages, 18 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a renewed overview of photosensor oculography (PSOG), an\neye-tracking technique based on the principle of using simple photosensors to\nmeasure the amount of reflected (usually infrared) light when the eye rotates.\nPhotosensor oculography can provide measurements with high precision, low\nlatency and reduced power consumption, and thus it appears as an attractive\noption for performing eye-tracking in the emerging head-mounted interaction\ndevices, e.g. augmented and virtual reality (AR/VR) headsets. In our current\nwork we employ an adjustable simulation framework as a common basis for\nperforming an exploratory study of the eye-tracking behavior of different\nphotosensor oculography designs. With the performed experiments we explore the\neffects from the variation of some basic parameters of the designs on the\nresulting accuracy and cross-talk, which are crucial characteristics for the\nseamless operation of human-computer interaction applications based on\neye-tracking. Our experimental results reveal the design trade-offs that need\nto be adopted to tackle the competing conditions that lead to optimum\nperformance of different eye-tracking characteristics. We also present the\ntransformations that arise in the eye-tracking output when sensor shifts occur,\nand assess the resulting degradation in accuracy for different combinations of\neye movements and sensor shifts.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jul 2017 23:31:57 GMT"}, {"version": "v2", "created": "Wed, 19 Jul 2017 04:26:55 GMT"}], "update_date": "2017-07-20", "authors_parsed": [["Rigas", "Ioannis", ""], ["Raffle", "Hayes", ""], ["Komogortsev", "Oleg V.", ""]]}, {"id": "1707.05414", "submitter": "Peng Liu", "authors": "Peng Liu, Ruogu Fang", "title": "Wide Inference Network for Image Denoising via Learning\n  Pixel-distribution Prior", "comments": "There is a code issue that makes our work may be regarded as entirely\n  out the way of the correct research direction. Therefore, we add the\n  correction into abstract to answer the questions being often asked. Besides.\n  we hope the most talent you may try to think about how to map the particular\n  matrix to generative ones. Then, you may have a significant innovation\n  published", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore an innovative strategy for image denoising by using convolutional\nneural networks (CNN) to learn similar pixel-distribution features from noisy\nimages. Many types of image noise follow a certain pixel-distribution in\ncommon, such as additive white Gaussian noise (AWGN). By increasing CNN's width\nwith larger reception fields and more channels in each layer, CNNs can reveal\nthe ability to extract more accurate pixel-distribution features. The key to\nour approach is a discovery that wider CNNs with more convolutions tend to\nlearn the similar pixel-distribution features, which reveals a new strategy to\nsolve low-level vision problems effectively that the inference mapping\nprimarily relies on the priors behind the noise property instead of deeper CNNs\nwith more stacked nonlinear layers. We evaluate our work, Wide inference\nNetworks (WIN), on AWGN and demonstrate that by learning pixel-distribution\nfeatures from images, WIN-based network consistently achieves significantly\nbetter performance than current state-of-the-art deep CNN-based methods in both\nquantitative and visual evaluations. \\textit{Code and models are available at\n\\url{https://github.com/cswin/WIN}}.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jul 2017 23:39:38 GMT"}, {"version": "v2", "created": "Wed, 16 Aug 2017 04:40:00 GMT"}, {"version": "v3", "created": "Wed, 23 Aug 2017 23:11:15 GMT"}, {"version": "v4", "created": "Wed, 16 May 2018 13:40:34 GMT"}, {"version": "v5", "created": "Sun, 3 Jun 2018 21:25:45 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Liu", "Peng", ""], ["Fang", "Ruogu", ""]]}, {"id": "1707.05425", "submitter": "Jin Yamanaka", "authors": "Jin Yamanaka, Shigesumi Kuwashima and Takio Kurita", "title": "Fast and Accurate Image Super Resolution by Deep CNN with Skip\n  Connection and Network in Network", "comments": "9 pages, 4 figures. This paper is accepted at 24th International\n  Conference On Neural Information Processing (ICONIP 2017)", "journal-ref": "24th International Conference of Neural Information Processing,\n  ICONIP 2017, Proceedings, Part II (pp.217-225)", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a highly efficient and faster Single Image Super-Resolution (SISR)\nmodel with Deep Convolutional neural networks (Deep CNN). Deep CNN have\nrecently shown that they have a significant reconstruction performance on\nsingle-image super-resolution. Current trend is using deeper CNN layers to\nimprove performance. However, deep models demand larger computation resources\nand is not suitable for network edge devices like mobile, tablet and IoT\ndevices. Our model achieves state of the art reconstruction performance with at\nleast 10 times lower calculation cost by Deep CNN with Residual Net, Skip\nConnection and Network in Network (DCSCN). A combination of Deep CNNs and Skip\nconnection layers is used as a feature extractor for image features on both\nlocal and global area. Parallelized 1x1 CNNs, like the one called Network in\nNetwork, is also used for image reconstruction. That structure reduces the\ndimensions of the previous layer's output for faster computation with less\ninformation loss, and make it possible to process original images directly.\nAlso we optimize the number of layers and filters of each CNN to significantly\nreduce the calculation cost. Thus, the proposed algorithm not only achieves the\nstate of the art performance but also achieves faster and efficient\ncomputation. Code is available at\nhttps://github.com/jiny2001/dcscn-super-resolution\n", "versions": [{"version": "v1", "created": "Tue, 18 Jul 2017 00:50:55 GMT"}, {"version": "v2", "created": "Mon, 4 Sep 2017 10:27:41 GMT"}, {"version": "v3", "created": "Tue, 5 Sep 2017 03:25:38 GMT"}, {"version": "v4", "created": "Thu, 21 Sep 2017 22:22:55 GMT"}, {"version": "v5", "created": "Sun, 23 Sep 2018 21:15:32 GMT"}, {"version": "v6", "created": "Sun, 30 Sep 2018 01:05:09 GMT"}, {"version": "v7", "created": "Tue, 8 Sep 2020 08:23:11 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Yamanaka", "Jin", ""], ["Kuwashima", "Shigesumi", ""], ["Kurita", "Takio", ""]]}, {"id": "1707.05427", "submitter": "Chunhua Shen", "authors": "Ruizhi Qiao, Lingqiao Liu, Chunhua Shen, Anton van den Hengel", "title": "Visually Aligned Word Embeddings for Improving Zero-shot Learning", "comments": "Appearing in Proc. British Mach. Vis. Conf. (BMVC) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zero-shot learning (ZSL) highly depends on a good semantic embedding to\nconnect the seen and unseen classes. Recently, distributed word embeddings\n(DWE) pre-trained from large text corpus have become a popular choice to draw\nsuch a connection. Compared with human defined attributes, DWEs are more\nscalable and easier to obtain. However, they are designed to reflect semantic\nsimilarity rather than visual similarity and thus using them in ZSL often leads\nto inferior performance. To overcome this visual-semantic discrepancy, this\nwork proposes an objective function to re-align the distributed word embeddings\nwith visual information by learning a neural network to map it into a new\nrepresentation called visually aligned word embedding (VAWE). Thus the\nneighbourhood structure of VAWEs becomes similar to that in the visual domain.\nNote that in this work we do not design a ZSL method that projects the visual\nfeatures and semantic embeddings onto a shared space but just impose a\nrequirement on the structure of the mapped word embeddings. This strategy\nallows the learned VAWE to generalize to various ZSL methods and visual\nfeatures. As evaluated via four state-of-the-art ZSL methods on four benchmark\ndatasets, the VAWE exhibit consistent performance improvement.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jul 2017 01:07:32 GMT"}], "update_date": "2017-07-19", "authors_parsed": [["Qiao", "Ruizhi", ""], ["Liu", "Lingqiao", ""], ["Shen", "Chunhua", ""], ["Hengel", "Anton van den", ""]]}, {"id": "1707.05446", "submitter": "Zaidao Wen", "authors": "Zaidao Wen, Biao Hou, Qian Wu, Licheng Jiao", "title": "Discriminative Transformation Learning for Fuzzy Sparse Subspace\n  Clustering", "comments": "IEEE Trans. Cybern. Accept", "journal-ref": null, "doi": "10.1109/TCYB.2017.2729542", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops a novel iterative framework for subspace clustering in a\nlearned discriminative feature domain. This framework consists of two modules\nof fuzzy sparse subspace clustering and discriminative transformation learning.\nIn the first module, fuzzy latent labels containing discriminative information\nand latent representations capturing the subspace structure will be\nsimultaneously evaluated in a feature domain. Then the linear transforming\noperator with respect to the feature domain will be successively updated in the\nsecond module with the advantages of more discrimination, subspace structure\npreservation and robustness to outliers. These two modules will be\nalternatively carried out and both theoretical analysis and empirical\nevaluations will demonstrate its effectiveness and superiorities. In\nparticular, experimental results on three benchmark databases for subspace\nclustering clearly illustrate that the proposed framework can achieve\nsignificant improvements than other state-of-the-art approaches in terms of\nclustering accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jul 2017 03:06:14 GMT"}], "update_date": "2017-08-04", "authors_parsed": [["Wen", "Zaidao", ""], ["Hou", "Biao", ""], ["Wu", "Qian", ""], ["Jiao", "Licheng", ""]]}, {"id": "1707.05455", "submitter": "Jie Lin", "authors": "Gaurav Manek, Jie Lin, Vijay Chandrasekhar, Lingyu Duan, Sateesh\n  Giduthuri, Xiaoli Li, Tomaso Poggio", "title": "Pruning Convolutional Neural Networks for Image Instance Retrieval", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work, we focus on the problem of image instance retrieval with deep\ndescriptors extracted from pruned Convolutional Neural Networks (CNN). The\nobjective is to heavily prune convolutional edges while maintaining retrieval\nperformance. To this end, we introduce both data-independent and data-dependent\nheuristics to prune convolutional edges, and evaluate their performance across\nvarious compression rates with different deep descriptors over several\nbenchmark datasets. Further, we present an end-to-end framework to fine-tune\nthe pruned network, with a triplet loss function specially designed for the\nretrieval task. We show that the combination of heuristic pruning and\nfine-tuning offers 5x compression rate without considerable loss in retrieval\nperformance.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jul 2017 03:49:59 GMT"}], "update_date": "2017-07-19", "authors_parsed": [["Manek", "Gaurav", ""], ["Lin", "Jie", ""], ["Chandrasekhar", "Vijay", ""], ["Duan", "Lingyu", ""], ["Giduthuri", "Sateesh", ""], ["Li", "Xiaoli", ""], ["Poggio", "Tomaso", ""]]}, {"id": "1707.05466", "submitter": "Qianggong Zhang", "authors": "Qianggong Zhang, Tat-Jun Chin", "title": "Coresets for Triangulation", "comments": "14 pages for TPAMI summission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple-view triangulation by $\\ell_{\\infty}$ minimisation has become\nestablished in computer vision. State-of-the-art $\\ell_{\\infty}$ triangulation\nalgorithms exploit the quasiconvexity of the cost function to derive iterative\nupdate rules that deliver the global minimum. Such algorithms, however, can be\ncomputationally costly for large problem instances that contain many image\nmeasurements, e.g., from web-based photo sharing sites or long-term video\nrecordings. In this paper, we prove that $\\ell_{\\infty}$ triangulation admits a\ncoreset approximation scheme, which seeks small representative subsets of the\ninput data called coresets. A coreset possesses the special property that the\nerror of the $\\ell_{\\infty}$ solution on the coreset is within known bounds\nfrom the global minimum. We establish the necessary mathematical underpinnings\nof the coreset algorithm, specifically, by enacting the stopping criterion of\nthe algorithm and proving that the resulting coreset gives the desired\napproximation accuracy. On large-scale triangulation problems, our method\nprovides theoretically sound approximate solutions. Iterated until convergence,\nour coreset algorithm is also guaranteed to reach the true optimum. On\npractical datasets, we show that our technique can in fact attain the global\nminimiser much faster than current methods\n", "versions": [{"version": "v1", "created": "Tue, 18 Jul 2017 04:47:31 GMT"}], "update_date": "2017-07-19", "authors_parsed": [["Zhang", "Qianggong", ""], ["Chin", "Tat-Jun", ""]]}, {"id": "1707.05471", "submitter": "Seungryong Kim", "authors": "Seungryong Kim, Dongbo Min, Stephen Lin, and Kwanghoon Sohn", "title": "DCTM: Discrete-Continuous Transformation Matching for Semantic Flow", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Techniques for dense semantic correspondence have provided limited ability to\ndeal with the geometric variations that commonly exist between semantically\nsimilar images. While variations due to scale and rotation have been examined,\nthere lack practical solutions for more complex deformations such as affine\ntransformations because of the tremendous size of the associated solution\nspace. To address this problem, we present a discrete-continuous transformation\nmatching (DCTM) framework where dense affine transformation fields are inferred\nthrough a discrete label optimization in which the labels are iteratively\nupdated via continuous regularization. In this way, our approach draws\nsolutions from the continuous space of affine transformations in a manner that\ncan be computed efficiently through constant-time edge-aware filtering and a\nproposed affine-varying CNN-based descriptor. Experimental results show that\nthis model outperforms the state-of-the-art methods for dense semantic\ncorrespondence on various benchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jul 2017 05:16:12 GMT"}], "update_date": "2017-07-19", "authors_parsed": [["Kim", "Seungryong", ""], ["Min", "Dongbo", ""], ["Lin", "Stephen", ""], ["Sohn", "Kwanghoon", ""]]}, {"id": "1707.05474", "submitter": "Shiwei Shen", "authors": "Shiwei Shen, Guoqing Jin, Ke Gao, Yongdong Zhang", "title": "APE-GAN: Adversarial Perturbation Elimination with GAN", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although neural networks could achieve state-of-the-art performance while\nrecongnizing images, they often suffer a tremendous defeat from adversarial\nexamples--inputs generated by utilizing imperceptible but intentional\nperturbation to clean samples from the datasets. How to defense against\nadversarial examples is an important problem which is well worth researching.\nSo far, very few methods have provided a significant defense to adversarial\nexamples. In this paper, a novel idea is proposed and an effective framework\nbased Generative Adversarial Nets named APE-GAN is implemented to defense\nagainst the adversarial examples. The experimental results on three benchmark\ndatasets including MNIST, CIFAR10 and ImageNet indicate that APE-GAN is\neffective to resist adversarial examples generated from five attacks.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jul 2017 05:29:27 GMT"}, {"version": "v2", "created": "Thu, 14 Sep 2017 09:39:36 GMT"}, {"version": "v3", "created": "Tue, 26 Sep 2017 03:38:36 GMT"}], "update_date": "2017-09-27", "authors_parsed": [["Shen", "Shiwei", ""], ["Jin", "Guoqing", ""], ["Gao", "Ke", ""], ["Zhang", "Yongdong", ""]]}, {"id": "1707.05489", "submitter": "Omid Poursaeed", "authors": "Omid Poursaeed, Tomas Matera, Serge Belongie", "title": "Vision-based Real Estate Price Estimation", "comments": null, "journal-ref": "Machine Vision and Applications, 29(4), 667-676, 2018", "doi": "10.1007/s00138-018-0922-2", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since the advent of online real estate database companies like Zillow, Trulia\nand Redfin, the problem of automatic estimation of market values for houses has\nreceived considerable attention. Several real estate websites provide such\nestimates using a proprietary formula. Although these estimates are often close\nto the actual sale prices, in some cases they are highly inaccurate. One of the\nkey factors that affects the value of a house is its interior and exterior\nappearance, which is not considered in calculating automatic value estimates.\nIn this paper, we evaluate the impact of visual characteristics of a house on\nits market value. Using deep convolutional neural networks on a large dataset\nof photos of home interiors and exteriors, we develop a method for estimating\nthe luxury level of real estate photos. We also develop a novel framework for\nautomated value assessment using the above photos in addition to home\ncharacteristics including size, offered price and number of bedrooms. Finally,\nby applying our proposed method for price estimation to a new dataset of real\nestate photos and metadata, we show that it outperforms Zillow's estimates.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jul 2017 06:29:02 GMT"}, {"version": "v2", "created": "Fri, 11 May 2018 19:47:00 GMT"}, {"version": "v3", "created": "Wed, 3 Oct 2018 15:39:27 GMT"}], "update_date": "2018-10-04", "authors_parsed": [["Poursaeed", "Omid", ""], ["Matera", "Tomas", ""], ["Belongie", "Serge", ""]]}, {"id": "1707.05495", "submitter": "Shang Fu Chen", "authors": "Shang-Fu Chen, Yi-Chen Chen, Chih-Kuan Yeh, Yu-Chiang Frank Wang", "title": "Order-Free RNN with Visual Attention for Multi-Label Classification", "comments": "Accepted at 32nd AAAI Conference on Artificial Intelligence (AAAI-18)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose the joint learning attention and recurrent neural\nnetwork (RNN) models for multi-label classification. While approaches based on\nthe use of either model exist (e.g., for the task of image captioning),\ntraining such existing network architectures typically require pre-defined\nlabel sequences. For multi-label classification, it would be desirable to have\na robust inference process, so that the prediction error would not propagate\nand thus affect the performance. Our proposed model uniquely integrates\nattention and Long Short Term Memory (LSTM) models, which not only addresses\nthe above problem but also allows one to identify visual objects of interests\nwith varying sizes without the prior knowledge of particular label ordering.\nMore importantly, label co-occurrence information can be jointly exploited by\nour LSTM model. Finally, by advancing the technique of beam search, prediction\nof multiple labels can be efficiently achieved by our proposed network model.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jul 2017 06:44:16 GMT"}, {"version": "v2", "created": "Wed, 20 Sep 2017 05:00:21 GMT"}, {"version": "v3", "created": "Wed, 20 Dec 2017 06:07:28 GMT"}], "update_date": "2017-12-21", "authors_parsed": [["Chen", "Shang-Fu", ""], ["Chen", "Yi-Chen", ""], ["Yeh", "Chih-Kuan", ""], ["Wang", "Yu-Chiang Frank", ""]]}, {"id": "1707.05537", "submitter": "Abrar  Abdulnabi", "authors": "Abrar H. Abdulnabi, Stefan Winkler, Gang Wang", "title": "Beyond Forward Shortcuts: Fully Convolutional Master-Slave Networks\n  (MSNets) with Backward Skip Connections for Semantic Segmentation", "comments": "9 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent deep CNNs contain forward shortcut connections; i.e. skip connections\nfrom low to high layers. Reusing features from lower layers that have higher\nresolution (location information) benefit higher layers to recover lost details\nand mitigate information degradation. However, during inference the lower\nlayers do not know about high layer features, although they contain contextual\nhigh semantics that benefit low layers to adaptively extract informative\nfeatures for later layers. In this paper, we study the influence of backward\nskip connections which are in the opposite direction to forward shortcuts, i.e.\npaths from high layers to low layers. To achieve this -- which indeed runs\ncounter to the nature of feed-forward networks -- we propose a new fully\nconvolutional model that consists of a pair of networks. A `Slave' network is\ndedicated to provide the backward connections from its top layers to the\n`Master' network's bottom layers. The Master network is used to produce the\nfinal label predictions. In our experiments we validate the proposed FCN model\non ADE20K (ImageNet scene parsing), PASCAL-Context, and PASCAL VOC 2011\ndatasets.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jul 2017 09:31:05 GMT"}], "update_date": "2017-07-19", "authors_parsed": [["Abdulnabi", "Abrar H.", ""], ["Winkler", "Stefan", ""], ["Wang", "Gang", ""]]}, {"id": "1707.05553", "submitter": "Youyi Cai", "authors": "Zhen Cui (1), You yi Cai (2 and 3), Wen ming Zheng (3), Jian Yang (1)\n  ((1) School of Computer Science and Engineering, Nanjing University of\n  Science and Technology, Nanjing, China (2) the Department of Information\n  Science and Engineering, Southeast University, Nanjing, China (3) the Key\n  Laboratory of Child Development and Learning Science of Ministry of\n  Education, Research Center for Learning Science, Southeast University,\n  Nanjing, China)", "title": "Spectral Filter Tracking", "comments": "11pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual object tracking is a challenging computer vision task with numerous\nreal-world applications. Here we propose a simple but efficient Spectral Filter\nTracking (SFT)method. To characterize rotational and translation invariance of\ntracking targets, the candidate image region is models as a pixelwise grid\ngraph. Instead of the conventional graph matching, we convert the tracking into\na plain least square regression problem to estimate the best center coordinate\nof the target. But different from the holistic regression of correlation filter\nbased methods, SFT can operate on localized surrounding regions of each pixel\n(i.e.,vertex) by using spectral graph filters, which thus is more robust to\nresist local variations and cluttered background.To bypass the eigenvalue\ndecomposition problem of the graph Laplacian matrix L, we parameterize spectral\ngraph filters as the polynomial of L by spectral graph theory, in which L k\nexactly encodes a k-hop local neighborhood of each vertex. Finally, the filter\nparameters (i.e., polynomial coefficients) as well as feature projecting\nfunctions are jointly integrated into the regression model.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jul 2017 10:40:08 GMT"}], "update_date": "2017-07-19", "authors_parsed": [["Cui", "Zhen", "", "2 and 3"], ["Cai", "You yi", "", "2 and 3"], ["Zheng", "Wen ming", ""], ["Yang", "Jian", ""]]}, {"id": "1707.05564", "submitter": "Suvam Patra", "authors": "Suvam Patra, Kartikeya Gupta, Faran Ahmad, Chetan Arora, Subhashis\n  Banerjee", "title": "Robust Monocular SLAM for Egocentric Videos", "comments": "Accepted for publication at IEEE WACV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regardless of the tremendous progress, a truly general purpose pipeline for\nSimultaneous Localization and Mapping (SLAM) remains a challenge. We\ninvestigate the reported failure of state of the art (SOTA) SLAM techniques on\negocentric videos. We find that the dominant 3D rotations, low parallax between\nsuccessive frames, and primarily forward motion in egocentric videos are the\nmost common causes of failures. The incremental nature of SOTA SLAM, in the\npresence of unreliable pose and 3D estimates in egocentric videos, with no\nopportunities for global loop closures, generates drifts and leads to the\neventual failures of such techniques. Taking inspiration from batch mode\nStructure from Motion (SFM) techniques, we propose to solve SLAM as an SFM\nproblem over the sliding temporal windows. This makes the problem well\nconstrained. Further, we propose to initialize the camera poses using 2D\nrotation averaging, followed by translation averaging before structure\nestimation using bundle adjustment. This helps in stabilizing the camera poses\nwhen 3D estimates are not reliable. We show that the proposed SLAM technique,\nincorporating the two key ideas works successfully for long, shaky egocentric\nvideos where other SOTA techniques have been reported to fail. Qualitative and\nquantitative comparisons on publicly available egocentric video datasets\nvalidate our results.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jul 2017 11:31:13 GMT"}, {"version": "v2", "created": "Sat, 17 Nov 2018 20:00:08 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Patra", "Suvam", ""], ["Gupta", "Kartikeya", ""], ["Ahmad", "Faran", ""], ["Arora", "Chetan", ""], ["Banerjee", "Subhashis", ""]]}, {"id": "1707.05572", "submitter": "Konda Reddy Mopuri", "authors": "Konda Reddy Mopuri, Utsav Garg and R. Venkatesh Babu", "title": "Fast Feature Fool: A data independent approach to universal adversarial\n  perturbations", "comments": "BMVC 2017 and codes are available at\n  https://github.com/utsavgarg/fast-feature-fool", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art object recognition Convolutional Neural Networks (CNNs) are\nshown to be fooled by image agnostic perturbations, called universal\nadversarial perturbations. It is also observed that these perturbations\ngeneralize across multiple networks trained on the same target data. However,\nthese algorithms require training data on which the CNNs were trained and\ncompute adversarial perturbations via complex optimization. The fooling\nperformance of these approaches is directly proportional to the amount of\navailable training data. This makes them unsuitable for practical attacks since\nits unreasonable for an attacker to have access to the training data. In this\npaper, for the first time, we propose a novel data independent approach to\ngenerate image agnostic perturbations for a range of CNNs trained for object\nrecognition. We further show that these perturbations are transferable across\nmultiple network architectures trained either on same or different data. In the\nabsence of data, our method generates universal adversarial perturbations\nefficiently via fooling the features learned at multiple layers thereby causing\nCNNs to misclassify. Experiments demonstrate impressive fooling rates and\nsurprising transferability for the proposed universal perturbations generated\nwithout any training data.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jul 2017 11:48:14 GMT"}], "update_date": "2017-07-19", "authors_parsed": [["Mopuri", "Konda Reddy", ""], ["Garg", "Utsav", ""], ["Babu", "R. Venkatesh", ""]]}, {"id": "1707.05574", "submitter": "Yandong Guo", "authors": "Yandong Guo, Lei Zhang", "title": "One-shot Face Recognition by Promoting Underrepresented Classes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the problem of training large-scale face\nidentification model with imbalanced training data. This problem naturally\nexists in many real scenarios including large-scale celebrity recognition,\nmovie actor annotation, etc. Our solution contains two components. First, we\nbuild a face feature extraction model, and improve its performance, especially\nfor the persons with very limited training samples, by introducing a\nregularizer to the cross entropy loss for the multi-nomial logistic regression\n(MLR) learning. This regularizer encourages the directions of the face features\nfrom the same class to be close to the direction of their corresponding\nclassification weight vector in the logistic regression. Second, we build a\nmulti-class classifier using MLR on top of the learned face feature extraction\nmodel. Since the standard MLR has poor generalization capability for the\none-shot classes even if these classes have been oversampled, we propose a\nnovel supervision signal called underrepresented-classes promotion loss, which\naligns the norms of the weight vectors of the one-shot classes (a.k.a.\nunderrepresented-classes) to those of the normal classes. In addition to the\noriginal cross entropy loss, this new loss term effectively promotes the\nunderrepresented classes in the learned model and leads to a remarkable\nimprovement in face recognition performance.\n  We test our solution on the MS-Celeb-1M low-shot learning benchmark task. Our\nsolution recognizes 94.89% of the test images at the precision of 99\\% for the\none-shot classes. To the best of our knowledge, this is the best performance\namong all the published methods using this benchmark task with the same setup,\nincluding all the participants in the recent MS-Celeb-1M challenge at ICCV\n2017.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jul 2017 11:51:13 GMT"}, {"version": "v2", "created": "Thu, 15 Mar 2018 21:44:39 GMT"}], "update_date": "2018-03-19", "authors_parsed": [["Guo", "Yandong", ""], ["Zhang", "Lei", ""]]}, {"id": "1707.05576", "submitter": "Luiza Sayfullina", "authors": "Luiza Sayfullina, Eric Malmi, Yiping Liao and Alex Jung", "title": "Domain Adaptation for Resume Classification Using Convolutional Neural\n  Networks", "comments": "To be published in AIST proceedings: Springer's Lecture Notes in\n  Computer Science (LNCS) series", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel method for classifying resume data of job applicants into\n27 different job categories using convolutional neural networks. Since resume\ndata is costly and hard to obtain due to its sensitive nature, we use domain\nadaptation. In particular, we train a classifier on a large number of freely\navailable job description snippets and then use it to classify resume data. We\nempirically verify a reasonable classification performance of our approach\ndespite having only a small amount of labeled resume data available.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jul 2017 12:06:09 GMT"}], "update_date": "2017-07-19", "authors_parsed": [["Sayfullina", "Luiza", ""], ["Malmi", "Eric", ""], ["Liao", "Yiping", ""], ["Jung", "Alex", ""]]}, {"id": "1707.05612", "submitter": "Fartash Faghri", "authors": "Fartash Faghri, David J. Fleet, Jamie Ryan Kiros and Sanja Fidler", "title": "VSE++: Improving Visual-Semantic Embeddings with Hard Negatives", "comments": "Accepted as spotlight presentation at British Machine Vision\n  Conference (BMVC) 2018. Code: https://github.com/fartashf/vsepp", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new technique for learning visual-semantic embeddings for\ncross-modal retrieval. Inspired by hard negative mining, the use of hard\nnegatives in structured prediction, and ranking loss functions, we introduce a\nsimple change to common loss functions used for multi-modal embeddings. That,\ncombined with fine-tuning and use of augmented data, yields significant gains\nin retrieval performance. We showcase our approach, VSE++, on MS-COCO and\nFlickr30K datasets, using ablation studies and comparisons with existing\nmethods. On MS-COCO our approach outperforms state-of-the-art methods by 8.8%\nin caption retrieval and 11.3% in image retrieval (at R@1).\n", "versions": [{"version": "v1", "created": "Tue, 18 Jul 2017 13:51:32 GMT"}, {"version": "v2", "created": "Mon, 30 Oct 2017 15:55:21 GMT"}, {"version": "v3", "created": "Mon, 23 Jul 2018 20:42:43 GMT"}, {"version": "v4", "created": "Sun, 29 Jul 2018 19:11:57 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Faghri", "Fartash", ""], ["Fleet", "David J.", ""], ["Kiros", "Jamie Ryan", ""], ["Fidler", "Sanja", ""]]}, {"id": "1707.05647", "submitter": "Bolin Liu", "authors": "Bolin Liu, Xiao Shu, Xiaolin Wu", "title": "Fast Screening Algorithm for Rotation and Scale Invariant Template\n  Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a generic pre-processor for expediting conventional\ntemplate matching techniques. Instead of locating the best matched patch in the\nreference image to a query template via exhaustive search, the proposed\nalgorithm rules out regions with no possible matches with minimum computational\nefforts. While working on simple patch features, such as mean, variance and\ngradient, the fast pre-screening is highly discriminative. Its computational\nefficiency is gained by using a novel octagonal-star-shaped template and the\ninclusion-exclusion principle to extract and compare patch features. Moreover,\nit can handle arbitrary rotation and scaling of reference images effectively.\nExtensive experiments demonstrate that the proposed algorithm greatly reduces\nthe search space while never missing the best match.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jul 2017 14:38:07 GMT"}, {"version": "v2", "created": "Wed, 19 Jul 2017 07:51:08 GMT"}], "update_date": "2017-07-20", "authors_parsed": [["Liu", "Bolin", ""], ["Shu", "Xiao", ""], ["Wu", "Xiaolin", ""]]}, {"id": "1707.05653", "submitter": "Chandrasekhar Bhagavatula", "authors": "Chandrasekhar Bhagavatula, Chenchen Zhu, Khoa Luu, Marios Savvides", "title": "Faster Than Real-time Facial Alignment: A 3D Spatial Transformer Network\n  Approach in Unconstrained Poses", "comments": "International Conference on Computer Vision (ICCV) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial alignment involves finding a set of landmark points on an image with a\nknown semantic meaning. However, this semantic meaning of landmark points is\noften lost in 2D approaches where landmarks are either moved to visible\nboundaries or ignored as the pose of the face changes. In order to extract\nconsistent alignment points across large poses, the 3D structure of the face\nmust be considered in the alignment step. However, extracting a 3D structure\nfrom a single 2D image usually requires alignment in the first place. We\npresent our novel approach to simultaneously extract the 3D shape of the face\nand the semantically consistent 2D alignment through a 3D Spatial Transformer\nNetwork (3DSTN) to model both the camera projection matrix and the warping\nparameters of a 3D model. By utilizing a generic 3D model and a Thin Plate\nSpline (TPS) warping function, we are able to generate subject specific 3D\nshapes without the need for a large 3D shape basis. In addition, our proposed\nnetwork can be trained in an end-to-end framework on entirely synthetic data\nfrom the 300W-LP dataset. Unlike other 3D methods, our approach only requires\none pass through the network resulting in a faster than real-time alignment.\nEvaluations of our model on the Annotated Facial Landmarks in the Wild (AFLW)\nand AFLW2000-3D datasets show our method achieves state-of-the-art performance\nover other 3D approaches to alignment.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jul 2017 14:51:35 GMT"}, {"version": "v2", "created": "Fri, 8 Sep 2017 16:58:53 GMT"}], "update_date": "2017-09-11", "authors_parsed": [["Bhagavatula", "Chandrasekhar", ""], ["Zhu", "Chenchen", ""], ["Luu", "Khoa", ""], ["Savvides", "Marios", ""]]}, {"id": "1707.05683", "submitter": "Dalton Lunga", "authors": "Dalton Lunga, Dilip Patlolla, Lexie Yang, Jeanette Weaver, and\n  Budhendra Bhadhuri", "title": "Exploiting Convolutional Representations for Multiscale Human Settlement\n  Detection", "comments": "4 pages, 5 figures, IGARSS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We test this premise and explore representation spaces from a single deep\nconvolutional network and their visualization to argue for a novel unified\nfeature extraction framework. The objective is to utilize and re-purpose\ntrained feature extractors without the need for network retraining on three\nremote sensing tasks i.e. superpixel mapping, pixel-level segmentation and\nsemantic based image visualization. By leveraging the same convolutional\nfeature extractors and viewing them as visual information extractors that\nencode different image representation spaces, we demonstrate a preliminary\ninductive transfer learning potential on multiscale experiments that\nincorporate edge-level details up to semantic-level information.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jul 2017 15:27:18 GMT"}], "update_date": "2017-07-19", "authors_parsed": [["Lunga", "Dalton", ""], ["Patlolla", "Dilip", ""], ["Yang", "Lexie", ""], ["Weaver", "Jeanette", ""], ["Bhadhuri", "Budhendra", ""]]}, {"id": "1707.05685", "submitter": "Dalton Lunga", "authors": "Dalton Lunga, Lexie Yang, and Budhendra Bhaduri", "title": "Hashed Binary Search Sampling for Convolutional Network Training with\n  Large Overhead Image Patches", "comments": "4 pages, 5 figures, IGARSS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Very large overhead imagery associated with ground truth maps has the\npotential to generate billions of training image patches for machine learning\nalgorithms. However, random sampling selection criteria often leads to\nredundant and noisy-image patches for model training. With minimal research\nefforts behind this challenge, the current status spells missed opportunities\nto develop supervised learning algorithms that generalize over wide\ngeographical scenes. In addition, much of the computational cycles for large\nscale machine learning are poorly spent crunching through noisy and redundant\nimage patches. We demonstrate a potential framework to address these challenges\nspecifically, while evaluating a human settlement detection task. A novel\nbinary search tree sampling scheme is fused with a kernel based hashing\nprocedure that maps image patches into hash-buckets using binary codes\ngenerated from image content. The framework exploits inherent redundancy within\nbillions of image patches to promote mostly high variance preserving samples\nfor accelerating algorithmic training and increasing model generalization.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jul 2017 15:29:27 GMT"}], "update_date": "2017-07-19", "authors_parsed": [["Lunga", "Dalton", ""], ["Yang", "Lexie", ""], ["Bhaduri", "Budhendra", ""]]}, {"id": "1707.05691", "submitter": "Xintong Han", "authors": "Xintong Han, Zuxuan Wu, Yu-Gang Jiang, Larry S. Davis", "title": "Learning Fashion Compatibility with Bidirectional LSTMs", "comments": "ACM MM 17", "journal-ref": null, "doi": "10.1145/3123266.3123394", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ubiquity of online fashion shopping demands effective recommendation\nservices for customers. In this paper, we study two types of fashion\nrecommendation: (i) suggesting an item that matches existing components in a\nset to form a stylish outfit (a collection of fashion items), and (ii)\ngenerating an outfit with multimodal (images/text) specifications from a user.\nTo this end, we propose to jointly learn a visual-semantic embedding and the\ncompatibility relationships among fashion items in an end-to-end fashion. More\nspecifically, we consider a fashion outfit to be a sequence (usually from top\nto bottom and then accessories) and each item in the outfit as a time step.\nGiven the fashion items in an outfit, we train a bidirectional LSTM (Bi-LSTM)\nmodel to sequentially predict the next item conditioned on previous ones to\nlearn their compatibility relationships. Further, we learn a visual-semantic\nspace by regressing image features to their semantic representations aiming to\ninject attribute and category information as a regularization for training the\nLSTM. The trained network can not only perform the aforementioned\nrecommendations effectively but also predict the compatibility of a given\noutfit. We conduct extensive experiments on our newly collected Polyvore\ndataset, and the results provide strong qualitative and quantitative evidence\nthat our framework outperforms alternative methods.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jul 2017 15:36:53 GMT"}], "update_date": "2017-07-19", "authors_parsed": [["Han", "Xintong", ""], ["Wu", "Zuxuan", ""], ["Jiang", "Yu-Gang", ""], ["Davis", "Larry S.", ""]]}, {"id": "1707.05733", "submitter": "Oier Mees", "authors": "Oier Mees, Andreas Eitel, Wolfram Burgard", "title": "Choosing Smartly: Adaptive Multimodal Fusion for Object Detection in\n  Changing Environments", "comments": "Published at the 2016 IEEE/RSJ International Conference on\n  Intelligent Robots and Systems. Added a new baseline with respect to the IROS\n  version. Project page with code, pretrained models and our InOutDoorPeople\n  RGB-D dataset at http://adaptivefusion.cs.uni-freiburg.de/", "journal-ref": null, "doi": "10.1109/IROS.2016.7759048", "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detection is an essential task for autonomous robots operating in\ndynamic and changing environments. A robot should be able to detect objects in\nthe presence of sensor noise that can be induced by changing lighting\nconditions for cameras and false depth readings for range sensors, especially\nRGB-D cameras. To tackle these challenges, we propose a novel adaptive fusion\napproach for object detection that learns weighting the predictions of\ndifferent sensor modalities in an online manner. Our approach is based on a\nmixture of convolutional neural network (CNN) experts and incorporates multiple\nmodalities including appearance, depth and motion. We test our method in\nextensive robot experiments, in which we detect people in a combined indoor and\noutdoor scenario from RGB-D data, and we demonstrate that our method can adapt\nto harsh lighting changes and severe camera motion blur. Furthermore, we\npresent a new RGB-D dataset for people detection in mixed in- and outdoor\nenvironments, recorded with a mobile robot. Code, pretrained models and dataset\nare available at http://adaptivefusion.cs.uni-freiburg.de\n", "versions": [{"version": "v1", "created": "Tue, 18 Jul 2017 16:36:56 GMT"}, {"version": "v2", "created": "Tue, 19 Nov 2019 12:43:54 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Mees", "Oier", ""], ["Eitel", "Andreas", ""], ["Burgard", "Wolfram", ""]]}, {"id": "1707.05740", "submitter": "Jun Liu", "authors": "Jun Liu, Gang Wang, Ling-Yu Duan, Kamila Abdiyeva and Alex C. Kot", "title": "Skeleton-Based Human Action Recognition with Global Context-Aware\n  Attention LSTM Networks", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2017.2785279", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human action recognition in 3D skeleton sequences has attracted a lot of\nresearch attention. Recently, Long Short-Term Memory (LSTM) networks have shown\npromising performance in this task due to their strengths in modeling the\ndependencies and dynamics in sequential data. As not all skeletal joints are\ninformative for action recognition, and the irrelevant joints often bring noise\nwhich can degrade the performance, we need to pay more attention to the\ninformative ones. However, the original LSTM network does not have explicit\nattention ability. In this paper, we propose a new class of LSTM network,\nGlobal Context-Aware Attention LSTM (GCA-LSTM), for skeleton based action\nrecognition. This network is capable of selectively focusing on the informative\njoints in each frame of each skeleton sequence by using a global context memory\ncell. To further improve the attention capability of our network, we also\nintroduce a recurrent attention mechanism, with which the attention performance\nof the network can be enhanced progressively. Moreover, we propose a stepwise\ntraining scheme in order to train our network effectively. Our approach\nachieves state-of-the-art performance on five challenging benchmark datasets\nfor skeleton based action recognition.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jul 2017 17:03:53 GMT"}, {"version": "v2", "created": "Mon, 21 Aug 2017 05:34:53 GMT"}, {"version": "v3", "created": "Tue, 22 Aug 2017 02:36:41 GMT"}, {"version": "v4", "created": "Wed, 13 Dec 2017 09:49:38 GMT"}, {"version": "v5", "created": "Thu, 11 Jan 2018 15:36:27 GMT"}], "update_date": "2018-02-14", "authors_parsed": [["Liu", "Jun", ""], ["Wang", "Gang", ""], ["Duan", "Ling-Yu", ""], ["Abdiyeva", "Kamila", ""], ["Kot", "Alex C.", ""]]}, {"id": "1707.05743", "submitter": "Shazia Akbar", "authors": "Shazia Akbar, Mohammad Peikari, Sherine Salama, Sharon Nofech-Mozes,\n  Anne Martel", "title": "Transitioning between Convolutional and Fully Connected Layers in Neural\n  Networks", "comments": "This work is to appear at the 3rd workshop on Deep Learning in\n  Medical Image Analysis (DLMIA), MICCAI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Digital pathology has advanced substantially over the last decade however\ntumor localization continues to be a challenging problem due to highly complex\npatterns and textures in the underlying tissue bed. The use of convolutional\nneural networks (CNNs) to analyze such complex images has been well adopted in\ndigital pathology. However in recent years, the architecture of CNNs have\naltered with the introduction of inception modules which have shown great\npromise for classification tasks. In this paper, we propose a modified\n\"transition\" module which learns global average pooling layers from filters of\nvarying sizes to encourage class-specific filters at multiple spatial\nresolutions. We demonstrate the performance of the transition module in AlexNet\nand ZFNet, for classifying breast tumors in two independent datasets of scanned\nhistology sections, of which the transition module was superior.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jul 2017 17:07:20 GMT"}], "update_date": "2017-07-19", "authors_parsed": [["Akbar", "Shazia", ""], ["Peikari", "Mohammad", ""], ["Salama", "Sherine", ""], ["Nofech-Mozes", "Sharon", ""], ["Martel", "Anne", ""]]}, {"id": "1707.05754", "submitter": "Dingzeyu Li", "authors": "Dingzeyu Li, Avinash S. Nair, Shree K. Nayar, Changxi Zheng", "title": "AirCode: Unobtrusive Physical Tags for Digital Fabrication", "comments": "ACM UIST 2017 Technical Papers", "journal-ref": null, "doi": "10.1145/3126594.3126635", "report-no": null, "categories": "cs.HC cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present AirCode, a technique that allows the user to tag physically\nfabricated objects with given information. An AirCode tag consists of a group\nof carefully designed air pockets placed beneath the object surface. These air\npockets are easily produced during the fabrication process of the object,\nwithout any additional material or postprocessing. Meanwhile, the air pockets\naffect only the scattering light transport under the surface, and thus are hard\nto notice to our naked eyes. But, by using a computational imaging method, the\ntags become detectable. We present a tool that automates the design of air\npockets for the user to encode information. AirCode system also allows the user\nto retrieve the information from captured images via a robust decoding\nalgorithm. We demonstrate our tagging technique with applications for metadata\nembedding, robotic grasping, as well as conveying object affordances.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jul 2017 17:27:16 GMT"}, {"version": "v2", "created": "Mon, 7 Aug 2017 22:34:53 GMT"}], "update_date": "2017-08-09", "authors_parsed": [["Li", "Dingzeyu", ""], ["Nair", "Avinash S.", ""], ["Nayar", "Shree K.", ""], ["Zheng", "Changxi", ""]]}, {"id": "1707.05776", "submitter": "Piotr Bojanowski", "authors": "Piotr Bojanowski, Armand Joulin, David Lopez-Paz, Arthur Szlam", "title": "Optimizing the Latent Space of Generative Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Networks (GANs) have achieved remarkable results in\nthe task of generating realistic natural images. In most successful\napplications, GAN models share two common aspects: solving a challenging saddle\npoint optimization problem, interpreted as an adversarial game between a\ngenerator and a discriminator functions; and parameterizing the generator and\nthe discriminator as deep convolutional neural networks. The goal of this paper\nis to disentangle the contribution of these two factors to the success of GANs.\nIn particular, we introduce Generative Latent Optimization (GLO), a framework\nto train deep convolutional generators using simple reconstruction losses.\nThroughout a variety of experiments, we show that GLO enjoys many of the\ndesirable properties of GANs: synthesizing visually-appealing samples,\ninterpolating meaningfully between samples, and performing linear arithmetic\nwith noise vectors; all of this without the adversarial optimization scheme.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jul 2017 17:58:34 GMT"}, {"version": "v2", "created": "Mon, 20 May 2019 13:19:44 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Bojanowski", "Piotr", ""], ["Joulin", "Armand", ""], ["Lopez-Paz", "David", ""], ["Szlam", "Arthur", ""]]}, {"id": "1707.05809", "submitter": "Chia-Yu Kao", "authors": "Chia-Yu Kao, Leonard McMillan", "title": "A Novel Deep Learning Architecture for Testis Histology Image\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unlike other histology analysis, classification of tubule status in testis\nhistology is very challenging due to their high similarity of texture and\nshape. Traditional deep learning networks have difficulties to capture nuance\ndetails among different tubule categories. In this paper, we propose a novel\ndeep learning architecture for feature learning, image classification, and\nimage reconstruction. It is based on stacked auto-encoders with an additional\nlayer, called a hyperlayer, which is created to capture features of an image at\ndifferent layers in the network. This addition effectively combines features at\ndifferent scales and thus provides a more complete profile for further\nclassification. Evaluation is performed on a set of 10,542 tubule image\npatches. We demonstrate our approach with two experiments on two different\nsubsets of the dataset. The results show that the features learned from our\narchitecture achieve more than 98% accuracy and represent an improvement over\ntraditional deep network architectures.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jul 2017 18:19:41 GMT"}], "update_date": "2017-07-20", "authors_parsed": [["Kao", "Chia-Yu", ""], ["McMillan", "Leonard", ""]]}, {"id": "1707.05821", "submitter": "Arslan Chaudhry", "authors": "Arslan Chaudhry, Puneet K. Dokania, Philip H.S. Torr", "title": "Discovering Class-Specific Pixels for Weakly-Supervised Semantic\n  Segmentation", "comments": null, "journal-ref": "28th British Machine Vision Conference (BMVC), 2017", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an approach to discover class-specific pixels for the\nweakly-supervised semantic segmentation task. We show that properly combining\nsaliency and attention maps allows us to obtain reliable cues capable of\nsignificantly boosting the performance. First, we propose a simple yet powerful\nhierarchical approach to discover the class-agnostic salient regions, obtained\nusing a salient object detector, which otherwise would be ignored. Second, we\nuse fully convolutional attention maps to reliably localize the class-specific\nregions in a given image. We combine these two cues to discover class-specific\npixels which are then used as an approximate ground truth for training a CNN.\nWhile solving the weakly supervised semantic segmentation task, we ensure that\nthe image-level classification task is also solved in order to enforce the CNN\nto assign at least one pixel to each object present in the image.\nExperimentally, on the PASCAL VOC12 val and test sets, we obtain the mIoU of\n60.8% and 61.9%, achieving the performance gains of 5.1% and 5.2% compared to\nthe published state-of-the-art results. The code is made publicly available.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jul 2017 19:03:22 GMT"}], "update_date": "2018-08-17", "authors_parsed": [["Chaudhry", "Arslan", ""], ["Dokania", "Puneet K.", ""], ["Torr", "Philip H. S.", ""]]}, {"id": "1707.05847", "submitter": "Zbigniew Wojna", "authors": "Zbigniew Wojna, Vittorio Ferrari, Sergio Guadarrama, Nathan Silberman,\n  Liang-Chieh Chen, Alireza Fathi, Jasper Uijlings", "title": "The Devil is in the Decoder: Classification, Regression and GANs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many machine vision applications, such as semantic segmentation and depth\nprediction, require predictions for every pixel of the input image. Models for\nsuch problems usually consist of encoders which decrease spatial resolution\nwhile learning a high-dimensional representation, followed by decoders who\nrecover the original input resolution and result in low-dimensional\npredictions. While encoders have been studied rigorously, relatively few\nstudies address the decoder side. This paper presents an extensive comparison\nof a variety of decoders for a variety of pixel-wise tasks ranging from\nclassification, regression to synthesis. Our contributions are: (1) Decoders\nmatter: we observe significant variance in results between different types of\ndecoders on various problems. (2) We introduce new residual-like connections\nfor decoders. (3) We introduce a novel decoder: bilinear additive upsampling.\n(4) We explore prediction artifacts.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jul 2017 20:33:54 GMT"}, {"version": "v2", "created": "Sat, 12 Aug 2017 21:59:03 GMT"}, {"version": "v3", "created": "Tue, 19 Feb 2019 21:27:50 GMT"}], "update_date": "2019-02-21", "authors_parsed": [["Wojna", "Zbigniew", ""], ["Ferrari", "Vittorio", ""], ["Guadarrama", "Sergio", ""], ["Silberman", "Nathan", ""], ["Chen", "Liang-Chieh", ""], ["Fathi", "Alireza", ""], ["Uijlings", "Jasper", ""]]}, {"id": "1707.05905", "submitter": "Thomas Shortell Iii", "authors": "Thomas Shortell and Ali Shokoufandeh", "title": "Secure SURF with Fully Homomorphic Encryption", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud computing is an important part of today's world because offloading\ncomputations is a method to reduce costs. In this paper, we investigate\ncomputing the Speeded Up Robust Features (SURF) using Fully Homomorphic\nEncryption (FHE). Performing SURF in FHE enables a method to offload the\ncomputations while maintaining security and privacy of the original data. In\nsupport of this research, we developed a framework to compute SURF via a\nrational number based compatible with FHE. Although floating point (R) to\nrational numbers (Q) conversion introduces error, our research provides tight\nbounds on the magnitude of error in terms of parameters of FHE. We empirically\nverified the proposed method against a set of images at different sizes and\nshowed that our framework accurately computes most of the SURF keypoints in\nFHE.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jul 2017 00:30:23 GMT"}], "update_date": "2017-07-20", "authors_parsed": [["Shortell", "Thomas", ""], ["Shokoufandeh", "Ali", ""]]}, {"id": "1707.05911", "submitter": "Yufei Wang", "authors": "Yufei Wang, Zhe Lin, Xiaohui Shen, Radomir Mech, Gavin Miller,\n  Garrison W. Cottrell", "title": "Recognizing and Curating Photo Albums via Event-Specific Image\n  Importance", "comments": "Accepted as oral in BMVC 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic organization of personal photos is a problem with many real world\nap- plications, and can be divided into two main tasks: recognizing the event\ntype of the photo collection, and selecting interesting images from the\ncollection. In this paper, we attempt to simultaneously solve both tasks:\nalbum-wise event recognition and image- wise importance prediction. We\ncollected an album dataset with both event type labels and image importance\nlabels, refined from an existing CUFED dataset. We propose a hybrid system\nconsisting of three parts: A siamese network-based event-specific image\nimportance prediction, a Convolutional Neural Network (CNN) that recognizes the\nevent type, and a Long Short-Term Memory (LSTM)-based sequence level event\nrecognizer. We propose an iterative updating procedure for event type and image\nimportance score prediction. We experimentally verified that image importance\nscore prediction and event type recognition can each help the performance of\nthe other.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jul 2017 01:03:04 GMT"}], "update_date": "2017-07-20", "authors_parsed": [["Wang", "Yufei", ""], ["Lin", "Zhe", ""], ["Shen", "Xiaohui", ""], ["Mech", "Radomir", ""], ["Miller", "Gavin", ""], ["Cottrell", "Garrison W.", ""]]}, {"id": "1707.05929", "submitter": "Yang Song", "authors": "Yang Song, Yuan Li, Bo Wu, Chao-Yeh Chen, Xiao Zhang, Hartwig Adam", "title": "Learning Unified Embedding for Apparel Recognition", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In apparel recognition, specialized models (e.g. models trained for a\nparticular vertical like dresses) can significantly outperform general models\n(i.e. models that cover a wide range of verticals). Therefore, deep neural\nnetwork models are often trained separately for different verticals. However,\nusing specialized models for different verticals is not scalable and expensive\nto deploy. This paper addresses the problem of learning one unified embedding\nmodel for multiple object verticals (e.g. all apparel classes) without\nsacrificing accuracy. The problem is tackled from two aspects: training data\nand training difficulty. On the training data aspect, we figure out that for a\nsingle model trained with triplet loss, there is an accuracy sweet spot in\nterms of how many verticals are trained together. To ease the training\ndifficulty, a novel learning scheme is proposed by using the output from\nspecialized models as learning targets so that L2 loss can be used instead of\ntriplet loss. This new loss makes the training easier and make it possible for\nmore efficient use of the feature space. The end result is a unified model\nwhich can achieve the same retrieval accuracy as a number of separate\nspecialized models, while having the model complexity as one. The effectiveness\nof our approach is shown in experiments.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jul 2017 03:33:51 GMT"}, {"version": "v2", "created": "Tue, 15 Aug 2017 13:36:30 GMT"}], "update_date": "2017-08-16", "authors_parsed": [["Song", "Yang", ""], ["Li", "Yuan", ""], ["Wu", "Bo", ""], ["Chen", "Chao-Yeh", ""], ["Zhang", "Xiao", ""], ["Adam", "Hartwig", ""]]}, {"id": "1707.05938", "submitter": "Vishnu Naresh Boddeti", "authors": "Vishnu Naresh Boddeti, Myung-Cheol Roh, Jongju Shin, Takaharu Oguri,\n  Takeo Kanade", "title": "Face Alignment Robust to Pose, Expressions and Occlusions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an Ensemble of Robust Constrained Local Models for alignment of\nfaces in the presence of significant occlusions and of any unknown pose and\nexpression. To account for partial occlusions we introduce, Robust Constrained\nLocal Models, that comprises of a deformable shape and local landmark\nappearance model and reasons over binary occlusion labels. Our occlusion\nreasoning proceeds by a hypothesize-and-test search over occlusion labels.\nHypotheses are generated by Constrained Local Model based shape fitting over\nrandomly sampled subsets of landmark detector responses and are evaluated by\nthe quality of face alignment. To span the entire range of facial pose and\nexpression variations we adopt an ensemble of independent Robust Constrained\nLocal Models to search over a discretized representation of pose and\nexpression. We perform extensive evaluation on a large number of face images,\nboth occluded and unoccluded. We find that our face alignment system trained\nentirely on facial images captured \"in-the-lab\" exhibits a high degree of\ngeneralization to facial images captured \"in-the-wild\". Our results are\naccurate and stable over a wide spectrum of occlusions, pose and expression\nvariations resulting in excellent performance on many real-world face datasets.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jul 2017 05:05:31 GMT"}], "update_date": "2017-07-20", "authors_parsed": [["Boddeti", "Vishnu Naresh", ""], ["Roh", "Myung-Cheol", ""], ["Shin", "Jongju", ""], ["Oguri", "Takaharu", ""], ["Kanade", "Takeo", ""]]}, {"id": "1707.05950", "submitter": "Shirui Li", "authors": "Erbo Li, Hanlin Mo, Dong Xu and Hua Li", "title": "Image Projective Invariants", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose relative projective differential invariants (RPDIs)\nwhich are invariant to general projective transformations. By using RPDIs and\nthe structural frame of integral invariant, projective weighted moment\ninvariants (PIs) can be constructed very easily. It is first proved that a kind\nof projective invariants exists in terms of weighted integration of images,\nwith relative differential invariants as the weight functions. Then, some\nsimple instances of PIs are given. In order to ensure the stability and\ndiscriminability of PIs, we discuss how to calculate partial derivatives of\ndiscrete images more accurately. Since the number of pixels in discrete images\nbefore and after the geometric transformation may be different, we design the\nmethod to normalize the number of pixels. These ways enhance the performance of\nPIs. Finally, we carry out some experiments based on synthetic and real image\ndatasets. We choose commonly used moment invariants for comparison. The results\nindicate that PIs have better performance than other moment invariants in image\nretrieval and classification. With PIs, one can compare the similarity between\nimages under the projective transformation without knowing the parameters of\nthe transformation, which provides a good tool to shape analysis in image\nprocessing, computer vision and pattern recognition.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jul 2017 06:25:56 GMT"}], "update_date": "2017-07-20", "authors_parsed": [["Li", "Erbo", ""], ["Mo", "Hanlin", ""], ["Xu", "Dong", ""], ["Li", "Hua", ""]]}, {"id": "1707.05956", "submitter": "Chunhua Shen", "authors": "Hao Lu, Lei Zhang, Zhiguo Cao, Wei Wei, Ke Xian, Chunhua Shen, Anton\n  van den Hengel", "title": "When Unsupervised Domain Adaptation Meets Tensor Representations", "comments": "16 pages. Accepted to Proc. Int. Conf. Computer Vision (ICCV 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain adaption (DA) allows machine learning methods trained on data sampled\nfrom one distribution to be applied to data sampled from another. It is thus of\ngreat practical importance to the application of such methods. Despite the fact\nthat tensor representations are widely used in Computer Vision to capture\nmulti-linear relationships that affect the data, most existing DA methods are\napplicable to vectors only. This renders them incapable of reflecting and\npreserving important structure in many problems. We thus propose here a\nlearning-based method to adapt the source and target tensor representations\ndirectly, without vectorization. In particular, a set of alignment matrices is\nintroduced to align the tensor representations from both domains into the\ninvariant tensor subspace. These alignment matrices and the tensor subspace are\nmodeled as a joint optimization problem and can be learned adaptively from the\ndata using the proposed alternative minimization scheme. Extensive experiments\nshow that our approach is capable of preserving the discriminative power of the\nsource domain, of resisting the effects of label noise, and works effectively\nfor small sample sizes, and even one-shot DA. We show that our method\noutperforms the state-of-the-art on the task of cross-domain visual recognition\nin both efficacy and efficiency, and particularly that it outperforms all\ncomparators when applied to DA of the convolutional activations of deep\nconvolutional networks.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jul 2017 07:11:11 GMT"}], "update_date": "2017-07-20", "authors_parsed": [["Lu", "Hao", ""], ["Zhang", "Lei", ""], ["Cao", "Zhiguo", ""], ["Wei", "Wei", ""], ["Xian", "Ke", ""], ["Shen", "Chunhua", ""], ["Hengel", "Anton van den", ""]]}, {"id": "1707.05961", "submitter": "Olivier Colliot", "authors": "Emilie Gerardin, Ga\\\"el Ch\\'etelat, Marie Chupin, R\\'emi Cuingnet,\n  B\\'eatrice Desgranges, Ho-Sung Kim, Marc Niethammer, Bruno Dubois, St\\'ephane\n  Leh\\'ericy, Line Garnero, Francis Eustache, Olivier Colliot", "title": "Multidimensional classification of hippocampal shape features\n  discriminates Alzheimer's disease and mild cognitive impairment from normal\n  aging", "comments": "Data used in the preparation of this article were obtained from the\n  Alzheimer's Disease Neuroimaging Initiative (ADNI) database", "journal-ref": "NeuroImage, 47 (4), pp.1476-86, 2009", "doi": "10.1016/j.neuroimage.2009.05.036", "report-no": null, "categories": "cs.CV q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a new method to automatically discriminate between patients with\nAlzheimer's disease (AD) or mild cognitive impairment (MCI) and elderly\ncontrols, based on multidimensional classification of hippocampal shape\nfeatures. This approach uses spherical harmonics (SPHARM) coefficients to model\nthe shape of the hippocampi, which are segmented from magnetic resonance images\n(MRI) using a fully automatic method that we previously developed. SPHARM\ncoefficients are used as features in a classification procedure based on\nsupport vector machines (SVM). The most relevant features for classification\nare selected using a bagging strategy. We evaluate the accuracy of our method\nin a group of 23 patients with AD (10 males, 13 females, age $\\pm$\nstandard-deviation (SD) = 73 $\\pm$ 6 years, mini-mental score (MMS) = 24.4\n$\\pm$ 2.8), 23 patients with amnestic MCI (10 males, 13 females, age $\\pm$ SD =\n74 $\\pm$ 8 years, MMS = 27.3 $\\pm$ 1.4) and 25 elderly healthy controls (13\nmales, 12 females, age $\\pm$ SD = 64 $\\pm$ 8 years), using leave-one-out\ncross-validation. For AD vs controls, we obtain a correct classification rate\nof 94%, a sensitivity of 96%, and a specificity of 92%. For MCI vs controls, we\nobtain a classification rate of 83%, a sensitivity of 83%, and a specificity of\n84%. This accuracy is superior to that of hippocampal volumetry and is\ncomparable to recently published SVM-based whole-brain classification methods,\nwhich relied on a different strategy. This new method may become a useful tool\nto assist in the diagnosis of Alzheimer's disease.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jul 2017 07:33:02 GMT"}], "update_date": "2017-07-20", "authors_parsed": [["Gerardin", "Emilie", ""], ["Ch\u00e9telat", "Ga\u00ebl", ""], ["Chupin", "Marie", ""], ["Cuingnet", "R\u00e9mi", ""], ["Desgranges", "B\u00e9atrice", ""], ["Kim", "Ho-Sung", ""], ["Niethammer", "Marc", ""], ["Dubois", "Bruno", ""], ["Leh\u00e9ricy", "St\u00e9phane", ""], ["Garnero", "Line", ""], ["Eustache", "Francis", ""], ["Colliot", "Olivier", ""]]}, {"id": "1707.05972", "submitter": "Meng-Ru Hsieh", "authors": "Meng-Ru Hsieh, Yen-Liang Lin, and Winston H. Hsu", "title": "Drone-based Object Counting by Spatially Regularized Regional Proposal\n  Network", "comments": "ICCV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing counting methods often adopt regression-based approaches and cannot\nprecisely localize the target objects, which hinders the further analysis\n(e.g., high-level understanding and fine-grained classification). In addition,\nmost of prior work mainly focus on counting objects in static environments with\nfixed cameras. Motivated by the advent of unmanned flying vehicles (i.e.,\ndrones), we are interested in detecting and counting objects in such dynamic\nenvironments. We propose Layout Proposal Networks (LPNs) and spatial kernels to\nsimultaneously count and localize target objects (e.g., cars) in videos\nrecorded by the drone. Different from the conventional region proposal methods,\nwe leverage the spatial layout information (e.g., cars often park regularly)\nand introduce these spatially regularized constraints into our network to\nimprove the localization accuracy. To evaluate our counting method, we present\na new large-scale car parking lot dataset (CARPK) that contains nearly 90,000\ncars captured from different parking lots. To the best of our knowledge, it is\nthe first and the largest drone view dataset that supports object counting, and\nprovides the bounding box annotations.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jul 2017 08:27:17 GMT"}, {"version": "v2", "created": "Tue, 1 Aug 2017 01:40:16 GMT"}, {"version": "v3", "created": "Thu, 3 Aug 2017 00:06:06 GMT"}], "update_date": "2017-08-04", "authors_parsed": [["Hsieh", "Meng-Ru", ""], ["Lin", "Yen-Liang", ""], ["Hsu", "Winston H.", ""]]}, {"id": "1707.05974", "submitter": "Jingdong Wang", "authors": "Jingdong Wang, Yajie Xing, Kexin Zhang, and Cha Zhang", "title": "Orthogonal and Idempotent Transformations for Learning Deep Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identity transformations, used as skip-connections in residual networks,\ndirectly connect convolutional layers close to the input and those close to the\noutput in deep neural networks, improving information flow and thus easing the\ntraining. In this paper, we introduce two alternative linear transforms,\northogonal transformation and idempotent transformation. According to the\ndefinition and property of orthogonal and idempotent matrices, the product of\nmultiple orthogonal (same idempotent) matrices, used to form linear\ntransformations, is equal to a single orthogonal (idempotent) matrix, resulting\nin that information flow is improved and the training is eased. One interesting\npoint is that the success essentially stems from feature reuse and gradient\nreuse in forward and backward propagation for maintaining the information\nduring flow and eliminating the gradient vanishing problem because of the\nexpress way through skip-connections. We empirically demonstrate the\neffectiveness of the proposed two transformations: similar performance in\nsingle-branch networks and even superior in multi-branch networks in comparison\nto identity transformations.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jul 2017 08:35:10 GMT"}], "update_date": "2017-07-20", "authors_parsed": [["Wang", "Jingdong", ""], ["Xing", "Yajie", ""], ["Zhang", "Kexin", ""], ["Zhang", "Cha", ""]]}, {"id": "1707.06005", "submitter": "Nicolas Chesneau", "authors": "Nicolas Chesneau, Gr\\'egory Rogez, Karteek Alahari, Cordelia Schmid", "title": "Detecting Parts for Action Localization", "comments": "BMVC 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a new framework for action localization that tracks\npeople in videos and extracts full-body human tubes, i.e., spatio-temporal\nregions localizing actions, even in the case of occlusions or truncations. This\nis achieved by training a novel human part detector that scores visible parts\nwhile regressing full-body bounding boxes. The core of our method is a\nconvolutional neural network which learns part proposals specific to certain\nbody parts. These are then combined to detect people robustly in each frame.\nOur tracking algorithm connects the image detections temporally to extract\nfull-body human tubes. We apply our new tube extraction method on the problem\nof human action localization, on the popular JHMDB dataset, and a very recent\nchallenging dataset DALY (Daily Action Localization in YouTube), showing\nstate-of-the-art results.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jul 2017 10:09:59 GMT"}, {"version": "v2", "created": "Fri, 21 Jul 2017 21:40:33 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Chesneau", "Nicolas", ""], ["Rogez", "Gr\u00e9gory", ""], ["Alahari", "Karteek", ""], ["Schmid", "Cordelia", ""]]}, {"id": "1707.06017", "submitter": "Shervine Amidi", "authors": "Afshine Amidi, Shervine Amidi, Dimitrios Vlachakis, Vasileios\n  Megalooikonomou, Nikos Paragios and Evangelia I. Zacharaki", "title": "EnzyNet: enzyme classification using 3D convolutional neural networks on\n  spatial representation", "comments": "11 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  During the past decade, with the significant progress of computational power\nas well as ever-rising data availability, deep learning techniques became\nincreasingly popular due to their excellent performance on computer vision\nproblems. The size of the Protein Data Bank has increased more than 15 fold\nsince 1999, which enabled the expansion of models that aim at predicting\nenzymatic function via their amino acid composition. Amino acid sequence\nhowever is less conserved in nature than protein structure and therefore\nconsidered a less reliable predictor of protein function. This paper presents\nEnzyNet, a novel 3D-convolutional neural networks classifier that predicts the\nEnzyme Commission number of enzymes based only on their voxel-based spatial\nstructure. The spatial distribution of biochemical properties was also examined\nas complementary information. The 2-layer architecture was investigated on a\nlarge dataset of 63,558 enzymes from the Protein Data Bank and achieved an\naccuracy of 78.4% by exploiting only the binary representation of the protein\nshape. Code and datasets are available at https://github.com/shervinea/enzynet.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jul 2017 10:59:29 GMT"}], "update_date": "2017-07-20", "authors_parsed": [["Amidi", "Afshine", ""], ["Amidi", "Shervine", ""], ["Vlachakis", "Dimitrios", ""], ["Megalooikonomou", "Vasileios", ""], ["Paragios", "Nikos", ""], ["Zacharaki", "Evangelia I.", ""]]}, {"id": "1707.06029", "submitter": "Jongwook Choi", "authors": "Youngjae Yu, Jongwook Choi, Yeonhwa Kim, Kyung Yoo, Sang-Hun Lee,\n  Gunhee Kim", "title": "Supervising Neural Attention Models for Video Captioning by Human Gaze\n  Data", "comments": "In CVPR 2017. 9 pages + supplementary 17 pages", "journal-ref": "The IEEE Conference on Computer Vision and Pattern Recognition\n  (CVPR), 2017, pp. 490-498", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The attention mechanisms in deep neural networks are inspired by human's\nattention that sequentially focuses on the most relevant parts of the\ninformation over time to generate prediction output. The attention parameters\nin those models are implicitly trained in an end-to-end manner, yet there have\nbeen few trials to explicitly incorporate human gaze tracking to supervise the\nattention models. In this paper, we investigate whether attention models can\nbenefit from explicit human gaze labels, especially for the task of video\ncaptioning. We collect a new dataset called VAS, consisting of movie clips, and\ncorresponding multiple descriptive sentences along with human gaze tracking\ndata. We propose a video captioning model named Gaze Encoding Attention Network\n(GEAN) that can leverage gaze tracking information to provide the spatial and\ntemporal attention for sentence generation. Through evaluation of language\nsimilarity metrics and human assessment via Amazon mechanical Turk, we\ndemonstrate that spatial attentions guided by human gaze data indeed improve\nthe performance of multiple captioning methods. Moreover, we show that the\nproposed approach achieves the state-of-the-art performance for both gaze\nprediction and video captioning not only in our VAS dataset but also in\nstandard datasets (e.g. LSMDC and Hollywood2).\n", "versions": [{"version": "v1", "created": "Wed, 19 Jul 2017 11:44:36 GMT"}], "update_date": "2017-07-20", "authors_parsed": [["Yu", "Youngjae", ""], ["Choi", "Jongwook", ""], ["Kim", "Yeonhwa", ""], ["Yoo", "Kyung", ""], ["Lee", "Sang-Hun", ""], ["Kim", "Gunhee", ""]]}, {"id": "1707.06053", "submitter": "Maayan Frid-Adar", "authors": "Maayan Frid-Adar, Idit Diamant, Eyal Klang, Michal Amitai, Jacob\n  Goldberger, Hayit Greenspan", "title": "Modeling the Intra-class Variability for Liver Lesion Detection using a\n  Multi-class Patch-based CNN", "comments": "To be presented at PatchMI: 3rd International Workshop on Patch-based\n  Techniques in Medical Imaging, MICCAI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic detection of liver lesions in CT images poses a great challenge for\nresearchers. In this work we present a deep learning approach that models\nexplicitly the variability within the non-lesion class, based on prior\nknowledge of the data, to support an automated lesion detection system. A\nmulti-class convolutional neural network (CNN) is proposed to categorize input\nimage patches into sub-categories of boundary and interior patches, the\ndecisions of which are fused to reach a binary lesion vs non-lesion decision.\nFor validation of our system, we use CT images of 132 livers and 498 lesions.\nOur approach shows highly improved detection results that outperform the\nstate-of-the-art fully convolutional network. Automated computerized tools, as\nshown in this work, have the potential in the future to support the\nradiologists towards improved detection.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jul 2017 12:52:38 GMT"}, {"version": "v2", "created": "Thu, 20 Jul 2017 05:13:29 GMT"}], "update_date": "2017-07-21", "authors_parsed": [["Frid-Adar", "Maayan", ""], ["Diamant", "Idit", ""], ["Klang", "Eyal", ""], ["Amitai", "Michal", ""], ["Goldberger", "Jacob", ""], ["Greenspan", "Hayit", ""]]}, {"id": "1707.06068", "submitter": "Anton Eremeev", "authors": "Anton V. Eremeev, Alexander V. Kelmanov, Artem V. Pyatkin and Igor A.\n  Ziegler", "title": "On Finding Maximum Cardinality Subset of Vectors with a Constraint on\n  Normalized Squared Length of Vectors Sum", "comments": "To appear in Proceedings of the 6th International Conference on\n  Analysis of Images, Social Networks, and Texts (AIST'2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the problem of finding a maximum cardinality\nsubset of vectors, given a constraint on the normalized squared length of\nvectors sum. This problem is closely related to Problem 1 from (Eremeev,\nKel'manov, Pyatkin, 2016). The main difference consists in swapping the\nconstraint with the optimization criterion.\n  We prove that the problem is NP-hard even in terms of finding a feasible\nsolution. An exact algorithm for solving this problem is proposed. The\nalgorithm has a pseudo-polynomial time complexity in the special case of the\nproblem, where the dimension of the space is bounded from above by a constant\nand the input data are integer. A computational experiment is carried out,\nwhere the proposed algorithm is compared to COINBONMIN solver, applied to a\nmixed integer quadratic programming formulation of the problem. The results of\nthe experiment indicate superiority of the proposed algorithm when the\ndimension of Euclidean space is low, while the COINBONMIN has an advantage for\nlarger dimensions.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jul 2017 13:05:46 GMT"}], "update_date": "2017-07-20", "authors_parsed": [["Eremeev", "Anton V.", ""], ["Kelmanov", "Alexander V.", ""], ["Pyatkin", "Artem V.", ""], ["Ziegler", "Igor A.", ""]]}, {"id": "1707.06089", "submitter": "M. Saquib Sarfraz", "authors": "M. Saquib Sarfraz, Arne Schumann, Yan Wang, Rainer Stiefelhagen", "title": "Deep View-Sensitive Pedestrian Attribute Inference in an end-to-end\n  Model", "comments": "accepted BMVC 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pedestrian attribute inference is a demanding problem in visual surveillance\nthat can facilitate person retrieval, search and indexing. To exploit semantic\nrelations between attributes, recent research treats it as a multi-label image\nclassification task. The visual cues hinting at attributes can be strongly\nlocalized and inference of person attributes such as hair, backpack, shorts,\netc., are highly dependent on the acquired view of the pedestrian. In this\npaper we assert this dependence in an end-to-end learning framework and show\nthat a view-sensitive attribute inference is able to learn better attribute\npredictions. Our proposed model jointly predicts the coarse pose (view) of the\npedestrian and learns specialized view-specific multi-label attribute\npredictions. We show in an extensive evaluation on three challenging datasets\n(PETA, RAP and WIDER) that our proposed end-to-end view-aware attribute\nprediction model provides competitive performance and improves on the published\nstate-of-the-art on these datasets.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jul 2017 13:49:47 GMT"}], "update_date": "2017-07-20", "authors_parsed": [["Sarfraz", "M. Saquib", ""], ["Schumann", "Arne", ""], ["Wang", "Yan", ""], ["Stiefelhagen", "Rainer", ""]]}, {"id": "1707.06119", "submitter": "Petar Palasek", "authors": "Petar Palasek, Ioannis Patras", "title": "Discriminative convolutional Fisher vector network for action\n  recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we propose a novel neural network architecture for the problem\nof human action recognition in videos. The proposed architecture expresses the\nprocessing steps of classical Fisher vector approaches, that is dimensionality\nreduction by principal component analysis (PCA) projection, Gaussian mixture\nmodel (GMM) and Fisher vector descriptor extraction, as network layers. By\ncontrast to other methods where these steps are performed consecutively and the\ncorresponding parameters are learned in an unsupervised manner, having them\ndefined as a single neural network allows us to refine the whole model\ndiscriminatively in an end to end fashion. Furthermore, we show that the\nproposed architecture can be used as a replacement for the fully connected\nlayers in popular convolutional networks achieving a comparable classification\nperformance, or even significantly surpassing the performance of similar\narchitectures while reducing the total number of trainable parameters by a\nfactor of 5. We show that our method achieves significant improvements in\ncomparison to the classical chain.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jul 2017 14:35:28 GMT"}], "update_date": "2017-07-20", "authors_parsed": [["Palasek", "Petar", ""], ["Patras", "Ioannis", ""]]}, {"id": "1707.06145", "submitter": "Xiang Li", "authors": "Xiang Li, Aoxiao Zhong, Ming Lin, Ning Guo, Mu Sun, Arkadiusz Sitek,\n  Jieping Ye, James Thrall, Quanzheng Li", "title": "Self-paced Convolutional Neural Network for Computer Aided Detection in\n  Medical Imaging Analysis", "comments": "accepted by 8th International Workshop on Machine Learning in Medical\n  Imaging (MLMI 2017)", "journal-ref": null, "doi": "10.1007/978-3-319-67389-9_25", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tissue characterization has long been an important component of Computer\nAided Diagnosis (CAD) systems for automatic lesion detection and further\nclinical planning. Motivated by the superior performance of deep learning\nmethods on various computer vision problems, there has been increasing work\napplying deep learning to medical image analysis. However, the development of a\nrobust and reliable deep learning model for computer-aided diagnosis is still\nhighly challenging due to the combination of the high heterogeneity in the\nmedical images and the relative lack of training samples. Specifically,\nannotation and labeling of the medical images is much more expensive and\ntime-consuming than other applications and often involves manual labor from\nmultiple domain experts. In this work, we propose a multi-stage, self-paced\nlearning framework utilizing a convolutional neural network (CNN) to classify\nComputed Tomography (CT) image patches. The key contribution of this approach\nis that we augment the size of training samples by refining the unlabeled\ninstances with a self-paced learning CNN. By implementing the framework on high\nperformance computing servers including the NVIDIA DGX1 machine, we obtained\nthe experimental result, showing that the self-pace boosted network\nconsistently outperformed the original network even with very scarce manual\nlabels. The performance gain indicates that applications with limited training\nsamples such as medical image analysis can benefit from using the proposed\nframework.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jul 2017 15:15:36 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Li", "Xiang", ""], ["Zhong", "Aoxiao", ""], ["Lin", "Ming", ""], ["Guo", "Ning", ""], ["Sun", "Mu", ""], ["Sitek", "Arkadiusz", ""], ["Ye", "Jieping", ""], ["Thrall", "James", ""], ["Li", "Quanzheng", ""]]}, {"id": "1707.06168", "submitter": "Yihui He", "authors": "Yihui He, Xiangyu Zhang, Jian Sun", "title": "Channel Pruning for Accelerating Very Deep Neural Networks", "comments": "To be appear at ICCV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a new channel pruning method to accelerate very\ndeep convolutional neural networks.Given a trained CNN model, we propose an\niterative two-step algorithm to effectively prune each layer, by a LASSO\nregression based channel selection and least square reconstruction. We further\ngeneralize this algorithm to multi-layer and multi-branch cases. Our method\nreduces the accumulated error and enhance the compatibility with various\narchitectures. Our pruned VGG-16 achieves the state-of-the-art results by 5x\nspeed-up along with only 0.3% increase of error. More importantly, our method\nis able to accelerate modern networks like ResNet, Xception and suffers only\n1.4%, 1.0% accuracy loss under 2x speed-up respectively, which is significant.\nCode has been made publicly available.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jul 2017 15:52:14 GMT"}, {"version": "v2", "created": "Mon, 21 Aug 2017 09:44:26 GMT"}], "update_date": "2017-08-22", "authors_parsed": [["He", "Yihui", ""], ["Zhang", "Xiangyu", ""], ["Sun", "Jian", ""]]}, {"id": "1707.06175", "submitter": "Taylor Mordan", "authors": "Taylor Mordan, Nicolas Thome, Matthieu Cord and Gilles Henaff", "title": "Deformable Part-based Fully Convolutional Network for Object Detection", "comments": "Accepted to BMVC 2017 (oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing region-based object detectors are limited to regions with fixed box\ngeometry to represent objects, even if those are highly non-rectangular. In\nthis paper we introduce DP-FCN, a deep model for object detection which\nexplicitly adapts to shapes of objects with deformable parts. Without\nadditional annotations, it learns to focus on discriminative elements and to\nalign them, and simultaneously brings more invariance for classification and\ngeometric information to refine localization. DP-FCN is composed of three main\nmodules: a Fully Convolutional Network to efficiently maintain spatial\nresolution, a deformable part-based RoI pooling layer to optimize positions of\nparts and build invariance, and a deformation-aware localization module\nexplicitly exploiting displacements of parts to improve accuracy of bounding\nbox regression. We experimentally validate our model and show significant\ngains. DP-FCN achieves state-of-the-art performances of 83.1% and 80.9% on\nPASCAL VOC 2007 and 2012 with VOC data only.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jul 2017 16:03:05 GMT"}], "update_date": "2017-07-20", "authors_parsed": [["Mordan", "Taylor", ""], ["Thome", "Nicolas", ""], ["Cord", "Matthieu", ""], ["Henaff", "Gilles", ""]]}, {"id": "1707.06180", "submitter": "Amogh Gudi", "authors": "Amogh Gudi, Nicolai van Rosmalen, Marco Loog, Jan van Gemert", "title": "Object-Extent Pooling for Weakly Supervised Single-Shot Localization", "comments": "In British Machine Vision Conference 2017 (BMVC'17)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the face of scarcity in detailed training annotations, the ability to\nperform object localization tasks in real-time with weak-supervision is very\nvaluable. However, the computational cost of generating and evaluating region\nproposals is heavy. We adapt the concept of Class Activation Maps (CAM) into\nthe very first weakly-supervised 'single-shot' detector that does not require\nthe use of region proposals. To facilitate this, we propose a novel global\npooling technique called Spatial Pyramid Averaged Max (SPAM) pooling for\ntraining this CAM-based network for object extent localisation with only weak\nimage-level supervision. We show this global pooling layer possesses a near\nideal flow of gradients for extent localization, that offers a good trade-off\nbetween the extremes of max and average pooling. Our approach only requires a\nsingle network pass and uses a fast-backprojection technique, completely\nomitting any region proposal steps. To the best of our knowledge, this is the\nfirst approach to do so. Due to this, we are able to perform inference in\nreal-time at 35fps, which is an order of magnitude faster than all previous\nweakly supervised object localization frameworks.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jul 2017 16:10:53 GMT"}], "update_date": "2017-07-20", "authors_parsed": [["Gudi", "Amogh", ""], ["van Rosmalen", "Nicolai", ""], ["Loog", "Marco", ""], ["van Gemert", "Jan", ""]]}, {"id": "1707.06183", "submitter": "Maxime Lafarge", "authors": "Maxime W. Lafarge, Josien P.W. Pluim, Koen A.J. Eppenhof, Pim Moeskops\n  and Mitko Veta", "title": "Domain-adversarial neural networks to address the appearance variability\n  of histopathology images", "comments": "MICCAI 2017 Workshop on Deep Learning in Medical Image Analysis", "journal-ref": null, "doi": "10.1007/978-3-319-67558-9", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Preparing and scanning histopathology slides consists of several steps, each\nwith a multitude of parameters. The parameters can vary between pathology labs\nand within the same lab over time, resulting in significant variability of the\ntissue appearance that hampers the generalization of automatic image analysis\nmethods. Typically, this is addressed with ad-hoc approaches such as staining\nnormalization that aim to reduce the appearance variability. In this paper, we\npropose a systematic solution based on domain-adversarial neural networks. We\nhypothesize that removing the domain information from the model representation\nleads to better generalization. We tested our hypothesis for the problem of\nmitosis detection in breast cancer histopathology images and made a comparative\nanalysis with two other approaches. We show that combining color augmentation\nwith domain-adversarial training is a better alternative than standard\napproaches to improve the generalization of deep learning methods.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jul 2017 16:21:59 GMT"}], "update_date": "2018-01-11", "authors_parsed": [["Lafarge", "Maxime W.", ""], ["Pluim", "Josien P. W.", ""], ["Eppenhof", "Koen A. J.", ""], ["Moeskops", "Pim", ""], ["Veta", "Mitko", ""]]}, {"id": "1707.06263", "submitter": "Enzo Ferrante", "authors": "Enzo Ferrante and Puneet K Dokania and Rafael Marini and Nikos\n  Paragios", "title": "Deformable Registration through Learning of Context-Specific Metric\n  Aggregation", "comments": "Accepted for publication in the 8th International Workshop on Machine\n  Learning in Medical Imaging (MLMI 2017), in conjunction with MICCAI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel weakly supervised discriminative algorithm for learning\ncontext specific registration metrics as a linear combination of conventional\nsimilarity measures. Conventional metrics have been extensively used over the\npast two decades and therefore both their strengths and limitations are known.\nThe challenge is to find the optimal relative weighting (or parameters) of\ndifferent metrics forming the similarity measure of the registration algorithm.\nHand-tuning these parameters would result in sub optimal solutions and quickly\nbecome infeasible as the number of metrics increases. Furthermore, such\nhand-crafted combination can only happen at global scale (entire volume) and\ntherefore will not be able to account for the different tissue properties. We\npropose a learning algorithm for estimating these parameters locally,\nconditioned to the data semantic classes. The objective function of our\nformulation is a special case of non-convex function, difference of convex\nfunction, which we optimize using the concave convex procedure. As a proof of\nconcept, we show the impact of our approach on three challenging datasets for\ndifferent anatomical structures and modalities.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jul 2017 19:06:38 GMT"}], "update_date": "2017-07-21", "authors_parsed": [["Ferrante", "Enzo", ""], ["Dokania", "Puneet K", ""], ["Marini", "Rafael", ""], ["Paragios", "Nikos", ""]]}, {"id": "1707.06267", "submitter": "Matheus Gadelha", "authors": "Matheus Gadelha, Subhransu Maji, Rui Wang", "title": "Shape Generation using Spatially Partitioned Point Clouds", "comments": "To appear at BMVC 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method to generate 3D shapes using point clouds. Given a\npoint-cloud representation of a 3D shape, our method builds a kd-tree to\nspatially partition the points. This orders them consistently across all\nshapes, resulting in reasonably good correspondences across all shapes. We then\nuse PCA analysis to derive a linear shape basis across the spatially\npartitioned points, and optimize the point ordering by iteratively minimizing\nthe PCA reconstruction error. Even with the spatial sorting, the point clouds\nare inherently noisy and the resulting distribution over the shape coefficients\ncan be highly multi-modal. We propose to use the expressive power of neural\nnetworks to learn a distribution over the shape coefficients in a\ngenerative-adversarial framework. Compared to 3D shape generative models\ntrained on voxel-representations, our point-based method is considerably more\nlight-weight and scalable, with little loss of quality. It also outperforms\nsimpler linear factor models such as Probabilistic PCA, both qualitatively and\nquantitatively, on a number of categories from the ShapeNet dataset.\nFurthermore, our method can easily incorporate other point attributes such as\nnormal and color information, an additional advantage over voxel-based\nrepresentations.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jul 2017 19:32:47 GMT"}], "update_date": "2017-07-21", "authors_parsed": [["Gadelha", "Matheus", ""], ["Maji", "Subhransu", ""], ["Wang", "Rui", ""]]}, {"id": "1707.06286", "submitter": "Amin Jourabloo", "authors": "Amin Jourabloo, Mao Ye, Xiaoming Liu, Liu Ren", "title": "Pose-Invariant Face Alignment with a Single CNN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face alignment has witnessed substantial progress in the last decade. One of\nthe recent focuses has been aligning a dense 3D face shape to face images with\nlarge head poses. The dominant technology used is based on the cascade of\nregressors, e.g., CNN, which has shown promising results. Nonetheless, the\ncascade of CNNs suffers from several drawbacks, e.g., lack of end-to-end\ntraining, hand-crafted features and slow training speed. To address these\nissues, we propose a new layer, named visualization layer, that can be\nintegrated into the CNN architecture and enables joint optimization with\ndifferent loss functions. Extensive evaluation of the proposed method on\nmultiple datasets demonstrates state-of-the-art accuracy, while reducing the\ntraining time by more than half compared to the typical cascade of CNNs. In\naddition, we compare multiple CNN architectures with the visualization layer to\nfurther demonstrate the advantage of its utilization.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jul 2017 20:34:08 GMT"}], "update_date": "2017-07-21", "authors_parsed": [["Jourabloo", "Amin", ""], ["Ye", "Mao", ""], ["Liu", "Xiaoming", ""], ["Ren", "Liu", ""]]}, {"id": "1707.06292", "submitter": "Burak Benligiray", "authors": "Burak Benligiray, Cihan Topal, Cuneyt Akinlar", "title": "STag: A Stable Fiducial Marker System", "comments": "Accepted to be published in Image and Vision Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fiducial markers provide better-defined features than the ones naturally\navailable in the scene. For this reason, they are widely utilized in computer\nvision applications where reliable pose estimation is required. Factors such as\nimaging noise and subtle changes in illumination induce jitter on the estimated\npose. Jitter impairs robustness in vision and robotics applications, and\ndeteriorates the sense of presence and immersion in AR/VR applications. In this\npaper, we propose STag, a fiducial marker system that provides stable pose\nestimation. STag is designed to be robust against jitter factors, thus sustains\npose stability better than the existing solutions. This is achieved by\nutilizing geometric features that can be localized more repeatably. The outer\nsquare border of the marker is used for detection and homography estimation.\nThis is followed by a novel homography refinement step using the inner circular\nborder. After refinement, the pose can be estimated stably and robustly across\nviewing conditions. These features are demonstrated with a comprehensive set of\nexperiments, including comparisons with the state of the art fiducial marker\nsystems.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jul 2017 20:53:27 GMT"}, {"version": "v2", "created": "Mon, 1 Jul 2019 10:35:14 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Benligiray", "Burak", ""], ["Topal", "Cihan", ""], ["Akinlar", "Cuneyt", ""]]}, {"id": "1707.06314", "submitter": "Aleksander Klibisz", "authors": "Aleksander Klibisz, Derek Rose, Matthew Eicholtz, Jay Blundon, and\n  Stanislav Zakharenko", "title": "Fast, Simple Calcium Imaging Segmentation with Fully Convolutional\n  Networks", "comments": "Accepted to 3rd Workshop on Deep Learning in Medical Image Analysis\n  (http://cs.adelaide.edu.au/~dlmia3/)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Calcium imaging is a technique for observing neuron activity as a series of\nimages showing indicator fluorescence over time. Manually segmenting neurons is\ntime-consuming, leading to research on automated calcium imaging segmentation\n(ACIS). We evaluated several deep learning models for ACIS on the Neurofinder\ncompetition datasets and report our best model: U-Net2DS, a fully convolutional\nnetwork that operates on 2D mean summary images. U-Net2DS requires minimal\ndomain-specific pre/post-processing and parameter adjustment, and predictions\nare made on full $512\\times512$ images at $\\approx$9K images per minute. It\nranks third in the Neurofinder competition ($F_1=0.569$) and is the best model\nto exclusively use deep learning. We also demonstrate useful segmentations on\ndata from outside the competition. The model's simplicity, speed, and quality\nresults make it a practical choice for ACIS and a strong baseline for more\ncomplex models in the future.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jul 2017 22:27:29 GMT"}], "update_date": "2017-07-21", "authors_parsed": [["Klibisz", "Aleksander", ""], ["Rose", "Derek", ""], ["Eicholtz", "Matthew", ""], ["Blundon", "Jay", ""], ["Zakharenko", "Stanislav", ""]]}, {"id": "1707.06316", "submitter": "Yi Zhu", "authors": "Yi Zhu and Shawn Newsam", "title": "DenseNet for Dense Flow", "comments": "Accepted at ICIP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classical approaches for estimating optical flow have achieved rapid progress\nin the last decade. However, most of them are too slow to be applied in\nreal-time video analysis. Due to the great success of deep learning, recent\nwork has focused on using CNNs to solve such dense prediction problems. In this\npaper, we investigate a new deep architecture, Densely Connected Convolutional\nNetworks (DenseNet), to learn optical flow. This specific architecture is ideal\nfor the problem at hand as it provides shortcut connections throughout the\nnetwork, which leads to implicit deep supervision. We extend current DenseNet\nto a fully convolutional network to learn motion estimation in an unsupervised\nmanner. Evaluation results on three standard benchmarks demonstrate that\nDenseNet is a better fit than other widely adopted CNN architectures for\noptical flow estimation.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jul 2017 22:37:46 GMT"}], "update_date": "2017-07-21", "authors_parsed": [["Zhu", "Yi", ""], ["Newsam", "Shawn", ""]]}, {"id": "1707.06320", "submitter": "Douwe Kiela", "authors": "Douwe Kiela, Alexis Conneau, Allan Jabri and Maximilian Nickel", "title": "Learning Visually Grounded Sentence Representations", "comments": "Published at NAACL-18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a variety of models, trained on a supervised image captioning\ncorpus to predict the image features for a given caption, to perform sentence\nrepresentation grounding. We train a grounded sentence encoder that achieves\ngood performance on COCO caption and image retrieval and subsequently show that\nthis encoder can successfully be transferred to various NLP tasks, with\nimproved performance over text-only models. Lastly, we analyze the contribution\nof grounding, and show that word embeddings learned by this system outperform\nnon-grounded ones.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jul 2017 23:12:57 GMT"}, {"version": "v2", "created": "Mon, 4 Jun 2018 20:19:28 GMT"}], "update_date": "2018-06-06", "authors_parsed": [["Kiela", "Douwe", ""], ["Conneau", "Alexis", ""], ["Jabri", "Allan", ""], ["Nickel", "Maximilian", ""]]}, {"id": "1707.06323", "submitter": "Renoh Johnson Chalakkal", "authors": "Renoh Johnson Chalakkal and Waleed Abdulla", "title": "Automatic Segmentation of Retinal Vasculature", "comments": "Published at IEEE International Conference on Acoustics Speech and\n  Signal Processing (ICASSP), 2017", "journal-ref": "IEEE International Conference on Acoustics Speech and Signal\n  Processing (ICASSP), page: 886-890, 2017", "doi": "10.1109/ICASSP.2017.7952283", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Segmentation of retinal vessels from retinal fundus images is the key step in\nthe automatic retinal image analysis. In this paper, we propose a new\nunsupervised automatic method to segment the retinal vessels from retinal\nfundus images. Contrast enhancement and illumination correction are carried out\nthrough a series of image processing steps followed by adaptive histogram\nequalization and anisotropic diffusion filtering. This image is then converted\nto a gray scale using weighted scaling. The vessel edges are enhanced by\nboosting the detail curvelet coefficients. Optic disk pixels are removed before\napplying fuzzy C-mean classification to avoid the misclassification.\nMorphological operations and connected component analysis are applied to obtain\nthe segmented retinal vessels. The performance of the proposed method is\nevaluated using DRIVE database to be able to compare with other state-of-art\nsupervised and unsupervised methods. The overall segmentation accuracy of the\nproposed method is 95.18% which outperforms the other algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jul 2017 23:29:13 GMT"}], "update_date": "2017-07-21", "authors_parsed": [["Chalakkal", "Renoh Johnson", ""], ["Abdulla", "Waleed", ""]]}, {"id": "1707.06330", "submitter": "Yancheng Bai", "authors": "Yancheng Bai and Bernard Ghanem", "title": "Multi-Branch Fully Convolutional Network for Face Detection", "comments": "9 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face detection is a fundamental problem in computer vision. It is still a\nchallenging task in unconstrained conditions due to significant variations in\nscale, pose, expressions, and occlusion. In this paper, we propose a\nmulti-branch fully convolutional network (MB-FCN) for face detection, which\nconsiders both efficiency and effectiveness in the design process. Our MB-FCN\ndetector can deal with faces at all scale ranges with only a single pass\nthrough the backbone network. As such, our MB-FCN model saves computation and\nthus is more efficient, compared to previous methods that make multiple passes.\nFor each branch, the specific skip connections of the convolutional feature\nmaps at different layers are exploited to represent faces in specific scale\nranges. Specifically, small faces can be represented with both shallow\nfine-grained and deep powerful coarse features. With this representation,\nsuperior improvement in performance is registered for the task of detecting\nsmall faces. We test our MB-FCN detector on two public face detection\nbenchmarks, including FDDB and WIDER FACE. Extensive experiments show that our\ndetector outperforms state-of-the-art methods on all these datasets in general\nand by a substantial margin on the most challenging among them (e.g. WIDER FACE\nHard subset). Also, MB-FCN runs at 15 FPS on a GPU for images of size 640 x 480\nwith no assumption on the minimum detectable face size.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jul 2017 00:49:38 GMT"}], "update_date": "2017-07-21", "authors_parsed": [["Bai", "Yancheng", ""], ["Ghanem", "Bernard", ""]]}, {"id": "1707.06335", "submitter": "Hong-Yu Zhou", "authors": "Hong-Yu Zhou, Bin-Bin Gao, Jianxin Wu", "title": "Sunrise or Sunset: Selective Comparison Learning for Subtle Attribute\n  Recognition", "comments": "accepted as a poster in BMVC 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The difficulty of image recognition has gradually increased from general\ncategory recognition to fine-grained recognition and to the recognition of some\nsubtle attributes such as temperature and geolocation. In this paper, we try to\nfocus on the classification between sunrise and sunset and hope to give a hint\nabout how to tell the difference in subtle attributes. Sunrise vs. sunset is a\ndifficult recognition task, which is challenging even for humans. Towards\nunderstanding this new problem, we first collect a new dataset made up of over\none hundred webcams from different places. Since existing algorithmic methods\nhave poor accuracy, we propose a new pairwise learning strategy to learn\nfeatures from selective pairs of images. Experiments show that our approach\nsurpasses baseline methods by a large margin and achieves better results even\ncompared with humans. We also apply our approach to existing subtle attribute\nrecognition problems, such as temperature estimation, and achieve\nstate-of-the-art results.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jul 2017 01:44:08 GMT"}], "update_date": "2017-07-21", "authors_parsed": [["Zhou", "Hong-Yu", ""], ["Gao", "Bin-Bin", ""], ["Wu", "Jianxin", ""]]}, {"id": "1707.06342", "submitter": "Jianxin Wu", "authors": "Jian-Hao Luo and Jianxin Wu and Weiyao Lin", "title": "ThiNet: A Filter Level Pruning Method for Deep Neural Network\n  Compression", "comments": "To appear in ICCV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an efficient and unified framework, namely ThiNet, to\nsimultaneously accelerate and compress CNN models in both training and\ninference stages. We focus on the filter level pruning, i.e., the whole filter\nwould be discarded if it is less important. Our method does not change the\noriginal network structure, thus it can be perfectly supported by any\noff-the-shelf deep learning libraries. We formally establish filter pruning as\nan optimization problem, and reveal that we need to prune filters based on\nstatistics information computed from its next layer, not the current layer,\nwhich differentiates ThiNet from existing methods. Experimental results\ndemonstrate the effectiveness of this strategy, which has advanced the\nstate-of-the-art. We also show the performance of ThiNet on ILSVRC-12\nbenchmark. ThiNet achieves 3.31$\\times$ FLOPs reduction and 16.63$\\times$\ncompression on VGG-16, with only 0.52$\\%$ top-5 accuracy drop. Similar\nexperiments with ResNet-50 reveal that even for a compact network, ThiNet can\nalso reduce more than half of the parameters and FLOPs, at the cost of roughly\n1$\\%$ top-5 accuracy drop. Moreover, the original VGG-16 model can be further\npruned into a very small model with only 5.05MB model size, preserving AlexNet\nlevel accuracy but showing much stronger generalization ability.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jul 2017 02:16:16 GMT"}], "update_date": "2017-07-21", "authors_parsed": [["Luo", "Jian-Hao", ""], ["Wu", "Jianxin", ""], ["Lin", "Weiyao", ""]]}, {"id": "1707.06355", "submitter": "Yunan Ye", "authors": "Yunan Ye, Zhou Zhao, Yimeng Li, Long Chen, Jun Xiao and Yueting Zhuang", "title": "Video Question Answering via Attribute-Augmented Attention Network\n  Learning", "comments": "Accepted for SIGIR 2017", "journal-ref": null, "doi": "10.1145/3077136.3080655", "report-no": null, "categories": "cs.CV cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video Question Answering is a challenging problem in visual information\nretrieval, which provides the answer to the referenced video content according\nto the question. However, the existing visual question answering approaches\nmainly tackle the problem of static image question, which may be ineffectively\nfor video question answering due to the insufficiency of modeling the temporal\ndynamics of video contents. In this paper, we study the problem of video\nquestion answering by modeling its temporal dynamics with frame-level attention\nmechanism. We propose the attribute-augmented attention network learning\nframework that enables the joint frame-level attribute detection and unified\nvideo representation learning for video question answering. We then incorporate\nthe multi-step reasoning process for our proposed attention network to further\nimprove the performance. We construct a large-scale video question answering\ndataset. We conduct the experiments on both multiple-choice and open-ended\nvideo question answering tasks to show the effectiveness of the proposed\nmethod.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jul 2017 03:12:29 GMT"}], "update_date": "2017-07-21", "authors_parsed": [["Ye", "Yunan", ""], ["Zhao", "Zhou", ""], ["Li", "Yimeng", ""], ["Chen", "Long", ""], ["Xiao", "Jun", ""], ["Zhuang", "Yueting", ""]]}, {"id": "1707.06375", "submitter": "Zhaoliang Lun", "authors": "Zhaoliang Lun, Matheus Gadelha, Evangelos Kalogerakis, Subhransu Maji,\n  Rui Wang", "title": "3D Shape Reconstruction from Sketches via Multi-view Convolutional\n  Networks", "comments": "3DV 2017 (oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method for reconstructing 3D shapes from 2D sketches in the form\nof line drawings. Our method takes as input a single sketch, or multiple\nsketches, and outputs a dense point cloud representing a 3D reconstruction of\nthe input sketch(es). The point cloud is then converted into a polygon mesh. At\nthe heart of our method lies a deep, encoder-decoder network. The encoder\nconverts the sketch into a compact representation encoding shape information.\nThe decoder converts this representation into depth and normal maps capturing\nthe underlying surface from several output viewpoints. The multi-view maps are\nthen consolidated into a 3D point cloud by solving an optimization problem that\nfuses depth and normals across all viewpoints. Based on our experiments,\ncompared to other methods, such as volumetric networks, our architecture offers\nseveral advantages, including more faithful reconstruction, higher output\nsurface resolution, better preservation of topology and shape structure.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jul 2017 05:05:26 GMT"}, {"version": "v2", "created": "Fri, 21 Jul 2017 18:38:22 GMT"}, {"version": "v3", "created": "Fri, 29 Sep 2017 05:18:30 GMT"}], "update_date": "2017-10-02", "authors_parsed": [["Lun", "Zhaoliang", ""], ["Gadelha", "Matheus", ""], ["Kalogerakis", "Evangelos", ""], ["Maji", "Subhransu", ""], ["Wang", "Rui", ""]]}, {"id": "1707.06397", "submitter": "Xiu-Shen Wei", "authors": "Xiu-Shen Wei and Chen-Lin Zhang and Jianxin Wu and Chunhua Shen and\n  Zhi-Hua Zhou", "title": "Unsupervised Object Discovery and Co-Localization by Deep Descriptor\n  Transforming", "comments": "This paper is extended based on our preliminary work published in\n  IJCAI 2017 [arXiv:1705.02758]", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reusable model design becomes desirable with the rapid expansion of computer\nvision and machine learning applications. In this paper, we focus on the\nreusability of pre-trained deep convolutional models. Specifically, different\nfrom treating pre-trained models as feature extractors, we reveal more\ntreasures beneath convolutional layers, i.e., the convolutional activations\ncould act as a detector for the common object in the image co-localization\nproblem. We propose a simple yet effective method, termed Deep Descriptor\nTransforming (DDT), for evaluating the correlations of descriptors and then\nobtaining the category-consistent regions, which can accurately locate the\ncommon object in a set of unlabeled images, i.e., unsupervised object\ndiscovery. Empirical studies validate the effectiveness of the proposed DDT\nmethod. On benchmark image co-localization datasets, DDT consistently\noutperforms existing state-of-the-art methods by a large margin. Moreover, DDT\nalso demonstrates good generalization ability for unseen categories and\nrobustness for dealing with noisy data. Beyond those, DDT can be also employed\nfor harvesting web images into valid external data sources for improving\nperformance of both image recognition and object detection.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jul 2017 07:07:33 GMT"}], "update_date": "2017-07-21", "authors_parsed": [["Wei", "Xiu-Shen", ""], ["Zhang", "Chen-Lin", ""], ["Wu", "Jianxin", ""], ["Shen", "Chunhua", ""], ["Zhou", "Zhi-Hua", ""]]}, {"id": "1707.06399", "submitter": "Jianxin Wu", "authors": "Hong-Yu Zhou and Bin-Bin Gao and Jianxin Wu", "title": "Adaptive Feeding: Achieving Fast and Accurate Detections by Adaptively\n  Combining Object Detectors", "comments": "To appear in ICCV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detection aims at high speed and accuracy simultaneously. However,\nfast models are usually less accurate, while accurate models cannot satisfy our\nneed for speed. A fast model can be 10 times faster but 50\\% less accurate than\nan accurate model. In this paper, we propose Adaptive Feeding (AF) to combine a\nfast (but less accurate) detector and an accurate (but slow) detector, by\nadaptively determining whether an image is easy or hard and choosing an\nappropriate detector for it. In practice, we build a cascade of detectors,\nincluding the AF classifier which make the easy vs. hard decision and the two\ndetectors. The AF classifier can be tuned to obtain different tradeoff between\nspeed and accuracy, which has negligible training time and requires no\nadditional training data. Experimental results on the PASCAL VOC, MS COCO and\nCaltech Pedestrian datasets confirm that AF has the ability to achieve\ncomparable speed as the fast detector and comparable accuracy as the accurate\none at the same time. As an example, by combining the fast SSD300 with the\naccurate SSD500 detector, AF leads to 50\\% speedup over SSD500 with the same\nprecision on the VOC2007 test set.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jul 2017 07:22:01 GMT"}], "update_date": "2017-07-21", "authors_parsed": [["Zhou", "Hong-Yu", ""], ["Gao", "Bin-Bin", ""], ["Wu", "Jianxin", ""]]}, {"id": "1707.06426", "submitter": "Qin Huang", "authors": "Qin Huang, Chunyang Xia, Chihao Wu, Siyang Li, Ye Wang, Yuhang Song,\n  C.-C. Jay Kuo", "title": "Semantic Segmentation with Reverse Attention", "comments": "accepted for oral presentation in BMVC 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent development in fully convolutional neural network enables efficient\nend-to-end learning of semantic segmentation. Traditionally, the convolutional\nclassifiers are taught to learn the representative semantic features of labeled\nsemantic objects. In this work, we propose a reverse attention network (RAN)\narchitecture that trains the network to capture the opposite concept (i.e.,\nwhat are not associated with a target class) as well. The RAN is a three-branch\nnetwork that performs the direct, reverse and reverse-attention learning\nprocesses simultaneously. Extensive experiments are conducted to show the\neffectiveness of the RAN in semantic segmentation. Being built upon the\nDeepLabv2-LargeFOV, the RAN achieves the state-of-the-art mIoU score (48.1%)\nfor the challenging PASCAL-Context dataset. Significant performance\nimprovements are also observed for the PASCAL-VOC, Person-Part, NYUDv2 and\nADE20K datasets.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jul 2017 09:45:08 GMT"}], "update_date": "2017-07-21", "authors_parsed": [["Huang", "Qin", ""], ["Xia", "Chunyang", ""], ["Wu", "Chihao", ""], ["Li", "Siyang", ""], ["Wang", "Ye", ""], ["Song", "Yuhang", ""], ["Kuo", "C. -C. Jay", ""]]}, {"id": "1707.06427", "submitter": "Gottfried Munda", "authors": "Gottfried Munda, Alexander Shekhovtsov, Patrick Kn\\\"obelreiter, Thomas\n  Pock", "title": "Scalable Full Flow with Learned Binary Descriptors", "comments": "GCPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method for large displacement optical flow in which local\nmatching costs are learned by a convolutional neural network (CNN) and a\nsmoothness prior is imposed by a conditional random field (CRF). We tackle the\ncomputation- and memory-intensive operations on the 4D cost volume by a\nmin-projection which reduces memory complexity from quadratic to linear and\nbinary descriptors for efficient matching. This enables evaluation of the cost\non the fly and allows to perform learning and CRF inference on high resolution\nimages without ever storing the 4D cost volume. To address the problem of\nlearning binary descriptors we propose a new hybrid learning scheme. In\ncontrast to current state of the art approaches for learning binary CNNs we can\ncompute the exact non-zero gradient within our model. We compare several\nmethods for training binary descriptors and show results on public available\nbenchmarks.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jul 2017 09:49:29 GMT"}], "update_date": "2017-07-21", "authors_parsed": [["Munda", "Gottfried", ""], ["Shekhovtsov", "Alexander", ""], ["Kn\u00f6belreiter", "Patrick", ""], ["Pock", "Thomas", ""]]}, {"id": "1707.06436", "submitter": "Hirokatsu Kataoka", "authors": "Hirokatsu Kataoka, Soma Shirakabe, Yun He, Shunya Ueta, Teppei Suzuki,\n  Kaori Abe, Asako Kanezaki, Shin'ichiro Morita, Toshiyuki Yabe, Yoshihiro\n  Kanehara, Hiroya Yatsuyanagi, Shinya Maruyama, Ryosuke Takasawa, Masataka\n  Fuchida, Yudai Miyashita, Kazushige Okayasu, Yuta Matsuzaki", "title": "cvpaper.challenge in 2016: Futuristic Computer Vision through 1,600\n  Papers Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper gives futuristic challenges disscussed in the cvpaper.challenge. In\n2015 and 2016, we thoroughly study 1,600+ papers in several\nconferences/journals such as CVPR/ICCV/ECCV/NIPS/PAMI/IJCV.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jul 2017 10:06:07 GMT"}], "update_date": "2017-07-21", "authors_parsed": [["Kataoka", "Hirokatsu", ""], ["Shirakabe", "Soma", ""], ["He", "Yun", ""], ["Ueta", "Shunya", ""], ["Suzuki", "Teppei", ""], ["Abe", "Kaori", ""], ["Kanezaki", "Asako", ""], ["Morita", "Shin'ichiro", ""], ["Yabe", "Toshiyuki", ""], ["Kanehara", "Yoshihiro", ""], ["Yatsuyanagi", "Hiroya", ""], ["Maruyama", "Shinya", ""], ["Takasawa", "Ryosuke", ""], ["Fuchida", "Masataka", ""], ["Miyashita", "Yudai", ""], ["Okayasu", "Kazushige", ""], ["Matsuzaki", "Yuta", ""]]}, {"id": "1707.06440", "submitter": "Mohammed Daoudi", "authors": "Anis Kacem, Mohamed Daoudi, Boulbaba Ben Amor, Juan Carlos\n  Alvarez-Paiva", "title": "A Novel Space-Time Representation on the Positive Semidefinite Con for\n  Facial Expression Recognition", "comments": "To be appeared at ICCV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the problem of facial expression recognition using a\nnovel space-time geometric representation. We describe the temporal evolution\nof facial landmarks as parametrized trajectories on the Riemannian manifold of\npositive semidefinite matrices of fixed-rank. Our representation has the\nadvantage to bring naturally a second desirable quantity when comparing shapes\n-- the spatial covariance -- in addition to the conventional affine-shape\nrepresentation. We derive then geometric and computational tools for\nrate-invariant analysis and adaptive re-sampling of trajectories, grounding on\nthe Riemannian geometry of the manifold. Specifically, our approach involves\nthree steps: 1) facial landmarks are first mapped into the Riemannian manifold\nof positive semidefinite matrices of rank 2, to build time-parameterized\ntrajectories; 2) a temporal alignment is performed on the trajectories,\nproviding a geometry-aware (dis-)similarity measure between them; 3) finally,\npairwise proximity function SVM (ppfSVM) is used to classify them,\nincorporating the latter (dis-)similarity measure into the kernel function. We\nshow the effectiveness of the proposed approach on four publicly available\nbenchmarks (CK+, MMI, Oulu-CASIA, and AFEW). The results of the proposed\napproach are comparable to or better than the state-of-the-art methods when\ninvolving only facial landmarks.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jul 2017 10:36:12 GMT"}], "update_date": "2017-07-21", "authors_parsed": [["Kacem", "Anis", ""], ["Daoudi", "Mohamed", ""], ["Amor", "Boulbaba Ben", ""], ["Alvarez-Paiva", "Juan Carlos", ""]]}, {"id": "1707.06474", "submitter": "Jonas Adler", "authors": "Jonas Adler and Ozan \\\"Oktem", "title": "Learned Primal-dual Reconstruction", "comments": "11 pages, 5 figures", "journal-ref": "IEEE Transactions on Medical Imaging (2018)", "doi": "10.1109/TMI.2018.2799231", "report-no": null, "categories": "math.OC cs.CV cs.NE math.FA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose the Learned Primal-Dual algorithm for tomographic reconstruction.\nThe algorithm accounts for a (possibly non-linear) forward operator in a deep\nneural network by unrolling a proximal primal-dual optimization method, but\nwhere the proximal operators have been replaced with convolutional neural\nnetworks. The algorithm is trained end-to-end, working directly from raw\nmeasured data and it does not depend on any initial reconstruction such as FBP.\n  We compare performance of the proposed method on low dose CT reconstruction\nagainst FBP, TV, and deep learning based post-processing of FBP. For the\nShepp-Logan phantom we obtain >6dB PSNR improvement against all compared\nmethods. For human phantoms the corresponding improvement is 6.6dB over TV and\n2.2dB over learned post-processing along with a substantial improvement in the\nSSIM. Finally, our algorithm involves only ten forward-back-projection\ncomputations, making the method feasible for time critical clinical\napplications.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jul 2017 12:34:51 GMT"}, {"version": "v2", "created": "Fri, 24 Nov 2017 12:39:52 GMT"}, {"version": "v3", "created": "Thu, 5 Jul 2018 15:34:04 GMT"}], "update_date": "2018-07-06", "authors_parsed": [["Adler", "Jonas", ""], ["\u00d6ktem", "Ozan", ""]]}, {"id": "1707.06484", "submitter": "Fisher Yu", "authors": "Fisher Yu, Dequan Wang, Evan Shelhamer, Trevor Darrell", "title": "Deep Layer Aggregation", "comments": "Published at the Conference on Computer Vision and Pattern\n  Recognition (CVPR) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual recognition requires rich representations that span levels from low to\nhigh, scales from small to large, and resolutions from fine to coarse. Even\nwith the depth of features in a convolutional network, a layer in isolation is\nnot enough: compounding and aggregating these representations improves\ninference of what and where. Architectural efforts are exploring many\ndimensions for network backbones, designing deeper or wider architectures, but\nhow to best aggregate layers and blocks across a network deserves further\nattention. Although skip connections have been incorporated to combine layers,\nthese connections have been \"shallow\" themselves, and only fuse by simple,\none-step operations. We augment standard architectures with deeper aggregation\nto better fuse information across layers. Our deep layer aggregation structures\niteratively and hierarchically merge the feature hierarchy to make networks\nwith better accuracy and fewer parameters. Experiments across architectures and\ntasks show that deep layer aggregation improves recognition and resolution\ncompared to existing branching and merging schemes. The code is at\nhttps://github.com/ucbdrive/dla.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jul 2017 12:59:08 GMT"}, {"version": "v2", "created": "Thu, 4 Jan 2018 05:45:30 GMT"}, {"version": "v3", "created": "Fri, 4 Jan 2019 09:26:55 GMT"}], "update_date": "2019-01-07", "authors_parsed": [["Yu", "Fisher", ""], ["Wang", "Dequan", ""], ["Shelhamer", "Evan", ""], ["Darrell", "Trevor", ""]]}, {"id": "1707.06543", "submitter": "Boyi Li", "authors": "Boyi Li and Xiulian Peng and Zhangyang Wang and Jizheng Xu and Dan\n  Feng", "title": "An All-in-One Network for Dehazing and Beyond", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes an image dehazing model built with a convolutional neural\nnetwork (CNN), called All-in-One Dehazing Network (AOD-Net). It is designed\nbased on a re-formulated atmospheric scattering model. Instead of estimating\nthe transmission matrix and the atmospheric light separately as most previous\nmodels did, AOD-Net directly generates the clean image through a light-weight\nCNN. Such a novel end-to-end design makes it easy to embed AOD-Net into other\ndeep models, e.g., Faster R-CNN, for improving high-level task performance on\nhazy images. Experimental results on both synthesized and natural hazy image\ndatasets demonstrate our superior performance than the state-of-the-art in\nterms of PSNR, SSIM and the subjective visual quality. Furthermore, when\nconcatenating AOD-Net with Faster R-CNN and training the joint pipeline from\nend to end, we witness a large improvement of the object detection performance\non hazy images.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jul 2017 14:30:35 GMT"}], "update_date": "2017-07-21", "authors_parsed": [["Li", "Boyi", ""], ["Peng", "Xiulian", ""], ["Wang", "Zhangyang", ""], ["Xu", "Jizheng", ""], ["Feng", "Dan", ""]]}, {"id": "1707.06545", "submitter": "Eddie Smolyansky", "authors": "Gilad Sharir, Eddie Smolyansky, Itamar Friedman", "title": "Video Object Segmentation using Tracked Object Proposals", "comments": "All authors contributed equally, CVPR-2017 workshop, DAVIS-2017\n  Challenge", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach to semi-supervised video object segmentation, in the\ncontext of the DAVIS 2017 challenge. Our approach combines category-based\nobject detection, category-independent object appearance segmentation and\ntemporal object tracking. We are motivated by the fact that the objects\nsemantic category tends not to change throughout the video while its appearance\nand location can vary considerably. In order to capture the specific object\nappearance independent of its category, for each video we train a fully\nconvolutional network using augmentations of the given annotated frame. We\nrefine the appearance segmentation mask with the bounding boxes provided either\nby a semantic object detection network, when applicable, or by a previous frame\nprediction. By introducing a temporal continuity constraint on the detected\nboxes, we are able to improve the object segmentation mask of the appearance\nnetwork and achieve competitive results on the DAVIS datasets.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jul 2017 14:31:12 GMT"}], "update_date": "2017-07-21", "authors_parsed": [["Sharir", "Gilad", ""], ["Smolyansky", "Eddie", ""], ["Friedman", "Itamar", ""]]}, {"id": "1707.06557", "submitter": "Konstantinos Amplianitis", "authors": "Dominik Rue{\\ss}, Konstantinos Amplianitis, Niklas Deckers, Michele\n  Adduci, Kristian Manthey, Ralf Reulke", "title": "leave a trace - A People Tracking System Meets Anomaly Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Video surveillance always had a negative connotation, among others because of\nthe loss of privacy and because it may not automatically increase public\nsafety. If it was able to detect atypical (i.e. dangerous) situations in real\ntime, autonomously and anonymously, this could change. A prerequisite for this\nis a reliable automatic detection of possibly dangerous situations from video\ndata. This is done classically by object extraction and tracking. From the\nderived trajectories, we then want to determine dangerous situations by\ndetecting atypical trajectories. However, due to ethical considerations it is\nbetter to develop such a system on data without people being threatened or even\nharmed, plus with having them know that there is such a tracking system\ninstalled. Another important point is that these situations do not occur very\noften in real, public CCTV areas and may be captured properly even less. In the\nartistic project leave a trace the tracked objects, people in an atrium of a\ninstitutional building, become actor and thus part of the installation.\nVisualisation in real-time allows interaction by these actors, which in turn\ncreates many atypical interaction situations on which we can develop our\nsituation detection. The data set has evolved over three years and hence, is\nhuge. In this article we describe the tracking system and several approaches\nfor the detection of atypical trajectories.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jul 2017 15:03:11 GMT"}], "update_date": "2017-07-21", "authors_parsed": [["Rue\u00df", "Dominik", ""], ["Amplianitis", "Konstantinos", ""], ["Deckers", "Niklas", ""], ["Adduci", "Michele", ""], ["Manthey", "Kristian", ""], ["Reulke", "Ralf", ""]]}, {"id": "1707.06563", "submitter": "Andre Wagner", "authors": "Andr\\'e Wagner", "title": "Pictures of Combinatorial Cubes", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove that the 8-point algorithm always fails to reconstruct a unique\nfundamental matrix $F$ independent on the camera positions, when its input are\nimage point configurations that are perspective projections of the vertices of\na combinatorial cube in $\\mathbb{R}^3$. We give an algorithm that improves the\n7- and 8-point algorithm in such a pathological situation. Additionally we\nanalyze the regions of focal point positions where a reconstruction of $F$ is\npossible at all, when the world points are the vertices of a combinatorial cube\nin $\\mathbb{R}^3$.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jul 2017 15:17:08 GMT"}], "update_date": "2017-07-21", "authors_parsed": [["Wagner", "Andr\u00e9", ""]]}, {"id": "1707.06633", "submitter": "Martin V\\\"olker", "authors": "Felix Burget, Lukas Dominique Josef Fiederer, Daniel Kuhner, Martin\n  V\\\"olker, Johannes Aldinger, Robin Tibor Schirrmeister, Chau Do, Joschka\n  Boedecker, Bernhard Nebel, Tonio Ball, Wolfram Burgard", "title": "Acting Thoughts: Towards a Mobile Robotic Service Assistant for Users\n  with Limited Communication Skills", "comments": "* FB, LDJF, DK, MV and JA contributed equally to the work. Accepted\n  as a conference paper at the European Conference on Mobile Robotics 2017\n  (ECMR 2017), 6 pages, 3 figures", "journal-ref": "2017 European Conference on Mobile Robots (ECMR)", "doi": "10.1109/ECMR.2017.8098658", "report-no": null, "categories": "cs.AI cs.CV cs.HC cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As autonomous service robots become more affordable and thus available also\nfor the general public, there is a growing need for user friendly interfaces to\ncontrol the robotic system. Currently available control modalities typically\nexpect users to be able to express their desire through either touch, speech or\ngesture commands. While this requirement is fulfilled for the majority of\nusers, paralyzed users may not be able to use such systems. In this paper, we\npresent a novel framework, that allows these users to interact with a robotic\nservice assistant in a closed-loop fashion, using only thoughts. The\nbrain-computer interface (BCI) system is composed of several interacting\ncomponents, i.e., non-invasive neuronal signal recording and decoding,\nhigh-level task planning, motion and manipulation planning as well as\nenvironment perception. In various experiments, we demonstrate its\napplicability and robustness in real world scenarios, considering\nfetch-and-carry tasks and tasks involving human-robot interaction. As our\nresults demonstrate, our system is capable of adapting to frequent changes in\nthe environment and reliably completing given tasks within a reasonable amount\nof time. Combined with high-level planning and autonomous robotic systems,\ninteresting new perspectives open up for non-invasive BCI-based human-robot\ninteractions.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jul 2017 17:51:12 GMT"}, {"version": "v2", "created": "Fri, 21 Jul 2017 06:30:43 GMT"}, {"version": "v3", "created": "Thu, 30 Nov 2017 08:25:20 GMT"}, {"version": "v4", "created": "Tue, 12 Jun 2018 14:52:41 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Burget", "Felix", ""], ["Fiederer", "Lukas Dominique Josef", ""], ["Kuhner", "Daniel", ""], ["V\u00f6lker", "Martin", ""], ["Aldinger", "Johannes", ""], ["Schirrmeister", "Robin Tibor", ""], ["Do", "Chau", ""], ["Boedecker", "Joschka", ""], ["Nebel", "Bernhard", ""], ["Ball", "Tonio", ""], ["Burgard", "Wolfram", ""]]}, {"id": "1707.06642", "submitter": "Oisin Mac Aodha", "authors": "Grant Van Horn, Oisin Mac Aodha, Yang Song, Yin Cui, Chen Sun, Alex\n  Shepard, Hartwig Adam, Pietro Perona, Serge Belongie", "title": "The iNaturalist Species Classification and Detection Dataset", "comments": "CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing image classification datasets used in computer vision tend to have a\nuniform distribution of images across object categories. In contrast, the\nnatural world is heavily imbalanced, as some species are more abundant and\neasier to photograph than others. To encourage further progress in challenging\nreal world conditions we present the iNaturalist species classification and\ndetection dataset, consisting of 859,000 images from over 5,000 different\nspecies of plants and animals. It features visually similar species, captured\nin a wide variety of situations, from all over the world. Images were collected\nwith different camera types, have varying image quality, feature a large class\nimbalance, and have been verified by multiple citizen scientists. We discuss\nthe collection of the dataset and present extensive baseline experiments using\nstate-of-the-art computer vision classification and detection models. Results\nshow that current non-ensemble based methods achieve only 67% top one\nclassification accuracy, illustrating the difficulty of the dataset.\nSpecifically, we observe poor results for classes with small numbers of\ntraining examples suggesting more attention is needed in low-shot learning.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jul 2017 17:59:55 GMT"}, {"version": "v2", "created": "Tue, 10 Apr 2018 20:22:13 GMT"}], "update_date": "2018-04-12", "authors_parsed": [["Van Horn", "Grant", ""], ["Mac Aodha", "Oisin", ""], ["Song", "Yang", ""], ["Cui", "Yin", ""], ["Sun", "Chen", ""], ["Shepard", "Alex", ""], ["Adam", "Hartwig", ""], ["Perona", "Pietro", ""], ["Belongie", "Serge", ""]]}, {"id": "1707.06682", "submitter": "Regina Meszl\\'enyi", "authors": "Regina Meszl\\'enyi, Krisztian Buza and Zolt\\'an Vidny\\'anszky", "title": "Resting state fMRI functional connectivity-based classification using a\n  convolutional neural network architecture", "comments": "25 pages, 4 figures, 1 table, plus supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning techniques have become increasingly popular in the field of\nresting state fMRI (functional magnetic resonance imaging) network based\nclassification. However, the application of convolutional networks has been\nproposed only very recently and has remained largely unexplored. In this paper\nwe describe a convolutional neural network architecture for functional\nconnectome classification called connectome-convolutional neural network\n(CCNN). Our results on simulated datasets and a publicly available dataset for\namnestic mild cognitive impairment classification demonstrate that our CCNN\nmodel can efficiently distinguish between subject groups. We also show that the\nconnectome-convolutional network is capable to combine information from diverse\nfunctional connectivity metrics and that models using a combination of\ndifferent connectivity descriptors are able to outperform classifiers using\nonly one metric. From this flexibility follows that our proposed CCNN model can\nbe easily adapted to a wide range of connectome based classification or\nregression tasks, by varying which connectivity descriptor combinations are\nused to train the network.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jul 2017 19:12:58 GMT"}], "update_date": "2017-07-24", "authors_parsed": [["Meszl\u00e9nyi", "Regina", ""], ["Buza", "Krisztian", ""], ["Vidny\u00e1nszky", "Zolt\u00e1n", ""]]}, {"id": "1707.06699", "submitter": "Somenath Das", "authors": "Somenath Das, Suchendra M. Bhandarkar", "title": "Local Geometry Inclusive Global Shape Representation", "comments": "11 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge of shape geometry plays a pivotal role in many shape analysis\napplications. In this paper we introduce a local geometry-inclusive global\nrepresentation of 3D shapes based on computation of the shortest quasi-geodesic\npaths between all possible pairs of points on the 3D shape manifold. In the\nproposed representation, the normal curvature along the quasi-geodesic paths\nbetween any two points on the shape surface is preserved. We employ the\neigenspectrum of the proposed global representation to address the problems of\ndetermination of region-based correspondence between isometric shapes and\ncharacterization of self-symmetry in the absence of prior knowledge in the form\nof user-defined correspondence maps. We further utilize the commutative\nproperty of the resulting shape descriptor to extract stable regions between\nisometric shapes that differ from one another by a high degree of isometry\ntransformation. We also propose various shape characterization metrics in terms\nof the eigenvector decomposition of the shape descriptor spectrum to quantify\nthe correspondence and self-symmetry of 3D shapes. The performance of the\nproposed 3D shape descriptor is experimentally compared with the performance of\nother relevant state-of-the-art 3D shape descriptors.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jul 2017 20:32:11 GMT"}], "update_date": "2017-07-24", "authors_parsed": [["Das", "Somenath", ""], ["Bhandarkar", "Suchendra M.", ""]]}, {"id": "1707.06718", "submitter": "Brendt Wohlberg", "authors": "Brendt Wohlberg and Paul Rodriguez", "title": "Convolutional Sparse Coding: Boundary Handling Revisited", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two different approaches have recently been proposed for boundary handling in\nconvolutional sparse representations, avoiding potential boundary artifacts\narising from the circular boundary conditions implied by the use of frequency\ndomain solution methods by introducing a spatial mask into the convolutional\nsparse coding problem. In the present paper we show that, under certain\ncircumstances, these methods fail in their design goal of avoiding boundary\nartifacts. The reasons for this failure are discussed, a solution is proposed,\nand the practical implications are illustrated in an image deblurring problem.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jul 2017 23:10:02 GMT"}], "update_date": "2017-11-09", "authors_parsed": [["Wohlberg", "Brendt", ""], ["Rodriguez", "Paul", ""]]}, {"id": "1707.06719", "submitter": "Aleksandr Savchenkov", "authors": "Aleksandr Savchenkov, Andrew Davis, Xuan Zhao", "title": "Generalized Convolutional Neural Networks for Point Cloud Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The introduction of cheap RGB-D cameras, stereo cameras, and LIDAR devices\nhas given the computer vision community 3D information that conventional RGB\ncameras cannot provide. This data is often stored as a point cloud. In this\npaper, we present a novel method to apply the concept of convolutional neural\nnetworks to this type of data. By creating a mapping of nearest neighbors in a\ndataset, and individually applying weights to spatial relationships between\npoints, we achieve an architecture that works directly with point clouds, but\nclosely resembles a convolutional neural net in both design and behavior. Such\na method bypasses the need for extensive feature engineering, while proving to\nbe computationally efficient and requiring few parameters.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jul 2017 23:12:11 GMT"}, {"version": "v2", "created": "Fri, 19 Oct 2018 02:09:03 GMT"}], "update_date": "2018-10-22", "authors_parsed": [["Savchenkov", "Aleksandr", ""], ["Davis", "Andrew", ""], ["Zhao", "Xuan", ""]]}, {"id": "1707.06750", "submitter": "Tianwei Lin", "authors": "Tianwei Lin, Xu Zhao, Zheng Shou", "title": "Temporal Convolution Based Action Proposal: Submission to ActivityNet\n  2017", "comments": "4 pages, Presented at ActivityNet Large Scale Activity Recognition\n  Challenge workshop at CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this notebook paper, we describe our approach in the submission to the\ntemporal action proposal (task 3) and temporal action localization (task 4) of\nActivityNet Challenge hosted at CVPR 2017. Since the accuracy in action\nclassification task is already very high (nearly 90% in ActivityNet dataset),\nwe believe that the main bottleneck for temporal action localization is the\nquality of action proposals. Therefore, we mainly focus on the temporal action\nproposal task and propose a new proposal model based on temporal convolutional\nnetwork. Our approach achieves the state-of-the-art performances on both\ntemporal action proposal task and temporal action localization task.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jul 2017 03:30:00 GMT"}, {"version": "v2", "created": "Mon, 24 Jul 2017 14:05:59 GMT"}, {"version": "v3", "created": "Wed, 26 Sep 2018 10:54:24 GMT"}], "update_date": "2018-09-27", "authors_parsed": [["Lin", "Tianwei", ""], ["Zhao", "Xu", ""], ["Shou", "Zheng", ""]]}, {"id": "1707.06757", "submitter": "Kelum Gajamannage", "authors": "Kelum Gajamannage, Randy Paffenroth, Erik M. Bollt", "title": "A Nonlinear Dimensionality Reduction Framework Using Smooth Geodesics", "comments": "13 pages, 7 figures, submitted to Pattern Recognition", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG math.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing dimensionality reduction methods are adept at revealing hidden\nunderlying manifolds arising from high-dimensional data and thereby producing a\nlow-dimensional representation. However, the smoothness of the manifolds\nproduced by classic techniques over sparse and noisy data is not guaranteed. In\nfact, the embedding generated using such data may distort the geometry of the\nmanifold and thereby produce an unfaithful embedding. Herein, we propose a\nframework for nonlinear dimensionality reduction that generates a manifold in\nterms of smooth geodesics that is designed to treat problems in which manifold\nmeasurements are either sparse or corrupted by noise. Our method generates a\nnetwork structure for given high-dimensional data using a nearest neighbors\nsearch and then produces piecewise linear shortest paths that are defined as\ngeodesics. Then, we fit points in each geodesic by a smoothing spline to\nemphasize the smoothness. The robustness of this approach for sparse and noisy\ndatasets is demonstrated by the implementation of the method on synthetic and\nreal-world datasets.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jul 2017 05:04:07 GMT"}, {"version": "v2", "created": "Fri, 13 Jul 2018 17:38:33 GMT"}], "update_date": "2018-07-16", "authors_parsed": [["Gajamannage", "Kelum", ""], ["Paffenroth", "Randy", ""], ["Bollt", "Erik M.", ""]]}, {"id": "1707.06772", "submitter": "Tsung-Yu Lin", "authors": "Tsung-Yu Lin, Subhransu Maji", "title": "Improved Bilinear Pooling with CNNs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bilinear pooling of Convolutional Neural Network (CNN) features [22, 23], and\ntheir compact variants [10], have been shown to be effective at fine-grained\nrecognition, scene categorization, texture recognition, and visual\nquestion-answering tasks among others. The resulting representation captures\nsecond-order statistics of convolutional features in a translationally\ninvariant manner. In this paper we investigate various ways of normalizing\nthese statistics to improve their representation power. In particular we find\nthat the matrix square-root normalization offers significant improvements and\noutperforms alternative schemes such as the matrix logarithm normalization when\ncombined with elementwise square-root and l2 normalization. This improves the\naccuracy by 2-3% on a range of fine-grained recognition datasets leading to a\nnew state of the art. We also investigate how the accuracy of matrix function\ncomputations effect network training and evaluation. In particular we compare\nagainst a technique for estimating matrix square-root gradients via solving a\nLyapunov equation that is more numerically accurate than computing gradients\nvia a Singular Value Decomposition (SVD). We find that while SVD gradients are\nnumerically inaccurate the overall effect on the final accuracy is negligible\nonce boundary cases are handled carefully. We present an alternative scheme for\ncomputing gradients that is faster and yet it offers improvements over the\nbaseline model. Finally we show that the matrix square-root computed\napproximately using a few Newton iterations is just as accurate for the\nclassification task but allows an order-of-magnitude faster GPU implementation\ncompared to SVD decomposition.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jul 2017 06:49:04 GMT"}], "update_date": "2017-07-24", "authors_parsed": [["Lin", "Tsung-Yu", ""], ["Maji", "Subhransu", ""]]}, {"id": "1707.06777", "submitter": "Hao Liu", "authors": "Hao Liu, Jiashi Feng, Zequn Jie, Karlekar Jayashree, Bo Zhao, Meibin\n  Qi, Jianguo Jiang, Shuicheng Yan", "title": "Neural Person Search Machines", "comments": "ICCV2017 camera ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the problem of person search in the wild in this work. Instead\nof comparing the query against all candidate regions generated in a query-blind\nmanner, we propose to recursively shrink the search area from the whole image\ntill achieving precise localization of the target person, by fully exploiting\ninformation from the query and contextual cues in every recursive search step.\nWe develop the Neural Person Search Machines (NPSM) to implement such recursive\nlocalization for person search. Benefiting from its neural search mechanism,\nNPSM is able to selectively shrink its focus from a loose region to a tighter\none containing the target automatically. In this process, NPSM employs an\ninternal primitive memory component to memorize the query representation which\nmodulates the attention and augments its robustness to other distracting\nregions. Evaluations on two benchmark datasets, CUHK-SYSU Person Search dataset\nand PRW dataset, have demonstrated that our method can outperform current\nstate-of-the-arts in both mAP and top-1 evaluation protocols.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jul 2017 07:11:51 GMT"}], "update_date": "2017-07-24", "authors_parsed": [["Liu", "Hao", ""], ["Feng", "Jiashi", ""], ["Jie", "Zequn", ""], ["Jayashree", "Karlekar", ""], ["Zhao", "Bo", ""], ["Qi", "Meibin", ""], ["Jiang", "Jianguo", ""], ["Yan", "Shuicheng", ""]]}, {"id": "1707.06783", "submitter": "Liqiang Zhang", "authors": "Fangyu Liu, Shuaipeng Li, Liqiang Zhang, Chenghu Zhou, Rongtian Ye,\n  Yuebin Wang, Jiwen Lu", "title": "3DCNN-DQN-RNN: A Deep Reinforcement Learning Framework for Semantic\n  Parsing of Large-scale 3D Point Clouds", "comments": "IEEE International Conference on Computer Vision (ICCV) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic parsing of large-scale 3D point clouds is an important research\ntopic in computer vision and remote sensing fields. Most existing approaches\nutilize hand-crafted features for each modality independently and combine them\nin a heuristic manner. They often fail to consider the consistency and\ncomplementary information among features adequately, which makes them difficult\nto capture high-level semantic structures. The features learned by most of the\ncurrent deep learning methods can obtain high-quality image classification\nresults. However, these methods are hard to be applied to recognize 3D point\nclouds due to unorganized distribution and various point density of data. In\nthis paper, we propose a 3DCNN-DQN-RNN method which fuses the 3D convolutional\nneural network (CNN), Deep Q-Network (DQN) and Residual recurrent neural\nnetwork (RNN) for an efficient semantic parsing of large-scale 3D point clouds.\nIn our method, an eye window under control of the 3D CNN and DQN can localize\nand segment the points of the object class efficiently. The 3D CNN and Residual\nRNN further extract robust and discriminative features of the points in the eye\nwindow, and thus greatly enhance the parsing accuracy of large-scale point\nclouds. Our method provides an automatic process that maps the raw data to the\nclassification results. It also integrates object localization, segmentation\nand classification into one framework. Experimental results demonstrate that\nthe proposed method outperforms the state-of-the-art point cloud classification\nmethods.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jul 2017 07:28:14 GMT"}], "update_date": "2017-07-24", "authors_parsed": [["Liu", "Fangyu", ""], ["Li", "Shuaipeng", ""], ["Zhang", "Liqiang", ""], ["Zhou", "Chenghu", ""], ["Ye", "Rongtian", ""], ["Wang", "Yuebin", ""], ["Lu", "Jiwen", ""]]}, {"id": "1707.06786", "submitter": "Guido Borghi", "authors": "Diego Ballotta, Guido Borghi, Roberto Vezzani, Rita Cucchiara", "title": "Head Detection with Depth Images in the Wild", "comments": "Accepted as full paper (oral) at VISAPP 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Head detection and localization is a demanding task and a key element for\nmany computer vision applications, like video surveillance, Human Computer\nInteraction and face analysis. The stunning amount of work done for detecting\nfaces on RGB images, together with the availability of huge face datasets,\nallowed to setup very effective systems on that domain. However, due to\nillumination issues, infrared or depth cameras may be required in real\napplications. In this paper, we introduce a novel method for head detection on\ndepth images that exploits the classification ability of deep learning\napproaches. In addition to reduce the dependency on the external illumination,\ndepth images implicitly embed useful information to deal with the scale of the\ntarget objects. Two public datasets have been exploited: the first one, called\nPandora, is used to train a deep binary classifier with face and non-face\nimages. The second one, collected by Cornell University, is used to perform a\ncross-dataset test during daily activities in unconstrained environments.\nExperimental results show that the proposed method overcomes the performance of\nstate-of-art methods working on depth images.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jul 2017 07:35:21 GMT"}, {"version": "v2", "created": "Wed, 8 Nov 2017 18:10:39 GMT"}], "update_date": "2017-11-09", "authors_parsed": [["Ballotta", "Diego", ""], ["Borghi", "Guido", ""], ["Vezzani", "Roberto", ""], ["Cucchiara", "Rita", ""]]}, {"id": "1707.06807", "submitter": "Tomasz Trzcinski", "authors": "Tomasz Trzcinski, Pawel Andruszkiewicz, Tomasz Bochenski, Przemyslaw\n  Rokita", "title": "Recurrent Neural Networks for Online Video Popularity Prediction", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-319-60438-1_15", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the problem of popularity prediction of online\nvideos shared in social media. We prove that this challenging task can be\napproached using recently proposed deep neural network architectures. We cast\nthe popularity prediction problem as a classification task and we aim to solve\nit using only visual cues extracted from videos. To that end, we propose a new\nmethod based on a Long-term Recurrent Convolutional Network (LRCN) that\nincorporates the sequentiality of the information in the model. Results\nobtained on a dataset of over 37'000 videos published on Facebook show that\nusing our method leads to over 30% improvement in prediction performance over\nthe traditional shallow approaches and can provide valuable insights for\ncontent creators.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jul 2017 09:03:34 GMT"}], "update_date": "2017-07-24", "authors_parsed": [["Trzcinski", "Tomasz", ""], ["Andruszkiewicz", "Pawel", ""], ["Bochenski", "Tomasz", ""], ["Rokita", "Przemyslaw", ""]]}, {"id": "1707.06810", "submitter": "Ayan Kumar Bhunia", "authors": "Ayan Kumar Bhunia, Gautam Kumar, Partha Pratim Roy, R.\n  Balasubramanian, Umapada Pal", "title": "Text Recognition in Scene Image and Video Frame using Color Channel\n  Selection", "comments": "Multimedia Tools and Applications, Springer", "journal-ref": null, "doi": "10.1007/s11042-017-4750-6", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, recognition of text from natural scene image and video frame\nhas got increased attention among the researchers due to its various\ncomplexities and challenges. Because of low resolution, blurring effect,\ncomplex background, different fonts, color and variant alignment of text within\nimages and video frames, etc., text recognition in such scenario is difficult.\nMost of the current approaches usually apply a binarization algorithm to\nconvert them into binary images and next OCR is applied to get the recognition\nresult. In this paper, we present a novel approach based on color channel\nselection for text recognition from scene images and video frames. In the\napproach, at first, a color channel is automatically selected and then selected\ncolor channel is considered for text recognition. Our text recognition\nframework is based on Hidden Markov Model (HMM) which uses Pyramidal Histogram\nof Oriented Gradient features extracted from selected color channel. From each\nsliding window of a color channel our color-channel selection approach analyzes\nthe image properties from the sliding window and then a multi-label Support\nVector Machine (SVM) classifier is applied to select the color channel that\nwill provide the best recognition results in the sliding window. This color\nchannel selection for each sliding window has been found to be more fruitful\nthan considering a single color channel for the whole word image. Five\ndifferent features have been analyzed for multi-label SVM based color channel\nselection where wavelet transform based feature outperforms others. Our\nframework has been tested on different publicly available scene/video text\nimage datasets. For Devanagari script, we collected our own data dataset. The\nperformances obtained from experimental results are encouraging and show the\nadvantage of the proposed method.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jul 2017 09:25:55 GMT"}, {"version": "v2", "created": "Thu, 27 Jul 2017 23:12:13 GMT"}], "update_date": "2017-07-31", "authors_parsed": [["Bhunia", "Ayan Kumar", ""], ["Kumar", "Gautam", ""], ["Roy", "Partha Pratim", ""], ["Balasubramanian", "R.", ""], ["Pal", "Umapada", ""]]}, {"id": "1707.06825", "submitter": "Tomasz Trzcinski", "authors": "Jacek Komorowski, Tomasz Trzcinski", "title": "Evaluation of Hashing Methods Performance on Binary Feature Descriptors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we evaluate performance of data-dependent hashing methods on\nbinary data. The goal is to find a hashing method that can effectively produce\nlower dimensional binary representation of 512-bit FREAK descriptors. A\nrepresentative sample of recent unsupervised, semi-supervised and supervised\nhashing methods was experimentally evaluated on large datasets of labelled\nbinary FREAK feature descriptors.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jul 2017 10:17:33 GMT"}], "update_date": "2017-07-24", "authors_parsed": [["Komorowski", "Jacek", ""], ["Trzcinski", "Tomasz", ""]]}, {"id": "1707.06828", "submitter": "Ayan Kumar Bhunia", "authors": "Partha Pratim Roy, Ayan Kumar Bhunia, Umapada Pal", "title": "HMM-based Writer Identification in Music Score Documents without\n  Staff-Line Removal", "comments": "Expert Systems with Applications, Elsevier(2017)", "journal-ref": null, "doi": "10.1016/j.eswa.2017.07.031", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Writer identification from musical score documents is a challenging task due\nto its inherent problem of overlapping of musical symbols with staff lines.\nMost of the existing works in the literature of writer identification in\nmusical score documents were performed after a preprocessing stage of staff\nlines removal. In this paper we propose a novel writer identification framework\nin musical documents without removing staff lines from documents. In our\napproach, Hidden Markov Model has been used to model the writing style of the\nwriters without removing staff lines. The sliding window features are extracted\nfrom musical score lines and they are used to build writer specific HMM models.\nGiven a query musical sheet, writer specific confidence for each musical line\nis returned by each writer specific model using a loglikelihood score. Next, a\nloglikelihood score in page level is computed by weighted combination of these\nscores from the corresponding line images of the page. A novel Factor Analysis\nbased feature selection technique is applied in sliding window features to\nreduce the noise appearing from staff lines which proves efficiency in writer\nidentification performance.In our framework we have also proposed a novel score\nline detection approach in musical sheet using HMM. The experiment has been\nperformed in CVC-MUSCIMA dataset and the results obtained that the proposed\napproach is efficient for score line detection and writer identification\nwithout removing staff lines. To get the idea of computation time of our\nmethod, detail analysis of execution time is also provided.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jul 2017 10:34:05 GMT"}, {"version": "v2", "created": "Thu, 27 Jul 2017 23:11:51 GMT"}], "update_date": "2017-07-31", "authors_parsed": [["Roy", "Partha Pratim", ""], ["Bhunia", "Ayan Kumar", ""], ["Pal", "Umapada", ""]]}, {"id": "1707.06833", "submitter": "Ayan Kumar Bhunia", "authors": "Partha Pratim Roy, Ayan Kumar Bhunia, Umapada Pal", "title": "Date-Field Retrieval in Scene Image and Video Frames using Text\n  Enhancement and Shape Coding", "comments": null, "journal-ref": null, "doi": "10.1016/j.neucom.2016.08.141", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text recognition in scene image and video frames is difficult because of low\nresolution, blur, background noise, etc. Since traditional OCRs do not perform\nwell in such images, information retrieval using keywords could be an\nalternative way to index/retrieve such text information. Date is a useful piece\nof information which has various applications including date-wise videos/scene\nsearching, indexing or retrieval. This paper presents a date spotting based\ninformation retrieval system for natural scene image and video frames where\ntext appears with complex backgrounds. We propose a line based date spotting\napproach using Hidden Markov Model (HMM) which is used to detect the date\ninformation in a given text. Different date models are searched from a line\nwithout segmenting characters or words. Given a text line image in RGB, we\napply an efficient gray image conversion to enhance the text information.\nWavelet decomposition and gradient sub-bands are used to enhance text\ninformation in gray scale. Next, Pyramid Histogram of Oriented Gradient (PHOG)\nfeature has been extracted from gray image and binary images for date-spotting\nframework. Binary and gray image features are combined by MLP based Tandem\napproach. Finally, to boost the performance further, a shape coding based\nscheme is used to combine the similar shape characters in same class during\nword spotting. For our experiment, three different date models have been\nconstructed to search similar date information having numeric dates that\ncontains numeral values and punctuations and semi-numeric that contains dates\nwith numerals along with months in scene/video text. We have tested our system\non 1648 text lines and the results show the effectiveness of our proposed date\nspotting approach.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jul 2017 10:45:00 GMT"}], "update_date": "2017-07-24", "authors_parsed": [["Roy", "Partha Pratim", ""], ["Bhunia", "Ayan Kumar", ""], ["Pal", "Umapada", ""]]}, {"id": "1707.06838", "submitter": "Fernando Moya Rueda", "authors": "Fernando Moya Rueda, Rene Grzeszick, Gernot A. Fink", "title": "Neuron Pruning for Compressing Deep Networks using Maxout Architectures", "comments": "10 pages, to be published in GCPR2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an efficient and robust approach for reducing the size of\ndeep neural networks by pruning entire neurons. It exploits maxout units for\ncombining neurons into more complex convex functions and it makes use of a\nlocal relevance measurement that ranks neurons according to their activation on\nthe training set for pruning them. Additionally, a parameter reduction\ncomparison between neuron and weight pruning is shown. It will be empirically\nshown that the proposed neuron pruning reduces the number of parameters\ndramatically. The evaluation is performed on two tasks, the MNIST handwritten\ndigit recognition and the LFW face verification, using a LeNet-5 and a VGG16\nnetwork architecture. The network size is reduced by up to $74\\%$ and $61\\%$,\nrespectively, without affecting the network's performance. The main advantage\nof neuron pruning is its direct influence on the size of the network\narchitecture. Furthermore, it will be shown that neuron pruning can be combined\nwith subsequent weight pruning, reducing the size of the LeNet-5 and VGG16 up\nto $92\\%$ and $80\\%$ respectively.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jul 2017 10:53:37 GMT"}], "update_date": "2017-07-24", "authors_parsed": [["Rueda", "Fernando Moya", ""], ["Grzeszick", "Rene", ""], ["Fink", "Gernot A.", ""]]}, {"id": "1707.06865", "submitter": "Behdad Dashtbozorg", "authors": "Behdad Dashtbozorg, Jiong Zhang, and Bart M. ter Haar Romeny", "title": "Retinal Microaneurysms Detection using Local Convergence Index Features", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2018.2815345", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Retinal microaneurysms are the earliest clinical sign of diabetic retinopathy\ndisease. Detection of microaneurysms is crucial for the early diagnosis of\ndiabetic retinopathy and prevention of blindness. In this paper, a novel and\nreliable method for automatic detection of microaneurysms in retinal images is\nproposed. In the first stage of the proposed method, several preliminary\nmicroaneurysm candidates are extracted using a gradient weighting technique and\nan iterative thresholding approach. In the next stage, in addition to intensity\nand shape descriptors, a new set of features based on local convergence index\nfilters is extracted for each candidate. Finally, the collective set of\nfeatures is fed to a hybrid sampling/boosting classifier to discriminate the\nMAs from non-MAs candidates. The method is evaluated on images with different\nresolutions and modalities (RGB and SLO) using five publicly available datasets\nincluding the Retinopathy Online Challenge's dataset. The proposed method\nachieves an average sensitivity score of 0.471 on the ROC dataset outperforming\nstate-of-the-art approaches in an extensive comparison. The experimental\nresults on the other four datasets demonstrate the effectiveness and robustness\nof the proposed microaneurysms detection method regardless of different image\nresolutions and modalities.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jul 2017 12:30:12 GMT"}], "update_date": "2018-05-09", "authors_parsed": [["Dashtbozorg", "Behdad", ""], ["Zhang", "Jiong", ""], ["Romeny", "Bart M. ter Haar", ""]]}, {"id": "1707.06873", "submitter": "Hao Dong", "authors": "Hao Dong, Simiao Yu, Chao Wu, Yike Guo", "title": "Semantic Image Synthesis via Adversarial Learning", "comments": "Accepted to ICCV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a way of synthesizing realistic images directly\nwith natural language description, which has many useful applications, e.g.\nintelligent image manipulation. We attempt to accomplish such synthesis: given\na source image and a target text description, our model synthesizes images to\nmeet two requirements: 1) being realistic while matching the target text\ndescription; 2) maintaining other image features that are irrelevant to the\ntext description. The model should be able to disentangle the semantic\ninformation from the two modalities (image and text), and generate new images\nfrom the combined semantics. To achieve this, we proposed an end-to-end neural\narchitecture that leverages adversarial learning to automatically learn\nimplicit loss functions, which are optimized to fulfill the aforementioned two\nrequirements. We have evaluated our model by conducting experiments on\nCaltech-200 bird dataset and Oxford-102 flower dataset, and have demonstrated\nthat our model is capable of synthesizing realistic images that match the given\ndescriptions, while still maintain other features of original images.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jul 2017 12:45:46 GMT"}], "update_date": "2017-07-24", "authors_parsed": [["Dong", "Hao", ""], ["Yu", "Simiao", ""], ["Wu", "Chao", ""], ["Guo", "Yike", ""]]}, {"id": "1707.06879", "submitter": "Aurelien Lucchi", "authors": "Pascal Kaiser, Jan Dirk Wegner, Aurelien Lucchi, Martin Jaggi, Thomas\n  Hofmann, Konrad Schindler", "title": "Learning Aerial Image Segmentation from Online Maps", "comments": "Published in IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING", "journal-ref": null, "doi": "10.1109/TGRS.2017.2719738", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study deals with semantic segmentation of high-resolution (aerial)\nimages where a semantic class label is assigned to each pixel via supervised\nclassification as a basis for automatic map generation. Recently, deep\nconvolutional neural networks (CNNs) have shown impressive performance and have\nquickly become the de-facto standard for semantic segmentation, with the added\nbenefit that task-specific feature design is no longer necessary. However, a\nmajor downside of deep learning methods is that they are extremely data-hungry,\nthus aggravating the perennial bottleneck of supervised classification, to\nobtain enough annotated training data. On the other hand, it has been observed\nthat they are rather robust against noise in the training labels. This opens up\nthe intriguing possibility to avoid annotating huge amounts of training data,\nand instead train the classifier from existing legacy data or crowd-sourced\nmaps which can exhibit high levels of noise. The question addressed in this\npaper is: can training with large-scale, publicly available labels replace a\nsubstantial part of the manual labeling effort and still achieve sufficient\nperformance? Such data will inevitably contain a significant portion of errors,\nbut in return virtually unlimited quantities of it are available in larger\nparts of the world. We adapt a state-of-the-art CNN architecture for semantic\nsegmentation of buildings and roads in aerial images, and compare its\nperformance when using different training data sets, ranging from manually\nlabeled, pixel-accurate ground truth of the same city to automatic training\ndata derived from OpenStreetMap data from distant locations. We report our\nresults that indicate that satisfying performance can be obtained with\nsignificantly less manual annotation effort, by exploiting noisy large-scale\ntraining data.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jul 2017 12:58:18 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Kaiser", "Pascal", ""], ["Wegner", "Jan Dirk", ""], ["Lucchi", "Aurelien", ""], ["Jaggi", "Martin", ""], ["Hofmann", "Thomas", ""], ["Schindler", "Konrad", ""]]}, {"id": "1707.06907", "submitter": "Ivona Tautkute", "authors": "Ivona Tautkute, Aleksandra Mo\\.zejko, Wojciech Stokowiec, Tomasz\n  Trzci\\'nski, {\\L}ukasz Brocki, Krzysztof Marasek", "title": "What Looks Good with my Sofa: Multimodal Search Engine for Interior\n  Design", "comments": "FEDCSIS 5th Conference on Multimedia, Interaction, Design and\n  Innovation (MIDI), 2017", "journal-ref": "Proceedings of the 2017 Federated Conference on Computer Science\n  and Information Systems", "doi": "10.15439/2017F56", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a multi-modal search engine for interior design\nthat combines visual and textual queries. The goal of our engine is to retrieve\ninterior objects, e.g. furniture or wall clocks, that share visual and\naesthetic similarities with the query. Our search engine allows the user to\ntake a photo of a room and retrieve with a high recall a list of items\nidentical or visually similar to those present in the photo. Additionally, it\nallows to return other items that aesthetically and stylistically fit well\ntogether. To achieve this goal, our system blends the results obtained using\ntextual and visual modalities. Thanks to this blending strategy, we increase\nthe average style similarity score of the retrieved items by 11%. Our work is\nimplemented as a Web-based application and it is planned to be opened to the\npublic.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jul 2017 14:08:42 GMT"}, {"version": "v2", "created": "Mon, 8 Jan 2018 14:14:59 GMT"}], "update_date": "2018-01-09", "authors_parsed": [["Tautkute", "Ivona", ""], ["Mo\u017cejko", "Aleksandra", ""], ["Stokowiec", "Wojciech", ""], ["Trzci\u0144ski", "Tomasz", ""], ["Brocki", "\u0141ukasz", ""], ["Marasek", "Krzysztof", ""]]}, {"id": "1707.06923", "submitter": "Biswa Sengupta", "authors": "Biswa Sengupta and Yu Qian", "title": "Multi-kernel learning of deep convolutional features for action\n  recognition", "comments": "ICCV 2017 Workshop on Video and Language Understanding: MovieQA and\n  the Large Scale Movie Description Challenge", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image understanding using deep convolutional network has reached human-level\nperformance, yet a closely related problem of video understanding especially,\naction recognition has not reached the requisite level of maturity. We combine\nmulti-kernels based support-vector-machines (SVM) with a multi-stream deep\nconvolutional neural network to achieve close to state-of-the-art performance\non a 51-class activity recognition problem (HMDB-51 dataset); this specific\ndataset has proved to be particularly challenging for deep neural networks due\nto the heterogeneity in camera viewpoints, video quality, etc. The resulting\narchitecture is named pillar networks as each (very) deep neural network acts\nas a pillar for the hierarchical classifiers. In addition, we illustrate that\nhand-crafted features such as improved dense trajectories (iDT) and Multi-skip\nFeature Stacking (MIFS), as additional pillars, can further supplement the\nperformance.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jul 2017 14:45:48 GMT"}, {"version": "v2", "created": "Sun, 12 Nov 2017 09:56:45 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Sengupta", "Biswa", ""], ["Qian", "Yu", ""]]}, {"id": "1707.06978", "submitter": "William Lotter", "authors": "William Lotter, Greg Sorensen, David Cox", "title": "A Multi-Scale CNN and Curriculum Learning Strategy for Mammogram\n  Classification", "comments": "Accepted to MICCAI 2017 Workshop on Deep Learning in Medical Image\n  Analysis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Screening mammography is an important front-line tool for the early detection\nof breast cancer, and some 39 million exams are conducted each year in the\nUnited States alone. Here, we describe a multi-scale convolutional neural\nnetwork (CNN) trained with a curriculum learning strategy that achieves high\nlevels of accuracy in classifying mammograms. Specifically, we first train\nCNN-based patch classifiers on segmentation masks of lesions in mammograms, and\nthen use the learned features to initialize a scanning-based model that renders\na decision on the whole image, trained end-to-end on outcome data. We\ndemonstrate that our approach effectively handles the \"needle in a haystack\"\nnature of full-image mammogram classification, achieving 0.92 AUROC on the DDSM\ndataset.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jul 2017 17:16:12 GMT"}], "update_date": "2017-07-24", "authors_parsed": [["Lotter", "William", ""], ["Sorensen", "Greg", ""], ["Cox", "David", ""]]}, {"id": "1707.06982", "submitter": "Rocio Gonzalez-Diaz", "authors": "J. Lamar-Leon, Raul Alonso-Baryolo, Edel Garcia-Reyes, R.\n  Gonzalez-Diaz", "title": "Persistent-homology-based gait recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gait recognition is an important biometric technique for video surveillance\ntasks, due to the advantage of using it at distance. In this paper, we present\na persistent homology-based method to extract topological features (the\nso-called {\\it topological gait signature}) from the the body silhouettes of a\ngait sequence. % It has been used before in several conference papers of the\nsame authors for human identification, gender classification, carried object\ndetection and monitoring human activities at distance. % The novelty of this\npaper is the study of the stability of the topological gait signature under\nsmall perturbations and the number of gait cycles contained in a gait sequence.\nIn other words, we show that the topological gait signature is robust to the\npresence of noise in the body silhouettes and to the number of gait cycles\ncontained in a given gait sequence. % We also show that computing our\ntopological gait signature of only the lowest fourth part of the body\nsilhouette, we avoid the upper body movements that are unrelated to the natural\ndynamic of the gait, caused for example by carrying a bag or wearing a coat.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jul 2017 17:24:54 GMT"}], "update_date": "2017-07-24", "authors_parsed": [["Lamar-Leon", "J.", ""], ["Alonso-Baryolo", "Raul", ""], ["Garcia-Reyes", "Edel", ""], ["Gonzalez-Diaz", "R.", ""]]}, {"id": "1707.06990", "submitter": "Geoff Pleiss", "authors": "Geoff Pleiss, Danlu Chen, Gao Huang, Tongcheng Li, Laurens van der\n  Maaten, Kilian Q. Weinberger", "title": "Memory-Efficient Implementation of DenseNets", "comments": "Technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The DenseNet architecture is highly computationally efficient as a result of\nfeature reuse. However, a naive DenseNet implementation can require a\nsignificant amount of GPU memory: If not properly managed, pre-activation batch\nnormalization and contiguous convolution operations can produce feature maps\nthat grow quadratically with network depth. In this technical report, we\nintroduce strategies to reduce the memory consumption of DenseNets during\ntraining. By strategically using shared memory allocations, we reduce the\nmemory cost for storing feature maps from quadratic to linear. Without the GPU\nmemory bottleneck, it is now possible to train extremely deep DenseNets.\nNetworks with 14M parameters can be trained on a single GPU, up from 4M. A\n264-layer DenseNet (73M parameters), which previously would have been\ninfeasible to train, can now be trained on a single workstation with 8 NVIDIA\nTesla M40 GPUs. On the ImageNet ILSVRC classification dataset, this large\nDenseNet obtains a state-of-the-art single-crop top-1 error of 20.26%.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jul 2017 17:51:36 GMT"}], "update_date": "2017-07-24", "authors_parsed": [["Pleiss", "Geoff", ""], ["Chen", "Danlu", ""], ["Huang", "Gao", ""], ["Li", "Tongcheng", ""], ["van der Maaten", "Laurens", ""], ["Weinberger", "Kilian Q.", ""]]}, {"id": "1707.07012", "submitter": "Quoc Le", "authors": "Barret Zoph, Vijay Vasudevan, Jonathon Shlens, Quoc V. Le", "title": "Learning Transferable Architectures for Scalable Image Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Developing neural network image classification models often requires\nsignificant architecture engineering. In this paper, we study a method to learn\nthe model architectures directly on the dataset of interest. As this approach\nis expensive when the dataset is large, we propose to search for an\narchitectural building block on a small dataset and then transfer the block to\na larger dataset. The key contribution of this work is the design of a new\nsearch space (the \"NASNet search space\") which enables transferability. In our\nexperiments, we search for the best convolutional layer (or \"cell\") on the\nCIFAR-10 dataset and then apply this cell to the ImageNet dataset by stacking\ntogether more copies of this cell, each with their own parameters to design a\nconvolutional architecture, named \"NASNet architecture\". We also introduce a\nnew regularization technique called ScheduledDropPath that significantly\nimproves generalization in the NASNet models. On CIFAR-10 itself, NASNet\nachieves 2.4% error rate, which is state-of-the-art. On ImageNet, NASNet\nachieves, among the published works, state-of-the-art accuracy of 82.7% top-1\nand 96.2% top-5 on ImageNet. Our model is 1.2% better in top-1 accuracy than\nthe best human-invented architectures while having 9 billion fewer FLOPS - a\nreduction of 28% in computational demand from the previous state-of-the-art\nmodel. When evaluated at different levels of computational cost, accuracies of\nNASNets exceed those of the state-of-the-art human-designed models. For\ninstance, a small version of NASNet also achieves 74% top-1 accuracy, which is\n3.1% better than equivalently-sized, state-of-the-art models for mobile\nplatforms. Finally, the learned features by NASNet used with the Faster-RCNN\nframework surpass state-of-the-art by 4.0% achieving 43.1% mAP on the COCO\ndataset.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jul 2017 18:10:26 GMT"}, {"version": "v2", "created": "Wed, 25 Oct 2017 01:37:56 GMT"}, {"version": "v3", "created": "Fri, 1 Dec 2017 07:48:01 GMT"}, {"version": "v4", "created": "Wed, 11 Apr 2018 05:12:21 GMT"}], "update_date": "2018-04-12", "authors_parsed": [["Zoph", "Barret", ""], ["Vasudevan", "Vijay", ""], ["Shlens", "Jonathon", ""], ["Le", "Quoc V.", ""]]}, {"id": "1707.07013", "submitter": "Akshayvarun Subramanya", "authors": "Akshayvarun Subramanya, Suraj Srinivas, R.Venkatesh Babu", "title": "Confidence estimation in Deep Neural networks via density modelling", "comments": "ICME 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art Deep Neural Networks can be easily fooled into providing\nincorrect high-confidence predictions for images with small amounts of\nadversarial noise. Does this expose a flaw with deep neural networks, or do we\nsimply need a better way to estimate confidence? In this paper we consider the\nproblem of accurately estimating predictive confidence. We formulate this\nproblem as that of density modelling, and show how traditional methods such as\nsoftmax produce poor estimates. To address this issue, we propose a novel\nconfidence measure based on density modelling approaches. We test these\nmeasures on images distorted by blur, JPEG compression, random noise and\nadversarial noise. Experiments show that our confidence measure consistently\nshows reduced confidence scores in the presence of such distortions - a\nproperty which softmax often lacks.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jul 2017 18:12:15 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Subramanya", "Akshayvarun", ""], ["Srinivas", "Suraj", ""], ["Babu", "R. Venkatesh", ""]]}, {"id": "1707.07074", "submitter": "Lin Wu", "authors": "Lin Wu, Yang Wang, Xue Li, Junbin Gao", "title": "What-and-Where to Match: Deep Spatially Multiplicative Integration\n  Networks for Person Re-identification", "comments": "Published at Pattern Recognition, Elsevier", "journal-ref": null, "doi": "10.1016/j.patcog.2017.10.004", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matching pedestrians across disjoint camera views, known as person\nre-identification (re-id), is a challenging problem that is of importance to\nvisual recognition and surveillance. Most existing methods exploit local\nregions within spatial manipulation to perform matching in local\ncorrespondence. However, they essentially extract \\emph{fixed} representations\nfrom pre-divided regions for each image and perform matching based on the\nextracted representation subsequently. For models in this pipeline, local finer\npatterns that are crucial to distinguish positive pairs from negative ones\ncannot be captured, and thus making them underperformed. In this paper, we\npropose a novel deep multiplicative integration gating function, which answers\nthe question of \\emph{what-and-where to match} for effective person re-id. To\naddress \\emph{what} to match, our deep network emphasizes common local patterns\nby learning joint representations in a multiplicative way. The network\ncomprises two Convolutional Neural Networks (CNNs) to extract convolutional\nactivations, and generates relevant descriptors for pedestrian matching. This\nthus, leads to flexible representations for pair-wise images. To address\n\\emph{where} to match, we combat the spatial misalignment by performing\nspatially recurrent pooling via a four-directional recurrent neural network to\nimpose spatial dependency over all positions with respect to the entire image.\nThe proposed network is designed to be end-to-end trainable to characterize\nlocal pairwise feature interactions in a spatially aligned manner. To\ndemonstrate the superiority of our method, extensive experiments are conducted\nover three benchmark data sets: VIPeR, CUHK03 and Market-1501.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jul 2017 23:50:58 GMT"}, {"version": "v2", "created": "Sun, 30 Jul 2017 06:45:22 GMT"}, {"version": "v3", "created": "Fri, 6 Oct 2017 23:36:09 GMT"}, {"version": "v4", "created": "Sat, 14 Oct 2017 00:24:20 GMT"}], "update_date": "2017-10-17", "authors_parsed": [["Wu", "Lin", ""], ["Wang", "Yang", ""], ["Li", "Xue", ""], ["Gao", "Junbin", ""]]}, {"id": "1707.07075", "submitter": "Michele Merler", "authors": "Michele Merler and Dhiraj Joshi and Quoc-Bao Nguyen and Stephen Hammer\n  and John Kent and John R. Smith and Rogerio S. Feris", "title": "Automatic Curation of Golf Highlights using Multimodal Excitement\n  Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The production of sports highlight packages summarizing a game's most\nexciting moments is an essential task for broadcast media. Yet, it requires\nlabor-intensive video editing. We propose a novel approach for auto-curating\nsports highlights, and use it to create a real-world system for the editorial\naid of golf highlight reels. Our method fuses information from the players'\nreactions (action recognition such as high-fives and fist pumps), spectators\n(crowd cheering), and commentator (tone of the voice and word analysis) to\ndetermine the most interesting moments of a game. We accurately identify the\nstart and end frames of key shot highlights with additional metadata, such as\nthe player's name and the hole number, allowing personalized content\nsummarization and retrieval. In addition, we introduce new techniques for\nlearning our classifiers with reduced manual training data annotation by\nexploiting the correlation of different modalities. Our work has been\ndemonstrated at a major golf tournament, successfully extracting highlights\nfrom live video streams over four consecutive days.\n", "versions": [{"version": "v1", "created": "Sat, 22 Jul 2017 00:06:50 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Merler", "Michele", ""], ["Joshi", "Dhiraj", ""], ["Nguyen", "Quoc-Bao", ""], ["Hammer", "Stephen", ""], ["Kent", "John", ""], ["Smith", "John R.", ""], ["Feris", "Rogerio S.", ""]]}, {"id": "1707.07089", "submitter": "Ningning Zhao", "authors": "Ningning Zhao and Daniel O'Connor and Adrian Basarab and Dan Ruan and\n  Peng Hu and Ke Sheng", "title": "Motion Compensated Dynamic MRI Reconstruction with Local Affine Optical\n  Flow Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel framework to reconstruct the dynamic magnetic\nresonance images (DMRI) with motion compensation (MC). Due to the inherent\nmotion effects during DMRI acquisition, reconstruction of DMRI using motion\nestimation/compensation (ME/MC) has been studied under a compressed sensing\n(CS) scheme. In this paper, by embedding the intensity-based optical flow (OF)\nconstraint into the traditional CS scheme, we are able to couple the DMRI\nreconstruction with motion field estimation. The formulated optimization\nproblem is solved by a primal-dual algorithm with linesearch due to its\nefficiency when dealing with non-differentiable problems. With the estimated\nmotion field, the DMRI reconstruction is refined through MC. By employing the\nmulti-scale coarse-to-fine strategy, we are able to update the\nvariables(temporal image sequences and motion vectors) and to refine the image\nreconstruction alternately. Moreover, the proposed framework is capable of\nhandling a wide class of prior information (regularizations) for DMRI\nreconstruction, such as sparsity, low rank and total variation. Experiments on\nvarious DMRI data, ranging from in vivo lung to cardiac dataset, validate the\nreconstruction quality improvement using the proposed scheme in comparison to\nseveral state-of-the-art algorithms.\n", "versions": [{"version": "v1", "created": "Sat, 22 Jul 2017 02:23:21 GMT"}, {"version": "v2", "created": "Mon, 19 Feb 2018 03:16:18 GMT"}, {"version": "v3", "created": "Thu, 14 Feb 2019 00:00:52 GMT"}], "update_date": "2019-02-15", "authors_parsed": [["Zhao", "Ningning", ""], ["O'Connor", "Daniel", ""], ["Basarab", "Adrian", ""], ["Ruan", "Dan", ""], ["Hu", "Peng", ""], ["Sheng", "Ke", ""]]}, {"id": "1707.07102", "submitter": "Xuwang Yin", "authors": "Xuwang Yin, Vicente Ordonez", "title": "OBJ2TEXT: Generating Visually Descriptive Language from Object Layouts", "comments": "Accepted at EMNLP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating captions for images is a task that has recently received\nconsiderable attention. In this work we focus on caption generation for\nabstract scenes, or object layouts where the only information provided is a set\nof objects and their locations. We propose OBJ2TEXT, a sequence-to-sequence\nmodel that encodes a set of objects and their locations as an input sequence\nusing an LSTM network, and decodes this representation using an LSTM language\nmodel. We show that our model, despite encoding object layouts as a sequence,\ncan represent spatial relationships between objects, and generate descriptions\nthat are globally coherent and semantically relevant. We test our approach in a\ntask of object-layout captioning by using only object annotations as inputs. We\nadditionally show that our model, combined with a state-of-the-art object\ndetector, improves an image captioning model from 0.863 to 0.950 (CIDEr score)\nin the test benchmark of the standard MS-COCO Captioning task.\n", "versions": [{"version": "v1", "created": "Sat, 22 Jul 2017 04:17:42 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Yin", "Xuwang", ""], ["Ordonez", "Vicente", ""]]}, {"id": "1707.07103", "submitter": "Guoliang Kang", "authors": "Guoliang Kang, Xuanyi Dong, Liang Zheng, Yi Yang", "title": "PatchShuffle Regularization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on regularizing the training of the convolutional neural\nnetwork (CNN). We propose a new regularization approach named ``PatchShuffle``\nthat can be adopted in any classification-oriented CNN models. It is easy to\nimplement: in each mini-batch, images or feature maps are randomly chosen to\nundergo a transformation such that pixels within each local patch are shuffled.\nThrough generating images and feature maps with interior orderless patches,\nPatchShuffle creates rich local variations, reduces the risk of network\noverfitting, and can be viewed as a beneficial supplement to various kinds of\ntraining regularization techniques, such as weight decay, model ensemble and\ndropout. Experiments on four representative classification datasets show that\nPatchShuffle improves the generalization ability of CNN especially when the\ndata is scarce. Moreover, we empirically illustrate that CNN models trained\nwith PatchShuffle are more robust to noise and local changes in an image.\n", "versions": [{"version": "v1", "created": "Sat, 22 Jul 2017 04:33:02 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Kang", "Guoliang", ""], ["Dong", "Xuanyi", ""], ["Zheng", "Liang", ""], ["Yang", "Yi", ""]]}, {"id": "1707.07119", "submitter": "Wuzhen Shi", "authors": "Wuzhen Shi, Feng Jiang, Shengping Zhang and Debin Zhao", "title": "Deep Networks for Compressed Image Sensing", "comments": "This paper has been accepted by the IEEE International Conference on\n  Multimedia and Expo (ICME) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The compressed sensing (CS) theory has been successfully applied to image\ncompression in the past few years as most image signals are sparse in a certain\ndomain. Several CS reconstruction models have been recently proposed and\nobtained superior performance. However, there still exist two important\nchallenges within the CS theory. The first one is how to design a sampling\nmechanism to achieve an optimal sampling efficiency, and the second one is how\nto perform the reconstruction to get the highest quality to achieve an optimal\nsignal recovery. In this paper, we try to deal with these two problems with a\ndeep network. First of all, we train a sampling matrix via the network training\ninstead of using a traditional manually designed one, which is much appropriate\nfor our deep network based reconstruct process. Then, we propose a deep network\nto recover the image, which imitates traditional compressed sensing\nreconstruction processes. Experimental results demonstrate that our deep\nnetworks based CS reconstruction method offers a very significant quality\nimprovement compared against state of the art ones.\n", "versions": [{"version": "v1", "created": "Sat, 22 Jul 2017 08:22:18 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Shi", "Wuzhen", ""], ["Jiang", "Feng", ""], ["Zhang", "Shengping", ""], ["Zhao", "Debin", ""]]}, {"id": "1707.07128", "submitter": "Wuzhen Shi", "authors": "Wuzhen Shi, Feng Jiang and Debin Zhao", "title": "Single Image Super-Resolution with Dilated Convolution based Multi-Scale\n  Information Learning Inception Module", "comments": "This paper has been accepted by the IEEE International Conference on\n  Image Processing (ICIP) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional works have shown that patches in a natural image tend to\nredundantly recur many times inside the image, both within the same scale, as\nwell as across different scales. Make full use of these multi-scale information\ncan improve the image restoration performance. However, the current proposed\ndeep learning based restoration methods do not take the multi-scale information\ninto account. In this paper, we propose a dilated convolution based inception\nmodule to learn multi-scale information and design a deep network for single\nimage super-resolution. Different dilated convolution learns different scale\nfeature, then the inception module concatenates all these features to fuse\nmulti-scale information. In order to increase the reception field of our\nnetwork to catch more contextual information, we cascade multiple inception\nmodules to constitute a deep network to conduct single image super-resolution.\nWith the novel dilated convolution based inception module, the proposed\nend-to-end single image super-resolution network can take advantage of\nmulti-scale information to improve image super-resolution performance.\nExperimental results show that our proposed method outperforms many\nstate-of-the-art single image super-resolution methods.\n", "versions": [{"version": "v1", "created": "Sat, 22 Jul 2017 09:27:04 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Shi", "Wuzhen", ""], ["Jiang", "Feng", ""], ["Zhao", "Debin", ""]]}, {"id": "1707.07139", "submitter": "Kun Li", "authors": "Kun Li, Joel W. Burdick", "title": "Clinical Patient Tracking in the Presence of Transient and Permanent\n  Occlusions via Geodesic Feature", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops a method to use RGB-D cameras to track the motions of a\nhuman spinal cord injury patient undergoing spinal stimulation and physical\nrehabilitation. Because clinicians must remain close to the patient during\ntraining sessions, the patient is usually under permanent and transient\nocclusions due to the training equipment and the movements of the attending\nclinicians. These occlusions can significantly degrade the accuracy of existing\nhuman tracking methods. To improve the data association problem in these\ncircumstances, we present a new global feature based on the geodesic distances\nof surface mesh points to a set of anchor points. Transient occlusions are\nhandled via a multi-hypothesis tracking framework. To evaluate the method, we\nsimulated different occlusion sizes on a data set captured from a human in\nvarying movement patterns, and compared the proposed feature with other\ntracking methods. The results show that the proposed method achieves robustness\nto both surface deformations and transient occlusions.\n", "versions": [{"version": "v1", "created": "Sat, 22 Jul 2017 11:12:18 GMT"}, {"version": "v2", "created": "Tue, 25 Jul 2017 00:29:31 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Li", "Kun", ""], ["Burdick", "Joel W.", ""]]}, {"id": "1707.07150", "submitter": "Ayan Kumar Bhunia", "authors": "Aneeshan Sain, Ayan Kumar Bhunia, Partha Pratim Roy, Umapada Pal", "title": "Multi-Oriented Text Detection and Verification in Video Frames and Scene\n  Images", "comments": "Accepted in Neurocomputing, Elsevier", "journal-ref": null, "doi": "10.1016/j.neucom.2017.09.089", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we bring forth a novel approach of video text detection using\nFourier-Laplacian filtering in the frequency domain that includes a\nverification technique using Hidden Markov Model (HMM). The proposed approach\ndeals with the text region appearing not only in horizontal or vertical\ndirections, but also in any other oblique or curved orientation in the image.\nUntil now only a few methods have been proposed that look into curved text\ndetection in video frames, wherein lies our novelty. In our approach, we first\napply Fourier-Laplacian transform on the image followed by an ideal\nLaplacian-Gaussian filtering. Thereafter K-means clustering is employed to\nobtain the asserted text areas depending on a maximum difference map. Next, the\nobtained connected components (CC) are skeletonized to distinguish various text\nstrings. Complex components are disintegrated into simpler ones according to a\njunction removal algorithm followed by a concatenation performed on possible\ncombination of the disjoint skeletons to obtain the corresponding text area.\nFinally these text hypotheses are verified using HMM-based text/non-text\nclassification system. False positives are thus eliminated giving us a robust\ntext detection performance. We have tested our framework in multi-oriented text\nlines in four scripts, namely, English, Chinese, Devanagari and Bengali, in\nvideo frames and scene texts. The results obtained show that proposed approach\nsurpasses existing methods on text detection.\n", "versions": [{"version": "v1", "created": "Sat, 22 Jul 2017 12:09:28 GMT"}, {"version": "v2", "created": "Wed, 4 Oct 2017 16:48:08 GMT"}], "update_date": "2017-10-05", "authors_parsed": [["Sain", "Aneeshan", ""], ["Bhunia", "Ayan Kumar", ""], ["Roy", "Partha Pratim", ""], ["Pal", "Umapada", ""]]}, {"id": "1707.07157", "submitter": "Li Sun Dr", "authors": "Li Sun, Gerardo Aragon-Camarasa, Simon Rogers, Rustam Stolkin, J. Paul\n  Siebert", "title": "Single-Shot Clothing Category Recognition in Free-Configurations with\n  Application to Autonomous Clothes Sorting", "comments": "9 pages, accepted by IROS2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a single-shot approach for recognising clothing\ncategories from 2.5D features. We propose two visual features, BSP (B-Spline\nPatch) and TSD (Topology Spatial Distances) for this task. The local BSP\nfeatures are encoded by LLC (Locality-constrained Linear Coding) and fused with\nthree different global features. Our visual feature is robust to deformable\nshapes and our approach is able to recognise the category of unknown clothing\nin unconstrained and random configurations. We integrated the category\nrecognition pipeline with a stereo vision system, clothing instance detection,\nand dual-arm manipulators to achieve an autonomous sorting system. To verify\nthe performance of our proposed method, we build a high-resolution RGBD\nclothing dataset of 50 clothing items of 5 categories sampled in random\nconfigurations (a total of 2,100 clothing samples). Experimental results show\nthat our approach is able to reach 83.2\\% accuracy while classifying clothing\nitems which were previously unseen during training. This advances beyond the\nprevious state-of-the-art by 36.2\\%. Finally, we evaluate the proposed approach\nin an autonomous robot sorting system, in which the robot recognises a clothing\nitem from an unconstrained pile, grasps it, and sorts it into a box according\nto its category. Our proposed sorting system achieves reasonable sorting\nsuccess rates with single-shot perception.\n", "versions": [{"version": "v1", "created": "Sat, 22 Jul 2017 13:06:24 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Sun", "Li", ""], ["Aragon-Camarasa", "Gerardo", ""], ["Rogers", "Simon", ""], ["Stolkin", "Rustam", ""], ["Siebert", "J. Paul", ""]]}, {"id": "1707.07165", "submitter": "Haroun Habeeb", "authors": "Haroun Habeeb and Ankit Anand and Mausam and Parag Singla", "title": "Coarse-to-Fine Lifted MAP Inference in Computer Vision", "comments": "Published in IJCAI 2017", "journal-ref": null, "doi": "10.24963/ijcai.2017/641", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a vast body of theoretical research on lifted inference in\nprobabilistic graphical models (PGMs). However, few demonstrations exist where\nlifting is applied in conjunction with top of the line applied algorithms. We\npursue the applicability of lifted inference for computer vision (CV), with the\ninsight that a globally optimal (MAP) labeling will likely have the same label\nfor two symmetric pixels. The success of our approach lies in efficiently\nhandling a distinct unary potential on every node (pixel), typical of CV\napplications. This allows us to lift the large class of algorithms that model a\nCV problem via PGM inference. We propose a generic template for coarse-to-fine\n(C2F) inference in CV, which progressively refines an initial coarsely lifted\nPGM for varying quality-time trade-offs. We demonstrate the performance of C2F\ninference by developing lifted versions of two near state-of-the-art CV\nalgorithms for stereo vision and interactive image segmentation. We find that,\nagainst flat algorithms, the lifted versions have a much superior anytime\nperformance, without any loss in final solution quality.\n", "versions": [{"version": "v1", "created": "Sat, 22 Jul 2017 13:45:28 GMT"}], "update_date": "2017-08-31", "authors_parsed": [["Habeeb", "Haroun", ""], ["Anand", "Ankit", ""], ["Mausam", "", ""], ["Singla", "Parag", ""]]}, {"id": "1707.07169", "submitter": "Zachary Pezzementi", "authors": "Zachary Pezzementi, Trenton Tabor, Peiyun Hu, Jonathan K. Chang, Deva\n  Ramanan, Carl Wellington, Benzun P. Wisely Babu, Herman Herman", "title": "Comparing Apples and Oranges: Off-Road Pedestrian Detection on the NREC\n  Agricultural Person-Detection Dataset", "comments": "Accepted to Journal of Field Robotics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person detection from vehicles has made rapid progress recently with the\nadvent of multiple highquality datasets of urban and highway driving, yet no\nlarge-scale benchmark is available for the same problem in off-road or\nagricultural environments. Here we present the NREC Agricultural\nPerson-Detection Dataset to spur research in these environments. It consists of\nlabeled stereo video of people in orange and apple orchards taken from two\nperception platforms (a tractor and a pickup truck), along with vehicle\nposition data from RTK GPS. We define a benchmark on part of the dataset that\ncombines a total of 76k labeled person images and 19k sampled person-free\nimages. The dataset highlights several key challenges of the domain, including\nvarying environment, substantial occlusion by vegetation, people in motion and\nin non-standard poses, and people seen from a variety of distances; meta-data\nare included to allow targeted evaluation of each of these effects. Finally, we\npresent baseline detection performance results for three leading approaches\nfrom urban pedestrian detection and our own convolutional neural network\napproach that benefits from the incorporation of additional image context. We\nshow that the success of existing approaches on urban data does not transfer\ndirectly to this domain.\n", "versions": [{"version": "v1", "created": "Sat, 22 Jul 2017 14:16:36 GMT"}, {"version": "v2", "created": "Thu, 26 Oct 2017 17:22:37 GMT"}], "update_date": "2017-10-27", "authors_parsed": [["Pezzementi", "Zachary", ""], ["Tabor", "Trenton", ""], ["Hu", "Peiyun", ""], ["Chang", "Jonathan K.", ""], ["Ramanan", "Deva", ""], ["Wellington", "Carl", ""], ["Babu", "Benzun P. Wisely", ""], ["Herman", "Herman", ""]]}, {"id": "1707.07180", "submitter": "Mohammed Daoudi", "authors": "Mohamed Daoudi, Stefano Berretti, Pietro Pala, Yvonne Delevoye,\n  Alberto Del Bimbo", "title": "Emotion Recognition by Body Movement Representation on the Manifold of\n  Symmetric Positive Definite Matrices", "comments": "accepted in I19th International Conference on Image Analysis and\n  processing (ICIAP), 11-15 september Catania, Italy, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emotion recognition is attracting great interest for its potential\napplication in a multitude of real-life situations. Much of the Computer Vision\nresearch in this field has focused on relating emotions to facial expressions,\nwith investigations rarely including more than upper body. In this work, we\npropose a new scenario, for which emotional states are related to 3D dynamics\nof the whole body motion. To address the complexity of human body movement, we\nused covariance descriptors of the sequence of the 3D skeleton joints, and\nrepresented them in the non-linear Riemannian manifold of Symmetric Positive\nDefinite matrices. In doing so, we exploited geodesic distances and geometric\nmeans on the manifold to perform emotion classification. Using sequences of\nspontaneous walking under the five primary emotional states, we report a method\nthat succeeded in classifying the different emotions, with comparable\nperformance to those observed in a human-based force-choice classification\ntask.\n", "versions": [{"version": "v1", "created": "Sat, 22 Jul 2017 15:55:56 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Daoudi", "Mohamed", ""], ["Berretti", "Stefano", ""], ["Pala", "Pietro", ""], ["Delevoye", "Yvonne", ""], ["Del Bimbo", "Alberto", ""]]}, {"id": "1707.07184", "submitter": "Axel Davy", "authors": "Lara Raad, Axel Davy, Agn\\`es Desolneux, Jean-Michel Morel", "title": "A survey of exemplar-based texture synthesis", "comments": "v2: Added comments and typos fixes. New section added to describe\n  FRAME. New method presented: CNNMRF", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exemplar-based texture synthesis is the process of generating, from an input\nsample, new texture images of arbitrary size and which are perceptually\nequivalent to the sample. The two main approaches are statistics-based methods\nand patch re-arrangement methods. In the first class, a texture is\ncharacterized by a statistical signature; then, a random sampling conditioned\nto this signature produces genuinely different texture images. The second class\nboils down to a clever \"copy-paste\" procedure, which stitches together large\nregions of the sample. Hybrid methods try to combine ideas from both approaches\nto avoid their hurdles. The recent approaches using convolutional neural\nnetworks fit to this classification, some being statistical and others\nperforming patch re-arrangement in the feature space. They produce impressive\nsynthesis on various kinds of textures. Nevertheless, we found that most real\ntextures are organized at multiple scales, with global structures revealed at\ncoarse scales and highly varying details at finer ones. Thus, when confronted\nwith large natural images of textures the results of state-of-the-art methods\ndegrade rapidly, and the problem of modeling them remains wide open.\n", "versions": [{"version": "v1", "created": "Sat, 22 Jul 2017 16:08:49 GMT"}, {"version": "v2", "created": "Fri, 24 Nov 2017 15:57:10 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Raad", "Lara", ""], ["Davy", "Axel", ""], ["Desolneux", "Agn\u00e8s", ""], ["Morel", "Jean-Michel", ""]]}, {"id": "1707.07188", "submitter": "Taras Iakymchuk", "authors": "Juan Barrios-Avil\\'es, Taras Iakymchuk, Jorge Samaniego, Alfredo\n  Rosado-Mu\\~noz", "title": "An Event-based Fast Movement Detection Algorithm for a Positioning Robot\n  Using POWERLINK Communication", "comments": "Videos of assembly and live system action:\n  https://youtu.be/KjkawFHd9_0 https://youtu.be/5X8d1Gw2Eco\n  https://youtu.be/Ou9ngd9pZng https://youtu.be/UUgifzsseHQ\n  https://youtu.be/L7G84E7jcoY https://youtu.be/RVOU5G1V7Io\n  https://youtu.be/bmXIbk8I5sk", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work develops a tracking system based on an event-based camera. A\nbioinspired filtering algorithm to reduce noise and transmitted data while\nkeeping the main features at the scene is implemented in FPGA which also serves\nas a network node. POWERLINK IEEE 61158 industrial network is used to\ncommunicate the FPGA with a controller connected to a self-developed two axis\nservo-controlled robot. The FPGA includes the network protocol to integrate the\nevent-based camera as any other existing network node. The inverse kinematics\nfor the robot is included in the controller. In addition, another network node\nis used to control pneumatic valves blowing the ball at different speed and\ntrajectories. To complete the system and provide a comparison, a traditional\nframe-based camera is also connected to the controller. The imaging data for\nthe tracking system are obtained either from the event-based or frame-based\ncamera. Results show that the robot can accurately follow the ball using fast\nimage recognition, with the intrinsic advantages of the event-based system\n(size, price, power). This works shows how the development of new equipment and\nalgorithms can be efficiently integrated in an industrial system, merging\ncommercial industrial equipment with the new devices so that new technologies\ncan rapidly enter into the industrial field.\n", "versions": [{"version": "v1", "created": "Sat, 22 Jul 2017 16:20:37 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Barrios-Avil\u00e9s", "Juan", ""], ["Iakymchuk", "Taras", ""], ["Samaniego", "Jorge", ""], ["Rosado-Mu\u00f1oz", "Alfredo", ""]]}, {"id": "1707.07204", "submitter": "Vivek Kwatra", "authors": "Steven Hickson, Nick Dufour, Avneesh Sud, Vivek Kwatra and Irfan Essa", "title": "Eyemotion: Classifying facial expressions in VR using eye-tracking\n  cameras", "comments": "Uploaded Supplementary PDF. Fixed author affiliation. Corrected typo\n  in personalization accuracy", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  One of the main challenges of social interaction in virtual reality settings\nis that head-mounted displays occlude a large portion of the face, blocking\nfacial expressions and thereby restricting social engagement cues among users.\nHence, auxiliary means of sensing and conveying these expressions are needed.\nWe present an algorithm to automatically infer expressions by analyzing only a\npartially occluded face while the user is engaged in a virtual reality\nexperience. Specifically, we show that images of the user's eyes captured from\nan IR gaze-tracking camera within a VR headset are sufficient to infer a select\nsubset of facial expressions without the use of any fixed external camera.\nUsing these inferences, we can generate dynamic avatars in real-time which\nfunction as an expressive surrogate for the user. We propose a novel data\ncollection pipeline as well as a novel approach for increasing CNN accuracy via\npersonalization. Our results show a mean accuracy of 74% ($F1$ of 0.73) among 5\n`emotive' expressions and a mean accuracy of 70% ($F1$ of 0.68) among 10\ndistinct facial action units, outperforming human raters.\n", "versions": [{"version": "v1", "created": "Sat, 22 Jul 2017 19:39:19 GMT"}, {"version": "v2", "created": "Fri, 28 Jul 2017 19:05:50 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Hickson", "Steven", ""], ["Dufour", "Nick", ""], ["Sud", "Avneesh", ""], ["Kwatra", "Vivek", ""], ["Essa", "Irfan", ""]]}, {"id": "1707.07210", "submitter": "Julian Georg Zilly", "authors": "Julian Zilly, Amit Boyarski, Micael Carvalho, Amir Atapour Abarghouei,\n  Konstantinos Amplianitis, Aleksandr Krasnov, Massimiliano Mancini, Hern\\'an\n  Gonzalez, Riccardo Spezialetti, Carlos Sampedro P\\'erez, Hao Li", "title": "Inspiring Computer Vision System Solutions", "comments": "5 pages. 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The \"digital Michelangelo project\" was a seminal computer vision project in\nthe early 2000's that pushed the capabilities of acquisition systems and\ninvolved multiple people from diverse fields, many of whom are now leaders in\nindustry and academia. Reviewing this project with modern eyes provides us with\nthe opportunity to reflect on several issues, relevant now as then to the field\nof computer vision and research in general, that go beyond the technical\naspects of the work.\n  This article was written in the context of a reading group competition at the\nweek-long International Computer Vision Summer School 2017 (ICVSS) on Sicily,\nItaly. To deepen the participants understanding of computer vision and to\nfoster a sense of community, various reading groups were tasked to highlight\nimportant lessons which may be learned from provided literature, going beyond\nthe contents of the paper. This report is the winning entry of this guided\ndiscourse (Fig. 1). The authors closely examined the origins, fruits and most\nimportantly lessons about research in general which may be distilled from the\n\"digital Michelangelo project\". Discussions leading to this report were held\nwithin the group as well as with Hao Li, the group mentor.\n", "versions": [{"version": "v1", "created": "Sat, 22 Jul 2017 20:20:57 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Zilly", "Julian", ""], ["Boyarski", "Amit", ""], ["Carvalho", "Micael", ""], ["Abarghouei", "Amir Atapour", ""], ["Amplianitis", "Konstantinos", ""], ["Krasnov", "Aleksandr", ""], ["Mancini", "Massimiliano", ""], ["Gonzalez", "Hern\u00e1n", ""], ["Spezialetti", "Riccardo", ""], ["P\u00e9rez", "Carlos Sampedro", ""], ["Li", "Hao", ""]]}, {"id": "1707.07213", "submitter": "Suman Saha", "authors": "Suman Saha and Gurkirt Singh and Michael Sapienza and Philip H. S.\n  Torr and Fabio Cuzzolin", "title": "Spatio-temporal Human Action Localisation and Instance Segmentation in\n  Temporally Untrimmed Videos", "comments": "Typos corrected", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current state-of-the-art human action recognition is focused on the\nclassification of temporally trimmed videos in which only one action occurs per\nframe. In this work we address the problem of action localisation and instance\nsegmentation in which multiple concurrent actions of the same class may be\nsegmented out of an image sequence. We cast the action tube extraction as an\nenergy maximisation problem in which configurations of region proposals in each\nframe are assigned a cost and the best action tubes are selected via two passes\nof dynamic programming. One pass associates region proposals in space and time\nfor each action category, and another pass is used to solve for the tube's\ntemporal extent and to enforce a smooth label sequence through the video. In\naddition, by taking advantage of recent work on action foreground-background\nsegmentation, we are able to associate each tube with class-specific\nsegmentations. We demonstrate the performance of our algorithm on the\nchallenging LIRIS-HARL dataset and achieve a new state-of-the-art result which\nis 14.3 times better than previous methods.\n", "versions": [{"version": "v1", "created": "Sat, 22 Jul 2017 20:46:11 GMT"}, {"version": "v2", "created": "Sun, 6 Aug 2017 15:22:59 GMT"}], "update_date": "2017-08-08", "authors_parsed": [["Saha", "Suman", ""], ["Singh", "Gurkirt", ""], ["Sapienza", "Michael", ""], ["Torr", "Philip H. S.", ""], ["Cuzzolin", "Fabio", ""]]}, {"id": "1707.07225", "submitter": "Feng Xu", "authors": "Qian Song, Feng Xu, Ya-Qiu Jin", "title": "SAR Image Colorization: Converting Single-Polarization to Fully\n  Polarimetric Using Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A deep neural networks based method is proposed to convert single\npolarization grayscale SAR image to fully polarimetric. It consists of two\ncomponents: a feature extractor network to extract hierarchical multi-scale\nspatial features of grayscale SAR image, followed by a feature translator\nnetwork to map spatial feature to polarimetric feature with which the\npolarimetric covariance matrix of each pixel can be reconstructed. Both\nqualitative and quantitative experiments with real fully polarimetric data are\nconducted to show the efficacy of the proposed method. The reconstructed\nfull-pol SAR image agrees well with the true full-pol image. Existing PolSAR\napplications such as model-based decomposition and unsupervised classification\ncan be applied directly to the reconstructed full-pol SAR images. This\nframework can be easily extended to reconstruction of full-pol data from\ncompact-pol data. The experiment results also show that the proposed method\ncould be potentially used for interference removal on the cross-polarization\nchannel.\n", "versions": [{"version": "v1", "created": "Sat, 22 Jul 2017 22:22:25 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Song", "Qian", ""], ["Xu", "Feng", ""], ["Jin", "Ya-Qiu", ""]]}, {"id": "1707.07244", "submitter": "Wim Abbeloos", "authors": "Wim Abbeloos, Fabian Gouwens, Simon Jansen, Berend K\\\"upers, Maurice\n  Ramaker, Toon Goedem\\'e", "title": "Team Applied Robotics: A closer look at our robotic picking system", "comments": "IEEE International Conference on Robotics and Automation (ICRA),\n  Warehouse Picking Automation Workshop, May 29 to June 3, 2017, Singapore", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the vision based robotic picking system that was\ndeveloped by our team, Team Applied Robotics, for the Amazon Picking Challenge\n2016. This competition challenged teams to develop a robotic system that is\nable to pick a large variety of products from a shelve or a tote. We discuss\nthe design considerations and our strategy, the high resolution 3D vision\nsystem, the use of a combination of texture and shape-based object detection\nalgorithms, the robot path planning and object manipulators that were\ndeveloped.\n", "versions": [{"version": "v1", "created": "Sun, 23 Jul 2017 04:19:52 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Abbeloos", "Wim", ""], ["Gouwens", "Fabian", ""], ["Jansen", "Simon", ""], ["K\u00fcpers", "Berend", ""], ["Ramaker", "Maurice", ""], ["Goedem\u00e9", "Toon", ""]]}, {"id": "1707.07248", "submitter": "Hengkai Guo", "authors": "Hengkai Guo, Guijin Wang, Xinghao Chen, Cairong Zhang", "title": "Towards Good Practices for Deep 3D Hand Pose Estimation", "comments": "Extended version of arXiv:1702.02447", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D hand pose estimation from single depth image is an important and\nchallenging problem for human-computer interaction. Recently deep convolutional\nnetworks (ConvNet) with sophisticated design have been employed to address it,\nbut the improvement over traditional random forest based methods is not so\napparent. To exploit the good practice and promote the performance for hand\npose estimation, we propose a tree-structured Region Ensemble Network (REN) for\ndirectly 3D coordinate regression. It first partitions the last convolution\noutputs of ConvNet into several grid regions. The results from separate\nfully-connected (FC) regressors on each regions are then integrated by another\nFC layer to perform the estimation. By exploitation of several training\nstrategies including data augmentation and smooth $L_1$ loss, proposed REN can\nsignificantly improve the performance of ConvNet to localize hand joints. The\nexperimental results demonstrate that our approach achieves the best\nperformance among state-of-the-art algorithms on three public hand pose\ndatasets. We also experiment our methods on fingertip detection and human pose\ndatasets and obtain state-of-the-art accuracy.\n", "versions": [{"version": "v1", "created": "Sun, 23 Jul 2017 05:14:31 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Guo", "Hengkai", ""], ["Wang", "Guijin", ""], ["Chen", "Xinghao", ""], ["Zhang", "Cairong", ""]]}, {"id": "1707.07255", "submitter": "Wim Abbeloos", "authors": "Wim Abbeloos, Sergio Caccamo, Esra Ataer-Cansizoglu, Yuichi Taguchi,\n  Chen Feng, Teng-Yok Lee", "title": "Detecting and Grouping Identical Objects for Region Proposal and\n  Classification", "comments": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR),\n  Workshop Deep Learning for Robotic Vision, 21 July, 2017, Honolulu, Hawaii", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Often multiple instances of an object occur in the same scene, for example in\na warehouse. Unsupervised multi-instance object discovery algorithms are able\nto detect and identify such objects. We use such an algorithm to provide object\nproposals to a convolutional neural network (CNN) based classifier. This\nresults in fewer regions to evaluate, compared to traditional region proposal\nalgorithms. Additionally, it enables using the joint probability of multiple\ninstances of an object, resulting in improved classification accuracy. The\nproposed technique can also split a single class into multiple sub-classes\ncorresponding to the different object types, enabling hierarchical\nclassification.\n", "versions": [{"version": "v1", "created": "Sun, 23 Jul 2017 07:11:35 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Abbeloos", "Wim", ""], ["Caccamo", "Sergio", ""], ["Ataer-Cansizoglu", "Esra", ""], ["Taguchi", "Yuichi", ""], ["Feng", "Chen", ""], ["Lee", "Teng-Yok", ""]]}, {"id": "1707.07256", "submitter": "Xi Li", "authors": "Liming Zhao, Xi Li, Jingdong Wang, Yueting Zhuang", "title": "Deeply-Learned Part-Aligned Representations for Person Re-Identification", "comments": "Accepted by ICCV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the problem of person re-identification, which\nrefers to associating the persons captured from different cameras. We propose a\nsimple yet effective human part-aligned representation for handling the body\npart misalignment problem. Our approach decomposes the human body into regions\n(parts) which are discriminative for person matching, accordingly computes the\nrepresentations over the regions, and aggregates the similarities computed\nbetween the corresponding regions of a pair of probe and gallery images as the\noverall matching score. Our formulation, inspired by attention models, is a\ndeep neural network modeling the three steps together, which is learnt through\nminimizing the triplet loss function without requiring body part labeling\ninformation. Unlike most existing deep learning algorithms that learn a global\nor spatial partition-based local representation, our approach performs human\nbody partition, and thus is more robust to pose changes and various human\nspatial distributions in the person bounding box. Our approach shows\nstate-of-the-art results over standard datasets, Market-$1501$, CUHK$03$,\nCUHK$01$ and VIPeR.\n", "versions": [{"version": "v1", "created": "Sun, 23 Jul 2017 07:25:07 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Zhao", "Liming", ""], ["Li", "Xi", ""], ["Wang", "Jingdong", ""], ["Zhuang", "Yueting", ""]]}, {"id": "1707.07301", "submitter": "Xi Li", "authors": "Shanshan Zhao, Xi Li and Omar El Farouk Bourahla", "title": "Deep Optical Flow Estimation Via Multi-Scale Correspondence Structure\n  Learning", "comments": "7 pages, 3 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As an important and challenging problem in computer vision, learning based\noptical flow estimation aims to discover the intrinsic correspondence structure\nbetween two adjacent video frames through statistical learning. Therefore, a\nkey issue to solve in this area is how to effectively model the multi-scale\ncorrespondence structure properties in an adaptive end-to-end learning fashion.\nMotivated by this observation, we propose an end-to-end multi-scale\ncorrespondence structure learning (MSCSL) approach for optical flow estimation.\nIn principle, the proposed MSCSL approach is capable of effectively capturing\nthe multi-scale inter-image-correlation correspondence structures within a\nmulti-level feature space from deep learning. Moreover, the proposed MSCSL\napproach builds a spatial Conv-GRU neural network model to adaptively model the\nintrinsic dependency relationships among these multi-scale correspondence\nstructures. Finally, the above procedures for correspondence structure learning\nand multi-scale dependency modeling are implemented in a unified end-to-end\ndeep learning framework. Experimental results on several benchmark datasets\ndemonstrate the effectiveness of the proposed approach.\n", "versions": [{"version": "v1", "created": "Sun, 23 Jul 2017 14:08:54 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Zhao", "Shanshan", ""], ["Li", "Xi", ""], ["Bourahla", "Omar El Farouk", ""]]}, {"id": "1707.07310", "submitter": "Hiroki Sayama", "authors": "Hiroki Sayama, Farnaz Zamani Esfahlani, Ali Jazayeri, J. Scott Turner", "title": "Robust Tracking and Behavioral Modeling of Movements of Biological\n  Collectives from Ordinary Video Recordings", "comments": "8 pages, 14 figures; to be published in the Proceedings of the IEEE\n  SSCI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.CV nlin.AO q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel computational method to extract information about\ninteractions among individuals with different behavioral states in a biological\ncollective from ordinary video recordings. Assuming that individuals are acting\nas finite state machines, our method first detects discrete behavioral states\nof those individuals and then constructs a model of their state transitions,\ntaking into account the positions and states of other individuals in the\nvicinity. We have tested the proposed method through applications to two\nreal-world biological collectives: termites in an experimental setting and\nhuman pedestrians in a university campus. For each application, a robust\ntracking system was developed in-house, utilizing interactive human\nintervention (for termite tracking) or online agent-based simulation (for\npedestrian tracking). In both cases, significant interactions were detected\nbetween nearby individuals with different states, demonstrating the\neffectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Sun, 23 Jul 2017 14:55:21 GMT"}, {"version": "v2", "created": "Sat, 30 Sep 2017 13:48:00 GMT"}], "update_date": "2017-10-03", "authors_parsed": [["Sayama", "Hiroki", ""], ["Esfahlani", "Farnaz Zamani", ""], ["Jazayeri", "Ali", ""], ["Turner", "J. Scott", ""]]}, {"id": "1707.07312", "submitter": "Kaylen Pfisterer", "authors": "Kaylen J. Pfisterer, Robert Amelard, Audrey G. Chung, Alexander Wong", "title": "A new take on measuring relative nutritional density: The feasibility of\n  using a deep neural network to assess commercially-prepared pureed food\n  concentrations", "comments": null, "journal-ref": null, "doi": "10.1016/j.jfoodeng.2017.10.016", "report-no": null, "categories": "cs.CV physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dysphagia affects 590 million people worldwide and increases risk for\nmalnutrition. Pureed food may reduce choking, however preparation differences\nimpact nutrient density making quality assurance necessary. This paper is the\nfirst study to investigate the feasibility of computational pureed food\nnutritional density analysis using an imaging system. Motivated by a\ntheoretical optical dilution model, a novel deep neural network (DNN) was\nevaluated using 390 samples from thirteen types of commercially prepared purees\nat five dilutions. The DNN predicted relative concentration of the puree sample\n(20%, 40%, 60%, 80%, 100% initial concentration). Data were captured using\nsame-side reflectance of multispectral imaging data at different polarizations\nat three exposures. Experimental results yielded an average top-1 prediction\naccuracy of 92.2+/-0.41% with sensitivity and specificity of 83.0+/-15.0% and\n95.0+/-4.8%, respectively. This DNN imaging system for nutrient density\nanalysis of pureed food shows promise as a novel tool for nutrient quality\nassurance.\n", "versions": [{"version": "v1", "created": "Sun, 23 Jul 2017 14:57:23 GMT"}, {"version": "v2", "created": "Fri, 3 Nov 2017 14:54:09 GMT"}], "update_date": "2017-11-06", "authors_parsed": [["Pfisterer", "Kaylen J.", ""], ["Amelard", "Robert", ""], ["Chung", "Audrey G.", ""], ["Wong", "Alexander", ""]]}, {"id": "1707.07321", "submitter": "Xinyi Tong", "authors": "Xin-Yi Tong, Gui-Song Xia, Fan Hu, Yanfei Zhong, Mihai Datcu, Liangpei\n  Zhang", "title": "Exploiting Deep Features for Remote Sensing Image Retrieval: A\n  Systematic Investigation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Remote sensing (RS) image retrieval is of great significant for geological\ninformation mining. Over the past two decades, a large amount of research on\nthis task has been carried out, which mainly focuses on the following three\ncore issues: feature extraction, similarity metric and relevance feedback. Due\nto the complexity and multiformity of ground objects in high-resolution remote\nsensing (HRRS) images, there is still room for improvement in the current\nretrieval approaches. In this paper, we analyze the three core issues of RS\nimage retrieval and provide a comprehensive review on existing methods.\nFurthermore, for the goal to advance the state-of-the-art in HRRS image\nretrieval, we focus on the feature extraction issue and delve how to use\npowerful deep representations to address this task. We conduct systematic\ninvestigation on evaluating correlative factors that may affect the performance\nof deep features. By optimizing each factor, we acquire remarkable retrieval\nresults on publicly available HRRS datasets. Finally, we explain the\nexperimental phenomenon in detail and draw conclusions according to our\nanalysis. Our work can serve as a guiding role for the research of\ncontent-based RS image retrieval.\n", "versions": [{"version": "v1", "created": "Sun, 23 Jul 2017 16:57:56 GMT"}, {"version": "v2", "created": "Sun, 15 Jul 2018 01:47:53 GMT"}, {"version": "v3", "created": "Wed, 20 Nov 2019 20:37:28 GMT"}], "update_date": "2019-11-22", "authors_parsed": [["Tong", "Xin-Yi", ""], ["Xia", "Gui-Song", ""], ["Hu", "Fan", ""], ["Zhong", "Yanfei", ""], ["Datcu", "Mihai", ""], ["Zhang", "Liangpei", ""]]}, {"id": "1707.07336", "submitter": "Alireza Rahimpour", "authors": "Alireza Rahimpour, Liu Liu, Ali Taalimi, Yang Song, Hairong Qi", "title": "Person Re-identification Using Visual Attention", "comments": "Published at IEEE International Conference on Image Processing 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite recent attempts for solving the person re-identification problem, it\nremains a challenging task since a person's appearance can vary significantly\nwhen large variations in view angle, human pose, and illumination are involved.\nIn this paper, we propose a novel approach based on using a gradient-based\nattention mechanism in deep convolution neural network for solving the person\nre-identification problem. Our model learns to focus selectively on parts of\nthe input image for which the networks' output is most sensitive to and\nprocesses them with high resolution while perceiving the surrounding image in\nlow resolution. Extensive comparative evaluations demonstrate that the proposed\nmethod outperforms state-of-the-art approaches on the challenging CUHK01,\nCUHK03, and Market 1501 datasets.\n", "versions": [{"version": "v1", "created": "Sun, 23 Jul 2017 19:36:39 GMT"}, {"version": "v2", "created": "Thu, 3 Aug 2017 00:16:26 GMT"}, {"version": "v3", "created": "Sun, 6 Aug 2017 20:13:16 GMT"}, {"version": "v4", "created": "Thu, 14 Sep 2017 20:03:32 GMT"}, {"version": "v5", "created": "Sun, 1 Apr 2018 19:58:14 GMT"}, {"version": "v6", "created": "Mon, 25 Jun 2018 03:08:25 GMT"}, {"version": "v7", "created": "Mon, 29 Apr 2019 04:15:22 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Rahimpour", "Alireza", ""], ["Liu", "Liu", ""], ["Taalimi", "Ali", ""], ["Song", "Yang", ""], ["Qi", "Hairong", ""]]}, {"id": "1707.07360", "submitter": "Jhony Kaesemodel Pontes", "authors": "Jhony K. Pontes, Chen Kong, Anders Eriksson, Clinton Fookes, Sridha\n  Sridharan, Simon Lucey", "title": "Compact Model Representation for 3D Reconstruction", "comments": "9 pages, 6 figures", "journal-ref": "2017 International Conference on 3D Vision (3DV)", "doi": "10.1109/3DV.2017.00020", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D reconstruction from 2D images is a central problem in computer vision.\nRecent works have been focusing on reconstruction directly from a single image.\nIt is well known however that only one image cannot provide enough information\nfor such a reconstruction. A prior knowledge that has been entertained are 3D\nCAD models due to its online ubiquity. A fundamental question is how to\ncompactly represent millions of CAD models while allowing generalization to new\nunseen objects with fine-scaled geometry. We introduce an approach to compactly\nrepresent a 3D mesh. Our method first selects a 3D model from a graph structure\nby using a novel free-form deformation FFD 3D-2D registration, and then the\nselected 3D model is refined to best fit the image silhouette. We perform a\ncomprehensive quantitative and qualitative analysis that demonstrates\nimpressive dense and realistic 3D reconstruction from single images.\n", "versions": [{"version": "v1", "created": "Sun, 23 Jul 2017 22:50:06 GMT"}], "update_date": "2019-01-25", "authors_parsed": [["Pontes", "Jhony K.", ""], ["Kong", "Chen", ""], ["Eriksson", "Anders", ""], ["Fookes", "Clinton", ""], ["Sridharan", "Sridha", ""], ["Lucey", "Simon", ""]]}, {"id": "1707.07381", "submitter": "Xi Li", "authors": "Lina Wei, Shanshan Zhao, Omar El Farouk Bourahla, Xi Li, Fei Wu", "title": "Group-wise Deep Co-saliency Detection", "comments": "IJCAI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an end-to-end group-wise deep co-saliency detection\napproach to address the co-salient object discovery problem based on the fully\nconvolutional network (FCN) with group input and group output. The proposed\napproach captures the group-wise interaction information for group images by\nlearning a semantics-aware image representation based on a convolutional neural\nnetwork, which adaptively learns the group-wise features for co-saliency\ndetection. Furthermore, the proposed approach discovers the collaborative and\ninteractive relationships between group-wise feature representation and\nsingle-image individual feature representation, and model this in a\ncollaborative learning framework. Finally, we set up a unified end-to-end deep\nlearning scheme to jointly optimize the process of group-wise feature\nrepresentation learning and the collaborative learning, leading to more\nreliable and robust co-saliency detection results. Experimental results\ndemonstrate the effectiveness of our approach in comparison with the\nstate-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jul 2017 02:32:43 GMT"}, {"version": "v2", "created": "Tue, 25 Jul 2017 05:58:49 GMT"}], "update_date": "2017-07-26", "authors_parsed": [["Wei", "Lina", ""], ["Zhao", "Shanshan", ""], ["Bourahla", "Omar El Farouk", ""], ["Li", "Xi", ""], ["Wu", "Fei", ""]]}, {"id": "1707.07388", "submitter": "Shichao Yang", "authors": "Shichao Yang, Yulan Huang, Sebastian Scherer", "title": "Semantic 3D Occupancy Mapping through Efficient High Order CRFs", "comments": "IROS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic 3D mapping can be used for many applications such as robot\nnavigation and virtual interaction. In recent years, there has been great\nprogress in semantic segmentation and geometric 3D mapping. However, it is\nstill challenging to combine these two tasks for accurate and large-scale\nsemantic mapping from images. In the paper, we propose an incremental and\n(near) real-time semantic mapping system. A 3D scrolling occupancy grid map is\nbuilt to represent the world, which is memory and computationally efficient and\nbounded for large scale environments. We utilize the CNN segmentation as prior\nprediction and further optimize 3D grid labels through a novel CRF model.\nSuperpixels are utilized to enforce smoothness and form robust P N high order\npotential. An efficient mean field inference is developed for the graph\noptimization. We evaluate our system on the KITTI dataset and improve the\nsegmentation accuracy by 10% over existing systems.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jul 2017 03:24:52 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Yang", "Shichao", ""], ["Huang", "Yulan", ""], ["Scherer", "Sebastian", ""]]}, {"id": "1707.07391", "submitter": "Ce Qi", "authors": "Ce Qi, Fei Su", "title": "Contrastive-center loss for deep neural networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The deep convolutional neural network(CNN) has significantly raised the\nperformance of image classification and face recognition. Softmax is usually\nused as supervision, but it only penalizes the classification loss. In this\npaper, we propose a novel auxiliary supervision signal called contrastivecenter\nloss, which can further enhance the discriminative power of the features, for\nit learns a class center for each class. The proposed contrastive-center loss\nsimultaneously considers intra-class compactness and inter-class separability,\nby penalizing the contrastive values between: (1)the distances of training\nsamples to their corresponding class centers, and (2)the sum of the distances\nof training samples to their non-corresponding class centers. Experiments on\ndifferent datasets demonstrate the effectiveness of contrastive-center loss.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jul 2017 03:45:47 GMT"}, {"version": "v2", "created": "Wed, 23 Aug 2017 12:17:30 GMT"}], "update_date": "2017-08-24", "authors_parsed": [["Qi", "Ce", ""], ["Su", "Fei", ""]]}, {"id": "1707.07394", "submitter": "Shin Fujieda", "authors": "Shin Fujieda, Kohei Takayama and Toshiya Hachisuka", "title": "Wavelet Convolutional Neural Networks for Texture Classification", "comments": "9 pages, 7 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Texture classification is an important and challenging problem in many image\nprocessing applications. While convolutional neural networks (CNNs) achieved\nsignificant successes for image classification, texture classification remains\na difficult problem since textures usually do not contain enough information\nregarding the shape of object. In image processing, texture classification has\nbeen traditionally studied well with spectral analyses which exploit repeated\nstructures in many textures. Since CNNs process images as-is in the spatial\ndomain whereas spectral analyses process images in the frequency domain, these\nmodels have different characteristics in terms of performance. We propose a\nnovel CNN architecture, wavelet CNNs, which integrates a spectral analysis into\nCNNs. Our insight is that the pooling layer and the convolution layer can be\nviewed as a limited form of a spectral analysis. Based on this insight, we\ngeneralize both layers to perform a spectral analysis with wavelet transform.\nWavelet CNNs allow us to utilize spectral information which is lost in\nconventional CNNs but useful in texture classification. The experiments\ndemonstrate that our model achieves better accuracy in texture classification\nthan existing models. We also show that our model has significantly fewer\nparameters than CNNs, making our model easier to train with less memory.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jul 2017 03:59:04 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Fujieda", "Shin", ""], ["Takayama", "Kohei", ""], ["Hachisuka", "Toshiya", ""]]}, {"id": "1707.07397", "submitter": "Anish Athalye", "authors": "Anish Athalye and Logan Engstrom and Andrew Ilyas and Kevin Kwok", "title": "Synthesizing Robust Adversarial Examples", "comments": "ICML 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Standard methods for generating adversarial examples for neural networks do\nnot consistently fool neural network classifiers in the physical world due to a\ncombination of viewpoint shifts, camera noise, and other natural\ntransformations, limiting their relevance to real-world systems. We demonstrate\nthe existence of robust 3D adversarial objects, and we present the first\nalgorithm for synthesizing examples that are adversarial over a chosen\ndistribution of transformations. We synthesize two-dimensional adversarial\nimages that are robust to noise, distortion, and affine transformation. We\napply our algorithm to complex three-dimensional objects, using 3D-printing to\nmanufacture the first physical adversarial objects. Our results demonstrate the\nexistence of 3D adversarial objects in the physical world.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jul 2017 04:17:33 GMT"}, {"version": "v2", "created": "Mon, 30 Oct 2017 14:58:29 GMT"}, {"version": "v3", "created": "Thu, 7 Jun 2018 16:25:12 GMT"}], "update_date": "2018-06-08", "authors_parsed": [["Athalye", "Anish", ""], ["Engstrom", "Logan", ""], ["Ilyas", "Andrew", ""], ["Kwok", "Kevin", ""]]}, {"id": "1707.07410", "submitter": "Daniel DeTone", "authors": "Daniel DeTone, Tomasz Malisiewicz, Andrew Rabinovich", "title": "Toward Geometric Deep SLAM", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a point tracking system powered by two deep convolutional neural\nnetworks. The first network, MagicPoint, operates on single images and extracts\nsalient 2D points. The extracted points are \"SLAM-ready\" because they are by\ndesign isolated and well-distributed throughout the image. We compare this\nnetwork against classical point detectors and discover a significant\nperformance gap in the presence of image noise. As transformation estimation is\nmore simple when the detected points are geometrically stable, we designed a\nsecond network, MagicWarp, which operates on pairs of point images (outputs of\nMagicPoint), and estimates the homography that relates the inputs. This\ntransformation engine differs from traditional approaches because it does not\nuse local point descriptors, only point locations. Both networks are trained\nwith simple synthetic data, alleviating the requirement of expensive external\ncamera ground truthing and advanced graphics rendering pipelines. The system is\nfast and lean, easily running 30+ FPS on a single CPU.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jul 2017 05:41:35 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["DeTone", "Daniel", ""], ["Malisiewicz", "Tomasz", ""], ["Rabinovich", "Andrew", ""]]}, {"id": "1707.07411", "submitter": "Fangyu Wu", "authors": "Fang-Yu Wu, Shi-Yang Yan, Jeremy S. Smith, Bai-Ling Zhang", "title": "Traffic scene recognition based on deep cnn and vlad spatial pyramids", "comments": "6 pages,4 figures, 2017 9th International Conference on Machine\n  Learning and Computing (ICMLC 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traffic scene recognition is an important and challenging issue in\nIntelligent Transportation Systems (ITS). Recently, Convolutional Neural\nNetwork (CNN) models have achieved great success in many applications,\nincluding scene classification. The remarkable representational learning\ncapability of CNN remains to be further explored for solving real-world\nproblems. Vector of Locally Aggregated Descriptors (VLAD) encoding has also\nproved to be a powerful method in catching global contextual information. In\nthis paper, we attempted to solve the traffic scene recognition problem by\ncombining the features representational capabilities of CNN with the VLAD\nencoding scheme. More specifically, the CNN features of image patches generated\nby a region proposal algorithm are encoded by applying VLAD, which subsequently\nrepresent an image in a compact representation. To catch the spatial\ninformation, spatial pyramids are exploited to encode CNN features. We\nexperimented with a dataset of 10 categories of traffic scenes, with\nsatisfactory categorization performances.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jul 2017 05:44:39 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Wu", "Fang-Yu", ""], ["Yan", "Shi-Yang", ""], ["Smith", "Jeremy S.", ""], ["Zhang", "Bai-Ling", ""]]}, {"id": "1707.07418", "submitter": "Zongyuan Ge", "authors": "ZongYuan Ge, Sergey Demyanov, Zetao Chen, Rahil Garnavi", "title": "Generative OpenMax for Multi-Class Open Set Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a conceptually new and flexible method for multi-class open set\nclassification. Unlike previous methods where unknown classes are inferred with\nrespect to the feature or decision distance to the known classes, our approach\nis able to provide explicit modelling and decision score for unknown classes.\nThe proposed method, called Gener- ative OpenMax (G-OpenMax), extends OpenMax\nby employing generative adversarial networks (GANs) for novel category image\nsynthesis. We validate the proposed method on two datasets of handwritten\ndigits and characters, resulting in superior results over previous deep\nlearning based method OpenMax Moreover, G-OpenMax provides a way to visualize\nsamples representing the unknown classes from open space. Our simple and\neffective approach could serve as a new direction to tackle the challenging\nmulti-class open set classification problem.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jul 2017 06:45:05 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Ge", "ZongYuan", ""], ["Demyanov", "Sergey", ""], ["Chen", "Zetao", ""], ["Garnavi", "Rahil", ""]]}, {"id": "1707.07432", "submitter": "Bruno Stuner", "authors": "Bruno Stuner, Cl\\'ement Chatelain, Thierry Paquet", "title": "LV-ROVER: Lexicon Verified Recognizer Output Voting Error Reduction", "comments": "Submitted to Pattern Recognition Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Offline handwritten text line recognition is a hard task that requires both\nan efficient optical character recognizer and language model. Handwriting\nrecognition state of the art methods are based on Long Short Term Memory (LSTM)\nrecurrent neural networks (RNN) coupled with the use of linguistic knowledge.\nMost of the proposed approaches in the literature focus on improving one of the\ntwo components and use constraint, dedicated to a database lexicon. However,\nstate of the art performance is achieved by combining multiple optical models,\nand possibly multiple language models with the Recognizer Output Voting Error\nReduction (ROVER) framework. Though handwritten line recognition with ROVER has\nbeen implemented by combining only few recognizers because training multiple\ncomplete recognizers is hard. In this paper we propose a Lexicon Verified\nROVER: LV-ROVER, that has a reduce complexity compare to the original one and\nthat can combine hundreds of recognizers without language models. We achieve\nstate of the art for handwritten line text on the RIMES dataset.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jul 2017 08:04:09 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Stuner", "Bruno", ""], ["Chatelain", "Cl\u00e9ment", ""], ["Paquet", "Thierry", ""]]}, {"id": "1707.07438", "submitter": "Nicola Strisciuglio", "authors": "Nicola Strisciuglio and Nicolai Petkov", "title": "Delineation of line patterns in images using B-COSFIRE filters", "comments": "International Work Conference on Bioinspired Intelligence, July\n  10-13, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Delineation of line patterns in images is a basic step required in various\napplications such as blood vessel detection in medical images, segmentation of\nrivers or roads in aerial images, detection of cracks in walls or pavements,\netc. In this paper we present trainable B-COSFIRE filters, which are a model of\nsome neurons in area V1 of the primary visual cortex, and apply it to the\ndelineation of line patterns in different kinds of images. B-COSFIRE filters\nare trainable as their selectivity is determined in an automatic configuration\nprocess given a prototype pattern of interest. They are configurable to detect\nany preferred line structure (e.g. segments, corners, cross-overs, etc.), so\nusable for automatic data representation learning. We carried out experiments\non two data sets, namely a line-network data set from INRIA and a data set of\nretinal fundus images named IOSTAR. The results that we achieved confirm the\nrobustness of the proposed approach and its effectiveness in the delineation of\nline structures in different kinds of images.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jul 2017 08:28:59 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Strisciuglio", "Nicola", ""], ["Petkov", "Nicolai", ""]]}, {"id": "1707.07538", "submitter": "Giorgio Roffo", "authors": "Giorgio Roffo, Simone Melzi, Umberto Castellani, Alessandro\n  Vinciarelli", "title": "Infinite Latent Feature Selection: A Probabilistic Latent Graph-Based\n  Ranking Approach", "comments": "Accepted at the IEEE International Conference on Computer Vision\n  (ICCV), 2017, Venice. Preprint copy", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature selection is playing an increasingly significant role with respect to\nmany computer vision applications spanning from object recognition to visual\nobject tracking. However, most of the recent solutions in feature selection are\nnot robust across different and heterogeneous set of data. In this paper, we\naddress this issue proposing a robust probabilistic latent graph-based feature\nselection algorithm that performs the ranking step while considering all the\npossible subsets of features, as paths on a graph, bypassing the combinatorial\nproblem analytically. An appealing characteristic of the approach is that it\naims to discover an abstraction behind low-level sensory data, that is,\nrelevancy. Relevancy is modelled as a latent variable in a PLSA-inspired\ngenerative process that allows the investigation of the importance of a feature\nwhen injected into an arbitrary set of cues. The proposed method has been\ntested on ten diverse benchmarks, and compared against eleven state of the art\nfeature selection methods. Results show that the proposed approach attains the\nhighest performance levels across many different scenarios and difficulties,\nthereby confirming its strong robustness while setting a new state of the art\nin feature selection domain.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jul 2017 13:21:25 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Roffo", "Giorgio", ""], ["Melzi", "Simone", ""], ["Castellani", "Umberto", ""], ["Vinciarelli", "Alessandro", ""]]}, {"id": "1707.07548", "submitter": "Yinghao Huang", "authors": "Yinghao Huang, Federica Bogo, Christoph Lassner, Angjoo Kanazawa,\n  Peter V. Gehler, Ijaz Akhter, Michael J. Black", "title": "Towards Accurate Markerless Human Shape and Pose Estimation over Time", "comments": "10 pages, 6 figures, 5 tables, published in 3DV-2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing marker-less motion capture methods often assume known backgrounds,\nstatic cameras, and sequence specific motion priors, which narrows its\napplication scenarios. Here we propose a fully automatic method that given\nmulti-view video, estimates 3D human motion and body shape. We take recent\nSMPLify \\cite{bogo2016keep} as the base method, and extend it in several ways.\nFirst we fit the body to 2D features detected in multi-view images. Second, we\nuse a CNN method to segment the person in each image and fit the 3D body model\nto the contours to further improves accuracy. Third we utilize a generic and\nrobust DCT temporal prior to handle the left and right side swapping issue\nsometimes introduced by the 2D pose estimator. Validation on standard\nbenchmarks shows our results are comparable to the state of the art and also\nprovide a realistic 3D shape avatar. We also demonstrate accurate results on\nHumanEva and on challenging dance sequences from YouTube in monocular case.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jul 2017 13:31:37 GMT"}, {"version": "v2", "created": "Wed, 26 Jul 2017 19:28:39 GMT"}, {"version": "v3", "created": "Mon, 18 Dec 2017 09:57:36 GMT"}, {"version": "v4", "created": "Wed, 20 Dec 2017 13:07:03 GMT"}, {"version": "v5", "created": "Mon, 30 Apr 2018 12:19:54 GMT"}], "update_date": "2018-05-01", "authors_parsed": [["Huang", "Yinghao", ""], ["Bogo", "Federica", ""], ["Lassner", "Christoph", ""], ["Kanazawa", "Angjoo", ""], ["Gehler", "Peter V.", ""], ["Akhter", "Ijaz", ""], ["Black", "Michael J.", ""]]}, {"id": "1707.07565", "submitter": "Thomas Wollmann", "authors": "Thomas Wollmann, Karl Rohr", "title": "Automatic breast cancer grading in lymph nodes using a deep neural\n  network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The progression of breast cancer can be quantified in lymph node whole-slide\nimages (WSIs). We describe a novel method for effectively performing\nclassification of whole-slide images and patient level breast cancer grading.\nOur method utilises a deep neural network. The method performs classification\non small patches and uses model averaging for boosting. In the first step,\nregion of interest patches are determined and cropped automatically by color\nthresholding and then classified by the deep neural network. The classification\nresults are used to determine a slide level class and for further aggregation\nto predict a patient level grade. Fast processing speed of our method enables\nhigh throughput image analysis.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jul 2017 14:09:47 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Wollmann", "Thomas", ""], ["Rohr", "Karl", ""]]}, {"id": "1707.07584", "submitter": "Xu Zhao", "authors": "Xu Zhao, Yingying Chen, Ming Tang, Jinqiao Wang", "title": "Joint Background Reconstruction and Foreground Segmentation via A\n  Two-stage Convolutional Neural Network", "comments": "ICME 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Foreground segmentation in video sequences is a classic topic in computer\nvision. Due to the lack of semantic and prior knowledge, it is difficult for\nexisting methods to deal with sophisticated scenes well. Therefore, in this\npaper, we propose an end-to-end two-stage deep convolutional neural network\n(CNN) framework for foreground segmentation in video sequences. In the first\nstage, a convolutional encoder-decoder sub-network is employed to reconstruct\nthe background images and encode rich prior knowledge of background scenes. In\nthe second stage, the reconstructed background and current frame are input into\na multi-channel fully-convolutional sub-network (MCFCN) for accurate foreground\nsegmentation. In the two-stage CNN, the reconstruction loss and segmentation\nloss are jointly optimized. The background images and foreground objects are\noutput simultaneously in an end-to-end way. Moreover, by incorporating the\nprior semantic knowledge of foreground and background in the pre-training\nprocess, our method could restrain the background noise and keep the integrity\nof foreground objects at the same time. Experiments on CDNet 2014 show that our\nmethod outperforms the state-of-the-art by 4.9%.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jul 2017 14:45:58 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Zhao", "Xu", ""], ["Chen", "Yingying", ""], ["Tang", "Ming", ""], ["Wang", "Jinqiao", ""]]}, {"id": "1707.07601", "submitter": "Spandana Gella", "authors": "Spandana Gella, Rico Sennrich, Frank Keller, Mirella Lapata", "title": "Image Pivoting for Learning Multilingual Multimodal Representations", "comments": "7 pages, EMNLP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper we propose a model to learn multimodal multilingual\nrepresentations for matching images and sentences in different languages, with\nthe aim of advancing multilingual versions of image search and image\nunderstanding. Our model learns a common representation for images and their\ndescriptions in two different languages (which need not be parallel) by\nconsidering the image as a pivot between two languages. We introduce a new\npairwise ranking loss function which can handle both symmetric and asymmetric\nsimilarity between the two modalities. We evaluate our models on\nimage-description ranking for German and English, and on semantic textual\nsimilarity of image descriptions in English. In both cases we achieve\nstate-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jul 2017 15:08:13 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Gella", "Spandana", ""], ["Sennrich", "Rico", ""], ["Keller", "Frank", ""], ["Lapata", "Mirella", ""]]}, {"id": "1707.07608", "submitter": "Markus D. Solbach", "authors": "Markus D. Solbach and John K. Tsotsos", "title": "Vision-Based Fallen Person Detection for the Elderly", "comments": "Accepted at ACVR 2017 Project page:\n  https://github.com/TsotsosLab/fallen-person-detector", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Falls are serious and costly for elderly people. The Centers for Disease\nControl and Prevention of the US reports that millions of older people, 65 and\nolder, fall each year at least once. Serious injuries such as; hip fractures,\nbroken bones or head injury, are caused by 20% of the falls. The time it takes\nto respond and treat a fallen person is crucial. With this paper we present a\nnew , non-invasive system for fallen people detection. Our approach uses only\nstereo camera data for passively sensing the environment. The key novelty is a\nhuman fall detector which uses a CNN based human pose estimator in combination\nwith stereo data to reconstruct the human pose in 3D and estimate the ground\nplane in 3D. Furthermore, our system consists of a reasoning module which\nformulates a number of measures to reason whether a person is fallen. We have\ntested our approach in different scenarios covering most activities elderly\npeople might encounter living at home. Based on our extensive evaluations, our\nsystems shows high accuracy and almost no miss-classification. To reproduce our\nresults, the implementation is publicly available to the scientific community.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jul 2017 15:33:49 GMT"}, {"version": "v2", "created": "Tue, 15 Aug 2017 14:28:06 GMT"}], "update_date": "2017-08-16", "authors_parsed": [["Solbach", "Markus D.", ""], ["Tsotsos", "John K.", ""]]}, {"id": "1707.07734", "submitter": "Eugene Vorontsov", "authors": "Eugene Vorontsov, An Tang, Chris Pal, Samuel Kadoury", "title": "Liver lesion segmentation informed by joint liver segmentation", "comments": "Late upload of conference version (ISBI)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a model for the joint segmentation of the liver and liver lesions\nin computed tomography (CT) volumes. We build the model from two fully\nconvolutional networks, connected in tandem and trained together end-to-end. We\nevaluate our approach on the 2017 MICCAI Liver Tumour Segmentation Challenge,\nattaining competitive liver and liver lesion detection and segmentation scores\nacross a wide range of metrics. Unlike other top performing methods, our model\noutput post-processing is trivial, we do not use data external to the\nchallenge, and we propose a simple single-stage model that is trained\nend-to-end. However, our method nearly matches the top lesion segmentation\nperformance and achieves the second highest precision for lesion detection\nwhile maintaining high recall.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jul 2017 20:03:46 GMT"}, {"version": "v2", "created": "Mon, 20 Nov 2017 17:19:47 GMT"}, {"version": "v3", "created": "Sat, 11 Aug 2018 21:31:45 GMT"}], "update_date": "2018-08-14", "authors_parsed": [["Vorontsov", "Eugene", ""], ["Tang", "An", ""], ["Pal", "Chris", ""], ["Kadoury", "Samuel", ""]]}, {"id": "1707.07747", "submitter": "Nicola Strisciuglio", "authors": "Nicola Strisciuglio, George Azzopardi and Nicolai Petkov", "title": "Detection of curved lines with B-COSFIRE filters: A case study on crack\n  delineation", "comments": "Accepted at Computer Analysis of Images and Patterns (CAIP) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The detection of curvilinear structures is an important step for various\ncomputer vision applications, ranging from medical image analysis for\nsegmentation of blood vessels, to remote sensing for the identification of\nroads and rivers, and to biometrics and robotics, among others. %The visual\nsystem of the brain has remarkable abilities to detect curvilinear structures\nin noisy images. This is a nontrivial task especially for the detection of thin\nor incomplete curvilinear structures surrounded with noise. We propose a\ngeneral purpose curvilinear structure detector that uses the brain-inspired\ntrainable B-COSFIRE filters. It consists of four main steps, namely nonlinear\nfiltering with B-COSFIRE, thinning with non-maximum suppression, hysteresis\nthresholding and morphological closing. We demonstrate its effectiveness on a\ndata set of noisy images with cracked pavements, where we achieve\nstate-of-the-art results (F-measure=0.865). The proposed method can be employed\nin any computer vision methodology that requires the delineation of curvilinear\nand elongated structures.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jul 2017 20:36:55 GMT"}], "update_date": "2017-07-26", "authors_parsed": [["Strisciuglio", "Nicola", ""], ["Azzopardi", "George", ""], ["Petkov", "Nicolai", ""]]}, {"id": "1707.07791", "submitter": "De Cheng", "authors": "De Cheng, Yihong Gong, Zhihui Li, Weiwei Shi, Alexander G. Hauptmann\n  and Nanning Zheng", "title": "Deep Feature Learning via Structured Graph Laplacian Embedding for\n  Person Re-Identification", "comments": "9 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning the distance metric between pairs of examples is of great importance\nfor visual recognition, especially for person re-identification (Re-Id).\nRecently, the contrastive and triplet loss are proposed to enhance the\ndiscriminative power of the deeply learned features, and have achieved\nremarkable success. As can be seen, either the contrastive or triplet loss is\njust one special case of the Euclidean distance relationships among these\ntraining samples. Therefore, we propose a structured graph Laplacian embedding\nalgorithm, which can formulate all these structured distance relationships into\nthe graph Laplacian form. The proposed method can take full advantages of the\nstructured distance relationships among these training samples, with the\nconstructed complete graph. Besides, this formulation makes our method\neasy-to-implement and super-effective. When embedding the proposed algorithm\nwith the softmax loss for the CNN training, our method can obtain much more\nrobust and discriminative deep features with inter-personal dispersion and\nintra-personal compactness, which is essential to person Re-Id. We illustrate\nthe effectiveness of our proposed method on top of three popular networks,\nnamely AlexNet, DGDNet and ResNet50, on recent four widely used Re-Id benchmark\ndatasets. Our proposed method achieves state-of-the-art performances.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jul 2017 02:28:22 GMT"}], "update_date": "2017-07-26", "authors_parsed": [["Cheng", "De", ""], ["Gong", "Yihong", ""], ["Li", "Zhihui", ""], ["Shi", "Weiwei", ""], ["Hauptmann", "Alexander G.", ""], ["Zheng", "Nanning", ""]]}, {"id": "1707.07815", "submitter": "Xi Li", "authors": "Lina Wei, Fangfang Wang, Xi Li, Fei Wu, Jun Xiao", "title": "Graph-Theoretic Spatiotemporal Context Modeling for Video Saliency\n  Detection", "comments": "ICIP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As an important and challenging problem in computer vision, video saliency\ndetection is typically cast as a spatiotemporal context modeling problem over\nconsecutive frames. As a result, a key issue in video saliency detection is how\nto effectively capture the intrinsical properties of atomic video structures as\nwell as their associated contextual interactions along the spatial and temporal\ndimensions. Motivated by this observation, we propose a graph-theoretic video\nsaliency detection approach based on adaptive video structure discovery, which\nis carried out within a spatiotemporal atomic graph. Through graph-based\nmanifold propagation, the proposed approach is capable of effectively modeling\nthe semantically contextual interactions among atomic video structures for\nsaliency detection while preserving spatial smoothness and temporal\nconsistency. Experiments demonstrate the effectiveness of the proposed approach\nover several benchmark datasets.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jul 2017 05:36:05 GMT"}], "update_date": "2017-07-26", "authors_parsed": [["Wei", "Lina", ""], ["Wang", "Fangfang", ""], ["Li", "Xi", ""], ["Wu", "Fei", ""], ["Xiao", "Jun", ""]]}, {"id": "1707.07819", "submitter": "Lingxi Xie", "authors": "Jianyu Wang, Cihang Xie, Zhishuai Zhang, Jun Zhu, Lingxi Xie, Alan\n  Yuille", "title": "Detecting Semantic Parts on Partially Occluded Objects", "comments": "Accepted to BMVC 2017 (13 pages, 3 figures)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the task of detecting semantic parts on partially\noccluded objects. We consider a scenario where the model is trained using\nnon-occluded images but tested on occluded images. The motivation is that there\nare infinite number of occlusion patterns in real world, which cannot be fully\ncovered in the training data. So the models should be inherently robust and\nadaptive to occlusions instead of fitting / learning the occlusion patterns in\nthe training data. Our approach detects semantic parts by accumulating the\nconfidence of local visual cues. Specifically, the method uses a simple voting\nmethod, based on log-likelihood ratio tests and spatial constraints, to combine\nthe evidence of local cues. These cues are called visual concepts, which are\nderived by clustering the internal states of deep networks. We evaluate our\nvoting scheme on the VehicleSemanticPart dataset with dense part annotations.\nWe randomly place two, three or four irrelevant objects onto the target object\nto generate testing images with various occlusions. Experiments show that our\nalgorithm outperforms several competitors in semantic part detection when\nocclusions are present.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jul 2017 05:54:01 GMT"}], "update_date": "2017-07-26", "authors_parsed": [["Wang", "Jianyu", ""], ["Xie", "Cihang", ""], ["Zhang", "Zhishuai", ""], ["Zhu", "Jun", ""], ["Xie", "Lingxi", ""], ["Yuille", "Alan", ""]]}, {"id": "1707.07825", "submitter": "Arun Mukundan", "authors": "Arun Mukundan, Giorgos Tolias, Ondrej Chum", "title": "Multiple-Kernel Local-Patch Descriptor", "comments": "To appear in the British Machine Vision Conference (BMVC), September\n  2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a multiple-kernel local-patch descriptor based on efficient match\nkernels of patch gradients. It combines two parametrizations of gradient\nposition and direction, each parametrization provides robustness to a different\ntype of patch miss-registration: polar parametrization for noise in the patch\ndominant orientation detection, Cartesian for imprecise location of the feature\npoint. Even though handcrafted, the proposed method consistently outperforms\nthe state-of-the-art methods on two local patch benchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jul 2017 06:15:41 GMT"}], "update_date": "2017-07-26", "authors_parsed": [["Mukundan", "Arun", ""], ["Tolias", "Giorgos", ""], ["Chum", "Ondrej", ""]]}, {"id": "1707.07830", "submitter": "Zhun Sun", "authors": "Zhun Sun, Mete Ozay, Takayuki Okatani", "title": "Improving Robustness of Feature Representations to Image Deformations\n  using Powered Convolution in CNNs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we address the problem of improvement of robustness of feature\nrepresentations learned using convolutional neural networks (CNNs) to image\ndeformation. We argue that higher moment statistics of feature distributions\ncould be shifted due to image deformations, and the shift leads to degrade of\nperformance and cannot be reduced by ordinary normalization methods as observed\nin experimental analyses. In order to attenuate this effect, we apply\nadditional non-linearity in CNNs by combining power functions with learnable\nparameters into convolution operation. In the experiments, we observe that CNNs\nwhich employ the proposed method obtain remarkable boost in both the\ngeneralization performance and the robustness under various types of\ndeformations using large scale benchmark datasets. For instance, a model\nequipped with the proposed method obtains 3.3\\% performance boost in mAP on\nPascal Voc object detection task using deformed images, compared to the\nreference model, while both models provide the same performance using original\nimages. To the best of our knowledge, this is the first work that studies\nrobustness of deep features learned using CNNs to a wide range of deformations\nfor object recognition and detection.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jul 2017 06:33:43 GMT"}], "update_date": "2017-07-26", "authors_parsed": [["Sun", "Zhun", ""], ["Ozay", "Mete", ""], ["Okatani", "Takayuki", ""]]}, {"id": "1707.07833", "submitter": "Inwan Yoo", "authors": "Inwan Yoo, David G. C. Hildebrand, Willie F. Tobin, Wei-Chung Allen\n  Lee, Won-Ki Jeong", "title": "ssEMnet: Serial-section Electron Microscopy Image Registration using a\n  Spatial Transformer Network with Learned Features", "comments": "DLMIA 2017 accepted", "journal-ref": null, "doi": "10.1007/978-3-319-67558-9_29", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The alignment of serial-section electron microscopy (ssEM) images is critical\nfor efforts in neuroscience that seek to reconstruct neuronal circuits.\nHowever, each ssEM plane contains densely packed structures that vary from one\nsection to the next, which makes matching features across images a challenge.\nAdvances in deep learning has resulted in unprecedented performance in similar\ncomputer vision problems, but to our knowledge, they have not been successfully\napplied to ssEM image co-registration. In this paper, we introduce a novel deep\nnetwork model that combines a spatial transformer for image deformation and a\nconvolutional autoencoder for unsupervised feature learning for robust ssEM\nimage alignment. This results in improved accuracy and robustness while\nrequiring substantially less user intervention than conventional methods. We\nevaluate our method by comparing registration quality across several datasets.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jul 2017 06:50:34 GMT"}, {"version": "v2", "created": "Tue, 5 Dec 2017 06:56:20 GMT"}], "update_date": "2017-12-06", "authors_parsed": [["Yoo", "Inwan", ""], ["Hildebrand", "David G. C.", ""], ["Tobin", "Willie F.", ""], ["Lee", "Wei-Chung Allen", ""], ["Jeong", "Won-Ki", ""]]}, {"id": "1707.07857", "submitter": "Xiaohua Xie", "authors": "Chunchao Guo, Jianhuang Lai, Xiaohua Xie", "title": "Motion-Appearance Interactive Encoding for Object Segmentation in\n  Unconstrained Videos", "comments": "11 pages, 7 figures", "journal-ref": null, "doi": "10.1109/TCSVT.2019.2908779", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel method of integrating motion and appearance cues for\nforeground object segmentation in unconstrained videos. Unlike conventional\nmethods encoding motion and appearance patterns individually, our method puts\nparticular emphasis on their mutual assistance. Specifically, we propose using\nan interactively constrained encoding (ICE) scheme to incorporate motion and\nappearance patterns into a graph that leads to a spatiotemporal energy\noptimization. The reason of utilizing ICE is that both motion and appearance\ncues for the same target share underlying correlative structure, thus can be\nexploited in a deeply collaborative manner. We perform ICE not only in the\ninitialization but also in the refinement stage of a two-layer framework for\nobject segmentation. This scheme allows our method to consistently capture\nstructural patterns about object perceptions throughout the whole framework.\nOur method can be operated on superpixels instead of raw pixels to reduce the\nnumber of graph nodes by two orders of magnitude. Moreover, we propose to\npartially explore the multi-object localization problem with inter-occlusion by\nweighted bipartite graph matching. Comprehensive experiments on three benchmark\ndatasets (i.e., SegTrack, MOViCS, and GaTech) demonstrate the effectiveness of\nour approach compared with extensive state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jul 2017 09:01:59 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Guo", "Chunchao", ""], ["Lai", "Jianhuang", ""], ["Xie", "Xiaohua", ""]]}, {"id": "1707.07863", "submitter": "Beatriz Remeseiro", "authors": "Pedro Herruzo, Laura Portell, Alberto Soto and Beatriz Remeseiro", "title": "Analyzing First-Person Stories Based on Socializing, Eating and\n  Sedentary Patterns", "comments": "Accepted at First International Workshop on Social Signal Processing\n  and Beyond, 19th International Conference on Image Analysis and Processing\n  (ICIAP), September 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  First-person stories can be analyzed by means of egocentric pictures acquired\nthroughout the whole active day with wearable cameras. This manuscript presents\nan egocentric dataset with more than 45,000 pictures from four people in\ndifferent environments such as working or studying. All the images were\nmanually labeled to identify three patterns of interest regarding people's\nlifestyle: socializing, eating and sedentary. Additionally, two different\napproaches are proposed to classify egocentric images into one of the 12 target\ncategories defined to characterize these three patterns. The approaches are\nbased on machine learning and deep learning techniques, including traditional\nclassifiers and state-of-art convolutional neural networks. The experimental\nresults obtained when applying these methods to the egocentric dataset\ndemonstrated their adequacy for the problem at hand.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jul 2017 09:15:44 GMT"}], "update_date": "2017-07-26", "authors_parsed": [["Herruzo", "Pedro", ""], ["Portell", "Laura", ""], ["Soto", "Alberto", ""], ["Remeseiro", "Beatriz", ""]]}, {"id": "1707.07890", "submitter": "Feng Xiong", "authors": "Feng Xiong, Xingjian Shi and Dit-Yan Yeung", "title": "Spatiotemporal Modeling for Crowd Counting in Videos", "comments": "Accepted by ICCV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Region of Interest (ROI) crowd counting can be formulated as a regression\nproblem of learning a mapping from an image or a video frame to a crowd density\nmap. Recently, convolutional neural network (CNN) models have achieved\npromising results for crowd counting. However, even when dealing with video\ndata, CNN-based methods still consider each video frame independently, ignoring\nthe strong temporal correlation between neighboring frames. To exploit the\notherwise very useful temporal information in video sequences, we propose a\nvariant of a recent deep learning model called convolutional LSTM (ConvLSTM)\nfor crowd counting. Unlike the previous CNN-based methods, our method fully\ncaptures both spatial and temporal dependencies. Furthermore, we extend the\nConvLSTM model to a bidirectional ConvLSTM model which can access long-range\ninformation in both directions. Extensive experiments using four publicly\navailable datasets demonstrate the reliability of our approach and the\neffectiveness of incorporating temporal information to boost the accuracy of\ncrowd counting. In addition, we also conduct some transfer learning experiments\nto show that once our model is trained on one dataset, its learning experience\ncan be transferred easily to a new dataset which consists of only very few\nvideo frames for model adaptation.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jul 2017 10:02:33 GMT"}], "update_date": "2017-07-26", "authors_parsed": [["Xiong", "Feng", ""], ["Shi", "Xingjian", ""], ["Yeung", "Dit-Yan", ""]]}, {"id": "1707.07923", "submitter": "Daniel S\\'aez Trigueros", "authors": "Daniel S\\'aez Trigueros, Li Meng, Margaret Hartnett", "title": "Enhancing Convolutional Neural Networks for Face Recognition with\n  Occlusion Maps and Batch Triplet Loss", "comments": "12 pages, 7 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the recent success of convolutional neural networks for computer\nvision applications, unconstrained face recognition remains a challenge. In\nthis work, we make two contributions to the field. Firstly, we consider the\nproblem of face recognition with partial occlusions and show how current\napproaches might suffer significant performance degradation when dealing with\nthis kind of face images. We propose a simple method to find out which parts of\nthe human face are more important to achieve a high recognition rate, and use\nthat information during training to force a convolutional neural network to\nlearn discriminative features from all the face regions more equally, including\nthose that typical approaches tend to pay less attention to. We test the\naccuracy of the proposed method when dealing with real-life occlusions using\nthe AR face database. Secondly, we propose a novel loss function called batch\ntriplet loss that improves the performance of the triplet loss by adding an\nextra term to the loss function to cause minimisation of the standard deviation\nof both positive and negative scores. We show consistent improvement in the\nLabeled Faces in the Wild (LFW) benchmark by applying both proposed adjustments\nto the convolutional neural network training.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jul 2017 11:35:18 GMT"}, {"version": "v2", "created": "Fri, 23 Mar 2018 15:02:34 GMT"}, {"version": "v3", "created": "Mon, 26 Mar 2018 14:05:34 GMT"}, {"version": "v4", "created": "Sun, 10 Jun 2018 09:42:33 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Trigueros", "Daniel S\u00e1ez", ""], ["Meng", "Li", ""], ["Hartnett", "Margaret", ""]]}, {"id": "1707.07932", "submitter": "Hongyoon Choi Dr", "authors": "Hongyoon Choi", "title": "Functional connectivity patterns of autism spectrum disorder identified\n  by deep feature learning", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autism spectrum disorder (ASD) is regarded as a brain disease with globally\ndisrupted neuronal networks. Even though fMRI studies have revealed abnormal\nfunctional connectivity in ASD, they have not reached a consensus of the\ndisrupted patterns. Here, a deep learning-based feature extraction method\nidentifies multivariate and nonlinear functional connectivity patterns of ASD.\nResting-state fMRI data of 972 subjects (465 ASD 507 normal controls) acquired\nfrom the Autism Brain Imaging Data Exchange were used. A functional\nconnectivity matrix of each subject was generated using 90 predefined brain\nregions. As a data-driven feature extraction method without prior knowledge\nsuch as subjects diagnosis, variational autoencoder (VAE) summarized the\nfunctional connectivity matrix into 2 features. Those feature values of ASD\npatients were statistically compared with those of controls. A feature was\nsignificantly different between ASD and normal controls. The extracted features\nwere visualized by VAE-based generator which can produce virtual functional\nconnectivity matrices. The ASD-related feature was associated with\nfrontoparietal connections, interconnections of the dorsal medial frontal\ncortex and corticostriatal connections. It also showed a trend of negative\ncorrelation with full-scale IQ. A data-driven feature extraction based on deep\nlearning could identify complex patterns of functional connectivity of ASD.\nThis approach will help discover complex patterns of abnormalities in brain\nconnectivity in various brain disorders.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jul 2017 11:59:33 GMT"}], "update_date": "2017-07-26", "authors_parsed": [["Choi", "Hongyoon", ""]]}, {"id": "1707.07958", "submitter": "Christian Wolf", "authors": "Damien Fourure, R\\'emi Emonet, Elisa Fromont, Damien Muselet, Alain\n  Tremeau and Christian Wolf", "title": "Residual Conv-Deconv Grid Network for Semantic Segmentation", "comments": "Accepted for publication at BMVC 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents GridNet, a new Convolutional Neural Network (CNN)\narchitecture for semantic image segmentation (full scene labelling). Classical\nneural networks are implemented as one stream from the input to the output with\nsubsampling operators applied in the stream in order to reduce the feature maps\nsize and to increase the receptive field for the final prediction. However, for\nsemantic image segmentation, where the task consists in providing a semantic\nclass to each pixel of an image, feature maps reduction is harmful because it\nleads to a resolution loss in the output prediction. To tackle this problem,\nour GridNet follows a grid pattern allowing multiple interconnected streams to\nwork at different resolutions. We show that our network generalizes many well\nknown networks such as conv-deconv, residual or U-Net networks. GridNet is\ntrained from scratch and achieves competitive results on the Cityscapes\ndataset.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jul 2017 12:49:11 GMT"}, {"version": "v2", "created": "Wed, 26 Jul 2017 08:18:54 GMT"}], "update_date": "2017-07-27", "authors_parsed": [["Fourure", "Damien", ""], ["Emonet", "R\u00e9mi", ""], ["Fromont", "Elisa", ""], ["Muselet", "Damien", ""], ["Tremeau", "Alain", ""], ["Wolf", "Christian", ""]]}, {"id": "1707.07998", "submitter": "Peter Anderson", "authors": "Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark\n  Johnson, Stephen Gould, Lei Zhang", "title": "Bottom-Up and Top-Down Attention for Image Captioning and Visual\n  Question Answering", "comments": "CVPR 2018 full oral, winner of the 2017 Visual Question Answering\n  challenge", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Top-down visual attention mechanisms have been used extensively in image\ncaptioning and visual question answering (VQA) to enable deeper image\nunderstanding through fine-grained analysis and even multiple steps of\nreasoning. In this work, we propose a combined bottom-up and top-down attention\nmechanism that enables attention to be calculated at the level of objects and\nother salient image regions. This is the natural basis for attention to be\nconsidered. Within our approach, the bottom-up mechanism (based on Faster\nR-CNN) proposes image regions, each with an associated feature vector, while\nthe top-down mechanism determines feature weightings. Applying this approach to\nimage captioning, our results on the MSCOCO test server establish a new\nstate-of-the-art for the task, achieving CIDEr / SPICE / BLEU-4 scores of\n117.9, 21.5 and 36.9, respectively. Demonstrating the broad applicability of\nthe method, applying the same approach to VQA we obtain first place in the 2017\nVQA Challenge.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jul 2017 13:50:17 GMT"}, {"version": "v2", "created": "Thu, 10 Aug 2017 23:24:23 GMT"}, {"version": "v3", "created": "Wed, 14 Mar 2018 05:24:23 GMT"}], "update_date": "2018-03-15", "authors_parsed": [["Anderson", "Peter", ""], ["He", "Xiaodong", ""], ["Buehler", "Chris", ""], ["Teney", "Damien", ""], ["Johnson", "Mark", ""], ["Gould", "Stephen", ""], ["Zhang", "Lei", ""]]}, {"id": "1707.08037", "submitter": "Dong Yang", "authors": "Dong Yang, Daguang Xu, S. Kevin Zhou, Bogdan Georgescu, Mingqing Chen,\n  Sasa Grbic, Dimitris Metaxas and Dorin Comaniciu", "title": "Automatic Liver Segmentation Using an Adversarial Image-to-Image Network", "comments": "Accepted by MICCAI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic liver segmentation in 3D medical images is essential in many\nclinical applications, such as pathological diagnosis of hepatic diseases,\nsurgical planning, and postoperative assessment. However, it is still a very\nchallenging task due to the complex background, fuzzy boundary, and various\nappearance of liver. In this paper, we propose an automatic and efficient\nalgorithm to segment liver from 3D CT volumes. A deep image-to-image network\n(DI2IN) is first deployed to generate the liver segmentation, employing a\nconvolutional encoder-decoder architecture combined with multi-level feature\nconcatenation and deep supervision. Then an adversarial network is utilized\nduring training process to discriminate the output of DI2IN from ground truth,\nwhich further boosts the performance of DI2IN. The proposed method is trained\non an annotated dataset of 1000 CT volumes with various different scanning\nprotocols (e.g., contrast and non-contrast, various resolution and position)\nand large variations in populations (e.g., ages and pathology). Our approach\noutperforms the state-of-the-art solutions in terms of segmentation accuracy\nand computing efficiency.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jul 2017 15:16:07 GMT"}], "update_date": "2017-07-26", "authors_parsed": [["Yang", "Dong", ""], ["Xu", "Daguang", ""], ["Zhou", "S. Kevin", ""], ["Georgescu", "Bogdan", ""], ["Chen", "Mingqing", ""], ["Grbic", "Sasa", ""], ["Metaxas", "Dimitris", ""], ["Comaniciu", "Dorin", ""]]}, {"id": "1707.08040", "submitter": "Vinay Verma Kumar", "authors": "Vinay Kumar Verma and Piyush Rai", "title": "A Simple Exponential Family Framework for Zero-Shot Learning", "comments": "Accepted in ECML-PKDD 2017, 16 Pages: Code and Data are available:\n  https://github.com/vkverma01/Zero-Shot/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  We present a simple generative framework for learning to predict previously\nunseen classes, based on estimating class-attribute-gated class-conditional\ndistributions. We model each class-conditional distribution as an exponential\nfamily distribution and the parameters of the distribution of each seen/unseen\nclass are defined as functions of the respective observed class attributes.\nThese functions can be learned using only the seen class data and can be used\nto predict the parameters of the class-conditional distribution of each unseen\nclass. Unlike most existing methods for zero-shot learning that represent\nclasses as fixed embeddings in some vector space, our generative model\nnaturally represents each class as a probability distribution. It is simple to\nimplement and also allows leveraging additional unlabeled data from unseen\nclasses to improve the estimates of their class-conditional distributions using\ntransductive/semi-supervised learning. Moreover, it extends seamlessly to\nfew-shot learning by easily updating these distributions when provided with a\nsmall number of additional labelled examples from unseen classes. Through a\ncomprehensive set of experiments on several benchmark data sets, we demonstrate\nthe efficacy of our framework.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jul 2017 15:28:22 GMT"}, {"version": "v2", "created": "Wed, 26 Jul 2017 06:50:51 GMT"}, {"version": "v3", "created": "Thu, 25 Jan 2018 05:37:04 GMT"}], "update_date": "2018-01-26", "authors_parsed": [["Verma", "Vinay Kumar", ""], ["Rai", "Piyush", ""]]}, {"id": "1707.08063", "submitter": "Chunhua Shen", "authors": "Ruoxi Deng, Tianqi Zhao, Chunhua Shen, Shengjun Liu", "title": "Relative Depth Order Estimation Using Multi-scale Densely Connected\n  Convolutional Networks", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of estimating the relative depth order of point pairs in\na monocular image. Recent advances mainly focus on using deep convolutional\nneural networks (DCNNs) to learn and infer the ordinal information from\nmultiple contextual information of the points pair such as global scene\ncontext, local contextual information, and the locations. However, it remains\nunclear how much each context contributes to the task. To address this, we\nfirst examine the contribution of each context cue [1], [2] to the performance\nin the context of depth order estimation. We find out the local context\nsurrounding the points pair contributes the most and the global scene context\nhelps little. Based on the findings, we propose a simple method, using a\nmulti-scale densely-connected network to tackle the task. Instead of learning\nthe global structure, we dedicate to explore the local structure by learning to\nregress from regions of multiple sizes around the point pairs. Moreover, we use\nthe recent densely connected network [3] to encourage substantial feature reuse\nas well as deepen our network to boost the performance. We show in experiments\nthat the results of our approach is on par with or better than the\nstate-of-the-art methods with the benefit of using only a small number of\ntraining data.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jul 2017 16:11:01 GMT"}, {"version": "v2", "created": "Thu, 27 Jul 2017 10:19:13 GMT"}], "update_date": "2017-07-28", "authors_parsed": [["Deng", "Ruoxi", ""], ["Zhao", "Tianqi", ""], ["Shen", "Chunhua", ""], ["Liu", "Shengjun", ""]]}, {"id": "1707.08095", "submitter": "Seyed Amir Tafrishi", "authors": "Seyed Amir Tafrishi and Vahid E. Kandjani", "title": "Line-Circle: A Geometric Filter for Single Camera Edge-Based Object\n  Detection", "comments": "Submitted to 5th International Conference on Robotics and\n  Mechatronics 2017", "journal-ref": "2017 5th RSI Int. Conf. on Robot. and Mech. (2017) 588-594", "doi": "10.1109/ICRoM.2017.8466193", "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a state-of-the-art approach in object detection for being\napplied in future SLAM problems. Although, many SLAM methods are proposed to\ncreate suitable autonomy for mobile robots namely ground vehicles, they still\nface overconfidence and large computations during entrance to immense spaces\nwith many landmarks. In particular, they suffer from impractical applications\nvia sole reliance on the limited sensors like camera. Proposed method claims\nthat unmanned ground vehicles without having huge amount of database for object\ndefinition and highly advance prediction parameters can deal with incoming\nobjects during straight motion of camera in real-time. Line-Circle (LC) filter\ntries to apply detection, tracking and learning to each defined experts to\nobtain more information for judging scene without over-calculation. In this\nfilter, circle expert let us summarize edges in groups. The Interactive\nfeedback learning between each expert creates minimal error that fights against\noverwhelming landmark signs in crowded scenes without mapping. Our experts\nbasically are dependent on trust factors' covariance with geometric definitions\nto ignore, emerge and compare detected landmarks. The experiment for validating\nthe model is taken place utilizing a camera beside an IMU sensor for location\nestimation.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jul 2017 17:19:42 GMT"}], "update_date": "2018-10-05", "authors_parsed": [["Tafrishi", "Seyed Amir", ""], ["Kandjani", "Vahid E.", ""]]}, {"id": "1707.08105", "submitter": "Nikolaos Passalis", "authors": "Nikolaos Passalis and Anastasios Tefas", "title": "Learning Bag-of-Features Pooling for Deep Convolutional Neural Networks", "comments": "Accepted at ICCV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNNs) are well established models capable of\nachieving state-of-the-art classification accuracy for various computer vision\ntasks. However, they are becoming increasingly larger, using millions of\nparameters, while they are restricted to handling images of fixed size. In this\npaper, a quantization-based approach, inspired from the well-known\nBag-of-Features model, is proposed to overcome these limitations. The proposed\napproach, called Convolutional BoF (CBoF), uses RBF neurons to quantize the\ninformation extracted from the convolutional layers and it is able to natively\nclassify images of various sizes as well as to significantly reduce the number\nof parameters in the network. In contrast to other global pooling operators and\nCNN compression techniques the proposed method utilizes a trainable pooling\nlayer that it is end-to-end differentiable, allowing the network to be trained\nusing regular back-propagation and to achieve greater distribution shift\ninvariance than competitive methods. The ability of the proposed method to\nreduce the parameters of the network and increase the classification accuracy\nover other state-of-the-art techniques is demonstrated using three image\ndatasets.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jul 2017 17:47:30 GMT"}, {"version": "v2", "created": "Wed, 26 Jul 2017 04:25:06 GMT"}], "update_date": "2017-07-27", "authors_parsed": [["Passalis", "Nikolaos", ""], ["Tefas", "Anastasios", ""]]}, {"id": "1707.08148", "submitter": "Afsheen Rafaqat Ali", "authors": "Afsheen Rafaqat Ali, Mohsen Ali", "title": "Emotional Filters: Automatic Image Transformation for Inducing Affect", "comments": "Published at British Machine Vision Conference (BMVC) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current image transformation and recoloring algorithms try to introduce\nartistic effects in the photographed images, based on user input of target\nimage(s) or selection of pre-designed filters. These manipulations, although\nintended to enhance the impact of an image on the viewer, do not include the\noption of image transformation by specifying the affect information. In this\npaper we present an automatic image-transformation method that transforms the\nsource image such that it can induce an emotional affect on the viewer, as\ndesired by the user. Our proposed novel image emotion transfer algorithm does\nnot require a user-specified target image. The proposed algorithm uses features\nextracted from top layers of deep convolutional neural network and the\nuser-specified emotion distribution to select multiple target images from an\nimage database for color transformation, such that the resultant image has\ndesired emotional impact. Our method can handle more diverse set of photographs\nthan the previous methods. We conducted a detailed user study showing the\neffectiveness of our proposed method. A discussion and reasoning of failure\ncases has also been provided, indicating inherent limitation of color-transfer\nbased methods in the use of emotion assignment.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jul 2017 18:38:35 GMT"}, {"version": "v2", "created": "Wed, 20 Dec 2017 09:16:35 GMT"}], "update_date": "2017-12-21", "authors_parsed": [["Ali", "Afsheen Rafaqat", ""], ["Ali", "Mohsen", ""]]}, {"id": "1707.08149", "submitter": "Marc Aubreville", "authors": "Marc Aubreville, Miguel Goncalves, Christian Knipfer, Nicolai Oetter,\n  Tobias Wuerfl, Helmut Neumann, Florian Stelzle, Christopher Bohr and Andreas\n  Maier", "title": "Patch-based Carcinoma Detection on Confocal Laser Endomicroscopy Images\n  -- A Cross-Site Robustness Assessment", "comments": "Erratum: In the previous version, the number of CLE sequences in the\n  vocal folds data set was inadequately reported", "journal-ref": "Proceedings of BIOIMAGING 2018, ISBN: 978-989-758-278-3", "doi": "10.5220/0006534700270034", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning technologies such as convolutional neural networks (CNN)\nprovide powerful methods for image recognition and have recently been employed\nin the field of automated carcinoma detection in confocal laser endomicroscopy\n(CLE) images. CLE is a (sub-)surface microscopic imaging technique that reaches\nmagnifications of up to 1000x and is thus suitable for in vivo structural\ntissue analysis. In this work, we aim to evaluate the prospects of a priorly\ndeveloped deep learning-based algorithm targeted at the identification of oral\nsquamous cell carcinoma with regard to its generalization to further anatomic\nlocations of squamous cell carcinomas in the area of head and neck. We applied\nthe algorithm on images acquired from the vocal fold area of five patients with\nhistologically verified squamous cell carcinoma and presumably healthy control\nimages of the clinically normal contra-lateral vocal cord. We find that the\nnetwork trained on the oral cavity data reaches an accuracy of 89.45% and an\narea-under-the-curve (AUC) value of 0.955, when applied on the vocal cords\ndata. Compared to the state of the art, we achieve very similar results, yet\nwith an algorithm that was trained on a completely disjunct data set.\nConcatenating both data sets yielded further improvements in cross-validation\nwith an accuracy of 90.81% and AUC of 0.970. In this study, for the first time\nto our knowledge, a deep learning mechanism for the identification of oral\ncarcinomas using CLE Images could be applied to other disciplines in the area\nof head and neck. This study shows the prospect of the algorithmic approach to\ngeneralize well on other malignant entities of the head and neck, regardless of\nthe anatomical location and furthermore in an examiner-independent manner.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jul 2017 18:43:03 GMT"}, {"version": "v2", "created": "Fri, 3 Jan 2020 13:49:53 GMT"}], "update_date": "2020-01-06", "authors_parsed": [["Aubreville", "Marc", ""], ["Goncalves", "Miguel", ""], ["Knipfer", "Christian", ""], ["Oetter", "Nicolai", ""], ["Wuerfl", "Tobias", ""], ["Neumann", "Helmut", ""], ["Stelzle", "Florian", ""], ["Bohr", "Christopher", ""], ["Maier", "Andreas", ""]]}, {"id": "1707.08183", "submitter": "Shihua Zhang", "authors": "Lihua Zhang and Shihua Zhang", "title": "A Unified Joint Matrix Factorization Framework for Data Integration", "comments": "14 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonnegative matrix factorization (NMF) is a powerful tool in data exploratory\nanalysis by discovering the hidden features and part-based patterns from\nhigh-dimensional data. NMF and its variants have been successfully applied into\ndiverse fields such as pattern recognition, signal processing, data mining,\nbioinformatics and so on. Recently, NMF has been extended to analyze multiple\nmatrices simultaneously. However, a unified framework is still lacking. In this\npaper, we introduce a sparse multiple relationship data regularized joint\nmatrix factorization (JMF) framework and two adapted prediction models for\npattern recognition and data integration. Next, we present four update\nalgorithms to solve this framework. The merits and demerits of these algorithms\nare systematically explored. Furthermore, extensive computational experiments\nusing both synthetic data and real data demonstrate the effectiveness of JMF\nframework and related algorithms on pattern recognition and data mining.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jul 2017 19:38:25 GMT"}], "update_date": "2017-07-27", "authors_parsed": [["Zhang", "Lihua", ""], ["Zhang", "Shihua", ""]]}, {"id": "1707.08254", "submitter": "Sharif Amit Kamran", "authors": "Sharif Amit Kamran and Ali Shihab Sabbir", "title": "Efficient Yet Deep Convolutional Neural Networks for Semantic\n  Segmentation", "comments": "8 pages", "journal-ref": null, "doi": "10.1109/SAIN.2018.8673354", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic Segmentation using deep convolutional neural network pose more\ncomplex challenge for any GPU intensive task. As it has to compute million of\nparameters, it results to huge memory consumption. Moreover, extracting finer\nfeatures and conducting supervised training tends to increase the complexity.\nWith the introduction of Fully Convolutional Neural Network, which uses finer\nstrides and utilizes deconvolutional layers for upsampling, it has been a go to\nfor any image segmentation task. In this paper, we propose two segmentation\narchitecture which not only needs one-third the parameters to compute but also\ngives better accuracy than the similar architectures. The model weights were\ntransferred from the popular neural net like VGG19 and VGG16 which were trained\non Imagenet classification data-set. Then we transform all the fully connected\nlayers to convolutional layers and use dilated convolution for decreasing the\nparameters. Lastly, we add finer strides and attach four skip architectures\nwhich are element-wise summed with the deconvolutional layers in steps. We\ntrain and test on different sparse and fine data-sets like Pascal VOC2012,\nPascal-Context and NYUDv2 and show how better our model performs in this tasks.\nOn the other hand our model has a faster inference time and consumes less\nmemory for training and testing on NVIDIA Pascal GPUs, making it more efficient\nand less memory consuming architecture for pixel-wise segmentation.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jul 2017 00:15:35 GMT"}, {"version": "v2", "created": "Sat, 23 Dec 2017 15:36:35 GMT"}, {"version": "v3", "created": "Sat, 28 Jul 2018 09:30:54 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Kamran", "Sharif Amit", ""], ["Sabbir", "Ali Shihab", ""]]}, {"id": "1707.08289", "submitter": "Bingke Zhu", "authors": "Bingke Zhu, Yingying Chen, Jinqiao Wang, Si Liu, Bo Zhang, Ming Tang", "title": "Fast Deep Matting for Portrait Animation on Mobile Phone", "comments": "ACM Multimedia Conference (MM) 2017 camera-ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image matting plays an important role in image and video editing. However,\nthe formulation of image matting is inherently ill-posed. Traditional methods\nusually employ interaction to deal with the image matting problem with trimaps\nand strokes, and cannot run on the mobile phone in real-time. In this paper, we\npropose a real-time automatic deep matting approach for mobile devices. By\nleveraging the densely connected blocks and the dilated convolution, a light\nfull convolutional network is designed to predict a coarse binary mask for\nportrait images. And a feathering block, which is edge-preserving and matting\nadaptive, is further developed to learn the guided filter and transform the\nbinary mask into alpha matte. Finally, an automatic portrait animation system\nbased on fast deep matting is built on mobile devices, which does not need any\ninteraction and can realize real-time matting with 15 fps. The experiments show\nthat the proposed approach achieves comparable results with the\nstate-of-the-art matting solvers.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jul 2017 05:05:48 GMT"}], "update_date": "2017-07-27", "authors_parsed": [["Zhu", "Bingke", ""], ["Chen", "Yingying", ""], ["Wang", "Jinqiao", ""], ["Liu", "Si", ""], ["Zhang", "Bo", ""], ["Tang", "Ming", ""]]}, {"id": "1707.08301", "submitter": "Renata Khasanova", "authors": "Renata Khasanova and Pascal Frossard", "title": "Graph-Based Classification of Omnidirectional Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Omnidirectional cameras are widely used in such areas as robotics and virtual\nreality as they provide a wide field of view. Their images are often processed\nwith classical methods, which might unfortunately lead to non-optimal solutions\nas these methods are designed for planar images that have different geometrical\nproperties than omnidirectional ones. In this paper we study image\nclassification task by taking into account the specific geometry of\nomnidirectional cameras with graph-based representations. In particular, we\nextend deep learning architectures to data on graphs; we propose a principled\nway of graph construction such that convolutional filters respond similarly for\nthe same pattern on different positions of the image regardless of lens\ndistortions. Our experiments show that the proposed method outperforms current\ntechniques for the omnidirectional image classification problem.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jul 2017 06:39:45 GMT"}], "update_date": "2017-07-27", "authors_parsed": [["Khasanova", "Renata", ""], ["Frossard", "Pascal", ""]]}, {"id": "1707.08313", "submitter": "Zhile Ren", "authors": "Zhile Ren, Deqing Sun, Jan Kautz, Erik B. Sudderth", "title": "Cascaded Scene Flow Prediction using Semantic Segmentation", "comments": "International Conference on 3D Vision (3DV), 2017 (oral presentation)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Given two consecutive frames from a pair of stereo cameras, 3D scene flow\nmethods simultaneously estimate the 3D geometry and motion of the observed\nscene. Many existing approaches use superpixels for regularization, but may\npredict inconsistent shapes and motions inside rigidly moving objects. We\ninstead assume that scenes consist of foreground objects rigidly moving in\nfront of a static background, and use semantic cues to produce pixel-accurate\nscene flow estimates. Our cascaded classification framework accurately models\n3D scenes by iteratively refining semantic segmentation masks, stereo\ncorrespondences, 3D rigid motion estimates, and optical flow fields. We\nevaluate our method on the challenging KITTI autonomous driving benchmark, and\nshow that accounting for the motion of segmented vehicles leads to\nstate-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jul 2017 07:53:54 GMT"}, {"version": "v2", "created": "Thu, 5 Oct 2017 22:52:31 GMT"}], "update_date": "2017-10-09", "authors_parsed": [["Ren", "Zhile", ""], ["Sun", "Deqing", ""], ["Kautz", "Jan", ""], ["Sudderth", "Erik B.", ""]]}, {"id": "1707.08340", "submitter": "Keze Wang", "authors": "Yukai Shi, Keze Wang, Chongyu Chen, Li Xu and Liang Lin", "title": "Structure-Preserving Image Super-resolution via Contextualized\n  Multi-task Learning", "comments": "To appear in Transactions on Multimedia 2017", "journal-ref": null, "doi": "10.1109/TMM.2017.2711263", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single image super resolution (SR), which refers to reconstruct a\nhigher-resolution (HR) image from the observed low-resolution (LR) image, has\nreceived substantial attention due to its tremendous application potentials.\nDespite the breakthroughs of recently proposed SR methods using convolutional\nneural networks (CNNs), their generated results usually lack of preserving\nstructural (high-frequency) details. In this paper, regarding global boundary\ncontext and residual context as complimentary information for enhancing\nstructural details in image restoration, we develop a contextualized multi-task\nlearning framework to address the SR problem. Specifically, our method first\nextracts convolutional features from the input LR image and applies one\ndeconvolutional module to interpolate the LR feature maps in a content-adaptive\nway. Then, the resulting feature maps are fed into two branched sub-networks.\nDuring the neural network training, one sub-network outputs salient image\nboundaries and the HR image, and the other sub-network outputs the local\nresidual map, i.e., the residual difference between the generated HR image and\nground-truth image. On several standard benchmarks (i.e., Set5, Set14 and\nBSD200), our extensive evaluations demonstrate the effectiveness of our SR\nmethod on achieving both higher restoration quality and computational\nefficiency compared with several state-of-the-art SR approaches. The source\ncode and some SR results can be found at:\nhttp://hcp.sysu.edu.cn/structure-preserving-image-super-resolution/\n", "versions": [{"version": "v1", "created": "Wed, 26 Jul 2017 09:48:03 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Shi", "Yukai", ""], ["Wang", "Keze", ""], ["Chen", "Chongyu", ""], ["Xu", "Li", ""], ["Lin", "Liang", ""]]}, {"id": "1707.08347", "submitter": "Xialei Liu", "authors": "Xialei Liu, Joost van de Weijer and Andrew D. Bagdanov", "title": "RankIQA: Learning from Rankings for No-reference Image Quality\n  Assessment", "comments": "Accepted by ICCV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a no-reference image quality assessment (NR-IQA) approach that\nlearns from rankings (RankIQA). To address the problem of limited IQA dataset\nsize, we train a Siamese Network to rank images in terms of image quality by\nusing synthetically generated distortions for which relative image quality is\nknown. These ranked image sets can be automatically generated without laborious\nhuman labeling. We then use fine-tuning to transfer the knowledge represented\nin the trained Siamese Network to a traditional CNN that estimates absolute\nimage quality from single images. We demonstrate how our approach can be made\nsignificantly more efficient than traditional Siamese Networks by forward\npropagating a batch of images through a single network and backpropagating\ngradients derived from all pairs of images in the batch. Experiments on the\nTID2013 benchmark show that we improve the state-of-the-art by over 5%.\nFurthermore, on the LIVE benchmark we show that our approach is superior to\nexisting NR-IQA techniques and that we even outperform the state-of-the-art in\nfull-reference IQA (FR-IQA) methods without having to resort to high-quality\nreference images to infer IQA.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jul 2017 10:02:40 GMT"}], "update_date": "2017-07-27", "authors_parsed": [["Liu", "Xialei", ""], ["van de Weijer", "Joost", ""], ["Bagdanov", "Andrew D.", ""]]}, {"id": "1707.08350", "submitter": "Seonghyeon Nam", "authors": "Seonghyeon Nam, Seon Joo Kim", "title": "Modelling the Scene Dependent Imaging in Cameras with a Deep Neural\n  Network", "comments": "To appear in ICCV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel deep learning framework that models the scene dependent\nimage processing inside cameras. Often called as the radiometric calibration,\nthe process of recovering RAW images from processed images (JPEG format in the\nsRGB color space) is essential for many computer vision tasks that rely on\nphysically accurate radiance values. All previous works rely on the\ndeterministic imaging model where the color transformation stays the same\nregardless of the scene and thus they can only be applied for images taken\nunder the manual mode. In this paper, we propose a data-driven approach to\nlearn the scene dependent and locally varying image processing inside cameras\nunder the automode. Our method incorporates both the global and the local scene\ncontext into pixel-wise features via multi-scale pyramid of learnable histogram\nlayers. The results show that we can model the imaging pipeline of different\ncameras that operate under the automode accurately in both directions (from RAW\nto sRGB, from sRGB to RAW) and we show how we can apply our method to improve\nthe performance of image deblurring.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jul 2017 10:04:23 GMT"}], "update_date": "2017-07-27", "authors_parsed": [["Nam", "Seonghyeon", ""], ["Kim", "Seon Joo", ""]]}, {"id": "1707.08364", "submitter": "Ali Sharifi Boroujerdi", "authors": "Ali Sharifi Boroujerdi, Maryam Khanian and Michael Breuss", "title": "Deep Interactive Region Segmentation and Captioning", "comments": "17, pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With recent innovations in dense image captioning, it is now possible to\ndescribe every object of the scene with a caption while objects are determined\nby bounding boxes. However, interpretation of such an output is not trivial due\nto the existence of many overlapping bounding boxes. Furthermore, in current\ncaptioning frameworks, the user is not able to involve personal preferences to\nexclude out of interest areas. In this paper, we propose a novel hybrid deep\nlearning architecture for interactive region segmentation and captioning where\nthe user is able to specify an arbitrary region of the image that should be\nprocessed. To this end, a dedicated Fully Convolutional Network (FCN) named\nLyncean FCN (LFCN) is trained using our special training data to isolate the\nUser Intention Region (UIR) as the output of an efficient segmentation. In\nparallel, a dense image captioning model is utilized to provide a wide variety\nof captions for that region. Then, the UIR will be explained with the caption\nof the best match bounding box. To the best of our knowledge, this is the first\nwork that provides such a comprehensive output. Our experiments show the\nsuperiority of the proposed approach over state-of-the-art interactive\nsegmentation methods on several well-known datasets. In addition, replacement\nof the bounding boxes with the result of the interactive segmentation leads to\na better understanding of the dense image captioning output as well as accuracy\nenhancement for the object detection in terms of Intersection over Union (IoU).\n", "versions": [{"version": "v1", "created": "Wed, 26 Jul 2017 10:40:33 GMT"}], "update_date": "2017-07-27", "authors_parsed": [["Boroujerdi", "Ali Sharifi", ""], ["Khanian", "Maryam", ""], ["Breuss", "Michael", ""]]}, {"id": "1707.08378", "submitter": "Alessio Tonioni", "authors": "Alessio Tonioni and Luigi Di Stefano", "title": "Product recognition in store shelves as a sub-graph isomorphism problem", "comments": "Slightly extended version of the paper accepted at ICIAP 2017. More\n  information @project_page -->\n  http://vision.disi.unibo.it/index.php?option=com_content&view=article&id=111&catid=78", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The arrangement of products in store shelves is carefully planned to maximize\nsales and keep customers happy. However, verifying compliance of real shelves\nto the ideal layout is a costly task routinely performed by the store\npersonnel. In this paper, we propose a computer vision pipeline to recognize\nproducts on shelves and verify compliance to the planned layout. We deploy\nlocal invariant features together with a novel formulation of the product\nrecognition problem as a sub-graph isomorphism between the items appearing in\nthe given image and the ideal layout. This allows for auto-localizing the given\nimage within the aisle or store and improving recognition dramatically.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jul 2017 11:20:58 GMT"}, {"version": "v2", "created": "Tue, 19 Sep 2017 13:52:04 GMT"}], "update_date": "2017-09-20", "authors_parsed": [["Tonioni", "Alessio", ""], ["Di Stefano", "Luigi", ""]]}, {"id": "1707.08385", "submitter": "Abdul Kawsar Tushar", "authors": "Abdul Kawsar Tushar, Akm Ashiquzzaman, Afia Afrin, Md. Rashedul Islam", "title": "A Novel Transfer Learning Approach upon Hindi, Arabic, and Bangla\n  Numerals using Convolutional Neural Networks", "comments": "10 pages; 2 figures, 4 tables; conference - International Conference\n  On Computational Vision and Bio Inspired Computing 2017 (http://iccvbic.com/)\n  (accepted)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Increased accuracy in predictive models for handwritten character recognition\nwill open up new frontiers for optical character recognition. Major drawbacks\nof predictive machine learning models are headed by the elongated training time\ntaken by some models, and the requirement that training and test data be in the\nsame feature space and consist of the same distribution. In this study, these\nobstacles are minimized by presenting a model for transferring knowledge from\none task to another. This model is presented for the recognition of handwritten\nnumerals in Indic languages. The model utilizes convolutional neural networks\nwith backpropagation for error reduction and dropout for data overfitting. The\noutput performance of the proposed neural network is shown to have closely\nmatched other state-of-the-art methods using only a fraction of time used by\nthe state-of-the-arts.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jul 2017 11:40:13 GMT"}], "update_date": "2017-07-27", "authors_parsed": [["Tushar", "Abdul Kawsar", ""], ["Ashiquzzaman", "Akm", ""], ["Afrin", "Afia", ""], ["Islam", "Md. Rashedul", ""]]}, {"id": "1707.08386", "submitter": "Abdul Kawsar Tushar", "authors": "Akm Ashiquzzaman, Abdul Kawsar Tushar, Md. Rashedul Islam, Jong-Myon\n  Kim", "title": "Reduction of Overfitting in Diabetes Prediction Using Deep Learning\n  Neural Network", "comments": "8 pages, 3 Figures, 3 Tables; Conference - 7th iCatse International\n  Conference on IT Convergence and Security, 2017\n  (http://icatse.org/icitcs2017/) (accepted)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Augmented accuracy in prediction of diabetes will open up new frontiers in\nhealth prognostics. Data overfitting is a performance-degrading issue in\ndiabetes prognosis. In this study, a prediction system for the disease of\ndiabetes is pre-sented where the issue of overfitting is minimized by using the\ndropout method. Deep learning neural network is used where both fully connected\nlayers are fol-lowed by dropout layers. The output performance of the proposed\nneural network is shown to have outperformed other state-of-art methods and it\nis recorded as by far the best performance for the Pima Indians Diabetes Data\nSet.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jul 2017 11:44:55 GMT"}], "update_date": "2017-07-27", "authors_parsed": [["Ashiquzzaman", "Akm", ""], ["Tushar", "Abdul Kawsar", ""], ["Islam", "Md. Rashedul", ""], ["Kim", "Jong-Myon", ""]]}, {"id": "1707.08390", "submitter": "Johanna Delanoy", "authors": "Johanna Delanoy, Mathieu Aubry, Phillip Isola, Alexei A. Efros, Adrien\n  Bousseau", "title": "3D Sketching using Multi-View Deep Volumetric Prediction", "comments": "See our accompanying video on https://youtu.be/DGIYzmlm2pQ, networks\n  and databases available at https://ns.inria.fr/d3/3DSketching/. To appear in\n  PACMCGIT", "journal-ref": null, "doi": "10.1145/3203197", "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sketch-based modeling strives to bring the ease and immediacy of drawing to\nthe 3D world. However, while drawings are easy for humans to create, they are\nvery challenging for computers to interpret due to their sparsity and\nambiguity. We propose a data-driven approach that tackles this challenge by\nlearning to reconstruct 3D shapes from one or more drawings. At the core of our\napproach is a deep convolutional neural network (CNN) that predicts occupancy\nof a voxel grid from a line drawing. This CNN provides us with an initial 3D\nreconstruction as soon as the user completes a single drawing of the desired\nshape. We complement this single-view network with an updater CNN that refines\nan existing prediction given a new drawing of the shape created from a novel\nviewpoint. A key advantage of our approach is that we can apply the updater\niteratively to fuse information from an arbitrary number of viewpoints, without\nrequiring explicit stroke correspondences between the drawings. We train both\nCNNs by rendering synthetic contour drawings from hand-modeled shape\ncollections as well as from procedurally-generated abstract shapes. Finally, we\nintegrate our CNNs in a minimal modeling interface that allows users to\nseamlessly draw an object, rotate it to see its 3D reconstruction, and refine\nit by re-drawing from another vantage point using the 3D reconstruction as\nguidance. The main strengths of our approach are its robustness to freehand\nbitmap drawings, its ability to adapt to different object categories, and the\ncontinuum it offers between single-view and multi-view sketch-based modeling.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jul 2017 11:49:48 GMT"}, {"version": "v2", "created": "Thu, 27 Jul 2017 06:24:49 GMT"}, {"version": "v3", "created": "Thu, 24 May 2018 20:56:46 GMT"}, {"version": "v4", "created": "Tue, 19 Jun 2018 12:24:54 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Delanoy", "Johanna", ""], ["Aubry", "Mathieu", ""], ["Isola", "Phillip", ""], ["Efros", "Alexei A.", ""], ["Bousseau", "Adrien", ""]]}, {"id": "1707.08391", "submitter": "Subhamoy Mandal", "authors": "Jaya Prakash, Subhamoy Mandal, Daniel Razansky, and Vasilis\n  Ntziachristos", "title": "Maximum entropy based non-negative optoacoustic tomographic image\n  reconstruction", "comments": "This article has been accepted for publication in IEEE Transactions\n  on Biomedical Engineering (30 Dec 2018)", "journal-ref": null, "doi": "10.1109/TBME.2019.2892842", "report-no": null, "categories": "physics.med-ph cs.CV eess.IV physics.optics", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective:Optoacoustic (photoacoustic) tomography is aimed at reconstructing\nmaps of the initial pressure rise induced by the absorption of light pulses in\ntissue. In practice, due to inaccurate assumptions in the forward model, noise\nand other experimental factors, the images are often afflicted by artifacts,\noccasionally manifested as negative values. The aim of the work is to develop\nan inversion method which reduces the occurrence of negative values and\nimproves the quantitative performance of optoacoustic imaging. Methods: We\npresent a novel method for optoacoustic tomography based on an entropy\nmaximization algorithm, which uses logarithmic regularization for attaining\nnon-negative reconstructions. The reconstruction image quality is further\nimproved using structural prior based fluence correction. Results: We report\nthe performance achieved by the entropy maximization scheme on numerical\nsimulation, experimental phantoms and in-vivo samples. Conclusion: The proposed\nalgorithm demonstrates superior reconstruction performance by delivering\nnon-negative pixel values with no visible distortion of anatomical structures.\nSignificance: Our method can enable quantitative optoacoustic imaging, and has\nthe potential to improve pre-clinical and translational imaging applications.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jul 2017 11:50:06 GMT"}, {"version": "v2", "created": "Thu, 3 Jan 2019 15:11:24 GMT"}, {"version": "v3", "created": "Sat, 12 Jan 2019 01:01:42 GMT"}], "update_date": "2019-01-21", "authors_parsed": [["Prakash", "Jaya", ""], ["Mandal", "Subhamoy", ""], ["Razansky", "Daniel", ""], ["Ntziachristos", "Vasilis", ""]]}, {"id": "1707.08398", "submitter": "Pawan Kumar Singh", "authors": "Supratim Das, Pawan Kumar Singh, Showmik Bhowmik, Ram Sarkar and Mita\n  Nasipuri", "title": "A Harmony Search Based Wrapper Feature Selection Method for Holistic\n  Bangla word Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A lot of search approaches have been explored for the selection of features\nin pattern classification domain in order to discover significant subset of the\nfeatures which produces better accuracy. In this paper, we introduced a Harmony\nSearch (HS) algorithm based feature selection method for feature dimensionality\nreduction in handwritten Bangla word recognition problem. This algorithm has\nbeen implemented to reduce the feature dimensionality of a technique described\nin one of our previous papers by S. Bhowmik et al.[1]. In the said paper, a set\nof 65 elliptical features were computed for handwritten Bangla word recognition\npurpose and a recognition accuracy of 81.37% was achieved using Multi Layer\nPerceptron (MLP) classifier. In the present work, a subset containing 48\nfeatures (approximately 75% of said feature vector) has been selected by HS\nbased wrapper feature selection method which produces an accuracy rate of\n90.29%. Reasonable outcomes also validates that the introduced algorithm\nutilizes optimal number of features while showing higher classification\naccuracies when compared to two standard evolutionary algorithms like Genetic\nAlgorithm (GA), Particle Swarm Optimization (PSO) and statistical feature\ndimensionality reduction technique like Principal Component Analysis (PCA).\nThis confirms the suitability of HS algorithm to the holistic handwritten word\nrecognition problem.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jul 2017 12:03:39 GMT"}], "update_date": "2017-07-27", "authors_parsed": [["Das", "Supratim", ""], ["Singh", "Pawan Kumar", ""], ["Bhowmik", "Showmik", ""], ["Sarkar", "Ram", ""], ["Nasipuri", "Mita", ""]]}, {"id": "1707.08401", "submitter": "Dezs\\H{o} Ribli", "authors": "Dezs\\H{o} Ribli, Anna Horv\\'ath, Zsuzsa Unger, P\\'eter Pollner,\n  Istv\\'an Csabai", "title": "Detecting and classifying lesions in mammograms with Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last two decades Computer Aided Diagnostics (CAD) systems were\ndeveloped to help radiologists analyze screening mammograms. The benefits of\ncurrent CAD technologies appear to be contradictory and they should be improved\nto be ultimately considered useful. Since 2012 deep convolutional neural\nnetworks (CNN) have been a tremendous success in image recognition, reaching\nhuman performance. These methods have greatly surpassed the traditional\napproaches, which are similar to currently used CAD solutions. Deep CNN-s have\nthe potential to revolutionize medical image analysis. We propose a CAD system\nbased on one of the most successful object detection frameworks, Faster R-CNN.\nThe system detects and classifies malignant or benign lesions on a mammogram\nwithout any human intervention. The proposed method sets the state of the art\nclassification performance on the public INbreast database, AUC = 0.95 . The\napproach described here has achieved the 2nd place in the Digital Mammography\nDREAM Challenge with AUC = 0.85 . When used as a detector, the system reaches\nhigh sensitivity with very few false positive marks per image on the INbreast\ndataset. Source code, the trained model and an OsiriX plugin are availaible\nonline at https://github.com/riblidezso/frcnn_cad .\n", "versions": [{"version": "v1", "created": "Wed, 26 Jul 2017 12:07:45 GMT"}, {"version": "v2", "created": "Wed, 20 Sep 2017 13:04:51 GMT"}, {"version": "v3", "created": "Thu, 9 Nov 2017 14:31:13 GMT"}], "update_date": "2017-11-10", "authors_parsed": [["Ribli", "Dezs\u0151", ""], ["Horv\u00e1th", "Anna", ""], ["Unger", "Zsuzsa", ""], ["Pollner", "P\u00e9ter", ""], ["Csabai", "Istv\u00e1n", ""]]}, {"id": "1707.08525", "submitter": "Marc Aubreville", "authors": "Marc Aubreville, Maximilian Krappmann, Christof Bertram, Robert\n  Klopfleisch and Andreas Maier", "title": "A Guided Spatial Transformer Network for Histology Cell Differentiation", "comments": "5 pages, 4 figures, EG VCBM 2017 contribution", "journal-ref": "EG VCBM 2017 Proceedings", "doi": "10.2312/vcbm.20171233", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identification and counting of cells and mitotic figures is a standard task\nin diagnostic histopathology. Due to the large overall cell count on\nhistological slides and the potential sparse prevalence of some relevant cell\ntypes or mitotic figures, retrieving annotation data for sufficient statistics\nis a tedious task and prone to a significant error in assessment. Automatic\nclassification and segmentation is a classic task in digital pathology, yet it\nis not solved to a sufficient degree.\n  We present a novel approach for cell and mitotic figure classification, based\non a deep convolutional network with an incorporated Spatial Transformer\nNetwork. The network was trained on a novel data set with ten thousand mitotic\nfigures, about ten times more than previous data sets. The algorithm is able to\nderive the cell class (mitotic tumor cells, non-mitotic tumor cells and\ngranulocytes) and their position within an image. The mean accuracy of the\nalgorithm in a five-fold cross-validation is 91.45%.\n  In our view, the approach is a promising step into the direction of a more\nobjective and accurate, semi-automatized mitosis counting supporting the\npathologist.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jul 2017 16:21:39 GMT"}], "update_date": "2017-09-08", "authors_parsed": [["Aubreville", "Marc", ""], ["Krappmann", "Maximilian", ""], ["Bertram", "Christof", ""], ["Klopfleisch", "Robert", ""], ["Maier", "Andreas", ""]]}, {"id": "1707.08554", "submitter": "Andre Mastmeyer", "authors": "Andre Mastmeyer, Matthias Wilms, Heinz Handels", "title": "Interpatient Respiratory Motion Model Transfer for Virtual Reality\n  Simulations of Liver Punctures", "comments": "World Society for Computer Graphics - WSCG 2017 publication, 9 pages,\n  5 figures, 1 movie online", "journal-ref": "Journal of WSCG, Vol.25, No.1, ISSN 1213-6972, 2017", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current virtual reality (VR) training simulators of liver punctures often\nrely on static 3D patient data and use an unrealistic (sinusoidal) periodic\nanimation of the respiratory movement. Existing methods for the animation of\nbreathing motion support simple mathematical or patient-specific, estimated\nbreathing models. However with personalized breathing models for each new\npatient, a heavily dose relevant or expensive 4D data acquisition is mandatory\nfor keyframe-based motion modeling. Given the reference 4D data, first a model\nbuilding stage using linear regression motion field modeling takes place. Then\nthe methodology shown here allows the transfer of existing reference\nrespiratory motion models of a 4D reference patient to a new static 3D patient.\nThis goal is achieved by using non-linear inter-patient registration to warp\none personalized 4D motion field model to new 3D patient data. This cost- and\ndose-saving new method is shown here visually in a qualitative proof-of-concept\nstudy.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jul 2017 17:34:09 GMT"}, {"version": "v2", "created": "Wed, 2 Aug 2017 10:46:40 GMT"}], "update_date": "2017-08-08", "authors_parsed": [["Mastmeyer", "Andre", ""], ["Wilms", "Matthias", ""], ["Handels", "Heinz", ""]]}, {"id": "1707.08559", "submitter": "Cheng-Yang Fu", "authors": "Cheng-Yang Fu, Joon Lee, Mohit Bansal, Alexander C. Berg", "title": "Video Highlight Prediction Using Audience Chat Reactions", "comments": "EMNLP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sports channel video portals offer an exciting domain for research on\nmultimodal, multilingual analysis. We present methods addressing the problem of\nautomatic video highlight prediction based on joint visual features and textual\nanalysis of the real-world audience discourse with complex slang, in both\nEnglish and traditional Chinese. We present a novel dataset based on League of\nLegends championships recorded from North American and Taiwanese Twitch.tv\nchannels (will be released for further research), and demonstrate strong\nresults on these using multimodal, character-level CNN-RNN model architectures.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jul 2017 17:44:38 GMT"}], "update_date": "2017-07-27", "authors_parsed": [["Fu", "Cheng-Yang", ""], ["Lee", "Joon", ""], ["Bansal", "Mohit", ""], ["Berg", "Alexander C.", ""]]}, {"id": "1707.08626", "submitter": "Can Pu", "authors": "Can Pu, Nanbo Li, Robert B Fisher", "title": "Robust Rigid Point Registration based on Convolution of Adaptive\n  Gaussian Mixture Models", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matching 3D rigid point clouds in complex environments robustly and\naccurately is still a core technique used in many applications. This paper\nproposes a new architecture combining error estimation from sample covariances\nand dual global probability alignment based on the convolution of adaptive\nGaussian Mixture Models (GMM) from point clouds. Firstly, a novel adaptive GMM\nis defined using probability distributions from the corresponding points. Then\nrigid point cloud alignment is performed by maximizing the global probability\nfrom the convolution of dual adaptive GMMs in the whole 2D or 3D space, which\ncan be efficiently optimized and has a large zone of accurate convergence.\nThousands of trials have been conducted on 200 models from public 2D and 3D\ndatasets to demonstrate superior robustness and accuracy in complex\nenvironments with unpredictable noise, outliers, occlusion, initial rotation,\nshape and missing points.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jul 2017 20:06:38 GMT"}], "update_date": "2017-07-28", "authors_parsed": [["Pu", "Can", ""], ["Li", "Nanbo", ""], ["Fisher", "Robert B", ""]]}, {"id": "1707.08630", "submitter": "Shizhong Han", "authors": "Shizhong Han, Zibo Meng, Zhiyuan Li, James O'Reilly, Jie Cai, Xiaofeng\n  Wang, Yan Tong", "title": "Optimizing Filter Size in Convolutional Neural Networks for Facial\n  Action Unit Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recognizing facial action units (AUs) during spontaneous facial displays is a\nchallenging problem. Most recently, Convolutional Neural Networks (CNNs) have\nshown promise for facial AU recognition, where predefined and fixed convolution\nfilter sizes are employed. In order to achieve the best performance, the\noptimal filter size is often empirically found by conducting extensive\nexperimental validation. Such a training process suffers from expensive\ntraining cost, especially as the network becomes deeper.\n  This paper proposes a novel Optimized Filter Size CNN (OFS-CNN), where the\nfilter sizes and weights of all convolutional layers are learned simultaneously\nfrom the training data along with learning convolution filters. Specifically,\nthe filter size is defined as a continuous variable, which is optimized by\nminimizing the training loss. Experimental results on two AU-coded spontaneous\ndatabases have shown that the proposed OFS-CNN is capable of estimating optimal\nfilter size for varying image resolution and outperforms traditional CNNs with\nthe best filter size obtained by exhaustive search. The OFS-CNN also beats the\nCNN using multiple filter sizes and more importantly, is much more efficient\nduring testing with the proposed forward-backward propagation algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jul 2017 20:15:13 GMT"}, {"version": "v2", "created": "Wed, 22 Nov 2017 19:33:09 GMT"}], "update_date": "2017-11-27", "authors_parsed": [["Han", "Shizhong", ""], ["Meng", "Zibo", ""], ["Li", "Zhiyuan", ""], ["O'Reilly", "James", ""], ["Cai", "Jie", ""], ["Wang", "Xiaofeng", ""], ["Tong", "Yan", ""]]}, {"id": "1707.08645", "submitter": "Yuan Zong", "authors": "Yuan Zong, Xiaohua Huang, Wenming Zheng, Zhen Cui, Guoying Zhao", "title": "Learning a Target Sample Re-Generator for Cross-Database\n  Micro-Expression Recognition", "comments": "To appear at ACM Multimedia 2017", "journal-ref": null, "doi": "10.1109/TIP.2018.2797479", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate the cross-database micro-expression recognition\nproblem, where the training and testing samples are from two different\nmicro-expression databases. Under this setting, the training and testing\nsamples would have different feature distributions and hence the performance of\nmost existing micro-expression recognition methods may decrease greatly. To\nsolve this problem, we propose a simple yet effective method called Target\nSample Re-Generator (TSRG) in this paper. By using TSRG, we are able to\nre-generate the samples from target micro-expression database and the\nre-generated target samples would share same or similar feature distributions\nwith the original source samples. For this reason, we can then use the\nclassifier learned based on the labeled source samples to accurately predict\nthe micro-expression categories of the unlabeled target samples. To evaluate\nthe performance of the proposed TSRG method, extensive cross-database\nmicro-expression recognition experiments designed based on SMIC and CASME II\ndatabases are conducted. Compared with recent state-of-the-art cross-database\nemotion recognition methods, the proposed TSRG achieves more promising results.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jul 2017 21:13:45 GMT"}], "update_date": "2018-04-04", "authors_parsed": [["Zong", "Yuan", ""], ["Huang", "Xiaohua", ""], ["Zheng", "Wenming", ""], ["Cui", "Zhen", ""], ["Zhao", "Guoying", ""]]}, {"id": "1707.08682", "submitter": "Wei Xiang", "authors": "Wei Xiang, Dong-Qing Zhang, Heather Yu, Vassilis Athitsos", "title": "Context-Aware Single-Shot Detector", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  SSD is one of the state-of-the-art object detection algorithms, and it\ncombines high detection accuracy with real-time speed. However, it is widely\nrecognized that SSD is less accurate in detecting small objects compared to\nlarge objects, because it ignores the context from outside the proposal boxes.\nIn this paper, we present CSSD--a shorthand for context-aware single-shot\nmultibox object detector. CSSD is built on top of SSD, with additional layers\nmodeling multi-scale contexts. We describe two variants of CSSD, which differ\nin their context layers, using dilated convolution layers (DiCSSD) and\ndeconvolution layers (DeCSSD) respectively. The experimental results show that\nthe multi-scale context modeling significantly improves the detection accuracy.\nIn addition, we study the relationship between effective receptive fields\n(ERFs) and the theoretical receptive fields (TRFs), particularly on a VGGNet.\nThe empirical results further strengthen our conclusion that SSD coupled with\ncontext layers achieves better detection results especially for small objects\n($+3.2\\% {\\rm AP}_{@0.5}$ on MS-COCO compared to the newest SSD), while\nmaintaining comparable runtime performance.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jul 2017 01:50:17 GMT"}, {"version": "v2", "created": "Sat, 24 Mar 2018 19:22:12 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Xiang", "Wei", ""], ["Zhang", "Dong-Qing", ""], ["Yu", "Heather", ""], ["Athitsos", "Vassilis", ""]]}, {"id": "1707.08705", "submitter": "Keke He", "authors": "Keke He, Yanwei Fu, Xiangyang Xue", "title": "A Jointly Learned Deep Architecture for Facial Attribute Analysis and\n  Face Detection in the Wild", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Facial attribute analysis in the real world scenario is very challenging\nmainly because of complex face variations. Existing works of analyzing face\nattributes are mostly based on the cropped and aligned face images. However,\nthis result in the capability of attribute prediction heavily relies on the\npreprocessing of face detector. To address this problem, we present a novel\njointly learned deep architecture for both facial attribute analysis and face\ndetection. Our framework can process the natural images in the wild and our\nexperiments on CelebA and LFWA datasets clearly show that the state-of-the-art\nperformance is obtained.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jul 2017 04:45:42 GMT"}], "update_date": "2017-07-28", "authors_parsed": [["He", "Keke", ""], ["Fu", "Yanwei", ""], ["Xue", "Xiangyang", ""]]}, {"id": "1707.08718", "submitter": "Saman Naderiparizi", "authors": "Saman Naderiparizi, Mehrdad Hessar, Vamsi Talla, Shyamnath Gollakota\n  and Joshua R. Smith", "title": "Ultra-low-power Wireless Streaming Cameras", "comments": "9 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.ET cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wireless video streaming has traditionally been considered an extremely\npower-hungry operation. Existing approaches optimize the camera and\ncommunication modules individually to minimize their power consumption.\nHowever, the joint redesign and optimization of wireless communication as well\nas the camera is what that provides more power saving. We present an\nultra-low-power wireless video streaming camera. To achieve this, we present a\nnovel \"analog\" video backscatter technique that feeds analog pixels from the\nphoto-diodes directly to the backscatter hardware, thereby eliminating power\nconsuming hardware components such as ADCs and amplifiers. We prototype our\nwireless camera using off-the-shelf hardware and show that our design can\nstream video at up to 13 FPS and can operate up to a distance of 150 feet from\nthe access point. Our COTS prototype consumes 2.36mW. Finally, to demonstrate\nthe potential of our design, we built two proof-of-concept applications: video\nstreaming for micro-robots and security cameras for face detection.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jul 2017 06:43:18 GMT"}], "update_date": "2017-07-28", "authors_parsed": [["Naderiparizi", "Saman", ""], ["Hessar", "Mehrdad", ""], ["Talla", "Vamsi", ""], ["Gollakota", "Shyamnath", ""], ["Smith", "Joshua R.", ""]]}, {"id": "1707.08721", "submitter": "Qingyi Tao", "authors": "Qingyi Tao, Hao Yang, Jianfei Cai", "title": "Exploiting Web Images for Weakly Supervised Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, the performance of object detection has advanced\nsignificantly with the evolving deep convolutional neural networks. However,\nthe state-of-the-art object detection methods still rely on accurate bounding\nbox annotations that require extensive human labelling. Object detection\nwithout bounding box annotations, i.e, weakly supervised detection methods, are\nstill lagging far behind. As weakly supervised detection only uses image level\nlabels and does not require the ground truth of bounding box location and label\nof each object in an image, it is generally very difficult to distill knowledge\nof the actual appearances of objects. Inspired by curriculum learning, this\npaper proposes an easy-to-hard knowledge transfer scheme that incorporates easy\nweb images to provide prior knowledge of object appearance as a good starting\npoint. While exploiting large-scale free web imagery, we introduce a\nsophisticated labour free method to construct a web dataset with good diversity\nin object appearance. After that, semantic relevance and distribution relevance\nare introduced and utilized in the proposed curriculum training scheme. Our\nend-to-end learning with the constructed web data achieves remarkable\nimprovement across most object classes especially for the classes that are\noften considered hard in other works.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jul 2017 06:58:39 GMT"}, {"version": "v2", "created": "Fri, 28 Jul 2017 04:37:16 GMT"}], "update_date": "2017-07-31", "authors_parsed": [["Tao", "Qingyi", ""], ["Yang", "Hao", ""], ["Cai", "Jianfei", ""]]}, {"id": "1707.08722", "submitter": "Andre Wagner", "authors": "Andr\\'e Wagner", "title": "Algebraic Relations and Triangulation of Unlabeled Image Points", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In multiview geometry when correspondences among multiple views are unknown\nthe image points can be understood as being unlabeled. This is a common problem\nin computer vision. We give a novel approach to handle such a situation by\nregarding unlabeled point configurations as points on the Chow variety\n$\\text{Sym}_m(\\mathbb{P}^2)$. For two unlabeled points we design an algorithm\nthat solves the triangulation problem with unknown correspondences. Further the\nunlabeled multiview variety $\\text{Sym}_m(V_A)$ is studied.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jul 2017 07:00:02 GMT"}], "update_date": "2017-07-28", "authors_parsed": [["Wagner", "Andr\u00e9", ""]]}, {"id": "1707.08813", "submitter": "Sean Maudsley-Barton Mr", "authors": "Sean Maudsley-Barton, Jamie McPheey, Anthony Bukowski, Daniel\n  Leightley and Moi Hoon Yap", "title": "A Comparative Study of the Clinical use of Motion Analysis from Kinect\n  Skeleton Data", "comments": null, "journal-ref": null, "doi": "10.1109/SMC.2017.8123052", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The analysis of human motion as a clinical tool can bring many benefits such\nas the early detection of disease and the monitoring of recovery, so in turn\nhelping people to lead independent lives. However, it is currently under used.\nDevelopments in depth cameras, such as Kinect, have opened up the use of motion\nanalysis in settings such as GP surgeries, care homes and private homes. To\nprovide an insight into the use of Kinect in the healthcare domain, we present\na review of the current state of the art. We then propose a method that can\nrepresent human motions from time-series data of arbitrary length, as a single\nvector. Finally, we demonstrate the utility of this method by extracting a set\nof clinically significant features and using them to detect the age related\nchanges in the motions of a set of 54 individuals, with a high degree of\ncertainty (F1- score between 0.9 - 1.0). Indicating its potential application\nin the detection of a range of age-related motion impairments.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jul 2017 10:55:43 GMT"}, {"version": "v2", "created": "Mon, 31 Jul 2017 08:42:04 GMT"}], "update_date": "2018-04-10", "authors_parsed": [["Maudsley-Barton", "Sean", ""], ["McPheey", "Jamie", ""], ["Bukowski", "Anthony", ""], ["Leightley", "Daniel", ""], ["Yap", "Moi Hoon", ""]]}, {"id": "1707.08814", "submitter": "Muhammad Shaban", "authors": "Abhinav Agarwalla, Muhammad Shaban, Nasir M. Rajpoot", "title": "Representation-Aggregation Networks for Segmentation of Multi-Gigapixel\n  Histology Images", "comments": "Published in Workshop on Deep Learning in Irregular Domains (DLID) in\n  BMVC2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Network (CNN) models have become the state-of-the-art\nfor most computer vision tasks with natural images. However, these are not best\nsuited for multi-gigapixel resolution Whole Slide Images (WSIs) of histology\nslides due to large size of these images. Current approaches construct smaller\npatches from WSIs which results in the loss of contextual information. We\npropose to capture the spatial context using novel Representation-Aggregation\nNetwork (RAN) for segmentation purposes, wherein the first network learns\npatch-level representation and the second network aggregates context from a\ngrid of neighbouring patches. We can use any CNN for representation learning,\nand can utilize CNN or 2D-Long Short Term Memory (2D-LSTM) for\ncontext-aggregation. Our method significantly outperformed conventional\npatch-based CNN approaches on segmentation of tumour in WSIs of breast cancer\ntissue sections.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jul 2017 10:56:58 GMT"}], "update_date": "2017-07-28", "authors_parsed": [["Agarwalla", "Abhinav", ""], ["Shaban", "Muhammad", ""], ["Rajpoot", "Nasir M.", ""]]}, {"id": "1707.08816", "submitter": "Marc Bola\\~nos", "authors": "Marc Bola\\~nos and Aina Ferr\\`a and Petia Radeva", "title": "Food Ingredients Recognition through Multi-label Learning", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatically constructing a food diary that tracks the ingredients consumed\ncan help people follow a healthy diet. We tackle the problem of food\ningredients recognition as a multi-label learning problem. We propose a method\nfor adapting a highly performing state of the art CNN in order to act as a\nmulti-label predictor for learning recipes in terms of their list of\ningredients. We prove that our model is able to, given a picture, predict its\nlist of ingredients, even if the recipe corresponding to the picture has never\nbeen seen by the model. We make public two new datasets suitable for this\npurpose. Furthermore, we prove that a model trained with a high variability of\nrecipes and ingredients is able to generalize better on new data, and visualize\nhow it specializes each of its neurons to different ingredients.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jul 2017 11:16:42 GMT"}], "update_date": "2017-07-28", "authors_parsed": [["Bola\u00f1os", "Marc", ""], ["Ferr\u00e0", "Aina", ""], ["Radeva", "Petia", ""]]}, {"id": "1707.08819", "submitter": "Patryk Chrabaszcz", "authors": "Patryk Chrabaszcz, Ilya Loshchilov and Frank Hutter", "title": "A Downsampled Variant of ImageNet as an Alternative to the CIFAR\n  datasets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The original ImageNet dataset is a popular large-scale benchmark for training\nDeep Neural Networks. Since the cost of performing experiments (e.g, algorithm\ndesign, architecture search, and hyperparameter tuning) on the original dataset\nmight be prohibitive, we propose to consider a downsampled version of ImageNet.\nIn contrast to the CIFAR datasets and earlier downsampled versions of ImageNet,\nour proposed ImageNet32$\\times$32 (and its variants ImageNet64$\\times$64 and\nImageNet16$\\times$16) contains exactly the same number of classes and images as\nImageNet, with the only difference that the images are downsampled to\n32$\\times$32 pixels per image (64$\\times$64 and 16$\\times$16 pixels for the\nvariants, respectively). Experiments on these downsampled variants are\ndramatically faster than on the original ImageNet and the characteristics of\nthe downsampled datasets with respect to optimal hyperparameters appear to\nremain similar. The proposed datasets and scripts to reproduce our results are\navailable at http://image-net.org/download-images and\nhttps://github.com/PatrykChrabaszcz/Imagenet32_Scripts\n", "versions": [{"version": "v1", "created": "Thu, 27 Jul 2017 11:22:22 GMT"}, {"version": "v2", "created": "Fri, 28 Jul 2017 10:36:32 GMT"}, {"version": "v3", "created": "Wed, 23 Aug 2017 16:06:20 GMT"}], "update_date": "2017-08-24", "authors_parsed": [["Chrabaszcz", "Patryk", ""], ["Loshchilov", "Ilya", ""], ["Hutter", "Frank", ""]]}, {"id": "1707.08821", "submitter": "Marc Bola\\~nos", "authors": "Gabriel Oliveira-Barra and Marc Bola\\~nos and Estefania Talavera and\n  Adri\\'an Due\\~nas and Olga Gelonch and Maite Garolera", "title": "Serious Games Application for Memory Training Using Egocentric Images", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mild cognitive impairment is the early stage of several neurodegenerative\ndiseases, such as Alzheimer's. In this work, we address the use of lifelogging\nas a tool to obtain pictures from a patient's daily life from an egocentric\npoint of view. We propose to use them in combination with serious games as a\nway to provide a non-pharmacological treatment to improve their quality of\nlife. To do so, we introduce a novel computer vision technique that classifies\nrich and non rich egocentric images and uses them in serious games. We present\nresults over a dataset composed by 10,997 images, recorded by 7 different\nusers, achieving 79% of F1-score. Our model presents the first method used for\nautomatic egocentric images selection applicable to serious games.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jul 2017 11:36:26 GMT"}], "update_date": "2017-07-28", "authors_parsed": [["Oliveira-Barra", "Gabriel", ""], ["Bola\u00f1os", "Marc", ""], ["Talavera", "Estefania", ""], ["Due\u00f1as", "Adri\u00e1n", ""], ["Gelonch", "Olga", ""], ["Garolera", "Maite", ""]]}, {"id": "1707.08831", "submitter": "Christian Bartz", "authors": "Christian Bartz, Haojin Yang, Christoph Meinel", "title": "STN-OCR: A single Neural Network for Text Detection and Text Recognition", "comments": "9 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting and recognizing text in natural scene images is a challenging, yet\nnot completely solved task. In re- cent years several new systems that try to\nsolve at least one of the two sub-tasks (text detection and text recognition)\nhave been proposed. In this paper we present STN-OCR, a step towards\nsemi-supervised neural networks for scene text recognition, that can be\noptimized end-to-end. In contrast to most existing works that consist of\nmultiple deep neural networks and several pre-processing steps we propose to\nuse a single deep neural network that learns to detect and recognize text from\nnatural images in a semi-supervised way. STN-OCR is a network that integrates\nand jointly learns a spatial transformer network, that can learn to detect text\nregions in an image, and a text recognition network that takes the identified\ntext regions and recognizes their textual content. We investigate how our model\nbehaves on a range of different tasks (detection and recognition of characters,\nand lines of text). Experimental results on public benchmark datasets show the\nability of our model to handle a variety of different tasks, without\nsubstantial changes in its overall network structure.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jul 2017 12:22:34 GMT"}], "update_date": "2017-07-28", "authors_parsed": [["Bartz", "Christian", ""], ["Yang", "Haojin", ""], ["Meinel", "Christoph", ""]]}, {"id": "1707.08935", "submitter": "Toufiq Parag", "authors": "Toufiq Parag, Fabian Tschopp, William Grisaitis, Srinivas C Turaga,\n  Xuewen Zhang, Brian Matejek, Lee Kamentsky, Jeff W. Lichtman, Hanspeter\n  Pfister", "title": "Anisotropic EM Segmentation by 3D Affinity Learning and Agglomeration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The field of connectomics has recently produced neuron wiring diagrams from\nrelatively large brain regions from multiple animals. Most of these neural\nreconstructions were computed from isotropic (e.g., FIBSEM) or near isotropic\n(e.g., SBEM) data. In spite of the remarkable progress on algorithms in recent\nyears, automatic dense reconstruction from anisotropic data remains a challenge\nfor the connectomics community. One significant hurdle in the segmentation of\nanisotropic data is the difficulty in generating a suitable initial\nover-segmentation. In this study, we present a segmentation method for\nanisotropic EM data that agglomerates a 3D over-segmentation computed from the\n3D affinity prediction. A 3D U-net is trained to predict 3D affinities by the\nMALIS approach. Experiments on multiple datasets demonstrates the strength and\nrobustness of the proposed method for anisotropic EM segmentation.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jul 2017 17:08:28 GMT"}, {"version": "v2", "created": "Fri, 3 Aug 2018 15:22:41 GMT"}], "update_date": "2018-08-06", "authors_parsed": [["Parag", "Toufiq", ""], ["Tschopp", "Fabian", ""], ["Grisaitis", "William", ""], ["Turaga", "Srinivas C", ""], ["Zhang", "Xuewen", ""], ["Matejek", "Brian", ""], ["Kamentsky", "Lee", ""], ["Lichtman", "Jeff W.", ""], ["Pfister", "Hanspeter", ""]]}, {"id": "1707.08943", "submitter": "Han Gong", "authors": "Han Gong, Graham D. Finlayson, Maryam M. Darrodi", "title": "Concise Radiometric Calibration Using The Power of Ranking", "comments": "accepted by BMVC 2017. Correction: Note that the reported model\n  parameter number in the original BMVC paper was wrong. It should be 408\n  rather than 158", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Compared with raw images, the more common JPEG images are less useful for\nmachine vision algorithms and professional photographers because JPEG-sRGB does\nnot preserve a linear relation between pixel values and the light measured from\nthe scene. A camera is said to be radiometrically calibrated if there is a\ncomputational model which can predict how the raw linear sensor image is mapped\nto the corresponding rendered image (e.g. JPEGs) and vice versa. This paper\nbegins with the observation that the rank order of pixel values are mostly\npreserved post colour correction. We show that this observation is the key to\nsolving for the whole camera pipeline (colour correction, tone and gamut\nmapping). Our rank-based calibration method is simpler than the prior art and\nso is parametrised by fewer variables which, concomitantly, can be solved for\nusing less calibration data. Another advantage is that we can derive the camera\npipeline from a single pair of raw-JPEG images. Experiments demonstrate that\nour method delivers state-of-the-art results (especially for the most\ninteresting case of JPEG to raw).\n", "versions": [{"version": "v1", "created": "Thu, 27 Jul 2017 17:31:25 GMT"}, {"version": "v2", "created": "Wed, 14 Mar 2018 16:22:46 GMT"}, {"version": "v3", "created": "Thu, 15 Mar 2018 11:10:59 GMT"}], "update_date": "2018-03-16", "authors_parsed": [["Gong", "Han", ""], ["Finlayson", "Graham D.", ""], ["Darrodi", "Maryam M.", ""]]}, {"id": "1707.08951", "submitter": "Manuel Ladra", "authors": "Jos\\'e Manuel Casas, Nick Inassaridze, Manuel Ladra, Susana Ladra", "title": "Handwritten character recognition using some (anti)-diagonal structural\n  features", "comments": "Revised version with a number of improvements and update references,\n  9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a methodology for off-line handwritten character\nrecognition. The proposed methodology relies on a new feature extraction\ntechnique based on structural characteristics, histograms and profiles. As\nnovelty, we propose the extraction of new eight histograms and four profiles\nfrom the $32\\times 32$ matrices that represent the characters, creating\n256-dimension feature vectors. These feature vectors are then employed in a\nclassification step that uses a $k$-means algorithm. We performed experiments\nusing the NIST database to evaluate our proposal. Namely, the recognition\nsystem was trained using 1000 samples and 64 classes for each symbol and was\ntested on 500 samples for each symbol. We obtain promising accuracy results\nthat vary from 81.74\\% to 93.75\\%, depending on the difficulty of the character\ncategory, showing better accuracy results than other methods from the state of\nthe art also based on structural characteristics.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jul 2017 17:50:13 GMT"}, {"version": "v2", "created": "Fri, 28 Jul 2017 21:59:32 GMT"}, {"version": "v3", "created": "Wed, 14 Feb 2018 16:51:15 GMT"}], "update_date": "2018-02-15", "authors_parsed": [["Casas", "Jos\u00e9 Manuel", ""], ["Inassaridze", "Nick", ""], ["Ladra", "Manuel", ""], ["Ladra", "Susana", ""]]}, {"id": "1707.08952", "submitter": "Amy Zhang", "authors": "Amy Zhang, Xianming Liu, Andreas Gros, Tobias Tiecke", "title": "Building Detection from Satellite Images on a Global Scale", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last several years, remote sensing technology has opened up the\npossibility of performing large scale building detection from satellite\nimagery. Our work is some of the first to create population density maps from\nbuilding detection on a large scale. The scale of our work on population\ndensity estimation via high resolution satellite images raises many issues,\nthat we will address in this paper. The first was data acquisition. Labeling\nbuildings from satellite images is a hard problem, one where we found our\nlabelers to only be about 85% accurate at. There is a tradeoff of quantity vs.\nquality of labels, so we designed two separate policies for labels meant for\ntraining sets and those meant for test sets, since our requirements of the two\nset types are quite different. We also trained weakly supervised footprint\ndetection models with the classification labels, and semi-supervised approaches\nwith a small number of pixel-level labels, which are very expensive to procure.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jul 2017 17:56:30 GMT"}], "update_date": "2017-07-28", "authors_parsed": [["Zhang", "Amy", ""], ["Liu", "Xianming", ""], ["Gros", "Andreas", ""], ["Tiecke", "Tobias", ""]]}, {"id": "1707.08985", "submitter": "Tomasz Trzcinski", "authors": "Maciej Suchecki, Tomasz Trzcinski", "title": "Understanding Aesthetics in Photography using Deep Convolutional Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evaluating aesthetic value of digital photographs is a challenging task,\nmainly due to numerous factors that need to be taken into account and\nsubjective manner of this process. In this paper, we propose to approach this\nproblem using deep convolutional neural networks. Using a dataset of over 1.7\nmillion photos collected from Flickr, we train and evaluate a deep learning\nmodel whose goal is to classify input images by analysing their aesthetic\nvalue. The result of this work is a publicly available Web-based application\nthat can be used in several real-life applications, e.g. to improve the\nworkflow of professional photographers by pre-selecting the best photos.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jul 2017 18:15:10 GMT"}, {"version": "v2", "created": "Tue, 8 Aug 2017 20:39:29 GMT"}], "update_date": "2017-08-10", "authors_parsed": [["Suchecki", "Maciej", ""], ["Trzcinski", "Tomasz", ""]]}, {"id": "1707.08991", "submitter": "Matthias Vestner", "authors": "Zorah L\\\"ahner, Matthias Vestner, Amit Boyarski, Or Litany, Ron\n  Slossberg, Tal Remez, Emanuele Rodol\\`a, Alex Bronstein, Michael Bronstein,\n  Ron Kimmel, Daniel Cremers", "title": "Efficient Deformable Shape Correspondence via Kernel Matching", "comments": "Accepted for oral presentation at 3DV 2017, including supplementary\n  material", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method to match three dimensional shapes under non-isometric\ndeformations, topology changes and partiality. We formulate the problem as\nmatching between a set of pair-wise and point-wise descriptors, imposing a\ncontinuity prior on the mapping, and propose a projected descent optimization\nprocedure inspired by difference of convex functions (DC) programming.\nSurprisingly, in spite of the highly non-convex nature of the resulting\nquadratic assignment problem, our method converges to a semantically meaningful\nand continuous mapping in most of our experiments, and scales well. We provide\npreliminary theoretical analysis and several interpretations of the method.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jul 2017 01:41:44 GMT"}, {"version": "v2", "created": "Tue, 1 Aug 2017 09:28:47 GMT"}, {"version": "v3", "created": "Fri, 15 Sep 2017 12:33:33 GMT"}], "update_date": "2017-09-18", "authors_parsed": [["L\u00e4hner", "Zorah", ""], ["Vestner", "Matthias", ""], ["Boyarski", "Amit", ""], ["Litany", "Or", ""], ["Slossberg", "Ron", ""], ["Remez", "Tal", ""], ["Rodol\u00e0", "Emanuele", ""], ["Bronstein", "Alex", ""], ["Bronstein", "Michael", ""], ["Kimmel", "Ron", ""], ["Cremers", "Daniel", ""]]}, {"id": "1707.09030", "submitter": "Marylesa Howard", "authors": "Marylesa Howard, Margaret C. Hock, B. T. Meehan, Leora\n  Dresselhaus-Cooper", "title": "A Locally Adapting Technique for Boundary Detection using Image\n  Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": "DOE/NV/25946--3282", "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rapid growth in the field of quantitative digital image analysis is paving\nthe way for researchers to make precise measurements about objects in an image.\nTo compute quantities from the image such as the density of compressed\nmaterials or the velocity of a shockwave, we must determine object boundaries.\nImages containing regions that each have a spatial trend in intensity are of\nparticular interest. We present a supervised image segmentation method that\nincorporates spatial information to locate boundaries between regions with\noverlapping intensity histograms. The segmentation of a pixel is determined by\ncomparing its intensity to distributions from local, nearby pixel intensities.\nBecause of the statistical nature of the algorithm, we use maximum likelihood\nestimation theory to quantify uncertainty about each boundary. We demonstrate\nthe success of this algorithm on a radiograph of a multicomponent cylinder and\non an optical image of a laser-induced shockwave, and we provide final boundary\nlocations with associated bands of uncertainty.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jul 2017 20:06:52 GMT"}], "update_date": "2017-07-31", "authors_parsed": [["Howard", "Marylesa", ""], ["Hock", "Margaret C.", ""], ["Meehan", "B. T.", ""], ["Dresselhaus-Cooper", "Leora", ""]]}, {"id": "1707.09074", "submitter": "Antoine Miech", "authors": "Antoine Miech, Jean-Baptiste Alayrac, Piotr Bojanowski, Ivan Laptev,\n  Josef Sivic", "title": "Learning from Video and Text via Large-Scale Discriminative Clustering", "comments": "To appear in ICCV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discriminative clustering has been successfully applied to a number of\nweakly-supervised learning tasks. Such applications include person and action\nrecognition, text-to-video alignment, object co-segmentation and colocalization\nin videos and images. One drawback of discriminative clustering, however, is\nits limited scalability. We address this issue and propose an online\noptimization algorithm based on the Block-Coordinate Frank-Wolfe algorithm. We\napply the proposed method to the problem of weakly supervised learning of\nactions and actors from movies together with corresponding movie scripts. The\nscaling up of the learning problem to 66 feature length movies enables us to\nsignificantly improve weakly supervised action recognition.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jul 2017 23:30:53 GMT"}], "update_date": "2017-07-31", "authors_parsed": [["Miech", "Antoine", ""], ["Alayrac", "Jean-Baptiste", ""], ["Bojanowski", "Piotr", ""], ["Laptev", "Ivan", ""], ["Sivic", "Josef", ""]]}, {"id": "1707.09092", "submitter": "Torsten Sattler", "authors": "Torsten Sattler and Will Maddern and Carl Toft and Akihiko Torii and\n  Lars Hammarstrand and Erik Stenborg and Daniel Safari and Masatoshi Okutomi\n  and Marc Pollefeys and Josef Sivic and Fredrik Kahl and Tomas Pajdla", "title": "Benchmarking 6DOF Outdoor Visual Localization in Changing Conditions", "comments": "Accepted to CVPR 2018 as a spotlight", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual localization enables autonomous vehicles to navigate in their\nsurroundings and augmented reality applications to link virtual to real worlds.\nPractical visual localization approaches need to be robust to a wide variety of\nviewing condition, including day-night changes, as well as weather and seasonal\nvariations, while providing highly accurate 6 degree-of-freedom (6DOF) camera\npose estimates. In this paper, we introduce the first benchmark datasets\nspecifically designed for analyzing the impact of such factors on visual\nlocalization. Using carefully created ground truth poses for query images taken\nunder a wide variety of conditions, we evaluate the impact of various factors\non 6DOF camera pose estimation accuracy through extensive experiments with\nstate-of-the-art localization approaches. Based on our results, we draw\nconclusions about the difficulty of different conditions, showing that\nlong-term localization is far from solved, and propose promising avenues for\nfuture work, including sequence-based localization approaches and the need for\nbetter local features. Our benchmark is available at visuallocalization.net.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jul 2017 02:51:11 GMT"}, {"version": "v2", "created": "Sun, 14 Jan 2018 16:31:28 GMT"}, {"version": "v3", "created": "Wed, 4 Apr 2018 18:08:02 GMT"}], "update_date": "2018-04-06", "authors_parsed": [["Sattler", "Torsten", ""], ["Maddern", "Will", ""], ["Toft", "Carl", ""], ["Torii", "Akihiko", ""], ["Hammarstrand", "Lars", ""], ["Stenborg", "Erik", ""], ["Safari", "Daniel", ""], ["Okutomi", "Masatoshi", ""], ["Pollefeys", "Marc", ""], ["Sivic", "Josef", ""], ["Kahl", "Fredrik", ""], ["Pajdla", "Tomas", ""]]}, {"id": "1707.09099", "submitter": "Kazuki Uehara", "authors": "Kazuki Uehara, Hidenori Sakanashi, Hirokazu Nosato, Masahiro Murakawa,\n  Hiroki Miyamoto, Ryosuke Nakamura", "title": "Object Detection of Satellite Images Using Multi-Channel Higher-order\n  Local Autocorrelation", "comments": "6 pages, 2 column, 7 figures, Accepted by IEEE International\n  Conference on Systems, Man, and Cybernetics (SMC) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Earth observation satellites have been monitoring the earth's surface for\na long time, and the images taken by the satellites contain large amounts of\nvaluable data. However, it is extremely hard work to manually analyze such huge\ndata. Thus, a method of automatic object detection is needed for satellite\nimages to facilitate efficient data analyses. This paper describes a new image\nfeature extended from higher-order local autocorrelation to the object\ndetection of multispectral satellite images. The feature has been extended to\nextract spectral inter-relationships in addition to spatial relationships to\nfully exploit multispectral information. The results of experiments with object\ndetection tasks conducted to evaluate the effectiveness of the proposed feature\nextension indicate that the feature realized a higher performance compared to\nexisting methods.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jul 2017 03:59:27 GMT"}], "update_date": "2017-07-31", "authors_parsed": [["Uehara", "Kazuki", ""], ["Sakanashi", "Hidenori", ""], ["Nosato", "Hirokazu", ""], ["Murakawa", "Masahiro", ""], ["Miyamoto", "Hiroki", ""], ["Nakamura", "Ryosuke", ""]]}, {"id": "1707.09100", "submitter": "Ernest C. H. Cheung", "authors": "Ernest C. Cheung, Tsan Kwong Wong, Aniket Bera, Dinesh Manocha", "title": "MixedPeds: Pedestrian Detection in Unannotated Videos using\n  Synthetically Generated Human-agents for Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new method for training pedestrian detectors on an unannotated\nset of images. We produce a mixed reality dataset that is composed of\nreal-world background images and synthetically generated static human-agents.\nOur approach is general, robust, and makes no other assumptions about the\nunannotated dataset regarding the number or location of pedestrians. We\nautomatically extract from the dataset: i) the vanishing point to calibrate the\nvirtual camera, and ii) the pedestrians' scales to generate a Spawn Probability\nMap, which is a novel concept that guides our algorithm to place the\npedestrians at appropriate locations. After putting synthetic human-agents in\nthe unannotated images, we use these augmented images to train a Pedestrian\nDetector, with the annotations generated along with the synthetic agents. We\nconducted our experiments using Faster R-CNN by comparing the detection results\non the unannotated dataset performed by the detector trained using our approach\nand detectors trained with other manually labeled datasets. We showed that our\napproach improves the average precision by 5-13% over these detectors.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jul 2017 04:05:33 GMT"}, {"version": "v2", "created": "Sat, 11 Nov 2017 19:12:55 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Cheung", "Ernest C.", ""], ["Wong", "Tsan Kwong", ""], ["Bera", "Aniket", ""], ["Manocha", "Dinesh", ""]]}, {"id": "1707.09102", "submitter": "Frederick Tung", "authors": "Frederick Tung, Srikanth Muralidharan, Greg Mori", "title": "Fine-Pruning: Joint Fine-Tuning and Compression of a Convolutional\n  Network with Bayesian Optimization", "comments": "BMVC 2017 oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When approaching a novel visual recognition problem in a specialized image\ndomain, a common strategy is to start with a pre-trained deep neural network\nand fine-tune it to the specialized domain. If the target domain covers a\nsmaller visual space than the source domain used for pre-training (e.g.\nImageNet), the fine-tuned network is likely to be over-parameterized. However,\napplying network pruning as a post-processing step to reduce the memory\nrequirements has drawbacks: fine-tuning and pruning are performed\nindependently; pruning parameters are set once and cannot adapt over time; and\nthe highly parameterized nature of state-of-the-art pruning methods make it\nprohibitive to manually search the pruning parameter space for deep networks,\nleading to coarse approximations. We propose a principled method for jointly\nfine-tuning and compressing a pre-trained convolutional network that overcomes\nthese limitations. Experiments on two specialized image domains (remote sensing\nimages and describable textures) demonstrate the validity of the proposed\napproach.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jul 2017 04:40:32 GMT"}], "update_date": "2017-07-31", "authors_parsed": [["Tung", "Frederick", ""], ["Muralidharan", "Srikanth", ""], ["Mori", "Greg", ""]]}, {"id": "1707.09119", "submitter": "Keze Wang", "authors": "Ziliang Chen and Keze Wang and Xiao Wang and Pai Peng and Ebroul\n  Izquierdo and Liang Lin", "title": "Deep Co-Space: Sample Mining Across Feature Transformation for\n  Semi-Supervised Learning", "comments": "To appear in IEEE Transactions on Circuits and Systems for Video\n  Technology (T-CSVT), 2017", "journal-ref": null, "doi": "10.1109/TCSVT.2017.2710478", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aiming at improving performance of visual classification in a cost-effective\nmanner, this paper proposes an incremental semi-supervised learning paradigm\ncalled Deep Co-Space (DCS). Unlike many conventional semi-supervised learning\nmethods usually performing within a fixed feature space, our DCS gradually\npropagates information from labeled samples to unlabeled ones along with deep\nfeature learning. We regard deep feature learning as a series of steps pursuing\nfeature transformation, i.e., projecting the samples from a previous space into\na new one, which tends to select the reliable unlabeled samples with respect to\nthis setting. Specifically, for each unlabeled image instance, we measure its\nreliability by calculating the category variations of feature transformation\nfrom two different neighborhood variation perspectives, and merged them into an\nunified sample mining criterion deriving from Hellinger distance. Then, those\nsamples keeping stable correlation to their neighboring samples (i.e., having\nsmall category variation in distribution) across the successive feature space\ntransformation, are automatically received labels and incorporated into the\nmodel for incrementally training in terms of classification. Our extensive\nexperiments on standard image classification benchmarks (e.g., Caltech-256 and\nSUN-397) demonstrate that the proposed framework is capable of effectively\nmining from large-scale unlabeled images, which boosts image classification\nperformance and achieves promising results compared to other semi-supervised\nlearning methods.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jul 2017 06:41:34 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Chen", "Ziliang", ""], ["Wang", "Keze", ""], ["Wang", "Xiao", ""], ["Peng", "Pai", ""], ["Izquierdo", "Ebroul", ""], ["Lin", "Liang", ""]]}, {"id": "1707.09135", "submitter": "Peng Liu", "authors": "Peng Liu, Ruogu Fang", "title": "Learning Pixel-Distribution Prior with Wider Convolution for Image\n  Denoising", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we explore an innovative strategy for image denoising by using\nconvolutional neural networks (CNN) to learn pixel-distribution from noisy\ndata. By increasing CNN's width with large reception fields and more channels\nin each layer, CNNs can reveal the ability to learn pixel-distribution, which\nis a prior existing in many different types of noise. The key to our approach\nis a discovery that wider CNNs tends to learn the pixel-distribution features,\nwhich provides the probability of that inference-mapping primarily relies on\nthe priors instead of deeper CNNs with more stacked nonlinear layers. We\nevaluate our work: Wide inference Networks (WIN) on additive white Gaussian\nnoise (AWGN) and demonstrate that by learning the pixel-distribution in images,\nWIN-based network consistently achieves significantly better performance than\ncurrent state-of-the-art deep CNN-based methods in both quantitative and visual\nevaluations. \\textit{Code and models are available at\n\\url{https://github.com/cswin/WIN}}.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jul 2017 08:00:21 GMT"}], "update_date": "2017-07-31", "authors_parsed": [["Liu", "Peng", ""], ["Fang", "Ruogu", ""]]}, {"id": "1707.09143", "submitter": "Pascal Mettes", "authors": "Pascal Mettes and Cees G. M. Snoek and Shih-Fu Chang", "title": "Localizing Actions from Video Labels and Pseudo-Annotations", "comments": "BMVC", "journal-ref": null, "doi": null, "report-no": "BMVC/2017/09", "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of this paper is to determine the spatio-temporal location of\nactions in video. Where training from hard to obtain box annotations is the\nnorm, we propose an intuitive and effective algorithm that localizes actions\nfrom their class label only. We are inspired by recent work showing that\nunsupervised action proposals selected with human point-supervision perform as\nwell as using expensive box annotations. Rather than asking users to provide\npoint supervision, we propose fully automatic visual cues that replace manual\npoint annotations. We call the cues pseudo-annotations, introduce five of them,\nand propose a correlation metric for automatically selecting and combining\nthem. Thorough evaluation on challenging action localization datasets shows\nthat we reach results comparable to results with full box supervision. We also\nshow that pseudo-annotations can be leveraged during testing to improve weakly-\nand strongly-supervised localizers.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jul 2017 08:16:34 GMT"}], "update_date": "2017-12-14", "authors_parsed": [["Mettes", "Pascal", ""], ["Snoek", "Cees G. M.", ""], ["Chang", "Shih-Fu", ""]]}, {"id": "1707.09145", "submitter": "Pascal Mettes", "authors": "Pascal Mettes and Cees G. M. Snoek", "title": "Spatial-Aware Object Embeddings for Zero-Shot Localization and\n  Classification of Actions", "comments": "ICCV", "journal-ref": null, "doi": null, "report-no": "ICCV/2017/10", "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We aim for zero-shot localization and classification of human actions in\nvideo. Where traditional approaches rely on global attribute or object\nclassification scores for their zero-shot knowledge transfer, our main\ncontribution is a spatial-aware object embedding. To arrive at spatial\nawareness, we build our embedding on top of freely available actor and object\ndetectors. Relevance of objects is determined in a word embedding space and\nfurther enforced with estimated spatial preferences. Besides local object\nawareness, we also embed global object awareness into our embedding to maximize\nactor and object interaction. Finally, we exploit the object positions and\nsizes in the spatial-aware embedding to demonstrate a new spatio-temporal\naction retrieval scenario with composite queries. Action localization and\nclassification experiments on four contemporary action video datasets support\nour proposal. Apart from state-of-the-art results in the zero-shot localization\nand classification settings, our spatial-aware embedding is even competitive\nwith recent supervised action localization alternatives.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jul 2017 08:21:11 GMT"}], "update_date": "2017-12-14", "authors_parsed": [["Mettes", "Pascal", ""], ["Snoek", "Cees G. M.", ""]]}, {"id": "1707.09173", "submitter": "Niki Martinel", "authors": "Giuseppe Lisanti, Niki Martinel, Alberto Del Bimbo, Gian Luca Foresti", "title": "Group Re-Identification via Unsupervised Transfer of Sparse Features\n  Encoding", "comments": "This paper has been accepted for publication at ICCV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification is best known as the problem of associating a single\nperson that is observed from one or more disjoint cameras. The existing\nliterature has mainly addressed such an issue, neglecting the fact that people\nusually move in groups, like in crowded scenarios. We believe that the\nadditional information carried by neighboring individuals provides a relevant\nvisual context that can be exploited to obtain a more robust match of single\npersons within the group. Despite this, re-identifying groups of people\ncompound the common single person re-identification problems by introducing\nchanges in the relative position of persons within the group and severe\nself-occlusions. In this paper, we propose a solution for group\nre-identification that grounds on transferring knowledge from single person\nre-identification to group re-identification by exploiting sparse dictionary\nlearning. First, a dictionary of sparse atoms is learned using patches\nextracted from single person images. Then, the learned dictionary is exploited\nto obtain a sparsity-driven residual group representation, which is finally\nmatched to perform the re-identification. Extensive experiments on the i-LIDS\ngroups and two newly collected datasets show that the proposed solution\noutperforms state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jul 2017 10:02:14 GMT"}], "update_date": "2017-07-31", "authors_parsed": [["Lisanti", "Giuseppe", ""], ["Martinel", "Niki", ""], ["Del Bimbo", "Alberto", ""], ["Foresti", "Gian Luca", ""]]}, {"id": "1707.09233", "submitter": "Alma Eguizabal", "authors": "Alma Eguizabal and Peter J. Schreier", "title": "A weighting strategy for Active Shape Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Active Shape Models (ASM) are an iterative segmentation technique to find a\nlandmark-based contour of an object. In each iteration, a least-squares fit of\na plausible shape to some detected target landmarks is determined. Finding\nthese targets is a critical step: some landmarks are more reliably detected\nthan others, and some landmarks may not be within the field of view of their\ndetectors. To add robustness while preserving simplicity at the same time, a\ngeneralized least-squares approach can be used, where a weighting matrix\nincorporates reliability information about the landmarks. We propose a strategy\nto choose this matrix, based on the covariance of empirically determined\nresiduals of the fit. We perform a further step to determine whether the target\nlandmarks are within the range of their detectors. We evaluate our strategy on\nfluoroscopic X-ray images to segment the femur. We show that our technique\noutperforms the standard ASM as well as other more heuristic weighted\nleast-squares strategies.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jul 2017 13:55:56 GMT"}], "update_date": "2017-07-31", "authors_parsed": [["Eguizabal", "Alma", ""], ["Schreier", "Peter J.", ""]]}, {"id": "1707.09240", "submitter": "Sam Toyer", "authors": "Sam Toyer, Anoop Cherian, Tengda Han, Stephen Gould", "title": "Human Pose Forecasting via Deep Markov Models", "comments": "Accepted to DICTA'17", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human pose forecasting is an important problem in computer vision with\napplications to human-robot interaction, visual surveillance, and autonomous\ndriving. Usually, forecasting algorithms use 3D skeleton sequences and are\ntrained to forecast for a few milliseconds into the future. Long-range\nforecasting is challenging due to the difficulty of estimating how long a\nperson continues an activity. To this end, our contributions are threefold: (i)\nwe propose a generative framework for poses using variational autoencoders\nbased on Deep Markov Models (DMMs); (ii) we evaluate our pose forecasts using a\npose-based action classifier, which we argue better reflects the subjective\nquality of pose forecasts than distance in coordinate space; (iii) last, for\nevaluation of the new model, we introduce a 480,000-frame video dataset called\nIkea Furniture Assembly (Ikea FA), which depicts humans repeatedly assembling\nand disassembling furniture. We demonstrate promising results for our approach\non both Ikea FA and the existing NTU RGB+D dataset.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jul 2017 23:50:23 GMT"}, {"version": "v2", "created": "Tue, 5 Sep 2017 23:26:48 GMT"}], "update_date": "2017-09-07", "authors_parsed": [["Toyer", "Sam", ""], ["Cherian", "Anoop", ""], ["Han", "Tengda", ""], ["Gould", "Stephen", ""]]}, {"id": "1707.09299", "submitter": "Tatjana Chavdarova", "authors": "Tatjana Chavdarova, Pierre Baqu\\'e, St\\'ephane Bouquet, Andrii Maksai,\n  Cijo Jose, Louis Lettry, Pascal Fua, Luc Van Gool and Fran\\c{c}ois Fleuret", "title": "The WILDTRACK Multi-Camera Person Dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  People detection methods are highly sensitive to the perpetual occlusions\namong the targets. As multi-camera set-ups become more frequently encountered,\njoint exploitation of the across views information would allow for improved\ndetection performances. We provide a large-scale HD dataset named WILDTRACK\nwhich finally makes advanced deep learning methods applicable to this problem.\nThe seven-static-camera set-up captures realistic and challenging scenarios of\nwalking people.\n  Notably, its camera calibration with jointly high-precision projection widens\nthe range of algorithms which may make use of this dataset. In aim to help\naccelerate the research on automatic camera calibration, such annotations also\naccompany this dataset.\n  Furthermore, the rich-in-appearance visual context of the pedestrian class\nmakes this dataset attractive for monocular pedestrian detection as well,\nsince: the HD cameras are placed relatively close to the people, and the size\nof the dataset further increases seven-fold.\n  In summary, we overview existing multi-camera datasets and detection methods,\nenumerate details of our dataset, and we benchmark multi-camera state of the\nart detectors on this new dataset.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jul 2017 16:05:06 GMT"}], "update_date": "2017-07-31", "authors_parsed": [["Chavdarova", "Tatjana", ""], ["Baqu\u00e9", "Pierre", ""], ["Bouquet", "St\u00e9phane", ""], ["Maksai", "Andrii", ""], ["Jose", "Cijo", ""], ["Lettry", "Louis", ""], ["Fua", "Pascal", ""], ["Van Gool", "Luc", ""], ["Fleuret", "Fran\u00e7ois", ""]]}, {"id": "1707.09316", "submitter": "Shihua Zhang", "authors": "Zhenxing Guo and Shihua Zhang", "title": "Sparse Deep Nonnegative Matrix Factorization", "comments": "13 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonnegative matrix factorization is a powerful technique to realize dimension\nreduction and pattern recognition through single-layer data representation\nlearning. Deep learning, however, with its carefully designed hierarchical\nstructure, is able to combine hidden features to form more representative\nfeatures for pattern recognition. In this paper, we proposed sparse deep\nnonnegative matrix factorization models to analyze complex data for more\naccurate classification and better feature interpretation. Such models are\ndesigned to learn localized features or generate more discriminative\nrepresentations for samples in distinct classes by imposing $L_1$-norm penalty\non the columns of certain factors. By extending one-layer model into\nmulti-layer one with sparsity, we provided a hierarchical way to analyze big\ndata and extract hidden features intuitively due to nonnegativity. We adopted\nthe Nesterov's accelerated gradient algorithm to accelerate the computing\nprocess with the convergence rate of $O(1/k^2)$ after $k$ steps iteration. We\nalso analyzed the computing complexity of our framework to demonstrate their\nefficiency. To improve the performance of dealing with linearly inseparable\ndata, we also considered to incorporate popular nonlinear functions into this\nframework and explored their performance. We applied our models onto two\nbenchmarking image datasets, demonstrating our models can achieve competitive\nor better classification performance and produce intuitive interpretations\ncompared with the typical NMF and competing multi-layer models.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jul 2017 16:37:37 GMT"}], "update_date": "2017-07-31", "authors_parsed": [["Guo", "Zhenxing", ""], ["Zhang", "Shihua", ""]]}, {"id": "1707.09332", "submitter": "Max Lieblich", "authors": "Max Lieblich and Lucas Van Meter", "title": "Two Hilbert schemes in computer vision", "comments": "Shortened version, to appear in SIAGA. Note that theorem, etc.,\n  numbering is different in published version than it is here. Corrected\n  notational error in Notation 3.2 and Proposition 3.3, enhanced reference\n  capitalization", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.AG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study multiview moduli problems that arise in computer vision. We show\nthat these moduli spaces are always smooth and irreducible, in both the\ncalibrated and uncalibrated cases, for any number of views. We also show that\nthese moduli spaces always admit open immersions into Hilbert schemes for more\nthan two views, extending and refining work of Aholt-Sturmfels-Thomas. We use\nthese moduli spaces to study and extend the classical twisted pair covering of\nthe essential variety.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jul 2017 17:13:22 GMT"}, {"version": "v2", "created": "Mon, 21 May 2018 23:18:00 GMT"}, {"version": "v3", "created": "Wed, 11 Jul 2018 22:36:53 GMT"}, {"version": "v4", "created": "Mon, 12 Aug 2019 13:11:32 GMT"}, {"version": "v5", "created": "Sat, 14 Dec 2019 01:23:53 GMT"}, {"version": "v6", "created": "Tue, 7 Jan 2020 19:58:40 GMT"}], "update_date": "2020-01-09", "authors_parsed": [["Lieblich", "Max", ""], ["Van Meter", "Lucas", ""]]}, {"id": "1707.09364", "submitter": "Weilin Cong", "authors": "Weilin Cong, Sanyuan Zhao, Hui Tian, Jianbing Shen", "title": "Improved Face Detection and Alignment using Cascade Deep Convolutional\n  Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-world face detection and alignment demand an advanced discriminative\nmodel to address challenges by pose, lighting and expression. Illuminated by\nthe deep learning algorithm, some convolutional neural networks based face\ndetection and alignment methods have been proposed. Recent studies have\nutilized the relation between face detection and alignment to make models\ncomputationally efficiency, however they ignore the connection between each\ncascade CNNs. In this paper, we propose an structure to propose higher quality\ntraining data for End-to-End cascade network training, which give computers\nmore space to automatic adjust weight parameter and accelerate convergence.\nExperiments demonstrate considerable improvement over existing detection and\nalignment models.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jul 2017 06:07:38 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Cong", "Weilin", ""], ["Zhao", "Sanyuan", ""], ["Tian", "Hui", ""], ["Shen", "Jianbing", ""]]}, {"id": "1707.09376", "submitter": "Blaz Meden", "authors": "Bla\\v{z} Meden, Refik Can Mall{\\i}, Sebastjan Fabijan, Haz{\\i}m Kemal\n  Ekenel, Vitomir \\v{S}truc, Peter Peer", "title": "Face Deidentification with Generative Deep Neural Networks", "comments": "IET Signal Processing Special Issue on Deidentification 2017", "journal-ref": null, "doi": "10.1049/iet-spr.2017.0049", "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face deidentification is an active topic amongst privacy and security\nresearchers. Early deidentification methods relying on image blurring or\npixelization were replaced in recent years with techniques based on formal\nanonymity models that provide privacy guaranties and at the same time aim at\nretaining certain characteristics of the data even after deidentification. The\nlatter aspect is particularly important, as it allows to exploit the\ndeidentified data in applications for which identity information is irrelevant.\nIn this work we present a novel face deidentification pipeline, which ensures\nanonymity by synthesizing artificial surrogate faces using generative neural\nnetworks (GNNs). The generated faces are used to deidentify subjects in images\nor video, while preserving non-identity-related aspects of the data and\nconsequently enabling data utilization. Since generative networks are very\nadaptive and can utilize a diverse set of parameters (pertaining to the\nappearance of the generated output in terms of facial expressions, gender,\nrace, etc.), they represent a natural choice for the problem of face\ndeidentification. To demonstrate the feasibility of our approach, we perform\nexperiments using automated recognition tools and human annotators. Our results\nshow that the recognition performance on deidentified images is close to\nchance, suggesting that the deidentification process based on GNNs is highly\neffective.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jul 2017 18:39:09 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Meden", "Bla\u017e", ""], ["Mall\u0131", "Refik Can", ""], ["Fabijan", "Sebastjan", ""], ["Ekenel", "Haz\u0131m Kemal", ""], ["\u0160truc", "Vitomir", ""], ["Peer", "Peter", ""]]}, {"id": "1707.09405", "submitter": "Qifeng Chen", "authors": "Qifeng Chen and Vladlen Koltun", "title": "Photographic Image Synthesis with Cascaded Refinement Networks", "comments": "Published at the International Conference on Computer Vision (ICCV\n  2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach to synthesizing photographic images conditioned on\nsemantic layouts. Given a semantic label map, our approach produces an image\nwith photographic appearance that conforms to the input layout. The approach\nthus functions as a rendering engine that takes a two-dimensional semantic\nspecification of the scene and produces a corresponding photographic image.\nUnlike recent and contemporaneous work, our approach does not rely on\nadversarial training. We show that photographic images can be synthesized from\nsemantic layouts by a single feedforward network with appropriate structure,\ntrained end-to-end with a direct regression objective. The presented approach\nscales seamlessly to high resolutions; we demonstrate this by synthesizing\nphotographic images at 2-megapixel resolution, the full resolution of our\ntraining data. Extensive perceptual experiments on datasets of outdoor and\nindoor scenes demonstrate that images synthesized by the presented approach are\nconsiderably more realistic than alternative approaches. The results are shown\nin the supplementary video at https://youtu.be/0fhUJT21-bs\n", "versions": [{"version": "v1", "created": "Fri, 28 Jul 2017 20:24:44 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Chen", "Qifeng", ""], ["Koltun", "Vladlen", ""]]}, {"id": "1707.09416", "submitter": "Michael Hong Gang Li", "authors": "Michael H. Li, Tiago A. Mestre, Susan H. Fox, Babak Taati", "title": "Vision-Based Assessment of Parkinsonism and Levodopa-Induced Dyskinesia\n  with Deep Learning Pose Estimation", "comments": "8 pages, 1 figure. Under review", "journal-ref": "Journal of NeuroEngineering and Rehabilitation (2018) 15:97", "doi": "10.1186/s12984-018-0446-z", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective: To apply deep learning pose estimation algorithms for vision-based\nassessment of parkinsonism and levodopa-induced dyskinesia (LID). Methods: Nine\nparticipants with Parkinson's disease (PD) and LID completed a levodopa\ninfusion protocol, where symptoms were assessed at regular intervals using the\nUnified Dyskinesia Rating Scale (UDysRS) and Unified Parkinson's Disease Rating\nScale (UPDRS). A state-of-the-art deep learning pose estimation method was used\nto extract movement trajectories from videos of PD assessments. Features of the\nmovement trajectories were used to detect and estimate the severity of\nparkinsonism and LID using random forest. Communication and drinking tasks were\nused to assess LID, while leg agility and toe tapping tasks were used to assess\nparkinsonism. Feature sets from tasks were also combined to predict total\nUDysRS and UPDRS Part III scores. Results: For LID, the communication task\nyielded the best results for dyskinesia (severity estimation: r = 0.661,\ndetection: AUC = 0.930). For parkinsonism, leg agility had better results for\nseverity estimation (r = 0.618), while toe tapping was better for detection\n(AUC = 0.773). UDysRS and UPDRS Part III scores were predicted with r = 0.741\nand 0.530, respectively. Conclusion: This paper presents the first application\nof deep learning for vision-based assessment of parkinsonism and LID and\ndemonstrates promising performance for the future translation of deep learning\nto PD clinical practices. Significance: The proposed system provides insight\ninto the potential of computer vision and deep learning for clinical\napplication in PD.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jul 2017 20:56:22 GMT"}, {"version": "v2", "created": "Tue, 1 Aug 2017 16:03:22 GMT"}], "update_date": "2018-11-12", "authors_parsed": [["Li", "Michael H.", ""], ["Mestre", "Tiago A.", ""], ["Fox", "Susan H.", ""], ["Taati", "Babak", ""]]}, {"id": "1707.09418", "submitter": "Chang Xiao", "authors": "Chang Xiao, Cheng Zhang and Changxi Zheng", "title": "FontCode: Embedding Information in Text Documents using Glyph\n  Perturbation", "comments": null, "journal-ref": null, "doi": "10.1145/3152823", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce FontCode, an information embedding technique for text documents.\nProvided a text document with specific fonts, our method embeds user-specified\ninformation in the text by perturbing the glyphs of text characters while\npreserving the text content. We devise an algorithm to chooses unobtrusive yet\nmachine-recognizable glyph perturbations, leveraging a recently developed\ngenerative model that alters the glyphs of each character continuously on a\nfont manifold. We then introduce an algorithm that embeds a user-provided\nmessage in the text document and produces an encoded document whose appearance\nis minimally perturbed from the original document. We also present a glyph\nrecognition method that recovers the embedded information from an encoded\ndocument stored as a vector graphic or pixel image, or even on a printed paper.\nIn addition, we introduce a new error-correction coding scheme that rectifies a\ncertain number of recognition errors. Lastly, we demonstrate that our technique\nenables a wide array of applications, using it as a text document metadata\nholder, an unobtrusive optical barcode, a cryptographic message embedding\nscheme, and a text document signature.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jul 2017 21:03:50 GMT"}], "update_date": "2019-06-12", "authors_parsed": [["Xiao", "Chang", ""], ["Zhang", "Cheng", ""], ["Zheng", "Changxi", ""]]}, {"id": "1707.09423", "submitter": "Ruichi Yu", "authors": "Ruichi Yu, Ang Li, Vlad I. Morariu, Larry S. Davis", "title": "Visual Relationship Detection with Internal and External Linguistic\n  Knowledge Distillation", "comments": "ICCV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding visual relationships involves identifying the subject, the\nobject, and a predicate relating them. We leverage the strong correlations\nbetween the predicate and the (subj,obj) pair (both semantically and spatially)\nto predict the predicates conditioned on the subjects and the objects. Modeling\nthe three entities jointly more accurately reflects their relationships, but\ncomplicates learning since the semantic space of visual relationships is huge\nand the training data is limited, especially for the long-tail relationships\nthat have few instances. To overcome this, we use knowledge of linguistic\nstatistics to regularize visual model learning. We obtain linguistic knowledge\nby mining from both training annotations (internal knowledge) and publicly\navailable text, e.g., Wikipedia (external knowledge), computing the conditional\nprobability distribution of a predicate given a (subj,obj) pair. Then, we\ndistill the knowledge into a deep model to achieve better generalization. Our\nexperimental results on the Visual Relationship Detection (VRD) and Visual\nGenome datasets suggest that with this linguistic knowledge distillation, our\nmodel outperforms the state-of-the-art methods significantly, especially when\npredicting unseen relationships (e.g., recall improved from 8.45% to 19.17% on\nVRD zero-shot testing set).\n", "versions": [{"version": "v1", "created": "Fri, 28 Jul 2017 21:31:00 GMT"}, {"version": "v2", "created": "Thu, 3 Aug 2017 00:11:33 GMT"}], "update_date": "2017-08-04", "authors_parsed": [["Yu", "Ruichi", ""], ["Li", "Ang", ""], ["Morariu", "Vlad I.", ""], ["Davis", "Larry S.", ""]]}, {"id": "1707.09457", "submitter": "Kai-Wei Chang", "authors": "Jieyu Zhao and Tianlu Wang and Mark Yatskar and Vicente Ordonez and\n  Kai-Wei Chang", "title": "Men Also Like Shopping: Reducing Gender Bias Amplification using\n  Corpus-level Constraints", "comments": "11 pages, published in EMNLP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Language is increasingly being used to define rich visual recognition\nproblems with supporting image collections sourced from the web. Structured\nprediction models are used in these tasks to take advantage of correlations\nbetween co-occurring labels and visual input but risk inadvertently encoding\nsocial biases found in web corpora. In this work, we study data and models\nassociated with multilabel object classification and visual semantic role\nlabeling. We find that (a) datasets for these tasks contain significant gender\nbias and (b) models trained on these datasets further amplify existing bias.\nFor example, the activity cooking is over 33% more likely to involve females\nthan males in a training set, and a trained model further amplifies the\ndisparity to 68% at test time. We propose to inject corpus-level constraints\nfor calibrating existing structured prediction models and design an algorithm\nbased on Lagrangian relaxation for collective inference. Our method results in\nalmost no performance loss for the underlying recognition task but decreases\nthe magnitude of bias amplification by 47.5% and 40.5% for multilabel\nclassification and visual semantic role labeling, respectively.\n", "versions": [{"version": "v1", "created": "Sat, 29 Jul 2017 03:38:32 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Zhao", "Jieyu", ""], ["Wang", "Tianlu", ""], ["Yatskar", "Mark", ""], ["Ordonez", "Vicente", ""], ["Chang", "Kai-Wei", ""]]}, {"id": "1707.09465", "submitter": "Yang Zhang", "authors": "Yang Zhang, Philip David, Boqing Gong", "title": "Curriculum Domain Adaptation for Semantic Segmentation of Urban Scenes", "comments": "This is the extended version of the ICCV 2017 paper \"Curriculum\n  Domain Adaptation for Semantic Segmentation of Urban Scenes\" with additional\n  GTA experiment", "journal-ref": null, "doi": "10.1109/ICCV.2017.223", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  During the last half decade, convolutional neural networks (CNNs) have\ntriumphed over semantic segmentation, which is one of the core tasks in many\napplications such as autonomous driving. However, to train CNNs requires a\nconsiderable amount of data, which is difficult to collect and laborious to\nannotate. Recent advances in computer graphics make it possible to train CNNs\non photo-realistic synthetic imagery with computer-generated annotations.\nDespite this, the domain mismatch between the real images and the synthetic\ndata cripples the models' performance. Hence, we propose a curriculum-style\nlearning approach to minimize the domain gap in urban scenery semantic\nsegmentation. The curriculum domain adaptation solves easy tasks first to infer\nnecessary properties about the target domain; in particular, the first task is\nto learn global label distributions over images and local distributions over\nlandmark superpixels. These are easy to estimate because images of urban scenes\nhave strong idiosyncrasies (e.g., the size and spatial relations of buildings,\nstreets, cars, etc.). We then train a segmentation network while regularizing\nits predictions in the target domain to follow those inferred properties. In\nexperiments, our method outperforms the baselines on two datasets and two\nbackbone networks. We also report extensive ablation studies about our\napproach.\n", "versions": [{"version": "v1", "created": "Sat, 29 Jul 2017 05:47:43 GMT"}, {"version": "v2", "created": "Sat, 5 Aug 2017 05:01:47 GMT"}, {"version": "v3", "created": "Tue, 5 Jun 2018 20:21:52 GMT"}, {"version": "v4", "created": "Fri, 19 Oct 2018 21:55:45 GMT"}, {"version": "v5", "created": "Wed, 14 Nov 2018 01:03:47 GMT"}], "update_date": "2018-11-15", "authors_parsed": [["Zhang", "Yang", ""], ["David", "Philip", ""], ["Gong", "Boqing", ""]]}, {"id": "1707.09468", "submitter": "Rowan Zellers", "authors": "Rowan Zellers, Yejin Choi", "title": "Zero-Shot Activity Recognition with Verb Attribute Induction", "comments": "accepted to EMNLP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate large-scale zero-shot activity recognition by\nmodeling the visual and linguistic attributes of action verbs. For example, the\nverb \"salute\" has several properties, such as being a light movement, a social\nact, and short in duration. We use these attributes as the internal mapping\nbetween visual and textual representations to reason about a previously unseen\naction. In contrast to much prior work that assumes access to gold standard\nattributes for zero-shot classes and focuses primarily on object attributes,\nour model uniquely learns to infer action attributes from dictionary\ndefinitions and distributed word representations. Experimental results confirm\nthat action attributes inferred from language can provide a predictive signal\nfor zero-shot prediction of previously unseen activities.\n", "versions": [{"version": "v1", "created": "Sat, 29 Jul 2017 06:05:52 GMT"}, {"version": "v2", "created": "Sat, 2 Sep 2017 19:53:20 GMT"}], "update_date": "2017-09-05", "authors_parsed": [["Zellers", "Rowan", ""], ["Choi", "Yejin", ""]]}, {"id": "1707.09472", "submitter": "Julia Peyre", "authors": "Julia Peyre, Ivan Laptev, Cordelia Schmid, Josef Sivic", "title": "Weakly-supervised learning of visual relations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a novel approach for modeling visual relations between\npairs of objects. We call relation a triplet of the form (subject, predicate,\nobject) where the predicate is typically a preposition (eg. 'under', 'in front\nof') or a verb ('hold', 'ride') that links a pair of objects (subject, object).\nLearning such relations is challenging as the objects have different spatial\nconfigurations and appearances depending on the relation in which they occur.\nAnother major challenge comes from the difficulty to get annotations,\nespecially at box-level, for all possible triplets, which makes both learning\nand evaluation difficult. The contributions of this paper are threefold. First,\nwe design strong yet flexible visual features that encode the appearance and\nspatial configuration for pairs of objects. Second, we propose a\nweakly-supervised discriminative clustering model to learn relations from\nimage-level labels only. Third we introduce a new challenging dataset of\nunusual relations (UnRel) together with an exhaustive annotation, that enables\naccurate evaluation of visual relation retrieval. We show experimentally that\nour model results in state-of-the-art results on the visual relationship\ndataset significantly improving performance on previously unseen relations\n(zero-shot learning), and confirm this observation on our newly introduced\nUnRel dataset.\n", "versions": [{"version": "v1", "created": "Sat, 29 Jul 2017 06:41:36 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Peyre", "Julia", ""], ["Laptev", "Ivan", ""], ["Schmid", "Cordelia", ""], ["Sivic", "Josef", ""]]}, {"id": "1707.09476", "submitter": "Shanghang Zhang", "authors": "Shanghang Zhang, Guanhang Wu, Jo\\~ao P. Costeira, Jos\\'e M. F. Moura", "title": "FCN-rLSTM: Deep Spatio-Temporal Neural Networks for Vehicle Counting in\n  City Cameras", "comments": "Accepted by International Conference on Computer Vision (ICCV), 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we develop deep spatio-temporal neural networks to\nsequentially count vehicles from low quality videos captured by city cameras\n(citycams). Citycam videos have low resolution, low frame rate, high occlusion\nand large perspective, making most existing methods lose their efficacy. To\novercome limitations of existing methods and incorporate the temporal\ninformation of traffic video, we design a novel FCN-rLSTM network to jointly\nestimate vehicle density and vehicle count by connecting fully convolutional\nneural networks (FCN) with long short term memory networks (LSTM) in a residual\nlearning fashion. Such design leverages the strengths of FCN for pixel-level\nprediction and the strengths of LSTM for learning complex temporal dynamics.\nThe residual learning connection reformulates the vehicle count regression as\nlearning residual functions with reference to the sum of densities in each\nframe, which significantly accelerates the training of networks. To preserve\nfeature map resolution, we propose a Hyper-Atrous combination to integrate\natrous convolution in FCN and combine feature maps of different convolution\nlayers. FCN-rLSTM enables refined feature representation and a novel end-to-end\ntrainable mapping from pixels to vehicle count. We extensively evaluated the\nproposed method on different counting tasks with three datasets, with\nexperimental results demonstrating their effectiveness and robustness. In\nparticular, FCN-rLSTM reduces the mean absolute error (MAE) from 5.31 to 4.21\non TRANCOS, and reduces the MAE from 2.74 to 1.53 on WebCamT. Training process\nis accelerated by 5 times on average.\n", "versions": [{"version": "v1", "created": "Sat, 29 Jul 2017 07:22:48 GMT"}, {"version": "v2", "created": "Tue, 1 Aug 2017 00:33:29 GMT"}], "update_date": "2017-08-02", "authors_parsed": [["Zhang", "Shanghang", ""], ["Wu", "Guanhang", ""], ["Costeira", "Jo\u00e3o P.", ""], ["Moura", "Jos\u00e9 M. F.", ""]]}, {"id": "1707.09482", "submitter": "Xianxu Hou", "authors": "Xianxu Hou, Jiang Duan, Guoping Qiu", "title": "Deep Feature Consistent Deep Image Transformations: Downscaling,\n  Decolorization and HDR Tone Mapping", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building on crucial insights into the determining factors of the visual\nintegrity of an image and the property of deep convolutional neural network\n(CNN), we have developed the Deep Feature Consistent Deep Image Transformation\n(DFC-DIT) framework which unifies challenging one-to-many mapping image\nprocessing problems such as image downscaling, decolorization (colour to\ngrayscale conversion) and high dynamic range (HDR) image tone mapping. We train\none CNN as a non-linear mapper to transform an input image to an output image\nfollowing what we term the deep feature consistency principle which is enforced\nthrough another pretrained and fixed deep CNN. This is the first work that uses\ndeep learning to solve and unify these three common image processing tasks. We\npresent experimental results to demonstrate the effectiveness of the DFC-DIT\ntechnique and its state of the art performances.\n", "versions": [{"version": "v1", "created": "Sat, 29 Jul 2017 08:37:32 GMT"}, {"version": "v2", "created": "Mon, 11 Sep 2017 02:26:53 GMT"}], "update_date": "2017-09-12", "authors_parsed": [["Hou", "Xianxu", ""], ["Duan", "Jiang", ""], ["Qiu", "Guoping", ""]]}, {"id": "1707.09531", "submitter": "Yu Liu", "authors": "Yu Liu, Hongyang Li, Junjie Yan, Fangyin Wei, Xiaogang Wang, Xiaoou\n  Tang", "title": "Recurrent Scale Approximation for Object Detection in CNN", "comments": "Accepted in ICCV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since convolutional neural network (CNN) lacks an inherent mechanism to\nhandle large scale variations, we always need to compute feature maps multiple\ntimes for multi-scale object detection, which has the bottleneck of\ncomputational cost in practice. To address this, we devise a recurrent scale\napproximation (RSA) to compute feature map once only, and only through this map\ncan we approximate the rest maps on other levels. At the core of RSA is the\nrecursive rolling out mechanism: given an initial map at a particular scale, it\ngenerates the prediction at a smaller scale that is half the size of input. To\nfurther increase efficiency and accuracy, we (a): design a scale-forecast\nnetwork to globally predict potential scales in the image since there is no\nneed to compute maps on all levels of the pyramid. (b): propose a landmark\nretracing network (LRN) to trace back locations of the regressed landmarks and\ngenerate a confidence score for each landmark; LRN can effectively alleviate\nfalse positives caused by the accumulated error in RSA. The whole system can be\ntrained end-to-end in a unified CNN framework. Experiments demonstrate that our\nproposed algorithm is superior against state-of-the-art methods on face\ndetection benchmarks and achieves comparable results for generic proposal\ngeneration. The source code of RSA is available at\ngithub.com/sciencefans/RSA-for-object-detection.\n", "versions": [{"version": "v1", "created": "Sat, 29 Jul 2017 15:38:27 GMT"}, {"version": "v2", "created": "Thu, 8 Feb 2018 05:49:21 GMT"}], "update_date": "2018-02-09", "authors_parsed": [["Liu", "Yu", ""], ["Li", "Hongyang", ""], ["Yan", "Junjie", ""], ["Wei", "Fangyin", ""], ["Wang", "Xiaogang", ""], ["Tang", "Xiaoou", ""]]}, {"id": "1707.09543", "submitter": "Lee Friedman", "authors": "Lee Friedman and Oleg Komogortsev", "title": "Synthetic Database for Evaluation of General, Fundamental Biometric\n  Principles", "comments": "8 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We create synthetic biometric databases to study general, fundamental,\nbiometric principles. First, we check the validity of the synthetic database\ndesign by comparing it to real data in terms of biometric performance. The real\ndata used for this validity check was from an eye-movement related biometric\ndatabase. Next, we employ our database to evaluate the impact of variations of\ntemporal persistence of features on biometric performance. We index temporal\npersistence with the intraclass correlation coefficient (ICC). We find that\nvariations in temporal persistence are extremely highly correlated with\nvariations in biometric performance. Finally, we use our synthetic database\nstrategy to determine how many features are required to achieve particular\nlevels of performance as the number of subjects in the database increases from\n100 to 10,000. An important finding is that the number of features required to\nachieve various EER values (2%, 0.3%, 0.15%) is essentially constant in the\ndatabase sizes that we studied. We hypothesize that the insights obtained from\nour study would be applicable to many biometric modalities where extracted\nfeature properties resemble the properties of the synthetic features we discuss\nin this work.\n", "versions": [{"version": "v1", "created": "Sat, 29 Jul 2017 17:35:49 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Friedman", "Lee", ""], ["Komogortsev", "Oleg", ""]]}, {"id": "1707.09557", "submitter": "Edward Smith", "authors": "Edward Smith, David Meger", "title": "Improved Adversarial Systems for 3D Object Generation and Reconstruction", "comments": "10 pages, accepted at CORL. Figures are best view in color, and\n  details only appear when zoomed in", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a new approach for training generative adversarial\nnetworks (GAN) to understand the detailed 3D shape of objects. While GANs have\nbeen used in this domain previously, they are notoriously hard to train,\nespecially for the complex joint data distribution over 3D objects of many\ncategories and orientations. Our method extends previous work by employing the\nWasserstein distance normalized with gradient penalization as a training\nobjective. This enables improved generation from the joint object shape\ndistribution. Our system can also reconstruct 3D shape from 2D images and\nperform shape completion from occluded 2.5D range scans. We achieve notable\nquantitative improvements in comparison to existing baselines\n", "versions": [{"version": "v1", "created": "Sat, 29 Jul 2017 21:03:22 GMT"}, {"version": "v2", "created": "Tue, 1 Aug 2017 15:44:52 GMT"}, {"version": "v3", "created": "Mon, 30 Oct 2017 18:49:29 GMT"}], "update_date": "2017-11-01", "authors_parsed": [["Smith", "Edward", ""], ["Meger", "David", ""]]}, {"id": "1707.09585", "submitter": "Avi Ben-Cohen", "authors": "Avi Ben-Cohen, Eyal Klang, Stephen P. Raskin, Michal Marianne Amitai,\n  and Hayit Greenspan", "title": "Virtual PET Images from CT Data Using Deep Convolutional Networks:\n  Initial Results", "comments": "To be presented at SASHIMI2017: Simulation and Synthesis in Medical\n  Imaging, MICCAI 2017", "journal-ref": null, "doi": "10.1007/978-3-319-68127-6_6", "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we present a novel system for PET estimation using CT scans. We\nexplore the use of fully convolutional networks (FCN) and conditional\ngenerative adversarial networks (GAN) to export PET data from CT data. Our\ndataset includes 25 pairs of PET and CT scans where 17 were used for training\nand 8 for testing. The system was tested for detection of malignant tumors in\nthe liver region. Initial results look promising showing high detection\nperformance with a TPR of 92.3% and FPR of 0.25 per case. Future work entails\nexpansion of the current system to the entire body using a much larger dataset.\nSuch a system can be used for tumor detection and drug treatment evaluation in\na CT-only environment instead of the expansive and radioactive PET-CT scan.\n", "versions": [{"version": "v1", "created": "Sun, 30 Jul 2017 06:43:42 GMT"}], "update_date": "2018-07-24", "authors_parsed": [["Ben-Cohen", "Avi", ""], ["Klang", "Eyal", ""], ["Raskin", "Stephen P.", ""], ["Amitai", "Michal Marianne", ""], ["Greenspan", "Hayit", ""]]}, {"id": "1707.09593", "submitter": "Kai Chen", "authors": "Kai Chen, Hang Song, Chen Change Loy, Dahua Lin", "title": "Discover and Learn New Objects from Documentaries", "comments": "Published on CVPR 2017 (spotlight)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the remarkable progress in recent years, detecting objects in a new\ncontext remains a challenging task. Detectors learned from a public dataset can\nonly work with a fixed list of categories, while training from scratch usually\nrequires a large amount of training data with detailed annotations. This work\naims to explore a novel approach -- learning object detectors from documentary\nfilms in a weakly supervised manner. This is inspired by the observation that\ndocumentaries often provide dedicated exposition of certain object categories,\nwhere visual presentations are aligned with subtitles. We believe that object\ndetectors can be learned from such a rich source of information. Towards this\ngoal, we develop a joint probabilistic framework, where individual pieces of\ninformation, including video frames and subtitles, are brought together via\nboth visual and linguistic links. On top of this formulation, we further derive\na weakly supervised learning algorithm, where object model learning and\ntraining set mining are unified in an optimization procedure. Experimental\nresults on a real world dataset demonstrate that this is an effective approach\nto learning new object detectors.\n", "versions": [{"version": "v1", "created": "Sun, 30 Jul 2017 07:52:29 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Chen", "Kai", ""], ["Song", "Hang", ""], ["Loy", "Chen Change", ""], ["Lin", "Dahua", ""]]}, {"id": "1707.09597", "submitter": "Hao Chen", "authors": "Huangjing Lin, Hao Chen, Qi Dou, Liansheng Wang, Jing Qin, Pheng-Ann\n  Heng", "title": "ScanNet: A Fast and Dense Scanning Framework for Metastatic Breast\n  Cancer Detection from Whole-Slide Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lymph node metastasis is one of the most significant diagnostic indicators in\nbreast cancer, which is traditionally observed under the microscope by\npathologists. In recent years, computerized histology diagnosis has become one\nof the most rapidly expanding fields in medical image computing, which\nalleviates pathologists' workload and reduces misdiagnosis rate. However,\nautomatic detection of lymph node metastases from whole slide images remains a\nchallenging problem, due to the large-scale data with enormous resolutions and\nexistence of hard mimics. In this paper, we propose a novel framework by\nleveraging fully convolutional networks for efficient inference to meet the\nspeed requirement for clinical practice, while reconstructing dense predictions\nunder different offsets for ensuring accurate detection on both micro- and\nmacro-metastases. Incorporating with the strategies of asynchronous sample\nprefetching and hard negative mining, the network can be effectively trained.\nExtensive experiments on the benchmark dataset of 2016 Camelyon Grand Challenge\ncorroborated the efficacy of our method. Compared with the state-of-the-art\nmethods, our method achieved superior performance with a faster speed on the\ntumor localization task and surpassed human performance on the WSI\nclassification task.\n", "versions": [{"version": "v1", "created": "Sun, 30 Jul 2017 08:51:32 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Lin", "Huangjing", ""], ["Chen", "Hao", ""], ["Dou", "Qi", ""], ["Wang", "Liansheng", ""], ["Qin", "Jing", ""], ["Heng", "Pheng-Ann", ""]]}, {"id": "1707.09603", "submitter": "Menandro Roxas", "authors": "Menandro Roxas, Tomoki Hori, Taiki Fukiage, Yasuhide Okamoto, Takeshi\n  Oishi", "title": "Occlusion Handling using Semantic Segmentation and Visibility-Based\n  Rendering for Mixed Reality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-time occlusion handling is a major problem in outdoor mixed reality\nsystem because it requires great computational cost mainly due to the\ncomplexity of the scene. Using only segmentation, it is difficult to accurately\nrender a virtual object occluded by complex objects such as trees, bushes etc.\nIn this paper, we propose a novel occlusion handling method for real-time,\noutdoor, and omni-directional mixed reality system using only the information\nfrom a monocular image sequence. We first present a semantic segmentation\nscheme for predicting the amount of visibility for different type of objects in\nthe scene. We also simultaneously calculate a foreground probability map using\ndepth estimation derived from optical flow. Finally, we combine the\nsegmentation result and the probability map to render the computer generated\nobject and the real scene using a visibility-based rendering method. Our\nresults show great improvement in handling occlusions compared to existing\nblending based methods.\n", "versions": [{"version": "v1", "created": "Sun, 30 Jul 2017 10:01:57 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Roxas", "Menandro", ""], ["Hori", "Tomoki", ""], ["Fukiage", "Taiki", ""], ["Okamoto", "Yasuhide", ""], ["Oishi", "Takeshi", ""]]}, {"id": "1707.09605", "submitter": "Vishwanath Sindagi", "authors": "Vishwanath A. Sindagi and Vishal M. Patel", "title": "CNN-based Cascaded Multi-task Learning of High-level Prior and Density\n  Estimation for Crowd Counting", "comments": "Accepted at AVSS 2017 (14th International Conference on Advanced\n  Video and Signal Based Surveillance)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating crowd count in densely crowded scenes is an extremely challenging\ntask due to non-uniform scale variations. In this paper, we propose a novel\nend-to-end cascaded network of CNNs to jointly learn crowd count classification\nand density map estimation. Classifying crowd count into various groups is\ntantamount to coarsely estimating the total count in the image thereby\nincorporating a high-level prior into the density estimation network. This\nenables the layers in the network to learn globally relevant discriminative\nfeatures which aid in estimating highly refined density maps with lower count\nerror. The joint training is performed in an end-to-end fashion. Extensive\nexperiments on highly challenging publicly available datasets show that the\nproposed method achieves lower count error and better quality density maps as\ncompared to the recent state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sun, 30 Jul 2017 10:24:08 GMT"}, {"version": "v2", "created": "Wed, 16 Aug 2017 06:32:00 GMT"}], "update_date": "2017-08-17", "authors_parsed": [["Sindagi", "Vishwanath A.", ""], ["Patel", "Vishal M.", ""]]}, {"id": "1707.09636", "submitter": "Yi Zhang", "authors": "Hu Chen, Yi Zhang, Yunjin Chen, Junfeng Zhang, Weihua Zhang,\n  Huaiqiaing Sun, Yang Lv, Peixi Liao, Jiliu Zhou and Ge Wang", "title": "LEARN: Learned Experts' Assessment-based Reconstruction Network for\n  Sparse-data CT", "comments": "18 pages, 15 figures, accepted by IEEE TMI", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.med-ph cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compressive sensing (CS) has proved effective for tomographic reconstruction\nfrom sparsely collected data or under-sampled measurements, which are\npractically important for few-view CT, tomosynthesis, interior tomography, and\nso on. To perform sparse-data CT, the iterative reconstruction commonly use\nregularizers in the CS framework. Currently, how to choose the parameters\nadaptively for regularization is a major open problem. In this paper, inspired\nby the idea of machine learning especially deep learning, we unfold a\nstate-of-the-art \"fields of experts\" based iterative reconstruction scheme up\nto a number of iterations for data-driven training, construct a Learned\nExperts' Assessment-based Reconstruction Network (\"LEARN\") for sparse-data CT,\nand demonstrate the feasibility and merits of our LEARN network. The\nexperimental results with our proposed LEARN network produces a competitive\nperformance with the well-known Mayo Clinic Low-Dose Challenge Dataset relative\nto several state-of-the-art methods, in terms of artifact reduction, feature\npreservation, and computational speed. This is consistent to our insight that\nbecause all the regularization terms and parameters used in the iterative\nreconstruction are now learned from the training data, our LEARN network\nutilizes application-oriented knowledge more effectively and recovers\nunderlying images more favorably than competing algorithms. Also, the number of\nlayers in the LEARN network is only 12, reducing the computational complexity\nof typical iterative algorithms by orders of magnitude.\n", "versions": [{"version": "v1", "created": "Sun, 30 Jul 2017 15:53:08 GMT"}, {"version": "v2", "created": "Wed, 16 Aug 2017 01:43:14 GMT"}, {"version": "v3", "created": "Sat, 10 Feb 2018 06:33:27 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Chen", "Hu", ""], ["Zhang", "Yi", ""], ["Chen", "Yunjin", ""], ["Zhang", "Junfeng", ""], ["Zhang", "Weihua", ""], ["Sun", "Huaiqiaing", ""], ["Lv", "Yang", ""], ["Liao", "Peixi", ""], ["Zhou", "Jiliu", ""], ["Wang", "Ge", ""]]}, {"id": "1707.09643", "submitter": "Viraj Mavani", "authors": "Viraj Mavani, Ayesha Gurnani, Jhanvi Shah", "title": "A Novel Approach for Image Segmentation based on Histograms computed\n  from Hue-data", "comments": "4 pages", "journal-ref": "International Journal for Scientific Research and Development\n  [Conference 10 : NCACSET 2017] (2017): pg. 176-179", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Computer Vision is growing day by day in terms of user specific applications.\nThe first step of any such application is segmenting an image. In this paper,\nwe propose a novel and grass-root level image segmentation algorithm for cases\nin which the background has uniform color distribution. This algorithm can be\nused for images of flowers, birds, insects and many more where such background\nconditions occur. By image segmentation, the visualization of a computer\nincreases manifolds and it can even attain near-human accuracy during\nclassification.\n", "versions": [{"version": "v1", "created": "Sun, 30 Jul 2017 17:23:56 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Mavani", "Viraj", ""], ["Gurnani", "Ayesha", ""], ["Shah", "Jhanvi", ""]]}, {"id": "1707.09669", "submitter": "Xiaobin Chang", "authors": "Xiaobin Chang, Tao Xiang, Timothy M. Hospedales", "title": "Scalable and Effective Deep CCA via Soft Decorrelation", "comments": "To Appear at CVPR2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently the widely used multi-view learning model, Canonical Correlation\nAnalysis (CCA) has been generalised to the non-linear setting via deep neural\nnetworks. Existing deep CCA models typically first decorrelate the feature\ndimensions of each view before the different views are maximally correlated in\na common latent space. This feature decorrelation is achieved by enforcing an\nexact decorrelation constraint; these models are thus computationally expensive\ndue to the matrix inversion or SVD operations required for exact decorrelation\nat each training iteration. Furthermore, the decorrelation step is often\nseparated from the gradient descent based optimisation, resulting in\nsub-optimal solutions. We propose a novel deep CCA model Soft CCA to overcome\nthese problems. Specifically, exact decorrelation is replaced by soft\ndecorrelation via a mini-batch based Stochastic Decorrelation Loss (SDL) to be\noptimised jointly with the other training objectives. Extensive experiments\nshow that the proposed soft CCA is more effective and efficient than existing\ndeep CCA models. In addition, our SDL loss can be applied to other deep models\nbeyond multi-view learning, and obtains superior performance compared to\nexisting decorrelation losses.\n", "versions": [{"version": "v1", "created": "Sun, 30 Jul 2017 20:53:54 GMT"}, {"version": "v2", "created": "Sat, 24 Mar 2018 14:58:58 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Chang", "Xiaobin", ""], ["Xiang", "Tao", ""], ["Hospedales", "Timothy M.", ""]]}, {"id": "1707.09695", "submitter": "Liang Lin", "authors": "Mude Lin and Liang Lin and Xiaodan Liang and Keze Wang and Hui Cheng", "title": "Recurrent 3D Pose Sequence Machines", "comments": "Published in CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D human articulated pose recovery from monocular image sequences is very\nchallenging due to the diverse appearances, viewpoints, occlusions, and also\nthe human 3D pose is inherently ambiguous from the monocular imagery. It is\nthus critical to exploit rich spatial and temporal long-range dependencies\namong body joints for accurate 3D pose sequence prediction. Existing approaches\nusually manually design some elaborate prior terms and human body kinematic\nconstraints for capturing structures, which are often insufficient to exploit\nall intrinsic structures and not scalable for all scenarios. In contrast, this\npaper presents a Recurrent 3D Pose Sequence Machine(RPSM) to automatically\nlearn the image-dependent structural constraint and sequence-dependent temporal\ncontext by using a multi-stage sequential refinement. At each stage, our RPSM\nis composed of three modules to predict the 3D pose sequences based on the\npreviously learned 2D pose representations and 3D poses: (i) a 2D pose module\nextracting the image-dependent pose representations, (ii) a 3D pose recurrent\nmodule regressing 3D poses and (iii) a feature adaption module serving as a\nbridge between module (i) and (ii) to enable the representation transformation\nfrom 2D to 3D domain. These three modules are then assembled into a sequential\nprediction framework to refine the predicted poses with multiple recurrent\nstages. Extensive evaluations on the Human3.6M dataset and HumanEva-I dataset\nshow that our RPSM outperforms all state-of-the-art approaches for 3D pose\nestimation.\n", "versions": [{"version": "v1", "created": "Mon, 31 Jul 2017 02:06:45 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Lin", "Mude", ""], ["Lin", "Liang", ""], ["Liang", "Xiaodan", ""], ["Wang", "Keze", ""], ["Cheng", "Hui", ""]]}, {"id": "1707.09700", "submitter": "Yikang Li", "authors": "Yikang Li, Wanli Ouyang, Bolei Zhou, Kun Wang, Xiaogang Wang", "title": "Scene Graph Generation from Objects, Phrases and Region Captions", "comments": "accepted by ICCV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detection, scene graph generation and region captioning, which are\nthree scene understanding tasks at different semantic levels, are tied\ntogether: scene graphs are generated on top of objects detected in an image\nwith their pairwise relationship predicted, while region captioning gives a\nlanguage description of the objects, their attributes, relations, and other\ncontext information. In this work, to leverage the mutual connections across\nsemantic levels, we propose a novel neural network model, termed as Multi-level\nScene Description Network (denoted as MSDN), to solve the three vision tasks\njointly in an end-to-end manner. Objects, phrases, and caption regions are\nfirst aligned with a dynamic graph based on their spatial and semantic\nconnections. Then a feature refining structure is used to pass messages across\nthe three levels of semantic tasks through the graph. We benchmark the learned\nmodel on three tasks, and show the joint learning across three tasks with our\nproposed method can bring mutual improvements over previous models.\nParticularly, on the scene graph generation task, our proposed method\noutperforms the state-of-art method with more than 3% margin.\n", "versions": [{"version": "v1", "created": "Mon, 31 Jul 2017 02:40:19 GMT"}, {"version": "v2", "created": "Fri, 15 Sep 2017 05:05:29 GMT"}], "update_date": "2017-09-18", "authors_parsed": [["Li", "Yikang", ""], ["Ouyang", "Wanli", ""], ["Zhou", "Bolei", ""], ["Wang", "Kun", ""], ["Wang", "Xiaogang", ""]]}, {"id": "1707.09715", "submitter": "Phung Manh Duong", "authors": "Manh Duong Phung, Van Truong Hoang, Tran Hiep Dinh and Quang Ha", "title": "Automatic Crack Detection in Built Infrastructure Using Unmanned Aerial\n  Vehicles", "comments": "In proceeding of The 34th International Symposium on Automation and\n  Robotics in Construction (ISARC), pp. 823-829, Taipei, Taiwan, 2017", "journal-ref": null, "doi": "10.22260/ISARC2017/0115", "report-no": null, "categories": "cs.SY cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of crack detection which is essential for\nhealth monitoring of built infrastructure. Our approach includes two stages,\ndata collection using unmanned aerial vehicles (UAVs) and crack detection using\nhistogram analysis. For the data collection, a 3D model of the structure is\nfirst created by using laser scanners. Based on the model, geometric properties\nare extracted to generate way points necessary for navigating the UAV to take\nimages of the structure. Then, our next step is to stick together those\nobtained images from the overlapped field of view. The resulting image is then\nclustered by histogram analysis and peak detection. Potential cracks are\nfinally identified by using locally adaptive thresholds. The whole process is\nautomatically carried out so that the inspection time is significantly improved\nwhile safety hazards can be minimised. A prototypical system has been developed\nfor evaluation and experimental results are included.\n", "versions": [{"version": "v1", "created": "Mon, 31 Jul 2017 04:18:09 GMT"}], "update_date": "2019-01-04", "authors_parsed": [["Phung", "Manh Duong", ""], ["Hoang", "Van Truong", ""], ["Dinh", "Tran Hiep", ""], ["Ha", "Quang", ""]]}, {"id": "1707.09725", "submitter": "Martin Thoma", "authors": "Martin Thoma", "title": "Analysis and Optimization of Convolutional Neural Network Architectures", "comments": "Master's thesis. 73 pages + 24 pages appendix; 39 figures; 33 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNNs) dominate various computer vision tasks\nsince Alex Krizhevsky showed that they can be trained effectively and reduced\nthe top-5 error from 26.2 % to 15.3 % on the ImageNet large scale visual\nrecognition challenge. Many aspects of CNNs are examined in various\npublications, but literature about the analysis and construction of neural\nnetwork architectures is rare. This work is one step to close this gap. A\ncomprehensive overview over existing techniques for CNN analysis and topology\nconstruction is provided. A novel way to visualize classification errors with\nconfusion matrices was developed. Based on this method, hierarchical\nclassifiers are described and evaluated. Additionally, some results are\nconfirmed and quantified for CIFAR-100. For example, the positive impact of\nsmaller batch sizes, averaging ensembles, data augmentation and test-time\ntransformations on the accuracy. Other results, such as the positive impact of\nlearned color transformation on the test accuracy could not be confirmed. A\nmodel which has only one million learned parameters for an input size of\n32x32x3 and 100 classes and which beats the state of the art on the benchmark\ndataset Asirra, GTSRB, HASYv2 and STL-10 was developed.\n", "versions": [{"version": "v1", "created": "Mon, 31 Jul 2017 05:35:12 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Thoma", "Martin", ""]]}, {"id": "1707.09733", "submitter": "Iaroslav Melekhov", "authors": "Zakaria Laskar, Iaroslav Melekhov, Surya Kalia, Juho Kannala", "title": "Camera Relocalization by Computing Pairwise Relative Poses Using\n  Convolutional Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new deep learning based approach for camera relocalization. Our\napproach localizes a given query image by using a convolutional neural network\n(CNN) for first retrieving similar database images and then predicting the\nrelative pose between the query and the database images, whose poses are known.\nThe camera location for the query image is obtained via triangulation from two\nrelative translation estimates using a RANSAC based approach. Each relative\npose estimate provides a hypothesis for the camera orientation and they are\nfused in a second RANSAC scheme. The neural network is trained for relative\npose estimation in an end-to-end manner using training image pairs. In contrast\nto previous work, our approach does not require scene-specific training of the\nnetwork, which improves scalability, and it can also be applied to scenes which\nare not available during the training of the network. As another main\ncontribution, we release a challenging indoor localisation dataset covering 5\ndifferent scenes registered to a common coordinate frame. We evaluate our\napproach using both our own dataset and the standard 7 Scenes benchmark. The\nresults show that the proposed approach generalizes well to previously unseen\nscenes and compares favourably to other recent CNN-based methods.\n", "versions": [{"version": "v1", "created": "Mon, 31 Jul 2017 06:36:55 GMT"}, {"version": "v2", "created": "Tue, 1 Aug 2017 10:50:39 GMT"}], "update_date": "2017-08-02", "authors_parsed": [["Laskar", "Zakaria", ""], ["Melekhov", "Iaroslav", ""], ["Kalia", "Surya", ""], ["Kannala", "Juho", ""]]}, {"id": "1707.09747", "submitter": "Lei Bi", "authors": "Lei Bi, Jinman Kim, Ashnil Kumar, Dagan Feng, Michael Fulham", "title": "Synthesis of Positron Emission Tomography (PET) Images via Multi-channel\n  Generative Adversarial Networks (GANs)", "comments": "9 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Positron emission tomography (PET) image synthesis plays an important role,\nwhich can be used to boost the training data for computer aided diagnosis\nsystems. However, existing image synthesis methods have problems in\nsynthesizing the low resolution PET images. To address these limitations, we\npropose multi-channel generative adversarial networks (M-GAN) based PET image\nsynthesis method. Different to the existing methods which rely on using\nlow-level features, the proposed M-GAN is capable to represent the features in\na high-level of semantic based on the adversarial learning concept. In\naddition, M-GAN enables to take the input from the annotation (label) to\nsynthesize the high uptake regions e.g., tumors and from the computed\ntomography (CT) images to constrain the appearance consistency and output the\nsynthetic PET images directly. Our results on 50 lung cancer PET-CT studies\nindicate that our method was much closer to the real PET images when compared\nwith the existing methods.\n", "versions": [{"version": "v1", "created": "Mon, 31 Jul 2017 07:56:34 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Bi", "Lei", ""], ["Kim", "Jinman", ""], ["Kumar", "Ashnil", ""], ["Feng", "Dagan", ""], ["Fulham", "Michael", ""]]}, {"id": "1707.09775", "submitter": "Endel Poder", "authors": "Endel Poder", "title": "Capacity limitations of visual search in deep convolutional neural\n  networks", "comments": "10 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks follow roughly the architecture of\nbiological visual systems and have shown a performance comparable to human\nobservers in object recognition tasks. In this study, I tested three pretrained\ndeep neural networks in visual search for simple visual features, and for\nfeature configurations. The results reveal a qualitative difference from human\nperformance. It appears that there is no clear difference between searches for\nsimple features that pop out in experiments with humans, and for feature\nconfigurations that exhibit strict capacity limitations in human vision. Both\ntypes of stimuli reveal comparable capacity limitations in the neural networks\ntested here.\n", "versions": [{"version": "v1", "created": "Mon, 31 Jul 2017 09:14:14 GMT"}, {"version": "v2", "created": "Mon, 29 Mar 2021 09:53:06 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Poder", "Endel", ""]]}, {"id": "1707.09798", "submitter": "Taeksoo Kim", "authors": "Taeksoo Kim, Byoungjip Kim, Moonsu Cha, Jiwon Kim", "title": "Unsupervised Visual Attribute Transfer with Reconfigurable Generative\n  Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning to transfer visual attributes requires supervision dataset.\nCorresponding images with varying attribute values with the same identity are\nrequired for learning the transfer function. This largely limits their\napplications, because capturing them is often a difficult task. To address the\nissue, we propose an unsupervised method to learn to transfer visual attribute.\nThe proposed method can learn the transfer function without any corresponding\nimages. Inspecting visualization results from various unsupervised attribute\ntransfer tasks, we verify the effectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Mon, 31 Jul 2017 10:52:41 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Kim", "Taeksoo", ""], ["Kim", "Byoungjip", ""], ["Cha", "Moonsu", ""], ["Kim", "Jiwon", ""]]}, {"id": "1707.09813", "submitter": "Shubham Jain", "authors": "Jay Patravali, Shubham Jain and Sasank Chilamkurthy", "title": "2D-3D Fully Convolutional Neural Networks for Cardiac MR Segmentation", "comments": "Accepted in STACOM '17", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we develop a 2D and 3D segmentation pipelines for fully\nautomated cardiac MR image segmentation using Deep Convolutional Neural\nNetworks (CNN). Our models are trained end-to-end from scratch using the ACD\nChallenge 2017 dataset comprising of 100 studies, each containing Cardiac MR\nimages in End Diastole and End Systole phase. We show that both our\nsegmentation models achieve near state-of-the-art performance scores in terms\nof distance metrics and have convincing accuracy in terms of clinical\nparameters. A comparative analysis is provided by introducing a novel dice loss\nfunction and its combination with cross entropy loss. By exploring different\nnetwork structures and comprehensive experiments, we discuss several key\ninsights to obtain optimal model performance, which also is central to the\ntheme of this challenge.\n", "versions": [{"version": "v1", "created": "Mon, 31 Jul 2017 12:17:23 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Patravali", "Jay", ""], ["Jain", "Shubham", ""], ["Chilamkurthy", "Sasank", ""]]}, {"id": "1707.09839", "submitter": "Alexandre B", "authors": "Alexandre Bali", "title": "Superposition de calques monochromes d'opacit\\'es variables", "comments": "Poorly written article", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a monochrome layer $x$ of opacity $0\\le o_x\\le1 $ placed on another\nmonochrome layer of opacity 1, the result given by the standard formula is\n$$\\small\\Pi\\left({\\bf\nC}_\\varphi\\right)=1+\\sum_{n=1}^2\\left(2-n-(-1)^no_{\\chi(\\varphi+1)}\\right)\\left(\\chi(n+\\varphi-1)-o_{\\chi(n+\\varphi-1)}\\right),$$\nthe formula being of course explained in detail in this paper. We will\neventually deduce a very simple theorem, generalize it and then see its\nvalidity with alternative formulas to this standard containing the same main\nproperties here exposed.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jul 2017 18:16:06 GMT"}, {"version": "v2", "created": "Sat, 29 Sep 2018 13:08:46 GMT"}, {"version": "v3", "created": "Tue, 16 Jul 2019 11:14:09 GMT"}], "update_date": "2019-07-17", "authors_parsed": [["Bali", "Alexandre", ""]]}, {"id": "1707.09842", "submitter": "Yifei Wang", "authors": "Yifei Wang, Wen Li, Dengxin Dai, Luc Van Gool", "title": "Deep Domain Adaptation by Geodesic Distance Minimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a new approach called Deep LogCORAL for\nunsupervised visual domain adaptation. Our work builds on the recently proposed\nDeep CORAL method, which proposed to train a convolutional neural network and\nsimultaneously minimize the Euclidean distance of convariance matrices between\nthe source and target domains. We propose to use the Riemannian distance,\napproximated by Log-Euclidean distance, to replace the naive Euclidean distance\nin Deep CORAL. We also consider first-order information, and minimize the\ndistance of mean vectors between two domains. We build an end-to-end model, in\nwhich we minimize both the classification loss, and the domain difference based\non the first and second order information between two domains. Our experiments\non the benchmark Office dataset demonstrate the improvements of our newly\nproposed Deep LogCORAL approach over the Deep CORAL method, as well as further\nimprovement when optimizing both orders of information.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jul 2017 11:34:11 GMT"}, {"version": "v2", "created": "Tue, 10 Oct 2017 20:26:55 GMT"}], "update_date": "2017-10-12", "authors_parsed": [["Wang", "Yifei", ""], ["Li", "Wen", ""], ["Dai", "Dengxin", ""], ["Van Gool", "Luc", ""]]}, {"id": "1707.09855", "submitter": "Yong Man Ro", "authors": "Tae Kwan Lee, Wissam J. Baddar, Seong Tae Kim, Yong Man Ro", "title": "Convolution with Logarithmic Filter Groups for Efficient Shallow CNN", "comments": "8 pages, 4 figures, 3 tables. Changes in abstract, result\n  representations and typo corrections", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In convolutional neural networks (CNNs), the filter grouping in convolution\nlayers is known to be useful to reduce the network parameter size. In this\npaper, we propose a new logarithmic filter grouping which can capture the\nnonlinearity of filter distribution in CNNs. The proposed logarithmic filter\ngrouping is installed in shallow CNNs applicable in a mobile application.\nExperiments were performed with the shallow CNNs for classification tasks. Our\nclassification results on Multi-PIE dataset for facial expression recognition\nand CIFAR-10 dataset for object classification reveal that the compact CNN with\nthe proposed logarithmic filter grouping scheme outperforms the same network\nwith the uniform filter grouping in terms of accuracy and parameter efficiency.\nOur results indicate that the efficiency of shallow CNNs can be improved by the\nproposed logarithmic filter grouping.\n", "versions": [{"version": "v1", "created": "Mon, 31 Jul 2017 14:16:06 GMT"}, {"version": "v2", "created": "Thu, 14 Sep 2017 12:23:14 GMT"}], "update_date": "2017-09-15", "authors_parsed": [["Lee", "Tae Kwan", ""], ["Baddar", "Wissam J.", ""], ["Kim", "Seong Tae", ""], ["Ro", "Yong Man", ""]]}, {"id": "1707.09858", "submitter": "Anna Jezierska", "authors": "Anna Jezierska and Hugues Talbot and Jean-Christophe Pesquet and\n  Gilbert Engler", "title": "Spatially variant PSF modeling in confocal macroscopy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point spread function (PSF) plays an essential role in image reconstruction.\nIn the context of confocal microscopy, optical performance degrades towards the\nedge of the field of view as astigmatism, coma and vignetting. Thus, one should\nexpect the related artifacts to be even stronger in macroscopy, where the field\nof view is much larger. The field aberrations in macroscopy fluorescence\nimaging system was observed to be symmetrical and to increase with the distance\nfrom the center of the field of view. In this paper we propose an experiment\nand an optimization method for assessing the center of the field of view. The\nobtained results constitute a step towards reducing the number of parameters in\nmacroscopy PSF model.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jul 2017 09:02:51 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Jezierska", "Anna", ""], ["Talbot", "Hugues", ""], ["Pesquet", "Jean-Christophe", ""], ["Engler", "Gilbert", ""]]}, {"id": "1707.09862", "submitter": "Jian Xu", "authors": "Jian Xu, Chunheng Wang, Chengzuo Qi, Cunzhao Shi, and Baihua Xiao", "title": "Iterative Manifold Embedding Layer Learned by Incomplete Data for\n  Large-scale Image Retrieval", "comments": "12 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing manifold learning methods are not appropriate for image retrieval\ntask, because most of them are unable to process query image and they have much\nadditional computational cost especially for large scale database. Therefore,\nwe propose the iterative manifold embedding (IME) layer, of which the weights\nare learned off-line by unsupervised strategy, to explore the intrinsic\nmanifolds by incomplete data. On the large scale database that contains 27000\nimages, IME layer is more than 120 times faster than other manifold learning\nmethods to embed the original representations at query time. We embed the\noriginal descriptors of database images which lie on manifold in a high\ndimensional space into manifold-based representations iteratively to generate\nthe IME representations in off-line learning stage. According to the original\ndescriptors and the IME representations of database images, we estimate the\nweights of IME layer by ridge regression. In on-line retrieval stage, we employ\nthe IME layer to map the original representation of query image with ignorable\ntime cost (2 milliseconds). We experiment on five public standard datasets for\nimage retrieval. The proposed IME layer significantly outperforms related\ndimension reduction methods and manifold learning methods. Without\npost-processing, Our IME layer achieves a boost in performance of\nstate-of-the-art image retrieval methods with post-processing on most datasets,\nand needs less computational cost.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jul 2017 08:53:11 GMT"}, {"version": "v2", "created": "Tue, 3 Apr 2018 07:21:16 GMT"}], "update_date": "2018-04-04", "authors_parsed": [["Xu", "Jian", ""], ["Wang", "Chunheng", ""], ["Qi", "Chengzuo", ""], ["Shi", "Cunzhao", ""], ["Xiao", "Baihua", ""]]}, {"id": "1707.09864", "submitter": "Sayed Kamaledin Ghiasi-Shirazi", "authors": "Kamaledin Ghiasi-Shirazi", "title": "Generalizing the Convolution Operator in Convolutional Neural Networks", "comments": "Neural Process Lett (2019)", "journal-ref": null, "doi": "10.1007/s11063-019-10043-7", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks have become a main tool for solving many\nmachine vision and machine learning problems. A major element of these networks\nis the convolution operator which essentially computes the inner product\nbetween a weight vector and the vectorized image patches extracted by sliding a\nwindow in the image planes of the previous layer. In this paper, we propose two\nclasses of surrogate functions for the inner product operation inherent in the\nconvolution operator and so attain two generalizations of the convolution\noperator. The first one is the class of positive definite kernel functions\nwhere their application is justified by the kernel trick. The second one is the\nclass of similarity measures defined based on a distance function. We justify\nthis by tracing back to the basic idea behind the neocognitron which is the\nancestor of CNNs. Both methods are then further generalized by allowing a\nmonotonically increasing function to be applied subsequently. Like any\ntrainable parameter in a neural network, the template pattern and the\nparameters of the kernel/distance function are trained with the\nback-propagation algorithm. As an aside, we use the proposed framework to\njustify the use of sine activation function in CNNs. Our experiments on the\nMNIST dataset show that the performance of ordinary CNNs can be achieved by\ngeneralized CNNs based on weighted L1/L2 distances, proving the applicability\nof the proposed generalization of the convolutional neural networks.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jul 2017 10:16:25 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Ghiasi-Shirazi", "Kamaledin", ""]]}, {"id": "1707.09865", "submitter": "Hamid Hamraz", "authors": "Hamid Hamraz and Marco A. Contreras", "title": "Remote sensing of forests using discrete return airborne LiDAR", "comments": "This manuscript is a book chapter that has provisionally been\n  accepted to be published in \"Recent Advances and Applications in Remote\n  Sensing\", ISBN 978-953-51-5564-5. Ed.: Hung, Ming Cheh. InTechOpen. The\n  chapter summarizes novel methods from four recently published journal ppapers\n  by the authors in a concise and cohesive manner", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CE cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Airborne discrete return light detection and ranging (LiDAR) point clouds\ncovering forested areas can be processed to segment individual trees and\nretrieve their morphological attributes. Segmenting individual trees in natural\ndeciduous forests however remained a challenge because of the complex and\nmulti-layered canopy. In this chapter, we present (i) a robust segmentation\nmethod that avoids a priori assumptions about the canopy structure, (ii) a\nvertical canopy stratification procedure that improves segmentation of\nunderstory trees, (iii) an occlusion model for estimating the point density of\neach canopy stratum, and (iv) a distributed computing approach for efficient\nprocessing at the forest level. When applied to the University of Kentucky\nRobinson Forest, the segmentation method detected about 90% of overstory and\n47% of understory trees with over-segmentation rates of 14% and 2%. Stratifying\nthe canopy improved the detection rate of understory trees to 68% at the cost\nof increasing their over-segmentations to 16%. According to our occlusion\nmodel, a point density of ~170 pt/m-sqr is needed to segment understory trees\nas accurately as overstory trees. Lastly, using the distributed approach, we\nsegmented about two million trees in the 7,440-ha forest in 2.5 hours using 192\nprocessors, which is 167 times faster than using a single processor. Keywords:\nindividual tree segmentation, multi-layered stand, vertical canopy\nstratification, segmentation evaluation, point density, canopy occlusion\neffect, big data, distributed computing.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jul 2017 19:06:18 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Hamraz", "Hamid", ""], ["Contreras", "Marco A.", ""]]}, {"id": "1707.09866", "submitter": "Tyng-Luh Liu", "authors": "Tyng-Luh Liu", "title": "Guided Co-training for Large-Scale Multi-View Spectral Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many real-world applications, we have access to multiple views of the\ndata, each of which characterizes the data from a distinct aspect. Several\nprevious algorithms have demonstrated that one can achieve better clustering\naccuracy by integrating information from all views appropriately than using\nonly an individual view. Owing to the effectiveness of spectral clustering,\nmany multi-view clustering methods are based on it. Unfortunately, they have\nlimited applicability to large-scale data due to the high computational\ncomplexity of spectral clustering. In this work, we propose a novel multi-view\nspectral clustering method for large-scale data. Our approach is structured\nunder the guided co-training scheme to fuse distinct views, and uses the\nsampling technique to accelerate spectral clustering. More specifically, we\nfirst select $p$ ($\\ll n$) landmark points and then approximate the\neigen-decomposition accordingly. The augmented view, which is essential to\nguided co-training process, can then be quickly determined by our method. The\nproposed algorithm scales linearly with the number of given data. Extensive\nexperiments have been performed and the results support the advantage of our\nmethod for handling the large-scale multi-view situation.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jul 2017 05:44:52 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Liu", "Tyng-Luh", ""]]}, {"id": "1707.09867", "submitter": "Nicolas Dobigeon", "authors": "Yanna Cruz Cavalcanti, Thomas Oberlin, Nicolas Dobigeon, Simon Stute,\n  Maria Ribeiro, Clovis Tauber", "title": "Unmixing dynamic PET images with variable specific binding kinetics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV physics.data-an stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To analyze dynamic positron emission tomography (PET) images, various generic\nmultivariate data analysis techniques have been considered in the literature,\nsuch as principal component analysis (PCA), independent component analysis\n(ICA), factor analysis and nonnegative matrix factorization (NMF).\nNevertheless, these conventional approaches neglect any possible nonlinear\nvariations in the time activity curves describing the kinetic behavior of\ntissues with specific binding, which limits their ability to recover a\nreliable, understandable and interpretable description of the data. This paper\nproposes an alternative analysis paradigm that accounts for spatial\nfluctuations in the exchange rate of the tracer between a free compartment and\na specifically bound ligand compartment. The method relies on the concept of\nlinear unmixing, usually applied on the hyperspectral domain, which combines\nNMF with a sum-to-one constraint that ensures an exhaustive description of the\nmixtures. The spatial variability of the signature corresponding to the\nspecific binding tissue is explicitly modeled through a perturbed component.\nThe performance of the method is assessed on both synthetic and real data and\nis shown to compete favorably when compared to other conventional analysis\nmethods. The proposed method improved both factor estimation and proportions\nextraction for specific binding. Modeling the variability of the specific\nbinding factor has a strong potential impact for dynamic PET image analysis.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jul 2017 12:17:57 GMT"}, {"version": "v2", "created": "Sat, 9 Dec 2017 13:33:48 GMT"}], "update_date": "2017-12-12", "authors_parsed": [["Cavalcanti", "Yanna Cruz", ""], ["Oberlin", "Thomas", ""], ["Dobigeon", "Nicolas", ""], ["Stute", "Simon", ""], ["Ribeiro", "Maria", ""], ["Tauber", "Clovis", ""]]}, {"id": "1707.09869", "submitter": "Washington Dos-Santos", "authors": "Washington LC dos-Santos, Angelo A Duarte, Luiz AR de Freitas", "title": "A comment on the paper Prediction of Kidney Function from Biopsy Images\n  using Convolutional Neural Networks", "comments": "2 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This letter presente a comment on the paper Prediction of Kidney Function\nfrom Biopsy Images using Convolutional Neural Networks by Ledbetter et al.\n(2017)\n", "versions": [{"version": "v1", "created": "Sun, 23 Jul 2017 01:35:55 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["dos-Santos", "Washington LC", ""], ["Duarte", "Angelo A", ""], ["de Freitas", "Luiz AR", ""]]}, {"id": "1707.09870", "submitter": "Cong Leng", "authors": "Cong Leng, Hao Li, Shenghuo Zhu, Rong Jin", "title": "Extremely Low Bit Neural Network: Squeeze the Last Bit Out with ADMM", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although deep learning models are highly effective for various learning\ntasks, their high computational costs prohibit the deployment to scenarios\nwhere either memory or computational resources are limited. In this paper, we\nfocus on compressing and accelerating deep models with network weights\nrepresented by very small numbers of bits, referred to as extremely low bit\nneural network. We model this problem as a discretely constrained optimization\nproblem. Borrowing the idea from Alternating Direction Method of Multipliers\n(ADMM), we decouple the continuous parameters from the discrete constraints of\nnetwork, and cast the original hard problem into several subproblems. We\npropose to solve these subproblems using extragradient and iterative\nquantization algorithms that lead to considerably faster convergency compared\nto conventional optimization methods. Extensive experiments on image\nrecognition and object detection verify that the proposed algorithm is more\neffective than state-of-the-art approaches when coming to extremely low bit\nneural network.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jul 2017 04:50:50 GMT"}, {"version": "v2", "created": "Wed, 13 Sep 2017 03:21:48 GMT"}], "update_date": "2017-09-14", "authors_parsed": [["Leng", "Cong", ""], ["Li", "Hao", ""], ["Zhu", "Shenghuo", ""], ["Jin", "Rong", ""]]}, {"id": "1707.09871", "submitter": "Yichen Pan", "authors": "Shitao Tang, Yichen Pan", "title": "Feature Extraction via Recurrent Random Deep Ensembles and its\n  Application in Gruop-level Happiness Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel ensemble framework to extract highly\ndiscriminative feature representation of image and its application for\ngroup-level happpiness intensity prediction in wild. In order to generate\nenough diversity of decisions, n convolutional neural networks are trained by\nbootstrapping the training set and extract n features for each image from them.\nA recurrent neural network (RNN) is then used to remember which network\nextracts better feature and generate the final feature representation for one\nindividual image. Several group emotion models (GEM) are used to aggregate face\nfea- tures in a group and use parameter-optimized support vector regressor\n(SVR) to get the final results. Through extensive experiments, the great\neffectiveness of the proposed recurrent random deep ensembles (RRDE) is\ndemonstrated in both structural and decisional ways. The best result yields a\n0.55 root-mean-square error (RMSE) on validation set of HAPPEI dataset,\nsignificantly better than the baseline of 0.78.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jul 2017 08:16:43 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Tang", "Shitao", ""], ["Pan", "Yichen", ""]]}, {"id": "1707.09872", "submitter": "Armand Vilalta", "authors": "Armand Vilalta, Dario Garcia-Gasulla, Ferran Par\\'es, Eduard\n  Ayguad\\'e, Jesus Labarta, Ulises Cort\\'es, Toyotaro Suzumura", "title": "Full-Network Embedding in a Multimodal Embedding Pipeline", "comments": "In 2nd Workshop on Semantic Deep Learning (SemDeep-2) at the 12th\n  International Conference on Computational Semantics (IWCS) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The current state-of-the-art for image annotation and image retrieval tasks\nis obtained through deep neural networks, which combine an image representation\nand a text representation into a shared embedding space. In this paper we\nevaluate the impact of using the Full-Network embedding in this setting,\nreplacing the original image representation in a competitive multimodal\nembedding generation scheme. Unlike the one-layer image embeddings typically\nused by most approaches, the Full-Network embedding provides a multi-scale\nrepresentation of images, which results in richer characterizations. To measure\nthe influence of the Full-Network embedding, we evaluate its performance on\nthree different datasets, and compare the results with the original multimodal\nembedding generation scheme when using a one-layer image embedding, and with\nthe rest of the state-of-the-art. Results for image annotation and image\nretrieval tasks indicate that the Full-Network embedding is consistently\nsuperior to the one-layer embedding. These results motivate the integration of\nthe Full-Network embedding on any multimodal embedding generation scheme,\nsomething feasible thanks to the flexibility of the approach.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jul 2017 10:27:33 GMT"}, {"version": "v2", "created": "Wed, 9 Aug 2017 13:11:42 GMT"}], "update_date": "2017-08-10", "authors_parsed": [["Vilalta", "Armand", ""], ["Garcia-Gasulla", "Dario", ""], ["Par\u00e9s", "Ferran", ""], ["Ayguad\u00e9", "Eduard", ""], ["Labarta", "Jesus", ""], ["Cort\u00e9s", "Ulises", ""], ["Suzumura", "Toyotaro", ""]]}, {"id": "1707.09873", "submitter": "Chun-Nan Chou", "authors": "Chun-Nan Chou, Chuen-Kai Shie, Fu-Chieh Chang, Jocelyn Chang, Edward\n  Y. Chang", "title": "Representation Learning on Large and Small Data", "comments": "Book chapter", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning owes its success to three key factors: scale of data, enhanced\nmodels to learn representations from data, and scale of computation. This book\nchapter presented the importance of the data-driven approach to learn good\nrepresentations from both big data and small data. In terms of big data, it has\nbeen widely accepted in the research community that the more data the better\nfor both representation and classification improvement. The question is then\nhow to learn representations from big data, and how to perform representation\nlearning when data is scarce. We addressed the first question by presenting CNN\nmodel enhancements in the aspects of representation, optimization, and\ngeneralization. To address the small data challenge, we showed transfer\nrepresentation learning to be effective. Transfer representation learning\ntransfers the learned representation from a source domain where abundant\ntraining data is available to a target domain where training data is scarce.\nTransfer representation learning gave the OM and melanoma diagnosis modules of\nour XPRIZE Tricorder device (which finished $2^{nd}$ out of $310$ competing\nteams) a significant boost in diagnosis accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jul 2017 04:14:18 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Chou", "Chun-Nan", ""], ["Shie", "Chuen-Kai", ""], ["Chang", "Fu-Chieh", ""], ["Chang", "Jocelyn", ""], ["Chang", "Edward Y.", ""]]}, {"id": "1707.09875", "submitter": "Fan Zhang", "authors": "Fan Zhang, Chen Hu, Qiang Yin, Wei Li, Hengchao Li and Wen Hong", "title": "SAR Target Recognition Using the Multi-aspect-aware Bidirectional LSTM\n  Recurrent Neural Networks", "comments": "11 pages, 10 figures", "journal-ref": "IEEE Access, vol.5, 2017", "doi": "10.1109/ACCESS.2017.2773363", "report-no": "26880-26891", "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The outstanding pattern recognition performance of deep learning brings new\nvitality to the synthetic aperture radar (SAR) automatic target recognition\n(ATR). However, there is a limitation in current deep learning based ATR\nsolution that each learning process only handle one SAR image, namely learning\nthe static scattering information, while missing the space-varying information.\nIt is obvious that multi-aspect joint recognition introduced space-varying\nscattering information should improve the classification accuracy and\nrobustness. In this paper, a novel multi-aspect-aware method is proposed to\nachieve this idea through the bidirectional Long Short-Term Memory (LSTM)\nrecurrent neural networks based space-varying scattering information learning.\nSpecifically, we first select different aspect images to generate the\nmulti-aspect space-varying image sequences. Then, the Gabor filter and\nthree-patch local binary pattern (TPLBP) are progressively implemented to\nextract a comprehensive spatial features, followed by dimensionality reduction\nwith the Multi-layer Perceptron (MLP) network. Finally, we design a\nbidirectional LSTM recurrent neural network to learn the multi-aspect features\nwith further integrating the softmax classifier to achieve target recognition.\nExperimental results demonstrate that the proposed method can achieve 99.9%\naccuracy for 10-class recognition. Besides, its anti-noise and anti-confusion\nperformance are also better than the conventional deep learning based methods.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jul 2017 04:01:25 GMT"}], "update_date": "2017-12-27", "authors_parsed": [["Zhang", "Fan", ""], ["Hu", "Chen", ""], ["Yin", "Qiang", ""], ["Li", "Wei", ""], ["Li", "Hengchao", ""], ["Hong", "Wen", ""]]}, {"id": "1707.09899", "submitter": "Ashwinkumar Ganesan", "authors": "Prutha Date, Ashwinkumar Ganesan, Tim Oates", "title": "Fashioning with Networks: Neural Style Transfer to Design Clothes", "comments": "ML4Fashion 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.IR cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks have been highly successful in performing a\nhost of computer vision tasks such as object recognition, object detection,\nimage segmentation and texture synthesis. In 2015, Gatys et. al [7] show how\nthe style of a painter can be extracted from an image of the painting and\napplied to another normal photograph, thus recreating the photo in the style of\nthe painter. The method has been successfully applied to a wide range of images\nand has since spawned multiple applications and mobile apps. In this paper, the\nneural style transfer algorithm is applied to fashion so as to synthesize new\ncustom clothes. We construct an approach to personalize and generate new custom\nclothes based on a users preference and by learning the users fashion choices\nfrom a limited set of clothes from their closet. The approach is evaluated by\nanalyzing the generated images of clothes and how well they align with the\nusers fashion style.\n", "versions": [{"version": "v1", "created": "Mon, 31 Jul 2017 14:54:11 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Date", "Prutha", ""], ["Ganesan", "Ashwinkumar", ""], ["Oates", "Tim", ""]]}, {"id": "1707.09926", "submitter": "Saeed Vahidian", "authors": "Mohammad Hossein Moghaddam, Mohammad Javad Azizipour, Saeed Vahidian,\n  Besma Smida", "title": "A Framework for Super-Resolution of Scalable Video via Sparse\n  Reconstruction of Residual Frames", "comments": "IEEE Military Communications Conference, MILCOM, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a framework for super-resolution of scalable video\nbased on compressive sensing and sparse representation of residual frames in\nreconnaissance and surveillance applications. We exploit efficient compressive\nsampling and sparse reconstruction algorithms to super-resolve the video\nsequence with respect to different compression rates. We use the sparsity of\nresidual information in residual frames as the key point in devising our\nframework. Moreover, a controlling factor as the compressibility threshold to\ncontrol the complexity-performance trade-off is defined. Numerical experiments\nconfirm the efficiency of the proposed framework in terms of the compression\nrate as well as the quality of reconstructed video sequence in terms of PSNR\nmeasure. The framework leads to a more efficient compression rate and higher\nvideo quality compared to other state-of-the-art algorithms considering\nperformance-complexity trade-offs.\n", "versions": [{"version": "v1", "created": "Mon, 31 Jul 2017 15:47:47 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Moghaddam", "Mohammad Hossein", ""], ["Azizipour", "Mohammad Javad", ""], ["Vahidian", "Saeed", ""], ["Smida", "Besma", ""]]}, {"id": "1707.09938", "submitter": "Jong Chul Ye", "authors": "Eunhee Kang, Jaejun Yoo, and Jong Chul Ye", "title": "Deep Convolutional Framelet Denosing for Low-Dose CT via Wavelet\n  Residual Network", "comments": "This will appear in IEEE Transaction on Medical Imaging, a special\n  issue of Machine Learning for Image Reconstruction", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model based iterative reconstruction (MBIR) algorithms for low-dose X-ray CT\nare computationally expensive. To address this problem, we recently proposed a\ndeep convolutional neural network (CNN) for low-dose X-ray CT and won the\nsecond place in 2016 AAPM Low-Dose CT Grand Challenge. However, some of the\ntexture were not fully recovered. To address this problem, here we propose a\nnovel framelet-based denoising algorithm using wavelet residual network which\nsynergistically combines the expressive power of deep learning and the\nperformance guarantee from the framelet-based denoising algorithms. The new\nalgorithms were inspired by the recent interpretation of the deep convolutional\nneural network (CNN) as a cascaded convolution framelet signal representation.\nExtensive experimental results confirm that the proposed networks have\nsignificantly improved performance and preserves the detail texture of the\noriginal images.\n", "versions": [{"version": "v1", "created": "Mon, 31 Jul 2017 16:17:31 GMT"}, {"version": "v2", "created": "Mon, 7 Aug 2017 16:10:04 GMT"}, {"version": "v3", "created": "Wed, 28 Mar 2018 07:46:15 GMT"}], "update_date": "2018-03-29", "authors_parsed": [["Kang", "Eunhee", ""], ["Yoo", "Jaejun", ""], ["Ye", "Jong Chul", ""]]}, {"id": "1707.09958", "submitter": "Evan Schwab", "authors": "Evan Schwab, Ren\\'e Vidal, Nicolas Charon", "title": "(k,q)-Compressed Sensing for dMRI with Joint Spatial-Angular Sparsity\n  Prior", "comments": "To be published in the 2017 Computational Diffusion MRI Workshop of\n  MICCAI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advanced diffusion magnetic resonance imaging (dMRI) techniques, like\ndiffusion spectrum imaging (DSI) and high angular resolution diffusion imaging\n(HARDI), remain underutilized compared to diffusion tensor imaging because the\nscan times needed to produce accurate estimations of fiber orientation are\nsignificantly longer. To accelerate DSI and HARDI, recent methods from\ncompressed sensing (CS) exploit a sparse underlying representation of the data\nin the spatial and angular domains to undersample in the respective k- and\nq-spaces. State-of-the-art frameworks, however, impose sparsity in the spatial\nand angular domains separately and involve the sum of the corresponding sparse\nregularizers. In contrast, we propose a unified (k,q)-CS formulation which\nimposes sparsity jointly in the spatial-angular domain to further increase\nsparsity of dMRI signals and reduce the required subsampling rate. To\nefficiently solve this large-scale global reconstruction problem, we introduce\na novel adaptation of the FISTA algorithm that exploits dictionary\nseparability. We show on phantom and real HARDI data that our approach achieves\nsignificantly more accurate signal reconstructions than the state of the art\nwhile sampling only 2-4% of the (k,q)-space, allowing for the potential of new\nlevels of dMRI acceleration.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jul 2017 03:50:44 GMT"}, {"version": "v2", "created": "Wed, 2 Aug 2017 18:36:57 GMT"}], "update_date": "2017-08-04", "authors_parsed": [["Schwab", "Evan", ""], ["Vidal", "Ren\u00e9", ""], ["Charon", "Nicolas", ""]]}, {"id": "1707.09959", "submitter": "Zhiwei Li", "authors": "Chengyue Zhang, Zhiwei Li, Qing Cheng, Xinghua Li, Huanfeng Shen", "title": "Correction of \"Cloud Removal By Fusing Multi-Source and Multi-Temporal\n  Images\"", "comments": "This is a correction version of the accepted IGARSS 2017 conference\n  paper", "journal-ref": "2017 IEEE International Geoscience and Remote Sensing Symposium\n  (IGARSS), pp.2577-2580, 2017", "doi": "10.1109/IGARSS.2017.8127522", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Remote sensing images often suffer from cloud cover. Cloud removal is\nrequired in many applications of remote sensing images. Multitemporal-based\nmethods are popular and effective to cope with thick clouds. This paper\ncontributes to a summarization and experimental comparation of the existing\nmultitemporal-based methods. Furthermore, we propose a spatiotemporal-fusion\nwith poisson-adjustment method to fuse multi-sensor and multi-temporal images\nfor cloud removal. The experimental results show that the proposed method has\npotential to address the problem of accuracy reduction of cloud removal in\nmulti-temporal images with significant changes.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jul 2017 04:20:18 GMT"}], "update_date": "2019-03-06", "authors_parsed": [["Zhang", "Chengyue", ""], ["Li", "Zhiwei", ""], ["Cheng", "Qing", ""], ["Li", "Xinghua", ""], ["Shen", "Huanfeng", ""]]}]