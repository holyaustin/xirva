[{"id": "1507.00019", "submitter": "Jingbin Wang", "authors": "Jingyan Wang, Yihua Zhou, Ming Yin, Shaochang Chen, Benjamin Edwards", "title": "Representing data by sparse combination of contextual data points for\n  classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the problem of using contextual da- ta points of a\ndata point for its classification problem. We propose to represent a data point\nas the sparse linear reconstruction of its context, and learn the sparse\ncontext to gather with a linear classifier in a su- pervised way to increase\nits discriminative ability. We proposed a novel formulation for context\nlearning, by modeling the learning of context reconstruction coefficients and\nclassifier in a unified objective. In this objective, the reconstruction error\nis minimized and the coefficient spar- sity is encouraged. Moreover, the hinge\nloss of the classifier is minimized and the complexity of the classifier is\nreduced. This objective is opti- mized by an alternative strategy in an\niterative algorithm. Experiments on three benchmark data set show its advantage\nover state-of-the-art context-based data representation and classification\nmethods.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2015 20:08:26 GMT"}, {"version": "v2", "created": "Tue, 18 Aug 2015 06:06:13 GMT"}], "update_date": "2015-08-19", "authors_parsed": [["Wang", "Jingyan", ""], ["Zhou", "Yihua", ""], ["Yin", "Ming", ""], ["Chen", "Shaochang", ""], ["Edwards", "Benjamin", ""]]}, {"id": "1507.00101", "submitter": "Huei-Fang Yang", "authors": "Huei-Fang Yang, Kevin Lin, Chu-Song Chen", "title": "Supervised Learning of Semantics-Preserving Hash via Deep Convolutional\n  Neural Networks", "comments": "To appear in IEEE Trans. Pattern Analysis and Machine Intelligence", "journal-ref": null, "doi": "10.1109/TPAMI.2017.2666812", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a simple yet effective supervised deep hash approach that\nconstructs binary hash codes from labeled data for large-scale image search. We\nassume that the semantic labels are governed by several latent attributes with\neach attribute on or off, and classification relies on these attributes. Based\non this assumption, our approach, dubbed supervised semantics-preserving deep\nhashing (SSDH), constructs hash functions as a latent layer in a deep network\nand the binary codes are learned by minimizing an objective function defined\nover classification error and other desirable hash codes properties. With this\ndesign, SSDH has a nice characteristic that classification and retrieval are\nunified in a single learning model. Moreover, SSDH performs joint learning of\nimage representations, hash codes, and classification in a point-wised manner,\nand thus is scalable to large-scale datasets. SSDH is simple and can be\nrealized by a slight enhancement of an existing deep architecture for\nclassification; yet it is effective and outperforms other hashing approaches on\nseveral benchmarks and large datasets. Compared with state-of-the-art\napproaches, SSDH achieves higher retrieval accuracy, while the classification\nperformance is not sacrificed.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2015 04:40:31 GMT"}, {"version": "v2", "created": "Tue, 14 Feb 2017 07:31:18 GMT"}], "update_date": "2017-02-15", "authors_parsed": [["Yang", "Huei-Fang", ""], ["Lin", "Kevin", ""], ["Chen", "Chu-Song", ""]]}, {"id": "1507.00110", "submitter": "Junfei Shi", "authors": "Fang Liu, Junfei Shi, Licheng Jiao, Hongying Liu, Shuyuan Yang, Jie\n  Wu, Hongxia Hao, Jialing Yuan", "title": "Polarimetric Hierarchical Semantic Model and Scattering Mechanism Based\n  PolSAR Image Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For polarimetric SAR (PolSAR) image classification, it is a challenge to\nclassify the aggregated terrain types, such as the urban area, into semantic\nhomogenous regions due to sharp bright-dark variations in intensity. The\naggregated terrain type is formulated by the similar ground objects aggregated\ntogether. In this paper, a polarimetric hierarchical semantic model (PHSM) is\nfirstly proposed to overcome this disadvantage based on the constructions of a\nprimal-level and a middle-level semantic. The primal-level semantic is a\npolarimetric sketch map which consists of sketch segments as the sparse\nrepresentation of a PolSAR image. The middle-level semantic is a region map\nwhich can extract semantic homogenous regions from the sketch map by exploiting\nthe topological structure of sketch segments. Mapping the region map to the\nPolSAR image, a complex PolSAR scene is partitioned into aggregated, structural\nand homogenous pixel-level subspaces with the characteristics of relatively\ncoherent terrain types in each subspace. Then, according to the characteristics\nof three subspaces above, three specific methods are adopted, and furthermore\npolarimetric information is exploited to improve the segmentation result.\nExperimental results on PolSAR data sets with different bands and sensors\ndemonstrate that the proposed method is superior to the state-of-the-art\nmethods in region homogeneity and edge preservation for terrain classification.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2015 05:47:18 GMT"}], "update_date": "2015-07-02", "authors_parsed": [["Liu", "Fang", ""], ["Shi", "Junfei", ""], ["Jiao", "Licheng", ""], ["Liu", "Hongying", ""], ["Yang", "Shuyuan", ""], ["Wu", "Jie", ""], ["Hao", "Hongxia", ""], ["Yuan", "Jialing", ""]]}, {"id": "1507.00136", "submitter": "Zhouye Chen", "authors": "Zhouye Chen, Adrian Basarab, Denis Kouam\\'e", "title": "Compressive Deconvolution in Medical Ultrasound Imaging", "comments": null, "journal-ref": null, "doi": "10.1109/TMI.2015.2493241", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The interest of compressive sampling in ultrasound imaging has been recently\nextensively evaluated by several research teams. Following the different\napplication setups, it has been shown that the RF data may be reconstructed\nfrom a small number of measurements and/or using a reduced number of ultrasound\npulse emissions. Nevertheless, RF image spatial resolution, contrast and signal\nto noise ratio are affected by the limited bandwidth of the imaging transducer\nand the physical phenomenon related to US wave propagation. To overcome these\nlimitations, several deconvolution-based image processing techniques have been\nproposed to enhance the ultrasound images. In this paper, we propose a novel\nframework, named compressive deconvolution, that reconstructs enhanced RF\nimages from compressed measurements. Exploiting an unified formulation of the\ndirect acquisition model, combining random projections and 2D convolution with\na spatially invariant point spread function, the benefit of our approach is the\njoint data volume reduction and image quality improvement. The proposed\noptimization method, based on the Alternating Direction Method of Multipliers,\nis evaluated on both simulated and in vivo data.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2015 07:48:18 GMT"}, {"version": "v2", "created": "Fri, 4 Dec 2015 14:25:44 GMT"}], "update_date": "2015-12-07", "authors_parsed": [["Chen", "Zhouye", ""], ["Basarab", "Adrian", ""], ["Kouam\u00e9", "Denis", ""]]}, {"id": "1507.00302", "submitter": "Weilong Yang", "authors": "Greg Mori, Caroline Pantofaru, Nisarg Kothari, Thomas Leung, George\n  Toderici, Alexander Toshev, Weilong Yang", "title": "Pose Embeddings: A Deep Architecture for Learning to Match Human Poses", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for learning an embedding that places images of humans in\nsimilar poses nearby. This embedding can be used as a direct method of\ncomparing images based on human pose, avoiding potential challenges of\nestimating body joint positions. Pose embedding learning is formulated under a\ntriplet-based distance criterion. A deep architecture is used to allow learning\nof a representation capable of making distinctions between different poses.\nExperiments on human pose matching and retrieval from video data demonstrate\nthe potential of the method.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2015 17:59:21 GMT"}], "update_date": "2015-07-02", "authors_parsed": [["Mori", "Greg", ""], ["Pantofaru", "Caroline", ""], ["Kothari", "Nisarg", ""], ["Leung", "Thomas", ""], ["Toderici", "George", ""], ["Toshev", "Alexander", ""], ["Yang", "Weilong", ""]]}, {"id": "1507.00410", "submitter": "Jonathan Barron", "authors": "Jonathan T. Barron", "title": "Convolutional Color Constancy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Color constancy is the problem of inferring the color of the light that\nilluminated a scene, usually so that the illumination color can be removed.\nBecause this problem is underconstrained, it is often solved by modeling the\nstatistical regularities of the colors of natural objects and illumination. In\ncontrast, in this paper we reformulate the problem of color constancy as a 2D\nspatial localization task in a log-chrominance space, thereby allowing us to\napply techniques from object detection and structured prediction to the color\nconstancy problem. By directly learning how to discriminate between correctly\nwhite-balanced images and poorly white-balanced images, our model is able to\nimprove performance on standard benchmarks by nearly 40%.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2015 02:16:42 GMT"}, {"version": "v2", "created": "Fri, 18 Sep 2015 18:01:47 GMT"}], "update_date": "2015-09-21", "authors_parsed": [["Barron", "Jonathan T.", ""]]}, {"id": "1507.00448", "submitter": "Saurabh Gupta", "authors": "Saurabh Gupta, Judy Hoffman, Jitendra Malik", "title": "Cross Modal Distillation for Supervision Transfer", "comments": "Updated version (v2) contains additional experiments and results", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we propose a technique that transfers supervision between images\nfrom different modalities. We use learned representations from a large labeled\nmodality as a supervisory signal for training representations for a new\nunlabeled paired modality. Our method enables learning of rich representations\nfor unlabeled modalities and can be used as a pre-training procedure for new\nmodalities with limited labeled data. We show experimental results where we\ntransfer supervision from labeled RGB images to unlabeled depth and optical\nflow images and demonstrate large improvements for both these cross modal\nsupervision transfers. Code, data and pre-trained models are available at\nhttps://github.com/s-gupta/fast-rcnn/tree/distillation\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2015 07:21:04 GMT"}, {"version": "v2", "created": "Wed, 25 Nov 2015 08:46:56 GMT"}], "update_date": "2015-11-26", "authors_parsed": [["Gupta", "Saurabh", ""], ["Hoffman", "Judy", ""], ["Malik", "Jitendra", ""]]}, {"id": "1507.00501", "submitter": "Remi Flamary", "authors": "Andr\\'e Ferrari (LAGRANGE, OCA), David Mary (LAGRANGE, OCA), R\\'emi\n  Flamary (LAGRANGE, OCA), C\\'edric Richard (LAGRANGE, OCA)", "title": "Distributed image reconstruction for very large arrays in radio\n  astronomy", "comments": "Sensor Array and Multichannel Signal Processing Workshop (SAM), 2014\n  IEEE 8th, Jun 2014, Coruna, Spain. 2014", "journal-ref": null, "doi": "10.1109/SAM.2014.6882424", "report-no": null, "categories": "astro-ph.IM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current and future radio interferometric arrays such as LOFAR and SKA are\ncharacterized by a paradox. Their large number of receptors (up to millions)\nallow theoretically unprecedented high imaging resolution. In the same time,\nthe ultra massive amounts of samples makes the data transfer and computational\nloads (correlation and calibration) order of magnitudes too high to allow any\ncurrently existing image reconstruction algorithm to achieve, or even approach,\nthe theoretical resolution. We investigate here decentralized and distributed\nimage reconstruction strategies which select, transfer and process only a\nfraction of the total data. The loss in MSE incurred by the proposed approach\nis evaluated theoretically and numerically on simple test cases.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2015 10:07:04 GMT"}], "update_date": "2015-07-03", "authors_parsed": [["Ferrari", "Andr\u00e9", "", "LAGRANGE, OCA"], ["Mary", "David", "", "LAGRANGE, OCA"], ["Flamary", "R\u00e9mi", "", "LAGRANGE, OCA"], ["Richard", "C\u00e9dric", "", "LAGRANGE, OCA"]]}, {"id": "1507.00908", "submitter": "Zhao Kang", "authors": "Zhao Kang, Chong Peng, Jie Cheng and Qiang Chen", "title": "LogDet Rank Minimization with Application to Subspace Clustering", "comments": "10 pages, 4 figures", "journal-ref": "Computational Intelligence and Neuroscience, Volume 2015, Article\n  ID 824289", "doi": "10.1155/2015/824289", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low-rank matrix is desired in many machine learning and computer vision\nproblems. Most of the recent studies use the nuclear norm as a convex surrogate\nof the rank operator. However, all singular values are simply added together by\nthe nuclear norm, and thus the rank may not be well approximated in practical\nproblems. In this paper, we propose to use a log-determinant (LogDet) function\nas a smooth and closer, though non-convex, approximation to rank for obtaining\na low-rank representation in subspace clustering. Augmented Lagrange\nmultipliers strategy is applied to iteratively optimize the LogDet-based\nnon-convex objective function on potentially large-scale data. By making use of\nthe angular information of principal directions of the resultant low-rank\nrepresentation, an affinity graph matrix is constructed for spectral\nclustering. Experimental results on motion segmentation and face clustering\ndata demonstrate that the proposed method often outperforms state-of-the-art\nsubspace clustering algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2015 13:30:41 GMT"}], "update_date": "2015-07-07", "authors_parsed": [["Kang", "Zhao", ""], ["Peng", "Chong", ""], ["Cheng", "Jie", ""], ["Chen", "Qiang", ""]]}, {"id": "1507.00913", "submitter": "Erik Rodner", "authors": "Erik Rodner and Marcel Simon and Gunnar Brehm and Stephanie Pietsch\n  and J. Wolfgang W\\\"agele and Joachim Denzler", "title": "Fine-grained Recognition Datasets for Biodiversity Analysis", "comments": "CVPR FGVC Workshop 2015; dataset available", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the following paper, we present and discuss challenging applications for\nfine-grained visual classification (FGVC): biodiversity and species analysis.\nWe not only give details about two challenging new datasets suitable for\ncomputer vision research with up to 675 highly similar classes, but also\npresent first results with localized features using convolutional neural\nnetworks (CNN). We conclude with a list of challenging new research directions\nin the area of visual classification for biodiversity research.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2015 13:53:26 GMT"}], "update_date": "2015-07-06", "authors_parsed": [["Rodner", "Erik", ""], ["Simon", "Marcel", ""], ["Brehm", "Gunnar", ""], ["Pietsch", "Stephanie", ""], ["W\u00e4gele", "J. Wolfgang", ""], ["Denzler", "Joachim", ""]]}, {"id": "1507.01053", "submitter": "KyungHyun Cho", "authors": "Kyunghyun Cho, Aaron Courville, Yoshua Bengio", "title": "Describing Multimedia Content using Attention-based Encoder--Decoder\n  Networks", "comments": "Submitted to IEEE Transactions on Multimedia Special Issue on Deep\n  Learning for Multimedia Computing", "journal-ref": null, "doi": "10.1109/TMM.2015.2477044", "report-no": null, "categories": "cs.NE cs.CL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Whereas deep neural networks were first mostly used for classification tasks,\nthey are rapidly expanding in the realm of structured output problems, where\nthe observed target is composed of multiple random variables that have a rich\njoint distribution, given the input. We focus in this paper on the case where\nthe input also has a rich structure and the input and output structures are\nsomehow related. We describe systems that learn to attend to different places\nin the input, for each element of the output, for a variety of tasks: machine\ntranslation, image caption generation, video clip description and speech\nrecognition. All these systems are based on a shared set of building blocks:\ngated recurrent neural networks and convolutional neural networks, along with\ntrained attention mechanisms. We report on experimental results with these\nsystems, showing impressively good performance and the advantage of the\nattention mechanism.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jul 2015 01:06:16 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Cho", "Kyunghyun", ""], ["Courville", "Aaron", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1507.01208", "submitter": "Puneet Dokania", "authors": "Puneet K. Dokania and M. Pawan Kumar", "title": "Parsimonious Labeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new family of discrete energy minimization problems, which we\ncall parsimonious labeling. Specifically, our energy functional consists of\nunary potentials and high-order clique potentials. While the unary potentials\nare arbitrary, the clique potentials are proportional to the {\\em diversity} of\nset of the unique labels assigned to the clique. Intuitively, our energy\nfunctional encourages the labeling to be parsimonious, that is, use as few\nlabels as possible. This in turn allows us to capture useful cues for important\ncomputer vision applications such as stereo correspondence and image denoising.\nFurthermore, we propose an efficient graph-cuts based algorithm for the\nparsimonious labeling problem that provides strong theoretical guarantees on\nthe quality of the solution. Our algorithm consists of three steps. First, we\napproximate a given diversity using a mixture of a novel hierarchical $P^n$\nPotts model. Second, we use a divide-and-conquer approach for each mixture\ncomponent, where each subproblem is solved using an effficient\n$\\alpha$-expansion algorithm. This provides us with a small number of putative\nlabelings, one for each mixture component. Third, we choose the best putative\nlabeling in terms of the energy value. Using both sythetic and standard real\ndatasets, we show that our algorithm significantly outperforms other graph-cuts\nbased approaches.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jul 2015 11:59:43 GMT"}], "update_date": "2015-07-07", "authors_parsed": [["Dokania", "Puneet K.", ""], ["Kumar", "M. Pawan", ""]]}, {"id": "1507.01209", "submitter": "Raghvendra Kannao", "authors": "Raghvendra Kannao and Prithwijit Guha", "title": "TV News Commercials Detection using Success based Locally Weighted\n  Kernel Combination", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Commercial detection in news broadcast videos involves judicious selection of\nmeaningful audio-visual feature combinations and efficient classifiers. And,\nthis problem becomes much simpler if these combinations can be learned from the\ndata. To this end, we propose an Multiple Kernel Learning based method for\nboosting successful kernel functions while ignoring the irrelevant ones. We\nadopt a intermediate fusion approach where, a SVM is trained with a weighted\nlinear combination of different kernel functions instead of single kernel\nfunction. Each kernel function is characterized by a feature set and kernel\ntype. We identify the feature sub-space locations of the prediction success of\na particular classifier trained only with particular kernel function. We\npropose to estimate a weighing function using support vector regression (with\nRBF kernel) for each kernel function which has high values (near 1.0) where the\nclassifier learned on kernel function succeeded and lower values (nearly 0.0)\notherwise. Second contribution of this work is TV News Commercials Dataset of\n150 Hours of News videos. Classifier trained with our proposed scheme has\noutperformed the baseline methods on 6 of 8 benchmark dataset and our own TV\ncommercials dataset.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jul 2015 12:01:34 GMT"}], "update_date": "2015-07-07", "authors_parsed": [["Kannao", "Raghvendra", ""], ["Guha", "Prithwijit", ""]]}, {"id": "1507.01238", "submitter": "Chong You", "authors": "Chong You, Daniel P. Robinson, Rene Vidal", "title": "Scalable Sparse Subspace Clustering by Orthogonal Matching Pursuit", "comments": "13 pages, 1 figure, 2 tables. Accepted to CVPR 2016 as an oral\n  presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Subspace clustering methods based on $\\ell_1$, $\\ell_2$ or nuclear norm\nregularization have become very popular due to their simplicity, theoretical\nguarantees and empirical success. However, the choice of the regularizer can\ngreatly impact both theory and practice. For instance, $\\ell_1$ regularization\nis guaranteed to give a subspace-preserving affinity (i.e., there are no\nconnections between points from different subspaces) under broad conditions\n(e.g., arbitrary subspaces and corrupted data). However, it requires solving a\nlarge scale convex optimization problem. On the other hand, $\\ell_2$ and\nnuclear norm regularization provide efficient closed form solutions, but\nrequire very strong assumptions to guarantee a subspace-preserving affinity,\ne.g., independent subspaces and uncorrupted data. In this paper we study a\nsubspace clustering method based on orthogonal matching pursuit. We show that\nthe method is both computationally efficient and guaranteed to give a\nsubspace-preserving affinity under broad conditions. Experiments on synthetic\ndata verify our theoretical analysis, and applications in handwritten digit and\nface clustering show that our approach achieves the best trade off between\naccuracy and efficiency.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jul 2015 16:29:31 GMT"}, {"version": "v2", "created": "Mon, 31 Aug 2015 04:25:27 GMT"}, {"version": "v3", "created": "Thu, 5 May 2016 20:16:54 GMT"}], "update_date": "2016-05-09", "authors_parsed": [["You", "Chong", ""], ["Robinson", "Daniel P.", ""], ["Vidal", "Rene", ""]]}, {"id": "1507.01251", "submitter": "Hamid Tizhoosh", "authors": "Zehra Camlica, H.R. Tizhoosh, Farzad Khalvati", "title": "Autoencoding the Retrieval Relevance of Medical Images", "comments": "To appear in proceedings of The 5th International Conference on Image\n  Processing Theory, Tools and Applications (IPTA'15), Nov 10-13, 2015,\n  Orleans, France", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Content-based image retrieval (CBIR) of medical images is a crucial task that\ncan contribute to a more reliable diagnosis if applied to big data. Recent\nadvances in feature extraction and classification have enormously improved CBIR\nresults for digital images. However, considering the increasing accessibility\nof big data in medical imaging, we are still in need of reducing both memory\nrequirements and computational expenses of image retrieval systems. This work\nproposes to exclude the features of image blocks that exhibit a low encoding\nerror when learned by a $n/p/n$ autoencoder ($p\\!<\\!n$). We examine the\nhistogram of autoendcoding errors of image blocks for each image class to\nfacilitate the decision which image regions, or roughly what percentage of an\nimage perhaps, shall be declared relevant for the retrieval task. This leads to\nreduction of feature dimensionality and speeds up the retrieval process. To\nvalidate the proposed scheme, we employ local binary patterns (LBP) and support\nvector machines (SVM) which are both well-established approaches in CBIR\nresearch community. As well, we use IRMA dataset with 14,410 x-ray images as\ntest data. The results show that the dimensionality of annotated feature\nvectors can be reduced by up to 50% resulting in speedups greater than 27% at\nexpense of less than 1% decrease in the accuracy of retrieval when validating\nthe precision and recall of the top 20 hits.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jul 2015 18:40:14 GMT"}], "update_date": "2015-07-07", "authors_parsed": [["Camlica", "Zehra", ""], ["Tizhoosh", "H. R.", ""], ["Khalvati", "Farzad", ""]]}, {"id": "1507.01330", "submitter": "Xiaojie Guo", "authors": "Xiaojie Guo", "title": "Visual Data Deblocking using Structural Layer Priors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The blocking artifact frequently appears in compressed real-world images or\nvideo sequences, especially coded at low bit rates, which is visually annoying\nand likely hurts the performance of many computer vision algorithms. A\ncompressed frame can be viewed as the superimposition of an intrinsic layer and\nan artifact one. Recovering the two layers from such frames seems to be a\nseverely ill-posed problem since the number of unknowns to recover is twice as\nmany as the given measurements. In this paper, we propose a simple and robust\nmethod to separate these two layers, which exploits structural layer priors\nincluding the gradient sparsity of the intrinsic layer, and the independence of\nthe gradient fields of the two layers. A novel Augmented Lagrangian Multiplier\nbased algorithm is designed to efficiently and effectively solve the recovery\nproblem. Extensive experimental results demonstrate the superior performance of\nour method over the state of the arts, in terms of visual quality and\nsimplicity.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2015 05:34:41 GMT"}], "update_date": "2015-07-07", "authors_parsed": [["Guo", "Xiaojie", ""]]}, {"id": "1507.01422", "submitter": "Xavier Gir\\'o-i-Nieto", "authors": "Junting Pan and Xavier Gir\\'o-i-Nieto", "title": "End-to-end Convolutional Network for Saliency Prediction", "comments": "Winner of the saliency prediction challenge in the Large-scale Scene\n  Understanding (LSUN) Challenge in the associated workshop of the IEEE\n  Conference on Computer Vision and Pattern Recognition (CVPR) 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The prediction of saliency areas in images has been traditionally addressed\nwith hand crafted features based on neuroscience principles. This paper however\naddresses the problem with a completely data-driven approach by training a\nconvolutional network. The learning process is formulated as a minimization of\na loss function that measures the Euclidean distance of the predicted saliency\nmap with the provided ground truth. The recent publication of large datasets of\nsaliency prediction has provided enough data to train a not very deep\narchitecture which is both fast and accurate. The convolutional network in this\npaper, named JuntingNet, won the LSUN 2015 challenge on saliency prediction\nwith a superior performance in all considered metrics.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2015 12:43:26 GMT"}], "update_date": "2015-07-07", "authors_parsed": [["Pan", "Junting", ""], ["Gir\u00f3-i-Nieto", "Xavier", ""]]}, {"id": "1507.01442", "submitter": "Shicong Liu", "authors": "Shicong Liu, Hongtao Lu", "title": "Learning Better Encoding for Approximate Nearest Neighbor Search with\n  Dictionary Annealing", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel dictionary optimization method for high-dimensional\nvector quantization employed in approximate nearest neighbor (ANN) search.\nVector quantization methods first seek a series of dictionaries, then\napproximate each vector by a sum of elements selected from these dictionaries.\nAn optimal series of dictionaries should be mutually independent, and each\ndictionary should generate a balanced encoding for the target dataset. Existing\nmethods did not explicitly consider this. To achieve these goals along with\nminimizing the quantization error (residue), we propose a novel dictionary\noptimization method called \\emph{Dictionary Annealing} that alternatively\n\"heats up\" a single dictionary by generating an intermediate dataset with\nresidual vectors, \"cools down\" the dictionary by fitting the intermediate\ndataset, then extracts the new residual vectors for the next iteration. Better\ncodes can be learned by DA for the ANN search tasks. DA is easily implemented\non GPU to utilize the latest computing technology, and can easily extended to\nan online dictionary learning scheme. We show by experiments that our optimized\ndictionaries substantially reduce the overall quantization error. Jointly used\nwith residual vector quantization, our optimized dictionaries lead to a better\napproximate nearest neighbor search performance compared to the\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2015 13:25:35 GMT"}], "update_date": "2015-07-07", "authors_parsed": [["Liu", "Shicong", ""], ["Lu", "Hongtao", ""]]}, {"id": "1507.01578", "submitter": "Subarna Tripathi", "authors": "Subarna Tripathi and Serge Belongie and Truong Nguyen", "title": "Beyond Semantic Image Segmentation : Exploring Efficient Inference in\n  Video", "comments": "CVPR 2015 workshop WiCV : Extended Abstract", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore the efficiency of the CRF inference module beyond image level\nsemantic segmentation. The key idea is to combine the best of two worlds of\nsemantic co-labeling and exploiting more expressive models. Similar to\n[Alvarez14] our formulation enables us perform inference over ten thousand\nimages within seconds. On the other hand, it can handle higher-order clique\npotentials similar to [vineet2014] in terms of region-level label consistency\nand context in terms of co-occurrences. We follow the mean-field updates for\nhigher order potentials similar to [vineet2014] and extend the spatial\nsmoothness and appearance kernels [DenseCRF13] to address video data inspired\nby [Alvarez14]; thus making the system amenable to perform video semantic\nsegmentation most effectively.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2015 08:06:52 GMT"}], "update_date": "2015-07-13", "authors_parsed": [["Tripathi", "Subarna", ""], ["Belongie", "Serge", ""], ["Nguyen", "Truong", ""]]}, {"id": "1507.01581", "submitter": "Jasper Uijlings", "authors": "Holger Caesar, Jasper Uijlings, Vittorio Ferrari", "title": "Joint Calibration for Semantic Segmentation", "comments": "Includes improved results based on VGG16 CNN", "journal-ref": "BMVC 2015", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic segmentation is the task of assigning a class-label to each pixel in\nan image. We propose a region-based semantic segmentation framework which\nhandles both full and weak supervision, and addresses three common problems:\n(1) Objects occur at multiple scales and therefore we should use regions at\nmultiple scales. However, these regions are overlapping which creates\nconflicting class predictions at the pixel-level. (2) Class frequencies are\nhighly imbalanced in realistic datasets. (3) Each pixel can only be assigned to\na single class, which creates competition between classes. We address all three\nproblems with a joint calibration method which optimizes a multi-class loss\ndefined over the final pixel-level output labeling, as opposed to simply region\nclassification. Our method outperforms the state-of-the-art on the popular SIFT\nFlow [18] dataset in both the fully and weakly supervised setting by a\nconsiderably margin (+6% and +10%, respectively).\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2015 19:55:18 GMT"}, {"version": "v2", "created": "Tue, 21 Jul 2015 14:36:17 GMT"}, {"version": "v3", "created": "Fri, 31 Jul 2015 15:57:09 GMT"}, {"version": "v4", "created": "Wed, 12 Aug 2015 14:20:08 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Caesar", "Holger", ""], ["Uijlings", "Jasper", ""], ["Ferrari", "Vittorio", ""]]}, {"id": "1507.02049", "submitter": "Cong Jie Ng", "authors": "Cong Jie Ng, Andrew Beng Jin Teoh", "title": "DCTNet : A Simple Learning-free Approach for Face Recognition", "comments": "APSIPA ASC 2015", "journal-ref": null, "doi": "10.1109/APSIPA.2015.7415375", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  PCANet was proposed as a lightweight deep learning network that mainly\nleverages Principal Component Analysis (PCA) to learn multistage filter banks\nfollowed by binarization and block-wise histograming. PCANet was shown worked\nsurprisingly well in various image classification tasks. However, PCANet is\ndata-dependence hence inflexible. In this paper, we proposed a\ndata-independence network, dubbed DCTNet for face recognition in which we adopt\nDiscrete Cosine Transform (DCT) as filter banks in place of PCA. This is\nmotivated by the fact that 2D DCT basis is indeed a good approximation for high\nranked eigenvectors of PCA. Both 2D DCT and PCA resemble a kind of modulated\nsine-wave patterns, which can be perceived as a bandpass filter bank. DCTNet is\nfree from learning as 2D DCT bases can be computed in advance. Besides that, we\nalso proposed an effective method to regulate the block-wise histogram feature\nvector of DCTNet for robustness. It is shown to provide surprising performance\nboost when the probe image is considerably different in appearance from the\ngallery image. We evaluate the performance of DCTNet extensively on a number of\nbenchmark face databases and being able to achieve on par with or often better\naccuracy performance than PCANet.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2015 07:15:41 GMT"}, {"version": "v2", "created": "Mon, 13 Jul 2015 05:55:52 GMT"}, {"version": "v3", "created": "Tue, 29 Sep 2015 11:38:10 GMT"}], "update_date": "2016-04-27", "authors_parsed": [["Ng", "Cong Jie", ""], ["Teoh", "Andrew Beng Jin", ""]]}, {"id": "1507.02084", "submitter": "Iago Landesa-V\\'azquez", "authors": "Iago Landesa-V\\'azquez, Jos\\'e Luis Alba-Castro", "title": "Shedding Light on the Asymmetric Learning Capability of AdaBoost", "comments": null, "journal-ref": "Pattern Recognition Letters 33 (2012) 247-255", "doi": "10.1016/j.patrec.2011.10.022", "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a different insight to analyze AdaBoost. This\nanalysis reveals that, beyond some preconceptions, AdaBoost can be directly\nused as an asymmetric learning algorithm, preserving all its theoretical\nproperties. A novel class-conditional description of AdaBoost, which models the\nactual asymmetric behavior of the algorithm, is presented.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2015 09:58:06 GMT"}], "update_date": "2015-07-15", "authors_parsed": [["Landesa-V\u00e1zquez", "Iago", ""], ["Alba-Castro", "Jos\u00e9 Luis", ""]]}, {"id": "1507.02144", "submitter": "Hossein Azizpour", "authors": "Hossein Azizpour, Mostafa Arefiyan, Sobhan Naderi Parizi, Stefan\n  Carlsson", "title": "Spotlight the Negatives: A Generalized Discriminative Latent Model", "comments": "Published in proceedings of BMVC 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discriminative latent variable models (LVM) are frequently applied to various\nvisual recognition tasks. In these systems the latent (hidden) variables\nprovide a formalism for modeling structured variation of visual features.\nConventionally, latent variables are de- fined on the variation of the\nforeground (positive) class. In this work we augment LVMs to include negative\nlatent variables corresponding to the background class. We formalize the\nscoring function of such a generalized LVM (GLVM). Then we discuss a framework\nfor learning a model based on the GLVM scoring function. We theoretically\nshowcase how some of the current visual recognition methods can benefit from\nthis generalization. Finally, we experiment on a generalized form of Deformable\nPart Models with negative latent variables and show significant improvements on\ntwo different detection tasks.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2015 13:26:13 GMT"}], "update_date": "2015-07-09", "authors_parsed": [["Azizpour", "Hossein", ""], ["Arefiyan", "Mostafa", ""], ["Parizi", "Sobhan Naderi", ""], ["Carlsson", "Stefan", ""]]}, {"id": "1507.02150", "submitter": "Xinhua Mao", "authors": "Xinhua Mao", "title": "SAR Imaging of Moving Target based on Knowledge-aided Two-dimensional\n  Autofocus", "comments": "11 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CV math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to uncertainty on target's motion, the range cell migration (RCM) and\nazimuth phase error (APE) of moving targets can't be completely compensated in\nsynthetic aperture radar (SAR) processing. Therefore, moving targets often\nappear two-dimensional (2-D) defocused in SAR images. In this paper, a 2-D\nautofocus method for refocusing defocused moving targets in SAR images is\npresented. The new method only requires a direct estimate of APE, while the\nresidual 2-D phase error ( or RCM) is computed from the estimated APE by\nexploiting the analytical relationship between the 2-D phase error ( or RCM)\nand APE. Because the parameter estimation is performed in the reduced-dimension\nspace by exploiting prior knowledge on phase error structure, the proposed\napproach offers clear advantages in both computational efficiency and\nestimation accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2015 13:34:25 GMT"}], "update_date": "2015-07-09", "authors_parsed": [["Mao", "Xinhua", ""]]}, {"id": "1507.02154", "submitter": "Iago Landesa-V\\'azquez", "authors": "Iago Landesa-V\\'azquez, Jos\\'e Luis Alba-Castro", "title": "Double-Base Asymmetric AdaBoost", "comments": null, "journal-ref": "Neurocomputing 118 (2013) 101-114", "doi": "10.1016/j.neucom.2013.02.019", "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Based on the use of different exponential bases to define class-dependent\nerror bounds, a new and highly efficient asymmetric boosting scheme, coined as\nAdaBoostDB (Double-Base), is proposed. Supported by a fully theoretical\nderivation procedure, unlike most of the other approaches in the literature,\nour algorithm preserves all the formal guarantees and properties of original\n(cost-insensitive) AdaBoost, similarly to the state-of-the-art Cost-Sensitive\nAdaBoost algorithm. However, the key advantage of AdaBoostDB is that our novel\nderivation scheme enables an extremely efficient conditional search procedure,\ndramatically improving and simplifying the training phase of the algorithm.\nExperiments, both over synthetic and real datasets, reveal that AdaBoostDB is\nable to save over 99% training time with regard to Cost-Sensitive AdaBoost,\nproviding the same cost-sensitive results. This computational advantage of\nAdaBoostDB can make a difference in problems managing huge pools of weak\nclassifiers in which boosting techniques are commonly used.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2015 13:44:34 GMT"}], "update_date": "2015-07-09", "authors_parsed": [["Landesa-V\u00e1zquez", "Iago", ""], ["Alba-Castro", "Jos\u00e9 Luis", ""]]}, {"id": "1507.02159", "submitter": "Limin Wang", "authors": "Limin Wang, Yuanjun Xiong, Zhe Wang, Yu Qiao", "title": "Towards Good Practices for Very Deep Two-Stream ConvNets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional networks have achieved great success for object\nrecognition in still images. However, for action recognition in videos, the\nimprovement of deep convolutional networks is not so evident. We argue that\nthere are two reasons that could probably explain this result. First the\ncurrent network architectures (e.g. Two-stream ConvNets) are relatively shallow\ncompared with those very deep models in image domain (e.g. VGGNet, GoogLeNet),\nand therefore their modeling capacity is constrained by their depth. Second,\nprobably more importantly, the training dataset of action recognition is\nextremely small compared with the ImageNet dataset, and thus it will be easy to\nover-fit on the training dataset.\n  To address these issues, this report presents very deep two-stream ConvNets\nfor action recognition, by adapting recent very deep architectures into video\ndomain. However, this extension is not easy as the size of action recognition\nis quite small. We design several good practices for the training of very deep\ntwo-stream ConvNets, namely (i) pre-training for both spatial and temporal\nnets, (ii) smaller learning rates, (iii) more data augmentation techniques,\n(iv) high drop out ratio. Meanwhile, we extend the Caffe toolbox into Multi-GPU\nimplementation with high computational efficiency and low memory consumption.\nWe verify the performance of very deep two-stream ConvNets on the dataset of\nUCF101 and it achieves the recognition accuracy of $91.4\\%$.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2015 14:00:35 GMT"}], "update_date": "2015-07-09", "authors_parsed": [["Wang", "Limin", ""], ["Xiong", "Yuanjun", ""], ["Wang", "Zhe", ""], ["Qiao", "Yu", ""]]}, {"id": "1507.02177", "submitter": "Shervin Minaee", "authors": "Shervin Minaee, AmirAli Abdolrashidi and Yao Wang", "title": "Iris Recognition Using Scattering Transform and Textural Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Iris recognition has drawn a lot of attention since the mid-twentieth\ncentury. Among all biometric features, iris is known to possess a rich set of\nfeatures. Different features have been used to perform iris recognition in the\npast. In this paper, two powerful sets of features are introduced to be used\nfor iris recognition: scattering transform-based features and textural\nfeatures. PCA is also applied on the extracted features to reduce the\ndimensionality of the feature vector while preserving most of the information\nof its initial value. Minimum distance classifier is used to perform template\nmatching for each new test sample. The proposed scheme is tested on a\nwell-known iris database, and showed promising results with the best accuracy\nrate of 99.2%.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2015 14:37:24 GMT"}], "update_date": "2015-07-09", "authors_parsed": [["Minaee", "Shervin", ""], ["Abdolrashidi", "AmirAli", ""], ["Wang", "Yao", ""]]}, {"id": "1507.02313", "submitter": "Keegan Kang", "authors": "Ben Athiwaratkun and Keegan Kang", "title": "Feature Representation in Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNNs) are powerful models that achieve\nimpressive results for image classification. In addition, pre-trained CNNs are\nalso useful for other computer vision tasks as generic feature extractors. This\npaper aims to gain insight into the feature aspect of CNN and demonstrate other\nuses of CNN features. Our results show that CNN feature maps can be used with\nRandom Forests and SVM to yield classification results that outperforms the\noriginal CNN. A CNN that is less than optimal (e.g. not fully trained or\noverfitting) can also extract features for Random Forest/SVM that yield\ncompetitive classification accuracy. In contrast to the literature which uses\nthe top-layer activations as feature representation of images for other tasks,\nusing lower-layer features can yield better results for classification.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2015 21:13:26 GMT"}], "update_date": "2015-07-10", "authors_parsed": [["Athiwaratkun", "Ben", ""], ["Kang", "Keegan", ""]]}, {"id": "1507.02346", "submitter": "Jaderick Pabico", "authors": "Jaderick P. Pabico, Alona V. De Grano, Alan L. Zarsuela", "title": "Neural Network Classifiers for Natural Food Products", "comments": "8 pages, 5 figures, appeared in H.N. Adorna, R.E.O. Roxas, and A.L.\n  Sioson (eds.) Proceedings of the 12th Philippine Computing Science Congress\n  (PCSC 2012), De La Salle Canlubang, Bi\\~nan, Laguna, Philippines, 01-03 March\n  2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Two cheap, off-the-shelf machine vision systems (MVS), each using an\nartificial neural network (ANN) as classifier, were developed, improved and\nevaluated to automate the classification of tomato ripeness and acceptability\nof eggs, respectively. Six thousand color images of human-graded tomatoes and\n750 images of human-graded eggs were used to train, test, and validate several\nmulti-layered ANNs. The ANNs output the corresponding grade of the produce by\naccepting as inputs the spectral patterns of the background-less image. In both\nMVS, the ANN with the highest validation rate was automatically chosen by a\nheuristic and its performance compared to that of the human graders'. Using the\nvalidation set, the MVS correctly graded 97.00\\% and 86.00\\% of the tomato and\negg data, respectively. The human grader's, however, were measured to perform\nat a daily average of 92.65\\% and 72.67\\% for tomato and egg grading,\nrespectively. This results show that an ANN-based MVS is a potential\nalternative to manual grading.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2015 02:09:19 GMT"}], "update_date": "2015-07-10", "authors_parsed": [["Pabico", "Jaderick P.", ""], ["De Grano", "Alona V.", ""], ["Zarsuela", "Alan L.", ""]]}, {"id": "1507.02355", "submitter": "Giovanni Viglietta", "authors": "Prosenjit Bose, Jean-Lou De Carufel, Michael G. Dobbins, Heuna Kim,\n  and Giovanni Viglietta", "title": "The Shadows of a Cycle Cannot All Be Paths", "comments": "6 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.CV math.MG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A \"shadow\" of a subset $S$ of Euclidean space is an orthogonal projection of\n$S$ into one of the coordinate hyperplanes. In this paper we show that it is\nnot possible for all three shadows of a cycle (i.e., a simple closed curve) in\n$\\mathbb R^3$ to be paths (i.e., simple open curves).\n  We also show two contrasting results: the three shadows of a path in $\\mathbb\nR^3$ can all be cycles (although not all convex) and, for every $d\\geq 1$,\nthere exists a $d$-sphere embedded in $\\mathbb R^{d+2}$ whose $d+2$ shadows\nhave no holes (i.e., they deformation-retract onto a point).\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2015 02:47:50 GMT"}], "update_date": "2015-07-10", "authors_parsed": [["Bose", "Prosenjit", ""], ["De Carufel", "Jean-Lou", ""], ["Dobbins", "Michael G.", ""], ["Kim", "Heuna", ""], ["Viglietta", "Giovanni", ""]]}, {"id": "1507.02379", "submitter": "Donglai Wei Mr.", "authors": "Donglai Wei, Bolei Zhou, Antonio Torrabla, William Freeman", "title": "Understanding Intra-Class Knowledge Inside CNN", "comments": "tech report for: http://vision03.csail.mit.edu/cnn_art/index.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Network (CNN) has been successful in image recognition\ntasks, and recent works shed lights on how CNN separates different classes with\nthe learned inter-class knowledge through visualization. In this work, we\ninstead visualize the intra-class knowledge inside CNN to better understand how\nan object class is represented in the fully-connected layers.\n  To invert the intra-class knowledge into more interpretable images, we\npropose a non-parametric patch prior upon previous CNN visualization models.\nWith it, we show how different \"styles\" of templates for an object class are\norganized by CNN in terms of location and content, and represented in a\nhierarchical and ensemble way. Moreover, such intra-class knowledge can be used\nin many interesting applications, e.g. style-based image retrieval and\nstyle-based object completion.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2015 05:20:43 GMT"}, {"version": "v2", "created": "Tue, 21 Jul 2015 10:18:57 GMT"}], "update_date": "2015-07-22", "authors_parsed": [["Wei", "Donglai", ""], ["Zhou", "Bolei", ""], ["Torrabla", "Antonio", ""], ["Freeman", "William", ""]]}, {"id": "1507.02380", "submitter": "Ran He", "authors": "Ran He and Tieniu Tan and Larry Davis and Zhenan Sun", "title": "Learning Structured Ordinal Measures for Video based Face Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a structured ordinal measure method for video-based face\nrecognition that simultaneously learns ordinal filters and structured ordinal\nfeatures. The problem is posed as a non-convex integer program problem that\nincludes two parts. The first part learns stable ordinal filters to project\nvideo data into a large-margin ordinal space. The second seeks self-correcting\nand discrete codes by balancing the projected data and a rank-one ordinal\nmatrix in a structured low-rank way. Unsupervised and supervised structures are\nconsidered for the ordinal matrix. In addition, as a complement to hierarchical\nstructures, deep feature representations are integrated into our method to\nenhance coding stability. An alternating minimization method is employed to\nhandle the discrete and low-rank constraints, yielding high-quality codes that\ncapture prior structures well. Experimental results on three commonly used face\nvideo databases show that our method with a simple voting classifier can\nachieve state-of-the-art recognition rates using fewer features and samples.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2015 05:34:36 GMT"}], "update_date": "2015-07-10", "authors_parsed": [["He", "Ran", ""], ["Tan", "Tieniu", ""], ["Davis", "Larry", ""], ["Sun", "Zhenan", ""]]}, {"id": "1507.02385", "submitter": "Qilong Wang", "authors": "Qilong Wang, Peihua Li, Lei Zhang, Wangmeng Zuo", "title": "Towards Effective Codebookless Model for Image Classification", "comments": null, "journal-ref": "Pattern Recognition 59 (2016) 63-71", "doi": "10.1016/j.patcog.2016.03.004", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The bag-of-features (BoF) model for image classification has been thoroughly\nstudied over the last decade. Different from the widely used BoF methods which\nmodeled images with a pre-trained codebook, the alternative codebook free image\nmodeling method, which we call Codebookless Model (CLM), attracted little\nattention. In this paper, we present an effective CLM that represents an image\nwith a single Gaussian for classification. By embedding Gaussian manifold into\na vector space, we show that the simple incorporation of our CLM into a linear\nclassifier achieves very competitive accuracy compared with state-of-the-art\nBoF methods (e.g., Fisher Vector). Since our CLM lies in a high dimensional\nRiemannian manifold, we further propose a joint learning method of low-rank\ntransformation with support vector machine (SVM) classifier on the Gaussian\nmanifold, in order to reduce computational and storage cost. To study and\nalleviate the side effect of background clutter on our CLM, we also present a\nsimple yet effective partial background removal method based on saliency\ndetection. Experiments are extensively conducted on eight widely used databases\nto demonstrate the effectiveness and efficiency of our CLM method.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2015 06:01:34 GMT"}, {"version": "v2", "created": "Mon, 20 Jul 2015 05:48:55 GMT"}], "update_date": "2018-04-03", "authors_parsed": [["Wang", "Qilong", ""], ["Li", "Peihua", ""], ["Zhang", "Lei", ""], ["Zuo", "Wangmeng", ""]]}, {"id": "1507.02407", "submitter": "Julian Yarkony", "authors": "Julian Yarkony, Charless C. Fowlkes", "title": "Planar Ultrametric Rounding for Image Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of hierarchical clustering on planar graphs. We\nformulate this in terms of an LP relaxation of ultrametric rounding. To solve\nthis LP efficiently we introduce a dual cutting plane scheme that uses minimum\ncost perfect matching as a subroutine in order to efficiently explore the space\nof planar partitions. We apply our algorithm to the problem of hierarchical\nimage segmentation.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2015 08:07:34 GMT"}, {"version": "v2", "created": "Thu, 30 Jul 2015 21:28:24 GMT"}, {"version": "v3", "created": "Thu, 10 Sep 2015 03:32:55 GMT"}], "update_date": "2015-09-11", "authors_parsed": [["Yarkony", "Julian", ""], ["Fowlkes", "Charless C.", ""]]}, {"id": "1507.02438", "submitter": "Tae Hyun Kim", "authors": "Tae Hyun Kim and Kyoung Mu Lee", "title": "Generalized Video Deblurring for Dynamic Scenes", "comments": "CVPR 2015 oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several state-of-the-art video deblurring methods are based on a strong\nassumption that the captured scenes are static. These methods fail to deblur\nblurry videos in dynamic scenes. We propose a video deblurring method to deal\nwith general blurs inherent in dynamic scenes, contrary to other methods. To\nhandle locally varying and general blurs caused by various sources, such as\ncamera shake, moving objects, and depth variation in a scene, we approximate\npixel-wise kernel with bidirectional optical flows. Therefore, we propose a\nsingle energy model that simultaneously estimates optical flows and latent\nframes to solve our deblurring problem. We also provide a framework and\nefficient solvers to optimize the energy model. By minimizing the proposed\nenergy function, we achieve significant improvements in removing blurs and\nestimating accurate optical flows in blurry frames. Extensive experimental\nresults demonstrate the superiority of the proposed method in real and\nchallenging videos that state-of-the-art methods fail in either deblurring or\noptical flow estimation.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2015 09:59:40 GMT"}], "update_date": "2015-07-10", "authors_parsed": [["Kim", "Tae Hyun", ""], ["Lee", "Kyoung Mu", ""]]}, {"id": "1507.02558", "submitter": "Ilaria Gori", "authors": "Ilaria Gori, J. K. Aggarwal, Larry Matthies, Michael S. Ryoo", "title": "Multi-Type Activity Recognition in Robot-Centric Scenarios", "comments": null, "journal-ref": "IEEE Robotics and Automation Letters (RA-L), 1(1):593-600, 2016", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Activity recognition is very useful in scenarios where robots interact with,\nmonitor or assist humans. In the past years many types of activities -- single\nactions, two persons interactions or ego-centric activities, to name a few --\nhave been analyzed. Whereas traditional methods treat such types of activities\nseparately, an autonomous robot should be able to detect and recognize multiple\ntypes of activities to effectively fulfill its tasks. We propose a method that\nis intrinsically able to detect and recognize activities of different types\nthat happen in sequence or concurrently. We present a new unified descriptor,\ncalled Relation History Image (RHI), which can be extracted from all the\nactivity types we are interested in. We then formulate an optimization\nprocedure to detect and recognize activities of different types. We apply our\napproach to a new dataset recorded from a robot-centric perspective and\nsystematically evaluate its quality compared to multiple baselines. Finally, we\nshow the efficacy of the RHI descriptor on publicly available datasets\nperforming extensive comparisons.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2015 15:33:40 GMT"}, {"version": "v2", "created": "Tue, 12 Apr 2016 01:33:06 GMT"}], "update_date": "2016-04-13", "authors_parsed": [["Gori", "Ilaria", ""], ["Aggarwal", "J. K.", ""], ["Matthies", "Larry", ""], ["Ryoo", "Michael S.", ""]]}, {"id": "1507.02620", "submitter": "Mircea Cimpoi", "authors": "Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Andrea Vedaldi", "title": "Deep filter banks for texture recognition, description, and segmentation", "comments": "29 pages; 13 figures; 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual textures have played a key role in image understanding because they\nconvey important semantics of images, and because texture representations that\npool local image descriptors in an orderless manner have had a tremendous\nimpact in diverse applications. In this paper we make several contributions to\ntexture understanding. First, instead of focusing on texture instance and\nmaterial category recognition, we propose a human-interpretable vocabulary of\ntexture attributes to describe common texture patterns, complemented by a new\ndescribable texture dataset for benchmarking. Second, we look at the problem of\nrecognizing materials and texture attributes in realistic imaging conditions,\nincluding when textures appear in clutter, developing corresponding benchmarks\non top of the recently proposed OpenSurfaces dataset. Third, we revisit classic\ntexture representations, including bag-of-visual-words and the Fisher vectors,\nin the context of deep learning and show that these have excellent efficiency\nand generalization properties if the convolutional layers of a deep model are\nused as filter banks. We obtain in this manner state-of-the-art performance in\nnumerous datasets well beyond textures, an efficient method to apply deep\nfeatures to image regions, as well as benefit in transferring features from one\ndomain to another.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2015 17:55:30 GMT"}, {"version": "v2", "created": "Wed, 18 Nov 2015 23:10:52 GMT"}], "update_date": "2015-11-20", "authors_parsed": [["Cimpoi", "Mircea", ""], ["Maji", "Subhransu", ""], ["Kokkinos", "Iasonas", ""], ["Vedaldi", "Andrea", ""]]}, {"id": "1507.02703", "submitter": "Shuran Song", "authors": "Shuran Song, Linguang Zhang, Jianxiong Xiao", "title": "Robot In a Room: Toward Perfect Object Recognition in Closed\n  Environments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While general object recognition is still far from being solved, this paper\nproposes a way for a robot to recognize every object at an almost human-level\naccuracy. Our key observation is that many robots will stay in a relatively\nclosed environment (e.g. a house or an office). By constraining a robot to stay\nin a limited territory, we can ensure that the robot has seen most objects\nbefore and the speed of introducing a new object is slow. Furthermore, we can\nbuild a 3D map of the environment to reliably subtract the background to make\nrecognition easier. We propose extremely robust algorithms to obtain a 3D map\nand enable humans to collectively annotate objects. During testing time, our\nalgorithm can recognize all objects very reliably, and query humans from crowd\nsourcing platform if confidence is low or new objects are identified. This\npaper explains design decisions in building such a system, and constructs a\nbenchmark for extensive evaluation. Experiments suggest that making robot\nvision appear to be working from an end user's perspective is a reachable goal\ntoday, as long as the robot stays in a closed environment. By formulating this\ntask, we hope to lay the foundation of a new direction in vision for robotics.\nCode and data will be available upon acceptance.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2015 20:40:58 GMT"}], "update_date": "2015-07-13", "authors_parsed": [["Song", "Shuran", ""], ["Zhang", "Linguang", ""], ["Xiao", "Jianxiong", ""]]}, {"id": "1507.02772", "submitter": "Anoop Cherian", "authors": "Anoop Cherian and Suvrit Sra", "title": "Riemannian Dictionary Learning and Sparse Coding for Positive Definite\n  Matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data encoded as symmetric positive definite (SPD) matrices frequently arise\nin many areas of computer vision and machine learning. While these matrices\nform an open subset of the Euclidean space of symmetric matrices, viewing them\nthrough the lens of non-Euclidean Riemannian geometry often turns out to be\nbetter suited in capturing several desirable data properties. However,\nformulating classical machine learning algorithms within such a geometry is\noften non-trivial and computationally expensive. Inspired by the great success\nof dictionary learning and sparse coding for vector-valued data, our goal in\nthis paper is to represent data in the form of SPD matrices as sparse conic\ncombinations of SPD atoms from a learned dictionary via a Riemannian geometric\napproach. To that end, we formulate a novel Riemannian optimization objective\nfor dictionary learning and sparse coding in which the representation loss is\ncharacterized via the affine invariant Riemannian metric. We also present a\ncomputationally simple algorithm for optimizing our model. Experiments on\nseveral computer vision datasets demonstrate superior classification and\nretrieval performance using our approach when compared to sparse coding via\nalternative non-Riemannian formulations.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2015 03:18:50 GMT"}, {"version": "v2", "created": "Thu, 17 Dec 2015 03:33:50 GMT"}], "update_date": "2015-12-18", "authors_parsed": [["Cherian", "Anoop", ""], ["Sra", "Suvrit", ""]]}, {"id": "1507.02779", "submitter": "Hai Pham", "authors": "Hai X. Pham, Chongyu Chen, Luc N. Dao, Vladimir Pavlovic, Jianfei Cai\n  and Tat-jen Cham", "title": "Robust Performance-driven 3D Face Tracking in Long Range Depth Scenes", "comments": "10 pages, 8 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel robust hybrid 3D face tracking framework from RGBD video\nstreams, which is capable of tracking head pose and facial actions without\npre-calibration or intervention from a user. In particular, we emphasize on\nimproving the tracking performance in instances where the tracked subject is at\na large distance from the cameras, and the quality of point cloud deteriorates\nseverely. This is accomplished by the combination of a flexible 3D shape\nregressor and the joint 2D+3D optimization on shape parameters. Our approach\nfits facial blendshapes to the point cloud of the human head, while being\ndriven by an efficient and rapid 3D shape regressor trained on generic RGB\ndatasets. As an on-line tracking system, the identity of the unknown user is\nadapted on-the-fly resulting in improved 3D model reconstruction and\nconsequently better tracking performance. The result is a robust RGBD face\ntracker, capable of handling a wide range of target scene depths, beyond those\nthat can be afforded by traditional depth or RGB face trackers. Lastly, since\nthe blendshape is not able to accurately recover the real facial shape, we use\nthe tracked 3D face model as a prior in a novel filtering process to further\nrefine the depth map for use in other tasks, such as 3D reconstruction.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2015 04:52:36 GMT"}], "update_date": "2015-07-13", "authors_parsed": [["Pham", "Hai X.", ""], ["Chen", "Chongyu", ""], ["Dao", "Luc N.", ""], ["Pavlovic", "Vladimir", ""], ["Cai", "Jianfei", ""], ["Cham", "Tat-jen", ""]]}, {"id": "1507.02879", "submitter": "M. Saquib Sarfraz", "authors": "M. Saquib Sarfraz and Rainer Stiefelhagen", "title": "Deep Perceptual Mapping for Thermal to Visible Face Recognition", "comments": "BMVC 2015 (oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross modal face matching between the thermal and visible spectrum is a much\nde- sired capability for night-time surveillance and security applications. Due\nto a very large modality gap, thermal-to-visible face recognition is one of the\nmost challenging face matching problem. In this paper, we present an approach\nto bridge this modality gap by a significant margin. Our approach captures the\nhighly non-linear relationship be- tween the two modalities by using a deep\nneural network. Our model attempts to learn a non-linear mapping from visible\nto thermal spectrum while preserving the identity in- formation. We show\nsubstantive performance improvement on a difficult thermal-visible face\ndataset. The presented approach improves the state-of-the-art by more than 10%\nin terms of Rank-1 identification and bridge the drop in performance due to the\nmodality gap by more than 40%.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2015 12:55:34 GMT"}], "update_date": "2015-07-13", "authors_parsed": [["Sarfraz", "M. Saquib", ""], ["Stiefelhagen", "Rainer", ""]]}, {"id": "1507.03060", "submitter": "Hongkai Yu", "authors": "Hongkai Yu, Youjie Zhou, Hui Qian, Min Xian, Yuewei Lin, Dazhou Guo,\n  Kang Zheng, Kareem Abdelfatah, Song Wang", "title": "LooseCut: Interactive Image Segmentation with Loosely Bounded Boxes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One popular approach to interactively segment the foreground object of\ninterest from an image is to annotate a bounding box that covers the foreground\nobject. Then, a binary labeling is performed to achieve a refined segmentation.\nOne major issue of the existing algorithms for such interactive image\nsegmentation is their preference of an input bounding box that tightly encloses\nthe foreground object. This increases the annotation burden, and prevents these\nalgorithms from utilizing automatically detected bounding boxes. In this paper,\nwe develop a new LooseCut algorithm that can handle cases where the input\nbounding box only loosely covers the foreground object. We propose a new Markov\nRandom Fields (MRF) model for segmentation with loosely bounded boxes,\nincluding a global similarity constraint to better distinguish the foreground\nand background, and an additional energy term to encourage consistent labeling\nof similar-appearance pixels. This MRF model is then solved by an iterated\nmax-flow algorithm. In the experiments, we evaluate LooseCut in three\npublicly-available image datasets, and compare its performance against several\nstate-of-the-art interactive image segmentation algorithms. We also show that\nLooseCut can be used for enhancing the performance of unsupervised video\nsegmentation and image saliency detection.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jul 2015 03:04:36 GMT"}, {"version": "v2", "created": "Sun, 22 Nov 2015 03:54:17 GMT"}], "update_date": "2015-11-24", "authors_parsed": [["Yu", "Hongkai", ""], ["Zhou", "Youjie", ""], ["Qian", "Hui", ""], ["Xian", "Min", ""], ["Lin", "Yuewei", ""], ["Guo", "Dazhou", ""], ["Zheng", "Kang", ""], ["Abdelfatah", "Kareem", ""], ["Wang", "Song", ""]]}, {"id": "1507.03148", "submitter": "Heng Yang", "authors": "Heng Yang and Wenxuan Mou and Yichi Zhang and Ioannis Patras and\n  Hatice Gunes and Peter Robinson", "title": "Face Alignment Assisted by Head Pose Estimation", "comments": "Accepted by BMVC2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a supervised initialization scheme for cascaded face\nalignment based on explicit head pose estimation. We first investigate the\nfailure cases of most state of the art face alignment approaches and observe\nthat these failures often share one common global property, i.e. the head pose\nvariation is usually large. Inspired by this, we propose a deep convolutional\nnetwork model for reliable and accurate head pose estimation. Instead of using\na mean face shape, or randomly selected shapes for cascaded face alignment\ninitialisation, we propose two schemes for generating initialisation: the first\none relies on projecting a mean 3D face shape (represented by 3D facial\nlandmarks) onto 2D image under the estimated head pose; the second one searches\nnearest neighbour shapes from the training set according to head pose distance.\nBy doing so, the initialisation gets closer to the actual shape, which enhances\nthe possibility of convergence and in turn improves the face alignment\nperformance. We demonstrate the proposed method on the benchmark 300W dataset\nand show very competitive performance in both head pose estimation and face\nalignment.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jul 2015 20:07:51 GMT"}, {"version": "v2", "created": "Sat, 18 Jul 2015 12:36:58 GMT"}], "update_date": "2015-07-21", "authors_parsed": [["Yang", "Heng", ""], ["Mou", "Wenxuan", ""], ["Zhang", "Yichi", ""], ["Patras", "Ioannis", ""], ["Gunes", "Hatice", ""], ["Robinson", "Peter", ""]]}, {"id": "1507.03196", "submitter": "Zhangyang Wang", "authors": "Zhangyang Wang, Jianchao Yang, Hailin Jin, Eli Shechtman, Aseem\n  Agarwala, Jonathan Brandt, Thomas S. Huang", "title": "DeepFont: Identify Your Font from An Image", "comments": "To Appear in ACM Multimedia as a full paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As font is one of the core design concepts, automatic font identification and\nsimilar font suggestion from an image or photo has been on the wish list of\nmany designers. We study the Visual Font Recognition (VFR) problem, and advance\nthe state-of-the-art remarkably by developing the DeepFont system. First of\nall, we build up the first available large-scale VFR dataset, named AdobeVFR,\nconsisting of both labeled synthetic data and partially labeled real-world\ndata. Next, to combat the domain mismatch between available training and\ntesting data, we introduce a Convolutional Neural Network (CNN) decomposition\napproach, using a domain adaptation technique based on a Stacked Convolutional\nAuto-Encoder (SCAE) that exploits a large corpus of unlabeled real-world text\nimages combined with synthetic data preprocessed in a specific way. Moreover,\nwe study a novel learning-based model compression approach, in order to reduce\nthe DeepFont model size without sacrificing its performance. The DeepFont\nsystem achieves an accuracy of higher than 80% (top-5) on our collected\ndataset, and also produces a good font similarity measure for font selection\nand suggestion. We also achieve around 6 times compression of the model without\nany visible loss of recognition accuracy.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jul 2015 07:25:14 GMT"}], "update_date": "2015-07-14", "authors_parsed": [["Wang", "Zhangyang", ""], ["Yang", "Jianchao", ""], ["Jin", "Hailin", ""], ["Shechtman", "Eli", ""], ["Agarwala", "Aseem", ""], ["Brandt", "Jonathan", ""], ["Huang", "Thomas S.", ""]]}, {"id": "1507.03360", "submitter": "Kedar Khare", "authors": "Charu Gaur, Baranidharan Mohan, and Kedar Khare", "title": "Sparsity assisted solution to the twin image problem in phase retrieval", "comments": null, "journal-ref": null, "doi": "10.1364/JOSAA.32.001922", "report-no": null, "categories": "cs.CV physics.optics", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The iterative phase retrieval problem for complex-valued objects from Fourier\ntransform magnitude data is known to suffer from the twin image problem. In\nparticular, when the object support is centro-symmetric, the iterative solution\noften stagnates such that the resultant complex image contains the features of\nboth the desired solution and its inverted and complex-conjugated replica. The\nconventional approach to address the twin image problem is to modify the object\nsupport during initial iterations which can possibly lead to elimination of one\nof the twin images. However, at present there seems to be no deterministic\nprocedure to make sure that the twin image will always be very weak or absent.\nIn this work we make an important observation that the ideal solution without\nthe twin image is typically more sparse (in some suitable transform domain) as\ncompared to the stagnated solution containing the twin image. We further show\nthat introducing a sparsity enhancing step in the iterative algorithm can\naddress the twin image problem without the need to change the object support\nthroughout the iterative process even when the object support is\ncentro-symmetric. In a simulation study, we use binary and gray-scale pure\nphase objects and illustrate the effectiveness of the sparsity assisted phase\nrecovery in the context of the twin image problem. The results have important\nimplications for a wide range of topics in Physics where the phase retrieval\nproblem plays a central role.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2015 08:52:03 GMT"}], "update_date": "2015-10-28", "authors_parsed": [["Gaur", "Charu", ""], ["Mohan", "Baranidharan", ""], ["Khare", "Kedar", ""]]}, {"id": "1507.03409", "submitter": "Zhujin Liang", "authors": "Zhujin Liang, Shengyong Ding, Liang Lin", "title": "Unconstrained Facial Landmark Localization with Backbone-Branches\n  Fully-Convolutional Networks", "comments": "This paper has been withdrawn due to several error", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates how to rapidly and accurately localize facial\nlandmarks in unconstrained, cluttered environments rather than in the well\nsegmented face images. We present a novel Backbone-Branches Fully-Convolutional\nNeural Network (BB-FCN), which produces facial landmark response maps directly\nfrom raw images without relying on pre-process or sliding window approaches.\nBB-FCN contains one backbone and a number of network branches with each\ncorresponding to one landmark type, and it operates in a progressive manner.\nSpecifically, the backbone roughly detects the locations of facial landmarks by\ntaking the whole image as input, and the branches further refine the\nlocalizations based on a local observation from the backbone's intermediate\nfeature map. Moreover, our backbone-branches architecture does not contain\nfull-connection layers for location regression, leading to efficient learning\nand inference. Our extensive experiments show that our model achieves superior\nperformances over other state-of-the-arts under both the constrained (i.e. with\nface regions) and the \"in the wild\" scenarios.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2015 11:58:09 GMT"}, {"version": "v2", "created": "Sun, 20 Sep 2015 14:52:17 GMT"}, {"version": "v3", "created": "Thu, 14 Apr 2016 07:38:52 GMT"}], "update_date": "2016-04-15", "authors_parsed": [["Liang", "Zhujin", ""], ["Ding", "Shengyong", ""], ["Lin", "Liang", ""]]}, {"id": "1507.03698", "submitter": "Ra\\'ul D\\'iaz", "authors": "Ra\\'ul D\\'iaz, Minhaeng Lee, Jochen Schubert, Charless C. Fowlkes", "title": "Lifting GIS Maps into Strong Geometric Context for Scene Understanding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contextual information can have a substantial impact on the performance of\nvisual tasks such as semantic segmentation, object detection, and geometric\nestimation. Data stored in Geographic Information Systems (GIS) offers a rich\nsource of contextual information that has been largely untapped by computer\nvision. We propose to leverage such information for scene understanding by\ncombining GIS resources with large sets of unorganized photographs using\nStructure from Motion (SfM) techniques. We present a pipeline to quickly\ngenerate strong 3D geometric priors from 2D GIS data using SfM models aligned\nwith minimal user input. Given an image resectioned against this model, we\ngenerate robust predictions of depth, surface normals, and semantic labels. We\nshow that the precision of the predicted geometry is substantially more\naccurate other single-image depth estimation methods. We then demonstrate the\nutility of these contextual constraints for re-scoring pedestrian detections,\nand use these GIS contextual features alongside object detection score maps to\nimprove a CRF-based semantic segmentation framework, boosting accuracy over\nbaseline models.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2015 02:04:10 GMT"}, {"version": "v2", "created": "Sat, 25 Jul 2015 00:37:21 GMT"}, {"version": "v3", "created": "Tue, 22 Sep 2015 23:21:09 GMT"}, {"version": "v4", "created": "Fri, 8 Jan 2016 19:52:32 GMT"}], "update_date": "2016-02-22", "authors_parsed": [["D\u00edaz", "Ra\u00fal", ""], ["Lee", "Minhaeng", ""], ["Schubert", "Jochen", ""], ["Fowlkes", "Charless C.", ""]]}, {"id": "1507.03751", "submitter": "Manfred Harringer", "authors": "Manfred Harringer", "title": "Closed Curves and Elementary Visual Object Identification", "comments": "13 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For two closed curves on a plane (discrete version) and local criteria for\nsimilarity of points on the curves one gets a potential, which describes the\nsimilarity between curve points. This is the base for a global similarity\nmeasure of closed curves (Fr\\'echet distance). I use borderlines of handwritten\ndigits to demonstrate an area of application. I imagine, measuring the\nsimilarity of closed curves is an essential and elementary task performed by a\nvisual system. This approach to similarity measures may be used by visual\nsystems.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2015 07:57:39 GMT"}], "update_date": "2015-07-15", "authors_parsed": [["Harringer", "Manfred", ""]]}, {"id": "1507.03811", "submitter": "Liliana Lo Presti", "authors": "Liliana Lo Presti and Marco La Cascia", "title": "Ensemble of Hankel Matrices for Face Emotion Recognition", "comments": "Paper to appear in Proc. of ICIAP 2015. arXiv admin note: text\n  overlap with arXiv:1506.05001", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a face emotion is considered as the result of the composition\nof multiple concurrent signals, each corresponding to the movements of a\nspecific facial muscle. These concurrent signals are represented by means of a\nset of multi-scale appearance features that might be correlated with one or\nmore concurrent signals. The extraction of these appearance features from a\nsequence of face images yields to a set of time series. This paper proposes to\nuse the dynamics regulating each appearance feature time series to recognize\namong different face emotions. To this purpose, an ensemble of Hankel matrices\ncorresponding to the extracted time series is used for emotion classification\nwithin a framework that combines nearest neighbor and a majority vote schema.\nExperimental results on a public available dataset shows that the adopted\nrepresentation is promising and yields state-of-the-art accuracy in emotion\nclassification.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2015 11:26:31 GMT"}], "update_date": "2015-07-19", "authors_parsed": [["Presti", "Liliana Lo", ""], ["La Cascia", "Marco", ""]]}, {"id": "1507.04060", "submitter": "Hayder Albehadili", "authors": "Hayder Albehadili and Naz Islam", "title": "Unsupervised Decision Forest for Data Clustering and Density Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An algorithm to improve performance parameter for unsupervised decision\nforest clustering and density estimation is presented. Specifically, a dual\nassignment parameter is introduced as a density estimator by combining Random\nForest and Gaussian Mixture Model. The Random Forest method has been\nspecifically applied to construct a robust affinity graph that provides\ninformation on the underlying structure of data objects used in clustering. The\nproposed algorithm differs from the commonly used spectral clustering methods\nwhere the computed distance metric is used to find similarities between data\npoints. Experiments were conducted using five datasets. A comparison with six\nother state-of-the-art methods shows that our model is superior to existing\napproaches. Efficiency of the proposed model is in capturing the underlying\nstructure for a given set of data points. The proposed method is also robust,\nand can discriminate between the complex features of data points among\ndifferent clusters.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2015 00:50:06 GMT"}], "update_date": "2015-07-19", "authors_parsed": [["Albehadili", "Hayder", ""], ["Islam", "Naz", ""]]}, {"id": "1507.04125", "submitter": "Iago Landesa-V\\'azquez", "authors": "Iago Landesa-V\\'azquez, Jos\\'e Luis Alba-Castro", "title": "Untangling AdaBoost-based Cost-Sensitive Classification. Part I:\n  Theoretical Perspective", "comments": "Extended version of paper submitted to Pattern Recognition (Revised\n  in July 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Boosting algorithms have been widely used to tackle a plethora of problems.\nIn the last few years, a lot of approaches have been proposed to provide\nstandard AdaBoost with cost-sensitive capabilities, each with a different\nfocus. However, for the researcher, these algorithms shape a tangled set with\ndiffuse differences and properties, lacking a unifying analysis to jointly\ncompare, classify, evaluate and discuss those approaches on a common basis. In\nthis series of two papers we aim to revisit the various proposals, both from\ntheoretical (Part I) and practical (Part II) perspectives, in order to analyze\ntheir specific properties and behavior, with the final goal of identifying the\nalgorithm providing the best and soundest results.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2015 08:50:09 GMT"}, {"version": "v2", "created": "Fri, 22 Jul 2016 17:44:11 GMT"}], "update_date": "2016-07-25", "authors_parsed": [["Landesa-V\u00e1zquez", "Iago", ""], ["Alba-Castro", "Jos\u00e9 Luis", ""]]}, {"id": "1507.04126", "submitter": "Iago Landesa-V\\'azquez", "authors": "Iago Landesa-V\\'azquez, Jos\\'e Luis Alba-Castro", "title": "Untangling AdaBoost-based Cost-Sensitive Classification. Part II:\n  Empirical Analysis", "comments": "Extended version of paper submitted to Pattern Recognition (Revised\n  in July 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A lot of approaches, each following a different strategy, have been proposed\nin the literature to provide AdaBoost with cost-sensitive properties. In the\nfirst part of this series of two papers, we have presented these algorithms in\na homogeneous notational framework, proposed a clustering scheme for them and\nperformed a thorough theoretical analysis of those approaches with a fully\ntheoretical foundation. The present paper, in order to complete our analysis,\nis focused on the empirical study of all the algorithms previously presented\nover a wide range of heterogeneous classification problems. The results of our\nexperiments, confirming the theoretical conclusions, seem to reveal that the\nsimplest approach, just based on cost-sensitive weight initialization, is the\none showing the best and soundest results, despite having been recurrently\noverlooked in the literature.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2015 08:51:18 GMT"}, {"version": "v2", "created": "Fri, 22 Jul 2016 17:44:33 GMT"}], "update_date": "2016-07-25", "authors_parsed": [["Landesa-V\u00e1zquez", "Iago", ""], ["Alba-Castro", "Jos\u00e9 Luis", ""]]}, {"id": "1507.04437", "submitter": "Pan Yang", "authors": "Guoqiang Zhong, Pan Yang, Sijiang Wang, Junyu Dong", "title": "A Deep Hashing Learning Network", "comments": "7 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hashing-based methods seek compact and efficient binary codes that preserve\nthe neighborhood structure in the original data space. For most existing\nhashing methods, an image is first encoded as a vector of hand-crafted visual\nfeature, followed by a hash projection and quantization step to get the compact\nbinary vector. Most of the hand-crafted features just encode the low-level\ninformation of the input, the feature may not preserve the semantic\nsimilarities of images pairs. Meanwhile, the hashing function learning process\nis independent with the feature representation, so the feature may not be\noptimal for the hashing projection. In this paper, we propose a supervised\nhashing method based on a well designed deep convolutional neural network,\nwhich tries to learn hashing code and compact representations of data\nsimultaneously. The proposed model learn the binary codes by adding a compact\nsigmoid layer before the loss layer. Experiments on several image data sets\nshow that the proposed model outperforms other state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2015 02:57:59 GMT"}], "update_date": "2015-07-17", "authors_parsed": [["Zhong", "Guoqiang", ""], ["Yang", "Pan", ""], ["Wang", "Sijiang", ""], ["Dong", "Junyu", ""]]}, {"id": "1507.04512", "submitter": "Hongyuan Zhu", "authors": "Hongyuan Zhu, Shijian Lu, Jianfei Cai, Quangqing Lee", "title": "Diagnosing State-Of-The-Art Object Proposal Methods", "comments": "Accepted to BMVC 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object proposal has become a popular paradigm to replace exhaustive sliding\nwindow search in current top-performing methods in PASCAL VOC and ImageNet.\nRecently, Hosang et al. conduct the first unified study of existing methods' in\nterms of various image-level degradations. On the other hand, the vital\nquestion \"what object-level characteristics really affect existing methods'\nperformance?\" is not yet answered. Inspired by Hoiem et al.'s work in\ncategorical object detection, this paper conducts the first meta-analysis of\nvarious object-level characteristics' impact on state-of-the-art object\nproposal methods. Specifically, we examine the effects of object size, aspect\nratio, iconic view, color contrast, shape regularity and texture. We also\nanalyse existing methods' localization accuracy and latency for various PASCAL\nVOC object classes. Our study reveals the limitations of existing methods in\nterms of non-iconic view, small object size, low color contrast, shape\nregularity etc. Based on our observations, lessons are also learned and shared\nwith respect to the selection of existing object proposal technologies as well\nas the design of the future ones.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2015 10:00:30 GMT"}], "update_date": "2015-07-17", "authors_parsed": [["Zhu", "Hongyuan", ""], ["Lu", "Shijian", ""], ["Cai", "Jianfei", ""], ["Lee", "Quangqing", ""]]}, {"id": "1507.04576", "submitter": "Maedeh Aghaei", "authors": "Maedeh Aghaei and Mariella Dimiccoli and Petia Radeva", "title": "Multi-Face Tracking by Extended Bag-of-Tracklets in Egocentric Videos", "comments": "27 pages, 18 figures, submitted to computer vision and image\n  understanding journal", "journal-ref": null, "doi": "10.1016/j.cviu.2016.02.013", "report-no": "YCVIU2393", "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wearable cameras offer a hands-free way to record egocentric images of daily\nexperiences, where social events are of special interest. The first step\ntowards detection of social events is to track the appearance of multiple\npersons involved in it. In this paper, we propose a novel method to find\ncorrespondences of multiple faces in low temporal resolution egocentric videos\nacquired through a wearable camera. This kind of photo-stream imposes\nadditional challenges to the multi-tracking problem with respect to\nconventional videos. Due to the free motion of the camera and to its low\ntemporal resolution, abrupt changes in the field of view, in illumination\ncondition and in the target location are highly frequent. To overcome such\ndifficulties, we propose a multi-face tracking method that generates a set of\ntracklets through finding correspondences along the whole sequence for each\ndetected face and takes advantage of the tracklets redundancy to deal with\nunreliable ones. Similar tracklets are grouped into the so called extended\nbag-of-tracklets (eBoT), which is aimed to correspond to a specific person.\nFinally, a prototype tracklet is extracted for each eBoT, where the occurred\nocclusions are estimated by relying on a new measure of confidence. We\nvalidated our approach over an extensive dataset of egocentric photo-streams\nand compared it to state of the art methods, demonstrating its effectiveness\nand robustness.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2015 13:51:47 GMT"}, {"version": "v2", "created": "Wed, 13 Jan 2016 12:26:09 GMT"}], "update_date": "2017-01-24", "authors_parsed": [["Aghaei", "Maedeh", ""], ["Dimiccoli", "Mariella", ""], ["Radeva", "Petia", ""]]}, {"id": "1507.04760", "submitter": "Lex Fridman", "authors": "Lex Fridman, Philipp Langhans, Joonbum Lee, Bryan Reimer", "title": "Driver Gaze Region Estimation Without Using Eye Movement", "comments": "Accepted for Publication in IEEE Intelligent Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated estimation of the allocation of a driver's visual attention may be\na critical component of future Advanced Driver Assistance Systems. In theory,\nvision-based tracking of the eye can provide a good estimate of gaze location.\nIn practice, eye tracking from video is challenging because of sunglasses,\neyeglass reflections, lighting conditions, occlusions, motion blur, and other\nfactors. Estimation of head pose, on the other hand, is robust to many of these\neffects, but cannot provide as fine-grained of a resolution in localizing the\ngaze. However, for the purpose of keeping the driver safe, it is sufficient to\npartition gaze into regions. In this effort, we propose a system that extracts\nfacial features and classifies their spatial configuration into six regions in\nreal-time. Our proposed method achieves an average accuracy of 91.4% at an\naverage decision rate of 11 Hz on a dataset of 50 drivers from an on-road\nstudy.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2015 20:16:20 GMT"}, {"version": "v2", "created": "Tue, 1 Mar 2016 17:21:25 GMT"}], "update_date": "2016-03-02", "authors_parsed": [["Fridman", "Lex", ""], ["Langhans", "Philipp", ""], ["Lee", "Joonbum", ""], ["Reimer", "Bryan", ""]]}, {"id": "1507.04816", "submitter": "Thanh The Van", "authors": "Thanh The Van and Thanh Manh Le", "title": "RBIR Based on Signature Graph", "comments": "4 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper approaches the image retrieval system on the base of visual\nfeatures local region RBIR (region-based image retrieval). First of all, the\npaper presents a method for extracting the interest points based on\nHarris-Laplace to create the feature region of the image. Next, in order to\nreduce the storage space and speed up query image, the paper builds the binary\nsignature structure to describe the visual content of image. Based on the\nimage's binary signature, the paper builds the SG (signature graph) to classify\nand store image's binary signatures. Since then, the paper builds the image\nretrieval algorithm on SG through the similar measure EMD (earth mover's\ndistance) between the image's binary signatures. Last but not least, the paper\ngives an image retrieval model RBIR, experiments and assesses the image\nretrieval method on Corel image database over 10,000 images.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2015 01:56:44 GMT"}], "update_date": "2015-07-20", "authors_parsed": [["Van", "Thanh The", ""], ["Le", "Thanh Manh", ""]]}, {"id": "1507.04831", "submitter": "Yongtao Hu", "authors": "Yongtao Hu, Jimmy Ren, Jingwen Dai, Chang Yuan, Li Xu, and Wenping\n  Wang", "title": "Deep Multimodal Speaker Naming", "comments": null, "journal-ref": null, "doi": "10.1145/2733373.2806293", "report-no": null, "categories": "cs.CV cs.LG cs.MM cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic speaker naming is the problem of localizing as well as identifying\neach speaking character in a TV/movie/live show video. This is a challenging\nproblem mainly attributes to its multimodal nature, namely face cue alone is\ninsufficient to achieve good performance. Previous multimodal approaches to\nthis problem usually process the data of different modalities individually and\nmerge them using handcrafted heuristics. Such approaches work well for simple\nscenes, but fail to achieve high performance for speakers with large appearance\nvariations. In this paper, we propose a novel convolutional neural networks\n(CNN) based learning framework to automatically learn the fusion function of\nboth face and audio cues. We show that without using face tracking, facial\nlandmark localization or subtitle/transcript, our system with robust multimodal\nfeature extraction is able to achieve state-of-the-art speaker naming\nperformance evaluated on two diverse TV series. The dataset and implementation\nof our algorithm are publicly available online.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2015 04:13:12 GMT"}], "update_date": "2015-07-20", "authors_parsed": [["Hu", "Yongtao", ""], ["Ren", "Jimmy", ""], ["Dai", "Jingwen", ""], ["Yuan", "Chang", ""], ["Xu", "Li", ""], ["Wang", "Wenping", ""]]}, {"id": "1507.04835", "submitter": "Cheng Tai", "authors": "Cheng Tai, Weinan E", "title": "Multiscale Adaptive Representation of Signals: I. The Basic Framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a framework for designing multi-scale, adaptive, shift-invariant\nframes and bi-frames for representing signals. The new framework, called\nAdaFrame, improves over dictionary learning-based techniques in terms of\ncomputational efficiency at inference time. It improves classical multi-scale\nbasis such as wavelet frames in terms of coding efficiency. It provides an\nattractive alternative to dictionary learning-based techniques for low level\nsignal processing tasks, such as compression and denoising, as well as high\nlevel tasks, such as feature extraction for object recognition. Connections\nwith deep convolutional networks are also discussed. In particular, the\nproposed framework reveals a drawback in the commonly used approach for\nvisualizing the activations of the intermediate layers in convolutional\nnetworks, and suggests a natural alternative.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2015 05:24:45 GMT"}], "update_date": "2015-07-20", "authors_parsed": [["Tai", "Cheng", ""], ["E", "Weinan", ""]]}, {"id": "1507.04844", "submitter": "Xiang Wu", "authors": "Xiang Wu", "title": "Learning Robust Deep Face Representation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the development of convolution neural network, more and more researchers\nfocus their attention on the advantage of CNN for face recognition task. In\nthis paper, we propose a deep convolution network for learning a robust face\nrepresentation. The deep convolution net is constructed by 4 convolution\nlayers, 4 max pooling layers and 2 fully connected layers, which totally\ncontains about 4M parameters. The Max-Feature-Map activation function is used\ninstead of ReLU because the ReLU might lead to the loss of information due to\nthe sparsity while the Max-Feature-Map can get the compact and discriminative\nfeature vectors. The model is trained on CASIA-WebFace dataset and evaluated on\nLFW dataset. The result on LFW achieves 97.77% on unsupervised setting for\nsingle net.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2015 06:21:31 GMT"}], "update_date": "2015-07-20", "authors_parsed": [["Wu", "Xiang", ""]]}, {"id": "1507.04908", "submitter": "Alessia Amelio Dr.", "authors": "Darko Brodic and Zoran N. Milivojevic and Alessia Amelio", "title": "Analysis of the South Slavic Scripts by Run-Length Features of the Image\n  Texture", "comments": "9 pages, 9 figures, In Electronics 2015, Elektronika IR\n  Elektrotechnika, ISSN 1392-1215 (in press)", "journal-ref": "Elektronika Ir Elektrotechnika, ISSN 1392-1215, VOL. 21, NO. 4,\n  2015", "doi": "10.5755/j01.eee.21.4.12785", "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper proposes an algorithm for the script recognition based on the\ntexture characteristics. The image texture is achieved by coding each letter\nwith the equivalent script type (number code) according to its position in the\ntext line. Each code is transformed into equivalent gray level pixel creating\nan 1-D image. Then, the image texture is subjected to the run-length analysis.\nThis analysis extracts the run-length features, which are classified to make a\ndistinction between the scripts under consideration. In the experiment, a\ncustom oriented database is subject to the proposed algorithm. The database\nconsists of some text documents written in Cyrillic, Latin and Glagolitic\nscripts. Furthermore, it is divided into training and test parts. The results\nof the experiment show that 3 out of 5 run-length features can be used for\neffective differentiation between the analyzed South Slavic scripts.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2015 10:34:23 GMT"}], "update_date": "2015-09-01", "authors_parsed": [["Brodic", "Darko", ""], ["Milivojevic", "Zoran N.", ""], ["Amelio", "Alessia", ""]]}, {"id": "1507.04913", "submitter": "Weiyao Lin", "authors": "Xintong Han, Chongyang Zhang, Weiyao Lin, Mingliang Xu, Bin Sheng, Tao\n  Mei", "title": "Tree-based Visualization and Optimization for Image Collection", "comments": "This manuscript is the accepted version for T-CYB (IEEE Transactions\n  on Cybernetics) IEEE Trans. Cybernetics, 2015", "journal-ref": null, "doi": "10.1109/TCYB.2015.2448236", "report-no": null, "categories": "cs.MM cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The visualization of an image collection is the process of displaying a\ncollection of images on a screen under some specific layout requirements. This\npaper focuses on an important problem that is not well addressed by the\nprevious methods: visualizing image collections into arbitrary layout shapes\nwhile arranging images according to user-defined semantic or visual\ncorrelations (e.g., color or object category). To this end, we first propose a\nproperty-based tree construction scheme to organize images of a collection into\na tree structure according to user-defined properties. In this way, images can\nbe adaptively placed with the desired semantic or visual correlations in the\nfinal visualization layout. Then, we design a two-step visualization\noptimization scheme to further optimize image layouts. As a result, multiple\nlayout effects including layout shape and image overlap ratio can be\neffectively controlled to guarantee a satisfactory visualization. Finally, we\nalso propose a tree-transfer scheme such that visualization layouts can be\nadaptively changed when users select different \"images of interest\". We\ndemonstrate the effectiveness of our proposed approach through the comparisons\nwith state-of-the-art visualization techniques.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2015 10:45:26 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Han", "Xintong", ""], ["Zhang", "Chongyang", ""], ["Lin", "Weiyao", ""], ["Xu", "Mingliang", ""], ["Sheng", "Bin", ""], ["Mei", "Tao", ""]]}, {"id": "1507.05033", "submitter": "Alejandro Frery", "authors": "Luis Gomez and Luis Alvarez and Luis Mazorra and Alejandro C. Frery", "title": "Classification of Complex Wishart Matrices with a Diffusion-Reaction\n  System guided by Stochastic Distances", "comments": "Accepted for publication in Philosophical Transactions A", "journal-ref": null, "doi": "10.1098/rsta.2015.0118", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new method for PolSAR (Polarimetric Synthetic Aperture Radar)\nimagery classification based on stochastic distances in the space of random\nmatrices obeying complex Wishart distributions. Given a collection of\nprototypes $\\{Z_m\\}_{m=1}^M$ and a stochastic distance $d(.,.)$, we classify\nany random matrix $X$ using two criteria in an iterative setup. Firstly, we\nassociate $X$ to the class which minimizes the weighted stochastic distance\n$w_md(X,Z_m)$, where the positive weights $w_m$ are computed to maximize the\nclass discrimination power. Secondly, we improve the result by embedding the\nclassification problem into a diffusion-reaction partial differential system\nwhere the diffusion term smooths the patches within the image, and the reaction\nterm tends to move the pixel values towards the closest class prototype. In\nparticular, the method inherits the benefits of speckle reduction by\ndiffusion-like methods. Results on synthetic and real PolSAR data show the\nperformance of the method.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2015 17:10:47 GMT"}], "update_date": "2016-02-17", "authors_parsed": [["Gomez", "Luis", ""], ["Alvarez", "Luis", ""], ["Mazorra", "Luis", ""], ["Frery", "Alejandro C.", ""]]}, {"id": "1507.05053", "submitter": "Keiron O'Shea Mr", "authors": "Keiron O'Shea", "title": "Massively Deep Artificial Neural Networks for Handwritten Digit\n  Recognition", "comments": "2 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Greedy Restrictive Boltzmann Machines yield an fairly low 0.72% error rate on\nthe famous MNIST database of handwritten digits. All that was required to\nachieve this result was a high number of hidden layers consisting of many\nneurons, and a graphics card to greatly speed up the rate of learning.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2015 17:48:49 GMT"}], "update_date": "2015-07-20", "authors_parsed": [["O'Shea", "Keiron", ""]]}, {"id": "1507.05243", "submitter": "Jayati Ghosh Dastidar", "authors": "Jonathan Fidelis Paul, Dibyabiva Seth, Cijo Paul, Jayati Ghosh\n  Dastidar", "title": "Hand Gesture Recognition Library", "comments": null, "journal-ref": "International Journal of Science and Applied Information\n  Technology, Volume 3, No.2, March - April 2014", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we have presented a hand gesture recognition library. Various\nfunctions include detecting cluster count, cluster orientation, finger pointing\ndirection, etc. To use these functions first the input image needs to be\nprocessed into a logical array for which a function has been developed. The\nlibrary has been developed keeping flexibility in mind and thus provides\napplication developers a wide range of options to develop custom gestures.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jul 2015 03:10:28 GMT"}], "update_date": "2015-07-21", "authors_parsed": [["Paul", "Jonathan Fidelis", ""], ["Seth", "Dibyabiva", ""], ["Paul", "Cijo", ""], ["Dastidar", "Jayati Ghosh", ""]]}, {"id": "1507.05244", "submitter": "Jayati Ghosh Dastidar", "authors": "Jayati Ghosh Dastidar, Surabhi Sarkar, Rick Punyadyuti Sinha, Kasturi\n  Basu", "title": "Handwriting Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the method to recognize offline handwritten characters.\nA robust algorithm for handwriting segmentation is described here with the help\nof which individual characters can be segmented from a selected word from a\nparagraph of handwritten text image which is given as input.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jul 2015 03:14:56 GMT"}], "update_date": "2015-07-21", "authors_parsed": [["Dastidar", "Jayati Ghosh", ""], ["Sarkar", "Surabhi", ""], ["Sinha", "Rick Punyadyuti", ""], ["Basu", "Kasturi", ""]]}, {"id": "1507.05348", "submitter": "Zhaowei Cai", "authors": "Zhaowei Cai, Mohammad Saberian, Nuno Vasconcelos", "title": "Learning Complexity-Aware Cascades for Deep Pedestrian Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The design of complexity-aware cascaded detectors, combining features of very\ndifferent complexities, is considered. A new cascade design procedure is\nintroduced, by formulating cascade learning as the Lagrangian optimization of a\nrisk that accounts for both accuracy and complexity. A boosting algorithm,\ndenoted as complexity aware cascade training (CompACT), is then derived to\nsolve this optimization. CompACT cascades are shown to seek an optimal\ntrade-off between accuracy and complexity by pushing features of higher\ncomplexity to the later cascade stages, where only a few difficult candidate\npatches remain to be classified. This enables the use of features of vastly\ndifferent complexities in a single detector. In result, the feature pool can be\nexpanded to features previously impractical for cascade design, such as the\nresponses of a deep convolutional neural network (CNN). This is demonstrated\nthrough the design of a pedestrian detector with a pool of features whose\ncomplexities span orders of magnitude. The resulting cascade generalizes the\ncombination of a CNN with an object proposal mechanism: rather than a\npre-processing stage, CompACT cascades seamlessly integrate CNNs in their\nstages. This enables state of the art performance on the Caltech and KITTI\ndatasets, at fairly fast speeds.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jul 2015 22:31:01 GMT"}], "update_date": "2015-07-21", "authors_parsed": [["Cai", "Zhaowei", ""], ["Saberian", "Mohammad", ""], ["Vasconcelos", "Nuno", ""]]}, {"id": "1507.05409", "submitter": "Bhaskar Mukhoty", "authors": "Bhaskar Mukhoty, Ruchir Gupta and Y. N. Singh", "title": "A Parameter-free Affinity Based Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several methods have been proposed to estimate the number of clusters in a\ndataset; the basic ideal behind all of them has been to study an index that\nmeasures inter-cluster separation and intra-cluster cohesion over a range of\ncluster numbers and report the number which gives an optimum value of the\nindex. In this paper we propose a simple, parameter free approach that is like\nhuman cognition to form clusters, where closely lying points are easily\nidentified to form a cluster and total number of clusters are revealed. To\nidentify closely lying points, affinity of two points is defined as a function\nof distance and a threshold affinity is identified, above which two points in a\ndataset are likely to be in the same cluster. Well separated clusters are\nidentified even in the presence of outliers, whereas for not so well separated\ndataset, final number of clusters are estimated and the detected clusters are\nmerged to produce the final clusters. Experiments performed with several large\ndimensional synthetic and real datasets show good results with robustness to\nnoise and density variation within dataset.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2015 07:59:17 GMT"}, {"version": "v2", "created": "Mon, 11 Jan 2016 10:24:38 GMT"}], "update_date": "2016-01-12", "authors_parsed": [["Mukhoty", "Bhaskar", ""], ["Gupta", "Ruchir", ""], ["Singh", "Y. N.", ""]]}, {"id": "1507.05489", "submitter": "Andrea Romanoni", "authors": "Andrea Romanoni, Matteo Matteucci", "title": "Efficient moving point handling for incremental 3D manifold\n  reconstruction", "comments": "Accepted in International Conference on Image Analysis and Processing\n  (ICIAP 2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As incremental Structure from Motion algorithms become effective, a good\nsparse point cloud representing the map of the scene becomes available\nframe-by-frame. From the 3D Delaunay triangulation of these points,\nstate-of-the-art algorithms build a manifold rough model of the scene. These\nalgorithms integrate incrementally new points to the 3D reconstruction only if\ntheir position estimate does not change. Indeed, whenever a point moves in a 3D\nDelaunay triangulation, for instance because its estimation gets refined, a set\nof tetrahedra have to be removed and replaced with new ones to maintain the\nDelaunay property; the management of the manifold reconstruction becomes thus\ncomplex and it entails a potentially big overhead. In this paper we investigate\ndifferent approaches and we propose an efficient policy to deal with moving\npoints in the manifold estimation process. We tested our approach with four\nsequences of the KITTI dataset and we show the effectiveness of our proposal in\ncomparison with state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2015 13:38:02 GMT"}], "update_date": "2015-07-21", "authors_parsed": [["Romanoni", "Andrea", ""], ["Matteucci", "Matteo", ""]]}, {"id": "1507.05532", "submitter": "Hongyu Miao", "authors": "Na Lu, Hongyu Miao", "title": "Clustering Tree-structured Data on Manifold", "comments": "14 pages, 7 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tree-structured data usually contain both topological and geometrical\ninformation, and are necessarily considered on manifold instead of Euclidean\nspace for appropriate data parameterization and analysis. In this study, we\npropose a novel tree-structured data parameterization, called\nTopology-Attribute matrix (T-A matrix), so the data clustering task can be\nconducted on matrix manifold. We incorporate the structure constraints embedded\nin data into the negative matrix factorization method to determine meta-trees\nfrom the T-A matrix, and the signature vector of each single tree can then be\nextracted by meta-tree decomposition. The meta-tree space turns out to be a\ncone space, in which we explore the distance metric and implement the\nclustering algorithm based on the concepts like Fr\\'echet mean. Finally, the\nT-A matrix based clustering (TAMBAC) framework is evaluated and compared using\nboth simulated data and real retinal images to illustrate its efficiency and\naccuracy.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2015 15:23:00 GMT"}, {"version": "v2", "created": "Thu, 10 Sep 2015 02:32:49 GMT"}], "update_date": "2015-09-11", "authors_parsed": [["Lu", "Na", ""], ["Miao", "Hongyu", ""]]}, {"id": "1507.05578", "submitter": "Anant Raj", "authors": "Anant Raj, Vinay P. Namboodiri and Tinne Tuytelaars", "title": "Subspace Alignment Based Domain Adaptation for RCNN Detector", "comments": "26th British Machine Vision Conference, Swansea, UK", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose subspace alignment based domain adaptation of the\nstate of the art RCNN based object detector. The aim is to be able to achieve\nhigh quality object detection in novel, real world target scenarios without\nrequiring labels from the target domain. While, unsupervised domain adaptation\nhas been studied in the case of object classification, for object detection it\nhas been relatively unexplored. In subspace based domain adaptation for\nobjects, we need access to source and target subspaces for the bounding box\nfeatures. The absence of supervision (labels and bounding boxes are absent)\nmakes the task challenging. In this paper, we show that we can still adapt sub-\nspaces that are localized to the object by obtaining detections from the RCNN\ndetector trained on source and applied on target. Then we form localized\nsubspaces from the detections and show that subspace alignment based adaptation\nbetween these subspaces yields improved object detection. This evaluation is\ndone by considering challenging real world datasets of PASCAL VOC as source and\nvalidation set of Microsoft COCO dataset as target for various categories.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2015 18:23:54 GMT"}], "update_date": "2015-07-21", "authors_parsed": [["Raj", "Anant", ""], ["Namboodiri", "Vinay P.", ""], ["Tuytelaars", "Tinne", ""]]}, {"id": "1507.05670", "submitter": "Yuke Zhu", "authors": "Yuke Zhu, Ce Zhang, Christopher R\\'e and Li Fei-Fei", "title": "Building a Large-scale Multimodal Knowledge Base System for Answering\n  Visual Queries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The complexity of the visual world creates significant challenges for\ncomprehensive visual understanding. In spite of recent successes in visual\nrecognition, today's vision systems would still struggle to deal with visual\nqueries that require a deeper reasoning. We propose a knowledge base (KB)\nframework to handle an assortment of visual queries, without the need to train\nnew classifiers for new tasks. Building such a large-scale multimodal KB\npresents a major challenge of scalability. We cast a large-scale MRF into a KB\nrepresentation, incorporating visual, textual and structured data, as well as\ntheir diverse relations. We introduce a scalable knowledge base construction\nsystem that is capable of building a KB with half billion variables and\nmillions of parameters in a few hours. Our system achieves competitive results\ncompared to purpose-built models on standard recognition and retrieval tasks,\nwhile exhibiting greater flexibility in answering richer visual queries.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2015 22:43:51 GMT"}, {"version": "v2", "created": "Mon, 9 Nov 2015 22:52:22 GMT"}], "update_date": "2015-11-11", "authors_parsed": [["Zhu", "Yuke", ""], ["Zhang", "Ce", ""], ["R\u00e9", "Christopher", ""], ["Fei-Fei", "Li", ""]]}, {"id": "1507.05699", "submitter": "Peiyun Hu", "authors": "Peiyun Hu, Deva Ramanan", "title": "Bottom-Up and Top-Down Reasoning with Hierarchical Rectified Gaussians", "comments": "To appear in CVPR 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural nets (CNNs) have demonstrated remarkable performance in\nrecent history. Such approaches tend to work in a unidirectional bottom-up\nfeed-forward fashion. However, practical experience and biological evidence\ntells us that feedback plays a crucial role, particularly for detailed spatial\nunderstanding tasks. This work explores bidirectional architectures that also\nreason with top-down feedback: neural units are influenced by both lower and\nhigher-level units.\n  We do so by treating units as rectified latent variables in a quadratic\nenergy function, which can be seen as a hierarchical Rectified Gaussian model\n(RGs). We show that RGs can be optimized with a quadratic program (QP), that\ncan in turn be optimized with a recurrent neural network (with rectified linear\nunits). This allows RGs to be trained with GPU-optimized gradient descent. From\na theoretical perspective, RGs help establish a connection between CNNs and\nhierarchical probabilistic models. From a practical perspective, RGs are well\nsuited for detailed spatial tasks that can benefit from top-down reasoning. We\nillustrate them on the challenging task of keypoint localization under\nocclusions, where local bottom-up evidence may be misleading. We demonstrate\nstate-of-the-art results on challenging benchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2015 04:00:44 GMT"}, {"version": "v2", "created": "Wed, 22 Jul 2015 05:18:01 GMT"}, {"version": "v3", "created": "Sun, 2 Aug 2015 04:44:09 GMT"}, {"version": "v4", "created": "Tue, 3 May 2016 05:50:39 GMT"}, {"version": "v5", "created": "Wed, 4 May 2016 22:50:35 GMT"}], "update_date": "2016-05-06", "authors_parsed": [["Hu", "Peiyun", ""], ["Ramanan", "Deva", ""]]}, {"id": "1507.05717", "submitter": "Cong Yao", "authors": "Baoguang Shi and Xiang Bai and Cong Yao", "title": "An End-to-End Trainable Neural Network for Image-based Sequence\n  Recognition and Its Application to Scene Text Recognition", "comments": "5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image-based sequence recognition has been a long-standing research topic in\ncomputer vision. In this paper, we investigate the problem of scene text\nrecognition, which is among the most important and challenging tasks in\nimage-based sequence recognition. A novel neural network architecture, which\nintegrates feature extraction, sequence modeling and transcription into a\nunified framework, is proposed. Compared with previous systems for scene text\nrecognition, the proposed architecture possesses four distinctive properties:\n(1) It is end-to-end trainable, in contrast to most of the existing algorithms\nwhose components are separately trained and tuned. (2) It naturally handles\nsequences in arbitrary lengths, involving no character segmentation or\nhorizontal scale normalization. (3) It is not confined to any predefined\nlexicon and achieves remarkable performances in both lexicon-free and\nlexicon-based scene text recognition tasks. (4) It generates an effective yet\nmuch smaller model, which is more practical for real-world application\nscenarios. The experiments on standard benchmarks, including the IIIT-5K,\nStreet View Text and ICDAR datasets, demonstrate the superiority of the\nproposed algorithm over the prior arts. Moreover, the proposed algorithm\nperforms well in the task of image-based music score recognition, which\nevidently verifies the generality of it.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2015 06:26:32 GMT"}], "update_date": "2015-07-22", "authors_parsed": [["Shi", "Baoguang", ""], ["Bai", "Xiang", ""], ["Yao", "Cong", ""]]}, {"id": "1507.05726", "submitter": "Aaron Wetzler", "authors": "Aaron Wetzler, Ron Slossberg, Ron Kimmel", "title": "Rule Of Thumb: Deep derotation for improved fingertip detection", "comments": "To be published in proceedings of BMVC 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate a novel global orientation regression approach for articulated\nobjects using a deep convolutional neural network. This is integrated with an\nin-plane image derotation scheme, DeROT, to tackle the problem of per-frame\nfingertip detection in depth images. The method reduces the complexity of\nlearning in the space of articulated poses which is demonstrated by using two\ndistinct state-of-the-art learning based hand pose estimation methods applied\nto fingertip detection. Significant classification improvements are shown over\nthe baseline implementation. Our framework involves no tracking, kinematic\nconstraints or explicit prior model of the articulated object in hand. To\nsupport our approach we also describe a new pipeline for high accuracy magnetic\nannotation and labeling of objects imaged by a depth camera.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2015 07:23:45 GMT"}], "update_date": "2015-07-22", "authors_parsed": [["Wetzler", "Aaron", ""], ["Slossberg", "Ron", ""], ["Kimmel", "Ron", ""]]}, {"id": "1507.05737", "submitter": "Chunhua Shen", "authors": "Xi Li, Chunhua Shen, Anthony Dick, Zhongfei Zhang, Yueting Zhuang", "title": "Online Metric-Weighted Linear Representations for Robust Visual Tracking", "comments": "51 pages. Appearing in IEEE Transactions on Pattern Analysis and\n  Machine Intelligence", "journal-ref": null, "doi": "10.1109/TPAMI.2015.2469276", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a visual tracker based on a metric-weighted linear\nrepresentation of appearance. In order to capture the interdependence of\ndifferent feature dimensions, we develop two online distance metric learning\nmethods using proximity comparison information and structured output learning.\nThe learned metric is then incorporated into a linear representation of\nappearance.\n  We show that online distance metric learning significantly improves the\nrobustness of the tracker, especially on those sequences exhibiting drastic\nappearance changes. In order to bound growth in the number of training samples,\nwe design a time-weighted reservoir sampling method.\n  Moreover, we enable our tracker to automatically perform object\nidentification during the process of object tracking, by introducing a\ncollection of static template samples belonging to several object classes of\ninterest. Object identification results for an entire video sequence are\nachieved by systematically combining the tracking information and visual\nrecognition at each frame. Experimental results on challenging video sequences\ndemonstrate the effectiveness of the method for both inter-frame tracking and\nobject identification.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2015 08:02:36 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Li", "Xi", ""], ["Shen", "Chunhua", ""], ["Dick", "Anthony", ""], ["Zhang", "Zhongfei", ""], ["Zhuang", "Yueting", ""]]}, {"id": "1507.05738", "submitter": "Serena Yeung", "authors": "Serena Yeung, Olga Russakovsky, Ning Jin, Mykhaylo Andriluka, Greg\n  Mori, Li Fei-Fei", "title": "Every Moment Counts: Dense Detailed Labeling of Actions in Complex\n  Videos", "comments": "To appear in IJCV", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Every moment counts in action recognition. A comprehensive understanding of\nhuman activity in video requires labeling every frame according to the actions\noccurring, placing multiple labels densely over a video sequence. To study this\nproblem we extend the existing THUMOS dataset and introduce MultiTHUMOS, a new\ndataset of dense labels over unconstrained internet videos. Modeling multiple,\ndense labels benefits from temporal relations within and across classes. We\ndefine a novel variant of long short-term memory (LSTM) deep networks for\nmodeling these temporal relations via multiple input and output connections. We\nshow that this model improves action labeling accuracy and further enables\ndeeper understanding tasks ranging from structured retrieval to action\nprediction.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2015 08:07:50 GMT"}, {"version": "v2", "created": "Fri, 31 Jul 2015 22:09:30 GMT"}, {"version": "v3", "created": "Fri, 9 Jun 2017 10:42:09 GMT"}], "update_date": "2017-06-12", "authors_parsed": [["Yeung", "Serena", ""], ["Russakovsky", "Olga", ""], ["Jin", "Ning", ""], ["Andriluka", "Mykhaylo", ""], ["Mori", "Greg", ""], ["Fei-Fei", "Li", ""]]}, {"id": "1507.05775", "submitter": "Shuchang Zhou", "authors": "Shuchang Zhou, Jia-Nan Wu", "title": "Compression of Fully-Connected Layer in Neural Network by Kronecker\n  Product", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose and study a technique to reduce the number of\nparameters and computation time in fully-connected layers of neural networks\nusing Kronecker product, at a mild cost of the prediction quality. The\ntechnique proceeds by replacing Fully-Connected layers with so-called Kronecker\nFully-Connected layers, where the weight matrices of the FC layers are\napproximated by linear combinations of multiple Kronecker products of smaller\nmatrices. In particular, given a model trained on SVHN dataset, we are able to\nconstruct a new KFC model with 73\\% reduction in total number of parameters,\nwhile the error only rises mildly. In contrast, using low-rank method can only\nachieve 35\\% reduction in total number of parameters given similar quality\ndegradation allowance. If we only compare the KFC layer with its counterpart\nfully-connected layer, the reduction in the number of parameters exceeds 99\\%.\nThe amount of computation is also reduced as we replace matrix product of the\nlarge matrices in FC layers with matrix products of a few smaller matrices in\nKFC layers. Further experiments on MNIST, SVHN and some Chinese Character\nrecognition models also demonstrate effectiveness of our technique.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2015 10:29:11 GMT"}, {"version": "v2", "created": "Wed, 22 Jul 2015 11:59:08 GMT"}], "update_date": "2015-07-23", "authors_parsed": [["Zhou", "Shuchang", ""], ["Wu", "Jia-Nan", ""]]}, {"id": "1507.05936", "submitter": "Se Rim Park", "authors": "Se Rim Park, Soheil Kolouri, Shinjini Kundu, Gustavo Rohde", "title": "The Cumulative Distribution Transform and Linear Pattern Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discriminating data classes emanating from sensors is an important problem\nwith many applications in science and technology. We describe a new transform\nfor pattern identification that interprets patterns as probability density\nfunctions, and has special properties with regards to classification. The\ntransform, which we denote as the Cumulative Distribution Transform (CDT) is\ninvertible, with well defined forward and inverse operations. We show that it\ncan be useful in `parsing out' variations (confounds) that are `Lagrangian'\n(displacement and intensity variations) by converting these to `Eulerian'\n(intensity variations) in transform space. This conversion is the basis for our\nmain result that describes when the CDT can allow for linear classification to\nbe possible in transform space. We also describe several properties of the\ntransform and show, with computational experiments that used both real and\nsimulated data, that the CDT can help render a variety of real world problems\nsimpler to solve.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2015 18:19:31 GMT"}, {"version": "v2", "created": "Mon, 13 Feb 2017 17:44:51 GMT"}, {"version": "v3", "created": "Tue, 14 Feb 2017 17:17:10 GMT"}], "update_date": "2017-02-15", "authors_parsed": [["Park", "Se Rim", ""], ["Kolouri", "Soheil", ""], ["Kundu", "Shinjini", ""], ["Rohde", "Gustavo", ""]]}, {"id": "1507.06105", "submitter": "Jianyuan Sun", "authors": "Jianyuan Sun and Guoqiang Zhong and Junyu Dong and Yajuan Cai", "title": "Banzhaf Random Forests", "comments": "arXiv admin note: text overlap with arXiv:1302.4853 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random forests are a type of ensemble method which makes predictions by\ncombining the results of several independent trees. However, the theory of\nrandom forests has long been outpaced by their application. In this paper, we\npropose a novel random forests algorithm based on cooperative game theory.\nBanzhaf power index is employed to evaluate the power of each feature by\ntraversing possible feature coalitions. Unlike the previously used information\ngain rate of information theory, which simply chooses the most informative\nfeature, the Banzhaf power index can be considered as a metric of the\nimportance of each feature on the dependency among a group of features. More\nimportantly, we have proved the consistency of the proposed algorithm, named\nBanzhaf random forests (BRF). This theoretical analysis takes a step towards\nnarrowing the gap between the theory and practice of random forests for\nclassification problems. Experiments on several UCI benchmark data sets show\nthat BRF is competitive with state-of-the-art classifiers and dramatically\noutperforms previous consistent random forests. Particularly, it is much more\nefficient than previous consistent random forests.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2015 09:10:15 GMT"}], "update_date": "2015-07-23", "authors_parsed": [["Sun", "Jianyuan", ""], ["Zhong", "Guoqiang", ""], ["Dong", "Junyu", ""], ["Cai", "Yajuan", ""]]}, {"id": "1507.06120", "submitter": "Marc Bola\\~nos", "authors": "Marc Bola\\~nos, Mariella Dimiccoli and Petia Radeva", "title": "Towards Storytelling from Visual Lifelogging: An Overview", "comments": "16 pages, 11 figures, Submitted to IEEE Transactions on Human-Machine\n  Systems", "journal-ref": null, "doi": "10.1109/THMS.2016.2616296", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual lifelogging consists of acquiring images that capture the daily\nexperiences of the user by wearing a camera over a long period of time. The\npictures taken offer considerable potential for knowledge mining concerning how\npeople live their lives, hence, they open up new opportunities for many\npotential applications in fields including healthcare, security, leisure and\nthe quantified self. However, automatically building a story from a huge\ncollection of unstructured egocentric data presents major challenges. This\npaper provides a thorough review of advances made so far in egocentric data\nanalysis, and in view of the current state of the art, indicates new lines of\nresearch to move us towards storytelling from visual lifelogging.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2015 10:23:50 GMT"}, {"version": "v2", "created": "Wed, 16 Dec 2015 08:00:41 GMT"}, {"version": "v3", "created": "Tue, 5 Apr 2016 09:47:07 GMT"}, {"version": "v4", "created": "Fri, 20 May 2016 10:36:14 GMT"}, {"version": "v5", "created": "Wed, 20 Jul 2016 07:38:01 GMT"}], "update_date": "2016-11-03", "authors_parsed": [["Bola\u00f1os", "Marc", ""], ["Dimiccoli", "Mariella", ""], ["Radeva", "Petia", ""]]}, {"id": "1507.06149", "submitter": "Suraj Srinivas", "authors": "Suraj Srinivas, R. Venkatesh Babu", "title": "Data-free parameter pruning for Deep Neural Networks", "comments": "BMVC 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural nets (NNs) with millions of parameters are at the heart of many\nstate-of-the-art computer vision systems today. However, recent works have\nshown that much smaller models can achieve similar levels of performance. In\nthis work, we address the problem of pruning parameters in a trained NN model.\nInstead of removing individual weights one at a time as done in previous works,\nwe remove one neuron at a time. We show how similar neurons are redundant, and\npropose a systematic way to remove them. Our experiments in pruning the densely\nconnected layers show that we can remove upto 85\\% of the total parameters in\nan MNIST-trained network, and about 35\\% for AlexNet without significantly\naffecting performance. Our method can be applied on top of most networks with a\nfully connected layer to give a smaller network.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2015 12:18:21 GMT"}], "update_date": "2015-07-23", "authors_parsed": [["Srinivas", "Suraj", ""], ["Babu", "R. Venkatesh", ""]]}, {"id": "1507.06173", "submitter": "Amit Adam", "authors": "Amit Adam, Christoph Dann, Omer Yair, Shai Mazor, Sebastian Nowozin", "title": "Bayesian Time-of-Flight for Realtime Shape, Illumination and Albedo", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a computational model for shape, illumination and albedo inference\nin a pulsed time-of-flight (TOF) camera. In contrast to TOF cameras based on\nphase modulation, our camera enables general exposure profiles. This results in\nadded flexibility and requires novel computational approaches.\n  To address this challenge we propose a generative probabilistic model that\naccurately relates latent imaging conditions to observed camera responses.\nWhile principled, realtime inference in the model turns out to be infeasible,\nand we propose to employ efficient non-parametric regression trees to\napproximate the model outputs. As a result we are able to provide, for each\npixel, at video frame rate, estimates and uncertainty for depth, effective\nalbedo, and ambient light intensity. These results we present are\nstate-of-the-art in depth imaging.\n  The flexibility of our approach allows us to easily enrich our generative\nmodel. We demonstrate that by extending the original single-path model to a\ntwo-path model, capable of describing some multipath effects. The new model is\nseamlessly integrated in the system at no additional computational cost.\n  Our work also addresses the important question of optimal exposure design in\npulsed TOF systems. Finally, for benchmark purposes and to obtain realistic\nempirical priors of multipath and insights into this phenomena, we propose a\nphysically accurate simulation of multipath phenomena.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2015 13:18:14 GMT"}], "update_date": "2015-07-23", "authors_parsed": [["Adam", "Amit", ""], ["Dann", "Christoph", ""], ["Yair", "Omer", ""], ["Mazor", "Shai", ""], ["Nowozin", "Sebastian", ""]]}, {"id": "1507.06266", "submitter": "Mariella Dimiccoli", "authors": "Mariella Dimiccoli, Jean-Pascal Jacob, Lionel Moisan", "title": "Particle detection and tracking in fluorescence time-lapse imaging: a\n  contrario approach", "comments": "Published in Journal of Machine Vision and Applications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a probabilistic approach for the detection and the\ntracking of particles in fluorescent time-lapse imaging. In the presence of a\nvery noised and poor-quality data, particles and trajectories can be\ncharacterized by an a contrario model, that estimates the probability of\nobserving the structures of interest in random data. This approach, first\nintroduced in the modeling of human visual perception and then successfully\napplied in many image processing tasks, leads to algorithms that neither\nrequire a previous learning stage, nor a tedious parameter tuning and are very\nrobust to noise. Comparative evaluations against a well-established baseline\nshow that the proposed approach outperforms the state of the art.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2015 17:43:54 GMT"}, {"version": "v2", "created": "Fri, 28 Aug 2015 10:30:47 GMT"}, {"version": "v3", "created": "Sat, 9 Jan 2016 12:41:56 GMT"}, {"version": "v4", "created": "Tue, 23 Feb 2016 12:21:10 GMT"}, {"version": "v5", "created": "Thu, 22 Dec 2016 13:05:45 GMT"}], "update_date": "2017-04-04", "authors_parsed": [["Dimiccoli", "Mariella", ""], ["Jacob", "Jean-Pascal", ""], ["Moisan", "Lionel", ""]]}, {"id": "1507.06332", "submitter": "Kevin Shih", "authors": "Kevin J. Shih, Arun Mallya, Saurabh Singh, Derek Hoiem", "title": "Part Localization using Multi-Proposal Consensus for Fine-Grained\n  Categorization", "comments": "BMVC 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a simple deep learning framework to simultaneously predict\nkeypoint locations and their respective visibilities and use those to achieve\nstate-of-the-art performance for fine-grained classification. We show that by\nconditioning the predictions on object proposals with sufficient image support,\nour method can do well without complicated spatial reasoning. Instead,\ninference methods with robustness to outliers, yield state-of-the-art for\nkeypoint localization. We demonstrate the effectiveness of our accurate\nkeypoint localization and visibility prediction on the fine-grained bird\nrecognition task with and without ground truth bird bounding boxes, and\noutperform existing state-of-the-art methods by over 2%.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2015 20:21:59 GMT"}], "update_date": "2015-07-24", "authors_parsed": [["Shih", "Kevin J.", ""], ["Mallya", "Arun", ""], ["Singh", "Saurabh", ""], ["Hoiem", "Derek", ""]]}, {"id": "1507.06397", "submitter": "Seyed Hamid Rezatofighi", "authors": "Seyed Hamid Rezatofighi, Stephen Gould, Ba Tuong Vo, Ba-Ngu Vo,\n  Katarina Mele, Richard Hartley", "title": "Multi-Target Tracking with Time-Varying Clutter Rate and Detection\n  Profile: Application to Time-lapse Cell Microscopy Sequences", "comments": null, "journal-ref": null, "doi": "10.1109/TMI.2015.2390647", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantitative analysis of the dynamics of tiny cellular and sub-cellular\nstructures, known as particles, in time-lapse cell microscopy sequences\nrequires the development of a reliable multi-target tracking method capable of\ntracking numerous similar targets in the presence of high levels of noise, high\ntarget density, complex motion patterns and intricate interactions. In this\npaper, we propose a framework for tracking these structures based on the random\nfinite set Bayesian filtering framework. We focus on challenging biological\napplications where image characteristics such as noise and background intensity\nchange during the acquisition process. Under these conditions, detection\nmethods usually fail to detect all particles and are often followed by missed\ndetections and many spurious measurements with unknown and time-varying rates.\nTo deal with this, we propose a bootstrap filter composed of an estimator and a\ntracker. The estimator adaptively estimates the required meta parameters for\nthe tracker such as clutter rate and the detection probability of the targets,\nwhile the tracker estimates the state of the targets. Our results show that the\nproposed approach can outperform state-of-the-art particle trackers on both\nsynthetic and real data in this regime.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2015 07:13:24 GMT"}], "update_date": "2015-07-24", "authors_parsed": [["Rezatofighi", "Seyed Hamid", ""], ["Gould", "Stephen", ""], ["Vo", "Ba Tuong", ""], ["Vo", "Ba-Ngu", ""], ["Mele", "Katarina", ""], ["Hartley", "Richard", ""]]}, {"id": "1507.06429", "submitter": "Albert Gordo", "authors": "Albert Gordo and Adrien Gaidon and Florent Perronnin", "title": "Deep Fishing: Gradient Features from Deep Nets", "comments": "To appear at BMVC 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Networks (ConvNets) have recently improved image recognition\nperformance thanks to end-to-end learning of deep feed-forward models from raw\npixels. Deep learning is a marked departure from the previous state of the art,\nthe Fisher Vector (FV), which relied on gradient-based encoding of local\nhand-crafted features. In this paper, we discuss a novel connection between\nthese two approaches. First, we show that one can derive gradient\nrepresentations from ConvNets in a similar fashion to the FV. Second, we show\nthat this gradient representation actually corresponds to a structured matrix\nthat allows for efficient similarity computation. We experimentally study the\nbenefits of transferring this representation over the outputs of ConvNet\nlayers, and find consistent improvements on the Pascal VOC 2007 and 2012\ndatasets.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2015 10:01:45 GMT"}], "update_date": "2015-07-24", "authors_parsed": [["Gordo", "Albert", ""], ["Gaidon", "Adrien", ""], ["Perronnin", "Florent", ""]]}, {"id": "1507.06504", "submitter": "Mariella Dimiccoli", "authors": "Jean-Pascal Jacob, Mariella Dimiccoli, Lionel Moisan", "title": "Active skeleton for bacteria modeling", "comments": "Published in Computer Methods in Biomechanics and Biomedical\n  Engineering: Imaging and Visualizationto appear in", "journal-ref": "Computer Methods in Biomechanics and Biomedical Engineering:\n  Imaging and Visualization, pp. 1-13, 2016", "doi": "10.1080/21681163.2015.1100099", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The investigation of spatio-temporal dynamics of bacterial cells and their\nmolecular components requires automated image analysis tools to track cell\nshape properties and molecular component locations inside the cells. In the\nstudy of bacteria aging, the molecular components of interest are protein\naggregates accumulated near bacteria boundaries. This particular location makes\nvery ambiguous the correspondence between aggregates and cells, since computing\naccurately bacteria boundaries in phase-contrast time-lapse imaging is a\nchallenging task. This paper proposes an active skeleton formulation for\nbacteria modeling which provides several advantages: an easy computation of\nshape properties (perimeter, length, thickness, orientation), an improved\nboundary accuracy in noisy images, and a natural bacteria-centered coordinate\nsystem that permits the intrinsic location of molecular components inside the\ncell. Starting from an initial skeleton estimate, the medial axis of the\nbacterium is obtained by minimizing an energy function which incorporates\nbacteria shape constraints. Experimental results on biological images and\ncomparative evaluation of the performances validate the proposed approach for\nmodeling cigar-shaped bacteria like Escherichia coli. The Image-J plugin of the\nproposed method can be found online at http://fluobactracker.inrialpes.fr.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2015 14:08:49 GMT"}, {"version": "v2", "created": "Wed, 19 Aug 2015 09:44:08 GMT"}, {"version": "v3", "created": "Mon, 26 Oct 2015 15:02:27 GMT"}, {"version": "v4", "created": "Thu, 22 Dec 2016 13:13:06 GMT"}], "update_date": "2016-12-23", "authors_parsed": [["Jacob", "Jean-Pascal", ""], ["Dimiccoli", "Mariella", ""], ["Moisan", "Lionel", ""]]}, {"id": "1507.06535", "submitter": "Alhussein Fawzi", "authors": "Alhussein Fawzi, Pascal Frossard", "title": "Manitest: Are classifiers really invariant?", "comments": "BMVC 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Invariance to geometric transformations is a highly desirable property of\nautomatic classifiers in many image recognition tasks. Nevertheless, it is\nunclear to which extent state-of-the-art classifiers are invariant to basic\ntransformations such as rotations and translations. This is mainly due to the\nlack of general methods that properly measure such an invariance. In this\npaper, we propose a rigorous and systematic approach for quantifying the\ninvariance to geometric transformations of any classifier. Our key idea is to\ncast the problem of assessing a classifier's invariance as the computation of\ngeodesics along the manifold of transformed images. We propose the Manitest\nmethod, built on the efficient Fast Marching algorithm to compute the\ninvariance of classifiers. Our new method quantifies in particular the\nimportance of data augmentation for learning invariance from data, and the\nincreased invariance of convolutional neural networks with depth. We foresee\nthat the proposed generic tool for measuring invariance to a large class of\ngeometric transformations and arbitrary classifiers will have many applications\nfor evaluating and comparing classifiers based on their invariance, and help\nimproving the invariance of existing classifiers.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2015 15:36:50 GMT"}], "update_date": "2015-07-24", "authors_parsed": [["Fawzi", "Alhussein", ""], ["Frossard", "Pascal", ""]]}, {"id": "1507.06550", "submitter": "Joao Carreira", "authors": "Joao Carreira, Pulkit Agrawal, Katerina Fragkiadaki, Jitendra Malik", "title": "Human Pose Estimation with Iterative Error Feedback", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hierarchical feature extractors such as Convolutional Networks (ConvNets)\nhave achieved impressive performance on a variety of classification tasks using\npurely feedforward processing. Feedforward architectures can learn rich\nrepresentations of the input space but do not explicitly model dependencies in\nthe output spaces, that are quite structured for tasks such as articulated\nhuman pose estimation or object segmentation. Here we propose a framework that\nexpands the expressive power of hierarchical feature extractors to encompass\nboth input and output spaces, by introducing top-down feedback. Instead of\ndirectly predicting the outputs in one go, we use a self-correcting model that\nprogressively changes an initial solution by feeding back error predictions, in\na process we call Iterative Error Feedback (IEF). IEF shows excellent\nperformance on the task of articulated pose estimation in the challenging MPII\nand LSP benchmarks, matching the state-of-the-art without requiring ground\ntruth scale annotation.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2015 16:20:57 GMT"}, {"version": "v2", "created": "Wed, 11 Nov 2015 12:37:48 GMT"}, {"version": "v3", "created": "Sun, 12 Jun 2016 19:10:55 GMT"}], "update_date": "2016-06-14", "authors_parsed": [["Carreira", "Joao", ""], ["Agrawal", "Pulkit", ""], ["Fragkiadaki", "Katerina", ""], ["Malik", "Jitendra", ""]]}, {"id": "1507.06617", "submitter": "Dario Prandi", "authors": "Amine Bohi, Dario Prandi, Vincente Guis, Fr\\'ed\\'eric Bouchara and\n  Jean-Paul Gauthier", "title": "Fourier descriptors based on the structure of the human primary visual\n  cortex with applications to object recognition", "comments": "16 pages, 5 figures -- Added acknowledgements", "journal-ref": null, "doi": "10.1007/s10851-016-0669-1", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a supervised object recognition method using new\nglobal features and inspired by the model of the human primary visual cortex V1\nas the semidiscrete roto-translation group $SE(2,N) = \\mathbb Z_N\\rtimes\n\\mathbb R^2$. The proposed technique is based on generalized Fourier\ndescriptors on the latter group, which are invariant to natural geometric\ntransformations (rotations, translations). These descriptors are then used to\nfeed an SVM classifier. We have tested our method against the COIL-100 image\ndatabase and the ORL face database, and compared it with other techniques based\non traditional descriptors, global and local. The obtained results have shown\nthat our approach looks extremely efficient and stable to noise, in presence of\nwhich it outperforms the other techniques analyzed in the paper.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2015 19:09:27 GMT"}, {"version": "v2", "created": "Fri, 4 Mar 2016 14:40:33 GMT"}, {"version": "v3", "created": "Mon, 23 May 2016 10:07:43 GMT"}, {"version": "v4", "created": "Tue, 28 Jun 2016 09:07:33 GMT"}], "update_date": "2019-02-14", "authors_parsed": [["Bohi", "Amine", ""], ["Prandi", "Dario", ""], ["Guis", "Vincente", ""], ["Bouchara", "Fr\u00e9d\u00e9ric", ""], ["Gauthier", "Jean-Paul", ""]]}, {"id": "1507.06821", "submitter": "Andreas Eitel", "authors": "Andreas Eitel, Jost Tobias Springenberg, Luciano Spinello, Martin\n  Riedmiller, Wolfram Burgard", "title": "Multimodal Deep Learning for Robust RGB-D Object Recognition", "comments": "Final version submitted to IROS'2015, results unchanged,\n  reformulation of some text passages in abstract and introduction", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust object recognition is a crucial ingredient of many, if not all,\nreal-world robotics applications. This paper leverages recent progress on\nConvolutional Neural Networks (CNNs) and proposes a novel RGB-D architecture\nfor object recognition. Our architecture is composed of two separate CNN\nprocessing streams - one for each modality - which are consecutively combined\nwith a late fusion network. We focus on learning with imperfect sensor data, a\ntypical problem in real-world robotics tasks. For accurate learning, we\nintroduce a multi-stage training methodology and two crucial ingredients for\nhandling depth data with CNNs. The first, an effective encoding of depth\ninformation for CNNs that enables learning without the need for large depth\ndatasets. The second, a data augmentation scheme for robust learning with depth\nimages by corrupting them with realistic noise patterns. We present\nstate-of-the-art results on the RGB-D object dataset and show recognition in\nchallenging RGB-D real-world noisy settings.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2015 12:20:19 GMT"}, {"version": "v2", "created": "Tue, 18 Aug 2015 13:04:29 GMT"}], "update_date": "2015-08-19", "authors_parsed": [["Eitel", "Andreas", ""], ["Springenberg", "Jost Tobias", ""], ["Spinello", "Luciano", ""], ["Riedmiller", "Martin", ""], ["Burgard", "Wolfram", ""]]}, {"id": "1507.06838", "submitter": "Modesto  Castrill\\'on-Santana", "authors": "M. Castrill\\'on-Santana, J. Lorenzo-Navarro and E. Ram\\'on-Balmaseda", "title": "Descriptors and regions of interest fusion for gender classification in\n  the wild. Comparison and combination with Convolutional Neural Networks", "comments": "Revised version containing 12 pages. This revision includes newer\n  referenes, results with CNN, fusion of local descriptors amd CNN and corrects\n  different typos", "journal-ref": null, "doi": "10.1016/j.imavis.2016.10.004", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gender classification (GC) has achieved high accuracy in different\nexperimental evaluations based mostly on inner facial details. However, these\nresults do not generalize well in unrestricted datasets and particularly in\ncross-database experiments, where the performance drops drastically. In this\npaper, we analyze the state-of-the-art GC accuracy on three large datasets:\nMORPH, LFW and GROUPS. We discuss their respective difficulties and bias,\nconcluding that the most challenging and wildest complexity is present in\nGROUPS. This dataset covers hard conditions such as low resolution imagery and\ncluttered background. Firstly, we analyze in depth the performance of different\ndescriptors extracted from the face and its local context on this dataset.\nSelecting the bests and studying their most suitable combination allows us to\ndesign a solution that beats any previously published results for GROUPS with\nthe Dago's protocol, reaching an accuracy over 94.2%, reducing the gap with\nother simpler datasets. The chosen solution based on local descriptors is later\nevaluated in a cross-database scenario with the three mentioned datasets, and\nfull dataset 5-fold cross validation. The achieved results are compared with a\nConvolutional Neural Network approach, achieving rather similar marks. Finally,\na solution is proposed combining both focuses, exhibiting great\ncomplementarity, boosting GC performance to beat previously published results\nin GC both cross-database, and full in-database evaluations.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2015 13:22:29 GMT"}, {"version": "v2", "created": "Fri, 19 Feb 2016 12:44:13 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Castrill\u00f3n-Santana", "M.", ""], ["Lorenzo-Navarro", "J.", ""], ["Ram\u00f3n-Balmaseda", "E.", ""]]}, {"id": "1507.07073", "submitter": "Weiyang Liu", "authors": "Yandong Wen, Weiyang Liu, Meng Yang, Zhifeng Li", "title": "Efficient Face Alignment via Locality-constrained Representation for\n  Robust Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Practical face recognition has been studied in the past decades, but still\nremains an open challenge. Current prevailing approaches have already achieved\nsubstantial breakthroughs in recognition accuracy. However, their performance\nusually drops dramatically if face samples are severely misaligned. To address\nthis problem, we propose a highly efficient misalignment-robust\nlocality-constrained representation (MRLR) algorithm for practical real-time\nface recognition. Specifically, the locality constraint that activates the most\ncorrelated atoms and suppresses the uncorrelated ones, is applied to construct\nthe dictionary for face alignment. Then we simultaneously align the warped face\nand update the locality-constrained dictionary, eventually obtaining the final\nalignment. Moreover, we make use of the block structure to accelerate the\nderived analytical solution. Experimental results on public data sets show that\nMRLR significantly outperforms several state-of-the-art approaches in terms of\nefficiency and scalability with even better performance.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jul 2015 06:17:08 GMT"}, {"version": "v2", "created": "Sat, 31 Oct 2015 14:53:49 GMT"}], "update_date": "2015-11-03", "authors_parsed": [["Wen", "Yandong", ""], ["Liu", "Weiyang", ""], ["Yang", "Meng", ""], ["Li", "Zhifeng", ""]]}, {"id": "1507.07075", "submitter": "V.P. Binu", "authors": "Keerthana S. Prakash, R. P. Prakash, V. P. Binu", "title": "A Study of Morphological Filtering Using Graph and Hypergraphs", "comments": "Advance Computing Conference (IACC), 2015 IEEE International,12-13\n  June 2015,Banglore India, Page(s):1017 - 1020,Print\n  ISBN:978-1-4799-8046-8,IEEE Xplore,2015", "journal-ref": null, "doi": "10.1109/IADCC.2015.7154858", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mathematical morphology (MM) helps to describe and analyze shapes using set\ntheory. MM can be effectively applied to binary images which are treated as\nsets. Basic morphological operators defined can be used as an effective tool in\nimage processing. Morphological operators are also developed based on graph and\nhypergraph. These operators have found better performance and applications in\nimage processing. Bino et al. [8], [9] developed the theory of morphological\noperators on hypergraph. A hypergraph structure is considered and basic\nmorphological operation erosion/dilation is defined. Several new operators\nopening/closing and filtering are also defined on the hypergraphs. Hypergraph\nbased filtering have found comparatively better performance with morphological\nfilters based on graph. In this paper we evaluate the effectiveness of\nhypergraph based ASF on binary images. Experimental results shows that\nhypergraph based ASF filters have outperformed graph based ASF.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jul 2015 06:48:55 GMT"}], "update_date": "2015-07-28", "authors_parsed": [["Prakash", "Keerthana S.", ""], ["Prakash", "R. P.", ""], ["Binu", "V. P.", ""]]}, {"id": "1507.07077", "submitter": "Vinayak Abrol", "authors": "V. Abrol and P. Sharma and A. K Sao", "title": "Making sense of randomness: an approach for fast recovery of\n  compressively sensed signals", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CV math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In compressed sensing (CS) framework, a signal is sampled below Nyquist rate,\nand the acquired compressed samples are generally random in nature. However,\nfor efficient estimation of the actual signal, the sensing matrix must preserve\nthe relative distances among the acquired compressed samples. Provided this\ncondition is fulfilled, we show that CS samples will preserve the envelope of\nthe actual signal even at different compression ratios. Exploiting this\nenvelope preserving property of CS samples, we propose a new fast dictionary\nlearning (DL) algorithm which is able to extract prototype signals from\ncompressive samples for efficient sparse representation and recovery of\nsignals. These prototype signals are orthogonal intrinsic mode functions (IMFs)\nextracted using empirical mode decomposition (EMD), which is one of the popular\nmethods to capture the envelope of a signal. The extracted IMFs are used to\nbuild the dictionary without even comprehending the original signal or the\nsensing matrix. Moreover, one can build the dictionary on-line as new CS\nsamples are available. In particularly, to recover first $L$ signals\n($\\in\\mathbb{R}^n$) at the decoder, one can build the dictionary in just\n$\\mathcal{O}(nL\\log n)$ operations, that is far less as compared to existing\napproaches. The efficiency of the proposed approach is demonstrated\nexperimentally for recovery of speech signals.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jul 2015 07:11:00 GMT"}], "update_date": "2015-07-28", "authors_parsed": [["Abrol", "V.", ""], ["Sharma", "P.", ""], ["Sao", "A. K", ""]]}, {"id": "1507.07096", "submitter": "V.P. Binu", "authors": "R. P. Prakash, Keerthana S. Prakash, V. P. Binu", "title": "Thinning Algorithm Using Hypergraph Based Morphological Operators", "comments": "Advance Computing Conference (IACC), 2015 IEEE International,Banglore\n  India", "journal-ref": "IEEE Xplore Advance Computing Conference (IACC) Proceedings Pages:\n  1017 - 1020,,Year: 2015", "doi": "10.1109/IADCC.2015.7154860", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The object recognition is a complex problem in the image processing.\nMathematical morphology is Shape oriented operations, that simplify image data,\npreserving their essential shape characteristics and eliminating irrelevancies.\nThis paper briefly describes morphological operators using hypergraph and its\napplications for thinning algorithms. The morphological operators using\nhypergraph method is used to preventing errors and irregularities in skeleton,\nand is an important step recognizing line objects. The morphological operators\nusing hypergraph such as dilation, erosion, opening, closing is a novel\napproach in image processing and it act as a filter remove the noise and errors\nin the images.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jul 2015 11:50:11 GMT"}], "update_date": "2015-07-28", "authors_parsed": [["Prakash", "R. P.", ""], ["Prakash", "Keerthana S.", ""], ["Binu", "V. P.", ""]]}, {"id": "1507.07203", "submitter": "Jaderick Pabico", "authors": "Louie Vincent A. Ngoho and Jaderick P. Pabico", "title": "Capturing the Dynamics of Pedestrian Traffic Using a Machine Vision\n  System", "comments": "11 pages, 10 figures, appeared in Proceedings (CDROM) of the 7th\n  National Conference on IT Education (NCITE 2009), Capitol University, Cagayan\n  De Oro City, Philippines, 21-23 October 2009", "journal-ref": "Philippine Information Technology Journal, 2(2):1-11, 2009", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We developed a machine vision system to automatically capture the dynamics of\npedestrians under four different traffic scenarios. By considering the overhead\nview of each pedestrian as a digital object, the system processes the image\nsequences to track the pedestrians. Considering the perspective effect of the\ncamera lens and the projected area of the hallway at the top-view scene, the\ndistance of each tracked object from its original position to its current\nposition is approximated every video frame. Using the approximated distance and\nthe video frame rate (30 frames per second), the respective velocity and\nacceleration of each tracked object are later derived. The quantified motion\ncharacteristics of the pedestrians are displayed by the system through\n2-dimensional graphs of the kinematics of motion. The system also outputs video\nimages of the pedestrians with superimposed markers for tracking. These visual\nmarkers were used to visually describe and quantify the behavior of the\npedestrians under different traffic scenarios.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jul 2015 14:29:55 GMT"}], "update_date": "2015-07-28", "authors_parsed": [["Ngoho", "Louie Vincent A.", ""], ["Pabico", "Jaderick P.", ""]]}, {"id": "1507.07242", "submitter": "Dayong Wang", "authors": "Dayong Wang and Charles Otto and Anil K. Jain", "title": "Face Search at Scale: 80 Million Gallery", "comments": "14 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": "MSU TECHNICAL REPORT MSU-CSE-15-11, JULY 24, 2015", "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the prevalence of social media websites, one challenge facing computer\nvision researchers is to devise methods to process and search for persons of\ninterest among the billions of shared photos on these websites. Facebook\nrevealed in a 2013 white paper that its users have uploaded more than 250\nbillion photos, and are uploading 350 million new photos each day. Due to this\nhumongous amount of data, large-scale face search for mining web images is both\nimportant and challenging. Despite significant progress in face recognition,\nsearching a large collection of unconstrained face images has not been\nadequately addressed. To address this challenge, we propose a face search\nsystem which combines a fast search procedure, coupled with a state-of-the-art\ncommercial off the shelf (COTS) matcher, in a cascaded framework. Given a probe\nface, we first filter the large gallery of photos to find the top-k most\nsimilar faces using deep features generated from a convolutional neural\nnetwork. The k candidates are re-ranked by combining similarities from deep\nfeatures and the COTS matcher. We evaluate the proposed face search system on a\ngallery containing 80 million web-downloaded face images. Experimental results\ndemonstrate that the deep features are competitive with state-of-the-art\nmethods on unconstrained face recognition benchmarks (LFW and IJB-A). Further,\nthe proposed face search system offers an excellent trade-off between accuracy\nand scalability on datasets consisting of millions of images. Additionally, in\nan experiment involving searching for face images of the Tsarnaev brothers,\nconvicted of the Boston Marathon bombing, the proposed face search system could\nfind the younger brother's (Dzhokhar Tsarnaev) photo at rank 1 in 1 second on a\n5M gallery and at rank 8 in 7 seconds on an 80M gallery.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jul 2015 20:06:43 GMT"}, {"version": "v2", "created": "Tue, 28 Jul 2015 22:09:17 GMT"}], "update_date": "2015-07-30", "authors_parsed": [["Wang", "Dayong", ""], ["Otto", "Charles", ""], ["Jain", "Anil K.", ""]]}, {"id": "1507.07458", "submitter": "Xun Xu", "authors": "Xun Xu, Timothy Hospedales, Shaogang Gong", "title": "Discovery of Shared Semantic Spaces for Multi-Scene Video Query and\n  Summarization", "comments": "Multi-Scene Traffic Behaviour Analysis ---- Accepted at IEEE\n  Transactions on Circuits and Systems for Video Technology", "journal-ref": null, "doi": "10.1109/TCSVT.2016.2532719", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The growing rate of public space CCTV installations has generated a need for\nautomated methods for exploiting video surveillance data including scene\nunderstanding, query, behaviour annotation and summarization. For this reason,\nextensive research has been performed on surveillance scene understanding and\nanalysis. However, most studies have considered single scenes, or groups of\nadjacent scenes. The semantic similarity between different but related scenes\n(e.g., many different traffic scenes of similar layout) is not generally\nexploited to improve any automated surveillance tasks and reduce manual effort.\nExploiting commonality, and sharing any supervised annotations, between\ndifferent scenes is however challenging due to: Some scenes are totally\nun-related -- and thus any information sharing between them would be\ndetrimental; while others may only share a subset of common activities -- and\nthus information sharing is only useful if it is selective. Moreover,\nsemantically similar activities which should be modelled together and shared\nacross scenes may have quite different pixel-level appearance in each scene. To\naddress these issues we develop a new framework for distributed multiple-scene\nglobal understanding that clusters surveillance scenes by their ability to\nexplain each other's behaviours; and further discovers which subset of\nactivities are shared versus scene-specific within each cluster. We show how to\nuse this structured representation of multiple scenes to improve common\nsurveillance tasks including scene activity understanding, cross-scene\nquery-by-example, behaviour classification with reduced supervised labelling\nrequirements, and video summarization. In each case we demonstrate how our\nmulti-scene model improves on a collection of standard single scene models and\na flat model of all scenes.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2015 15:50:03 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Xu", "Xun", ""], ["Hospedales", "Timothy", ""], ["Gong", "Shaogang", ""]]}, {"id": "1507.07505", "submitter": "Shun Miao", "authors": "Shun Miao, Z. Jane Wang, Rui Liao", "title": "Real-time 2D/3D Registration via CNN Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a Convolutional Neural Network (CNN) regression\napproach for real-time 2-D/3-D registration. Different from optimization-based\nmethods, which iteratively optimize the transformation parameters over a\nscalar-valued metric function representing the quality of the registration, the\nproposed method exploits the information embedded in the appearances of the\nDigitally Reconstructed Radiograph and X-ray images, and employs CNN regressors\nto directly estimate the transformation parameters. The CNN regressors are\ntrained for local zones and applied in a hierarchical manner to break down the\ncomplex regression task into simpler sub-tasks that can be learned separately.\nOur experiment results demonstrate the advantage of the proposed method in\ncomputational efficiency with negligible degradation of registration accuracy\ncompared to intensity-based methods.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2015 18:13:52 GMT"}, {"version": "v2", "created": "Tue, 29 Sep 2015 20:02:29 GMT"}, {"version": "v3", "created": "Mon, 25 Apr 2016 06:49:42 GMT"}], "update_date": "2016-04-26", "authors_parsed": [["Miao", "Shun", ""], ["Wang", "Z. Jane", ""], ["Liao", "Rui", ""]]}, {"id": "1507.07508", "submitter": "Peng Sun", "authors": "Peng Sun, Haoyin Zhou, Devon Lundine, James K. Min, Guanglei Xiong", "title": "Fast Segmentation of Left Ventricle in CT Images by Explicit Shape\n  Regression using Random Pixel Difference Features", "comments": "8 pages, link to a video demo", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, machine learning has been successfully applied to model-based left\nventricle (LV) segmentation. The general framework involves two stages, which\nstarts with LV localization and is followed by boundary delineation. Both are\ndriven by supervised learning techniques. When compared to previous\nnon-learning-based methods, several advantages have been shown, including full\nautomation and improved accuracy. However, the speed is still slow, in the\norder of several seconds, for applications involving a large number of cases or\ncase loads requiring real-time performance. In this paper, we propose a fast LV\nsegmentation algorithm by joint localization and boundary delineation via\ntraining explicit shape regressor with random pixel difference features. Tested\non 3D cardiac computed tomography (CT) image volumes, the average running time\nof the proposed algorithm is 1.2 milliseconds per case. On a dataset consisting\nof 139 CT volumes, a 5-fold cross validation shows the segmentation error is\n$1.21 \\pm 0.11$ for LV endocardium and $1.23 \\pm 0.11$ millimeters for\nepicardium. Compared with previous work, the proposed method is more stable\n(lower standard deviation) without significant compromise to the accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2015 18:17:55 GMT"}, {"version": "v2", "created": "Tue, 28 Jul 2015 14:07:05 GMT"}], "update_date": "2015-07-29", "authors_parsed": [["Sun", "Peng", ""], ["Zhou", "Haoyin", ""], ["Lundine", "Devon", ""], ["Min", "James K.", ""], ["Xiong", "Guanglei", ""]]}, {"id": "1507.07583", "submitter": "Dagmar Kainmueller", "authors": "David L. Richmond, Dagmar Kainmueller, Michael Y. Yang, Eugene W.\n  Myers, and Carsten Rother", "title": "Mapping Auto-context Decision Forests to Deep ConvNets for Semantic\n  Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the task of pixel-wise semantic segmentation given a small set of\nlabeled training images. Among two of the most popular techniques to address\nthis task are Decision Forests (DF) and Neural Networks (NN). In this work, we\nexplore the relationship between two special forms of these techniques: stacked\nDFs (namely Auto-context) and deep Convolutional Neural Networks (ConvNet). Our\nmain contribution is to show that Auto-context can be mapped to a deep ConvNet\nwith novel architecture, and thereby trained end-to-end. This mapping can be\nused as an initialization of a deep ConvNet, enabling training even in the face\nof very limited amounts of training data. We also demonstrate an approximate\nmapping back from the refined ConvNet to a second stacked DF, with improved\nperformance over the original. We experimentally verify that these mappings\noutperform stacked DFs for two different applications in computer vision and\nbiology: Kinect-based body part labeling from depth images, and somite\nsegmentation in microscopy images of developing zebrafish. Finally, we revisit\nthe core mapping from a Decision Tree (DT) to a NN, and show that it is also\npossible to map a fuzzy DT, with sigmoidal split decisions, to a NN. This\naddresses multiple limitations of the previous mapping, and yields new insights\ninto the popular Rectified Linear Unit (ReLU), and more recently proposed\nconcatenated ReLU (CReLU), activation functions.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2015 20:44:51 GMT"}, {"version": "v2", "created": "Tue, 22 Dec 2015 20:01:52 GMT"}, {"version": "v3", "created": "Mon, 13 Aug 2018 12:43:01 GMT"}], "update_date": "2018-08-14", "authors_parsed": [["Richmond", "David L.", ""], ["Kainmueller", "Dagmar", ""], ["Yang", "Michael Y.", ""], ["Myers", "Eugene W.", ""], ["Rother", "Carsten", ""]]}, {"id": "1507.07646", "submitter": "Angjoo Kanazawa", "authors": "Angjoo Kanazawa, Shahar Kovalsky, Ronen Basri, David W. Jacobs", "title": "Learning 3D Deformation of Animals from 2D Images", "comments": "10 pages, Eurographics 2016 (Best paper award)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding how an animal can deform and articulate is essential for a\nrealistic modification of its 3D model. In this paper, we show that such\ninformation can be learned from user-clicked 2D images and a template 3D model\nof the target animal. We present a volumetric deformation framework that\nproduces a set of new 3D models by deforming a template 3D model according to a\nset of user-clicked images. Our framework is based on a novel locally-bounded\ndeformation energy, where every local region has its own stiffness value that\nbounds how much distortion is allowed at that location. We jointly learn the\nlocal stiffness bounds as we deform the template 3D mesh to match each\nuser-clicked image. We show that this seemingly complex task can be solved as a\nsequence of convex optimization problems. We demonstrate the effectiveness of\nour approach on cats and horses, which are highly deformable and articulated\nanimals. Our framework produces new 3D models of animals that are significantly\nmore plausible than methods without learned stiffness.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2015 05:28:34 GMT"}, {"version": "v2", "created": "Sun, 15 May 2016 18:55:44 GMT"}, {"version": "v3", "created": "Tue, 24 May 2016 18:32:58 GMT"}], "update_date": "2016-05-25", "authors_parsed": [["Kanazawa", "Angjoo", ""], ["Kovalsky", "Shahar", ""], ["Basri", "Ronen", ""], ["Jacobs", "David W.", ""]]}, {"id": "1507.07760", "submitter": "Konrad Simon", "authors": "Konrad Simon, Sameer Sheorey, David Jacobs and Ronen Basri", "title": "A Hyperelastic Two-Scale Optimization Model for Shape Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We suggest a novel shape matching algorithm for three-dimensional surface\nmeshes of disk or sphere topology. The method is based on the physical theory\nof nonlinear elasticity and can hence handle large rotations and deformations.\nDeformation boundary conditions that supplement the underlying equations are\nusually unknown. Given an initial guess, these are optimized such that the\nmechanical boundary forces that are responsible for the deformation are of a\nsimple nature. We show a heuristic way to approximate the nonlinear\noptimization problem by a sequence of convex problems using finite elements.\nThe deformation cost, i.e, the forces, is measured on a coarse scale while\nICP-like matching is done on the fine scale. We demonstrate the plausibility of\nour algorithm on examples taken from different datasets.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2015 13:27:51 GMT"}], "update_date": "2015-07-29", "authors_parsed": [["Simon", "Konrad", ""], ["Sheorey", "Sameer", ""], ["Jacobs", "David", ""], ["Basri", "Ronen", ""]]}, {"id": "1507.07800", "submitter": "Jonathan Heras", "authors": "Gadea Mata and J\\'onathan Heras and Miguel Morales and Ana Romero and\n  Julio Rubio", "title": "SynapCountJ --- a Tool for Analyzing Synaptic Densities in Neurons", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The quantification of synapses is instrumental to measure the evolution of\nsynaptic densities of neurons under the effect of some physiological\nconditions, neuronal diseases or even drug treatments. However, the manual\nquantification of synapses is a tedious, error-prone, time-consuming and\nsubjective task; therefore, tools that might automate this process are\ndesirable. In this paper, we present SynapCountJ, an ImageJ plugin, that can\nmeasure synaptic density of individual neurons obtained by immunofluorescence\ntechniques, and also can be applied for batch processing of neurons that have\nbeen obtained in the same experiment or using the same setting. The procedure\nto quantify synapses implemented in SynapCountJ is based on the colocalization\nof three images of the same neuron (the neuron marked with two antibody markers\nand the structure of the neuron) and is inspired by methods coming from\nComputational Algebraic Topology. SynapCountJ provides a procedure to\nsemi-automatically quantify the number of synapses of neuron cultures; as a\nresult, the time required for such an analysis is greatly reduced.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2015 15:12:42 GMT"}], "update_date": "2015-07-29", "authors_parsed": [["Mata", "Gadea", ""], ["Heras", "J\u00f3nathan", ""], ["Morales", "Miguel", ""], ["Romero", "Ana", ""], ["Rubio", "Julio", ""]]}, {"id": "1507.07815", "submitter": "Svebor Karaman", "authors": "Giuseppe Lisanti and Svebor Karaman and Daniele Pezzatini and Alberto\n  Del Bimbo", "title": "A Multi-Camera Image Processing and Visualization System for Train\n  Safety Assessment", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a machine vision system to efficiently monitor,\nanalyze and present visual data acquired with a railway overhead gantry\nequipped with multiple cameras. This solution aims to improve the safety of\ndaily life railway transportation in a two- fold manner: (1) by providing\nautomatic algorithms that can process large imagery of trains (2) by helping\ntrain operators to keep attention on any possible malfunction. The system is\ndesigned with the latest cutting edge, high-rate visible and thermal cameras\nthat ob- serve a train passing under an railway overhead gantry. The machine\nvision system is composed of three principal modules: (1) an automatic wagon\nidentification system, recognizing the wagon ID according to the UIC\nclassification of railway coaches; (2) a temperature monitoring system; (3) a\nsystem for the detection, localization and visualization of the pantograph of\nthe train. These three machine vision modules process batch trains sequences\nand their resulting analysis are presented to an operator using a multitouch\nuser interface. We detail all technical aspects of our multi-camera portal: the\nhardware requirements, the software developed to deal with the high-frame rate\ncameras and ensure reliable acquisition, the algorithms proposed to solve each\ncomputer vision task, and the multitouch interaction and visualization\ninterface. We evaluate each component of our system on a dataset recorded in an\nad-hoc railway test-bed, showing the potential of our proposed portal for train\nsafety assessment.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2015 15:36:24 GMT"}], "update_date": "2015-07-29", "authors_parsed": [["Lisanti", "Giuseppe", ""], ["Karaman", "Svebor", ""], ["Pezzatini", "Daniele", ""], ["Del Bimbo", "Alberto", ""]]}, {"id": "1507.07830", "submitter": "Yongxin Yang", "authors": "Yongxin Yang and Timothy Hospedales", "title": "Zero-Shot Domain Adaptation via Kernel Regression on the Grassmannian", "comments": "Accepted to BMVC 2015 Workshop on Differential Geometry in Computer\n  Vision (DIFF-CV)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most visual recognition methods implicitly assume the data distribution\nremains unchanged from training to testing. However, in practice domain shift\noften exists, where real-world factors such as lighting and sensor type change\nbetween train and test, and classifiers do not generalise from source to target\ndomains. It is impractical to train separate models for all possible situations\nbecause collecting and labelling the data is expensive. Domain adaptation\nalgorithms aim to ameliorate domain shift, allowing a model trained on a source\nto perform well on a different target domain. However, even for the setting of\nunsupervised domain adaptation, where the target domain is unlabelled,\ncollecting data for every possible target domain is still costly. In this\npaper, we propose a new domain adaptation method that has no need to access\neither data or labels of the target domain when it can be described by a\nparametrised vector and there exits several related source domains within the\nsame parametric space. It greatly reduces the burden of data collection and\nannotation, and our experiments show some promising results.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2015 16:13:48 GMT"}, {"version": "v2", "created": "Wed, 29 Jul 2015 17:53:59 GMT"}], "update_date": "2015-07-30", "authors_parsed": [["Yang", "Yongxin", ""], ["Hospedales", "Timothy", ""]]}, {"id": "1507.07882", "submitter": "Samarth Manoj Brahmbhatt", "authors": "Samarth Brahmbhatt, Heni Ben Amor and Henrik Christensen", "title": "Occlusion-Aware Object Localization, Segmentation and Pose Estimation", "comments": "British Machine Vision Conference 2015 (poster)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a learning approach for localization and segmentation of objects\nin an image in a manner that is robust to partial occlusion. Our algorithm\nproduces a bounding box around the full extent of the object and labels pixels\nin the interior that belong to the object. Like existing segmentation aware\ndetection approaches, we learn an appearance model of the object and consider\nregions that do not fit this model as potential occlusions. However, in\naddition to the established use of pairwise potentials for encouraging local\nconsistency, we use higher order potentials which capture information at the\nlevel of im- age segments. We also propose an efficient loss function that\ntargets both localization and segmentation performance. Our algorithm achieves\n13.52% segmentation error and 0.81 area under the false-positive per image vs.\nrecall curve on average over the challenging CMU Kitchen Occlusion Dataset.\nThis is a 42.44% decrease in segmentation error and a 16.13% increase in\nlocalization performance compared to the state-of-the-art. Finally, we show\nthat the visibility labelling produced by our algorithm can make full 3D pose\nestimation from a single image robust to occlusion.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2015 18:16:35 GMT"}], "update_date": "2015-07-29", "authors_parsed": [["Brahmbhatt", "Samarth", ""], ["Amor", "Heni Ben", ""], ["Christensen", "Henrik", ""]]}, {"id": "1507.07909", "submitter": "Luiz Gustavo Hafemann", "authors": "Luiz G. Hafemann, Robert Sabourin, Luiz S. Oliveira", "title": "Offline Handwritten Signature Verification - Literature Review", "comments": "Accepted to the International Conference on Image Processing Theory,\n  Tools and Applications (IPTA 2017)", "journal-ref": null, "doi": "10.1109/IPTA.2017.8310112", "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The area of Handwritten Signature Verification has been broadly researched in\nthe last decades, but remains an open research problem. The objective of\nsignature verification systems is to discriminate if a given signature is\ngenuine (produced by the claimed individual), or a forgery (produced by an\nimpostor). This has demonstrated to be a challenging task, in particular in the\noffline (static) scenario, that uses images of scanned signatures, where the\ndynamic information about the signing process is not available. Many\nadvancements have been proposed in the literature in the last 5-10 years, most\nnotably the application of Deep Learning methods to learn feature\nrepresentations from signature images. In this paper, we present how the\nproblem has been handled in the past few decades, analyze the recent\nadvancements in the field, and the potential directions for future research.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2015 19:31:44 GMT"}, {"version": "v2", "created": "Wed, 19 Aug 2015 20:40:01 GMT"}, {"version": "v3", "created": "Wed, 2 Aug 2017 15:11:20 GMT"}, {"version": "v4", "created": "Mon, 16 Oct 2017 12:56:24 GMT"}], "update_date": "2019-01-25", "authors_parsed": [["Hafemann", "Luiz G.", ""], ["Sabourin", "Robert", ""], ["Oliveira", "Luiz S.", ""]]}, {"id": "1507.08030", "submitter": "Anthony Cazanoves Mr", "authors": "Anthony Cazasnoves and Fanny Buyens and Sylvie Sevestre", "title": "Adapted sampling for 3D X-ray computed tomography", "comments": "The 13th International Meeting on Fully Three-Dimensional Image\n  Reconstruction in Radiology and Nuclear Medicine", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a method to build an adapted mesh representation\nof a 3D object for X-Ray tomography reconstruction. Using this representation,\nwe provide means to reduce the computational cost of reconstruction by way of\niterative algorithms. The adapted sampling of the reconstruction space is\ndirectly obtained from the projection dataset and prior to any reconstruction.\nIt is built following two stages : firstly, 2D structural information is\nextracted from the projection images and is secondly merged in 3D to obtain a\n3D pointcloud sampling the interfaces of the object. A relevant mesh is then\nbuilt from this cloud by way of tetrahedralization. Critical parameters\nselections have been automatized through a statistical framework, thus avoiding\ndependence on users expertise. Applying this approach on geometrical shapes and\non a 3D Shepp-Logan phantom, we show the relevance of such a sampling -\nobtained in a few seconds - and the drastic decrease in cells number to be\nestimated during reconstruction when compared to the usual regular voxel\nlattice. A first iterative reconstruction of the Shepp-Logan using this kind of\nsampling shows the relevant advantages in terms of low dose or sparse\nacquisition sampling contexts. The method can also prove useful for other\napplications such as finite element method computations.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2015 06:30:04 GMT"}], "update_date": "2015-07-30", "authors_parsed": [["Cazasnoves", "Anthony", ""], ["Buyens", "Fanny", ""], ["Sevestre", "Sylvie", ""]]}, {"id": "1507.08064", "submitter": "Xiaochao Qu", "authors": "Xiaochao Qu, Suah Kim, Run Cui, Hyoung Joong Kim", "title": "Collaborative Representation Classification Ensemble for Face\n  Recognition", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collaborative Representation Classification (CRC) for face recognition\nattracts a lot attention recently due to its good recognition performance and\nfast speed. Compared to Sparse Representation Classification (SRC), CRC\nachieves a comparable recognition performance with 10-1000 times faster speed.\nIn this paper, we propose to ensemble several CRC models to promote the\nrecognition rate, where each CRC model uses different and divergent randomly\ngenerated biologically-inspired features as the face representation. The\nproposed ensemble algorithm calculates an ensemble weight for each CRC model\nthat guided by the underlying classification rule of CRC. The obtained weights\nreflect the confidences of those CRC models where the more confident CRC models\nhave larger weights. The proposed weighted ensemble method proves to be very\neffective and improves the performance of each CRC model significantly.\nExtensive experiments are conducted to show the superior performance of the\nproposed method.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2015 08:47:49 GMT"}], "update_date": "2015-07-30", "authors_parsed": [["Qu", "Xiaochao", ""], ["Kim", "Suah", ""], ["Cui", "Run", ""], ["Kim", "Hyoung Joong", ""]]}, {"id": "1507.08076", "submitter": "Annan Li", "authors": "Annan Li, Shiguang Shan, Xilin Chen, Bingpeng Ma, Shuicheng Yan, Wen\n  Gao", "title": "Cross-pose Face Recognition by Canonical Correlation Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The pose problem is one of the bottlenecks in automatic face recognition. We\nargue that one of the diffculties in this problem is the severe misalignment in\nface images or feature vectors with different poses. In this paper, we propose\nthat this problem can be statistically solved or at least mitigated by\nmaximizing the intra-subject across-pose correlations via canonical correlation\nanalysis (CCA). In our method, based on the data set with coupled face images\nof the same identities and across two different poses, CCA learns\nsimultaneously two linear transforms, each for one pose. In the transformed\nsubspace, the intra-subject correlations between the different poses are\nmaximized, which implies pose-invariance or pose-robustness is achieved. The\nexperimental results show that our approach could considerably improve the\nrecognition performance. And if further enhanced with holistic+local feature\nrepresentation, the performance could be comparable to the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2015 09:28:11 GMT"}], "update_date": "2015-07-30", "authors_parsed": [["Li", "Annan", ""], ["Shan", "Shiguang", ""], ["Chen", "Xilin", ""], ["Ma", "Bingpeng", ""], ["Yan", "Shuicheng", ""], ["Gao", "Wen", ""]]}, {"id": "1507.08085", "submitter": "Gao Zhu", "authors": "Gao Zhu, Fatih Porikli, Hongdong Li", "title": "Tracking Randomly Moving Objects on Edge Box Proposals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most tracking-by-detection methods employ a local search window around the\npredicted object location in the current frame assuming the previous location\nis accurate, the trajectory is smooth, and the computational capacity permits a\nsearch radius that can accommodate the maximum speed yet small enough to reduce\nmismatches. These, however, may not be valid always, in particular for fast and\nirregularly moving objects. Here, we present an object tracker that is not\nlimited to a local search window and has ability to probe efficiently the\nentire frame. Our method generates a small number of \"high-quality\" proposals\nby a novel instance-specific objectness measure and evaluates them against the\nobject model that can be adopted from an existing tracking-by-detection\napproach as a core tracker. During the tracking process, we update the object\nmodel concentrating on hard false-positives supplied by the proposals, which\nhelp suppressing distractors caused by difficult background clutters, and learn\nhow to re-rank proposals according to the object model. Since we reduce\nsignificantly the number of hypotheses the core tracker evaluates, we can use\nricher object descriptors and stronger detector. Our method outperforms most\nrecent state-of-the-art trackers on popular tracking benchmarks, and provides\nimproved robustness for fast moving objects as well as for ultra low-frame-rate\nvideos.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2015 09:59:42 GMT"}, {"version": "v2", "created": "Sun, 29 Nov 2015 23:51:02 GMT"}], "update_date": "2015-12-01", "authors_parsed": [["Zhu", "Gao", ""], ["Porikli", "Fatih", ""], ["Li", "Hongdong", ""]]}, {"id": "1507.08155", "submitter": "Teng Qiu", "authors": "Teng Qiu, Yongjie Li", "title": "IT-Dendrogram: A New Member of the In-Tree (IT) Clustering Family", "comments": "13 pages, 6 figures. IT-Dendrogram: An Effective Method to Visualize\n  the In-Tree structure by Dendrogram", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Previously, we proposed a physically-inspired method to construct data points\ninto an effective in-tree (IT) structure, in which the underlying cluster\nstructure in the dataset is well revealed. Although there are some edges in the\nIT structure requiring to be removed, such undesired edges are generally\ndistinguishable from other edges and thus are easy to be determined. For\ninstance, when the IT structures for the 2-dimensional (2D) datasets are\ngraphically presented, those undesired edges can be easily spotted and\ninteractively determined. However, in practice, there are many datasets that do\nnot lie in the 2D Euclidean space, thus their IT structures cannot be\ngraphically presented. But if we can effectively map those IT structures into a\nvisualized space in which the salient features of those undesired edges are\npreserved, then the undesired edges in the IT structures can still be visually\ndetermined in a visualization environment. Previously, this purpose was reached\nby our method called IT-map. The outstanding advantage of IT-map is that\nclusters can still be found even with the so-called crowding problem in the\nembedding.\n  In this paper, we propose another method, called IT-Dendrogram, to achieve\nthe same goal through an effective combination of the IT structure and the\nsingle link hierarchical clustering (SLHC) method. Like IT-map, IT-Dendrogram\ncan also effectively represent the IT structures in a visualization\nenvironment, whereas using another form, called the Dendrogram. IT-Dendrogram\ncan serve as another visualization method to determine the undesired edges in\nthe IT structures and thus benefit the IT-based clustering analysis. This was\ndemonstrated on several datasets with different shapes, dimensions, and\nattributes. Unlike IT-map, IT-Dendrogram can always avoid the crowding problem,\nwhich could help users make more reliable cluster analysis in certain problems.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2015 14:22:13 GMT"}], "update_date": "2015-07-30", "authors_parsed": [["Qiu", "Teng", ""], ["Li", "Yongjie", ""]]}, {"id": "1507.08173", "submitter": "Nauman Shahid", "authors": "Nauman Shahid, Nathanael Perraudin, Vassilis Kalofolias, Gilles Puy,\n  Pierre Vandergheynst", "title": "Fast Robust PCA on Graphs", "comments": null, "journal-ref": null, "doi": "10.1109/JSTSP.2016.2555239", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mining useful clusters from high dimensional data has received significant\nattention of the computer vision and pattern recognition community in the\nrecent years. Linear and non-linear dimensionality reduction has played an\nimportant role to overcome the curse of dimensionality. However, often such\nmethods are accompanied with three different problems: high computational\ncomplexity (usually associated with the nuclear norm minimization),\nnon-convexity (for matrix factorization methods) and susceptibility to gross\ncorruptions in the data. In this paper we propose a principal component\nanalysis (PCA) based solution that overcomes these three issues and\napproximates a low-rank recovery method for high dimensional datasets. We\ntarget the low-rank recovery by enforcing two types of graph smoothness\nassumptions, one on the data samples and the other on the features by designing\na convex optimization problem. The resulting algorithm is fast, efficient and\nscalable for huge datasets with O(nlog(n)) computational complexity in the\nnumber of data samples. It is also robust to gross corruptions in the dataset\nas well as to the model parameters. Clustering experiments on 7 benchmark\ndatasets with different types of corruptions and background separation\nexperiments on 3 video datasets show that our proposed model outperforms 10\nstate-of-the-art dimensionality reduction models. Our theoretical analysis\nproves that the proposed model is able to recover approximate low-rank\nrepresentations with a bounded error for clusterable data.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2015 14:53:33 GMT"}, {"version": "v2", "created": "Mon, 25 Jan 2016 20:29:57 GMT"}], "update_date": "2016-05-25", "authors_parsed": [["Shahid", "Nauman", ""], ["Perraudin", "Nathanael", ""], ["Kalofolias", "Vassilis", ""], ["Puy", "Gilles", ""], ["Vandergheynst", "Pierre", ""]]}, {"id": "1507.08184", "submitter": "Teodora Szasz", "authors": "Teodora Szasz, Adrian Basarab, and Denis Kouam\\'e", "title": "Beamforming through regularized inverse problems in ultrasound medical\n  imaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Beamforming in ultrasound imaging has significant impact on the quality of\nthe final image, controlling its resolution and contrast. Despite its low\nspatial resolution and contrast, delay-and-sum is still extensively used\nnowadays in clinical applications, due to its real-time capabilities. The most\ncommon alternatives are minimum variance method and its variants, which\novercome the drawbacks of delay-and-sum, at the cost of higher computational\ncomplexity that limits its utilization in real-time applications.\n  In this paper, we propose to perform beamforming in ultrasound imaging\nthrough a regularized inverse problem based on a linear model relating the\nreflected echoes to the signal to be recovered. Our approach presents two major\nadvantages: i) its flexibility in the choice of statistical assumptions on the\nsignal to be beamformed (Laplacian and Gaussian statistics are tested herein)\nand ii) its robustness to a reduced number of pulse emissions. The proposed\nframework is flexible and allows for choosing the right trade-off between noise\nsuppression and sharpness of the resulted image. We illustrate the performance\nof our approach on both simulated and experimental data, with \\textit{in vivo}\nexamples of carotid and thyroid. Compared to delay-and-sum, minimimum variance\nand two other recently published beamforming techniques, our method offers\nbetter spatial resolution, respectively contrast, when using Laplacian and\nGaussian priors.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2015 15:29:09 GMT"}, {"version": "v2", "created": "Wed, 18 May 2016 22:34:36 GMT"}], "update_date": "2016-05-20", "authors_parsed": [["Szasz", "Teodora", ""], ["Basarab", "Adrian", ""], ["Kouam\u00e9", "Denis", ""]]}, {"id": "1507.08286", "submitter": "David Held", "authors": "David Held, Sebastian Thrun, Silvio Savarese", "title": "Deep Learning for Single-View Instance Recognition", "comments": "16 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning methods have typically been trained on large datasets in which\nmany training examples are available. However, many real-world product datasets\nhave only a small number of images available for each product. We explore the\nuse of deep learning methods for recognizing object instances when we have only\na single training example per class. We show that feedforward neural networks\noutperform state-of-the-art methods for recognizing objects from novel\nviewpoints even when trained from just a single image per object. To further\nimprove our performance on this task, we propose to take advantage of a\nsupplementary dataset in which we observe a separate set of objects from\nmultiple viewpoints. We introduce a new approach for training deep learning\nmethods for instance recognition with limited training data, in which we use an\nauxiliary multi-view dataset to train our network to be robust to viewpoint\nchanges. We find that this approach leads to a more robust classifier for\nrecognizing objects from novel viewpoints, outperforming previous\nstate-of-the-art approaches including keypoint-matching, template-based\ntechniques, and sparse coding.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2015 20:11:12 GMT"}], "update_date": "2015-07-31", "authors_parsed": [["Held", "David", ""], ["Thrun", "Sebastian", ""], ["Savarese", "Silvio", ""]]}, {"id": "1507.08363", "submitter": "Massimo Piccardi", "authors": "Shaukat Abidi, Massimo Piccardi, Mary-Anne Williams", "title": "Action recognition in still images by latent superpixel classification", "comments": "To appear in the Proceedings of the IEEE International Conference on\n  Image Processing. Copyright 2015 IEEE. Please be aware of your obligations\n  with respect to copyrighted material", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Action recognition from still images is an important task of computer vision\napplications such as image annotation, robotic navigation, video surveillance\nand several others. Existing approaches mainly rely on either bag-of-feature\nrepresentations or articulated body-part models. However, the relationship\nbetween the action and the image segments is still substantially unexplored.\nFor this reason, in this paper we propose to approach action recognition by\nleveraging an intermediate layer of \"superpixels\" whose latent classes can act\nas attributes of the action. In the proposed approach, the action class is\npredicted by a structural model(learnt by Latent Structural SVM) based on\nmeasurements from the image superpixels and their latent classes. Experimental\nresults over the challenging Stanford 40 Actions dataset report a significant\naverage accuracy of 74.06% for the positive class and 88.50% for the negative\nclass, giving evidence to the performance of the proposed approach.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2015 03:05:47 GMT"}], "update_date": "2015-07-31", "authors_parsed": [["Abidi", "Shaukat", ""], ["Piccardi", "Massimo", ""], ["Williams", "Mary-Anne", ""]]}, {"id": "1507.08373", "submitter": "Mehrtash Harandi", "authors": "Mehrtash Harandi, Mathieu Salzmann, Fatih Porikli", "title": "When VLAD met Hilbert", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vectors of Locally Aggregated Descriptors (VLAD) have emerged as powerful\nimage/video representations that compete with or even outperform\nstate-of-the-art approaches on many challenging visual recognition tasks. In\nthis paper, we address two fundamental limitations of VLAD: its requirement for\nthe local descriptors to have vector form and its restriction to linear\nclassifiers due to its high-dimensionality. To this end, we introduce a\nkernelized version of VLAD. This not only lets us inherently exploit more\nsophisticated classification schemes, but also enables us to efficiently\naggregate non-vector descriptors (e.g., tensors) in the VLAD framework.\nFurthermore, we propose three approximate formulations that allow us to\naccelerate the coding process while still benefiting from the properties of\nkernel VLAD. Our experiments demonstrate the effectiveness of our approach at\nhandling manifold-valued data, such as covariance descriptors, on several\nclassification tasks. Our results also evidence the benefits of our nonlinear\nVLAD descriptors against the linear ones in Euclidean space using several\nstandard benchmark datasets.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2015 04:17:02 GMT"}], "update_date": "2015-07-31", "authors_parsed": [["Harandi", "Mehrtash", ""], ["Salzmann", "Mathieu", ""], ["Porikli", "Fatih", ""]]}, {"id": "1507.08429", "submitter": "Shuchang Zhou", "authors": "Shuchang Zhou, Yuxin Wu", "title": "Multilinear Map Layer: Prediction Regularization by Structural\n  Constraint", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose and study a technique to impose structural\nconstraints on the output of a neural network, which can reduce amount of\ncomputation and number of parameters besides improving prediction accuracy when\nthe output is known to approximately conform to the low-rankness prior. The\ntechnique proceeds by replacing the output layer of neural network with the\nso-called MLM layers, which forces the output to be the result of some\nMultilinear Map, like a hybrid-Kronecker-dot product or Kronecker Tensor\nProduct. In particular, given an \"autoencoder\" model trained on SVHN dataset,\nwe can construct a new model with MLM layer achieving 62\\% reduction in total\nnumber of parameters and reduction of $\\ell_2$ reconstruction error from 0.088\nto 0.004. Further experiments on other autoencoder model variants trained on\nSVHN datasets also demonstrate the efficacy of MLM layers.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2015 09:34:30 GMT"}], "update_date": "2015-07-31", "authors_parsed": [["Zhou", "Shuchang", ""], ["Wu", "Yuxin", ""]]}, {"id": "1507.08445", "submitter": "Ankan Bansal", "authors": "Ankan Bansal and K.S. Venkatesh", "title": "People Counting in High Density Crowds from Still Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method of estimating the number of people in high density crowds\nfrom still images. The method estimates counts by fusing information from\nmultiple sources. Most of the existing work on crowd counting deals with very\nsmall crowds (tens of individuals) and use temporal information from videos.\nOur method uses only still images to estimate the counts in high density images\n(hundreds to thousands of individuals). At this scale, we cannot rely on only\none set of features for count estimation. We, therefore, use multiple sources,\nviz. interest points (SIFT), Fourier analysis, wavelet decomposition, GLCM\nfeatures and low confidence head detections, to estimate the counts. Each of\nthese sources gives a separate estimate of the count along with confidences and\nother statistical measures which are then combined to obtain the final\nestimate. We test our method on an existing dataset of fifty images containing\nover 64000 individuals. Further, we added another fifty annotated images of\ncrowds and tested on the complete dataset of hundred images containing over\n87000 individuals. The counts per image range from 81 to 4633. We report the\nperformance in terms of mean absolute error, which is a measure of accuracy of\nthe method, and mean normalised absolute error, which is a measure of the\nrobustness.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2015 10:47:31 GMT"}], "update_date": "2015-07-31", "authors_parsed": [["Bansal", "Ankan", ""], ["Venkatesh", "K. S.", ""]]}, {"id": "1507.08571", "submitter": "Weiya Ren", "authors": "Wei-Ya Ren, Shuo-Hao Li, Qiang Guo, Guo-Hui Li, Jun Zhang", "title": "Agglomerative clustering and collectiveness measure via exponent\n  generating function", "comments": "11 pages. written on 2015-7-18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The key in agglomerative clustering is to define the affinity measure between\ntwo sets. A novel agglomerative clustering method is proposed by utilizing the\npath integral to define the affinity measure. Firstly, the path integral\ndescriptor of an edge, a node and a set is computed by path integral and\nexponent generating function. Then, the affinity measure between two sets is\nobtained by path integral descriptor of sets. Several good properties of the\npath integral descriptor is proposed in this paper. In addition, we give the\nphysical interpretation of the proposed path integral descriptor of a set. The\nproposed path integral descriptor of a set can be regard as the collectiveness\nmeasure of a set, which can be a moving system such as human crowd, sheep herd\nand so on. Self-driven particle (SDP) model is used to test the ability of the\nproposed method in measuring collectiveness.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2015 16:30:00 GMT"}, {"version": "v2", "created": "Fri, 7 Aug 2015 16:01:20 GMT"}], "update_date": "2015-08-10", "authors_parsed": [["Ren", "Wei-Ya", ""], ["Li", "Shuo-Hao", ""], ["Guo", "Qiang", ""], ["Li", "Guo-Hui", ""], ["Zhang", "Jun", ""]]}, {"id": "1507.08711", "submitter": "Mehrtash Harandi", "authors": "Mehrtash Harandi, Mathieu Salzmann, Mahsa Baktashmotlagh", "title": "Beyond Gauss: Image-Set Matching on the Riemannian Manifold of PDFs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art image-set matching techniques typically implicitly model\neach image-set with a Gaussian distribution. Here, we propose to go beyond\nthese representations and model image-sets as probability distribution\nfunctions (PDFs) using kernel density estimators. To compare and match\nimage-sets, we exploit Csiszar f-divergences, which bear strong connections to\nthe geodesic distance defined on the space of PDFs, i.e., the statistical\nmanifold. Furthermore, we introduce valid positive definite kernels on the\nstatistical manifolds, which let us make use of more powerful classification\nschemes to match image-sets. Finally, we introduce a supervised dimensionality\nreduction technique that learns a latent space where f-divergences reflect the\nclass labels of the data. Our experiments on diverse problems, such as\nvideo-based face recognition and dynamic texture classification, evidence the\nbenefits of our approach over the state-of-the-art image-set matching methods.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2015 00:11:11 GMT"}], "update_date": "2015-08-03", "authors_parsed": [["Harandi", "Mehrtash", ""], ["Salzmann", "Mathieu", ""], ["Baktashmotlagh", "Mahsa", ""]]}, {"id": "1507.08736", "submitter": "Stephen Odaibo", "authors": "Stephen G. Odaibo", "title": "A Sinc Wavelet Describes the Receptive Fields of Neurons in the Motion\n  Cortex", "comments": "This work was presented in part at the 44th Annual Meeting of the\n  Society for Neuroscience in Washington, DC", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.CV cs.IT math.IT physics.bio-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual perception results from a systematic transformation of the information\nflowing through the visual system. In the neuronal hierarchy, the response\nproperties of single neurons are determined by neurons located one level below,\nand in turn, determine the responses of neurons located one level above.\nTherefore in modeling receptive fields, it is essential to ensure that the\nresponse properties of neurons in a given level can be generated by combining\nthe response models of neurons in its input levels. However, existing response\nmodels of neurons in the motion cortex do not inherently yield the temporal\nfrequency filtering gradient (TFFG) property that is known to emerge along the\nprimary visual cortex (V1) to middle temporal (MT) motion processing stream.\nTFFG is the change from predominantly lowpass to predominantly bandpass\ntemporal frequency filtering character along the V1 to MT pathway (Foster et al\n1985; DeAngelis et al 1993; Hawken et al 1996). We devised a new model, the\nsinc wavelet model (Odaibo, 2014), which logically and efficiently generates\nthe TFFG. The model replaces the Gabor function's sine wave carrier with a sinc\n(sin(x)/x) function, and has the same or fewer number of parameters as existing\nmodels. Because of its logical consistency with the emergent network property\nof TFFG, we conclude that the sinc wavelet is a better model for the receptive\nfields of motion cortex neurons. This model will provide new physiological\ninsights into how the brain represents visual information.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2015 02:55:54 GMT"}], "update_date": "2015-08-03", "authors_parsed": [["Odaibo", "Stephen G.", ""]]}, {"id": "1507.08750", "submitter": "Junhyuk Oh", "authors": "Junhyuk Oh, Xiaoxiao Guo, Honglak Lee, Richard Lewis, Satinder Singh", "title": "Action-Conditional Video Prediction using Deep Networks in Atari Games", "comments": "Published at NIPS 2015 (Advances in Neural Information Processing\n  Systems 28)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by vision-based reinforcement learning (RL) problems, in particular\nAtari games from the recent benchmark Aracade Learning Environment (ALE), we\nconsider spatio-temporal prediction problems where future (image-)frames are\ndependent on control variables or actions as well as previous frames. While not\ncomposed of natural scenes, frames in Atari games are high-dimensional in size,\ncan involve tens of objects with one or more objects being controlled by the\nactions directly and many other objects being influenced indirectly, can\ninvolve entry and departure of objects, and can involve deep partial\nobservability. We propose and evaluate two deep neural network architectures\nthat consist of encoding, action-conditional transformation, and decoding\nlayers based on convolutional neural networks and recurrent neural networks.\nExperimental results show that the proposed architectures are able to generate\nvisually-realistic frames that are also useful for control over approximately\n100-step action-conditional futures in some games. To the best of our\nknowledge, this paper is the first to make and evaluate long-term predictions\non high-dimensional video conditioned by control inputs.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2015 04:43:30 GMT"}, {"version": "v2", "created": "Tue, 22 Dec 2015 04:26:54 GMT"}], "update_date": "2015-12-23", "authors_parsed": [["Oh", "Junhyuk", ""], ["Guo", "Xiaoxiao", ""], ["Lee", "Honglak", ""], ["Lewis", "Richard", ""], ["Singh", "Satinder", ""]]}, {"id": "1507.08754", "submitter": "Peijun Hu", "authors": "Fa Wu, Peijun Hu, and Dexing Kong", "title": "Flip-Rotate-Pooling Convolution and Split Dropout on Convolution Neural\n  Networks for Image Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new version of Dropout called Split Dropout (sDropout)\nand rotational convolution techniques to improve CNNs' performance on image\nclassification. The widely used standard Dropout has advantage of preventing\ndeep neural networks from overfitting by randomly dropping units during\ntraining. Our sDropout randomly splits the data into two subsets and keeps both\nrather than discards one subset. We also introduce two rotational convolution\ntechniques, i.e. rotate-pooling convolution (RPC) and flip-rotate-pooling\nconvolution (FRPC) to boost CNNs' performance on the robustness for rotation\ntransformation. These two techniques encode rotation invariance into the\nnetwork without adding extra parameters. Experimental evaluations on\nImageNet2012 classification task demonstrate that sDropout not only enhances\nthe performance but also converges faster. Additionally, RPC and FRPC make CNNs\nmore robust for rotation transformations. Overall, FRPC together with sDropout\nbring $1.18\\%$ (model of Zeiler and Fergus~\\cite{zeiler2013visualizing},\n10-view, top-1) accuracy increase in ImageNet 2012 classification task compared\nto the original network.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2015 05:02:56 GMT"}], "update_date": "2015-08-03", "authors_parsed": [["Wu", "Fa", ""], ["Hu", "Peijun", ""], ["Kong", "Dexing", ""]]}, {"id": "1507.08761", "submitter": "Amir Shahroudy", "authors": "Amir Shahroudy, Gang Wang, Tian-Tsong Ng, Qingxiong Yang", "title": "Multimodal Multipart Learning for Action Recognition in Depth Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The articulated and complex nature of human actions makes the task of action\nrecognition difficult. One approach to handle this complexity is dividing it to\nthe kinetics of body parts and analyzing the actions based on these partial\ndescriptors. We propose a joint sparse regression based learning method which\nutilizes the structured sparsity to model each action as a combination of\nmultimodal features from a sparse set of body parts. To represent dynamics and\nappearance of parts, we employ a heterogeneous set of depth and skeleton based\nfeatures. The proper structure of multimodal multipart features are formulated\ninto the learning framework via the proposed hierarchical mixed norm, to\nregularize the structured features of each part and to apply sparsity between\nthem, in favor of a group feature selection. Our experimental results expose\nthe effectiveness of the proposed learning method in which it outperforms other\nmethods in all three tested datasets while saturating one of them by achieving\nperfect accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2015 06:02:56 GMT"}], "update_date": "2015-08-03", "authors_parsed": [["Shahroudy", "Amir", ""], ["Wang", "Gang", ""], ["Ng", "Tian-Tsong", ""], ["Yang", "Qingxiong", ""]]}, {"id": "1507.08818", "submitter": "Dario Garcia-Gasulla", "authors": "D. Garcia-Gasulla, J. B\\'ejar, U. Cort\\'es, E. Ayguad\\'e, J. Labarta,\n  T. Suzumura and R. Chen", "title": "A Visual Embedding for the Unsupervised Extraction of Abstract Semantics", "comments": "14 pages, 5 figures, accepted at Cognitive Systems Research", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vector-space word representations obtained from neural network models have\nbeen shown to enable semantic operations based on vector arithmetic. In this\npaper, we explore the existence of similar information on vector\nrepresentations of images. For that purpose we define a methodology to obtain\nlarge, sparse vector representations of image classes, and generate vectors\nthrough the state-of-the-art deep learning architecture GoogLeNet for 20K\nimages obtained from ImageNet. We first evaluate the resultant vector-space\nsemantics through its correlation with WordNet distances, and find vector\ndistances to be strongly correlated with linguistic semantics. We then explore\nthe location of images within the vector space, finding elements close in\nWordNet to be clustered together, regardless of significant visual variances\n(e.g. 118 dog types). More surprisingly, we find that the space unsupervisedly\nseparates complex classes without prior knowledge (e.g. living things).\nAfterwards, we consider vector arithmetics. Although we are unable to obtain\nmeaningful results on this regard, we discuss the various problem we\nencountered, and how we consider to solve them. Finally, we discuss the impact\nof our research for cognitive systems, focusing on the role of the architecture\nbeing used.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2015 10:16:42 GMT"}, {"version": "v2", "created": "Tue, 10 Nov 2015 17:27:56 GMT"}, {"version": "v3", "created": "Mon, 16 Nov 2015 17:03:54 GMT"}, {"version": "v4", "created": "Thu, 22 Sep 2016 14:37:13 GMT"}, {"version": "v5", "created": "Fri, 25 Nov 2016 09:05:50 GMT"}, {"version": "v6", "created": "Fri, 16 Dec 2016 13:58:59 GMT"}], "update_date": "2016-12-19", "authors_parsed": [["Garcia-Gasulla", "D.", ""], ["B\u00e9jar", "J.", ""], ["Cort\u00e9s", "U.", ""], ["Ayguad\u00e9", "E.", ""], ["Labarta", "J.", ""], ["Suzumura", "T.", ""], ["Chen", "R.", ""]]}, {"id": "1507.08847", "submitter": "Fei  Guo", "authors": "Jiachen Yanga, Zhiyong Dinga, Fei Guoa, Huogen Wanga, Nick Hughesb", "title": "A novel multivariate performance optimization method based on sparse\n  coding and hyper-predictor learning", "comments": null, "journal-ref": null, "doi": "10.1016/j.neunet.2015.07.011", "report-no": null, "categories": "cs.LG cs.CV cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate the problem of optimization multivariate\nperformance measures, and propose a novel algorithm for it. Different from\ntraditional machine learning methods which optimize simple loss functions to\nlearn prediction function, the problem studied in this paper is how to learn\neffective hyper-predictor for a tuple of data points, so that a complex loss\nfunction corresponding to a multivariate performance measure can be minimized.\nWe propose to present the tuple of data points to a tuple of sparse codes via a\ndictionary, and then apply a linear function to compare a sparse code against a\ngive candidate class label. To learn the dictionary, sparse codes, and\nparameter of the linear function, we propose a joint optimization problem. In\nthis problem, the both the reconstruction error and sparsity of sparse code,\nand the upper bound of the complex loss function are minimized. Moreover, the\nupper bound of the loss function is approximated by the sparse codes and the\nlinear function parameter. To optimize this problem, we develop an iterative\nalgorithm based on descent gradient methods to learn the sparse codes and\nhyper-predictor parameter alternately. Experiment results on some benchmark\ndata sets show the advantage of the proposed methods over other\nstate-of-the-art algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2015 12:14:35 GMT"}], "update_date": "2015-08-03", "authors_parsed": [["Yanga", "Jiachen", ""], ["Dinga", "Zhiyong", ""], ["Guoa", "Fei", ""], ["Wanga", "Huogen", ""], ["Hughesb", "Nick", ""]]}, {"id": "1507.08861", "submitter": "Muhammet Bastan", "authors": "Fatih Calisir, Muhammet Bastan, Ozgur Ulusoy, Ugur Gudukbay", "title": "Mobile Multi-View Object Image Search", "comments": "Multimedia Tools and Applications, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  High user interaction capability of mobile devices can help improve the\naccuracy of mobile visual search systems. At query time, it is possible to\ncapture multiple views of an object from different viewing angles and at\ndifferent scales with the mobile device camera to obtain richer information\nabout the object compared to a single view and hence return more accurate\nresults. Motivated by this, we developed a mobile multi-view object image\nsearch system, using a client-server architecture. Multi-view images of objects\nacquired by the mobile clients are processed and local features are sent to the\nserver, which combines the query image representations with early/late fusion\nmethods based on bag-of-visual-words and sends back the query results. We\nperformed a comprehensive analysis of early and late fusion approaches using\nvarious similarity functions, on an existing single view and a new multi-view\nobject image database. The experimental results show that multi-view search\nprovides significantly better retrieval accuracy compared to single view\nsearch.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2015 13:02:23 GMT"}, {"version": "v2", "created": "Mon, 30 Apr 2018 14:23:01 GMT"}], "update_date": "2018-05-01", "authors_parsed": [["Calisir", "Fatih", ""], ["Bastan", "Muhammet", ""], ["Ulusoy", "Ozgur", ""], ["Gudukbay", "Ugur", ""]]}, {"id": "1507.08905", "submitter": "Ding Liu", "authors": "Zhaowen Wang, Ding Liu, Jianchao Yang, Wei Han, and Thomas Huang", "title": "Deep Networks for Image Super-Resolution with Sparse Prior", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning techniques have been successfully applied in many areas of\ncomputer vision, including low-level image restoration problems. For image\nsuper-resolution, several models based on deep neural networks have been\nrecently proposed and attained superior performance that overshadows all\nprevious handcrafted models. The question then arises whether large-capacity\nand data-driven models have become the dominant solution to the ill-posed\nsuper-resolution problem. In this paper, we argue that domain expertise\nrepresented by the conventional sparse coding model is still valuable, and it\ncan be combined with the key ingredients of deep learning to achieve further\nimproved results. We show that a sparse coding model particularly designed for\nsuper-resolution can be incarnated as a neural network, and trained in a\ncascaded structure from end to end. The interpretation of the network based on\nsparse coding leads to much more efficient and effective training, as well as a\nreduced model size. Our model is evaluated on a wide range of images, and shows\nclear advantage over existing state-of-the-art methods in terms of both\nrestoration accuracy and human subjective quality.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2015 14:56:30 GMT"}, {"version": "v2", "created": "Mon, 3 Aug 2015 18:23:43 GMT"}, {"version": "v3", "created": "Mon, 24 Aug 2015 01:45:18 GMT"}, {"version": "v4", "created": "Thu, 15 Oct 2015 19:02:26 GMT"}], "update_date": "2015-10-16", "authors_parsed": [["Wang", "Zhaowen", ""], ["Liu", "Ding", ""], ["Yang", "Jianchao", ""], ["Han", "Wei", ""], ["Huang", "Thomas", ""]]}, {"id": "1507.08958", "submitter": "Roman Fedorov", "authors": "Roman Fedorov, Piero Fraternali, Chiara Pasini, Marco Tagliasacchi", "title": "SnowWatch: Snow Monitoring through Acquisition and Analysis of\n  User-Generated Content", "comments": "IEEE International Conference on Multimedia and Expo, ICME 2015.\n  Accepted and presented technical demo proposal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a system for complementing snow phenomena monitoring with virtual\nmeasurements extracted from public visual content. The proposed system\nintegrates an automatic acquisition and analysis of photographs and webcam\nimages depicting Alpine mountains. In particular, the technical demonstration\nconsists in a web portal that interfaces the whole system with the population.\nIt acts as an entertaining photo-sharing social web site, acquiring at the same\ntime visual content necessary for environmental monitoring.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2015 17:41:55 GMT"}], "update_date": "2015-08-11", "authors_parsed": [["Fedorov", "Roman", ""], ["Fraternali", "Piero", ""], ["Pasini", "Chiara", ""], ["Tagliasacchi", "Marco", ""]]}]