[{"id": "1510.00098", "submitter": "Michael Xie", "authors": "Michael Xie, Neal Jean, Marshall Burke, David Lobell, Stefano Ermon", "title": "Transfer Learning from Deep Features for Remote Sensing and Poverty\n  Mapping", "comments": "In Proc. 30th AAAI Conference on Artificial Intelligence", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The lack of reliable data in developing countries is a major obstacle to\nsustainable development, food security, and disaster relief. Poverty data, for\nexample, is typically scarce, sparse in coverage, and labor-intensive to\nobtain. Remote sensing data such as high-resolution satellite imagery, on the\nother hand, is becoming increasingly available and inexpensive. Unfortunately,\nsuch data is highly unstructured and currently no techniques exist to\nautomatically extract useful insights to inform policy decisions and help\ndirect humanitarian efforts. We propose a novel machine learning approach to\nextract large-scale socioeconomic indicators from high-resolution satellite\nimagery. The main challenge is that training data is very scarce, making it\ndifficult to apply modern techniques such as Convolutional Neural Networks\n(CNN). We therefore propose a transfer learning approach where nighttime light\nintensities are used as a data-rich proxy. We train a fully convolutional CNN\nmodel to predict nighttime lights from daytime imagery, simultaneously learning\nfeatures that are useful for poverty prediction. The model learns filters\nidentifying different terrains and man-made structures, including roads,\nbuildings, and farmlands, without any supervision beyond nighttime lights. We\ndemonstrate that these learned features are highly informative for poverty\nmapping, even approaching the predictive performance of survey data collected\nin the field.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2015 03:04:29 GMT"}, {"version": "v2", "created": "Sat, 27 Feb 2016 23:21:48 GMT"}], "update_date": "2016-03-01", "authors_parsed": [["Xie", "Michael", ""], ["Jean", "Neal", ""], ["Burke", "Marshall", ""], ["Lobell", "David", ""], ["Ermon", "Stefano", ""]]}, {"id": "1510.00143", "submitter": "Ningning Zhao", "authors": "Ningning Zhao, Qi Wei, Adrian Basarab, Nicolas Dobigeon, Denis Kouame\n  and Jean-Yves Tourneret", "title": "Fast Single Image Super-Resolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of single image super-resolution (SR), which\nconsists of recovering a high resolution image from its blurred, decimated and\nnoisy version. The existing algorithms for single image SR use different\nstrategies to handle the decimation and blurring operators. In addition to the\ntraditional first-order gradient methods, recent techniques investigate\nsplitting-based methods dividing the SR problem into up-sampling and\ndeconvolution steps that can be easily solved. Instead of following this\nsplitting strategy, we propose to deal with the decimation and blurring\noperators simultaneously by taking advantage of their particular properties in\nthe frequency domain, leading to a new fast SR approach. Specifically, an\nanalytical solution can be obtained and implemented efficiently for the\nGaussian prior or any other regularization that can be formulated into an\n$\\ell_2$-regularized quadratic model, i.e., an $\\ell_2$-$\\ell_2$ optimization\nproblem. Furthermore, the flexibility of the proposed SR scheme is shown\nthrough the use of various priors/regularizations, ranging from generic image\npriors to learning-based approaches. In the case of non-Gaussian priors, we\nshow how the analytical solution derived from the Gaussian case can be embedded\nintotraditional splitting frameworks, allowing the computation cost of existing\nalgorithms to be decreased significantly. Simulation results conducted on\nseveral images with different priors illustrate the effectiveness of our fast\nSR approach compared with the existing techniques.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2015 08:47:28 GMT"}, {"version": "v2", "created": "Tue, 2 Feb 2016 09:51:18 GMT"}, {"version": "v3", "created": "Mon, 2 May 2016 13:40:30 GMT"}], "update_date": "2016-05-03", "authors_parsed": [["Zhao", "Ningning", ""], ["Wei", "Qi", ""], ["Basarab", "Adrian", ""], ["Dobigeon", "Nicolas", ""], ["Kouame", "Denis", ""], ["Tourneret", "Jean-Yves", ""]]}, {"id": "1510.00149", "submitter": "Song Han", "authors": "Song Han, Huizi Mao, William J. Dally", "title": "Deep Compression: Compressing Deep Neural Networks with Pruning, Trained\n  Quantization and Huffman Coding", "comments": "Published as a conference paper at ICLR 2016 (oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks are both computationally intensive and memory intensive,\nmaking them difficult to deploy on embedded systems with limited hardware\nresources. To address this limitation, we introduce \"deep compression\", a three\nstage pipeline: pruning, trained quantization and Huffman coding, that work\ntogether to reduce the storage requirement of neural networks by 35x to 49x\nwithout affecting their accuracy. Our method first prunes the network by\nlearning only the important connections. Next, we quantize the weights to\nenforce weight sharing, finally, we apply Huffman coding. After the first two\nsteps we retrain the network to fine tune the remaining connections and the\nquantized centroids. Pruning, reduces the number of connections by 9x to 13x;\nQuantization then reduces the number of bits that represent each connection\nfrom 32 to 5. On the ImageNet dataset, our method reduced the storage required\nby AlexNet by 35x, from 240MB to 6.9MB, without loss of accuracy. Our method\nreduced the size of VGG-16 by 49x from 552MB to 11.3MB, again with no loss of\naccuracy. This allows fitting the model into on-chip SRAM cache rather than\noff-chip DRAM memory. Our compression method also facilitates the use of\ncomplex neural networks in mobile applications where application size and\ndownload bandwidth are constrained. Benchmarked on CPU, GPU and mobile GPU,\ncompressed network has 3x to 4x layerwise speedup and 3x to 7x better energy\nefficiency.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2015 09:03:44 GMT"}, {"version": "v2", "created": "Tue, 27 Oct 2015 23:53:10 GMT"}, {"version": "v3", "created": "Fri, 20 Nov 2015 06:35:19 GMT"}, {"version": "v4", "created": "Tue, 19 Jan 2016 09:04:04 GMT"}, {"version": "v5", "created": "Mon, 15 Feb 2016 06:25:40 GMT"}], "update_date": "2016-02-16", "authors_parsed": [["Han", "Song", ""], ["Mao", "Huizi", ""], ["Dally", "William J.", ""]]}, {"id": "1510.00203", "submitter": "Kardi Teknomo", "authors": "R. Alampay and K. Teknomo", "title": "Data Association for an Adaptive Multi-target Particle Filter Tracking\n  System", "comments": null, "journal-ref": "Philippine Computing Journal Vol 7 (1) August 2012, p. 16-25", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel approach to improve the accuracy of tracking\nmultiple objects in a static scene using a particle filter system by\nintroducing a data association step, a state queue for the collection of\ntracked objects and adaptive parameters to the system. The data association\nstep makes use of the object detection phase and appearance model to determine\nif the approximated targets given by the particle filter step match the given\nset of detected objects. The remaining detected objects are used as information\nto instantiate new objects for tracking. State queues are also used for each\ntracked object to deal with occlusion events and occlusion recovery. Finally we\npresent how the parameters adjust to occlusion events. The adaptive property of\nthe system is also used for possible occlusion recovery. Results of the system\nare then compared to a ground truth data set for performance evaluation. Our\nsystem produced accurate results and was able to handle partially occluded\nobjects as well as proper occlusion recovery from tracking multiple objects\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2015 12:40:18 GMT"}], "update_date": "2015-10-02", "authors_parsed": [["Alampay", "R.", ""], ["Teknomo", "K.", ""]]}, {"id": "1510.00384", "submitter": "Greg Ongie", "authors": "Greg Ongie, Mathews Jacob", "title": "Off-the-Grid Recovery of Piecewise Constant Images from Few Fourier\n  Samples", "comments": "Accepted for publication in SIAM Journal on Imaging Sciences", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a method to recover a continuous domain representation of a\npiecewise constant two-dimensional image from few low-pass Fourier samples.\nAssuming the edge set of the image is localized to the zero set of a\ntrigonometric polynomial, we show the Fourier coefficients of the partial\nderivatives of the image satisfy a linear annihilation relation. We present\nnecessary and sufficient conditions for unique recovery of the image from\nfinite low-pass Fourier samples using the annihilation relation. We also\npropose a practical two-stage recovery algorithm which is robust to\nmodel-mismatch and noise. In the first stage we estimate a continuous domain\nrepresentation of the edge set of the image. In the second stage we perform an\nextrapolation in Fourier domain by a least squares two-dimensional linear\nprediction, which recovers the exact Fourier coefficients of the underlying\nimage. We demonstrate our algorithm on the super-resolution recovery of MRI\nphantoms and real MRI data from low-pass Fourier samples, which shows benefits\nover standard approaches for single-image super-resolution MRI.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2015 20:01:37 GMT"}, {"version": "v2", "created": "Thu, 21 Apr 2016 21:25:20 GMT"}], "update_date": "2016-04-25", "authors_parsed": [["Ongie", "Greg", ""], ["Jacob", "Mathews", ""]]}, {"id": "1510.00477", "submitter": "Jun-Yan Zhu", "authors": "Jun-Yan Zhu, Philipp Kr\\\"ahenb\\\"uhl, Eli Shechtman, Alexei A. Efros", "title": "Learning a Discriminative Model for the Perception of Realism in\n  Composite Images", "comments": "International Conference on Computer Vision (ICCV) 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  What makes an image appear realistic? In this work, we are answering this\nquestion from a data-driven perspective by learning the perception of visual\nrealism directly from large amounts of data. In particular, we train a\nConvolutional Neural Network (CNN) model that distinguishes natural photographs\nfrom automatically generated composite images. The model learns to predict\nvisual realism of a scene in terms of color, lighting and texture\ncompatibility, without any human annotations pertaining to it. Our model\noutperforms previous works that rely on hand-crafted heuristics, for the task\nof classifying realistic vs. unrealistic photos. Furthermore, we apply our\nlearned model to compute optimal parameters of a compositing method, to\nmaximize the visual realism score predicted by our CNN model. We demonstrate\nits advantage against existing methods via a human perception study.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2015 03:06:34 GMT"}], "update_date": "2015-10-05", "authors_parsed": [["Zhu", "Jun-Yan", ""], ["Kr\u00e4henb\u00fchl", "Philipp", ""], ["Shechtman", "Eli", ""], ["Efros", "Alexei A.", ""]]}, {"id": "1510.00479", "submitter": "Ishan Jindal", "authors": "Ishan Jindal, Shanmuganathan Raman", "title": "Effective Object Tracking in Unstructured Crowd Scenes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we are presenting a rotation variant Oriented Texture Curve\n(OTC) descriptor based mean shift algorithm for tracking an object in an\nunstructured crowd scene. The proposed algorithm works by first obtaining the\nOTC features for a manually selected object target, then a visual vocabulary is\ncreated by using all the OTC features of the target. The target histogram is\nobtained using codebook encoding method which is then used in mean shift\nframework to perform similarity search. Results are obtained on different\nvideos of challenging scenes and the comparison of the proposed approach with\nseveral state-of-the-art approaches are provided. The analysis shows the\nadvantages and limitations of the proposed approach for tracking an object in\nunstructured crowd scenes.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2015 03:19:16 GMT"}], "update_date": "2015-10-05", "authors_parsed": [["Jindal", "Ishan", ""], ["Raman", "Shanmuganathan", ""]]}, {"id": "1510.00542", "submitter": "Gaurav Sharma", "authors": "Gaurav Sharma and Frederic Jurie", "title": "Local Higher-Order Statistics (LHS) describing images with statistics of\n  local non-binarized pixel patterns", "comments": "CVIU preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new image representation for texture categorization and facial\nanalysis, relying on the use of higher-order local differential statistics as\nfeatures. It has been recently shown that small local pixel pattern\ndistributions can be highly discriminative while being extremely efficient to\ncompute, which is in contrast to the models based on the global structure of\nimages. Motivated by such works, we propose to use higher-order statistics of\nlocal non-binarized pixel patterns for the image description. The proposed\nmodel does not require either (i) user specified quantization of the space (of\npixel patterns) or (ii) any heuristics for discarding low occupancy volumes of\nthe space. We propose to use a data driven soft quantization of the space, with\nparametric mixture models, combined with higher-order statistics, based on\nFisher scores. We demonstrate that this leads to a more expressive\nrepresentation which, when combined with discriminatively learned classifiers\nand metrics, achieves state-of-the-art performance on challenging texture and\nfacial analysis datasets, in low complexity setup. Further, it is complementary\nto higher complexity features and when combined with them improves performance.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2015 09:41:39 GMT"}], "update_date": "2015-10-05", "authors_parsed": [["Sharma", "Gaurav", ""], ["Jurie", "Frederic", ""]]}, {"id": "1510.00562", "submitter": "Lin Sun", "authors": "Lin Sun, Kui Jia, Dit-Yan Yeung, Bertram E. Shi", "title": "Human Action Recognition using Factorized Spatio-Temporal Convolutional\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human actions in video sequences are three-dimensional (3D) spatio-temporal\nsignals characterizing both the visual appearance and motion dynamics of the\ninvolved humans and objects. Inspired by the success of convolutional neural\nnetworks (CNN) for image classification, recent attempts have been made to\nlearn 3D CNNs for recognizing human actions in videos. However, partly due to\nthe high complexity of training 3D convolution kernels and the need for large\nquantities of training videos, only limited success has been reported. This has\ntriggered us to investigate in this paper a new deep architecture which can\nhandle 3D signals more effectively. Specifically, we propose factorized\nspatio-temporal convolutional networks (FstCN) that factorize the original 3D\nconvolution kernel learning as a sequential process of learning 2D spatial\nkernels in the lower layers (called spatial convolutional layers), followed by\nlearning 1D temporal kernels in the upper layers (called temporal convolutional\nlayers). We introduce a novel transformation and permutation operator to make\nfactorization in FstCN possible. Moreover, to address the issue of sequence\nalignment, we propose an effective training and inference strategy based on\nsampling multiple video clips from a given action video sequence. We have\ntested FstCN on two commonly used benchmark datasets (UCF-101 and HMDB-51).\nWithout using auxiliary training videos to boost the performance, FstCN\noutperforms existing CNN based methods and achieves comparable performance with\na recent method that benefits from using auxiliary training videos.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2015 11:24:04 GMT"}], "update_date": "2015-10-05", "authors_parsed": [["Sun", "Lin", ""], ["Jia", "Kui", ""], ["Yeung", "Dit-Yan", ""], ["Shi", "Bertram E.", ""]]}, {"id": "1510.00745", "submitter": "Eric Orenstein", "authors": "Eric C. Orenstein, Oscar Beijbom, Emily E. Peacock and Heidi M. Sosik", "title": "WHOI-Plankton- A Large Scale Fine Grained Visual Recognition Benchmark\n  Dataset for Plankton Classification", "comments": "2 pages, 1 figure, presented at the Third Workshop on Fine-Grained\n  Visual Categorization at CVPR 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Planktonic organisms are of fundamental importance to marine ecosystems: they\nform the basis of the food web, provide the link between the atmosphere and the\ndeep ocean, and influence global-scale biogeochemical cycles. Scientists are\nincreasingly using imaging-based technologies to study these creatures in their\nnatural habit. Images from such systems provide an unique opportunity to model\nand understand plankton ecosystems, but the collected datasets can be enormous.\nThe Imaging FlowCytobot (IFCB) at Woods Hole Oceanographic Institution, for\nexample, is an \\emph{in situ} system that has been continuously imaging\nplankton since 2006. To date, it has generated more than 700 million samples.\nManual classification of such a vast image collection is impractical due to the\nsize of the data set. In addition, the annotation task is challenging due to\nthe large space of relevant classes, intra-class variability, and inter-class\nsimilarity. Methods for automated classification exist, but the accuracy is\noften below that of human experts. Here we introduce WHOI-Plankton: a large\nscale, fine-grained visual recognition dataset for plankton classification,\nwhich comprises over 3.4 million expert-labeled images across 70 classes. The\nlabeled image set is complied from over 8 years of near continuous data\ncollection with the IFCB at the Martha's Vineyard Coastal Observatory (MVCO).\nWe discuss relevant metrics for evaluation of classification performance and\nprovide results for a traditional method based on hand-engineered features and\ntwo methods based on convolutional neural networks.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2015 22:06:52 GMT"}], "update_date": "2015-10-06", "authors_parsed": [["Orenstein", "Eric C.", ""], ["Beijbom", "Oscar", ""], ["Peacock", "Emily E.", ""], ["Sosik", "Heidi M.", ""]]}, {"id": "1510.00771", "submitter": "Carlos Jaramillo", "authors": "Carlos Jaramillo (City University of New York, Graduate Center)", "title": "Design and Analysis of a Single-Camera Omnistereo Sensor for Quadrotor\n  Micro Aerial Vehicles (MAVs)", "comments": "49 pages, 22 figures, journal article draft", "journal-ref": "Sensors 16 (2016) 217", "doi": "10.3390/s16020217", "report-no": null, "categories": "cs.CV cs.RO cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe the design and 3D sensing performance of an omnidirectional\nstereo-vision system (omnistereo) as applied to Micro Aerial Vehicles (MAVs).\nThe proposed omnistereo model employs a monocular camera that is co-axially\naligned with a pair of hyperboloidal mirrors (folded catadioptric\nconfiguration). We show that this arrangement is practical for performing\nstereo-vision when mounted on top of propeller-based MAVs characterized by low\npayloads. The theoretical single viewpoint (SVP) constraint helps us derive\nanalytical solutions for the sensor's projective geometry and generate\nSVP-compliant panoramic images to compute 3D information from stereo\ncorrespondences (in a truly synchronous fashion). We perform an extensive\nanalysis on various system characteristics such as its size, catadioptric\nspatial resolution, field-of-view. In addition, we pose a probabilistic model\nfor uncertainty estimation of the depth from triangulation for skew\nback-projection rays. We expect to motivate the reproducibility of our solution\nsince it can be adapted (optimally) to other catadioptric-based omnistereo\nvision applications.\n", "versions": [{"version": "v1", "created": "Sat, 3 Oct 2015 02:53:55 GMT"}], "update_date": "2016-02-09", "authors_parsed": [["Jaramillo", "Carlos", "", "City University of New York, Graduate Center"]]}, {"id": "1510.00857", "submitter": "Ramazan Gokberk Cinbis", "authors": "Ramazan Gokberk Cinbis, Jakob Verbeek, Cordelia Schmid", "title": "Approximate Fisher Kernels of non-iid Image Models for Image\n  Categorization", "comments": "IEEE Transactions on Pattern Analysis and Machine Intelligence, in\n  press, 2015", "journal-ref": "IEEE Trans. on Pattern Analysis and Machine Intelligence, vol. 38,\n  no. 6, pp. 1084-1098, June 1 2016", "doi": "10.1109/TPAMI.2015.2484342", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The bag-of-words (BoW) model treats images as sets of local descriptors and\nrepresents them by visual word histograms. The Fisher vector (FV)\nrepresentation extends BoW, by considering the first and second order\nstatistics of local descriptors. In both representations local descriptors are\nassumed to be identically and independently distributed (iid), which is a poor\nassumption from a modeling perspective. It has been experimentally observed\nthat the performance of BoW and FV representations can be improved by employing\ndiscounting transformations such as power normalization. In this paper, we\nintroduce non-iid models by treating the model parameters as latent variables\nwhich are integrated out, rendering all local regions dependent. Using the\nFisher kernel principle we encode an image by the gradient of the data\nlog-likelihood w.r.t. the model hyper-parameters. Our models naturally generate\ndiscounting effects in the representations; suggesting that such\ntransformations have proven successful because they closely correspond to the\nrepresentations obtained for non-iid models. To enable tractable computation,\nwe rely on variational free-energy bounds to learn the hyper-parameters and to\ncompute approximate Fisher kernels. Our experimental evaluation results\nvalidate that our models lead to performance improvements comparable to using\npower normalization, as employed in state-of-the-art feature aggregation\nmethods.\n", "versions": [{"version": "v1", "created": "Sat, 3 Oct 2015 19:35:38 GMT"}], "update_date": "2016-05-30", "authors_parsed": [["Cinbis", "Ramazan Gokberk", ""], ["Verbeek", "Jakob", ""], ["Schmid", "Cordelia", ""]]}, {"id": "1510.00889", "submitter": "Kardi Teknomo", "authors": "Kardi Teknomo and Proceso Fernandez", "title": "Background Image Generation Using Boolean Operations", "comments": null, "journal-ref": "Philippine Computing Journal Vol 4 No 2, December 2009, pp. 43-49", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tracking moving objects from a video sequence requires segmentation of these\nobjects from the background image. However, getting the actual background image\nautomatically without object detection and using only the video is difficult.\nIn this paper, we describe a novel algorithm that generates background from\nreal world images without foreground detection. The algorithm assumes that the\nbackground image is shown in the majority of the video. Given this simple\nassumption, the method described in this paper is able to accurately generate,\nwith high probability, the background image from a video using only a small\nnumber of binary operations.\n", "versions": [{"version": "v1", "created": "Sun, 4 Oct 2015 00:34:56 GMT"}], "update_date": "2015-10-06", "authors_parsed": [["Teknomo", "Kardi", ""], ["Fernandez", "Proceso", ""]]}, {"id": "1510.00921", "submitter": "Chunhua Shen", "authors": "Lingqiao Liu, Chunhua Shen, Anton van den Hengel", "title": "Cross-convolutional-layer Pooling for Image Recognition", "comments": "Fixed typos. Journal extension of arXiv:1411.7466. Accepted to IEEE\n  Transactions on Pattern Analysis and Machine Intelligence", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies have shown that a Deep Convolutional Neural Network (DCNN)\npretrained on a large image dataset can be used as a universal image\ndescriptor, and that doing so leads to impressive performance for a variety of\nimage classification tasks. Most of these studies adopt activations from a\nsingle DCNN layer, usually the fully-connected layer, as the image\nrepresentation. In this paper, we proposed a novel way to extract image\nrepresentations from two consecutive convolutional layers: one layer is\nutilized for local feature extraction and the other serves as guidance to pool\nthe extracted features. By taking different viewpoints of convolutional layers,\nwe further develop two schemes to realize this idea. The first one directly\nuses convolutional layers from a DCNN. The second one applies the pretrained\nCNN on densely sampled image regions and treats the fully-connected activations\nof each image region as convolutional feature activations. We then train\nanother convolutional layer on top of that as the pooling-guidance\nconvolutional layer. By applying our method to three popular visual\nclassification tasks, we find our first scheme tends to perform better on the\napplications which need strong discrimination on subtle object patterns within\nsmall regions while the latter excels in the cases that require discrimination\non category-level patterns. Overall, the proposed method achieves superior\nperformance over existing ways of extracting image representations from a DCNN.\n", "versions": [{"version": "v1", "created": "Sun, 4 Oct 2015 10:27:36 GMT"}, {"version": "v2", "created": "Thu, 9 Jun 2016 07:37:09 GMT"}, {"version": "v3", "created": "Sun, 23 Oct 2016 05:48:16 GMT"}, {"version": "v4", "created": "Wed, 7 Dec 2016 00:00:42 GMT"}, {"version": "v5", "created": "Thu, 8 Dec 2016 01:31:05 GMT"}, {"version": "v6", "created": "Thu, 22 Dec 2016 04:43:19 GMT"}], "update_date": "2016-12-23", "authors_parsed": [["Liu", "Lingqiao", ""], ["Shen", "Chunhua", ""], ["Hengel", "Anton van den", ""]]}, {"id": "1510.00981", "submitter": "Byeongkeun Kang", "authors": "Byeongkeun Kang, Yeejin Lee, and Truong Q. Nguyen", "title": "Efficient Hand Articulations Tracking using Adaptive Hand Model and\n  Depth map", "comments": "Advances in Visual Computing: 11th International Symposium on Visual\n  Computing (ISVC'15)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-time hand articulations tracking is important for many applications such\nas interacting with virtual / augmented reality devices or tablets. However,\nmost of existing algorithms highly rely on expensive and high power-consuming\nGPUs to achieve real-time processing. Consequently, these systems are\ninappropriate for mobile and wearable devices. In this paper, we propose an\nefficient hand tracking system which does not require high performance GPUs. In\nour system, we track hand articulations by minimizing discrepancy between depth\nmap from sensor and computer-generated hand model. We also initialize hand pose\nat each frame using finger detection and classification. Our contributions are:\n(a) propose adaptive hand model to consider different hand shapes of users\nwithout generating personalized hand model; (b) improve the highly efficient\nframe initialization for robust tracking and automatic initialization; (c)\npropose hierarchical random sampling of pixels from each depth map to improve\ntracking accuracy while limiting required computations. To the best of our\nknowledge, it is the first system that achieves both automatic hand model\nadjustment and real-time tracking without using GPUs.\n", "versions": [{"version": "v1", "created": "Sun, 4 Oct 2015 20:34:15 GMT"}, {"version": "v2", "created": "Wed, 14 Oct 2015 19:06:16 GMT"}, {"version": "v3", "created": "Sun, 18 Oct 2015 03:21:48 GMT"}], "update_date": "2015-10-20", "authors_parsed": [["Kang", "Byeongkeun", ""], ["Lee", "Yeejin", ""], ["Nguyen", "Truong Q.", ""]]}, {"id": "1510.01018", "submitter": "Huimin Lu", "authors": "Huimin Lu, Yujie Li, Shota Nakashima, Seiichi Serikawa", "title": "Single Image Dehazing through Improved Atmospheric Light Estimation", "comments": "Multimedia Tools and Applications (2015)", "journal-ref": null, "doi": "10.1007/s11042-015-2977-7", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Image contrast enhancement for outdoor vision is important for smart car\nauxiliary transport systems. The video frames captured in poor weather\nconditions are often characterized by poor visibility. Most image dehazing\nalgorithms consider to use a hard threshold assumptions or user input to\nestimate atmospheric light. However, the brightest pixels sometimes are objects\nsuch as car lights or streetlights, especially for smart car auxiliary\ntransport systems. Simply using a hard threshold may cause a wrong estimation.\nIn this paper, we propose a single optimized image dehazing method that\nestimates atmospheric light efficiently and removes haze through the estimation\nof a semi-globally adaptive filter. The enhanced images are characterized with\nlittle noise and good exposure in dark regions. The textures and edges of the\nprocessed images are also enhanced significantly.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2015 02:50:42 GMT"}], "update_date": "2015-10-06", "authors_parsed": [["Lu", "Huimin", ""], ["Li", "Yujie", ""], ["Nakashima", "Shota", ""], ["Serikawa", "Seiichi", ""]]}, {"id": "1510.01027", "submitter": "Xinggang Wang", "authors": "Xinggang Wang, Zhuotun Zhu, Cong Yao, Xiang Bai", "title": "Relaxed Multiple-Instance SVM with Application to Object Discovery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple-instance learning (MIL) has served as an important tool for a wide\nrange of vision applications, for instance, image classification, object\ndetection, and visual tracking. In this paper, we propose a novel method to\nsolve the classical MIL problem, named relaxed multiple-instance SVM (RMI-SVM).\nWe treat the positiveness of instance as a continuous variable, use Noisy-OR\nmodel to enforce the MIL constraints, and jointly optimize the bag label and\ninstance label in a unified framework. The optimization problem can be\nefficiently solved using stochastic gradient decent. The extensive experiments\ndemonstrate that RMI-SVM consistently achieves superior performance on various\nbenchmarks for MIL. Moreover, we simply applied RMI-SVM to a challenging vision\ntask, common object discovery. The state-of-the-art results of object discovery\non Pascal VOC datasets further confirm the advantages of the proposed method.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2015 04:18:18 GMT"}], "update_date": "2015-10-06", "authors_parsed": [["Wang", "Xinggang", ""], ["Zhu", "Zhuotun", ""], ["Yao", "Cong", ""], ["Bai", "Xiang", ""]]}, {"id": "1510.01041", "submitter": "Gil Shapira", "authors": "Gil Shapira, Tal Hassner", "title": "GPU-Based Computation of 2D Least Median of Squares with Applications to\n  Fast and Robust Line Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The 2D Least Median of Squares (LMS) is a popular tool in robust regression\nbecause of its high breakdown point: up to half of the input data can be\ncontaminated with outliers without affecting the accuracy of the LMS estimator.\nThe complexity of 2D LMS estimation has been shown to be $\\Omega(n^2)$ where\n$n$ is the total number of points. This high theoretical complexity along with\nthe availability of graphics processing units (GPU) motivates the development\nof a fast, parallel, GPU-based algorithm for LMS computation. We present a CUDA\nbased algorithm for LMS computation and show it to be much faster than the\noptimal state of the art single threaded CPU algorithm. We begin by describing\nthe proposed method and analyzing its performance. We then demonstrate how it\ncan be used to modify the well-known Hough Transform (HT) in order to\nefficiently detect image lines in noisy images. Our method is compared with\nstandard HT-based line detection methods and shown to overcome their\nshortcomings in terms of both efficiency and accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2015 06:38:03 GMT"}], "update_date": "2015-10-06", "authors_parsed": [["Shapira", "Gil", ""], ["Hassner", "Tal", ""]]}, {"id": "1510.01077", "submitter": "Guy Gilboa", "authors": "Guy Gilboa, Michael Moeller, Martin Burger", "title": "Nonlinear Spectral Analysis via One-homogeneous Functionals - Overview\n  and Future Prospects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.SP cs.CV cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present in this paper the motivation and theory of nonlinear spectral\nrepresentations, based on convex regularizing functionals. Some comparisons and\nanalogies are drawn to the fields of signal processing, harmonic analysis and\nsparse representations. The basic approach, main results and initial\napplications are shown. A discussion of open problems and future directions\nconcludes this work.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2015 09:44:13 GMT"}], "update_date": "2015-10-06", "authors_parsed": [["Gilboa", "Guy", ""], ["Moeller", "Michael", ""], ["Burger", "Martin", ""]]}, {"id": "1510.01098", "submitter": "Boshra Rajaei", "authors": "Boshra Rajaei, Eric W. Tramel, Sylvain Gigan, Florent Krzakala,\n  Laurent Daudet", "title": "Intensity-only optical compressive imaging using a multiply scattering\n  material and a double phase retrieval approach", "comments": null, "journal-ref": "Proceedings of the 2016 IEEE International Conference on\n  Acoustics, Speech and Signal Processing (ICASSP) pages: 4054 - 4058", "doi": "10.1109/ICASSP.2016.7472439", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, the problem of compressive imaging is addressed using natural\nrandomization by means of a multiply scattering medium. To utilize the medium\nin this way, its corresponding transmission matrix must be estimated. To\ncalibrate the imager, we use a digital micromirror device (DMD) as a simple,\ncheap, and high-resolution binary intensity modulator. We propose a phase\nretrieval algorithm which is well adapted to intensity-only measurements on the\ncamera, and to the input binary intensity patterns, both to estimate the\ncomplex transmission matrix as well as image reconstruction. We demonstrate\npromising experimental results for the proposed algorithm using the MNIST\ndataset of handwritten digits as example images.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2015 11:07:30 GMT"}, {"version": "v2", "created": "Mon, 25 Jan 2016 14:35:44 GMT"}], "update_date": "2016-08-26", "authors_parsed": [["Rajaei", "Boshra", ""], ["Tramel", "Eric W.", ""], ["Gigan", "Sylvain", ""], ["Krzakala", "Florent", ""], ["Daudet", "Laurent", ""]]}, {"id": "1510.01113", "submitter": "Paul Guerrero", "authors": "Paul Guerrero, Niloy J. Mitra, Peter Wonka", "title": "RAID: A Relation-Augmented Image Descriptor", "comments": "Fixed affiliation and email address of first author", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As humans, we regularly interpret images based on the relations between image\nregions. For example, a person riding object X, or a plank bridging two\nobjects. Current methods provide limited support to search for images based on\nsuch relations. We present RAID, a relation-augmented image descriptor that\nsupports queries based on inter-region relations. The key idea of our\ndescriptor is to capture the spatial distribution of simple point-to-region\nrelationships to describe more complex relationships between two image regions.\nWe evaluate the proposed descriptor by querying into a large subset of the\nMicrosoft COCO database and successfully extract nontrivial images\ndemonstrating complex inter-region relations, which are easily missed or\nerroneously classified by existing methods.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2015 11:58:12 GMT"}, {"version": "v2", "created": "Tue, 6 Oct 2015 12:54:01 GMT"}], "update_date": "2015-10-07", "authors_parsed": [["Guerrero", "Paul", ""], ["Mitra", "Niloy J.", ""], ["Wonka", "Peter", ""]]}, {"id": "1510.01130", "submitter": "Laurent Hoeltgen", "authors": "Laurent Hoeltgen and Michael Breu{\\ss}", "title": "Bregman Iteration for Correspondence Problems: A Study of Optical Flow", "comments": "30 pages, 2 Figures, Correctecd several minor typos, extended\n  references to take recent developments into account", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bregman iterations are known to yield excellent results for denoising,\ndeblurring and compressed sensing tasks, but so far this technique has rarely\nbeen used for other image processing problems. In this paper we give a thorough\ndescription of the Bregman iteration, unifying thereby results of different\nauthors within a common framework. Then we show how to adapt the split Bregman\niteration, originally developed by Goldstein and Osher for image restoration\npurposes, to optical flow which is a fundamental correspondence problem in\ncomputer vision. We consider some classic and modern optical flow models and\npresent detailed algorithms that exhibit the benefits of the Bregman iteration.\nBy making use of the results of the Bregman framework, we address the issues of\nconvergence and error estimation for the algorithms. Numerical examples\ncomplement the theoretical part.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2015 12:44:00 GMT"}, {"version": "v2", "created": "Tue, 27 Sep 2016 13:58:47 GMT"}], "update_date": "2016-09-28", "authors_parsed": [["Hoeltgen", "Laurent", ""], ["Breu\u00df", "Michael", ""]]}, {"id": "1510.01148", "submitter": "Fanghui Liu", "authors": "Fanghui Liu, Tao Zhou, Irene Y.H. Gu and Jie Yang", "title": "Visual Tracking via Nonnegative Regularization Multiple Locality Coding", "comments": "8 pages, 5 figures, ICCVW", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel object tracking method based on approximated\nLocality-constrained Linear Coding (LLC). Rather than using a non-negativity\nconstraint on encoding coefficients to guarantee these elements nonnegative, in\nthis paper, the non-negativity constraint is substituted for a conventional\n$\\ell_2$ norm regularization term in approximated LLC to obtain the similar\nnonnegative effect. And we provide a detailed and adequate explanation in\ntheoretical analysis to clarify the rationality of this replacement. Instead of\nspecifying fixed K nearest neighbors to construct the local dictionary, a\nseries of different dictionaries with pre-defined numbers of nearest neighbors\nare selected. Weights of these various dictionaries are also learned from\napproximated LLC in the similar framework. In order to alleviate tracking\ndrifts, we propose a simple and efficient occlusion detection method. The\nocclusion detection criterion mainly depends on whether negative templates are\nselected to represent the severe occluded target. Both qualitative and\nquantitative evaluations on several challenging sequences show that the\nproposed tracking algorithm achieves favorable performance compared with other\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2015 13:41:36 GMT"}, {"version": "v2", "created": "Tue, 13 Oct 2015 02:27:48 GMT"}, {"version": "v3", "created": "Thu, 29 Oct 2015 11:05:04 GMT"}], "update_date": "2015-10-30", "authors_parsed": [["Liu", "Fanghui", ""], ["Zhou", "Tao", ""], ["Gu", "Irene Y. H.", ""], ["Yang", "Jie", ""]]}, {"id": "1510.01257", "submitter": "Yongxi Lu", "authors": "Yongxi Lu and Tara Javidi", "title": "Efficient Object Detection for High Resolution Images", "comments": null, "journal-ref": null, "doi": "10.1109/ALLERTON.2015.7447130", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient generation of high-quality object proposals is an essential step in\nstate-of-the-art object detection systems based on deep convolutional neural\nnetworks (DCNN) features. Current object proposal algorithms are\ncomputationally inefficient in processing high resolution images containing\nsmall objects, which makes them the bottleneck in object detection systems. In\nthis paper we present effective methods to detect objects for high resolution\nimages. We combine two complementary strategies. The first approach is to\npredict bounding boxes based on adjacent visual features. The second approach\nuses high level image features to guide a two-step search process that\nadaptively focuses on regions that are likely to contain small objects. We\nextract features required for the two strategies by utilizing a pre-trained\nDCNN model known as AlexNet. We demonstrate the effectiveness of our algorithm\nby showing its performance on a high-resolution image subset of the SUN 2012\nobject detection dataset.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2015 17:48:02 GMT"}], "update_date": "2016-04-14", "authors_parsed": [["Lu", "Yongxi", ""], ["Javidi", "Tara", ""]]}, {"id": "1510.01344", "submitter": "Mohammad Havaei", "authors": "Mohammad Havaei, Hugo Larochelle, Philippe Poulin, Pierre-Marc Jodoin", "title": "Within-Brain Classification for Brain Tumor Segmentation", "comments": null, "journal-ref": null, "doi": "10.1007/s11548-015-1311-1", "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: In this paper, we investigate a framework for interactive brain\ntumor segmentation which, at its core, treats the problem of interactive brain\ntumor segmentation as a machine learning problem.\n  Methods: This method has an advantage over typical machine learning methods\nfor this task where generalization is made across brains. The problem with\nthese methods is that they need to deal with intensity bias correction and\nother MRI-specific noise. In this paper, we avoid these issues by approaching\nthe problem as one of within brain generalization. Specifically, we propose a\nsemi-automatic method that segments a brain tumor by training and generalizing\nwithin that brain only, based on some minimum user interaction.\n  Conclusion: We investigate how adding spatial feature coordinates (i.e. $i$,\n$j$, $k$) to the intensity features can significantly improve the performance\nof different classification methods such as SVM, kNN and random forests. This\nwould only be possible within an interactive framework. We also investigate the\nuse of a more appropriate kernel and the adaptation of hyper-parameters\nspecifically for each brain.\n  Results: As a result of these experiments, we obtain an interactive method\nwhose results reported on the MICCAI-BRATS 2013 dataset are the second most\naccurate compared to published methods, while using significantly less memory\nand processing power than most state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2015 20:32:04 GMT"}], "update_date": "2016-05-20", "authors_parsed": [["Havaei", "Mohammad", ""], ["Larochelle", "Hugo", ""], ["Poulin", "Philippe", ""], ["Jodoin", "Pierre-Marc", ""]]}, {"id": "1510.01401", "submitter": "Hon-Leung Lee", "authors": "Sameer Agarwal, Hon-Leung Lee, Bernd Sturmfels, Rekha R. Thomas", "title": "On the Existence of Epipolar Matrices", "comments": "19 pages, 2 figures; This paper is related to our previous paper\n  arXiv:1407.5367. However, the two papers differ enough in their focus and\n  results that they merit being archived separately", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV math.AG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the foundational question of the existence of a\nfundamental (resp. essential) matrix given $m$ point correspondences in two\nviews. We present a complete answer for the existence of fundamental matrices\nfor any value of $m$. Using examples we disprove the widely held beliefs that\nfundamental matrices always exist whenever $m \\leq 7$. At the same time, we\nprove that they exist unconditionally when $m \\leq 5$. Under a mild genericity\ncondition, we show that an essential matrix always exists when $m \\leq 4$. We\nalso characterize the six and seven point configurations in two views for which\nall matrices satisfying the epipolar constraint have rank at most one.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2015 00:00:27 GMT"}], "update_date": "2015-10-07", "authors_parsed": [["Agarwal", "Sameer", ""], ["Lee", "Hon-Leung", ""], ["Sturmfels", "Bernd", ""], ["Thomas", "Rekha R.", ""]]}, {"id": "1510.01431", "submitter": "Alexander Mathews", "authors": "Alexander Mathews, Lexing Xie, Xuming He", "title": "SentiCap: Generating Image Descriptions with Sentiments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent progress on image recognition and language modeling is making\nautomatic description of image content a reality. However, stylized,\nnon-factual aspects of the written description are missing from the current\nsystems. One such style is descriptions with emotions, which is commonplace in\neveryday communication, and influences decision-making and interpersonal\nrelationships. We design a system to describe an image with emotions, and\npresent a model that automatically generates captions with positive or negative\nsentiments. We propose a novel switching recurrent neural network with\nword-level regularization, which is able to produce emotional image captions\nusing only 2000+ training sentences containing sentiments. We evaluate the\ncaptions with different automatic and crowd-sourcing metrics. Our model\ncompares favourably in common quality metrics for image captioning. In 84.6% of\ncases the generated positive captions were judged as being at least as\ndescriptive as the factual captions. Of these positive captions 88% were\nconfirmed by the crowd-sourced workers as having the appropriate sentiment.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2015 04:57:47 GMT"}, {"version": "v2", "created": "Sun, 13 Dec 2015 23:03:23 GMT"}], "update_date": "2015-12-15", "authors_parsed": [["Mathews", "Alexander", ""], ["Xie", "Lexing", ""], ["He", "Xuming", ""]]}, {"id": "1510.01440", "submitter": "Baoyuan Wang", "authors": "Ruobing Wu and Baoyuan Wang and Wenping Wang and Yizhou Yu", "title": "Harvesting Discriminative Meta Objects with Deep CNN Features for Scene\n  Classification", "comments": "To Appear in ICCV 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work on scene classification still makes use of generic CNN features\nin a rudimentary manner. In this ICCV 2015 paper, we present a novel pipeline\nbuilt upon deep CNN features to harvest discriminative visual objects and parts\nfor scene classification. We first use a region proposal technique to generate\na set of high-quality patches potentially containing objects, and apply a\npre-trained CNN to extract generic deep features from these patches. Then we\nperform both unsupervised and weakly supervised learning to screen these\npatches and discover discriminative ones representing category-specific objects\nand parts. We further apply discriminative clustering enhanced with local CNN\nfine-tuning to aggregate similar objects and parts into groups, called meta\nobjects. A scene image representation is constructed by pooling the feature\nresponse maps of all the learned meta objects at multiple spatial scales. We\nhave confirmed that the scene image representation obtained using this new\npipeline is capable of delivering state-of-the-art performance on two popular\nscene benchmark datasets, MIT Indoor 67~\\cite{MITIndoor67} and\nSun397~\\cite{Sun397}\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2015 05:59:11 GMT"}], "update_date": "2015-10-07", "authors_parsed": [["Wu", "Ruobing", ""], ["Wang", "Baoyuan", ""], ["Wang", "Wenping", ""], ["Yu", "Yizhou", ""]]}, {"id": "1510.01442", "submitter": "Baoyuan Wang", "authors": "Huan Yang and Baoyuan Wang and Stephen Lin and David Wipf and Minyi\n  Guo and Baining Guo", "title": "Unsupervised Extraction of Video Highlights Via Robust Recurrent\n  Auto-encoders", "comments": "To Appear in ICCV 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the growing popularity of short-form video sharing platforms such as\n\\em{Instagram} and \\em{Vine}, there has been an increasing need for techniques\nthat automatically extract highlights from video. Whereas prior works have\napproached this problem with heuristic rules or supervised learning, we present\nan unsupervised learning approach that takes advantage of the abundance of\nuser-edited videos on social media websites such as YouTube. Based on the idea\nthat the most significant sub-events within a video class are commonly present\namong edited videos while less interesting ones appear less frequently, we\nidentify the significant sub-events via a robust recurrent auto-encoder trained\non a collection of user-edited videos queried for each particular class of\ninterest. The auto-encoder is trained using a proposed shrinking exponential\nloss function that makes it robust to noise in the web-crawled training data,\nand is configured with bidirectional long short term memory\n(LSTM)~\\cite{LSTM:97} cells to better model the temporal structure of highlight\nsegments. Different from supervised techniques, our method can infer highlights\nusing only a set of downloaded edited videos, without also needing their\npre-edited counterparts which are rarely available online. Extensive\nexperiments indicate the promise of our proposed solution in this challenging\nunsupervised settin\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2015 06:07:50 GMT"}], "update_date": "2015-10-07", "authors_parsed": [["Yang", "Huan", ""], ["Wang", "Baoyuan", ""], ["Lin", "Stephen", ""], ["Wipf", "David", ""], ["Guo", "Minyi", ""], ["Guo", "Baining", ""]]}, {"id": "1510.01490", "submitter": "Carsten Gottschlich", "authors": "Duy Hoang Thai and Carsten Gottschlich", "title": "Directional Global Three-part Image Decomposition", "comments": null, "journal-ref": "EURASIP Journal on Image and Video Processing, vol. 2016, no. 12,\n  pp. 1-20, Mar. 2016", "doi": "10.1186/s13640-016-0110-0", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the task of image decomposition and we introduce a new model\ncoined directional global three-part decomposition (DG3PD) for solving it. As\nkey ingredients of the DG3PD model, we introduce a discrete multi-directional\ntotal variation norm and a discrete multi-directional G-norm. Using these novel\nnorms, the proposed discrete DG3PD model can decompose an image into two parts\nor into three parts. Existing models for image decomposition by Vese and Osher,\nby Aujol and Chambolle, by Starck et al., and by Thai and Gottschlich are\nincluded as special cases in the new model. Decomposition of an image by DG3PD\nresults in a cartoon image, a texture image and a residual image. Advantages of\nthe DG3PD model over existing ones lie in the properties enforced on the\ncartoon and texture images. The geometric objects in the cartoon image have a\nvery smooth surface and sharp edges. The texture image yields oscillating\npatterns on a defined scale which is both smooth and sparse. Moreover, the\nDG3PD method achieves the goal of perfect reconstruction by summation of all\ncomponents better than the other considered methods. Relevant applications of\nDG3PD are a novel way of image compression as well as feature extraction for\napplications such as latent fingerprint processing and optical character\nrecognition.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2015 09:11:47 GMT"}], "update_date": "2016-04-08", "authors_parsed": [["Thai", "Duy Hoang", ""], ["Gottschlich", "Carsten", ""]]}, {"id": "1510.01544", "submitter": "Efstratios Gavves Dr.", "authors": "Efstratios Gavves and Thomas Mensink and Tatiana Tommasi and Cees G.M.\n  Snoek and Tinne Tuytelaars", "title": "Active Transfer Learning with Zero-Shot Priors: Reusing Past Datasets\n  for Future Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How can we reuse existing knowledge, in the form of available datasets, when\nsolving a new and apparently unrelated target task from a set of unlabeled\ndata? In this work we make a first contribution to answer this question in the\ncontext of image classification. We frame this quest as an active learning\nproblem and use zero-shot classifiers to guide the learning process by linking\nthe new task to the existing classifiers. By revisiting the dual formulation of\nadaptive SVM, we reveal two basic conditions to choose greedily only the most\nrelevant samples to be annotated. On this basis we propose an effective active\nlearning algorithm which learns the best possible target classification model\nwith minimum human labeling effort. Extensive experiments on two challenging\ndatasets show the value of our approach compared to the state-of-the-art active\nlearning methodologies, as well as its potential to reuse past datasets with\nminimal effort for future tasks.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2015 12:06:19 GMT"}], "update_date": "2015-10-07", "authors_parsed": [["Gavves", "Efstratios", ""], ["Mensink", "Thomas", ""], ["Tommasi", "Tatiana", ""], ["Snoek", "Cees G. M.", ""], ["Tuytelaars", "Tinne", ""]]}, {"id": "1510.01553", "submitter": "Dan Xu", "authors": "Dan Xu, Elisa Ricci, Yan Yan, Jingkuan Song, Nicu Sebe", "title": "Learning Deep Representations of Appearance and Motion for Anomalous\n  Event Detection", "comments": "Oral paper in BMVC 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel unsupervised deep learning framework for anomalous event\ndetection in complex video scenes. While most existing works merely use\nhand-crafted appearance and motion features, we propose Appearance and Motion\nDeepNet (AMDN) which utilizes deep neural networks to automatically learn\nfeature representations. To exploit the complementary information of both\nappearance and motion patterns, we introduce a novel double fusion framework,\ncombining both the benefits of traditional early fusion and late fusion\nstrategies. Specifically, stacked denoising autoencoders are proposed to\nseparately learn both appearance and motion features as well as a joint\nrepresentation (early fusion). Based on the learned representations, multiple\none-class SVM models are used to predict the anomaly scores of each input,\nwhich are then integrated with a late fusion strategy for final anomaly\ndetection. We evaluate the proposed method on two publicly available video\nsurveillance datasets, showing competitive performance with respect to state of\nthe art approaches.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2015 12:42:55 GMT"}], "update_date": "2015-10-07", "authors_parsed": [["Xu", "Dan", ""], ["Ricci", "Elisa", ""], ["Yan", "Yan", ""], ["Song", "Jingkuan", ""], ["Sebe", "Nicu", ""]]}, {"id": "1510.01576", "submitter": "Daniel Castro Chin", "authors": "Daniel Castro, Steven Hickson, Vinay Bettadapura, Edison Thomaz,\n  Gregory Abowd, Henrik Christensen, Irfan Essa", "title": "Predicting Daily Activities From Egocentric Images Using Deep Learning", "comments": "8 pages", "journal-ref": "ISWC '15 Proceedings of the 2015 ACM International Symposium on\n  Wearable Computers - Pages 75-82", "doi": "10.1145/2802083.2808398", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method to analyze images taken from a passive egocentric\nwearable camera along with the contextual information, such as time and day of\nweek, to learn and predict everyday activities of an individual. We collected a\ndataset of 40,103 egocentric images over a 6 month period with 19 activity\nclasses and demonstrate the benefit of state-of-the-art deep learning\ntechniques for learning and predicting daily activities. Classification is\nconducted using a Convolutional Neural Network (CNN) with a classification\nmethod we introduce called a late fusion ensemble. This late fusion ensemble\nincorporates relevant contextual information and increases our classification\naccuracy. Our technique achieves an overall accuracy of 83.07% in predicting a\nperson's activity across the 19 activity classes. We also demonstrate some\npromising results from two additional users by fine-tuning the classifier with\none day of training data.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2015 13:56:50 GMT"}], "update_date": "2015-10-07", "authors_parsed": [["Castro", "Daniel", ""], ["Hickson", "Steven", ""], ["Bettadapura", "Vinay", ""], ["Thomaz", "Edison", ""], ["Abowd", "Gregory", ""], ["Christensen", "Henrik", ""], ["Essa", "Irfan", ""]]}, {"id": "1510.01628", "submitter": "Panagiotis Traganitis", "authors": "Panagiotis A. Traganitis, Konstantinos Slavakis, Georgios B. Giannakis", "title": "Large-scale subspace clustering using sketching and validation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The nowadays massive amounts of generated and communicated data present major\nchallenges in their processing. While capable of successfully classifying\nnonlinearly separable objects in various settings, subspace clustering (SC)\nmethods incur prohibitively high computational complexity when processing\nlarge-scale data. Inspired by the random sampling and consensus (RANSAC)\napproach to robust regression, the present paper introduces a randomized scheme\nfor SC, termed sketching and validation (SkeVa-)SC, tailored for large-scale\ndata. At the heart of SkeVa-SC lies a randomized scheme for approximating the\nunderlying probability density function of the observed data by kernel\nsmoothing arguments. Sparsity in data representations is also exploited to\nreduce the computational burden of SC, while achieving high clustering\naccuracy. Performance analysis as well as extensive numerical tests on\nsynthetic and real data corroborate the potential of SkeVa-SC and its\ncompetitive performance relative to state-of-the-art scalable SC approaches.\nKeywords: Subspace clustering, big data, kernel smoothing, randomization,\nsketching, validation, sparsity.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2015 15:34:32 GMT"}], "update_date": "2015-10-07", "authors_parsed": [["Traganitis", "Panagiotis A.", ""], ["Slavakis", "Konstantinos", ""], ["Giannakis", "Georgios B.", ""]]}, {"id": "1510.01648", "submitter": "George Chen", "authors": "George Chen, Devavrat Shah, Polina Golland", "title": "A Latent Source Model for Patch-Based Image Segmentation", "comments": "International Conference on Medical Image Computing and Computer\n  Assisted Interventions 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the popularity and empirical success of patch-based nearest-neighbor\nand weighted majority voting approaches to medical image segmentation, there\nhas been no theoretical development on when, why, and how well these\nnonparametric methods work. We bridge this gap by providing a theoretical\nperformance guarantee for nearest-neighbor and weighted majority voting\nsegmentation under a new probabilistic model for patch-based image\nsegmentation. Our analysis relies on a new local property for how similar\nnearby patches are, and fuses existing lines of work on modeling natural\nimagery patches and theory for nonparametric classification. We use the model\nto derive a new patch-based segmentation algorithm that iterates between\ninferring local label patches and merging these local segmentations to produce\na globally consistent image segmentation. Many existing patch-based algorithms\narise as special cases of the new algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2015 16:28:44 GMT"}], "update_date": "2015-10-07", "authors_parsed": [["Chen", "George", ""], ["Shah", "Devavrat", ""], ["Golland", "Polina", ""]]}, {"id": "1510.01663", "submitter": "Kiran Kumar Vupparaboina", "authors": "Kiran Kumar Vupparaboina, Kamala Raghavan and Soumya Jana", "title": "Euclidean Auto Calibration of Camera Networks: Baseline Constraint\n  Removes Scale Ambiguity", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Metric auto calibration of a camera network from multiple views has been\nreported by several authors. Resulting 3D reconstruction recovers shape\nfaithfully, but not scale. However, preservation of scale becomes critical in\napplications, such as multi-party telepresence, where multiple 3D scenes need\nto be fused into a single coordinate system. In this context, we propose a\ncamera network configuration that includes a stereo pair with known baseline\nseparation, and analytically demonstrate Euclidean auto calibration of such\nnetwork under mild conditions. Further, we experimentally validate our theory\nusing a four-camera network. Importantly, our method not only recovers scale,\nbut also compares favorably with the well known Zhang and Pollefeys methods in\nterms of shape recovery.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2015 16:56:22 GMT"}], "update_date": "2015-10-07", "authors_parsed": [["Vupparaboina", "Kiran Kumar", ""], ["Raghavan", "Kamala", ""], ["Jana", "Soumya", ""]]}, {"id": "1510.01722", "submitter": "Vikas Sindhwani", "authors": "Vikas Sindhwani and Tara N. Sainath and Sanjiv Kumar", "title": "Structured Transforms for Small-Footprint Deep Learning", "comments": "To appear in NIPS 2015; 9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the task of building compact deep learning pipelines suitable for\ndeployment on storage and power constrained mobile devices. We propose a\nunified framework to learn a broad family of structured parameter matrices that\nare characterized by the notion of low displacement rank. Our structured\ntransforms admit fast function and gradient evaluation, and span a rich range\nof parameter sharing configurations whose statistical modeling capacity can be\nexplicitly tuned along a continuum from structured to unstructured.\nExperimental results show that these transforms can significantly accelerate\ninference and forward/backward passes during training, and offer superior\naccuracy-compactness-speed tradeoffs in comparison to a number of existing\ntechniques. In keyword spotting applications in mobile speech recognition, our\nmethods are much more effective than standard linear low-rank bottleneck layers\nand nearly retain the performance of state of the art models, while providing\nmore than 3.5-fold compression.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2015 19:42:22 GMT"}], "update_date": "2015-10-07", "authors_parsed": [["Sindhwani", "Vikas", ""], ["Sainath", "Tara N.", ""], ["Kumar", "Sanjiv", ""]]}, {"id": "1510.01972", "submitter": "Guillermo Gallego Bonet", "authors": "Guillermo Gallego, Christian Forster, Elias Mueggler, Davide\n  Scaramuzza", "title": "Event-based Camera Pose Tracking using a Generative Event Model", "comments": "7 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Event-based vision sensors mimic the operation of biological retina and they\nrepresent a major paradigm shift from traditional cameras. Instead of providing\nframes of intensity measurements synchronously, at artificially chosen rates,\nevent-based cameras provide information on brightness changes asynchronously,\nwhen they occur. Such non-redundant pieces of information are called \"events\".\nThese sensors overcome some of the limitations of traditional cameras (response\ntime, bandwidth and dynamic range) but require new methods to deal with the\ndata they output. We tackle the problem of event-based camera localization in a\nknown environment, without additional sensing, using a probabilistic generative\nevent model in a Bayesian filtering framework. Our main contribution is the\ndesign of the likelihood function used in the filter to process the observed\nevents. Based on the physical characteristics of the sensor and on empirical\nevidence of the Gaussian-like distribution of spiked events with respect to the\nbrightness change, we propose to use the contrast residual as a measure of how\nwell the estimated pose of the event-based camera and the environment explain\nthe observed events. The filter allows for localization in the general case of\nsix degrees-of-freedom motions.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2015 14:52:08 GMT"}], "update_date": "2015-10-08", "authors_parsed": [["Gallego", "Guillermo", ""], ["Forster", "Christian", ""], ["Mueggler", "Elias", ""], ["Scaramuzza", "Davide", ""]]}, {"id": "1510.02055", "submitter": "Akshaya Mishra Dr", "authors": "Justin A. Eichel, Akshaya Mishra, Nicholas Miller, Nicholas Jankovic,\n  Mohan A. Thomas, Tyler Abbott, Douglas Swanson, Joel Keller", "title": "Diverse Large-Scale ITS Dataset Created from Continuous Learning for\n  Real-Time Vehicle Detection", "comments": "13 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In traffic engineering, vehicle detectors are trained on limited datasets\nresulting in poor accuracy when deployed in real world applications. Annotating\nlarge-scale high quality datasets is challenging. Typically, these datasets\nhave limited diversity; they do not reflect the real-world operating\nenvironment. There is a need for a large-scale, cloud based positive and\nnegative mining (PNM) process and a large-scale learning and evaluation system\nfor the application of traffic event detection. The proposed positive and\nnegative mining process addresses the quality of crowd sourced ground truth\ndata through machine learning review and human feedback mechanisms. The\nproposed learning and evaluation system uses a distributed cloud computing\nframework to handle data-scaling issues associated with large numbers of\nsamples and a high-dimensional feature space. The system is trained using\nAdaBoost on $1,000,000$ Haar-like features extracted from $70,000$ annotated\nvideo frames. The trained real-time vehicle detector achieves an accuracy of at\nleast $95\\%$ for $1/2$ and about $78\\%$ for $19/20$ of the time when tested on\napproximately $7,500,000$ video frames. At the end of 2015, the dataset is\nexpect to have over one billion annotated video frames.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2015 18:34:36 GMT"}], "update_date": "2015-10-08", "authors_parsed": [["Eichel", "Justin A.", ""], ["Mishra", "Akshaya", ""], ["Miller", "Nicholas", ""], ["Jankovic", "Nicholas", ""], ["Thomas", "Mohan A.", ""], ["Abbott", "Tyler", ""], ["Swanson", "Douglas", ""], ["Keller", "Joel", ""]]}, {"id": "1510.02071", "submitter": "Vinay Bettadapura", "authors": "Vinay Bettadapura, Grant Schindler, Thomaz Plotz, Irfan Essa", "title": "Augmenting Bag-of-Words: Data-Driven Discovery of Temporal and\n  Structural Information for Activity Recognition", "comments": "8 pages", "journal-ref": "Proceedings of the 2013 IEEE Conference on Computer Vision and\n  Pattern Recognition (CVPR 2013) -- Pages 2619 - 2626", "doi": "10.1109/CVPR.2013.338", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present data-driven techniques to augment Bag of Words (BoW) models, which\nallow for more robust modeling and recognition of complex long-term activities,\nespecially when the structure and topology of the activities are not known a\npriori. Our approach specifically addresses the limitations of standard BoW\napproaches, which fail to represent the underlying temporal and causal\ninformation that is inherent in activity streams. In addition, we also propose\nthe use of randomly sampled regular expressions to discover and encode patterns\nin activities. We demonstrate the effectiveness of our approach in experimental\nevaluations where we successfully recognize activities and detect anomalies in\nfour complex datasets.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2015 19:37:11 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Bettadapura", "Vinay", ""], ["Schindler", "Grant", ""], ["Plotz", "Thomaz", ""], ["Essa", "Irfan", ""]]}, {"id": "1510.02073", "submitter": "Vinay Bettadapura", "authors": "Vinay Bettadapura, Irfan Essa, Caroline Pantofaru", "title": "Egocentric Field-of-View Localization Using First-Person Point-of-View\n  Devices", "comments": "8 pages in Proceedings of the 2015 IEEE Winter Conference on\n  Applications of Computer Vision (WACV 2015)", "journal-ref": null, "doi": "10.1109/WACV.2015.89", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a technique that uses images, videos and sensor data taken from\nfirst-person point-of-view devices to perform egocentric field-of-view (FOV)\nlocalization. We define egocentric FOV localization as capturing the visual\ninformation from a person's field-of-view in a given environment and\ntransferring this information onto a reference corpus of images and videos of\nthe same space, hence determining what a person is attending to. Our method\nmatches images and video taken from the first-person perspective with the\nreference corpus and refines the results using the first-person's head\norientation information obtained using the device sensors. We demonstrate\nsingle and multi-user egocentric FOV localization in different indoor and\noutdoor environments with applications in augmented reality, event\nunderstanding and studying social interactions.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2015 19:39:26 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Bettadapura", "Vinay", ""], ["Essa", "Irfan", ""], ["Pantofaru", "Caroline", ""]]}, {"id": "1510.02078", "submitter": "Vinay Bettadapura", "authors": "Vinay Bettadapura, Edison Thomaz, Aman Parnami, Gregory Abowd, Irfan\n  Essa", "title": "Leveraging Context to Support Automated Food Recognition in Restaurants", "comments": "8 pages in Proceedings of the 2015 IEEE Winter Conference on\n  Applications of Computer Vision (WACV 2015)", "journal-ref": null, "doi": "10.1109/WACV.2015.83", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The pervasiveness of mobile cameras has resulted in a dramatic increase in\nfood photos, which are pictures reflecting what people eat. In this paper, we\nstudy how taking pictures of what we eat in restaurants can be used for the\npurpose of automating food journaling. We propose to leverage the context of\nwhere the picture was taken, with additional information about the restaurant,\navailable online, coupled with state-of-the-art computer vision techniques to\nrecognize the food being consumed. To this end, we demonstrate image-based\nrecognition of foods eaten in restaurants by training a classifier with images\nfrom restaurant's online menu databases. We evaluate the performance of our\nsystem in unconstrained, real-world settings with food images taken in 10\nrestaurants across 5 different types of food (American, Indian, Italian,\nMexican and Thai).\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2015 19:51:23 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Bettadapura", "Vinay", ""], ["Thomaz", "Edison", ""], ["Parnami", "Aman", ""], ["Abowd", "Gregory", ""], ["Essa", "Irfan", ""]]}, {"id": "1510.02131", "submitter": "Forrest Iandola", "authors": "Forrest N. Iandola, Anting Shen, Peter Gao, Kurt Keutzer", "title": "DeepLogo: Hitting Logo Recognition with the Deep Neural Network Hammer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, there has been a flurry of industrial activity around logo\nrecognition, such as Ditto's service for marketers to track their brands in\nuser-generated images, and LogoGrab's mobile app platform for logo recognition.\nHowever, relatively little academic or open-source logo recognition progress\nhas been made in the last four years. Meanwhile, deep convolutional neural\nnetworks (DCNNs) have revolutionized a broad range of object recognition\napplications. In this work, we apply DCNNs to logo recognition. We propose\nseveral DCNN architectures, with which we surpass published state-of-art\naccuracy on a popular logo recognition dataset.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2015 21:01:34 GMT"}], "update_date": "2015-10-09", "authors_parsed": [["Iandola", "Forrest N.", ""], ["Shen", "Anting", ""], ["Gao", "Peter", ""], ["Keutzer", "Kurt", ""]]}, {"id": "1510.02173", "submitter": "John-Alexander Assael", "authors": "John-Alexander M. Assael, Niklas Wahlstr\\\"om, Thomas B. Sch\\\"on, Marc\n  Peter Deisenroth", "title": "Data-Efficient Learning of Feedback Policies from Image Pixels using\n  Deep Dynamical Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data-efficient reinforcement learning (RL) in continuous state-action spaces\nusing very high-dimensional observations remains a key challenge in developing\nfully autonomous systems. We consider a particularly important instance of this\nchallenge, the pixels-to-torques problem, where an RL agent learns a\nclosed-loop control policy (\"torques\") from pixel information only. We\nintroduce a data-efficient, model-based reinforcement learning algorithm that\nlearns such a closed-loop policy directly from pixel information. The key\ningredient is a deep dynamical model for learning a low-dimensional feature\nembedding of images jointly with a predictive model in this low-dimensional\nfeature space. Joint learning is crucial for long-term predictions, which lie\nat the core of the adaptive nonlinear model predictive control strategy that we\nuse for closed-loop control. Compared to state-of-the-art RL methods for\ncontinuous states and actions, our approach learns quickly, scales to\nhigh-dimensional state spaces, is lightweight and an important step toward\nfully autonomous end-to-end learning from pixels to torques.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2015 00:20:42 GMT"}, {"version": "v2", "created": "Fri, 9 Oct 2015 15:21:01 GMT"}], "update_date": "2015-10-12", "authors_parsed": [["Assael", "John-Alexander M.", ""], ["Wahlstr\u00f6m", "Niklas", ""], ["Sch\u00f6n", "Thomas B.", ""], ["Deisenroth", "Marc Peter", ""]]}, {"id": "1510.02192", "submitter": "Eric Tzeng", "authors": "Eric Tzeng, Judy Hoffman, Trevor Darrell, Kate Saenko", "title": "Simultaneous Deep Transfer Across Domains and Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent reports suggest that a generic supervised deep CNN model trained on a\nlarge-scale dataset reduces, but does not remove, dataset bias. Fine-tuning\ndeep models in a new domain can require a significant amount of labeled data,\nwhich for many applications is simply not available. We propose a new CNN\narchitecture to exploit unlabeled and sparsely labeled target domain data. Our\napproach simultaneously optimizes for domain invariance to facilitate domain\ntransfer and uses a soft label distribution matching loss to transfer\ninformation between tasks. Our proposed adaptation method offers empirical\nperformance which exceeds previously published results on two standard\nbenchmark visual domain adaptation tasks, evaluated across supervised and\nsemi-supervised adaptation settings.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2015 03:42:45 GMT"}], "update_date": "2015-10-09", "authors_parsed": [["Tzeng", "Eric", ""], ["Hoffman", "Judy", ""], ["Darrell", "Trevor", ""], ["Saenko", "Kate", ""]]}, {"id": "1510.02364", "submitter": "Ralph Versteegen", "authors": "Ralph Versteegen, Georgy Gimel'farb, Patricia Riddle", "title": "Texture Modelling with Nested High-order Markov-Gibbs Random Fields", "comments": "Submitted to Computer Vision and Image Understanding", "journal-ref": null, "doi": "10.1016/j.cviu.2015.11.003", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Currently, Markov-Gibbs random field (MGRF) image models which include\nhigh-order interactions are almost always built by modelling responses of a\nstack of local linear filters. Actual interaction structure is specified\nimplicitly by the filter coefficients. In contrast, we learn an explicit\nhigh-order MGRF structure by considering the learning process in terms of\ngeneral exponential family distributions nested over base models, so that\npotentials added later can build on previous ones. We relatively rapidly add\nnew features by skipping over the costly optimisation of parameters.\n  We introduce the use of local binary patterns as features in MGRF texture\nmodels, and generalise them by learning offsets to the surrounding pixels.\nThese prove effective as high-order features, and are fast to compute. Several\nschemes for selecting high-order features by composition or search of a small\nsubclass are compared. Additionally we present a simple modification of the\nmaximum likelihood as a texture modelling-specific objective function which\naims to improve generalisation by local windowing of statistics.\n  The proposed method was experimentally evaluated by learning high-order MGRF\nmodels for a broad selection of complex textures and then performing texture\nsynthesis, and succeeded on much of the continuum from stochastic through\nirregularly structured to near-regular textures. Learning interaction structure\nis very beneficial for textures with large-scale structure, although those with\ncomplex irregular structure still provide difficulties. The texture models were\nalso quantitatively evaluated on two tasks and found to be competitive with\nother works: grading of synthesised textures by a panel of observers; and\ncomparison against several recent MGRF models by evaluation on a constrained\ninpainting task.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2015 15:22:21 GMT"}], "update_date": "2015-12-01", "authors_parsed": [["Versteegen", "Ralph", ""], ["Gimel'farb", "Georgy", ""], ["Riddle", "Patricia", ""]]}, {"id": "1510.02413", "submitter": "Tinghui Zhou", "authors": "Tinghui Zhou and Philipp Kr\\\"ahenb\\\"uhl and Alexei A. Efros", "title": "Learning Data-driven Reflectance Priors for Intrinsic Image\n  Decomposition", "comments": "International Conference on Computer Vision (ICCV) 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a data-driven approach for intrinsic image decomposition, which is\nthe process of inferring the confounding factors of reflectance and shading in\nan image. We pose this as a two-stage learning problem. First, we train a model\nto predict relative reflectance ordering between image patches (`brighter',\n`darker', `same') from large-scale human annotations, producing a data-driven\nreflectance prior. Second, we show how to naturally integrate this learned\nprior into existing energy minimization frameworks for intrinsic image\ndecomposition. We compare our method to the state-of-the-art approach of Bell\net al. on both decomposition and image relighting tasks, demonstrating the\nbenefits of the simple relative reflectance prior, especially for scenes under\nchallenging lighting conditions.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2015 17:28:22 GMT"}], "update_date": "2015-10-09", "authors_parsed": [["Zhou", "Tinghui", ""], ["Kr\u00e4henb\u00fchl", "Philipp", ""], ["Efros", "Alexei A.", ""]]}, {"id": "1510.02644", "submitter": "Yi Li", "authors": "Yi Li, Yi-Zhe Song, Timothy Hospedales, Shaogang Gong", "title": "Free-hand Sketch Synthesis with Deformable Stroke Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a generative model which can automatically summarize the stroke\ncomposition of free-hand sketches of a given category. When our model is fit to\na collection of sketches with similar poses, it discovers and learns the\nstructure and appearance of a set of coherent parts, with each part represented\nby a group of strokes. It represents both consistent (topology) as well as\ndiverse aspects (structure and appearance variations) of each sketch category.\nKey to the success of our model are important insights learned from a\ncomprehensive study performed on human stroke data. By fitting this model to\nimages, we are able to synthesize visually similar and pleasant free-hand\nsketches.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2015 12:08:38 GMT"}], "update_date": "2015-10-12", "authors_parsed": [["Li", "Yi", ""], ["Song", "Yi-Zhe", ""], ["Hospedales", "Timothy", ""], ["Gong", "Shaogang", ""]]}, {"id": "1510.02710", "submitter": "Daisuke Iwai", "authors": "Kosuke Sato, Daisuke Iwai, Sei Ikeda, Noriko Takemura", "title": "Procams-Based Cybernetics", "comments": "2 pages, 2 figures, IEEE VR 2015 Lab/Project presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Procams-based cybernetics is a unique, emerging research field, which aims at\nenhancing and supporting our activities by naturally connecting human and\ncomputers/machines as a cooperative integrated system via projector-camera\nsystems (procams). It rests on various research domains such as\nvirtual/augmented reality, computer vision, computer graphics, projection\ndisplay, human computer interface, human robot interaction and so on. This\nlaboratory presentation provides a brief history including recent achievements\nof our procams-based cybernetics project.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2015 15:47:00 GMT"}], "update_date": "2015-10-12", "authors_parsed": [["Sato", "Kosuke", ""], ["Iwai", "Daisuke", ""], ["Ikeda", "Sei", ""], ["Takemura", "Noriko", ""]]}, {"id": "1510.02774", "submitter": "Eugene Borovikov", "authors": "Eugene Borovikov", "title": "Human Head Pose Estimation by Facial Features Location", "comments": "This is a master's thesis completed at UMCP in 1998, being published\n  here given enough of the demand on its contents from the Computer Vision R&D\n  community", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a method for estimating human head pose in a color image that\ncontains enough of information to locate the head silhouette and detect\nnon-trivial color edges of individual facial features. The method works by\nspotting the human head on an arbitrary background, extracting the head\noutline, and locating facial features necessary to describe the head\norientation in the 3D space. It is robust enough to work with both color and\ngray-level images featuring quasi-frontal views of a human head under variable\nlighting conditions.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2015 19:15:52 GMT"}], "update_date": "2015-10-12", "authors_parsed": [["Borovikov", "Eugene", ""]]}, {"id": "1510.02781", "submitter": "Thierry Moreira", "authors": "Thierry Pinheiro Moreira, Mauricio Lisboa Perez, Rafael de Oliveira\n  Werneck, Eduardo Valle", "title": "Where Is My Puppy? Retrieving Lost Dogs by Facial Features", "comments": "17 pages, 8 figures, 1 table, Multimedia Tools and Applications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A pet that goes missing is among many people's worst fears: a moment of\ndistraction is enough for a dog or a cat wandering off from home. Some measures\nhelp matching lost animals to their owners; but automated visual recognition is\none that - although convenient, highly available, and low-cost - is\nsurprisingly overlooked. In this paper, we inaugurate that promising avenue by\npursuing face recognition for dogs. We contrast four ready-to-use human facial\nrecognizers (EigenFaces, FisherFaces, LBPH, and a Sparse method) to two\noriginal solutions based upon convolutional neural networks: BARK (inspired in\narchitecture-optimized networks employed for human facial recognition) and WOOF\n(based upon off-the-shelf OverFeat features). Human facial recognizers perform\npoorly for dogs (up to 60.5% accuracy), showing that dog facial recognition is\nnot a trivial extension of human facial recognition. The convolutional network\nsolutions work much better, with BARK attaining up to 81.1% accuracy, and WOOF,\n89.4%. The tests were conducted in two datasets: Flickr-dog, with 42 dogs of\ntwo breeds (pugs and huskies); and Snoopybook, with 18 mongrel dogs.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2015 19:39:15 GMT"}, {"version": "v2", "created": "Mon, 1 Aug 2016 20:02:15 GMT"}], "update_date": "2016-08-03", "authors_parsed": [["Moreira", "Thierry Pinheiro", ""], ["Perez", "Mauricio Lisboa", ""], ["Werneck", "Rafael de Oliveira", ""], ["Valle", "Eduardo", ""]]}, {"id": "1510.02795", "submitter": "S{\\o}ren Hauberg", "authors": "S{\\o}ren Hauberg, Oren Freifeld, Anders Boesen Lindbo Larsen, John W.\n  Fisher III, Lars Kai Hansen", "title": "Dreaming More Data: Class-dependent Distributions over Diffeomorphisms\n  for Learned Data Augmentation", "comments": null, "journal-ref": "Proceedings of the 19th International Conference on Artificial\n  Intelligence and Statistics, pp. 342-350, 2016", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data augmentation is a key element in training high-dimensional models. In\nthis approach, one synthesizes new observations by applying pre-specified\ntransformations to the original training data; e.g.~new images are formed by\nrotating old ones. Current augmentation schemes, however, rely on manual\nspecification of the applied transformations, making data augmentation an\nimplicit form of feature engineering. With an eye towards true end-to-end\nlearning, we suggest learning the applied transformations on a per-class basis.\nParticularly, we align image pairs within each class under the assumption that\nthe spatial transformation between images belongs to a large class of\ndiffeomorphisms. We then learn a class-specific probabilistic generative models\nof the transformations in a Riemannian submanifold of the Lie group of\ndiffeomorphisms. We demonstrate significant performance improvements in\ntraining deep neural nets over manually-specified augmentation schemes. Our\ncode and augmented datasets are available online.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2015 20:00:47 GMT"}, {"version": "v2", "created": "Thu, 30 Jun 2016 06:12:38 GMT"}], "update_date": "2016-07-01", "authors_parsed": [["Hauberg", "S\u00f8ren", ""], ["Freifeld", "Oren", ""], ["Larsen", "Anders Boesen Lindbo", ""], ["Fisher", "John W.", "III"], ["Hansen", "Lars Kai", ""]]}, {"id": "1510.02866", "submitter": "Yilun Wang", "authors": "Liangtian He, Yilun Wang", "title": "Wavelet Frame Based Image Restoration Using Sparsity, Nonlocal and\n  Support Prior of Frame Coefficients", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The wavelet frame systems have been widely investigated and applied for image\nrestoration and many other image processing problems over the past decades,\nattributing to their good capability of sparsely approximating piece-wise\nsmooth functions such as images. Most wavelet frame based models exploit the\n$l_1$ norm of frame coefficients for a sparsity constraint in the past. The\nauthors in \\cite{ZhangY2013, Dong2013} proposed an $l_0$ minimization model,\nwhere the $l_0$ norm of wavelet frame coefficients is penalized instead, and\nhave demonstrated that significant improvements can be achieved compared to the\ncommonly used $l_1$ minimization model. Very recently, the authors in\n\\cite{Chen2015} proposed $l_0$-$l_2$ minimization model, where the nonlocal\nprior of frame coefficients is incorporated. This model proved to outperform\nthe single $l_0$ minimization based model in terms of better recovered image\nquality. In this paper, we propose a truncated $l_0$-$l_2$ minimization model\nwhich combines sparsity, nonlocal and support prior of the frame coefficients.\nThe extensive experiments have shown that the recovery results from the\nproposed regularization method performs better than existing state-of-the-art\nwavelet frame based methods, in terms of edge enhancement and texture\npreserving performance.\n", "versions": [{"version": "v1", "created": "Sat, 10 Oct 2015 02:52:48 GMT"}], "update_date": "2015-10-13", "authors_parsed": [["He", "Liangtian", ""], ["Wang", "Yilun", ""]]}, {"id": "1510.02884", "submitter": "Wufeng Xue", "authors": "Wufeng Xue and Xuanqin Mou and Lei Zhang", "title": "Learn to Evaluate Image Perceptual Quality Blindly from Statistics of\n  Self-similarity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Among the various image quality assessment (IQA) tasks, blind IQA (BIQA) is\nparticularly challenging due to the absence of knowledge about the reference\nimage and distortion type. Features based on natural scene statistics (NSS)\nhave been successfully used in BIQA, while the quality relevance of the feature\nplays an essential role to the quality prediction performance. Motivated by the\nfact that the early processing stage in human visual system aims to remove the\nsignal redundancies for efficient visual coding, we propose a simple but very\neffective BIQA method by computing the statistics of self-similarity (SOS) in\nan image. Specifically, we calculate the inter-scale similarity and intra-scale\nsimilarity of the distorted image, extract the SOS features from these\nsimilarities, and learn a regression model to map the SOS features to the\nsubjective quality score. Extensive experiments demonstrate very competitive\nquality prediction performance and generalization ability of the proposed SOS\nbased BIQA method.\n", "versions": [{"version": "v1", "created": "Sat, 10 Oct 2015 07:23:30 GMT"}], "update_date": "2015-10-13", "authors_parsed": [["Xue", "Wufeng", ""], ["Mou", "Xuanqin", ""], ["Zhang", "Lei", ""]]}, {"id": "1510.02899", "submitter": "Xirong Li", "authors": "Masoud Mazloom and Xirong Li and Cees G. M. Snoek", "title": "TagBook: A Semantic Video Representation without Supervision for Event\n  Detection", "comments": "accepted for publication as a regular paper in the IEEE Transactions\n  on Multimedia", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of event detection in video for scenarios where only\nfew, or even zero examples are available for training. For this challenging\nsetting, the prevailing solutions in the literature rely on a semantic video\nrepresentation obtained from thousands of pre-trained concept detectors.\nDifferent from existing work, we propose a new semantic video representation\nthat is based on freely available social tagged videos only, without the need\nfor training any intermediate concept detectors. We introduce a simple\nalgorithm that propagates tags from a video's nearest neighbors, similar in\nspirit to the ones used for image retrieval, but redesign it for video event\ndetection by including video source set refinement and varying the video tag\nassignment. We call our approach TagBook and study its construction,\ndescriptiveness and detection performance on the TRECVID 2013 and 2014\nmultimedia event detection datasets and the Columbia Consumer Video dataset.\nDespite its simple nature, the proposed TagBook video representation is\nremarkably effective for few-example and zero-example event detection, even\noutperforming very recent state-of-the-art alternatives building on supervised\nrepresentations.\n", "versions": [{"version": "v1", "created": "Sat, 10 Oct 2015 09:28:56 GMT"}, {"version": "v2", "created": "Sat, 23 Apr 2016 13:23:03 GMT"}], "update_date": "2016-04-26", "authors_parsed": [["Mazloom", "Masoud", ""], ["Li", "Xirong", ""], ["Snoek", "Cees G. M.", ""]]}, {"id": "1510.02906", "submitter": "Min Yang", "authors": "Min Yang and Yunde Jia", "title": "Temporal Dynamic Appearance Modeling for Online Multi-Person Tracking", "comments": null, "journal-ref": null, "doi": "10.1016/j.cviu.2016.05.003", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust online multi-person tracking requires the correct associations of\nonline detection responses with existing trajectories. We address this problem\nby developing a novel appearance modeling approach to provide accurate\nappearance affinities to guide data association. In contrast to most existing\nalgorithms that only consider the spatial structure of human appearances, we\nexploit the temporal dynamic characteristics within temporal appearance\nsequences to discriminate different persons. The temporal dynamic makes a\nsufficient complement to the spatial structure of varying appearances in the\nfeature space, which significantly improves the affinity measurement between\ntrajectories and detections. We propose a feature selection algorithm to\ndescribe the appearance variations with mid-level semantic features, and\ndemonstrate its usefulness in terms of temporal dynamic appearance modeling.\nMoreover, the appearance model is learned incrementally by alternatively\nevaluating newly-observed appearances and adjusting the model parameters to be\nsuitable for online tracking. Reliable tracking of multiple persons in complex\nscenes is achieved by incorporating the learned model into an online\ntracking-by-detection framework. Our experiments on the challenging benchmark\nMOTChallenge 2015 demonstrate that our method outperforms the state-of-the-art\nmulti-person tracking algorithms.\n", "versions": [{"version": "v1", "created": "Sat, 10 Oct 2015 10:13:04 GMT"}], "update_date": "2017-03-14", "authors_parsed": [["Yang", "Min", ""], ["Jia", "Yunde", ""]]}, {"id": "1510.02923", "submitter": "Adri\\'an Mart\\'in", "authors": "Adrian Martin, Emanuele Schiavi and Sergio Segura de Leon", "title": "On 1-Laplacian Elliptic Equations Modeling Magnetic Resonance Image\n  Rician Denoising", "comments": null, "journal-ref": null, "doi": "10.1007/s10851-016-0675-3", "report-no": null, "categories": "math.AP cs.CV math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling magnitude Magnetic Resonance Images (MRI) rician denoising in a\nBayesian or generalized Tikhonov framework using Total Variation (TV) leads\nnaturally to the consideration of nonlinear elliptic equations. These involve\nthe so called $1$-Laplacian operator and special care is needed to properly\nformulate the problem. The rician statistics of the data are introduced through\na singular equation with a reaction term defined in terms of modified first\norder Bessel functions. An existence theory is provided here together with\nother qualitative properties of the solutions. Remarkably, each positive global\nminimum of the associated functional is one of such solutions. Moreover, we\ndirectly solve this non--smooth non--convex minimization problem using a\nconvergent Proximal Point Algorithm. Numerical results based on synthetic and\nreal MRI demonstrate a better performance of the proposed method when compared\nto previous TV based models for rician denoising which regularize or convexify\nthe problem. Finally, an application on real Diffusion Tensor Images, a\nstrongly affected by rician noise MRI modality, is presented and discussed.\n", "versions": [{"version": "v1", "created": "Sat, 10 Oct 2015 13:11:57 GMT"}, {"version": "v2", "created": "Tue, 12 Jul 2016 09:19:28 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Martin", "Adrian", ""], ["Schiavi", "Emanuele", ""], ["de Leon", "Sergio Segura", ""]]}, {"id": "1510.02927", "submitter": "Srinivas S S Kruthiventi", "authors": "Srinivas S. S. Kruthiventi, Kumar Ayush and R. Venkatesh Babu", "title": "DeepFix: A Fully Convolutional Neural Network for predicting Human Eye\n  Fixations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding and predicting the human visual attentional mechanism is an\nactive area of research in the fields of neuroscience and computer vision. In\nthis work, we propose DeepFix, a first-of-its-kind fully convolutional neural\nnetwork for accurate saliency prediction. Unlike classical works which\ncharacterize the saliency map using various hand-crafted features, our model\nautomatically learns features in a hierarchical fashion and predicts saliency\nmap in an end-to-end manner. DeepFix is designed to capture semantics at\nmultiple scales while taking global context into account using network layers\nwith very large receptive fields. Generally, fully convolutional nets are\nspatially invariant which prevents them from modeling location dependent\npatterns (e.g. centre-bias). Our network overcomes this limitation by\nincorporating a novel Location Biased Convolutional layer. We evaluate our\nmodel on two challenging eye fixation datasets -- MIT300, CAT2000 and show that\nit outperforms other recent approaches by a significant margin.\n", "versions": [{"version": "v1", "created": "Sat, 10 Oct 2015 13:36:31 GMT"}], "update_date": "2015-10-13", "authors_parsed": [["Kruthiventi", "Srinivas S. S.", ""], ["Ayush", "Kumar", ""], ["Babu", "R. Venkatesh", ""]]}, {"id": "1510.02930", "submitter": "Yunjin Chen", "authors": "Wensen Feng and Yunjin Chen", "title": "Fast and Accurate Poisson Denoising with Optimized Nonlinear Diffusion", "comments": "11 pages, 12 figures, technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The degradation of the acquired signal by Poisson noise is a common problem\nfor various imaging applications, such as medical imaging, night vision and\nmicroscopy. Up to now, many state-of-the-art Poisson denoising techniques\nmainly concentrate on achieving utmost performance, with little consideration\nfor the computation efficiency. Therefore, in this study we aim to propose an\nefficient Poisson denoising model with both high computational efficiency and\nrecovery quality. To this end, we exploit the newly-developed trainable\nnonlinear reaction diffusion model which has proven an extremely fast image\nrestoration approach with performance surpassing recent state-of-the-arts. We\nretrain the model parameters, including the linear filters and influence\nfunctions by taking into account the Poisson noise statistics, and end up with\nan optimized nonlinear diffusion model specialized for Poisson denoising. The\ntrained model provides strongly competitive results against state-of-the-art\napproaches, meanwhile bearing the properties of simple structure and high\nefficiency. Furthermore, our proposed model comes along with an additional\nadvantage, that the diffusion process is well-suited for parallel computation\non GPUs. For images of size $512 \\times 512$, our GPU implementation takes less\nthan 0.1 seconds to produce state-of-the-art Poisson denoising performance.\n", "versions": [{"version": "v1", "created": "Sat, 10 Oct 2015 13:44:47 GMT"}], "update_date": "2015-10-13", "authors_parsed": [["Feng", "Wensen", ""], ["Chen", "Yunjin", ""]]}, {"id": "1510.02934", "submitter": "Cheng Koay", "authors": "Cheng Guan Koay, Ping-Hong Yeh, John M. Ollinger, M. Okan\n  \\.Irfano\\u{g}lu, Carlo Pierpaoli, Peter J. Basser, Terrence R. Oakes, Gerard\n  Riedy", "title": "Tract Orientation and Angular Dispersion Deviation Indicator (TOADDI): A\n  framework for single-subject analysis in diffusion tensor imaging", "comments": "49 pages, 6 figures, 2 tables", "journal-ref": null, "doi": "10.1016/j.neuroimage.2015.11.046", "report-no": null, "categories": "physics.med-ph cs.CV stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The purpose of this work is to develop a framework for single-subject\nanalysis of diffusion tensor imaging (DTI) data. This framework (termed TOADDI)\nis capable of testing whether an individual tract as represented by the major\neigenvector of the diffusion tensor and its corresponding angular dispersion\nare significantly different from a group of tracts on a voxel-by-voxel basis.\nThis work develops two complementary statistical tests based on the elliptical\ncone of uncertainty (COU), which is a model of uncertainty or dispersion of the\nmajor eigenvector of the diffusion tensor. The orientation deviation test\nexamines whether the major eigenvector from a single subject is within the\naverage elliptical COU formed by a collection of elliptical COUs. The shape\ndeviation test is based on the two-tailed Wilcoxon-Mann-Whitney two-sample test\nbetween the normalized shape measures (area and circumference) of the\nelliptical cones of uncertainty of the single subject against a group of\ncontrols. The False Discovery Rate (FDR) and False Non-discovery Rate (FNR)\nwere incorporated in the orientation deviation test. The shape deviation test\nuses FDR only. TOADDI was found to be numerically accurate and statistically\neffective. Clinical data from two Traumatic Brain Injury (TBI) patients and one\nnon-TBI subject were tested against the data obtained from a group of 45\nnon-TBI controls to illustrate the application of the proposed framework in\nsingle-subject analysis. The frontal portion of the superior longitudinal\nfasciculus seemed to be implicated in both tests as significantly different\nfrom that of the control group. The TBI patients and the single non-TBI subject\nwere well separated under the shape deviation test at the chosen FDR level of\n0.0005. TOADDI is a simple but novel geometrically based statistical framework\nfor analyzing DTI data.\n", "versions": [{"version": "v1", "created": "Sat, 10 Oct 2015 13:54:07 GMT"}, {"version": "v2", "created": "Fri, 20 Nov 2015 01:35:14 GMT"}], "update_date": "2015-11-23", "authors_parsed": [["Koay", "Cheng Guan", ""], ["Yeh", "Ping-Hong", ""], ["Ollinger", "John M.", ""], ["\u0130rfano\u011flu", "M. Okan", ""], ["Pierpaoli", "Carlo", ""], ["Basser", "Peter J.", ""], ["Oakes", "Terrence R.", ""], ["Riedy", "Gerard", ""]]}, {"id": "1510.02942", "submitter": "Baris Gecer", "authors": "Baris Gecer, Ozge Yalcinkaya, Onur Tasar and Selim Aksoy", "title": "Evaluation of Joint Multi-Instance Multi-Label Learning For Breast\n  Cancer Diagnosis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-instance multi-label (MIML) learning is a challenging problem in many\naspects. Such learning approaches might be useful for many medical diagnosis\napplications including breast cancer detection and classification. In this\nstudy subset of digiPATH dataset (whole slide digital breast cancer\nhistopathology images) are used for training and evaluation of six\nstate-of-the-art MIML methods.\n  At the end, performance comparison of these approaches are given by means of\neffective evaluation metrics. It is shown that MIML-kNN achieve the best\nperformance that is %65.3 average precision, where most of other methods attain\nacceptable results as well.\n", "versions": [{"version": "v1", "created": "Sat, 10 Oct 2015 14:30:25 GMT"}], "update_date": "2015-10-13", "authors_parsed": [["Gecer", "Baris", ""], ["Yalcinkaya", "Ozge", ""], ["Tasar", "Onur", ""], ["Aksoy", "Selim", ""]]}, {"id": "1510.02949", "submitter": "Marcus Rohrbach", "authors": "Damian Mrowca, Marcus Rohrbach, Judy Hoffman, Ronghang Hu, Kate\n  Saenko, Trevor Darrell", "title": "Spatial Semantic Regularisation for Large Scale Object Detection", "comments": "accepted at ICCV 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large scale object detection with thousands of classes introduces the problem\nof many contradicting false positive detections, which have to be suppressed.\nClass-independent non-maximum suppression has traditionally been used for this\nstep, but it does not scale well as the number of classes grows. Traditional\nnon-maximum suppression does not consider label- and instance-level\nrelationships nor does it allow an exploitation of the spatial layout of\ndetection proposals. We propose a new multi-class spatial semantic\nregularisation method based on affinity propagation clustering, which\nsimultaneously optimises across all categories and all proposed locations in\nthe image, to improve both the localisation and categorisation of selected\ndetection proposals. Constraints are shared across the labels through the\nsemantic WordNet hierarchy. Our approach proves to be especially useful in\nlarge scale settings with thousands of classes, where spatial and semantic\ninteractions are very frequent and only weakly supervised detectors can be\nbuilt due to a lack of bounding box annotations. Detection experiments are\nconducted on the ImageNet and COCO dataset, and in settings with thousands of\ndetected categories. Our method provides a significant precision improvement by\nreducing false positives, while simultaneously improving the recall.\n", "versions": [{"version": "v1", "created": "Sat, 10 Oct 2015 15:15:45 GMT"}], "update_date": "2015-10-13", "authors_parsed": [["Mrowca", "Damian", ""], ["Rohrbach", "Marcus", ""], ["Hoffman", "Judy", ""], ["Hu", "Ronghang", ""], ["Saenko", "Kate", ""], ["Darrell", "Trevor", ""]]}, {"id": "1510.02969", "submitter": "Pooya Khorrami", "authors": "Pooya Khorrami, Tom Le Paine, Thomas S. Huang", "title": "Do Deep Neural Networks Learn Facial Action Units When Doing Expression\n  Recognition?", "comments": "Accepted at ICCV 2015 CV4AC Workshop. Corrected numbers in Tables 2\n  and 3", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite being the appearance-based classifier of choice in recent years,\nrelatively few works have examined how much convolutional neural networks\n(CNNs) can improve performance on accepted expression recognition benchmarks\nand, more importantly, examine what it is they actually learn. In this work,\nnot only do we show that CNNs can achieve strong performance, but we also\nintroduce an approach to decipher which portions of the face influence the\nCNN's predictions. First, we train a zero-bias CNN on facial expression data\nand achieve, to our knowledge, state-of-the-art performance on two expression\nrecognition benchmarks: the extended Cohn-Kanade (CK+) dataset and the Toronto\nFace Dataset (TFD). We then qualitatively analyze the network by visualizing\nthe spatial patterns that maximally excite different neurons in the\nconvolutional layers and show how they resemble Facial Action Units (FAUs).\nFinally, we use the FAU labels provided in the CK+ dataset to verify that the\nFAUs observed in our filter visualizations indeed align with the subject's\nfacial movements.\n", "versions": [{"version": "v1", "created": "Sat, 10 Oct 2015 18:53:21 GMT"}, {"version": "v2", "created": "Fri, 28 Oct 2016 06:12:07 GMT"}, {"version": "v3", "created": "Thu, 16 Mar 2017 03:07:21 GMT"}], "update_date": "2017-03-17", "authors_parsed": [["Khorrami", "Pooya", ""], ["Paine", "Tom Le", ""], ["Huang", "Thomas S.", ""]]}, {"id": "1510.02975", "submitter": "Guillermo Gallego", "authors": "Daniel Berj\\'on, Guillermo Gallego, Carlos Cuevas, Francisco Mor\\'an\n  and Narciso Garc\\'ia", "title": "Optimal Piecewise Linear Function Approximation for GPU-based\n  Applications", "comments": "12 pages, 12 figures, post-print, IEEE Transactions on Cybernetics,\n  Oct. 2015", "journal-ref": "IEEE Transactions on Cybernetics, vol. 46, no. 11, pp. 2584-2595,\n  Nov. 2016", "doi": "10.1109/TCYB.2015.2482365", "report-no": null, "categories": "math.OC cs.CV cs.DC cs.NA cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many computer vision and human-computer interaction applications developed in\nrecent years need evaluating complex and continuous mathematical functions as\nan essential step toward proper operation. However, rigorous evaluation of this\nkind of functions often implies a very high computational cost, unacceptable in\nreal-time applications. To alleviate this problem, functions are commonly\napproximated by simpler piecewise-polynomial representations. Following this\nidea, we propose a novel, efficient, and practical technique to evaluate\ncomplex and continuous functions using a nearly optimal design of two types of\npiecewise linear approximations in the case of a large budget of evaluation\nsubintervals. To this end, we develop a thorough error analysis that yields\nasymptotically tight bounds to accurately quantify the approximation\nperformance of both representations. It provides an improvement upon previous\nerror estimates and allows the user to control the trade-off between the\napproximation error and the number of evaluation subintervals. To guarantee\nreal-time operation, the method is suitable for, but not limited to, an\nefficient implementation in modern Graphics Processing Units (GPUs), where it\noutperforms previous alternative approaches by exploiting the fixed-function\ninterpolation routines present in their texture units. The proposed technique\nis a perfect match for any application requiring the evaluation of continuous\nfunctions, we have measured in detail its quality and efficiency on several\nfunctions, and, in particular, the Gaussian function because it is extensively\nused in many areas of computer vision and cybernetics, and it is expensive to\nevaluate.\n", "versions": [{"version": "v1", "created": "Sat, 10 Oct 2015 20:49:17 GMT"}], "update_date": "2017-11-10", "authors_parsed": [["Berj\u00f3n", "Daniel", ""], ["Gallego", "Guillermo", ""], ["Cuevas", "Carlos", ""], ["Mor\u00e1n", "Francisco", ""], ["Garc\u00eda", "Narciso", ""]]}, {"id": "1510.03125", "submitter": "Chunhua Shen", "authors": "Qichang Hu, Sakrapee Paisitkriangkrai, Chunhua Shen, Anton van den\n  Hengel, Fatih Porikli", "title": "Fast detection of multiple objects in traffic scenes with a common\n  detection framework", "comments": "Appearing in IEEE Transactions on Intelligent Transportation Systems", "journal-ref": null, "doi": "10.1109/TITS.2015.2496795", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traffic scene perception (TSP) aims to real-time extract accurate on-road\nenvironment information, which in- volves three phases: detection of objects of\ninterest, recognition of detected objects, and tracking of objects in motion.\nSince recognition and tracking often rely on the results from detection, the\nability to detect objects of interest effectively plays a crucial role in TSP.\nIn this paper, we focus on three important classes of objects: traffic signs,\ncars, and cyclists. We propose to detect all the three important objects in a\nsingle learning based detection framework. The proposed framework consists of a\ndense feature extractor and detectors of three important classes. Once the\ndense features have been extracted, these features are shared with all\ndetectors. The advantage of using one common framework is that the detection\nspeed is much faster, since all dense features need only to be evaluated once\nin the testing phase. In contrast, most previous works have designed specific\ndetectors using different features for each of these objects. To enhance the\nfeature robustness to noises and image deformations, we introduce spatially\npooled features as a part of aggregated channel features. In order to further\nimprove the generalization performance, we propose an object subcategorization\nmethod as a means of capturing intra-class variation of objects. We\nexperimentally demonstrate the effectiveness and efficiency of the proposed\nframework in three detection applications: traffic sign detection, car\ndetection, and cyclist detection. The proposed framework achieves the\ncompetitive performance with state-of- the-art approaches on several benchmark\ndatasets.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2015 02:30:22 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Hu", "Qichang", ""], ["Paisitkriangkrai", "Sakrapee", ""], ["Shen", "Chunhua", ""], ["Hengel", "Anton van den", ""], ["Porikli", "Fatih", ""]]}, {"id": "1510.03199", "submitter": "B\\'ereng\\`ere Mathieu", "authors": "B\\'ereng\\`ere Mathieu, Alain Crouzil, Jean-Baptiste Puel", "title": "Interactive multiclass segmentation using superpixel classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper adresses the problem of interactive multiclass segmentation. We\npropose a fast and efficient new interactive segmentation method called\nSuperpixel Classification-based Interactive Segmentation (SCIS). From a few\nstrokes drawn by a human user over an image, this method extracts relevant\nsemantic objects. To get a fast calculation and an accurate segmentation, SCIS\nuses superpixel over-segmentation and support vector machine classification. In\nthis paper, we demonstrate that SCIS significantly outperfoms competing\nalgorithms by evaluating its performances on the reference benchmarks of\nMcGuinness and Santner.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2015 09:36:53 GMT"}], "update_date": "2015-10-13", "authors_parsed": [["Mathieu", "B\u00e9reng\u00e8re", ""], ["Crouzil", "Alain", ""], ["Puel", "Jean-Baptiste", ""]]}, {"id": "1510.03250", "submitter": "Dan Adam", "authors": "Yael Petrank, Nahum Smirin, Yossi Tsadok, Zvi Friedman, Peter\n  Lysiansky, Dan Adam", "title": "Using Anatomical Markers for Left Ventricular Segmentation of Long Axis\n  Ultrasound Images", "comments": "11 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Left ventricular segmentation is essential for measuring left ventricular\nfunction indices. Segmentation of one or several images requires an initial\nguess of the contour. It is hypothesized here that creating an initial guess by\nfirst detecting anatomical markers, would lead to correct detection of the\nendocardium. The first step of the algorithm presented here includes automatic\ndetection of the mitral valve. Next, the apex is detected in the same frame.\nThe valve is then tracked throughout the cardiac cycle. Contours passing from\nthe apex to each valve corner are then found using a dynamic programming\nalgorithm. The resulting contour is used as an input to an active contour\nalgorithm. The algorithm was tested on 21 long axis ultrasound clips and showed\ngood agreement with manually traced contours. Thus, this study demonstrates\nthat detection of anatomic markers leads to a reliable initial guess of the\nleft ventricle border.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2015 12:20:23 GMT"}], "update_date": "2015-10-13", "authors_parsed": [["Petrank", "Yael", ""], ["Smirin", "Nahum", ""], ["Tsadok", "Yossi", ""], ["Friedman", "Zvi", ""], ["Lysiansky", "Peter", ""], ["Adam", "Dan", ""]]}, {"id": "1510.03283", "submitter": "Weilin Huang", "authors": "Tong He, Weilin Huang, Yu Qiao, and Jian Yao", "title": "Text-Attentional Convolutional Neural Networks for Scene Text Detection", "comments": "To appear in IEEE Trans. on Image Processing, 2016", "journal-ref": null, "doi": "10.1109/TIP.2016.2547588", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent deep learning models have demonstrated strong capabilities for\nclassifying text and non-text components in natural images. They extract a\nhigh-level feature computed globally from a whole image component (patch),\nwhere the cluttered background information may dominate true text features in\nthe deep representation. This leads to less discriminative power and poorer\nrobustness. In this work, we present a new system for scene text detection by\nproposing a novel Text-Attentional Convolutional Neural Network (Text-CNN) that\nparticularly focuses on extracting text-related regions and features from the\nimage components. We develop a new learning mechanism to train the Text-CNN\nwith multi-level and rich supervised information, including text region mask,\ncharacter label, and binary text/nontext information. The rich supervision\ninformation enables the Text-CNN with a strong capability for discriminating\nambiguous texts, and also increases its robustness against complicated\nbackground components. The training process is formulated as a multi-task\nlearning problem, where low-level supervised information greatly facilitates\nmain task of text/non-text classification. In addition, a powerful low-level\ndetector called Contrast- Enhancement Maximally Stable Extremal Regions\n(CE-MSERs) is developed, which extends the widely-used MSERs by enhancing\nintensity contrast between text patterns and background. This allows it to\ndetect highly challenging text patterns, resulting in a higher recall. Our\napproach achieved promising results on the ICDAR 2013 dataset, with a F-measure\nof 0.82, improving the state-of-the-art results substantially.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2015 13:53:13 GMT"}, {"version": "v2", "created": "Thu, 24 Mar 2016 23:25:52 GMT"}], "update_date": "2016-05-04", "authors_parsed": [["He", "Tong", ""], ["Huang", "Weilin", ""], ["Qiao", "Yu", ""], ["Yao", "Jian", ""]]}, {"id": "1510.03608", "submitter": "Denis Tome'", "authors": "Denis Tom\\`e, Federico Monti, Luca Baroffio, Luca Bondi, Marco\n  Tagliasacchi, Stefano Tubaro", "title": "Deep convolutional neural networks for pedestrian detection", "comments": "submitted to Elsevier Signal Processing: Image Communication special\n  Issue on Deep Learning", "journal-ref": null, "doi": "10.1016/j.image.2016.05.007", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pedestrian detection is a popular research topic due to its paramount\nimportance for a number of applications, especially in the fields of\nautomotive, surveillance and robotics. Despite the significant improvements,\npedestrian detection is still an open challenge that calls for more and more\naccurate algorithms. In the last few years, deep learning and in particular\nconvolutional neural networks emerged as the state of the art in terms of\naccuracy for a number of computer vision tasks such as image classification,\nobject detection and segmentation, often outperforming the previous gold\nstandards by a large margin. In this paper, we propose a pedestrian detection\nsystem based on deep learning, adapting a general-purpose convolutional network\nto the task at hand. By thoroughly analyzing and optimizing each step of the\ndetection pipeline we propose an architecture that outperforms traditional\nmethods, achieving a task accuracy close to that of state-of-the-art\napproaches, while requiring a low computational time. Finally, we tested the\nsystem on an NVIDIA Jetson TK1, a 192-core platform that is envisioned to be a\nforerunner computational brain of future self-driving cars.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2015 09:57:46 GMT"}, {"version": "v2", "created": "Wed, 14 Oct 2015 15:57:44 GMT"}, {"version": "v3", "created": "Thu, 15 Oct 2015 07:39:22 GMT"}, {"version": "v4", "created": "Fri, 4 Mar 2016 11:53:54 GMT"}, {"version": "v5", "created": "Mon, 7 Mar 2016 09:42:33 GMT"}], "update_date": "2017-04-27", "authors_parsed": [["Tom\u00e8", "Denis", ""], ["Monti", "Federico", ""], ["Baroffio", "Luca", ""], ["Bondi", "Luca", ""], ["Tagliasacchi", "Marco", ""], ["Tubaro", "Stefano", ""]]}, {"id": "1510.03727", "submitter": "Stuart Golodetz", "authors": "Stuart Golodetz, Michael Sapienza, Julien P. C. Valentin, Vibhav\n  Vineet, Ming-Ming Cheng, Anurag Arnab, Victor A. Prisacariu, Olaf K\\\"ahler,\n  Carl Yuheng Ren, David W. Murray, Shahram Izadi and Philip H. S. Torr", "title": "SemanticPaint: A Framework for the Interactive Segmentation of 3D Scenes", "comments": "33 pages, Project: http://www.semantic-paint.com, Code:\n  https://github.com/torrvision/spaint", "journal-ref": null, "doi": "10.1145/2751556", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an open-source, real-time implementation of SemanticPaint, a\nsystem for geometric reconstruction, object-class segmentation and learning of\n3D scenes. Using our system, a user can walk into a room wearing a depth camera\nand a virtual reality headset, and both densely reconstruct the 3D scene and\ninteractively segment the environment into object classes such as 'chair',\n'floor' and 'table'. The user interacts physically with the real-world scene,\ntouching objects and using voice commands to assign them appropriate labels.\nThese user-generated labels are leveraged by an online random forest-based\nmachine learning algorithm, which is used to predict labels for previously\nunseen parts of the scene. The entire pipeline runs in real time, and the user\nstays 'in the loop' throughout the process, receiving immediate feedback about\nthe progress of the labelling and interacting with the scene as necessary to\nrefine the predicted segmentation.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2015 15:06:03 GMT"}], "update_date": "2017-09-05", "authors_parsed": [["Golodetz", "Stuart", ""], ["Sapienza", "Michael", ""], ["Valentin", "Julien P. C.", ""], ["Vineet", "Vibhav", ""], ["Cheng", "Ming-Ming", ""], ["Arnab", "Anurag", ""], ["Prisacariu", "Victor A.", ""], ["K\u00e4hler", "Olaf", ""], ["Ren", "Carl Yuheng", ""], ["Murray", "David W.", ""], ["Izadi", "Shahram", ""], ["Torr", "Philip H. S.", ""]]}, {"id": "1510.03730", "submitter": "Miguel Masciopinto", "authors": "Fernando P\\'erez-Gonz\\'alez, Iria Gonz\\'alez-Iglesias, Miguel\n  Masciopinto and Pedro Comesa\\~na", "title": "Fast sequential forensic camera identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two sequential camera source identification methods are proposed. Sequential\ntests implement a log-likelihood ratio test in an incremental way, thus\nenabling a reliable decision with a minimal number of observations. One of our\nmethods adapts Goljan et al.'s to sequential operation. The second, which\noffers better performance in terms of error probabilities and average number of\ntest observations, is based on treating the alternative hypothesis as a doubly\nstochastic model. We also discuss how the standard sequential test can be\ncorrected to account for the event of weak fingerprints. Finally, we validate\nthe goodness of our methods with experiments.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2015 15:07:46 GMT"}], "update_date": "2015-10-14", "authors_parsed": [["P\u00e9rez-Gonz\u00e1lez", "Fernando", ""], ["Gonz\u00e1lez-Iglesias", "Iria", ""], ["Masciopinto", "Miguel", ""], ["Comesa\u00f1a", "Pedro", ""]]}, {"id": "1510.03743", "submitter": "Scott Workman", "authors": "Scott Workman, Richard Souvenir, Nathan Jacobs", "title": "Wide-Area Image Geolocalization with Aerial Reference Imagery", "comments": "International Conference on Computer Vision (ICCV) 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose to use deep convolutional neural networks to address the problem\nof cross-view image geolocalization, in which the geolocation of a ground-level\nquery image is estimated by matching to georeferenced aerial images. We use\nstate-of-the-art feature representations for ground-level images and introduce\na cross-view training approach for learning a joint semantic feature\nrepresentation for aerial images. We also propose a network architecture that\nfuses features extracted from aerial images at multiple spatial scales. To\nsupport training these networks, we introduce a massive database that contains\npairs of aerial and ground-level images from across the United States. Our\nmethods significantly out-perform the state of the art on two benchmark\ndatasets. We also show, qualitatively, that the proposed feature\nrepresentations are discriminative at both local and continental spatial\nscales.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2015 15:33:01 GMT"}], "update_date": "2015-10-14", "authors_parsed": [["Workman", "Scott", ""], ["Souvenir", "Richard", ""], ["Jacobs", "Nathan", ""]]}, {"id": "1510.03909", "submitter": "Robert Walecki Mr", "authors": "Robert Walecki, Ognjen Rudovic, Vladimir Pavlovic, Maja Pantic", "title": "Variable-state Latent Conditional Random Fields for Facial Expression\n  Recognition and Action Unit Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated recognition of facial expressions of emotions, and detection of\nfacial action units (AUs), from videos depends critically on modeling of their\ndynamics. These dynamics are characterized by changes in temporal phases\n(onset-apex-offset) and intensity of emotion expressions and AUs, the\nappearance of which may vary considerably among target subjects, making the\nrecognition/detection task very challenging. The state-of-the-art Latent\nConditional Random Fields (L-CRF) framework allows one to efficiently encode\nthese dynamics through the latent states accounting for the temporal\nconsistency in emotion expression and ordinal relationships between its\nintensity levels, these latent states are typically assumed to be either\nunordered (nominal) or fully ordered (ordinal). Yet, such an approach is often\ntoo restrictive. For instance, in the case of AU detection, the goal is to\ndiscriminate between the segments of an image sequence in which this AU is\nactive or inactive. While the sequence segments containing activation of the\ntarget AU may better be described using ordinal latent states, the inactive\nsegments better be described using unordered (nominal) latent states, as no\nassumption can be made about their underlying structure (since they can contain\neither neutral faces or activations of non-target AUs). To address this, we\npropose the variable-state L-CRF (VSL-CRF) model that automatically selects the\noptimal latent states for the target image sequence. To reduce the model\noverfitting either the nominal or ordinal latent states, we propose a novel\ngraph-Laplacian regularization of the latent states. Our experiments on three\npublic expression databases show that the proposed model achieves better\ngeneralization performance compared to traditional L-CRFs and other related\nstate-of-the-art models.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2015 21:57:47 GMT"}], "update_date": "2015-10-15", "authors_parsed": [["Walecki", "Robert", ""], ["Rudovic", "Ognjen", ""], ["Pavlovic", "Vladimir", ""], ["Pantic", "Maja", ""]]}, {"id": "1510.03979", "submitter": "Limin Wang", "authors": "Limin Wang, Zhe Wang, Sheng Guo, Yu Qiao", "title": "Better Exploiting OS-CNNs for Better Event Recognition in Images", "comments": "8 pages. This work is following our previous work:\n  http://arxiv.org/abs/1505.00296", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Event recognition from still images is one of the most important problems for\nimage understanding. However, compared with object recognition and scene\nrecognition, event recognition has received much less research attention in\ncomputer vision community. This paper addresses the problem of cultural event\nrecognition in still images and focuses on applying deep learning methods on\nthis problem. In particular, we utilize the successful architecture of\nObject-Scene Convolutional Neural Networks (OS-CNNs) to perform event\nrecognition. OS-CNNs are composed of object nets and scene nets, which transfer\nthe learned representations from the pre-trained models on large-scale object\nand scene recognition datasets, respectively. We propose four types of\nscenarios to explore OS-CNNs for event recognition by treating them as either\n\"end-to-end event predictors\" or \"generic feature extractors\". Our experimental\nresults demonstrate that the global and local representations of OS-CNNs are\ncomplementary to each other. Finally, based on our investigation of OS-CNNs, we\ncome up with a solution for the cultural event recognition track at the ICCV\nChaLearn Looking at People (LAP) challenge 2015. Our team secures the third\nplace at this challenge and our result is very close to the best performance.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2015 06:56:54 GMT"}], "update_date": "2015-10-15", "authors_parsed": [["Wang", "Limin", ""], ["Wang", "Zhe", ""], ["Guo", "Sheng", ""], ["Qiao", "Yu", ""]]}, {"id": "1510.04004", "submitter": "Behrooz Nasihatkon", "authors": "Behrooz Nasihatkon and Fredrik Kahl", "title": "Multiresolution Search of the Rigid Motion Space for Intensity Based\n  Registration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the relation between the target functions of low-resolution and\nhigh-resolution intensity-based registration for the class of rigid\ntransformations. Our results show that low resolution target values can tightly\nbound the high-resolution target function in natural images. This can help with\nanalyzing and better understanding the process of multiresolution image\nregistration. It also gives a guideline for designing multiresolution\nalgorithms in which the search space in higher resolution registration is\nrestricted given the fitness values for lower resolution image pairs. To\ndemonstrate this, we incorporate our multiresolution technique into a Lipschitz\nglobal optimization framework. We show that using the multiresolution scheme\ncan result in large gains in the efficiency of such algorithms. The method is\nevaluated by applying to 2D and 3D registration problems as well as the\ndetection of reflective symmetry in 2D and 3D images.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2015 08:47:12 GMT"}], "update_date": "2015-10-15", "authors_parsed": [["Nasihatkon", "Behrooz", ""], ["Kahl", "Fredrik", ""]]}, {"id": "1510.04074", "submitter": "Marian George", "authors": "Marian George, Dejan Mircic, G\\'abor S\\\"or\\\"os, Christian\n  Floerkemeier, Friedemann Mattern", "title": "Fine-Grained Product Class Recognition for Assisted Shopping", "comments": "Accepted at ICCV Workshop on Assistive Computer Vision and Robotics\n  (ICCV-ACVR) 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Assistive solutions for a better shopping experience can improve the quality\nof life of people, in particular also of visually impaired shoppers. We present\na system that visually recognizes the fine-grained product classes of items on\na shopping list, in shelves images taken with a smartphone in a grocery store.\nOur system consists of three components: (a) We automatically recognize useful\ntext on product packaging, e.g., product name and brand, and build a mapping of\nwords to product classes based on the large-scale GroceryProducts dataset. When\nthe user populates the shopping list, we automatically infer the product class\nof each entered word. (b) We perform fine-grained product class recognition\nwhen the user is facing a shelf. We discover discriminative patches on product\npackaging to differentiate between visually similar product classes and to\nincrease the robustness against continuous changes in product design. (c) We\ncontinuously improve the recognition accuracy through active learning. Our\nexperiments show the robustness of the proposed method against cross-domain\nchallenges, and the scalability to an increasing number of products with\nminimal re-training.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2015 13:07:05 GMT"}], "update_date": "2015-10-15", "authors_parsed": [["George", "Marian", ""], ["Mircic", "Dejan", ""], ["S\u00f6r\u00f6s", "G\u00e1bor", ""], ["Floerkemeier", "Christian", ""], ["Mattern", "Friedemann", ""]]}, {"id": "1510.04238", "submitter": "Simon Henrot", "authors": "Simon Henrot and Jocelyn Chanussot and Christian Jutten", "title": "Dynamical spectral unmixing of multitemporal hyperspectral images", "comments": "13 pages, 10 figures", "journal-ref": null, "doi": "10.1109/TIP.2016.2562562", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the problem of unmixing a time series of\nhyperspectral images. We propose a dynamical model based on linear mixing\nprocesses at each time instant. The spectral signatures and fractional\nabundances of the pure materials in the scene are seen as latent variables, and\nassumed to follow a general dynamical structure. Based on a simplified version\nof this model, we derive an efficient spectral unmixing algorithm to estimate\nthe latent variables by performing alternating minimizations. The performance\nof the proposed approach is demonstrated on synthetic and real multitemporal\nhyperspectral images.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2015 18:51:51 GMT"}], "update_date": "2016-06-29", "authors_parsed": [["Henrot", "Simon", ""], ["Chanussot", "Jocelyn", ""], ["Jutten", "Christian", ""]]}, {"id": "1510.04373", "submitter": "Muhammad Ghifary", "authors": "Muhammad Ghifary and David Balduzzi and W. Bastiaan Kleijn and Mengjie\n  Zhang", "title": "Scatter Component Analysis: A Unified Framework for Domain Adaptation\n  and Domain Generalization", "comments": "to appear in IEEE Transactions on Pattern Analysis and Machine\n  Intelligence", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses classification tasks on a particular target domain in\nwhich labeled training data are only available from source domains different\nfrom (but related to) the target. Two closely related frameworks, domain\nadaptation and domain generalization, are concerned with such tasks, where the\nonly difference between those frameworks is the availability of the unlabeled\ntarget data: domain adaptation can leverage unlabeled target information, while\ndomain generalization cannot. We propose Scatter Component Analyis (SCA), a\nfast representation learning algorithm that can be applied to both domain\nadaptation and domain generalization. SCA is based on a simple geometrical\nmeasure, i.e., scatter, which operates on reproducing kernel Hilbert space. SCA\nfinds a representation that trades between maximizing the separability of\nclasses, minimizing the mismatch between domains, and maximizing the\nseparability of data; each of which is quantified through scatter. The\noptimization problem of SCA can be reduced to a generalized eigenvalue problem,\nwhich results in a fast and exact solution. Comprehensive experiments on\nbenchmark cross-domain object recognition datasets verify that SCA performs\nmuch faster than several state-of-the-art algorithms and also provides\nstate-of-the-art classification accuracy in both domain adaptation and domain\ngeneralization. We also show that scatter can be used to establish a\ntheoretical generalization bound in the case of domain adaptation.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2015 01:41:12 GMT"}, {"version": "v2", "created": "Tue, 26 Jul 2016 21:35:08 GMT"}], "update_date": "2016-07-28", "authors_parsed": [["Ghifary", "Muhammad", ""], ["Balduzzi", "David", ""], ["Kleijn", "W. Bastiaan", ""], ["Zhang", "Mengjie", ""]]}, {"id": "1510.04389", "submitter": "Yusuke Matsui", "authors": "Yusuke Matsui, Kota Ito, Yuji Aramaki, Toshihiko Yamasaki, Kiyoharu\n  Aizawa", "title": "Sketch-based Manga Retrieval using Manga109 Dataset", "comments": "13 pages", "journal-ref": "Multimedia Tools and Applications, Volume 76, Issue 20, 2017", "doi": "10.1007/s11042-016-4020-z", "report-no": null, "categories": "cs.CV cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Manga (Japanese comics) are popular worldwide. However, current e-manga\narchives offer very limited search support, including keyword-based search by\ntitle or author, or tag-based categorization. To make the manga search\nexperience more intuitive, efficient, and enjoyable, we propose a content-based\nmanga retrieval system. First, we propose a manga-specific image-describing\nframework. It consists of efficient margin labeling, edge orientation histogram\nfeature description, and approximate nearest-neighbor search using product\nquantization. Second, we propose a sketch-based interface as a natural way to\ninteract with manga content. The interface provides sketch-based querying,\nrelevance feedback, and query retouch. For evaluation, we built a novel dataset\nof manga images, Manga109, which consists of 109 comic books of 21,142 pages\ndrawn by professional manga artists. To the best of our knowledge, Manga109 is\ncurrently the biggest dataset of manga images available for research. We\nconducted a comparative study, a localization evaluation, and a large-scale\nqualitative study. From the experiments, we verified that: (1) the retrieval\naccuracy of the proposed method is higher than those of previous methods; (2)\nthe proposed method can localize an object instance with reasonable runtime and\naccuracy; and (3) sketch querying is useful for manga search.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2015 03:47:46 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Matsui", "Yusuke", ""], ["Ito", "Kota", ""], ["Aramaki", "Yuji", ""], ["Yamasaki", "Toshihiko", ""], ["Aizawa", "Kiyoharu", ""]]}, {"id": "1510.04390", "submitter": "Manolis Tsakiris", "authors": "Manolis C. Tsakiris and Rene Vidal", "title": "Dual Principal Component Pursuit", "comments": "fixed two typos in section 7.3", "journal-ref": "Journal of Machine Learning Research 19 (2018) 1-49", "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning a linear subspace from data corrupted by\noutliers. Classical approaches are typically designed for the case in which the\nsubspace dimension is small relative to the ambient dimension. Our approach\nworks with a dual representation of the subspace and hence aims to find its\northogonal complement; as such, it is particularly suitable for subspaces whose\ndimension is close to the ambient dimension (subspaces of high relative\ndimension). We pose the problem of computing normal vectors to the inlier\nsubspace as a non-convex $\\ell_1$ minimization problem on the sphere, which we\ncall Dual Principal Component Pursuit (DPCP) problem. We provide theoretical\nguarantees under which every global solution to DPCP is a vector in the\northogonal complement of the inlier subspace. Moreover, we relax the non-convex\nDPCP problem to a recursion of linear programs whose solutions are shown to\nconverge in a finite number of steps to a vector orthogonal to the subspace. In\nparticular, when the inlier subspace is a hyperplane, the solutions to the\nrecursion of linear programs converge to the global minimum of the non-convex\nDPCP problem in a finite number of steps. We also propose algorithms based on\nalternating minimization and iteratively re-weighted least squares, which are\nsuitable for dealing with large-scale data. Experiments on synthetic data show\nthat the proposed methods are able to handle more outliers and higher relative\ndimensions than current state-of-the-art methods, while experiments in the\ncontext of the three-view geometry problem in computer vision suggest that the\nproposed methods can be a useful or even superior alternative to traditional\nRANSAC-based approaches for computer vision and other applications.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2015 03:50:01 GMT"}, {"version": "v2", "created": "Sun, 4 Jun 2017 21:42:05 GMT"}, {"version": "v3", "created": "Sat, 22 Jul 2017 16:24:27 GMT"}, {"version": "v4", "created": "Sat, 4 Aug 2018 18:53:49 GMT"}, {"version": "v5", "created": "Thu, 7 Nov 2019 11:11:23 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Tsakiris", "Manolis C.", ""], ["Vidal", "Rene", ""]]}, {"id": "1510.04396", "submitter": "Manolis Tsakiris", "authors": "Manolis C. Tsakiris and Rene Vidal", "title": "Filtrated Spectral Algebraic Subspace Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Algebraic Subspace Clustering (ASC) is a simple and elegant method based on\npolynomial fitting and differentiation for clustering noiseless data drawn from\nan arbitrary union of subspaces. In practice, however, ASC is limited to\nequi-dimensional subspaces because the estimation of the subspace dimension via\nalgebraic methods is sensitive to noise. This paper proposes a new ASC\nalgorithm that can handle noisy data drawn from subspaces of arbitrary\ndimensions. The key ideas are (1) to construct, at each point, a decreasing\nsequence of subspaces containing the subspace passing through that point; (2)\nto use the distances from any other point to each subspace in the sequence to\nconstruct a subspace clustering affinity, which is superior to alternative\naffinities both in theory and in practice. Experiments on the Hopkins 155\ndataset demonstrate the superiority of the proposed method with respect to\nsparse and low rank subspace clustering methods.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2015 04:12:37 GMT"}], "update_date": "2015-10-16", "authors_parsed": [["Tsakiris", "Manolis C.", ""], ["Vidal", "Rene", ""]]}, {"id": "1510.04437", "submitter": "Satyabrata Maity", "authors": "Satyabrata Maity, Debotosh Bhattacharjee and Amlan Chakrabarti", "title": "A Novel Approach for Human Action Recognition from Silhouette Images", "comments": "Manuscript", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a novel human action recognition technique from video is\npresented. Any action of human is a combination of several micro action\nsequences performed by one or more body parts of the human. The proposed\napproach uses spatio-temporal body parts movement (STBPM) features extracted\nfrom foreground silhouette of the human objects. The newly proposed STBPM\nfeature estimates the movements of different body parts for any given time\nsegment to classify actions. We also proposed a rule based logic named rule\naction classifier (RAC), which uses a series of condition action rules based on\nprior knowledge and hence does not required training to classify any action.\nSince we don't require training to classify actions, the proposed approach is\nview independent. The experimental results on publicly available Wizeman and\nMuHVAi datasets are compared with that of the related research work in terms of\naccuracy in the human action detection, and proposed technique outperforms the\nothers.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2015 08:10:42 GMT"}], "update_date": "2015-10-16", "authors_parsed": [["Maity", "Satyabrata", ""], ["Bhattacharjee", "Debotosh", ""], ["Chakrabarti", "Amlan", ""]]}, {"id": "1510.04445", "submitter": "Ali Diba", "authors": "Amir Ghodrati, Ali Diba, Marco Pedersoli, Tinne Tuytelaars, Luc Van\n  Gool", "title": "DeepProposal: Hunting Objects by Cascading Deep Convolutional Layers", "comments": "ICCV 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we evaluate the quality of the activation layers of a\nconvolutional neural network (CNN) for the gen- eration of object proposals. We\ngenerate hypotheses in a sliding-window fashion over different activation\nlayers and show that the final convolutional layers can find the object of\ninterest with high recall but poor localization due to the coarseness of the\nfeature maps. Instead, the first layers of the network can better localize the\nobject of interest but with a reduced recall. Based on this observation we\ndesign a method for proposing object locations that is based on CNN features\nand that combines the best of both worlds. We build an inverse cascade that,\ngoing from the final to the initial convolutional layers of the CNN, selects\nthe most promising object locations and refines their boxes in a coarse-to-fine\nmanner. The method is efficient, because i) it uses the same features extracted\nfor detection, ii) it aggregates features using integral images, and iii) it\navoids a dense evaluation of the proposals due to the inverse coarse-to-fine\ncascade. The method is also accurate; it outperforms most of the previously\nproposed object proposals approaches and when plugged into a CNN-based detector\nproduces state-of-the- art detection performance.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2015 08:34:54 GMT"}], "update_date": "2015-10-16", "authors_parsed": [["Ghodrati", "Amir", ""], ["Diba", "Ali", ""], ["Pedersoli", "Marco", ""], ["Tuytelaars", "Tinne", ""], ["Van Gool", "Luc", ""]]}, {"id": "1510.04493", "submitter": "Spyridoula Xenaki", "authors": "Spyridoula D. Xenaki, Konstantinos D. Koutroumbas and Athanasios A.\n  Rontogiannis", "title": "Sparsity-aware Possibilistic Clustering Algorithms", "comments": "arXiv admin note: text overlap with arXiv:1412.3613", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper two novel possibilistic clustering algorithms are presented,\nwhich utilize the concept of sparsity. The first one, called sparse\npossibilistic c-means, exploits sparsity and can deal well with closely located\nclusters that may also be of significantly different densities. The second one,\ncalled sparse adaptive possibilistic c-means, is an extension of the first,\nwhere now the involved parameters are dynamically adapted. The latter can deal\nwell with even more challenging cases, where, in addition to the above,\nclusters may be of significantly different variances. More specifically, it\nprovides improved estimates of the cluster representatives, while, in addition,\nit has the ability to estimate the actual number of clusters, given an\noverestimate of it. Extensive experimental results on both synthetic and real\ndata sets support the previous statements.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2015 12:18:58 GMT"}], "update_date": "2015-10-16", "authors_parsed": [["Xenaki", "Spyridoula D.", ""], ["Koutroumbas", "Konstantinos D.", ""], ["Rontogiannis", "Athanasios A.", ""]]}, {"id": "1510.04563", "submitter": "Konrad Simon", "authors": "Konrad Simon, Ronen Basri", "title": "Elasticity-based Matching by Minimizing the Symmetric Difference of\n  Shapes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of matching two shapes assuming these shapes are\nrelated by an elastic deformation. Using linearized elasticity theory and the\nfinite element method we seek an elastic deformation that is caused by simple\nexternal boundary forces and accounts for the difference between the two\nshapes. Our main contribution is in proposing a cost function and an\noptimization procedure to minimize the symmetric difference between the\ndeformed and the target shapes as an alternative to point matches that guide\nthe matching in other techniques. We show how to approximate the nonlinear\noptimization problem by a sequence of convex problems. We demonstrate the\nutility of our method in experiments and compare it to an ICP-like matching\nalgorithm.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2015 14:51:35 GMT"}], "update_date": "2015-10-16", "authors_parsed": [["Simon", "Konrad", ""], ["Basri", "Ronen", ""]]}, {"id": "1510.04565", "submitter": "Zhenzhong Lan", "authors": "Zhenzhong Lan, Alexander G. Hauptmann", "title": "Beyond Spatial Pyramid Matching: Space-time Extended Descriptor for\n  Action Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of generating video features for action recognition.\nThe spatial pyramid and its variants have been very popular feature models due\nto their success in balancing spatial location encoding and spatial invariance.\nAlthough it seems straightforward to extend spatial pyramid to the temporal\ndomain (spatio-temporal pyramid), the large spatio-temporal diversity of\nunconstrained videos and the resulting significantly higher dimensional\nrepresentations make it less appealing. This paper introduces the space-time\nextended descriptor, a simple but efficient alternative way to include the\nspatio-temporal location into the video features. Instead of only coding motion\ninformation and leaving the spatio-temporal location to be represented at the\npooling stage, location information is used as part of the encoding step. This\nmethod is a much more effective and efficient location encoding method as\ncompared to the fixed grid model because it avoids the danger of over\ncommitting to artificial boundaries and its dimension is relatively low.\nExperimental results on several benchmark datasets show that, despite its\nsimplicity, this method achieves comparable or better results than\nspatio-temporal pyramid.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2015 14:57:37 GMT"}], "update_date": "2015-10-16", "authors_parsed": [["Lan", "Zhenzhong", ""], ["Hauptmann", "Alexander G.", ""]]}, {"id": "1510.04585", "submitter": "Kezhi Li", "authors": "Kezhi Li", "title": "A Brief Survey of Image Processing Algorithms in Electrical Capacitance\n  Tomography", "comments": "Internal Report, MRRC, University of Cambridge", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To study the fundamental physics of complex multiphase flow systems using\nadvanced measurement techniques, especially the electrical capacitance\ntomography (ECT) approach, this article carries out an initial literature\nreview of the ECT method from a point of view of signal processing and\nalgorithm design. After introducing the physical laws governing the ECT system,\nwe will focus on various reconstruction techniques that are capable to recover\nthe image of the internal characteristics of a specified region based on the\nmeasuring capacitances of multi-electrode sensors surrounding the region. Each\ntechnique has its own advantages and limitations, and many algorithms have been\nexamined by simulations or experiments. Future researches in 3D reconstruction\nand other potential improvements of the system are discussed in the end.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2015 15:36:03 GMT"}], "update_date": "2015-10-16", "authors_parsed": [["Li", "Kezhi", ""]]}, {"id": "1510.04601", "submitter": "Tal Remez", "authors": "Tal Remez, Or Litany and Alex Bronstein", "title": "A Picture is Worth a Billion Bits: Real-Time Image Reconstruction from\n  Dense Binary Pixels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The pursuit of smaller pixel sizes at ever increasing resolution in digital\nimage sensors is mainly driven by the stringent price and form-factor\nrequirements of sensors and optics in the cellular phone market. Recently, Eric\nFossum proposed a novel concept of an image sensor with dense sub-diffraction\nlimit one-bit pixels jots, which can be considered a digital emulation of\nsilver halide photographic film. This idea has been recently embodied as the\nEPFL Gigavision camera. A major bottleneck in the design of such sensors is the\nimage reconstruction process, producing a continuous high dynamic range image\nfrom oversampled binary measurements. The extreme quantization of the Poisson\nstatistics is incompatible with the assumptions of most standard image\nprocessing and enhancement frameworks. The recently proposed maximum-likelihood\n(ML) approach addresses this difficulty, but suffers from image artifacts and\nhas impractically high computational complexity. In this work, we study a\nvariant of a sensor with binary threshold pixels and propose a reconstruction\nalgorithm combining an ML data fitting term with a sparse synthesis prior. We\nalso show an efficient hardware-friendly real-time approximation of this\ninverse operator.Promising results are shown on synthetic data as well as on\nHDR data emulated using multiple exposures of a regular CMOS sensor.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2015 16:05:06 GMT"}, {"version": "v2", "created": "Sat, 5 Dec 2015 10:13:16 GMT"}], "update_date": "2015-12-08", "authors_parsed": [["Remez", "Tal", ""], ["Litany", "Or", ""], ["Bronstein", "Alex", ""]]}, {"id": "1510.04609", "submitter": "Bharat Singh", "authors": "Bharat Singh, Soham De, Yangmuzi Zhang, Thomas Goldstein, and Gavin\n  Taylor", "title": "Layer-Specific Adaptive Learning Rates for Deep Networks", "comments": "ICMLA 2015, deep learning, adaptive learning rates for training,\n  layer specific learning rate", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing complexity of deep learning architectures is resulting in\ntraining time requiring weeks or even months. This slow training is due in part\nto vanishing gradients, in which the gradients used by back-propagation are\nextremely large for weights connecting deep layers (layers near the output\nlayer), and extremely small for shallow layers (near the input layer); this\nresults in slow learning in the shallow layers. Additionally, it has also been\nshown that in highly non-convex problems, such as deep neural networks, there\nis a proliferation of high-error low curvature saddle points, which slows down\nlearning dramatically. In this paper, we attempt to overcome the two above\nproblems by proposing an optimization method for training deep neural networks\nwhich uses learning rates which are both specific to each layer in the network\nand adaptive to the curvature of the function, increasing the learning rate at\nlow curvature points. This enables us to speed up learning in the shallow\nlayers of the network and quickly escape high-error low curvature saddle\npoints. We test our method on standard image classification datasets such as\nMNIST, CIFAR10 and ImageNet, and demonstrate that our method increases accuracy\nas well as reduces the required training time over standard algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2015 16:31:46 GMT"}], "update_date": "2015-10-16", "authors_parsed": [["Singh", "Bharat", ""], ["De", "Soham", ""], ["Zhang", "Yangmuzi", ""], ["Goldstein", "Thomas", ""], ["Taylor", "Gavin", ""]]}, {"id": "1510.04706", "submitter": "John Stuart Haberl Baxter", "authors": "John S.H. Baxter, Jing Yuan and Terry M. Peters", "title": "Shape Complexes in Continuous Max-Flow Hierarchical Multi-Labeling\n  Problems", "comments": "9 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although topological considerations amongst multiple labels have been\npreviously investigated in the context of continuous max-flow image\nsegmentation, similar investigations have yet to be made about shape\nconsiderations in a general and extendable manner. This paper presents shape\ncomplexes for segmentation, which capture more complex shapes by combining\nmultiple labels and super-labels constrained by geodesic star convexity. Shape\ncomplexes combine geodesic star convexity constraints with hierarchical label\norganization, which together allow for more complex shapes to be represented.\nThis framework avoids the use of co-ordinate system warping techniques to\nconvert shape constraints into topological constraints, which may be ambiguous\nor ill-defined for certain segmentation problems.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2015 20:10:49 GMT"}], "update_date": "2015-10-19", "authors_parsed": [["Baxter", "John S. H.", ""], ["Yuan", "Jing", ""], ["Peters", "Terry M.", ""]]}, {"id": "1510.04709", "submitter": "Desmond Elliott", "authors": "Desmond Elliott, Stella Frank, Eva Hasler", "title": "Multilingual Image Description with Neural Sequence Models", "comments": "Under review as a conference paper at ICLR 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present an approach to multi-language image description\nbringing together insights from neural machine translation and neural image\ndescription. To create a description of an image for a given target language,\nour sequence generation models condition on feature vectors from the image, the\ndescription from the source language, and/or a multimodal vector computed over\nthe image and a description in the source language. In image description\nexperiments on the IAPR-TC12 dataset of images aligned with English and German\nsentences, we find significant and substantial improvements in BLEU4 and Meteor\nscores for models trained over multiple languages, compared to a monolingual\nbaseline.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2015 20:29:21 GMT"}, {"version": "v2", "created": "Wed, 18 Nov 2015 17:04:35 GMT"}], "update_date": "2015-11-19", "authors_parsed": [["Elliott", "Desmond", ""], ["Frank", "Stella", ""], ["Hasler", "Eva", ""]]}, {"id": "1510.04842", "submitter": "David Varas", "authors": "David Varas, M\\'onica Alfaro and Ferran Marques", "title": "Multiresolution hierarchy co-clustering for semantic segmentation in\n  sequences with small variations", "comments": "International Conference on Computer Vision (ICCV) 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a co-clustering technique that, given a collection of\nimages and their hierarchies, clusters nodes from these hierarchies to obtain a\ncoherent multiresolution representation of the image collection. We formalize\nthe co-clustering as a Quadratic Semi-Assignment Problem and solve it with a\nlinear programming relaxation approach that makes effective use of information\nfrom hierarchies. Initially, we address the problem of generating an optimal,\ncoherent partition per image and, afterwards, we extend this method to a\nmultiresolution framework. Finally, we particularize this framework to an\niterative multiresolution video segmentation algorithm in sequences with small\nvariations. We evaluate the algorithm on the Video Occlusion/Object Boundary\nDetection Dataset, showing that it produces state-of-the-art results in these\nscenarios.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2015 11:25:33 GMT"}], "update_date": "2015-10-19", "authors_parsed": [["Varas", "David", ""], ["Alfaro", "M\u00f3nica", ""], ["Marques", "Ferran", ""]]}, {"id": "1510.04860", "submitter": "Tomislav Petkovi\\'c", "authors": "Kristian Kova\\v{c}i\\'c, Edouard Ivanjko, Niko Jelu\\v{s}i\\'c", "title": "Measurement of Road Traffic Parameters Based on Multi-Vehicle Tracking", "comments": "Part of the Proceedings of the Croatian Computer Vision Workshop,\n  CCVW 2015, Year 3", "journal-ref": null, "doi": null, "report-no": "UniZg-CRV-CCVW/2015/0010", "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Development of computing power and cheap video cameras enabled today's\ntraffic management systems to include more cameras and computer vision\napplications for transportation system monitoring and control. Combined with\nimage processing algorithms cameras are used as sensors to measure road traffic\nparameters like flow volume, origin-destination matrices, classify vehicles,\netc. In this paper we propose a system for measurement of road traffic\nparameters (basic motion model parameters and macro-scopic traffic parameters).\nThe system is based on Local Binary Pattern (LBP) image features classification\nwith a cascade of Gentle Adaboost (GAB) classifiers to determine vehicle\nexistence and its location in an image. Additionally, vehicle tracking and\ncounting in a road traffic video is performed by using Extended Kalman Filter\n(EKF) and virtual markers. The newly proposed system is compared with a system\nbased on background subtraction. Comparison is performed by the means of\nexecution time and accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2015 12:26:29 GMT"}], "update_date": "2015-10-19", "authors_parsed": [["Kova\u010di\u0107", "Kristian", ""], ["Ivanjko", "Edouard", ""], ["Jelu\u0161i\u0107", "Niko", ""]]}, {"id": "1510.04861", "submitter": "Tomislav Petkovi\\'c", "authors": "Martin Bla\\v{z}evi\\'c, Karla Brki\\'c, Tomislav Hrka\\'c", "title": "Towards Reversible De-Identification in Video Sequences Using 3D Avatars\n  and Steganography", "comments": "Part of the Proceedings of the Croatian Computer Vision Workshop,\n  CCVW 2015, Year 3", "journal-ref": null, "doi": null, "report-no": "UniZg-CRV-CCVW/2015/0011", "categories": "cs.CV cs.MM", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We propose a de-identification pipeline that protects the privacy of humans\nin video sequences by replacing them with rendered 3D human models, hence\nconcealing their identity while retaining the naturalness of the scene. The\noriginal images of humans are steganographically encoded in the carrier image,\ni.e. the image containing the original scene and the rendered 3D human models.\nWe qualitatively explore the feasibility of our approach, utilizing the Kinect\nsensor and its libraries to detect and localize human joints. A 3D avatar is\nrendered into the scene using the obtained joint positions, and the original\nhuman image is steganographically encoded in the new scene. Our qualitative\nevaluation shows reasonably good results that merit further exploration.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2015 12:31:29 GMT"}], "update_date": "2015-10-19", "authors_parsed": [["Bla\u017eevi\u0107", "Martin", ""], ["Brki\u0107", "Karla", ""], ["Hrka\u0107", "Tomislav", ""]]}, {"id": "1510.04862", "submitter": "Dima Damen", "authors": "Dima Damen, Teesid Leelasawassuk, Walterio Mayol-Cuevas", "title": "You-Do, I-Learn: Unsupervised Multi-User egocentric Approach Towards\n  Video-Based Guidance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an unsupervised approach towards automatically extracting\nvideo-based guidance on object usage, from egocentric video and wearable gaze\ntracking, collected from multiple users while performing tasks. The approach i)\ndiscovers task relevant objects, ii) builds a model for each, iii)\ndistinguishes different ways in which each discovered object has been used and\niv) discovers the dependencies between object interactions. The work\ninvestigates using appearance, position, motion and attention, and presents\nresults using each and a combination of relevant features. Moreover, an online\nscalable approach is presented and is compared to offline results. The paper\nproposes a method for selecting a suitable video guide to be displayed to a\nnovice user indicating how to use an object, purely triggered by the user's\ngaze. The potential assistive mode can also recommend an object to be used next\nbased on the learnt sequence of object interactions. The approach was tested on\na variety of daily tasks such as initialising a printer, preparing a coffee and\nsetting up a gym machine.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2015 12:32:26 GMT"}, {"version": "v2", "created": "Sat, 19 Mar 2016 17:44:58 GMT"}], "update_date": "2016-03-22", "authors_parsed": [["Damen", "Dima", ""], ["Leelasawassuk", "Teesid", ""], ["Mayol-Cuevas", "Walterio", ""]]}, {"id": "1510.04863", "submitter": "Tomislav Petkovi\\'c", "authors": "Tomislav Petkovi\\'c, Sven Lon\\v{c}ari\\'c", "title": "An Extension to Hough Transform Based on Gradient Orientation", "comments": "Part of the Proceedings of the Croatian Computer Vision Workshop,\n  CCVW 2015, Year 3", "journal-ref": null, "doi": null, "report-no": "UniZg-CRV-CCVW/2015/0012", "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The Hough transform is one of the most common methods for line detection. In\nthis paper we propose a novel extension of the regular Hough transform. The\nproposed extension combines the extension of the accumulator space and the\nlocal gradient orientation resulting in clutter reduction and yielding more\nprominent peaks, thus enabling better line identification. We demonstrate\nbenefits in applications such as visual quality inspection and rectangle\ndetection.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2015 12:36:13 GMT"}], "update_date": "2015-10-19", "authors_parsed": [["Petkovi\u0107", "Tomislav", ""], ["Lon\u010dari\u0107", "Sven", ""]]}, {"id": "1510.04908", "submitter": "Pascal Mettes", "authors": "Pascal Mettes, Jan C. van Gemert, Cees G. M. Snoek", "title": "No Spare Parts: Sharing Part Detectors for Image Categorization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work aims for image categorization using a representation of distinctive\nparts. Different from existing part-based work, we argue that parts are\nnaturally shared between image categories and should be modeled as such. We\nmotivate our approach with a quantitative and qualitative analysis by\nbacktracking where selected parts come from. Our analysis shows that in\naddition to the category parts defining the class, the parts coming from the\nbackground context and parts from other image categories improve categorization\nperformance. Part selection should not be done separately for each category,\nbut instead be shared and optimized over all categories. To incorporate part\nsharing between categories, we present an algorithm based on AdaBoost to\njointly optimize part sharing and selection, as well as fusion with the global\nimage representation. We achieve results competitive to the state-of-the-art on\nobject, scene, and action categories, further improving over deep convolutional\nneural networks.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2015 15:05:41 GMT"}, {"version": "v2", "created": "Tue, 12 Jul 2016 17:05:27 GMT"}], "update_date": "2016-07-13", "authors_parsed": [["Mettes", "Pascal", ""], ["van Gemert", "Jan C.", ""], ["Snoek", "Cees G. M.", ""]]}, {"id": "1510.05138", "submitter": "Shoaib Ehsan", "authors": "Shoaib Ehsan, Adrian F. Clark, Naveed ur Rehman and Klaus D.\n  McDonald-Maier", "title": "Integral Images: Efficient Algorithms for Their Computation and Storage\n  in Resource-Constrained Embedded Vision Systems", "comments": null, "journal-ref": "Sensors 2015, 15, 16804-16830", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The integral image, an intermediate image representation, has found extensive\nuse in multi-scale local feature detection algorithms, such as Speeded-Up\nRobust Features (SURF), allowing fast computation of rectangular features at\nconstant speed, independent of filter size. For resource-constrained real-time\nembedded vision systems, computation and storage of integral image presents\nseveral design challenges due to strict timing and hardware limitations.\nAlthough calculation of the integral image only consists of simple addition\noperations, the total number of operations is large owing to the generally\nlarge size of image data. Recursive equations allow substantial decrease in the\nnumber of operations but require calculation in a serial fashion. This paper\npresents two new hardware algorithms that are based on the decomposition of\nthese recursive equations, allowing calculation of up to four integral image\nvalues in a row-parallel way without significantly increasing the number of\noperations. An efficient design strategy is also proposed for a parallel\nintegral image computation unit to reduce the size of the required internal\nmemory (nearly 35% for common HD video). Addressing the storage problem of\nintegral image in embedded vision systems, the paper presents two algorithms\nwhich allow substantial decrease (at least 44.44%) in the memory requirements.\nFinally, the paper provides a case study that highlights the utility of the\nproposed architectures in embedded vision systems.\n", "versions": [{"version": "v1", "created": "Sat, 17 Oct 2015 15:41:26 GMT"}], "update_date": "2015-10-20", "authors_parsed": [["Ehsan", "Shoaib", ""], ["Clark", "Adrian F.", ""], ["Rehman", "Naveed ur", ""], ["McDonald-Maier", "Klaus D.", ""]]}, {"id": "1510.05142", "submitter": "Shoaib Ehsan", "authors": "Shoaib Ehsan, Adrian F. Clark, Wah M. Cheung, Arjunsingh M. Bais,\n  Bayar I. Menzat, Nadia Kanwal and Klaus D. McDonald-Maier", "title": "Memory-Efficient Design Strategy for a Parallel Embedded Integral Image\n  Computation Engine", "comments": "Machine Vision and Image Processing Conference (IMVIP), 2011", "journal-ref": null, "doi": "10.1109/IMVIP.2011.29", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In embedded vision systems, parallel computation of the integral image\npresents several design challenges in terms of hardware resources, speed and\npower consumption. Although recursive equations significantly reduce the number\nof operations for computing the integral image, the required internal memory\nbecomes prohibitively large for an embedded integral image computation engine\nfor increasing image sizes. With the objective of achieving high-throughput\nwith minimum hardware resources, this paper proposes a memory-efficient design\nstrategy for a parallel embedded integral image computation engine. Results\nshow that the design achieves nearly 35% reduction in memory for common HD\nvideo.\n", "versions": [{"version": "v1", "created": "Sat, 17 Oct 2015 16:04:33 GMT"}], "update_date": "2015-10-20", "authors_parsed": [["Ehsan", "Shoaib", ""], ["Clark", "Adrian F.", ""], ["Cheung", "Wah M.", ""], ["Bais", "Arjunsingh M.", ""], ["Menzat", "Bayar I.", ""], ["Kanwal", "Nadia", ""], ["McDonald-Maier", "Klaus D.", ""]]}, {"id": "1510.05145", "submitter": "Shoaib Ehsan", "authors": "Shoaib Ehsan, Adrian F. Clark and Klaus D. McDonald-Maier", "title": "Rapid Online Analysis of Local Feature Detectors and Their\n  Complementarity", "comments": null, "journal-ref": "Sensors 2013, 13, 10876-10907", "doi": "10.3390/s130810876", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A vision system that can assess its own performance and take appropriate\nactions online to maximize its effectiveness would be a step towards achieving\nthe long-cherished goal of imitating humans. This paper proposes a method for\nperforming an online performance analysis of local feature detectors, the\nprimary stage of many practical vision systems. It advocates the spatial\ndistribution of local image features as a good performance indicator and\npresents a metric that can be calculated rapidly, concurs with human visual\nassessments and is complementary to existing offline measures such as\nrepeatability. The metric is shown to provide a measure of complementarity for\ncombinations of detectors, correctly reflecting the underlying principles of\nindividual detectors. Qualitative results on well-established datasets for\nseveral state-of-the-art detectors are presented based on the proposed measure.\nUsing a hypothesis testing approach and a newly-acquired, larger image\ndatabase, statistically-significant performance differences are identified.\nDifferent detector pairs and triplets are examined quantitatively and the\nresults provide a useful guideline for combining detectors in applications that\nrequire a reasonable spatial distribution of image features. A principled\nframework for combining feature detectors in these applications is also\npresented. Timing results reveal the potential of the metric for online\napplications.\n", "versions": [{"version": "v1", "created": "Sat, 17 Oct 2015 16:14:11 GMT"}], "update_date": "2015-10-20", "authors_parsed": [["Ehsan", "Shoaib", ""], ["Clark", "Adrian F.", ""], ["McDonald-Maier", "Klaus D.", ""]]}, {"id": "1510.05156", "submitter": "Shoaib Ehsan", "authors": "Shoaib Ehsan, Adrian F. Clark, Bruno Ferrarini, Naveed Ur Rehman and\n  Klaus D. McDonald-Maier", "title": "Assessing The Performance Bounds Of Local Feature Detectors: Taking\n  Inspiration From Electronics Design Practices", "comments": "IWSSIP 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since local feature detection has been one of the most active research areas\nin computer vision, a large number of detectors have been proposed. This has\nrendered the task of characterizing the performance of various feature\ndetection methods an important issue in vision research. Inspired by the good\npractices of electronic system design, a generic framework based on the\nimproved repeatability measure is presented in this paper that allows\nassessment of the upper and lower bounds of detector performance in an effort\nto design more reliable and effective vision systems. This framework is then\nemployed to establish operating and guarantee regions for several state-of-the\nart detectors for JPEG compression and uniform light changes. The results are\nobtained using a newly acquired, large image database (15092 images) with 539\ndifferent scenes. These results provide new insights into the behavior of\ndetectors and are also useful from the vision systems design perspective.\n", "versions": [{"version": "v1", "created": "Sat, 17 Oct 2015 19:07:44 GMT"}], "update_date": "2015-10-20", "authors_parsed": [["Ehsan", "Shoaib", ""], ["Clark", "Adrian F.", ""], ["Ferrarini", "Bruno", ""], ["Rehman", "Naveed Ur", ""], ["McDonald-Maier", "Klaus D.", ""]]}, {"id": "1510.05157", "submitter": "Shoaib Ehsan", "authors": "Bruno Ferrarini, Shoaib Ehsan, Naveed Ur Rehman and Klaus D.\n  McDonald-Maier", "title": "Performance Characterization of Image Feature Detectors in Relation to\n  the Scene Content Utilizing a Large Image Database", "comments": "IWSSIP 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Selecting the most suitable local invariant feature detector for a particular\napplication has rendered the task of evaluating feature detectors a critical\nissue in vision research. No state-of-the-art image feature detector works\nsatisfactorily under all types of image transformations. Although the\nliterature offers a variety of comparison works focusing on performance\nevaluation of image feature detectors under several types of image\ntransformation, the influence of the scene content on the performance of local\nfeature detectors has received little attention so far. This paper aims to\nbridge this gap with a new framework for determining the type of scenes, which\nmaximize and minimize the performance of detectors in terms of repeatability\nrate. Several state-of-the-art feature detectors have been assessed utilizing a\nlarge database of 12936 images generated by applying uniform light and blur\nchanges to 539 scenes captured from the real world. The results obtained\nprovide new insights into the behaviour of feature detectors.\n", "versions": [{"version": "v1", "created": "Sat, 17 Oct 2015 19:13:10 GMT"}], "update_date": "2015-10-20", "authors_parsed": [["Ferrarini", "Bruno", ""], ["Ehsan", "Shoaib", ""], ["Rehman", "Naveed Ur", ""], ["McDonald-Maier", "Klaus D.", ""]]}, {"id": "1510.05275", "submitter": "Hongmin Li", "authors": "Hongmin Li, Pei Jing, Guoqi Li", "title": "Real-time Tracking Based on Neuromrophic Vision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-time tracking is an important problem in computer vision in which most\nmethods are based on the conventional cameras. Neuromorphic vision is a concept\ndefined by incorporating neuromorphic vision sensors such as silicon retinas in\nvision processing system. With the development of the silicon technology,\nasynchronous event-based silicon retinas that mimic neuro-biological\narchitectures has been developed in recent years. In this work, we combine the\nvision tracking algorithm of computer vision with the information encoding\nmechanism of event-based sensors which is inspired from the neural rate coding\nmechanism. The real-time tracking of single object with the advantage of high\nspeed of 100 time bins per second is successfully realized. Our method\ndemonstrates that the computer vision methods could be used for the\nneuromorphic vision processing and we can realize fast real-time tracking using\nneuromorphic vision sensors compare to the conventional camera.\n", "versions": [{"version": "v1", "created": "Sun, 18 Oct 2015 16:27:36 GMT"}], "update_date": "2015-10-20", "authors_parsed": [["Li", "Hongmin", ""], ["Jing", "Pei", ""], ["Li", "Guoqi", ""]]}, {"id": "1510.05436", "submitter": "David Helbert", "authors": "Mohamed Malek and David Helbert and Philippe Carre", "title": "Color graph based wavelet transform with perceptual information", "comments": null, "journal-ref": "Journal of Electronic Imaging (SPIE), 24(5), 2015", "doi": "10.1117/1.JEI.24.5.053004", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a numerical strategy to define a multiscale\nanalysis for color and multicomponent images based on the representation of\ndata on a graph. Our approach consists in computing the graph of an image using\nthe psychovisual information and analysing it by using the spectral graph\nwavelet transform. We suggest introducing color dimension into the computation\nof the weights of the graph and using the geodesic distance as a means of\ndistance measurement. We thus have defined a wavelet transform based on a graph\nwith perceptual information by using the CIELab color distance. This new\nrepresentation is illustrated with denoising and inpainting applications.\nOverall, by introducing psychovisual information in the graph computation for\nthe graph wavelet transform we obtain very promising results. Therefore results\nin image restoration highlight the interest of the appropriate use of color\ninformation.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2015 11:48:23 GMT"}], "update_date": "2015-10-28", "authors_parsed": [["Malek", "Mohamed", ""], ["Helbert", "David", ""], ["Carre", "Philippe", ""]]}, {"id": "1510.05484", "submitter": "Liming Zhao", "authors": "Xi Li, Liming Zhao, Lina Wei, Ming-Hsuan Yang, Fei Wu, Yueting Zhuang,\n  Haibin Ling, and Jingdong Wang", "title": "DeepSaliency: Multi-Task Deep Neural Network Model for Salient Object\n  Detection", "comments": "To appear in IEEE Transactions on Image Processing (TIP), Project\n  Website: http://www.zhaoliming.net/research/deepsaliency", "journal-ref": null, "doi": "10.1109/TIP.2016.2579306", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key problem in salient object detection is how to effectively model the\nsemantic properties of salient objects in a data-driven manner. In this paper,\nwe propose a multi-task deep saliency model based on a fully convolutional\nneural network (FCNN) with global input (whole raw images) and global output\n(whole saliency maps). In principle, the proposed saliency model takes a\ndata-driven strategy for encoding the underlying saliency prior information,\nand then sets up a multi-task learning scheme for exploring the intrinsic\ncorrelations between saliency detection and semantic image segmentation.\nThrough collaborative feature learning from such two correlated tasks, the\nshared fully convolutional layers produce effective features for object\nperception. Moreover, it is capable of capturing the semantic information on\nsalient objects across different levels using the fully convolutional layers,\nwhich investigate the feature-sharing properties of salient object detection\nwith great feature redundancy reduction. Finally, we present a graph Laplacian\nregularized nonlinear regression model for saliency refinement. Experimental\nresults demonstrate the effectiveness of our approach in comparison with the\nstate-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2015 14:11:49 GMT"}, {"version": "v2", "created": "Tue, 7 Jun 2016 15:20:44 GMT"}], "update_date": "2016-08-24", "authors_parsed": [["Li", "Xi", ""], ["Zhao", "Liming", ""], ["Wei", "Lina", ""], ["Yang", "Ming-Hsuan", ""], ["Wu", "Fei", ""], ["Zhuang", "Yueting", ""], ["Ling", "Haibin", ""], ["Wang", "Jingdong", ""]]}, {"id": "1510.05559", "submitter": "Jong Chul Ye", "authors": "Kyong Hwan Jin and Jong Chul Ye", "title": "Sparse + Low Rank Decomposition of Annihilating Filter-based Hankel\n  Matrix for Impulse Noise Removal", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, so called annihilating filer-based low rank Hankel matrix (ALOHA)\napproach was proposed as a powerful image inpainting method. Based on the\nobservation that smoothness or textures within an image patch corresponds to\nsparse spectral components in the frequency domain, ALOHA exploits the\nexistence of annihilating filters and the associated rank-deficient Hankel\nmatrices in the image domain to estimate the missing pixels. By extending this\nidea, here we propose a novel impulse noise removal algorithm using sparse +\nlow rank decomposition of an annihilating filter-based Hankel matrix. The new\napproach, what we call the robust ALOHA, is motivated by the observation that\nan image corrupted with impulse noises has intact pixels; so the impulse noises\ncan be modeled as sparse components, whereas the underlying image can be still\nmodeled using a low-rank Hankel structured matrix. To solve the sparse + low\nrank decomposition problem, we propose an alternating direction method of\nmultiplier (ADMM) method with initial factorized matrices coming from low rank\nmatrix fitting (LMaFit) algorithm. To adapt the local image statistics that\nhave distinct spectral distributions, the robust ALOHA is applied patch by\npatch. Experimental results from two types of impulse noises - random valued\nimpulse noises and salt/pepper noises - for both single channel and\nmulti-channel color images demonstrate that the robust ALOHA outperforms the\nexisting algorithms up to 8dB in terms of the peak signal to noise ratio\n(PSNR).\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2015 16:11:24 GMT"}], "update_date": "2015-10-20", "authors_parsed": [["Jin", "Kyong Hwan", ""], ["Ye", "Jong Chul", ""]]}, {"id": "1510.05613", "submitter": "Venkatraman Narayanan", "authors": "Venkatraman Narayanan and Maxim Likhachev", "title": "PERCH: Perception via Search for Multi-Object Recognition and\n  Localization", "comments": "8 pages, International Conference on Robotics and Automation (ICRA),\n  2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many robotic domains such as flexible automated manufacturing or personal\nassistance, a fundamental perception task is that of identifying and localizing\nobjects whose 3D models are known. Canonical approaches to this problem include\ndiscriminative methods that find correspondences between feature descriptors\ncomputed over the model and observed data. While these methods have been\nemployed successfully, they can be unreliable when the feature descriptors fail\nto capture variations in observed data; a classic cause being occlusion. As a\nstep towards deliberative reasoning, we present PERCH: PErception via SeaRCH,\nan algorithm that seeks to find the best explanation of the observed sensor\ndata by hypothesizing possible scenes in a generative fashion. Our\ncontributions are: i) formulating the multi-object recognition and localization\ntask as an optimization problem over the space of hypothesized scenes, ii)\nexploiting structure in the optimization to cast it as a combinatorial search\nproblem on what we call the Monotone Scene Generation Tree, and iii) leveraging\nparallelization and recent advances in multi-heuristic search in making\ncombinatorial search tractable. We prove that our system can guaranteedly\nproduce the best explanation of the scene under the chosen cost function, and\nvalidate our claims on real world RGB-D test data. Our experimental results\nshow that we can identify and localize objects under heavy occlusion--cases\nwhere state-of-the-art methods struggle.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2015 18:39:05 GMT"}, {"version": "v2", "created": "Wed, 16 Mar 2016 20:49:52 GMT"}], "update_date": "2016-03-18", "authors_parsed": [["Narayanan", "Venkatraman", ""], ["Likhachev", "Maxim", ""]]}, {"id": "1510.05822", "submitter": "Xavier Gibert", "authors": "Xavier Gibert, Vishal M. Patel, Rama Chellappa", "title": "Sequential Score Adaptation with Extreme Value Theory for Robust Railway\n  Track Inspection", "comments": "To be presented at the 3rd Workshop on Computer Vision for Road Scene\n  Understanding and Autonomous Driving (CVRSUAD 2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Periodic inspections are necessary to keep railroad tracks in state of good\nrepair and prevent train accidents. Automatic track inspection using machine\nvision technology has become a very effective inspection tool. Because of its\nnon-contact nature, this technology can be deployed on virtually any railway\nvehicle to continuously survey the tracks and send exception reports to track\nmaintenance personnel. However, as appearance and imaging conditions vary,\nfalse alarm rates can dramatically change, making it difficult to select a good\noperating point. In this paper, we use extreme value theory (EVT) within a\nBayesian framework to optimally adjust the sensitivity of anomaly detectors. We\nshow that by approximating the lower tail of the probability density function\n(PDF) of the scores with an Exponential distribution (a special case of the\nGeneralized Pareto distribution), and using the Gamma conjugate prior learned\nfrom the training data, it is possible to reduce the variability in false alarm\nrate and improve the overall performance. This method has shown an increase in\nthe defect detection rate of rail fasteners in the presence of clutter (at PFA\n0.1%) from 95.40% to 99.26% on the 85-mile Northeast Corridor (NEC) 2012-2013\nconcrete tie dataset.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2015 10:16:43 GMT"}], "update_date": "2015-10-21", "authors_parsed": [["Gibert", "Xavier", ""], ["Patel", "Vishal M.", ""], ["Chellappa", "Rama", ""]]}, {"id": "1510.05879", "submitter": "Boris Schauerte", "authors": "Christian Wittner and Boris Schauerte and Rainer Stiefelhagen", "title": "What's the point? Frame-wise Pointing Gesture Recognition with\n  Latent-Dynamic Conditional Random Fields", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We use Latent-Dynamic Conditional Random Fields to perform skeleton-based\npointing gesture classification at each time instance of a video sequence,\nwhere we achieve a frame-wise pointing accuracy of roughly 83%. Subsequently,\nwe determine continuous time sequences of arbitrary length that form individual\npointing gestures and this way reliably detect pointing gestures at a false\npositive detection rate of 0.63%.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2015 13:16:07 GMT"}], "update_date": "2015-10-21", "authors_parsed": [["Wittner", "Christian", ""], ["Schauerte", "Boris", ""], ["Stiefelhagen", "Rainer", ""]]}, {"id": "1510.05893", "submitter": "Pierre-Antoine Thouvenin", "authors": "Pierre-Antoine Thouvenin, Nicolas Dobigeon, Jean-Yves Tourneret", "title": "Online Unmixing of Multitemporal Hyperspectral Images accounting for\n  Spectral Variability", "comments": "27 pages, 11 figures, accepted in IEEE Trans. Image Process., 2016", "journal-ref": null, "doi": "10.1109/TIP.2016.2579309", "report-no": null, "categories": "physics.data-an cs.CV stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hyperspectral unmixing is aimed at identifying the reference spectral\nsignatures composing an hyperspectral image and their relative abundance\nfractions in each pixel. In practice, the identified signatures may vary\nspectrally from an image to another due to varying acquisition conditions, thus\ninducing possibly significant estimation errors. Against this background,\nhyperspectral unmixing of several images acquired over the same area is of\nconsiderable interest. Indeed, such an analysis enables the endmembers of the\nscene to be tracked and the corresponding endmember variability to be\ncharacterized. Sequential endmember estimation from a set of hyperspectral\nimages is expected to provide improved performance when compared to methods\nanalyzing the images independently. However, the significant size of\nhyperspectral data precludes the use of batch procedures to jointly estimate\nthe mixture parameters of a sequence of hyperspectral images. Provided that\neach elementary component is present in at least one image of the sequence, we\npropose to perform an online hyperspectral unmixing accounting for temporal\nendmember variability. The online hyperspectral unmixing is formulated as a\ntwo-stage stochastic program, which can be solved using a stochastic\napproximation. The performance of the proposed method is evaluated on synthetic\nand real data. A comparison with independent unmixing algorithms finally\nillustrates the interest of the proposed strategy.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2015 13:47:24 GMT"}, {"version": "v2", "created": "Tue, 23 Feb 2016 09:17:00 GMT"}, {"version": "v3", "created": "Mon, 6 Jun 2016 16:05:14 GMT"}], "update_date": "2016-08-24", "authors_parsed": [["Thouvenin", "Pierre-Antoine", ""], ["Dobigeon", "Nicolas", ""], ["Tourneret", "Jean-Yves", ""]]}, {"id": "1510.05970", "submitter": "Jure \\v{Z}bontar", "authors": "Jure \\v{Z}bontar and Yann LeCun", "title": "Stereo Matching by Training a Convolutional Neural Network to Compare\n  Image Patches", "comments": null, "journal-ref": "JMLR 17(65):1-32, 2016", "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for extracting depth information from a rectified image\npair. Our approach focuses on the first stage of many stereo algorithms: the\nmatching cost computation. We approach the problem by learning a similarity\nmeasure on small image patches using a convolutional neural network. Training\nis carried out in a supervised manner by constructing a binary classification\ndata set with examples of similar and dissimilar pairs of patches. We examine\ntwo network architectures for this task: one tuned for speed, the other for\naccuracy. The output of the convolutional neural network is used to initialize\nthe stereo matching cost. A series of post-processing steps follow: cross-based\ncost aggregation, semiglobal matching, a left-right consistency check, subpixel\nenhancement, a median filter, and a bilateral filter. We evaluate our method on\nthe KITTI 2012, KITTI 2015, and Middlebury stereo data sets and show that it\noutperforms other approaches on all three data sets.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2015 17:15:05 GMT"}, {"version": "v2", "created": "Wed, 18 May 2016 19:53:41 GMT"}], "update_date": "2016-05-19", "authors_parsed": [["\u017dbontar", "Jure", ""], ["LeCun", "Yann", ""]]}, {"id": "1510.06093", "submitter": "Qifei Wang", "authors": "Yao Zhai, Qifei Wang, Yan Lu, Shipeng Li", "title": "Content adaptive screen image scaling", "comments": "ICIP 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes an efficient content adaptive screen image scaling scheme\nfor the real-time screen applications like remote desktop and screen sharing.\nIn the proposed screen scaling scheme, a screen content classification step is\nfirst introduced to classify the screen image into text and pictorial regions.\nAfterward, we propose an adaptive shift linear interpolation algorithm to\npredict the new pixel values with the shift offset adapted to the content type\nof each pixel. The shift offset for each screen content type is offline\noptimized by minimizing the theoretical interpolation error based on the\ntraining samples respectively. The proposed content adaptive screen image\nscaling scheme can achieve good visual quality and also keep the low complexity\nfor real-time applications.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2015 00:49:12 GMT"}], "update_date": "2015-10-22", "authors_parsed": [["Zhai", "Yao", ""], ["Wang", "Qifei", ""], ["Lu", "Yan", ""], ["Li", "Shipeng", ""]]}, {"id": "1510.06223", "submitter": "Tomasz Trzcinski", "authors": "Tomasz Trzcinski and Przemyslaw Rokita", "title": "Predicting popularity of online videos using Support Vector Regression", "comments": null, "journal-ref": "Transactions on Multimedia, 2017", "doi": "10.1109/TMM.2017.2695439", "report-no": null, "categories": "cs.SI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a regression method to predict the popularity of an\nonline video based on temporal and visual cues. Our method uses Support Vector\nRegression with Gaussian Radial Basis Functions. We show that modelling\npopularity patterns with this approach provides higher and more stable\nprediction results, mainly thanks to the non-linearity character of the\nproposed method as well as its resistance against overfitting. We compare our\nmethod with the state of the art on datasets containing over 14,000 videos from\nYouTube and Facebook. Furthermore, we show that results obtained relying only\non the early distribution patterns, can be improved by adding social and visual\nmetadata.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2015 12:06:15 GMT"}, {"version": "v2", "created": "Wed, 2 Dec 2015 17:26:45 GMT"}, {"version": "v3", "created": "Tue, 19 Jan 2016 15:49:59 GMT"}, {"version": "v4", "created": "Fri, 12 May 2017 01:51:58 GMT"}], "update_date": "2017-11-02", "authors_parsed": [["Trzcinski", "Tomasz", ""], ["Rokita", "Przemyslaw", ""]]}, {"id": "1510.06375", "submitter": "Shuo Li", "authors": "Xiantong Zhen, Shuo Li", "title": "Towards Direct Medical Image Analysis without Segmentation", "comments": "2 pages perspective", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Direct methods have recently emerged as an effective and efficient tool in\nautomated medical image analysis and become a trend to solve diverse\nchallenging tasks in clinical practise. Compared to traditional methods, direct\nmethods are of much more clinical significance by straightly targeting to the\nfinal clinical goal rather than relying on any intermediate steps. These\nintermediate steps, e.g., segmentation, registration and tracking, are actually\nnot necessary and only limited to very constrained tasks far from being used in\npractical clinical applications; moreover they are computationally expensive\nand time-consuming, which causes a high waste of research resources. The\nadvantages of direct methods stem from \\textbf{1)} removal of intermediate\nsteps, e.g., segmentation, tracking and registration; \\textbf{2)} avoidance of\nuser inputs and initialization; \\textbf{3)} reformulation of conventional\nchallenging problems, e.g., inversion problem, with efficient solutions.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2015 19:16:34 GMT"}], "update_date": "2015-10-22", "authors_parsed": [["Zhen", "Xiantong", ""], ["Li", "Shuo", ""]]}, {"id": "1510.06479", "submitter": "Yukiyasu Kamitani", "authors": "Tomoyasu Horikawa and Yukiyasu Kamitani", "title": "Generic decoding of seen and imagined objects using hierarchical visual\n  features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Object recognition is a key function in both human and machine vision. While\nrecent studies have achieved fMRI decoding of seen and imagined contents, the\nprediction is limited to training examples. We present a decoding approach for\narbitrary objects, using the machine vision principle that an object category\nis represented by a set of features rendered invariant through hierarchical\nprocessing. We show that visual features including those from a convolutional\nneural network can be predicted from fMRI patterns and that greater accuracy is\nachieved for low/high-level features with lower/higher-level visual areas,\nrespectively. Predicted features are used to identify seen/imagined object\ncategories (extending beyond decoder training) from a set of computed features\nfor numerous object images. Furthermore, the decoding of imagined objects\nreveals progressive recruitment of higher to lower visual representations. Our\nresults demonstrate a homology between human and machine vision and its utility\nfor brain-based information retrieval.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2015 02:34:03 GMT"}, {"version": "v2", "created": "Fri, 23 Oct 2015 22:47:13 GMT"}, {"version": "v3", "created": "Tue, 27 Sep 2016 14:27:20 GMT"}], "update_date": "2016-09-28", "authors_parsed": [["Horikawa", "Tomoyasu", ""], ["Kamitani", "Yukiyasu", ""]]}, {"id": "1510.06503", "submitter": "Xiangbo Shu", "authors": "Xiangbo Shu, Jinhui Tang, Hanjiang Lai, Luoqi Liu, Shuicheng Yan", "title": "Personalized Age Progression with Aging Dictionary", "comments": "in International Conference on Computer Vision, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we aim to automatically render aging faces in a personalized\nway. Basically, a set of age-group specific dictionaries are learned, where the\ndictionary bases corresponding to the same index yet from different\ndictionaries form a particular aging process pattern cross different age\ngroups, and a linear combination of these patterns expresses a particular\npersonalized aging process. Moreover, two factors are taken into consideration\nin the dictionary learning process. First, beyond the aging dictionaries, each\nsubject may have extra personalized facial characteristics, e.g. mole, which\nare invariant in the aging process. Second, it is challenging or even\nimpossible to collect faces of all age groups for a particular subject, yet\nmuch easier and more practical to get face pairs from neighboring age groups.\nThus a personality-aware coupled reconstruction loss is utilized to learn the\ndictionaries based on face pairs from neighboring age groups. Extensive\nexperiments well demonstrate the advantages of our proposed solution over other\nstate-of-the-arts in term of personalized aging progression, as well as the\nperformance gain for cross-age face verification by synthesizing aging faces.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2015 07:33:18 GMT"}], "update_date": "2015-10-23", "authors_parsed": [["Shu", "Xiangbo", ""], ["Tang", "Jinhui", ""], ["Lai", "Hanjiang", ""], ["Liu", "Luoqi", ""], ["Yan", "Shuicheng", ""]]}, {"id": "1510.06507", "submitter": "Reiner Lenz", "authors": "Satoshi Oshima, Rica Mochizuki, Reiner Lenz, Jinhui Chao", "title": "Modelling, Measuring and Compensating Color Weak Vision", "comments": "Full resolution color pictures are available from the authors", "journal-ref": null, "doi": "10.1109/TIP.2016.2539679", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We use methods from Riemann geometry to investigate transformations between\nthe color spaces of color-normal and color weak observers. The two main\napplications are the simulation of the perception of a color weak observer for\na color normal observer and the compensation of color images in a way that a\ncolor weak observer has approximately the same perception as a color normal\nobserver. The metrics in the color spaces of interest are characterized with\nthe help of ellipsoids defined by the just-noticable-differences between color\nwhich are measured with the help of color-matching experiments. The constructed\nmappings are isometries of Riemann spaces that preserve the perceived\ncolor-differences for both observers. Among the two approaches to build such an\nisometry, we introduce normal coordinates in Riemann spaces as a tool to\nconstruct a global color-weak compensation map. Compared to previously used\nmethods this method is free from approximation errors due to local\nlinearizations and it avoids the problem of shifting locations of the origin of\nthe local coordinate system. We analyse the variations of the Riemann metrics\nfor different observers obtained from new color matching experiments and\ndescribe three variations of the basic method. The performance of the methods\nis evaluated with the help of semantic differential (SD) tests.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2015 07:44:24 GMT"}], "update_date": "2016-05-04", "authors_parsed": [["Oshima", "Satoshi", ""], ["Mochizuki", "Rica", ""], ["Lenz", "Reiner", ""], ["Chao", "Jinhui", ""]]}, {"id": "1510.06595", "submitter": "Angela Yao", "authors": "Bj\\\"orn Kr\\\"uger, Anna V\\\"ogele, Tobias Willig, Angela Yao, Reinhard\n  Klein, Andreas Weber", "title": "Efficient Unsupervised Temporal Segmentation of Motion Data", "comments": "15 pages, submitted to TPAMI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a method for automated temporal segmentation of human motion\ndata into distinct actions and compositing motion primitives based on\nself-similar structures in the motion sequence. We use neighbourhood graphs for\nthe partitioning and the similarity information in the graph is further\nexploited to cluster the motion primitives into larger entities of semantic\nsignificance. The method requires no assumptions about the motion sequences at\nhand and no user interaction is required for the segmentation or clustering. In\naddition, we introduce a feature bundling preprocessing technique to make the\nsegmentation more robust to noise, as well as a notion of motion symmetry for\nmore refined primitive detection. We test our method on several sensor\nmodalities, including markered and markerless motion capture as well as on\nelectromyograph and accelerometer recordings. The results highlight our\nsystem's capabilities for both segmentation and for analysis of the finer\nstructures of motion data, all in a completely unsupervised manner.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2015 12:20:04 GMT"}], "update_date": "2015-10-23", "authors_parsed": [["Kr\u00fcger", "Bj\u00f6rn", ""], ["V\u00f6gele", "Anna", ""], ["Willig", "Tobias", ""], ["Yao", "Angela", ""], ["Klein", "Reinhard", ""], ["Weber", "Andreas", ""]]}, {"id": "1510.06706", "submitter": "Aleksandar Zlateski", "authors": "Aleksandar Zlateski, Kisuk Lee and H. Sebastian Seung", "title": "ZNN - A Fast and Scalable Algorithm for Training 3D Convolutional\n  Networks on Multi-Core and Many-Core Shared Memory Machines", "comments": null, "journal-ref": null, "doi": "10.1109/IPDPS.2016.119", "report-no": null, "categories": "cs.NE cs.CV cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional networks (ConvNets) have become a popular approach to computer\nvision. It is important to accelerate ConvNet training, which is\ncomputationally costly. We propose a novel parallel algorithm based on\ndecomposition into a set of tasks, most of which are convolutions or FFTs.\nApplying Brent's theorem to the task dependency graph implies that linear\nspeedup with the number of processors is attainable within the PRAM model of\nparallel computation, for wide network architectures. To attain such\nperformance on real shared-memory machines, our algorithm computes convolutions\nconverging on the same node of the network with temporal locality to reduce\ncache misses, and sums the convergent convolution outputs via an almost\nwait-free concurrent method to reduce time spent in critical sections. We\nimplement the algorithm with a publicly available software package called ZNN.\nBenchmarking with multi-core CPUs shows that ZNN can attain speedup roughly\nequal to the number of physical cores. We also show that ZNN can attain over\n90x speedup on a many-core CPU (Xeon Phi Knights Corner). These speedups are\nachieved for network architectures with widths that are in common use. The task\nparallelism of the ZNN algorithm is suited to CPUs, while the SIMD parallelism\nof previous algorithms is compatible with GPUs. Through examples, we show that\nZNN can be either faster or slower than certain GPU implementations depending\non specifics of the network architecture, kernel sizes, and density and size of\nthe output patch. ZNN may be less costly to develop and maintain, due to the\nrelative ease of general-purpose CPU programming.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2015 18:14:42 GMT"}], "update_date": "2016-06-21", "authors_parsed": [["Zlateski", "Aleksandar", ""], ["Lee", "Kisuk", ""], ["Seung", "H. Sebastian", ""]]}, {"id": "1510.06767", "submitter": "E. M. De La Calleja Mora", "authors": "E. M. De la Calleja, F. Cervantes, J. De la Calleja", "title": "Order-Fractal transition in abstract paintings", "comments": null, "journal-ref": null, "doi": "10.1016/j.aop.2016.04.007", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We report the degree of order of twenty-two Jackson Pollock's paintings using\n\\emph{Hausdorff-Besicovitch fractal dimension}. Through the maximum value of\neach multi-fractal spectrum, the artworks are classify by the year in which\nthey were painted. It has been reported that Pollock's paintings are fractal\nand it increased on his latest works. However our results show that fractal\ndimension of the paintings are on a range of fractal dimension with values\nclose to two. We identify this behavior as a fractal-order transition. Based on\nthe study of disorder-order transition in physical systems, we interpreted the\nfractal-order transition through its dark paint strokes in Pollocks' paintings,\nas structured lines following a power law measured by fractal dimension. We\nobtain self-similarity in some specific Pollock's paintings, that reveal an\nimportant dependence on the scale of observation. We also characterize by its\nfractal spectrum, the called \\emph{Teri's Find}. We obtained similar spectrums\nbetween \\emph{Teri's Find} and \\emph{Number 5} from Pollock, suggesting that\nfractal dimension cannot be completely rejected as a quantitative parameter to\nauthenticate this kind of artworks.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2015 21:03:50 GMT"}, {"version": "v2", "created": "Thu, 29 Oct 2015 17:35:42 GMT"}, {"version": "v3", "created": "Tue, 12 Apr 2016 12:28:31 GMT"}], "update_date": "2016-08-24", "authors_parsed": [["De la Calleja", "E. M.", ""], ["Cervantes", "F.", ""], ["De la Calleja", "J.", ""]]}, {"id": "1510.06895", "submitter": "Canyi Lu", "authors": "Canyi Lu, Jinhui Tang, Shuicheng Yan, Zhouchen Lin", "title": "Nonconvex Nonsmooth Low-Rank Minimization via Iteratively Reweighted\n  Nuclear Norm", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2015.2511584", "report-no": null, "categories": "cs.LG cs.CV cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The nuclear norm is widely used as a convex surrogate of the rank function in\ncompressive sensing for low rank matrix recovery with its applications in image\nrecovery and signal processing. However, solving the nuclear norm based relaxed\nconvex problem usually leads to a suboptimal solution of the original rank\nminimization problem. In this paper, we propose to perform a family of\nnonconvex surrogates of $L_0$-norm on the singular values of a matrix to\napproximate the rank function. This leads to a nonconvex nonsmooth minimization\nproblem. Then we propose to solve the problem by Iteratively Reweighted Nuclear\nNorm (IRNN) algorithm. IRNN iteratively solves a Weighted Singular Value\nThresholding (WSVT) problem, which has a closed form solution due to the\nspecial properties of the nonconvex surrogate functions. We also extend IRNN to\nsolve the nonconvex problem with two or more blocks of variables. In theory, we\nprove that IRNN decreases the objective function value monotonically, and any\nlimit point is a stationary point. Extensive experiments on both synthesized\ndata and real images demonstrate that IRNN enhances the low-rank matrix\nrecovery compared with state-of-the-art convex algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2015 11:28:06 GMT"}], "update_date": "2016-01-20", "authors_parsed": [["Lu", "Canyi", ""], ["Tang", "Jinhui", ""], ["Yan", "Shuicheng", ""], ["Lin", "Zhouchen", ""]]}, {"id": "1510.06915", "submitter": "Kanishka Sharma", "authors": "Kanishka Sharma, Loic Peter, Christian Rupprecht, Anna Caroli, Lichao\n  Wang, Andrea Remuzzi, Maximilian Baust, Nassir Navab", "title": "Semi-Automatic Segmentation of Autosomal Dominant Polycystic Kidneys\n  using Random Forests", "comments": "5 pages, 3 Figures, parallel submission to International Symposium on\n  Biomedical Imaging 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a method for 3D segmentation of kidneys from patients\nwith autosomal dominant polycystic kidney disease (ADPKD) and severe renal\ninsufficiency, using computed tomography (CT) data. ADPKD severely alters the\nshape of the kidneys due to non-uniform formation of cysts. As a consequence,\nfully automatic segmentation of such kidneys is very challenging. We present a\nsegmentation method with minimal user interaction based on a random forest\nclassifier. One of the major novelties of the proposed approach is the usage of\ngeodesic distance volumes as additional source of information. These volumes\ncontain the intensity weighted distance to a manual outline of the respective\nkidney in only one slice (for each kidney) of the CT volume. We evaluate our\nmethod qualitatively and quantitatively on 55 CT acquisitions using ground\ntruth annotations from clinical experts.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2015 12:35:58 GMT"}], "update_date": "2015-10-26", "authors_parsed": [["Sharma", "Kanishka", ""], ["Peter", "Loic", ""], ["Rupprecht", "Christian", ""], ["Caroli", "Anna", ""], ["Wang", "Lichao", ""], ["Remuzzi", "Andrea", ""], ["Baust", "Maximilian", ""], ["Navab", "Nassir", ""]]}, {"id": "1510.06925", "submitter": "Leigh Robinson", "authors": "Leigh Robinson, Benjamin Graham", "title": "Confusing Deep Convolution Networks by Relabelling", "comments": "Submitted to BMVC 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks have become the gold standard for image\nrecognition tasks, demonstrating many current state-of-the-art results and even\nachieving near-human level performance on some tasks. Despite this fact it has\nbeen shown that their strong generalisation qualities can be fooled to\nmisclassify previously correctly classified natural images and give erroneous\nhigh confidence classifications to nonsense synthetic images. In this paper we\nextend that work, by presenting a straightforward way to perturb an image in\nsuch a way as to cause it to acquire any other label from within the dataset\nwhile leaving this perturbed image visually indistinguishable from the\noriginal.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2015 13:02:55 GMT"}, {"version": "v2", "created": "Thu, 3 Dec 2015 18:38:08 GMT"}], "update_date": "2015-12-04", "authors_parsed": [["Robinson", "Leigh", ""], ["Graham", "Benjamin", ""]]}, {"id": "1510.06939", "submitter": "Mihir Jain", "authors": "Mihir Jain, Jan C. van Gemert, Thomas Mensink and Cees G. M. Snoek", "title": "Objects2action: Classifying and localizing actions without any video\n  example", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of this paper is to recognize actions in video without the need for\nexamples. Different from traditional zero-shot approaches we do not demand the\ndesign and specification of attribute classifiers and class-to-attribute\nmappings to allow for transfer from seen classes to unseen classes. Our key\ncontribution is objects2action, a semantic word embedding that is spanned by a\nskip-gram model of thousands of object categories. Action labels are assigned\nto an object encoding of unseen video based on a convex combination of action\nand object affinities. Our semantic embedding has three main characteristics to\naccommodate for the specifics of actions. First, we propose a mechanism to\nexploit multiple-word descriptions of actions and objects. Second, we\nincorporate the automated selection of the most responsive objects per action.\nAnd finally, we demonstrate how to extend our zero-shot approach to the\nspatio-temporal localization of actions in video. Experiments on four action\ndatasets demonstrate the potential of our approach.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2015 14:23:44 GMT"}], "update_date": "2015-10-26", "authors_parsed": [["Jain", "Mihir", ""], ["van Gemert", "Jan C.", ""], ["Mensink", "Thomas", ""], ["Snoek", "Cees G. M.", ""]]}, {"id": "1510.07112", "submitter": "Abhishek Dutta", "authors": "Abhishek Dutta", "title": "Predicting Performance of a Face Recognition System Based on Image\n  Quality", "comments": "PhD thesis publicly defended at the University of Twente\n  (Netherlands) on April 24, 2015 at 12.45", "journal-ref": null, "doi": "10.3990/1.9789036538725", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this dissertation, we present a generative model to capture the relation\nbetween facial image quality features (like pose, illumination direction, etc)\nand face recognition performance. Such a model can be used to predict the\nperformance of a face recognition system. Since the model is based solely on\nimage quality features, performance predictions can be done even before the\nactual recognition has taken place thereby facilitating many preemptive action.\nA practical limitation of such a data driven generative model is the limited\nnature of training data set. To address this limitation, we have developed a\nBayesian approach to model the distribution of recognition performance measure\nbased on the number of match and non-match scores in small regions of the image\nquality space. Random samples drawn from these models provide the initial data\nessential for training the generative model. Experiment results based on six\nface recognition systems operating on three independent data sets show that the\nproposed performance prediction model can accurately predict face recognition\nperformance using an accurate and unbiased Image Quality Assessor (IQA).\nFurthermore, our results show that variability in the unaccounted quality space\n-- the image quality features not considered by the IQA -- is the major factor\ncausing inaccuracies in predicted performance.\n", "versions": [{"version": "v1", "created": "Sat, 24 Oct 2015 06:56:02 GMT"}], "update_date": "2015-10-27", "authors_parsed": [["Dutta", "Abhishek", ""]]}, {"id": "1510.07119", "submitter": "Abhishek Dutta", "authors": "Abhishek Dutta, Raymond Veldhuis and Luuk Spreeuwers", "title": "Predicting Face Recognition Performance Using Image Quality", "comments": "Submitted to TPAMI journal on Apr. 22, 2015. Decision of \"Revise and\n  resubmit as new\" received on Sep. 10, 2015. At present, updating the paper to\n  address the feedback and concerns of the two reviewers. The re-submitted\n  paper will be uploaded as version 2 on arXiv", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a data driven model to predict the performance of a face\nrecognition system based on image quality features. We model the relationship\nbetween image quality features (e.g. pose, illumination, etc.) and recognition\nperformance measures using a probability density function. To address the issue\nof limited nature of practical training data inherent in most data driven\nmodels, we have developed a Bayesian approach to model the distribution of\nrecognition performance measures in small regions of the quality space. Since\nthe model is based solely on image quality features, it can predict performance\neven before the actual recognition has taken place. We evaluate the performance\npredictive capabilities of the proposed model for six face recognition systems\n(two commercial and four open source) operating on three independent data sets:\nMultiPIE, FRGC and CAS-PEAL. Our results show that the proposed model can\naccurately predict performance using an accurate and unbiased Image Quality\nAssessor (IQA). Furthermore, our experiments highlight the impact of the\nunaccounted quality space -- the image quality features not considered by IQA\n-- in contributing to performance prediction errors.\n", "versions": [{"version": "v1", "created": "Sat, 24 Oct 2015 08:05:52 GMT"}], "update_date": "2015-10-27", "authors_parsed": [["Dutta", "Abhishek", ""], ["Veldhuis", "Raymond", ""], ["Spreeuwers", "Luuk", ""]]}, {"id": "1510.07136", "submitter": "Marian George", "authors": "Marian George", "title": "Image Parsing with a Wide Range of Classes and Scene-Level Context", "comments": "Published at CVPR 2015, Computer Vision and Pattern Recognition\n  (CVPR), 2015 IEEE Conference on", "journal-ref": null, "doi": "10.1109/CVPR.2015.7298985", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a nonparametric scene parsing approach that improves the\noverall accuracy, as well as the coverage of foreground classes in scene\nimages. We first improve the label likelihood estimates at superpixels by\nmerging likelihood scores from different probabilistic classifiers. This boosts\nthe classification performance and enriches the representation of\nless-represented classes. Our second contribution consists of incorporating\nsemantic context in the parsing process through global label costs. Our method\ndoes not rely on image retrieval sets but rather assigns a global likelihood\nestimate to each label, which is plugged into the overall energy function. We\nevaluate our system on two large-scale datasets, SIFTflow and LMSun. We achieve\nstate-of-the-art performance on the SIFTflow dataset and near-record results on\nLMSun.\n", "versions": [{"version": "v1", "created": "Sat, 24 Oct 2015 12:16:27 GMT"}], "update_date": "2015-10-27", "authors_parsed": [["George", "Marian", ""]]}, {"id": "1510.07182", "submitter": "Ali Borji", "authors": "Laurent Itti and Ali Borji", "title": "Computational models of attention", "comments": null, "journal-ref": "Cognitive Neuroscience: The Biology of the Mind (Fifth Edition),\n  (M. S. Gazzaniga, R. B. Ivry, G. R. Mangun Ed.), pp. 1-10, 2014", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This chapter reviews recent computational models of visual attention. We\nbegin with models for the bottom-up or stimulus-driven guidance of attention to\nsalient visual items, which we examine in seven different broad categories. We\nthen examine more complex models which address the top-down or goal-oriented\nguidance of attention towards items that are more relevant to the task at hand.\n", "versions": [{"version": "v1", "created": "Sat, 24 Oct 2015 20:26:08 GMT"}], "update_date": "2015-10-28", "authors_parsed": [["Itti", "Laurent", ""], ["Borji", "Ali", ""]]}, {"id": "1510.07234", "submitter": "Remus Brad", "authors": "Raluca Brad, Eugen H\\u{A}loiu, Remus Brad", "title": "Seam Puckering Objective Evaluation Method for Sewing Process", "comments": null, "journal-ref": "Annals of the University of Oradea, volume XV, no.1, pp.23-28,\n  2014, ISSN 1843-813X", "doi": null, "report-no": null, "categories": "cs.CV cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper presents an automated method for the assessment and classification\nof puckering defects detected during the preproduction control stage of the\nsewing machine or product inspection. In this respect, we have presented the\npossible causes and remedies of the wrinkle nonconformities. Subjective factors\nrelated to the control environment and operators during the seams evaluation\ncan be reduced using an automated system whose operation is based on image\nprocessing. Our implementation involves spectral image analysis using Fourier\ntransform and an unsupervised neural network, the Kohonen Map, employed to\nclassify material specimens, the input images, into five discrete degrees of\nquality, from grade 5 (best) to grade 1 (the worst).\n", "versions": [{"version": "v1", "created": "Sun, 25 Oct 2015 11:37:07 GMT"}], "update_date": "2015-10-27", "authors_parsed": [["Brad", "Raluca", ""], ["H\u0102loiu", "Eugen", ""], ["Brad", "Remus", ""]]}, {"id": "1510.07317", "submitter": "S. Hussain Raza", "authors": "S. Hussain Raza, Omar Javed, Aveek Das, Harpreet Sawhney, Hui Cheng,\n  Irfan Essa", "title": "Depth Extraction from Videos Using Geometric Context and Occlusion\n  Boundaries", "comments": "British Machine Vision Conference (BMVC) 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an algorithm to estimate depth in dynamic video scenes. We propose\nto learn and infer depth in videos from appearance, motion, occlusion\nboundaries, and geometric context of the scene. Using our method, depth can be\nestimated from unconstrained videos with no requirement of camera pose\nestimation, and with significant background/foreground motions. We start by\ndecomposing a video into spatio-temporal regions. For each spatio-temporal\nregion, we learn the relationship of depth to visual appearance, motion, and\ngeometric classes. Then we infer the depth information of new scenes using\npiecewise planar parametrization estimated within a Markov random field (MRF)\nframework by combining appearance to depth learned mappings and occlusion\nboundary guided smoothness constraints. Subsequently, we perform temporal\nsmoothing to obtain temporally consistent depth maps. To evaluate our depth\nestimation algorithm, we provide a novel dataset with ground truth depth for\noutdoor video scenes. We present a thorough evaluation of our algorithm on our\nnew dataset and the publicly available Make3d static image dataset.\n", "versions": [{"version": "v1", "created": "Sun, 25 Oct 2015 22:41:24 GMT"}], "update_date": "2015-10-27", "authors_parsed": [["Raza", "S. Hussain", ""], ["Javed", "Omar", ""], ["Das", "Aveek", ""], ["Sawhney", "Harpreet", ""], ["Cheng", "Hui", ""], ["Essa", "Irfan", ""]]}, {"id": "1510.07320", "submitter": "S. Hussain Raza", "authors": "S. Hussain Raza, Matthias Grundmann, Irfan Essa", "title": "Geometric Context from Videos", "comments": "Computer Vision and Pattern Recognition (CVPR), 2013 IEEE Conference\n  on", "journal-ref": null, "doi": "10.1109/CVPR.2013.396", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel algorithm for estimating the broad 3D geometric structure\nof outdoor video scenes. Leveraging spatio-temporal video segmentation, we\ndecompose a dynamic scene captured by a video into geometric classes, based on\npredictions made by region-classifiers that are trained on appearance and\nmotion features. By examining the homogeneity of the prediction, we combine\npredictions across multiple segmentation hierarchy levels alleviating the need\nto determine the granularity a priori. We built a novel, extensive dataset on\ngeometric context of video to evaluate our method, consisting of over 100\nground-truth annotated outdoor videos with over 20,000 frames. To further scale\nbeyond this dataset, we propose a semi-supervised learning framework to expand\nthe pool of labeled data with high confidence predictions obtained from\nunlabeled data. Our system produces an accurate prediction of geometric context\nof video achieving 96% accuracy across main geometric classes.\n", "versions": [{"version": "v1", "created": "Sun, 25 Oct 2015 22:58:30 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Raza", "S. Hussain", ""], ["Grundmann", "Matthias", ""], ["Essa", "Irfan", ""]]}, {"id": "1510.07323", "submitter": "S. Hussain Raza", "authors": "S. Hussain Raza, Ahmad Humayun, Matthias Grundmann, David Anderson,\n  Irfan Essa", "title": "Finding Temporally Consistent Occlusion Boundaries in Videos using\n  Geometric Context", "comments": "Applications of Computer Vision (WACV), 2015 IEEE Winter Conference\n  on", "journal-ref": null, "doi": "10.1109/WACV.2015.141", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an algorithm for finding temporally consistent occlusion\nboundaries in videos to support segmentation of dynamic scenes. We learn\nocclusion boundaries in a pairwise Markov random field (MRF) framework. We\nfirst estimate the probability of an spatio-temporal edge being an occlusion\nboundary by using appearance, flow, and geometric features. Next, we enforce\nocclusion boundary continuity in a MRF model by learning pairwise occlusion\nprobabilities using a random forest. Then, we temporally smooth boundaries to\nremove temporal inconsistencies in occlusion boundary estimation. Our proposed\nframework provides an efficient approach for finding temporally consistent\nocclusion boundaries in video by utilizing causality, redundancy in videos, and\nsemantic layout of the scene. We have developed a dataset with fully annotated\nground-truth occlusion boundaries of over 30 videos ($5000 frames). This\ndataset is used to evaluate temporal occlusion boundaries and provides a much\nneeded baseline for future studies. We perform experiments to demonstrate the\nrole of scene layout, and temporal information for occlusion reasoning in\ndynamic scenes.\n", "versions": [{"version": "v1", "created": "Sun, 25 Oct 2015 23:20:38 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Raza", "S. Hussain", ""], ["Humayun", "Ahmad", ""], ["Grundmann", "Matthias", ""], ["Anderson", "David", ""], ["Essa", "Irfan", ""]]}, {"id": "1510.07390", "submitter": "GyongIl Ryang", "authors": "YongChol Sin, MyongSong Choe, GyongIl Ryang", "title": "Pan-Tilt Camera and PIR Sensor Fusion Based Moving Object Detection for\n  Mobile Security Robots", "comments": "13 pages,5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of fundamental issues for security robots is to detect and track people\nin the surroundings. The main problems of this task are real-time constraints,\na changing background, varying illumination conditions and a non-rigid shape of\nthe person to be tracked. In this paper, we propose a solution for tracking\nwith a pan-tilt camera and a passive infrared range (PIR) sensor to detect the\nmoving object based on consecutive frame difference. The proposed method is\nexcellent in real-time performance because it requires only a little memory and\ncomputation. Experiment results show that this method can detect the moving\nobject such as human efficiently and accurately in non-stationary and complex\nindoor environment.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2015 07:40:14 GMT"}], "update_date": "2015-10-27", "authors_parsed": [["Sin", "YongChol", ""], ["Choe", "MyongSong", ""], ["Ryang", "GyongIl", ""]]}, {"id": "1510.07391", "submitter": "Reza Fuad Rachmadi", "authors": "Reza Fuad Rachmadi, I Ketut Eddy Purnama", "title": "Vehicle Color Recognition using Convolutional Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vehicle color information is one of the important elements in ITS\n(Intelligent Traffic System). In this paper, we present a vehicle color\nrecognition method using convolutional neural network (CNN). Naturally, CNN is\ndesigned to learn classification method based on shape information, but we\nproved that CNN can also learn classification based on color distribution. In\nour method, we convert the input image to two different color spaces, HSV and\nCIE Lab, and run it to some CNN architecture. The training process follow\nprocedure introduce by Krizhevsky, that learning rate is decreasing by factor\nof 10 after some iterations. To test our method, we use publicly vehicle color\nrecognition dataset provided by Chen. The results, our model outperform the\noriginal system provide by Chen with 2% higher overall accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2015 07:41:32 GMT"}, {"version": "v2", "created": "Sun, 29 Oct 2017 10:07:07 GMT"}, {"version": "v3", "created": "Wed, 15 Aug 2018 06:49:37 GMT"}], "update_date": "2018-08-16", "authors_parsed": [["Rachmadi", "Reza Fuad", ""], ["Purnama", "I Ketut Eddy", ""]]}, {"id": "1510.07474", "submitter": "Alexander Gomez Villa A. G\\'omez", "authors": "Alexander G\\'omez, German D\\'iez, Jhony Giraldo, Augusto Salazar and\n  Juan M. Daza", "title": "A Markov Random Field and Active Contour Image Segmentation Model for\n  Animal Spots Patterns", "comments": "11th International Symposium on Visual Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-intrusive biometrics of animals using images allows to analyze phenotypic\npopulations and individuals with patterns like stripes and spots without\naffecting the studied subjects. However, non-intrusive biometrics demand a well\ntrained subject or the development of computer vision algorithms that ease the\nidentification task. In this work, an analysis of classic segmentation\napproaches that require a supervised tuning of their parameters such as\nthreshold, adaptive threshold, histogram equalization, and saturation\ncorrection is presented. In contrast, a general unsupervised algorithm using\nMarkov Random Fields (MRF) for segmentation of spots patterns is proposed.\nActive contours are used to boost results using MRF output as seeds. As study\nsubject the Diploglossus millepunctatus lizard is used. The proposed method\nachieved a maximum efficiency of $91.11\\%$.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2015 13:26:46 GMT"}], "update_date": "2015-10-27", "authors_parsed": [["G\u00f3mez", "Alexander", ""], ["D\u00edez", "German", ""], ["Giraldo", "Jhony", ""], ["Salazar", "Augusto", ""], ["Daza", "Juan M.", ""]]}, {"id": "1510.07493", "submitter": "Artem Babenko", "authors": "Artem Babenko and Victor Lempitsky", "title": "Aggregating Deep Convolutional Features for Image Retrieval", "comments": "accepted for ICCV 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several recent works have shown that image descriptors produced by deep\nconvolutional neural networks provide state-of-the-art performance for image\nclassification and retrieval problems. It has also been shown that the\nactivations from the convolutional layers can be interpreted as local features\ndescribing particular image regions. These local features can be aggregated\nusing aggregation approaches developed for local features (e.g. Fisher\nvectors), thus providing new powerful global descriptors.\n  In this paper we investigate possible ways to aggregate local deep features\nto produce compact global descriptors for image retrieval. First, we show that\ndeep features and traditional hand-engineered features have quite different\ndistributions of pairwise similarities, hence existing aggregation methods have\nto be carefully re-evaluated. Such re-evaluation reveals that in contrast to\nshallow features, the simple aggregation method based on sum pooling provides\narguably the best performance for deep convolutional features. This method is\nefficient, has few parameters, and bears little risk of overfitting when e.g.\nlearning the PCA matrix. Overall, the new compact global descriptor improves\nthe state-of-the-art on four common benchmarks considerably.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2015 14:35:26 GMT"}], "update_date": "2015-10-27", "authors_parsed": [["Babenko", "Artem", ""], ["Lempitsky", "Victor", ""]]}, {"id": "1510.07573", "submitter": "Krzysztof Chalupka", "authors": "Krzysztof Chalupka and Michael Dickinson and Pietro Perona", "title": "Generalized Regressive Motion: a Visual Cue to Collision", "comments": null, "journal-ref": null, "doi": "10.1088/1748-3190/11/4/046008", "report-no": null, "categories": "cs.RO cs.CV cs.MA cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Brains and sensory systems evolved to guide motion. Central to this task is\ncontrolling the approach to stationary obstacles and detecting moving\norganisms. Looming has been proposed as the main monocular visual cue for\ndetecting the approach of other animals and avoiding collisions with stationary\nobstacles. Elegant neural mechanisms for looming detection have been found in\nthe brain of insects and vertebrates. However, looming has not been analyzed in\nthe context of collisions between two moving animals. We propose an alternative\nstrategy, Generalized Regressive Motion (GRM), which is consistent with\nrecently observed behavior in fruit flies. Geometric analysis proves that GRM\nis a reliable cue to collision among conspecifics, whereas agent-based modeling\nsuggests that GRM is a better cue than looming as a means to detect approach,\nprevent collisions and maintain mobility.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2015 18:07:27 GMT"}], "update_date": "2016-09-21", "authors_parsed": [["Chalupka", "Krzysztof", ""], ["Dickinson", "Michael", ""], ["Perona", "Pietro", ""]]}, {"id": "1510.07712", "submitter": "Haonan Yu", "authors": "Haonan Yu and Jiang Wang and Zhiheng Huang and Yi Yang and Wei Xu", "title": "Video Paragraph Captioning Using Hierarchical Recurrent Neural Networks", "comments": "In CVPR2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach that exploits hierarchical Recurrent Neural Networks\n(RNNs) to tackle the video captioning problem, i.e., generating one or multiple\nsentences to describe a realistic video. Our hierarchical framework contains a\nsentence generator and a paragraph generator. The sentence generator produces\none simple short sentence that describes a specific short video interval. It\nexploits both temporal- and spatial-attention mechanisms to selectively focus\non visual elements during generation. The paragraph generator captures the\ninter-sentence dependency by taking as input the sentential embedding produced\nby the sentence generator, combining it with the paragraph history, and\noutputting the new initial state for the sentence generator. We evaluate our\napproach on two large-scale benchmark datasets: YouTubeClips and\nTACoS-MultiLevel. The experiments demonstrate that our approach significantly\noutperforms the current state-of-the-art methods with BLEU@4 scores 0.499 and\n0.305 respectively.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2015 22:47:00 GMT"}, {"version": "v2", "created": "Wed, 6 Apr 2016 02:24:35 GMT"}], "update_date": "2016-04-07", "authors_parsed": [["Yu", "Haonan", ""], ["Wang", "Jiang", ""], ["Huang", "Zhiheng", ""], ["Yang", "Yi", ""], ["Xu", "Wei", ""]]}, {"id": "1510.07740", "submitter": "Saeed Saremi", "authors": "Saeed Saremi, Terrence J. Sejnowski", "title": "The Wilson Machine for Image Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cond-mat.stat-mech cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning the distribution of natural images is one of the hardest and most\nimportant problems in machine learning. The problem remains open, because the\nenormous complexity of the structures in natural images spans all length\nscales. We break down the complexity of the problem and show that the hierarchy\nof structures in natural images fuels a new class of learning algorithms based\non the theory of critical phenomena and stochastic processes. We approach this\nproblem from the perspective of the theory of critical phenomena, which was\ndeveloped in condensed matter physics to address problems with infinite\nlength-scale fluctuations, and build a framework to integrate the criticality\nof natural images into a learning algorithm. The problem is broken down by\nmapping images into a hierarchy of binary images, called bitplanes. In this\nrepresentation, the top bitplane is critical, having fluctuations in structures\nover a vast range of scales. The bitplanes below go through a gradual\nstochastic heating process to disorder. We turn this representation into a\ndirected probabilistic graphical model, transforming the learning problem into\nthe unsupervised learning of the distribution of the critical bitplane and the\nsupervised learning of the conditional distributions for the remaining\nbitplanes. We learnt the conditional distributions by logistic regression in a\nconvolutional architecture. Conditioned on the critical binary image, this\nsimple architecture can generate large, natural-looking images, with many\nshades of gray, without the use of hidden units, unprecedented in the studies\nof natural images. The framework presented here is a major step in bringing\ncriticality and stochastic processes to machine learning and in studying\nnatural image statistics.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2015 01:04:05 GMT"}, {"version": "v2", "created": "Wed, 11 Nov 2015 22:28:55 GMT"}], "update_date": "2015-11-13", "authors_parsed": [["Saremi", "Saeed", ""], ["Sejnowski", "Terrence J.", ""]]}, {"id": "1510.07748", "submitter": "Ali Borji", "authors": "Laurent Itti and Ali Borji", "title": "Computational models: Bottom-up and top-down aspects", "comments": null, "journal-ref": "The Oxford Handbook of Attention, (A. C. Nobre, S. Kastner Ed.),\n  pp. 1-20, 2013", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computational models of visual attention have become popular over the past\ndecade, we believe primarily for two reasons: First, models make testable\npredictions that can be explored by experimentalists as well as theoreticians,\nsecond, models have practical and technological applications of interest to the\napplied science and engineering communities. In this chapter, we take a\ncritical look at recent attention modeling efforts. We focus on {\\em\ncomputational models of attention} as defined by Tsotsos \\& Rothenstein\n\\shortcite{Tsotsos_Rothenstein11}: Models which can process any visual stimulus\n(typically, an image or video clip), which can possibly also be given some task\ndefinition, and which make predictions that can be compared to human or animal\nbehavioral or physiological responses elicited by the same stimulus and task.\nThus, we here place less emphasis on abstract models, phenomenological models,\npurely data-driven fitting or extrapolation models, or models specifically\ndesigned for a single task or for a restricted class of stimuli. For\ntheoretical models, we refer the reader to a number of previous reviews that\naddress attention theories and models more generally\n\\cite{Itti_Koch01nrn,Paletta_etal05,Frintrop_etal10,Rothenstein_Tsotsos08,Gottlieb_Balan10,Toet11,Borji_Itti12pami}.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2015 01:47:26 GMT"}], "update_date": "2015-10-28", "authors_parsed": [["Itti", "Laurent", ""], ["Borji", "Ali", ""]]}, {"id": "1510.07867", "submitter": "Rasmus Rothe", "authors": "Rasmus Rothe and Radu Timofte and Luc Van Gool", "title": "Some like it hot - visual guidance for preference prediction", "comments": "accepted for publication at CVPR 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For people first impressions of someone are of determining importance. They\nare hard to alter through further information. This begs the question if a\ncomputer can reach the same judgement. Earlier research has already pointed out\nthat age, gender, and average attractiveness can be estimated with reasonable\nprecision. We improve the state-of-the-art, but also predict - based on\nsomeone's known preferences - how much that particular person is attracted to a\nnovel face. Our computational pipeline comprises a face detector, convolutional\nneural networks for the extraction of deep features, standard support vector\nregression for gender, age and facial beauty, and - as the main novelties -\nvisual regularized collaborative filtering to infer inter-person preferences as\nwell as a novel regression technique for handling visual queries without rating\nhistory. We validate the method using a very large dataset from a dating site\nas well as images from celebrities. Our experiments yield convincing results,\ni.e. we predict 76% of the ratings correctly solely based on an image, and\nreveal some sociologically relevant conclusions. We also validate our\ncollaborative filtering solution on the standard MovieLens rating dataset,\naugmented with movie posters, to predict an individual's movie rating. We\ndemonstrate our algorithms on howhot.io which went viral around the Internet\nwith more than 50 million pictures evaluated in the first month.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2015 11:17:46 GMT"}, {"version": "v2", "created": "Thu, 10 Mar 2016 15:02:19 GMT"}], "update_date": "2016-03-11", "authors_parsed": [["Rothe", "Rasmus", ""], ["Timofte", "Radu", ""], ["Van Gool", "Luc", ""]]}, {"id": "1510.07905", "submitter": "Remus Brad", "authors": "Raluca Brad, Lavinia Barac, Remus Brad", "title": "Defect Detection Techniques for Airbag Production Sewing Stages", "comments": null, "journal-ref": "Journal of Textiles, vol. 2014, Article ID 738504, 7 pages, 2014", "doi": "10.1155/2014/738504", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Airbags are subject to strict quality control in order to ensure passengers\nsafety. The quality of fabric and sewing thread influence the final product and\ntherefore, sewing defects must be early and accurately detected, in order to\nremove the item from production. Airbag seams assembly can take various forms,\nusing linear and circle primitives, with threads of different colors and length\ndensities, creating lockstitch or double threads chainstitch. The paper\npresents a framework for the automatic detection of defects occurring during\nthe airbag sewing stage. Types of defects as skipped stitch, missed stitch or\nsuperimposed seam for lockstitch and two threads chainstitch are detected and\nmarked. Using image processing methods, the proposed framework follows the\nseams path and determines if a color pattern of the considered stitches is\nvalid.\n", "versions": [{"version": "v1", "created": "Sun, 25 Oct 2015 09:17:57 GMT"}], "update_date": "2015-10-28", "authors_parsed": [["Brad", "Raluca", ""], ["Barac", "Lavinia", ""], ["Brad", "Remus", ""]]}, {"id": "1510.07945", "submitter": "Hyeonseob Nam", "authors": "Hyeonseob Nam and Bohyung Han", "title": "Learning Multi-Domain Convolutional Neural Networks for Visual Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel visual tracking algorithm based on the representations\nfrom a discriminatively trained Convolutional Neural Network (CNN). Our\nalgorithm pretrains a CNN using a large set of videos with tracking\nground-truths to obtain a generic target representation. Our network is\ncomposed of shared layers and multiple branches of domain-specific layers,\nwhere domains correspond to individual training sequences and each branch is\nresponsible for binary classification to identify the target in each domain. We\ntrain the network with respect to each domain iteratively to obtain generic\ntarget representations in the shared layers. When tracking a target in a new\nsequence, we construct a new network by combining the shared layers in the\npretrained CNN with a new binary classification layer, which is updated online.\nOnline tracking is performed by evaluating the candidate windows randomly\nsampled around the previous target state. The proposed algorithm illustrates\noutstanding performance compared with state-of-the-art methods in existing\ntracking benchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2015 15:53:00 GMT"}, {"version": "v2", "created": "Wed, 6 Jan 2016 06:14:53 GMT"}], "update_date": "2016-01-07", "authors_parsed": [["Nam", "Hyeonseob", ""], ["Han", "Bohyung", ""]]}, {"id": "1510.08012", "submitter": "Guofeng Zhang", "authors": "Guofeng Zhang, Haomin Liu, Zilong Dong, Jiaya Jia, Tien-Tsin Wong and\n  Hujun Bao", "title": "ENFT: Efficient Non-Consecutive Feature Tracking for Robust\n  Structure-from-Motion", "comments": "15 pages, 12 figures", "journal-ref": null, "doi": "10.1109/TIP.2016.2607425", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structure-from-motion (SfM) largely relies on feature tracking. In image\nsequences, if disjointed tracks caused by objects moving in and out of the\nfield of view, occasional occlusion, or image noise, are not handled well,\ncorresponding SfM could be affected. This problem becomes severer for\nlarge-scale scenes, which typically requires to capture multiple sequences to\ncover the whole scene. In this paper, we propose an efficient non-consecutive\nfeature tracking (ENFT) framework to match interrupted tracks distributed in\ndifferent subsequences or even in different videos. Our framework consists of\nsteps of solving the feature `dropout' problem when indistinctive structures,\nnoise or large image distortion exists, and of rapidly recognizing and joining\ncommon features located in different subsequences. In addition, we contribute\nan effective segment-based coarse-to-fine SfM algorithm for robustly handling\nlarge datasets. Experimental results on challenging video data demonstrate the\neffectiveness of the proposed system.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2015 18:00:42 GMT"}, {"version": "v2", "created": "Wed, 12 Oct 2016 11:29:33 GMT"}], "update_date": "2016-10-13", "authors_parsed": [["Zhang", "Guofeng", ""], ["Liu", "Haomin", ""], ["Dong", "Zilong", ""], ["Jia", "Jiaya", ""], ["Wong", "Tien-Tsin", ""], ["Bao", "Hujun", ""]]}, {"id": "1510.08039", "submitter": "Georg Poier", "authors": "Georg Poier, Konstantinos Roditakis, Samuel Schulter, Damien Michel,\n  Horst Bischof, Antonis A. Argyros", "title": "Hybrid One-Shot 3D Hand Pose Estimation by Exploiting Uncertainties", "comments": "BMVC 2015 (oral); see also\n  http://lrs.icg.tugraz.at/research/hybridhape/", "journal-ref": null, "doi": "10.5244/C.29.182", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model-based approaches to 3D hand tracking have been shown to perform well in\na wide range of scenarios. However, they require initialisation and cannot\nrecover easily from tracking failures that occur due to fast hand motions.\nData-driven approaches, on the other hand, can quickly deliver a solution, but\nthe results often suffer from lower accuracy or missing anatomical validity\ncompared to those obtained from model-based approaches. In this work we propose\na hybrid approach for hand pose estimation from a single depth image. First, a\nlearned regressor is employed to deliver multiple initial hypotheses for the 3D\nposition of each hand joint. Subsequently, the kinematic parameters of a 3D\nhand model are found by deliberately exploiting the inherent uncertainty of the\ninferred joint proposals. This way, the method provides anatomically valid and\naccurate solutions without requiring manual initialisation or suffering from\ntrack losses. Quantitative results on several standard datasets demonstrate\nthat the proposed method outperforms state-of-the-art representatives of the\nmodel-based, data-driven and hybrid paradigms.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2015 19:44:44 GMT"}], "update_date": "2015-10-28", "authors_parsed": [["Poier", "Georg", ""], ["Roditakis", "Konstantinos", ""], ["Schulter", "Samuel", ""], ["Michel", "Damien", ""], ["Bischof", "Horst", ""], ["Argyros", "Antonis A.", ""]]}, {"id": "1510.08160", "submitter": "Jianan Li", "authors": "Jianan Li, Xiaodan Liang, ShengMei Shen, Tingfa Xu, Jiashi Feng,\n  Shuicheng Yan", "title": "Scale-aware Fast R-CNN for Pedestrian Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we consider the problem of pedestrian detection in natural\nscenes. Intuitively, instances of pedestrians with different spatial scales may\nexhibit dramatically different features. Thus, large variance in instance\nscales, which results in undesirable large intra-category variance in features,\nmay severely hurt the performance of modern object instance detection methods.\nWe argue that this issue can be substantially alleviated by the\ndivide-and-conquer philosophy. Taking pedestrian detection as an example, we\nillustrate how we can leverage this philosophy to develop a Scale-Aware Fast\nR-CNN (SAF R-CNN) framework. The model introduces multiple built-in\nsub-networks which detect pedestrians with scales from disjoint ranges. Outputs\nfrom all the sub-networks are then adaptively combined to generate the final\ndetection results that are shown to be robust to large variance in instance\nscales, via a gate function defined over the sizes of object proposals.\nExtensive evaluations on several challenging pedestrian detection datasets well\ndemonstrate the effectiveness of the proposed SAF R-CNN. Particularly, our\nmethod achieves state-of-the-art performance on Caltech, INRIA, and ETH, and\nobtains competitive results on KITTI.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2015 01:59:14 GMT"}, {"version": "v2", "created": "Mon, 9 Nov 2015 06:08:18 GMT"}, {"version": "v3", "created": "Sat, 25 Jun 2016 09:26:07 GMT"}], "update_date": "2016-06-28", "authors_parsed": [["Li", "Jianan", ""], ["Liang", "Xiaodan", ""], ["Shen", "ShengMei", ""], ["Xu", "Tingfa", ""], ["Feng", "Jiashi", ""], ["Yan", "Shuicheng", ""]]}, {"id": "1510.08174", "submitter": "Subhamoy Mandal", "authors": "Subhamoy Mandal, Xos\\'e Lu\\'is De\\'an-Ben and Daniel Razansky", "title": "Visual Quality Enhancement in Optoacoustic Tomography using Active\n  Contour Segmentation Priors", "comments": "Accepted for publication in IEEE Transactions on Medical Imaging", "journal-ref": "IEEE Transactions on Medical Imaging, vol. 35, no. 10, pp.\n  2209-2217, Oct. 2016", "doi": "10.1109/TMI.2016.2553156", "report-no": null, "categories": "physics.med-ph cs.CV eess.IV physics.optics", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Segmentation of biomedical images is essential for studying and\ncharacterizing anatomical structures, detection and evaluation of pathological\ntissues. Segmentation has been further shown to enhance the reconstruction\nperformance in many tomographic imaging modalities by accounting for\nheterogeneities of the excitation field and tissue properties in the imaged\nregion. This is particularly relevant in optoacoustic tomography, where\ndiscontinuities in the optical and acoustic tissue properties, if not properly\naccounted for, may result in deterioration of the imaging performance.\nEfficient segmentation of optoacoustic images is often hampered by the\nrelatively low intrinsic contrast of large anatomical structures, which is\nfurther impaired by the limited angular coverage of some commonly employed\ntomographic imaging configurations. Herein, we analyze the performance of\nactive contour models for boundary segmentation in cross-sectional optoacoustic\ntomography. The segmented mask is employed to construct a two compartment model\nfor the acoustic and optical parameters of the imaged tissues, which is\nsubsequently used to improve accuracy of the image reconstruction routines. The\nperformance of the suggested segmentation and modeling approach are showcased\nin tissue-mimicking phantoms and small animal imaging experiments.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2015 03:11:48 GMT"}, {"version": "v2", "created": "Wed, 6 Apr 2016 23:20:23 GMT"}, {"version": "v3", "created": "Mon, 11 Apr 2016 02:24:06 GMT"}], "update_date": "2019-01-11", "authors_parsed": [["Mandal", "Subhamoy", ""], ["De\u00e1n-Ben", "Xos\u00e9 Lu\u00eds", ""], ["Razansky", "Daniel", ""]]}, {"id": "1510.08233", "submitter": "Luigi Palmieri", "authors": "Luigi Palmieri, Andrey Rudenko, Kai O. Arras", "title": "A Fast Randomized Method to Find Homotopy Classes for Socially-Aware\n  Navigation", "comments": "In Proceedings of the IROS 2015 Workshop on Assistance and Service\n  Robotics in a Human Environment Workshop, Hamburg, Germany, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce and show preliminary results of a fast randomized method that\nfinds a set of K paths lying in distinct homotopy classes. We frame the path\nplanning task as a graph search problem, where the navigation graph is based on\na Voronoi diagram. The search is biased by a cost function derived from the\nsocial force model that is used to generate and select the paths. We compare\nour method to Yen's algorithm, and empirically show that our approach is faster\nto find a subset of homotopy classes. Furthermore our approach computes a set\nof more diverse paths with respect to the baseline while obtaining a negligible\nloss in path quality.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2015 09:18:57 GMT"}], "update_date": "2019-01-14", "authors_parsed": [["Palmieri", "Luigi", ""], ["Rudenko", "Andrey", ""], ["Arras", "Kai O.", ""]]}, {"id": "1510.08291", "submitter": "Florian Bernard", "authors": "Florian Bernard, Peter Gemmar, Frank Hertel, Jorge Goncalves, Johan\n  Thunberg", "title": "Linear Shape Deformation Models with Local Support Using Graph-based\n  Structured Matrix Factorisation", "comments": "Please cite CVPR 2016 version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Representing 3D shape deformations by linear models in high-dimensional space\nhas many applications in computer vision and medical imaging, such as\nshape-based interpolation or segmentation. Commonly, using Principal Components\nAnalysis a low-dimensional (affine) subspace of the high-dimensional shape\nspace is determined. However, the resulting factors (the most dominant\neigenvectors of the covariance matrix) have global support, i.e. changing the\ncoefficient of a single factor deforms the entire shape. In this paper, a\nmethod to obtain deformation factors with local support is presented. The\nbenefits of such models include better flexibility and interpretability as well\nas the possibility of interactively deforming shapes locally. For that, based\non a well-grounded theoretical motivation, we formulate a matrix factorisation\nproblem employing sparsity and graph-based regularisation terms. We demonstrate\nthat for brain shapes our method outperforms the state of the art in local\nsupport models with respect to generalisation ability and sparse shape\nreconstruction, whereas for human body shapes our method gives more realistic\ndeformations.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2015 12:49:48 GMT"}, {"version": "v2", "created": "Wed, 11 May 2016 18:24:54 GMT"}], "update_date": "2016-05-12", "authors_parsed": [["Bernard", "Florian", ""], ["Gemmar", "Peter", ""], ["Hertel", "Frank", ""], ["Goncalves", "Jorge", ""], ["Thunberg", "Johan", ""]]}, {"id": "1510.08470", "submitter": "Jason Holloway", "authors": "Jason Holloway, M. Salman Asif, Manoj Kumar Sharma, Nathan Matsuda,\n  Roarke Horstmeyer, Oliver Cossairt, and Ashok Veeraraghavan", "title": "Toward Long Distance, Sub-diffraction Imaging Using Coherent Camera\n  Arrays", "comments": "13 pages, 16 figures, submitted to IEEE Transactions on Computational\n  Imaging", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV physics.optics", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose using camera arrays coupled with coherent\nillumination as an effective method of improving spatial resolution in long\ndistance images by a factor of ten and beyond. Recent advances in ptychography\nhave demonstrated that one can image beyond the diffraction limit of the\nobjective lens in a microscope. We demonstrate a similar imaging system to\nimage beyond the diffraction limit in long range imaging. We emulate a camera\narray with a single camera attached to an X-Y translation stage. We show that\nan appropriate phase retrieval based reconstruction algorithm can be used to\neffectively recover the lost high resolution details from the multiple low\nresolution acquired images. We analyze the effects of noise, required degree of\nimage overlap, and the effect of increasing synthetic aperture size on the\nreconstructed image quality. We show that coherent camera arrays have the\npotential to greatly improve imaging performance. Our simulations show\nresolution gains of 10x and more are achievable. Furthermore, experimental\nresults from our proof-of-concept systems show resolution gains of 4x-7x for\nreal scenes. Finally, we introduce and analyze in simulation a new strategy to\ncapture macroscopic Fourier Ptychography images in a single snapshot, albeit\nusing a camera array.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2015 20:17:25 GMT"}], "update_date": "2015-11-29", "authors_parsed": [["Holloway", "Jason", ""], ["Asif", "M. Salman", ""], ["Sharma", "Manoj Kumar", ""], ["Matsuda", "Nathan", ""], ["Horstmeyer", "Roarke", ""], ["Cossairt", "Oliver", ""], ["Veeraraghavan", "Ashok", ""]]}, {"id": "1510.08520", "submitter": "Yingzhen Yang", "authors": "Yingzhen Yang, Jiashi Feng, Jianchao Yang, Thomas S. Huang", "title": "Learning with $\\ell^{0}$-Graph: $\\ell^{0}$-Induced Sparse Subspace\n  Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse subspace clustering methods, such as Sparse Subspace Clustering (SSC)\n\\cite{ElhamifarV13} and $\\ell^{1}$-graph \\cite{YanW09,ChengYYFH10}, are\neffective in partitioning the data that lie in a union of subspaces. Most of\nthose methods use $\\ell^{1}$-norm or $\\ell^{2}$-norm with thresholding to\nimpose the sparsity of the constructed sparse similarity graph, and certain\nassumptions, e.g. independence or disjointness, on the subspaces are required\nto obtain the subspace-sparse representation, which is the key to their\nsuccess. Such assumptions are not guaranteed to hold in practice and they limit\nthe application of sparse subspace clustering on subspaces with general\nlocation. In this paper, we propose a new sparse subspace clustering method\nnamed $\\ell^{0}$-graph. In contrast to the required assumptions on subspaces\nfor most existing sparse subspace clustering methods, it is proved that\nsubspace-sparse representation can be obtained by $\\ell^{0}$-graph for\narbitrary distinct underlying subspaces almost surely under the mild i.i.d.\nassumption on the data generation. We develop a proximal method to obtain the\nsub-optimal solution to the optimization problem of $\\ell^{0}$-graph with\nproved guarantee of convergence. Moreover, we propose a regularized\n$\\ell^{0}$-graph that encourages nearby data to have similar neighbors so that\nthe similarity graph is more aligned within each cluster and the graph\nconnectivity issue is alleviated. Extensive experimental results on various\ndata sets demonstrate the superiority of $\\ell^{0}$-graph compared to other\ncompeting clustering methods, as well as the effectiveness of regularized\n$\\ell^{0}$-graph.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2015 22:48:09 GMT"}, {"version": "v2", "created": "Wed, 18 Nov 2015 07:11:42 GMT"}], "update_date": "2015-11-19", "authors_parsed": [["Yang", "Yingzhen", ""], ["Feng", "Jiashi", ""], ["Yang", "Jianchao", ""], ["Huang", "Thomas S.", ""]]}, {"id": "1510.08583", "submitter": "Ashwini Tonge", "authors": "Ashwini Tonge and Cornelia Caragea", "title": "Privacy Prediction of Images Shared on Social Media Sites Using Deep\n  Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online image sharing in social media sites such as Facebook, Flickr, and\nInstagram can lead to unwanted disclosure and privacy violations, when privacy\nsettings are used inappropriately. With the exponential increase in the number\nof images that are shared online every day, the development of effective and\nefficient prediction methods for image privacy settings are highly needed. The\nperformance of models critically depends on the choice of the feature\nrepresentation. In this paper, we present an approach to image privacy\nprediction that uses deep features and deep image tags as feature\nrepresentations. Specifically, we explore deep features at various neural\nnetwork layers and use the top layer (probability) as an auto-annotation\nmechanism. The results of our experiments show that models trained on the\nproposed deep features and deep image tags substantially outperform baselines\nsuch as those based on SIFT and GIST as well as those that use \"bag of tags\" as\nfeatures.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2015 07:23:38 GMT"}, {"version": "v2", "created": "Tue, 3 Nov 2015 01:05:54 GMT"}, {"version": "v3", "created": "Thu, 5 Nov 2015 19:27:47 GMT"}], "update_date": "2015-11-06", "authors_parsed": [["Tonge", "Ashwini", ""], ["Caragea", "Cornelia", ""]]}, {"id": "1510.08893", "submitter": "Lorenzo Baraldi", "authors": "Lorenzo Baraldi, Costantino Grana and Rita Cucchiara", "title": "A Deep Siamese Network for Scene Detection in Broadcast Videos", "comments": "ACM Multimedia 2015", "journal-ref": null, "doi": "10.1145/2733373.2806316", "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a model that automatically divides broadcast videos into coherent\nscenes by learning a distance measure between shots. Experiments are performed\nto demonstrate the effectiveness of our approach by comparing our algorithm\nagainst recent proposals for automatic scene segmentation. We also propose an\nimproved performance measure that aims to reduce the gap between numerical\nevaluation and expected results, and propose and release a new benchmark\ndataset.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2015 20:34:15 GMT"}], "update_date": "2015-11-02", "authors_parsed": [["Baraldi", "Lorenzo", ""], ["Grana", "Costantino", ""], ["Cucchiara", "Rita", ""]]}, {"id": "1510.08971", "submitter": "Zhao Kang", "authors": "Zhao Kang, Chong Peng, Qiang Cheng", "title": "Robust Subspace Clustering via Tighter Rank Approximation", "comments": "ACM CIKM 2015", "journal-ref": null, "doi": "10.1145/2806416.2806506", "report-no": null, "categories": "cs.CV cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matrix rank minimization problem is in general NP-hard. The nuclear norm is\nused to substitute the rank function in many recent studies. Nevertheless, the\nnuclear norm approximation adds all singular values together and the\napproximation error may depend heavily on the magnitudes of singular values.\nThis might restrict its capability in dealing with many practical problems. In\nthis paper, an arctangent function is used as a tighter approximation to the\nrank function. We use it on the challenging subspace clustering problem. For\nthis nonconvex minimization problem, we develop an effective optimization\nprocedure based on a type of augmented Lagrange multipliers (ALM) method.\nExtensive experiments on face clustering and motion segmentation show that the\nproposed method is effective for rank approximation.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2015 05:34:49 GMT"}], "update_date": "2015-11-02", "authors_parsed": [["Kang", "Zhao", ""], ["Peng", "Chong", ""], ["Cheng", "Qiang", ""]]}, {"id": "1510.08973", "submitter": "Fereshteh Sadeghi", "authors": "Fereshteh Sadeghi, C. Lawrence Zitnick, Ali Farhadi", "title": "VISALOGY: Answering Visual Analogy Questions", "comments": "To appear in NIPS 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the problem of answering visual analogy questions.\nThese questions take the form of image A is to image B as image C is to what.\nAnswering these questions entails discovering the mapping from image A to image\nB and then extending the mapping to image C and searching for the image D such\nthat the relation from A to B holds for C to D. We pose this problem as\nlearning an embedding that encourages pairs of analogous images with similar\ntransformations to be close together using convolutional neural networks with a\nquadruple Siamese architecture. We introduce a dataset of visual analogy\nquestions in natural images, and show first results of its kind on solving\nanalogy questions on natural images.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2015 05:43:41 GMT"}], "update_date": "2015-11-02", "authors_parsed": [["Sadeghi", "Fereshteh", ""], ["Zitnick", "C. Lawrence", ""], ["Farhadi", "Ali", ""]]}, {"id": "1510.09041", "submitter": "Yehuda Dar", "authors": "Yehuda Dar, Alfred M. Bruckstein, Michael Elad, Raja Giryes", "title": "Postprocessing of Compressed Images via Sequential Denoising", "comments": "Submitted to IEEE Transactions on Image Processing", "journal-ref": null, "doi": "10.1109/TIP.2016.2558825", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we propose a novel postprocessing technique for\ncompression-artifact reduction. Our approach is based on posing this task as an\ninverse problem, with a regularization that leverages on existing\nstate-of-the-art image denoising algorithms. We rely on the recently proposed\nPlug-and-Play Prior framework, suggesting the solution of general inverse\nproblems via Alternating Direction Method of Multipliers (ADMM), leading to a\nsequence of Gaussian denoising steps. A key feature in our scheme is a\nlinearization of the compression-decompression process, so as to get a\nformulation that can be optimized. In addition, we supply a thorough analysis\nof this linear approximation for several basic compression procedures. The\nproposed method is suitable for diverse compression techniques that rely on\ntransform coding. Specifically, we demonstrate impressive gains in image\nquality for several leading compression methods - JPEG, JPEG2000, and HEVC.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2015 10:28:55 GMT"}, {"version": "v2", "created": "Fri, 18 Mar 2016 09:26:13 GMT"}], "update_date": "2016-06-29", "authors_parsed": [["Dar", "Yehuda", ""], ["Bruckstein", "Alfred M.", ""], ["Elad", "Michael", ""], ["Giryes", "Raja", ""]]}, {"id": "1510.09083", "submitter": "Hanjiang Lai", "authors": "Hanjiang Lai, Shengtao Xiao, Yan Pan, Zhen Cui, Jiashi Feng, Chunyan\n  Xu, Jian Yin and Shuicheng Yan", "title": "Deep Recurrent Regression for Facial Landmark Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel end-to-end deep architecture for face landmark detection,\nbased on a deep convolutional and deconvolutional network followed by carefully\ndesigned recurrent network structures. The pipeline of this architecture\nconsists of three parts. Through the first part, we encode an input face image\nto resolution-preserved deconvolutional feature maps via a deep network with\nstacked convolutional and deconvolutional layers. Then, in the second part, we\nestimate the initial coordinates of the facial key points by an additional\nconvolutional layer on top of these deconvolutional feature maps. In the last\npart, by using the deconvolutional feature maps and the initial facial key\npoints as input, we refine the coordinates of the facial key points by a\nrecurrent network that consists of multiple Long-Short Term Memory (LSTM)\ncomponents. Extensive evaluations on several benchmark datasets show that the\nproposed deep architecture has superior performance against the\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2015 13:34:18 GMT"}, {"version": "v2", "created": "Fri, 13 Nov 2015 01:54:11 GMT"}, {"version": "v3", "created": "Mon, 31 Oct 2016 03:29:54 GMT"}], "update_date": "2016-11-01", "authors_parsed": [["Lai", "Hanjiang", ""], ["Xiao", "Shengtao", ""], ["Pan", "Yan", ""], ["Cui", "Zhen", ""], ["Feng", "Jiashi", ""], ["Xu", "Chunyan", ""], ["Yin", "Jian", ""], ["Yan", "Shuicheng", ""]]}, {"id": "1510.09171", "submitter": "Hang Chu", "authors": "Hang Chu, Hongyuan Mei, Mohit Bansal, Matthew R. Walter", "title": "Accurate Vision-based Vehicle Localization using Satellite Imagery", "comments": "9 pages, 8 figures. Full version is submitted to ICRA 2016. Short\n  version is to appear at NIPS 2015 Workshop on Transfer and Multi-Task\n  Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method for accurately localizing ground vehicles with the aid of\nsatellite imagery. Our approach takes a ground image as input, and outputs the\nlocation from which it was taken on a georeferenced satellite image. We perform\nvisual localization by estimating the co-occurrence probabilities between the\nground and satellite images based on a ground-satellite feature dictionary. The\nmethod is able to estimate likelihoods over arbitrary locations without the\nneed for a dense ground image database. We present a ranking-loss based\nalgorithm that learns location-discriminative feature projection matrices that\nresult in further improvements in accuracy. We evaluate our method on the\nMalaga and KITTI public datasets and demonstrate significant improvements over\na baseline that performs exhaustive search.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2015 17:35:23 GMT"}], "update_date": "2015-11-02", "authors_parsed": [["Chu", "Hang", ""], ["Mei", "Hongyuan", ""], ["Bansal", "Mohit", ""], ["Walter", "Matthew R.", ""]]}, {"id": "1510.09184", "submitter": "Alina Zare", "authors": "Taylor Glenn and Alina Zare", "title": "Estimating Target Signatures with Diverse Density", "comments": "Appeared in Proceedings of the 7th IEEE Workshop on Hyperspectral\n  Image and Signal Processing: Evolution in Remote Sensing, Tokyo, Japan, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Hyperspectral target detection algorithms rely on knowing the desired target\nsignature in advance. However, obtaining an effective target signature can be\ndifficult; signatures obtained from laboratory measurements or\nhand-spectrometers in the field may not transfer to airborne imagery\neffectively. One approach to dealing with this difficulty is to learn an\neffective target signature from training data. An approach for learning target\nsignatures from training data is presented. The proposed approach addresses\nuncertainty and imprecision in groundtruth in the training data using a\nmultiple instance learning, diverse density (DD) based objective function.\nAfter learning the target signature given data with uncertain and imprecise\ngroundtruth, target detection can be applied on test data. Results are shown on\nsimulated and real data.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2015 18:26:51 GMT"}], "update_date": "2015-11-02", "authors_parsed": [["Glenn", "Taylor", ""], ["Zare", "Alina", ""]]}]