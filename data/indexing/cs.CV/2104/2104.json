[{"id": "2104.00024", "submitter": "Yawar Siddiqui", "authors": "Yawar Siddiqui, Justus Thies, Fangchang Ma, Qi Shan, Matthias\n  Nie{\\ss}ner, Angela Dai", "title": "RetrievalFuse: Neural 3D Scene Reconstruction with a Database", "comments": "Project Page: https://nihalsid.github.io/retrieval-fuse/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D reconstruction of large scenes is a challenging problem due to the\nhigh-complexity nature of the solution space, in particular for generative\nneural networks. In contrast to traditional generative learned models which\nencode the full generative process into a neural network and can struggle with\nmaintaining local details at the scene level, we introduce a new method that\ndirectly leverages scene geometry from the training database. First, we learn\nto synthesize an initial estimate for a 3D scene, constructed by retrieving a\ntop-k set of volumetric chunks from the scene database. These candidates are\nthen refined to a final scene generation with an attention-based refinement\nthat can effectively select the most consistent set of geometry from the\ncandidates and combine them together to create an output scene, facilitating\ntransfer of coherent structures and local detail from train scene geometry. We\ndemonstrate our neural scene reconstruction with a database for the tasks of 3D\nsuper resolution and surface reconstruction from sparse point clouds, showing\nthat our approach enables generation of more coherent, accurate 3D scenes,\nimproving on average by over 8% in IoU over state-of-the-art scene\nreconstruction.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 18:00:09 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Siddiqui", "Yawar", ""], ["Thies", "Justus", ""], ["Ma", "Fangchang", ""], ["Shan", "Qi", ""], ["Nie\u00dfner", "Matthias", ""], ["Dai", "Angela", ""]]}, {"id": "2104.00031", "submitter": "Tien-Ju Yang", "authors": "Tien-Ju Yang, Yi-Lun Liao, Vivienne Sze", "title": "NetAdaptV2: Efficient Neural Architecture Search with Fast Super-Network\n  Training and Architecture Optimization", "comments": "Accepted by CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural architecture search (NAS) typically consists of three main steps:\ntraining a super-network, training and evaluating sampled deep neural networks\n(DNNs), and training the discovered DNN. Most of the existing efforts speed up\nsome steps at the cost of a significant slowdown of other steps or sacrificing\nthe support of non-differentiable search metrics. The unbalanced reduction in\nthe time spent per step limits the total search time reduction, and the\ninability to support non-differentiable search metrics limits the performance\nof discovered DNNs.\n  In this paper, we present NetAdaptV2 with three innovations to better balance\nthe time spent for each step while supporting non-differentiable search\nmetrics. First, we propose channel-level bypass connections that merge network\ndepth and layer width into a single search dimension to reduce the time for\ntraining and evaluating sampled DNNs. Second, ordered dropout is proposed to\ntrain multiple DNNs in a single forward-backward pass to decrease the time for\ntraining a super-network. Third, we propose the multi-layer coordinate descent\noptimizer that considers the interplay of multiple layers in each iteration of\noptimization to improve the performance of discovered DNNs while supporting\nnon-differentiable search metrics. With these innovations, NetAdaptV2 reduces\nthe total search time by up to $5.8\\times$ on ImageNet and $2.4\\times$ on NYU\nDepth V2, respectively, and discovers DNNs with better\naccuracy-latency/accuracy-MAC trade-offs than state-of-the-art NAS works.\nMoreover, the discovered DNN outperforms NAS-discovered MobileNetV3 by 1.8%\nhigher top-1 accuracy with the same latency. The project website is\nhttp://netadapt.mit.edu.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 18:03:46 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Yang", "Tien-Ju", ""], ["Liao", "Yi-Lun", ""], ["Sze", "Vivienne", ""]]}, {"id": "2104.00059", "submitter": "Atul Ingle", "authors": "Atul Ingle, Trevor Seets, Mauro Buttafava, Shantanu Gupta, Alberto\n  Tosi, Mohit Gupta, Andreas Velten", "title": "Passive Inter-Photon Imaging", "comments": "Accepted to CVPR 2021 as an oral presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Digital camera pixels measure image intensities by converting incident light\nenergy into an analog electrical current, and then digitizing it into a\nfixed-width binary representation. This direct measurement method, while\nconceptually simple, suffers from limited dynamic range and poor performance\nunder extreme illumination -- electronic noise dominates under low\nillumination, and pixel full-well capacity results in saturation under bright\nillumination. We propose a novel intensity cue based on measuring inter-photon\ntiming, defined as the time delay between detection of successive photons.\nBased on the statistics of inter-photon times measured by a time-resolved\nsingle-photon sensor, we develop theory and algorithms for a scene brightness\nestimator which works over extreme dynamic range; we experimentally demonstrate\nimaging scenes with a dynamic range of over ten million to one. The proposed\ntechniques, aided by the emergence of single-photon sensors such as\nsingle-photon avalanche diodes (SPADs) with picosecond timing resolution, will\nhave implications for a wide range of imaging applications: robotics, consumer\nphotography, astronomy, microscopy and biomedical imaging.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 18:44:52 GMT"}, {"version": "v2", "created": "Sun, 11 Apr 2021 02:15:51 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Ingle", "Atul", ""], ["Seets", "Trevor", ""], ["Buttafava", "Mauro", ""], ["Gupta", "Shantanu", ""], ["Tosi", "Alberto", ""], ["Gupta", "Mohit", ""], ["Velten", "Andreas", ""]]}, {"id": "2104.00073", "submitter": "Khoi Nguyen", "authors": "Khoi Nguyen, Sinisa Todorovic", "title": "FAPIS: A Few-shot Anchor-free Part-based Instance Segmenter", "comments": "Accepted to CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper is about few-shot instance segmentation, where training and test\nimage sets do not share the same object classes. We specify and evaluate a new\nfew-shot anchor-free part-based instance segmenter FAPIS. Our key novelty is in\nexplicit modeling of latent object parts shared across training object classes,\nwhich is expected to facilitate our few-shot learning on new classes in\ntesting. We specify a new anchor-free object detector aimed at scoring and\nregressing locations of foreground bounding boxes, as well as estimating\nrelative importance of latent parts within each box. Also, we specify a new\nnetwork for delineating and weighting latent parts for the final instance\nsegmentation within every detected bounding box. Our evaluation on the\nbenchmark COCO-20i dataset demonstrates that we significantly outperform the\nstate of the art.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 19:09:43 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Nguyen", "Khoi", ""], ["Todorovic", "Sinisa", ""]]}, {"id": "2104.00084", "submitter": "Timo Rehfeld", "authors": "Li Zhang, Faezeh Tafazzoli, Gunther Krehl, Runsheng Xu, Timo Rehfeld,\n  Manuel Schier, Arunava Seal", "title": "Hierarchical Road Topology Learning for Urban Map-less Driving", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The majority of current approaches in autonomous driving rely on\nHigh-Definition (HD) maps which detail the road geometry and surrounding area.\nYet, this reliance is one of the obstacles to mass deployment of autonomous\nvehicles due to poor scalability of such prior maps. In this paper, we tackle\nthe problem of online road map extraction via leveraging the sensory system\naboard the vehicle itself. To this end, we design a structured model where a\ngraph representation of the road network is generated in a hierarchical fashion\nwithin a fully convolutional network. The method is able to handle complex road\ntopology and does not require a user in the loop.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 19:51:25 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Zhang", "Li", ""], ["Tafazzoli", "Faezeh", ""], ["Krehl", "Gunther", ""], ["Xu", "Runsheng", ""], ["Rehfeld", "Timo", ""], ["Schier", "Manuel", ""], ["Seal", "Arunava", ""]]}, {"id": "2104.00085", "submitter": "Hudson Bruno", "authors": "Hudson M. S. Bruno and Esther L. Colombini", "title": "A comparative evaluation of learned feature descriptors on hybrid\n  monocular visual SLAM methods", "comments": "6 pages, Published in 2020 Latin American Robotics Symposium (LARS)", "journal-ref": "2020 Latin American Robotics Symposium (LARS), Natal, Brazil,\n  2020, pp. 1-6", "doi": "10.1109/LARS/SBR/WRE51543.2020.9307033", "report-no": null, "categories": "cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Classical Visual Simultaneous Localization and Mapping (VSLAM) algorithms can\nbe easily induced to fail when either the robot's motion or the environment is\ntoo challenging. The use of Deep Neural Networks to enhance VSLAM algorithms\nhas recently achieved promising results, which we call hybrid methods. In this\npaper, we compare the performance of hybrid monocular VSLAM methods with\ndifferent learned feature descriptors. To this end, we propose a set of\nexperiments to evaluate the robustness of the algorithms under different\nenvironments, camera motion, and camera sensor noise. Experiments conducted on\nKITTI and Euroc MAV datasets confirm that learned feature descriptors can\ncreate more robust VSLAM systems.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 19:56:32 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Bruno", "Hudson M. S.", ""], ["Colombini", "Esther L.", ""]]}, {"id": "2104.00099", "submitter": "Hudson Bruno", "authors": "Hudson M. S. Bruno and Esther L. Colombini", "title": "LIFT-SLAM: a deep-learning feature-based monocular visual SLAM method", "comments": "30 pages, Published in Neurocomputing", "journal-ref": null, "doi": "10.1016/j.neucom.2021.05.027", "report-no": null, "categories": "cs.CV cs.AI cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Simultaneous Localization and Mapping (SLAM) problem addresses the\npossibility of a robot to localize itself in an unknown environment and\nsimultaneously build a consistent map of this environment. Recently, cameras\nhave been successfully used to get the environment's features to perform SLAM,\nwhich is referred to as visual SLAM (VSLAM). However, classical VSLAM\nalgorithms can be easily induced to fail when either the motion of the robot or\nthe environment is too challenging. Although new approaches based on Deep\nNeural Networks (DNNs) have achieved promising results in VSLAM, they still are\nunable to outperform traditional methods. To leverage the robustness of deep\nlearning to enhance traditional VSLAM systems, we propose to combine the\npotential of deep learning-based feature descriptors with the traditional\ngeometry-based VSLAM, building a new VSLAM system called LIFT-SLAM. Experiments\nconducted on KITTI and Euroc datasets show that deep learning can be used to\nimprove the performance of traditional VSLAM systems, as the proposed approach\nwas able to achieve results comparable to the state-of-the-art while being\nrobust to sensorial noise. We enhance the proposed VSLAM pipeline by avoiding\nparameter tuning for specific datasets with an adaptive approach while\nevaluating how transfer learning can affect the quality of the features\nextracted.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 20:35:10 GMT"}, {"version": "v2", "created": "Tue, 1 Jun 2021 13:44:22 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Bruno", "Hudson M. S.", ""], ["Colombini", "Esther L.", ""]]}, {"id": "2104.00100", "submitter": "Shuo Han", "authors": "Shuo Han, Samuel Remedios, Aaron Carass, Michael Sch\\\"ar, Jerry L.\n  Prince", "title": "MR Slice Profile Estimation by Learning to Match Internal Patch\n  Distributions", "comments": "12 pages, 6 figures, accepted by Information Processing in Medical\n  Imaging (IPMI) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  To super-resolve the through-plane direction of a multi-slice 2D magnetic\nresonance (MR) image, its slice selection profile can be used as the\ndegeneration model from high resolution (HR) to low resolution (LR) to create\npaired data when training a supervised algorithm. Existing super-resolution\nalgorithms make assumptions about the slice selection profile since it is not\nreadily known for a given image. In this work, we estimate a slice selection\nprofile given a specific image by learning to match its internal patch\ndistributions. Specifically, we assume that after applying the correct slice\nselection profile, the image patch distribution along HR in-plane directions\nshould match the distribution along the LR through-plane direction. Therefore,\nwe incorporate the estimation of a slice selection profile as part of learning\na generator in a generative adversarial network (GAN). In this way, the slice\nselection profile can be learned without any external data. Our algorithm was\ntested using simulations from isotropic MR images, incorporated in a\nthrough-plane super-resolution algorithm to demonstrate its benefits, and also\nused as a tool to measure image resolution. Our code is at\nhttps://github.com/shuohan/espreso2.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 20:35:44 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Han", "Shuo", ""], ["Remedios", "Samuel", ""], ["Carass", "Aaron", ""], ["Sch\u00e4r", "Michael", ""], ["Prince", "Jerry L.", ""]]}, {"id": "2104.00107", "submitter": "Aviral Joshi", "authors": "Abhinav Khattar, Aviral Joshi, Har Simrat Singh, Pulkit Goel, Rohit\n  Prakash Barnwal", "title": "Analysis on Image Set Visual Question Answering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We tackle the challenge of Visual Question Answering in multi-image setting\nfor the ISVQA dataset. Traditional VQA tasks have focused on a single-image\nsetting where the target answer is generated from a single image. Image set\nVQA, however, comprises of a set of images and requires finding connection\nbetween images, relate the objects across images based on these connections and\ngenerate a unified answer. In this report, we work with 4 approaches in a bid\nto improve the performance on the task. We analyse and compare our results with\nthree baseline models - LXMERT, HME-VideoQA and VisualBERT - and show that our\napproaches can provide a slight improvement over the baselines. In specific, we\ntry to improve on the spatial awareness of the model and help the model\nidentify color using enhanced pre-training, reduce language dependence using\nadversarial regularization, and improve counting using regression loss and\ngraph based deduplication. We further delve into an in-depth analysis on the\nlanguage bias in the ISVQA dataset and show how models trained on ISVQA\nimplicitly learn to associate language more strongly with the final answer.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 20:47:32 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Khattar", "Abhinav", ""], ["Joshi", "Aviral", ""], ["Singh", "Har Simrat", ""], ["Goel", "Pulkit", ""], ["Barnwal", "Rohit Prakash", ""]]}, {"id": "2104.00125", "submitter": "Faraz Lotfi Dr", "authors": "Farnoosh Faraji, Faraz Lotfi, Javad Khorramdel, Ali Najafi, Ali\n  Ghaffari", "title": "Drowsiness Detection Based On Driver Temporal Behavior Using a New\n  Developed Dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Driver drowsiness detection has been the subject of many researches in the\npast few decades and various methods have been developed to detect it. In this\nstudy, as an image-based approach with adequate accuracy, along with the\nexpedite process, we applied YOLOv3 (You Look Only Once-version3) CNN\n(Convolutional Neural Network) for extracting facial features automatically.\nThen, LSTM (Long-Short Term Memory) neural network is employed to learn driver\ntemporal behaviors including yawning and blinking time period as well as\nsequence classification. To train YOLOv3, we utilized our collected dataset\nalongside the transfer learning method. Moreover, the dataset for the LSTM\ntraining process is produced by the mentioned CNN and is formatted as a\ntwo-dimensional sequence comprised of eye blinking and yawning time durations.\nThe developed dataset considers both disturbances such as illumination and\ndrivers' head posture. To have real-time experiments a multi-thread framework\nis developed to run both CNN and LSTM in parallel. Finally, results indicate\nthe hybrid of CNN and LSTM ability in drowsiness detection and the\neffectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 21:15:29 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Faraji", "Farnoosh", ""], ["Lotfi", "Faraz", ""], ["Khorramdel", "Javad", ""], ["Najafi", "Ali", ""], ["Ghaffari", "Ali", ""]]}, {"id": "2104.00138", "submitter": "Piotr Slomka", "authors": "Kajetan Grodecki, Aditya Killekar, Andrew Lin, Sebastien Cadet,\n  Priscilla McElhinney, Aryabod Razipour, Cato Chan, Barry D. Pressman, Peter\n  Julien, Judit Simon, Pal Maurovich-Horvat, Nicola Gaibazzi, Udit Thakur,\n  Elisabetta Mancini, Cecilia Agalbato, Jiro Munechika, Hidenari Matsumoto,\n  Roberto Men\\`e, Gianfranco Parati, Franco Cernigliaro, Nitesh Nerlekar,\n  Camilla Torlasco, Gianluca Pontone, Damini Dey, Piotr J. Slomka", "title": "Rapid quantification of COVID-19 pneumonia burden from computed\n  tomography with convolutional LSTM networks", "comments": "Fixed some typing mistakes in v2. No other results changed", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Quantitative lung measures derived from computed tomography (CT) have been\ndemonstrated to improve prognostication in coronavirus disease (COVID-19)\npatients, but are not part of the clinical routine since required manual\nsegmentation of lung lesions is prohibitively time-consuming. We propose a new\nfully automated deep learning framework for rapid quantification and\ndifferentiation between lung lesions in COVID-19 pneumonia from both contrast\nand non-contrast CT images using convolutional Long Short-Term Memory\n(ConvLSTM) networks. Utilizing the expert annotations, model training was\nperformed 5 times with separate hold-out sets using 5-fold cross-validation to\nsegment ground-glass opacity and high opacity (including consolidation and\npleural effusion). The performance of the method was evaluated on CT data sets\nfrom 197 patients with positive reverse transcription polymerase chain reaction\ntest result for SARS-CoV-2. Strong agreement between expert manual and\nautomatic segmentation was obtained for lung lesions with a Dice score\ncoefficient of 0.876 $\\pm$ 0.005; excellent correlations of 0.978 and 0.981 for\nground-glass opacity and high opacity volumes. In the external validation set\nof 67 patients, there was dice score coefficient of 0.767 $\\pm$ 0.009 as well\nas excellent correlations of 0.989 and 0.996 for ground-glass opacity and high\nopacity volumes. Computations for a CT scan comprising 120 slices were\nperformed under 2 seconds on a personal computer equipped with NVIDIA Titan RTX\ngraphics processing unit. Therefore, our deep learning-based method allows\nrapid fully-automated quantitative measurement of pneumonia burden from CT and\nmay generate results with an accuracy similar to the expert readers.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 22:09:14 GMT"}, {"version": "v2", "created": "Tue, 13 Jul 2021 17:38:07 GMT"}, {"version": "v3", "created": "Sat, 17 Jul 2021 00:14:16 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Grodecki", "Kajetan", ""], ["Killekar", "Aditya", ""], ["Lin", "Andrew", ""], ["Cadet", "Sebastien", ""], ["McElhinney", "Priscilla", ""], ["Razipour", "Aryabod", ""], ["Chan", "Cato", ""], ["Pressman", "Barry D.", ""], ["Julien", "Peter", ""], ["Simon", "Judit", ""], ["Maurovich-Horvat", "Pal", ""], ["Gaibazzi", "Nicola", ""], ["Thakur", "Udit", ""], ["Mancini", "Elisabetta", ""], ["Agalbato", "Cecilia", ""], ["Munechika", "Jiro", ""], ["Matsumoto", "Hidenari", ""], ["Men\u00e8", "Roberto", ""], ["Parati", "Gianfranco", ""], ["Cernigliaro", "Franco", ""], ["Nerlekar", "Nitesh", ""], ["Torlasco", "Camilla", ""], ["Pontone", "Gianluca", ""], ["Dey", "Damini", ""], ["Slomka", "Piotr J.", ""]]}, {"id": "2104.00139", "submitter": "Florian Dubost", "authors": "Gerda Bortsova, Florian Dubost, Laurens Hogeweg, Ioannis Katramados,\n  Marleen de Bruijne", "title": "Adversarial Heart Attack: Neural Networks Fooled to Segment Heart\n  Symbols in Chest X-Ray Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Adversarial attacks consist in maliciously changing the input data to mislead\nthe predictions of automated decision systems and are potentially a serious\nthreat for automated medical image analysis. Previous studies have shown that\nit is possible to adversarially manipulate automated segmentations produced by\nneural networks in a targeted manner in the white-box attack setting. In this\narticle, we studied the effectiveness of adversarial attacks in targeted\nmodification of segmentations of anatomical structures in chest X-rays.\nFirstly, we experimented with using anatomically implausible shapes as targets\nfor adversarial manipulation. We showed that, by adding almost imperceptible\nnoise to the image, we can reliably force state-of-the-art neural networks to\nsegment the heart as a heart symbol instead of its real anatomical shape.\nMoreover, such heart-shaping attack did not appear to require higher\nadversarial noise level than an untargeted attack based the same attack method.\nSecondly, we attempted to explore the limits of adversarial manipulation of\nsegmentations. For that, we assessed the effectiveness of shrinking and\nenlarging segmentation contours for the three anatomical structures. We\nobserved that adversarially extending segmentations of structures into regions\nwith intensity and texture uncharacteristic for them presented a challenge to\nour attacks, as well as, in some cases, changing segmentations in ways that\nconflict with class adjacency priors learned by the target network.\nAdditionally, we evaluated performances of the untargeted attacks and targeted\nheart attacks in the black-box attack scenario, using a surrogate network\ntrained on a different subset of images. In both cases, the attacks were\nsubstantially less effective. We believe these findings bring novel insights\ninto the current capabilities and limits of adversarial attacks for semantic\nsegmentation.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 22:20:59 GMT"}, {"version": "v2", "created": "Wed, 7 Apr 2021 21:20:07 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Bortsova", "Gerda", ""], ["Dubost", "Florian", ""], ["Hogeweg", "Laurens", ""], ["Katramados", "Ioannis", ""], ["de Bruijne", "Marleen", ""]]}, {"id": "2104.00152", "submitter": "Igor Vasiljevic", "authors": "Vitor Guizilini, Igor Vasiljevic, Rares Ambrus, Greg Shakhnarovich,\n  Adrien Gaidon", "title": "Full Surround Monodepth from Multiple Cameras", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-supervised monocular depth and ego-motion estimation is a promising\napproach to replace or supplement expensive depth sensors such as LiDAR for\nrobotics applications like autonomous driving. However, most research in this\narea focuses on a single monocular camera or stereo pairs that cover only a\nfraction of the scene around the vehicle. In this work, we extend monocular\nself-supervised depth and ego-motion estimation to large-baseline multi-camera\nrigs. Using generalized spatio-temporal contexts, pose consistency constraints,\nand carefully designed photometric loss masking, we learn a single network\ngenerating dense, consistent, and scale-aware point clouds that cover the same\nfull surround 360 degree field of view as a typical LiDAR scanner. We also\npropose a new scale-consistent evaluation metric more suitable to multi-camera\nsettings. Experiments on two challenging benchmarks illustrate the benefits of\nour approach over strong baselines.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 22:52:04 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Guizilini", "Vitor", ""], ["Vasiljevic", "Igor", ""], ["Ambrus", "Rares", ""], ["Shakhnarovich", "Greg", ""], ["Gaidon", "Adrien", ""]]}, {"id": "2104.00161", "submitter": "Jose M. Saavedra PhD", "authors": "Andres Baloian, Nils Murrugarra-Llerena, Jose M. Saavedra", "title": "Scalable Visual Attribute Extraction through Hidden Layers of a Residual\n  ConvNet", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Visual attributes play an essential role in real applications based on image\nretrieval. For instance, the extraction of attributes from images allows an\neCommerce search engine to produce retrieval results with higher precision. The\ntraditional manner to build an attribute extractor is by training a\nconvnet-based classifier with a fixed number of classes. However, this approach\ndoes not scale for real applications where the number of attributes changes\nfrequently. Therefore in this work, we propose an approach for extracting\nvisual attributes from images, leveraging the learned capability of the hidden\nlayers of a general convolutional network to discriminate among different\nvisual features. We run experiments with a resnet-50 trained on Imagenet, on\nwhich we evaluate the output of its different blocks to discriminate between\ncolors and textures. Our results show that the second block of the resnet is\nappropriate for discriminating colors, while the fourth block can be used for\ntextures. In both cases, the achieved accuracy of attribute classification is\nsuperior to 93%. We also show that the proposed embeddings form local\nstructures in the underlying feature space, which makes it possible to apply\nreduction techniques like UMAP, maintaining high accuracy and widely reducing\nthe size of the feature space.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 23:39:20 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Baloian", "Andres", ""], ["Murrugarra-Llerena", "Nils", ""], ["Saavedra", "Jose M.", ""]]}, {"id": "2104.00169", "submitter": "Muhyun Back", "authors": "Muhyun Back, Jinkyu Lee, Kyuho Bae, Sung Soo Hwang, Il Yong Chun", "title": "Improved and efficient inter-vehicle distance estimation using road\n  gradients of both ego and target vehicles", "comments": "5 pages, 3 figures, 2 tables, submitted to IEEE ICAS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In advanced driver assistant systems and autonomous driving, it is crucial to\nestimate distances between an ego vehicle and target vehicles. Existing\ninter-vehicle distance estimation methods assume that the ego and target\nvehicles drive on a same ground plane. In practical driving environments,\nhowever, they may drive on different ground planes. This paper proposes an\ninter-vehicle distance estimation framework that can consider slope changes of\na road forward, by estimating road gradients of \\emph{both} ego vehicle and\ntarget vehicles and using a 2D object detection deep net. Numerical experiments\ndemonstrate that the proposed method significantly improves the distance\nestimation accuracy and time complexity, compared to deep learning-based depth\nestimation methods.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 00:12:39 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Back", "Muhyun", ""], ["Lee", "Jinkyu", ""], ["Bae", "Kyuho", ""], ["Hwang", "Sung Soo", ""], ["Chun", "Il Yong", ""]]}, {"id": "2104.00170", "submitter": "Robik Shrestha", "authors": "Robik Shrestha, Kushal Kafle and Christopher Kanan", "title": "An Investigation of Critical Issues in Bias Mitigation Techniques", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A critical problem in deep learning is that systems learn inappropriate\nbiases, resulting in their inability to perform well on minority groups. This\nhas led to the creation of multiple algorithms that endeavor to mitigate bias.\nHowever, it is not clear how effective these methods are. This is because study\nprotocols differ among papers, systems are tested on datasets that fail to test\nmany forms of bias, and systems have access to hidden knowledge or are tuned\nspecifically to the test set. To address this, we introduce an improved\nevaluation protocol, sensible metrics, and a new dataset, which enables us to\nask and answer critical questions about bias mitigation algorithms. We evaluate\nseven state-of-the-art algorithms using the same network architecture and\nhyperparameter selection policy across three benchmark datasets. We introduce a\nnew dataset called Biased MNIST that enables assessment of robustness to\nmultiple bias sources. We use Biased MNIST and a visual question answering\n(VQA) benchmark to assess robustness to hidden biases. Rather than only tuning\nto the test set distribution, we study robustness across different tuning\ndistributions, which is critical because for many applications the test\ndistribution may not be known during development. We find that algorithms\nexploit hidden biases, are unable to scale to multiple forms of bias, and are\nhighly sensitive to the choice of tuning set. Based on our findings, we implore\nthe community to adopt more rigorous assessment of future bias mitigation\nmethods. All data, code, and results are publicly available at:\nhttps://github.com/erobic/bias-mitigators.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 00:14:45 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Shrestha", "Robik", ""], ["Kafle", "Kushal", ""], ["Kanan", "Christopher", ""]]}, {"id": "2104.00177", "submitter": "Samrudhdhi Bharatkumar Rangrej", "authors": "Samrudhdhi B. Rangrej, James J. Clark", "title": "Visual Attention in Imaginative Agents", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a recurrent agent who perceives surroundings through a series of\ndiscrete fixations. At each timestep, the agent imagines a variety of plausible\nscenes consistent with the fixation history. The next fixation is planned using\nuncertainty in the content of the imagined scenes. As time progresses, the\nagent becomes more certain about the content of the surrounding, and the\nvariety in the imagined scenes reduces. The agent is built using a variational\nautoencoder and normalizing flows, and trained in an unsupervised manner on a\nproxy task of scene-reconstruction. The latent representations of the imagined\nscenes are found to be useful for performing pixel-level and scene-level tasks\nby higher-order modules. The agent is tested on various 2D and 3D datasets.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 00:44:23 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Rangrej", "Samrudhdhi B.", ""], ["Clark", "James J.", ""]]}, {"id": "2104.00179", "submitter": "Chunhui Liu", "authors": "Chunhui Liu, Xinyu Li, Hao Chen, Davide Modolo, Joseph Tighe", "title": "Selective Feature Compression for Efficient Activity Recognition\n  Inference", "comments": "Accepted by ICCV 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Most action recognition solutions rely on dense sampling to precisely cover\nthe informative temporal clip. Extensively searching temporal region is\nexpensive for a real-world application. In this work, we focus on improving the\ninference efficiency of current action recognition backbones on trimmed videos,\nand illustrate that one action model can also cover then informative region by\ndropping non-informative features. We present Selective Feature Compression\n(SFC), an action recognition inference strategy that greatly increase model\ninference efficiency without any accuracy compromise. Differently from previous\nworks that compress kernel sizes and decrease the channel dimension, we propose\nto compress feature flow at spatio-temporal dimension without changing any\nbackbone parameters. Our experiments on Kinetics-400, UCF101 and ActivityNet\nshow that SFC is able to reduce inference speed by 6-7x and memory usage by\n5-6x compared with the commonly used 30 crops dense sampling procedure, while\nalso slightly improving Top1 Accuracy. We thoroughly quantitatively and\nqualitatively evaluate SFC and all its components and show how does SFC learn\nto attend to important video regions and to drop temporal features that are\nuninformative for the task of action recognition.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 00:54:51 GMT"}, {"version": "v2", "created": "Thu, 29 Jul 2021 10:59:15 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Liu", "Chunhui", ""], ["Li", "Xinyu", ""], ["Chen", "Hao", ""], ["Modolo", "Davide", ""], ["Tighe", "Joseph", ""]]}, {"id": "2104.00185", "submitter": "Jurandy Almeida", "authors": "Samuel Felipe dos Santos and Jurandy Almeida", "title": "Less is More: Accelerating Faster Neural Networks Straight from JPEG", "comments": "arXiv admin note: text overlap with arXiv:2012.14426", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most image data available are often stored in a compressed format, from which\nJPEG is the most widespread. To feed this data on a convolutional neural\nnetwork (CNN), a preliminary decoding process is required to obtain RGB pixels,\ndemanding a high computational load and memory usage. For this reason, the\ndesign of CNNs for processing JPEG compressed data has gained attention in\nrecent years. In most existing works, typical CNN architectures are adapted to\nfacilitate the learning with the DCT coefficients rather than RGB pixels.\nAlthough they are effective, their architectural changes either raise the\ncomputational costs or neglect relevant information from DCT inputs. In this\npaper, we examine different ways of speeding up CNNs designed for DCT inputs,\nexploiting learning strategies to reduce the computational complexity by taking\nfull advantage of DCT inputs. Our experiments were conducted on the ImageNet\ndataset. Results show that learning how to combine all DCT inputs in a\ndata-driven fashion is better than discarding them by hand, and its combination\nwith a reduction of layers has proven to be effective for reducing the\ncomputational costs while retaining accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 01:21:24 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Santos", "Samuel Felipe dos", ""], ["Almeida", "Jurandy", ""]]}, {"id": "2104.00192", "submitter": "Zishen Wan", "authors": "Zishen Wan, Yuyang Zhang, Arijit Raychowdhury, Bo Yu, Yanjun Zhang,\n  Shaoshan Liu", "title": "An Energy-Efficient Quad-Camera Visual System for Autonomous Machines on\n  FPGA Platform", "comments": "To appear in IEEE International Conference on Artificial Intelligence\n  Circuits and Systems (AICAS), June 6-9, 2021, Virtual", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In our past few years' of commercial deployment experiences, we identify\nlocalization as a critical task in autonomous machine applications, and a great\nacceleration target. In this paper, based on the observation that the visual\nfrontend is a major performance and energy consumption bottleneck, we present\nour design and implementation of an energy-efficient hardware architecture for\nORB (Oriented-Fast and Rotated- BRIEF) based localization system on FPGAs. To\nsupport our multi-sensor autonomous machine localization system, we present\nhardware synchronization, frame-multiplexing, and parallelization techniques,\nwhich are integrated in our design. Compared to Nvidia TX1 and Intel i7, our\nFPGA-based implementation achieves 5.6x and 3.4x speedup, as well as 3.0x and\n34.6x power reduction, respectively.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 01:42:16 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Wan", "Zishen", ""], ["Zhang", "Yuyang", ""], ["Raychowdhury", "Arijit", ""], ["Yu", "Bo", ""], ["Zhang", "Yanjun", ""], ["Liu", "Shaoshan", ""]]}, {"id": "2104.00194", "submitter": "Peng Chu", "authors": "Peng Chu, Jiang Wang, Quanzeng You, Haibin Ling, Zicheng Liu", "title": "TransMOT: Spatial-Temporal Graph Transformer for Multiple Object\n  Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Tracking multiple objects in videos relies on modeling the spatial-temporal\ninteractions of the objects. In this paper, we propose a solution named\nTransMOT, which leverages powerful graph transformers to efficiently model the\nspatial and temporal interactions among the objects. TransMOT effectively\nmodels the interactions of a large number of objects by arranging the\ntrajectories of the tracked objects as a set of sparse weighted graphs, and\nconstructing a spatial graph transformer encoder layer, a temporal transformer\nencoder layer, and a spatial graph transformer decoder layer based on the\ngraphs. TransMOT is not only more computationally efficient than the\ntraditional Transformer, but it also achieves better tracking accuracy. To\nfurther improve the tracking speed and accuracy, we propose a cascade\nassociation framework to handle low-score detections and long-term occlusions\nthat require large computational resources to model in TransMOT. The proposed\nmethod is evaluated on multiple benchmark datasets including MOT15, MOT16,\nMOT17, and MOT20, and it achieves state-of-the-art performance on all the\ndatasets.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 01:49:05 GMT"}, {"version": "v2", "created": "Sat, 3 Apr 2021 05:12:03 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Chu", "Peng", ""], ["Wang", "Jiang", ""], ["You", "Quanzeng", ""], ["Ling", "Haibin", ""], ["Liu", "Zicheng", ""]]}, {"id": "2104.00201", "submitter": "Xiaohang Fu", "authors": "Xiaohang Fu, Lei Bi, Ashnil Kumar, Michael Fulham, and Jinman Kim", "title": "Graph-Based Intercategory and Intermodality Network for Multilabel\n  Classification and Melanoma Diagnosis of Skin Lesions in Dermoscopy and\n  Clinical Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The identification of melanoma involves an integrated analysis of skin lesion\nimages acquired using the clinical and dermoscopy modalities. Dermoscopic\nimages provide a detailed view of the subsurface visual structures that\nsupplement the macroscopic clinical images. Melanoma diagnosis is commonly\nbased on the 7-point visual category checklist (7PC). The 7PC contains\nintrinsic relationships between categories that can aid classification, such as\nshared features, correlations, and the contributions of categories towards\ndiagnosis. Manual classification is subjective and prone to intra- and\ninterobserver variability. This presents an opportunity for automated methods\nto improve diagnosis. Current state-of-the-art methods focus on a single image\nmodality and ignore information from the other, or do not fully leverage the\ncomplementary information from both modalities. Further, there is not a method\nto exploit the intercategory relationships in the 7PC. In this study, we\naddress these issues by proposing a graph-based intercategory and intermodality\nnetwork (GIIN) with two modules. A graph-based relational module (GRM)\nleverages intercategorical relations, intermodal relations, and prioritises the\nvisual structure details from dermoscopy by encoding category representations\nin a graph network. The category embedding learning module (CELM) captures\nrepresentations that are specialised for each category and support the GRM. We\nshow that our modules are effective at enhancing classification performance\nusing a public dataset of dermoscopy-clinical images, and show that our method\noutperforms the state-of-the-art at classifying the 7PC categories and\ndiagnosis.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 02:06:48 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Fu", "Xiaohang", ""], ["Bi", "Lei", ""], ["Kumar", "Ashnil", ""], ["Fulham", "Michael", ""], ["Kim", "Jinman", ""]]}, {"id": "2104.00202", "submitter": "Junhui Yin", "authors": "Junhui Yin, Jiayan Qiu, Siqing Zhang, Jiyang Xie, Zhanyu Ma, and Jun\n  Guo", "title": "Unsupervised Person Re-identification via Simultaneous Clustering and\n  Consistency Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Unsupervised person re-identification (re-ID) has become an important topic\ndue to its potential to resolve the scalability problem of supervised re-ID\nmodels. However, existing methods simply utilize pseudo labels from clustering\nfor supervision and thus have not yet fully explored the semantic information\nin data itself, which limits representation capabilities of learned models. To\naddress this problem, we design a pretext task for unsupervised re-ID by\nlearning visual consistency from still images and temporal consistency during\ntraining process, such that the clustering network can separate the images into\nsemantic clusters automatically. Specifically, the pretext task learns\nsemantically meaningful representations by maximizing the agreement between two\nencoded views of the same image via a consistency loss in latent space.\nMeanwhile, we optimize the model by grouping the two encoded views into same\ncluster, thus enhancing the visual consistency between views. Experiments on\nMarket-1501, DukeMTMC-reID and MSMT17 datasets demonstrate that our proposed\napproach outperforms the state-of-the-art methods by large margins.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 02:10:42 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Yin", "Junhui", ""], ["Qiu", "Jiayan", ""], ["Zhang", "Siqing", ""], ["Xie", "Jiyang", ""], ["Ma", "Zhanyu", ""], ["Guo", "Jun", ""]]}, {"id": "2104.00205", "submitter": "Kun Huang", "authors": "Andrew Price, Kun Huang, Dmitry Berenson", "title": "Fusing RGBD Tracking and Segmentation Tree Sampling for Multi-Hypothesis\n  Volumetric Segmentation", "comments": "7 pages, 7 figures, 2021 IEEE International Conference on Robotics\n  and Automation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Despite rapid progress in scene segmentation in recent years, 3D segmentation\nmethods are still limited when there is severe occlusion. The key challenge is\nestimating the segment boundaries of (partially) occluded objects, which are\ninherently ambiguous when considering only a single frame. In this work, we\npropose Multihypothesis Segmentation Tracking (MST), a novel method for\nvolumetric segmentation in changing scenes, which allows scene ambiguity to be\ntracked and our estimates to be adjusted over time as we interact with the\nscene. Two main innovations allow us to tackle this difficult problem: 1) A\nnovel way to sample possible segmentations from a segmentation tree; and 2) A\nnovel approach to fusing tracking results with multiple segmentation estimates.\nThese methods allow MST to track the segmentation state over time and\nincorporate new information, such as new objects being revealed. We evaluate\nour method on several cluttered tabletop environments in simulation and\nreality. Our results show that MST outperforms baselines in all tested scenes.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 02:17:18 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Price", "Andrew", ""], ["Huang", "Kun", ""], ["Berenson", "Dmitry", ""]]}, {"id": "2104.00210", "submitter": "Jaeyong Chung", "authors": "Phuoc Pham, Jacob Abraham, Jaeyong Chung", "title": "Training Multi-bit Quantized and Binarized Networks with A Learnable\n  Symmetric Quantizer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Quantizing weights and activations of deep neural networks is essential for\ndeploying them in resource-constrained devices, or cloud platforms for at-scale\nservices. While binarization is a special case of quantization, this extreme\ncase often leads to several training difficulties, and necessitates specialized\nmodels and training methods. As a result, recent quantization methods do not\nprovide binarization, thus losing the most resource-efficient option, and\nquantized and binarized networks have been distinct research areas. We examine\nbinarization difficulties in a quantization framework and find that all we need\nto enable the binary training are a symmetric quantizer, good initialization,\nand careful hyperparameter selection. These techniques also lead to substantial\nimprovements in multi-bit quantization. We demonstrate our unified quantization\nframework, denoted as UniQ, on the ImageNet dataset with various architectures\nsuch as ResNet-18,-34 and MobileNetV2. For multi-bit quantization, UniQ\noutperforms existing methods to achieve the state-of-the-art accuracy. In\nbinarization, the achieved accuracy is comparable to existing state-of-the-art\nmethods even without modifying the original architectures.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 02:33:31 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Pham", "Phuoc", ""], ["Abraham", "Jacob", ""], ["Chung", "Jaeyong", ""]]}, {"id": "2104.00222", "submitter": "Yujing Ma", "authors": "Qi Zhao, Yujing Ma, Shuchang Lyu, Lijiang Chen", "title": "Embedded Self-Distillation in Compact Multi-Branch Ensemble Network for\n  Remote Sensing Scene Classification", "comments": "14 pages,9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Remote sensing (RS) image scene classification task faces many challenges due\nto the interference from different characteristics of different geographical\nelements. To solve this problem, we propose a multi-branch ensemble network to\nenhance the feature representation ability by fusing features in final output\nlogits and intermediate feature maps. However, simply adding branches will\nincrease the complexity of models and decline the inference efficiency. On this\nissue, we embed self-distillation (SD) method to transfer knowledge from\nensemble network to main-branch in it. Through optimizing with SD, main-branch\nwill have close performance as ensemble network. During inference, we can cut\nother branches to simplify the whole model. In this paper, we first design\ncompact multi-branch ensemble network, which can be trained in an end-to-end\nmanner. Then, we insert SD method on output logits and feature maps. Compared\nto previous methods, our proposed architecture (ESD-MBENet) performs strongly\non classification accuracy with compact design. Extensive experiments are\napplied on three benchmark RS datasets AID, NWPU-RESISC45 and UC-Merced with\nthree classic baseline models, VGG16, ResNet50 and DenseNet121. Results prove\nthat our proposed ESD-MBENet can achieve better accuracy than previous\nstate-of-the-art (SOTA) complex models. Moreover, abundant visualization\nanalysis make our method more convincing and interpretable.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 03:08:52 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Zhao", "Qi", ""], ["Ma", "Yujing", ""], ["Lyu", "Shuchang", ""], ["Chen", "Lijiang", ""]]}, {"id": "2104.00226", "submitter": "Junhui Yin", "authors": "Junhui Yin, Zhanyu Ma, Jiyang Xie, Shibo Nie, Kongming Liang, and Jun\n  Guo", "title": "DF^2AM: Dual-level Feature Fusion and Affinity Modeling for RGB-Infrared\n  Cross-modality Person Re-identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  RGB-infrared person re-identification is a challenging task due to the\nintra-class variations and cross-modality discrepancy. Existing works mainly\nfocus on learning modality-shared global representations by aligning image\nstyles or feature distributions across modalities, while local feature from\nbody part and relationships between person images are largely neglected. In\nthis paper, we propose a Dual-level (i.e., local and global) Feature Fusion\n(DF^2) module by learning attention for discriminative feature from local to\nglobal manner. In particular, the attention for a local feature is determined\nlocally, i.e., applying a learned transformation function on itself. Meanwhile,\nto further mining the relationships between global features from person images,\nwe propose an Affinities Modeling (AM) module to obtain the optimal intra- and\ninter-modality image matching. Specifically, AM employes intra-class\ncompactness and inter-class separability in the sample similarities as\nsupervised information to model the affinities between intra- and\ninter-modality samples. Experimental results show that our proposed method\noutperforms state-of-the-arts by large margins on two widely used\ncross-modality re-ID datasets SYSU-MM01 and RegDB, respectively.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 03:12:56 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Yin", "Junhui", ""], ["Ma", "Zhanyu", ""], ["Xie", "Jiyang", ""], ["Nie", "Shibo", ""], ["Liang", "Kongming", ""], ["Guo", "Jun", ""]]}, {"id": "2104.00231", "submitter": "Jun Wang", "authors": "Jun Wang", "title": "Two-phase weakly supervised object detection with pseudo ground truth\n  mining", "comments": "10 tables, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weakly Supervised Object Detection (WSOD), aiming to train detectors with\nonly image-level dataset, has arisen increasing attention for researchers. In\nthis project, we focus on two-phase WSOD architecture which integrates a\npowerful detector with a pure WSOD model. We explore the effectiveness of some\nrepresentative detectors utilized as the second-phase detector in two-phase\nWSOD and propose a two-phase WSOD architecture. In addition, we present a\nstrategy to establish the pseudo ground truth (PGT) used to train the\nsecond-phase detector. Unlike previous works that regard top one bounding boxes\nas PGT, we consider more bounding boxes to establish the PGT annotations. This\nalleviates the insufficient learning problem caused by the low recall of PGT.\nWe also propose some strategies to refine the PGT during the training of the\nsecond detector. Our strategies suspend the training in specific epoch, then\nrefine the PGT by the outputs of the second-phase detector. After that, the\nalgorithm continues the training with the same gradients and weights as those\nbefore suspending. Elaborate experiments are conduceted on the PASCAL VOC 2007\ndataset to verify the effectiveness of our methods. As results demonstrate, our\ntwo-phase architecture improves the mAP from 49.17% to 53.21% compared with the\nsingle PCL model. Additionally, the best PGT generation strategy obtains a 0.7%\nmAP increment. Our best refinement strategy boosts the performance by 1.74%\nmAP. The best results adopting all of our methods achieve 55.231% mAP which is\nthe state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 03:21:24 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Wang", "Jun", ""]]}, {"id": "2104.00232", "submitter": "Jiahui She", "authors": "Jiahui She, Yibo Hu, Hailin Shi, Jun Wang, Qiu Shen, Tao Mei", "title": "Dive into Ambiguity: Latent Distribution Mining and Pairwise Uncertainty\n  Estimation for Facial Expression Recognition", "comments": "Accepted by CVPR21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Due to the subjective annotation and the inherent interclass similarity of\nfacial expressions, one of key challenges in Facial Expression Recognition\n(FER) is the annotation ambiguity. In this paper, we proposes a solution, named\nDMUE, to address the problem of annotation ambiguity from two perspectives: the\nlatent Distribution Mining and the pairwise Uncertainty Estimation. For the\nformer, an auxiliary multi-branch learning framework is introduced to better\nmine and describe the latent distribution in the label space. For the latter,\nthe pairwise relationship of semantic feature between instances are fully\nexploited to estimate the ambiguity extent in the instance space. The proposed\nmethod is independent to the backbone architectures, and brings no extra burden\nfor inference. The experiments are conducted on the popular real-world\nbenchmarks and the synthetic noisy datasets. Either way, the proposed DMUE\nstably achieves leading performance.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 03:21:57 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["She", "Jiahui", ""], ["Hu", "Yibo", ""], ["Shi", "Hailin", ""], ["Wang", "Jun", ""], ["Shen", "Qiu", ""], ["Mei", "Tao", ""]]}, {"id": "2104.00233", "submitter": "Xirong Li", "authors": "Jie Wang and Kaibin Tian and Dayong Ding and Gang Yang and Xirong Li", "title": "Unsupervised Domain Expansion for Visual Categorization", "comments": "accepted as regular paper by ACM Transactions on Multimedia Computing\n  Communications and Applications (TOMM). Project URL\n  https://github.com/li-xirong/ude", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Expanding visual categorization into a novel domain without the need of extra\nannotation has been a long-term interest for multimedia intelligence.\nPreviously, this challenge has been approached by unsupervised domain\nadaptation (UDA). Given labeled data from a source domain and unlabeled data\nfrom a target domain, UDA seeks for a deep representation that is both\ndiscriminative and domain-invariant. While UDA focuses on the target domain, we\nargue that the performance on both source and target domains matters, as in\npractice which domain a test example comes from is unknown. In this paper we\nextend UDA by proposing a new task called unsupervised domain expansion (UDE),\nwhich aims to adapt a deep model for the target domain with its unlabeled data,\nmeanwhile maintaining the model's performance on the source domain. We propose\nKnowledge Distillation Domain Expansion (KDDE) as a general method for the UDE\ntask. Its domain-adaptation module can be instantiated with any existing model.\nWe develop a knowledge distillation based learning mechanism, enabling KDDE to\noptimize a single objective wherein the source and target domains are equally\ntreated. Extensive experiments on two major benchmarks, i.e., Office-Home and\nDomainNet, show that KDDE compares favorably against four competitive\nbaselines, i.e., DDC, DANN, DAAN, and CDAN, for both UDA and UDE tasks. Our\nstudy also reveals that the current UDA models improve their performance on the\ntarget domain at the cost of noticeable performance loss on the source domain.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 03:27:35 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Wang", "Jie", ""], ["Tian", "Kaibin", ""], ["Ding", "Dayong", ""], ["Yang", "Gang", ""], ["Li", "Xirong", ""]]}, {"id": "2104.00234", "submitter": "Xiushan Nie", "authors": "Xinfang Liu, Xiushan Nie (Member, IEEE), Zhifang Tan, Jie Guo, Yilong\n  Yin", "title": "A Survey on Natural Language Video Localization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural language video localization (NLVL), which aims to locate a target\nmoment from a video that semantically corresponds to a text query, is a novel\nand challenging task. Toward this end, in this paper, we present a\ncomprehensive survey of the NLVL algorithms, where we first propose the\npipeline of NLVL, and then categorize them into supervised and\nweakly-supervised methods, following by the analysis of the strengths and\nweaknesses of each kind of methods. Subsequently, we present the dataset,\nevaluation protocols and the general performance analysis. Finally, the\npossible perspectives are obtained by summarizing the existing methods.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 03:30:45 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Liu", "Xinfang", "", "Member, IEEE"], ["Nie", "Xiushan", "", "Member, IEEE"], ["Tan", "Zhifang", ""], ["Guo", "Jie", ""], ["Yin", "Yilong", ""]]}, {"id": "2104.00239", "submitter": "Jinxing Zhou", "authors": "Jinxing Zhou, Liang Zheng, Yiran Zhong, Shijie Hao, Meng Wang", "title": "Positive Sample Propagation along the Audio-Visual Event Line", "comments": "Accepted to CVPR 2021. Code is available at\n  https://github.com/jasongief/PSP_CVPR_2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual and audio signals often coexist in natural environments, forming\naudio-visual events (AVEs). Given a video, we aim to localize video segments\ncontaining an AVE and identify its category. In order to learn discriminative\nfeatures for a classifier, it is pivotal to identify the helpful (or positive)\naudio-visual segment pairs while filtering out the irrelevant ones, regardless\nwhether they are synchronized or not. To this end, we propose a new positive\nsample propagation (PSP) module to discover and exploit the closely related\naudio-visual pairs by evaluating the relationship within every possible pair.\nIt can be done by constructing an all-pair similarity map between each audio\nand visual segment, and only aggregating the features from the pairs with high\nsimilarity scores. To encourage the network to extract high correlated features\nfor positive samples, a new audio-visual pair similarity loss is proposed. We\nalso propose a new weighting branch to better exploit the temporal correlations\nin weakly supervised setting. We perform extensive experiments on the public\nAVE dataset and achieve new state-of-the-art accuracy in both fully and weakly\nsupervised settings, thus verifying the effectiveness of our method.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 03:53:57 GMT"}, {"version": "v2", "created": "Mon, 5 Apr 2021 07:28:13 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Zhou", "Jinxing", ""], ["Zheng", "Liang", ""], ["Zhong", "Yiran", ""], ["Hao", "Shijie", ""], ["Wang", "Meng", ""]]}, {"id": "2104.00240", "submitter": "Ziyuan Huang", "authors": "Ziyuan Huang, Shiwei Zhang, Jianwen Jiang, Mingqian Tang, Rong Jin,\n  Marcelo Ang", "title": "Self-supervised Motion Learning from Static Images", "comments": "To appear in CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motions are reflected in videos as the movement of pixels, and actions are\nessentially patterns of inconsistent motions between the foreground and the\nbackground. To well distinguish the actions, especially those with complicated\nspatio-temporal interactions, correctly locating the prominent motion areas is\nof crucial importance. However, most motion information in existing videos are\ndifficult to label and training a model with good motion representations with\nsupervision will thus require a large amount of human labour for annotation. In\nthis paper, we address this problem by self-supervised learning. Specifically,\nwe propose to learn Motion from Static Images (MoSI). The model learns to\nencode motion information by classifying pseudo motions generated by MoSI. We\nfurthermore introduce a static mask in pseudo motions to create local motion\npatterns, which forces the model to additionally locate notable motion areas\nfor the correct classification.We demonstrate that MoSI can discover regions\nwith large motion even without fine-tuning on the downstream datasets. As a\nresult, the learned motion representations boost the performance of tasks\nrequiring understanding of complex scenes and motions, i.e., action\nrecognition. Extensive experiments show the consistent and transferable\nimprovements achieved by MoSI. Codes will be soon released.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 03:55:50 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Huang", "Ziyuan", ""], ["Zhang", "Shiwei", ""], ["Jiang", "Jianwen", ""], ["Tang", "Mingqian", ""], ["Jin", "Rong", ""], ["Ang", "Marcelo", ""]]}, {"id": "2104.00246", "submitter": "Qing Yu", "authors": "Qing Yu, Atsushi Hashimoto, Yoshitaka Ushiku", "title": "Divergence Optimization for Noisy Universal Domain Adaptation", "comments": "CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Universal domain adaptation (UniDA) has been proposed to transfer knowledge\nlearned from a label-rich source domain to a label-scarce target domain without\nany constraints on the label sets. In practice, however, it is difficult to\nobtain a large amount of perfectly clean labeled data in a source domain with\nlimited resources. Existing UniDA methods rely on source samples with correct\nannotations, which greatly limits their application in the real world. Hence,\nwe consider a new realistic setting called Noisy UniDA, in which classifiers\nare trained with noisy labeled data from the source domain and unlabeled data\nwith an unknown class distribution from the target domain. This paper\nintroduces a two-head convolutional neural network framework to solve all\nproblems simultaneously. Our network consists of one common feature generator\nand two classifiers with different decision boundaries. By optimizing the\ndivergence between the two classifiers' outputs, we can detect noisy source\nsamples, find \"unknown\" classes in the target domain, and align the\ndistribution of the source and target domains. In an extensive evaluation of\ndifferent domain adaptation settings, the proposed method outperformed existing\nmethods by a large margin in most settings.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 04:16:04 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Yu", "Qing", ""], ["Hashimoto", "Atsushi", ""], ["Ushiku", "Yoshitaka", ""]]}, {"id": "2104.00249", "submitter": "Seong Hyeon Park", "authors": "ByeoungDo Kim, Seong Hyeon Park, Seokhwan Lee, Elbek Khoshimjonov,\n  Dongsuk Kum, Junsoo Kim, Jeong Soo Kim, Jun Won Choi", "title": "LaPred: Lane-Aware Prediction of Multi-Modal Future Trajectories of\n  Dynamic Agents", "comments": "13 pages, 2 figures, 7 tables, CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the problem of predicting the future motion of a\ndynamic agent (called a target agent) given its current and past states as well\nas the information on its environment. It is paramount to develop a prediction\nmodel that can exploit the contextual information in both static and dynamic\nenvironments surrounding the target agent and generate diverse trajectory\nsamples that are meaningful in a traffic context. We propose a novel prediction\nmodel, referred to as the lane-aware prediction (LaPred) network, which uses\nthe instance-level lane entities extracted from a semantic map to predict the\nmulti-modal future trajectories. For each lane candidate found in the\nneighborhood of the target agent, LaPred extracts the joint features relating\nthe lane and the trajectories of the neighboring agents. Then, the features for\nall lane candidates are fused with the attention weights learned through a\nself-supervised learning task that identifies the lane candidate likely to be\nfollowed by the target agent. Using the instance-level lane information, LaPred\ncan produce the trajectories compliant with the surroundings better than 2D\nraster image-based methods and generate the diverse future trajectories given\nmultiple lane candidates. The experiments conducted on the public nuScenes\ndataset and Argoverse dataset demonstrate that the proposed LaPred method\nsignificantly outperforms the existing prediction models, achieving\nstate-of-the-art performance in the benchmarks.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 04:33:36 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Kim", "ByeoungDo", ""], ["Park", "Seong Hyeon", ""], ["Lee", "Seokhwan", ""], ["Khoshimjonov", "Elbek", ""], ["Kum", "Dongsuk", ""], ["Kim", "Junsoo", ""], ["Kim", "Jeong Soo", ""], ["Choi", "Jun Won", ""]]}, {"id": "2104.00253", "submitter": "Yi Wang", "authors": "Yunhao Yang, Yuhan Zheng, Yi Wang and Chandrajit Bajaj", "title": "Learning Deep Latent Subspaces for Image Denoising", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Heterogeneity exists in most camera images. This heterogeneity manifests\nitself across the image space as varied Moire ringing, motion-blur,\ncolor-bleaching or lens based projection distortions. Moreover, combinations of\nthese image artifacts can be present in small or large pixel neighborhoods,\nwithin an acquired image. Current camera image processing pipelines, including\ndeep trained versions, tend to rectify the issue applying a single filter that\nis homogeneously applied to the entire image. This is also particularly true\nwhen an encoder-decoder type deep architecture is trained for the task. In this\npaper, we present a structured deep learning model that solves the\nheterogeneous image artifact filtering problem. We call our deep trained model\nthe Patch Subspace Variational Autoencoder (PS-VAE) for Camera ISP. PS-VAE does\nnot necessarily assume uniform image distortion levels nor similar artifact\ntypes within the image. Rather, our model attempts to learn to cluster\ndifferent patches extracted from images into artifact type and distortion\nlevels, within multiple latent subspaces (e.g. Moire ringing artifacts are\noften a higher dimensional latent distortion than a Gaussian motion blur\nartifact). Each image's patches are encoded into soft-clusters in their\nappropriate latent sub-space, using a prior mixture model. The decoders of the\nPS-VAE are also trained in an unsupervised manner for each of the image patches\nin each soft-cluster. Our experimental results demonstrates the flexibility and\nperformance that one can achieve through improved heterogeneous filtering. We\ncompare our results to a conventional one-encoder-one-decoder architecture.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 04:40:22 GMT"}, {"version": "v2", "created": "Thu, 22 Apr 2021 14:29:47 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Yang", "Yunhao", ""], ["Zheng", "Yuhan", ""], ["Wang", "Yi", ""], ["Bajaj", "Chandrajit", ""]]}, {"id": "2104.00272", "submitter": "Kevin Lin", "authors": "Kevin Lin, Lijuan Wang, Zicheng Liu", "title": "Mesh Graphormer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a graph-convolution-reinforced transformer, named Mesh Graphormer,\nfor 3D human pose and mesh reconstruction from a single image. Recently both\ntransformers and graph convolutional neural networks (GCNNs) have shown\npromising progress in human mesh reconstruction. Transformer-based approaches\nare effective in modeling non-local interactions among 3D mesh vertices and\nbody joints, whereas GCNNs are good at exploiting neighborhood vertex\ninteractions based on a pre-specified mesh topology. In this paper, we study\nhow to combine graph convolutions and self-attentions in a transformer to model\nboth local and global interactions. Experimental results show that our proposed\nmethod, Mesh Graphormer, significantly outperforms the previous\nstate-of-the-art methods on multiple benchmarks, including Human3.6M, 3DPW, and\nFreiHAND datasets\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 06:16:36 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Lin", "Kevin", ""], ["Wang", "Lijuan", ""], ["Liu", "Zicheng", ""]]}, {"id": "2104.00285", "submitter": "Luowei Zhou", "authors": "Luowei Zhou, Jingjing Liu, Yu Cheng, Zhe Gan, Lei Zhang", "title": "CUPID: Adaptive Curation of Pre-training Data for Video-and-Language\n  Representation Learning", "comments": "12 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work concerns video-language pre-training and representation learning.\nIn this now ubiquitous training scheme, a model first performs pre-training on\npaired videos and text (e.g., video clips and accompanied subtitles) from a\nlarge uncurated source corpus, before transferring to specific downstream\ntasks. This two-stage training process inevitably raises questions about the\ngeneralization ability of the pre-trained model, which is particularly\npronounced when a salient domain gap exists between source and target data\n(e.g., instructional cooking videos vs. movies). In this paper, we first bring\nto light the sensitivity of pre-training objectives (contrastive vs.\nreconstructive) to domain discrepancy. Then, we propose a simple yet effective\nframework, CUPID, to bridge this domain gap by filtering and adapting source\ndata to the target data, followed by domain-focused pre-training. Comprehensive\nexperiments demonstrate that pre-training on a considerably small subset of\ndomain-focused data can effectively close the source-target domain gap and\nachieve significant performance gain, compared to random sampling or even\nexploiting the full pre-training dataset. CUPID yields new state-of-the-art\nperformance across multiple video-language and video tasks, including\ntext-to-video retrieval [72, 37], video question answering [36], and video\ncaptioning [72], with consistent performance lift over different pre-training\nmethods.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 06:42:16 GMT"}, {"version": "v2", "created": "Tue, 13 Apr 2021 06:04:48 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Zhou", "Luowei", ""], ["Liu", "Jingjing", ""], ["Cheng", "Yu", ""], ["Gan", "Zhe", ""], ["Zhang", "Lei", ""]]}, {"id": "2104.00287", "submitter": "Sifei Liu", "authors": "Yang Fu, Sifei Liu, Umar Iqbal, Shalini De Mello, Humphrey Shi, Jan\n  Kautz", "title": "Learning to Track Instances without Video Annotations", "comments": "CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Tracking segmentation masks of multiple instances has been intensively\nstudied, but still faces two fundamental challenges: 1) the requirement of\nlarge-scale, frame-wise annotation, and 2) the complexity of two-stage\napproaches. To resolve these challenges, we introduce a novel semi-supervised\nframework by learning instance tracking networks with only a labeled image\ndataset and unlabeled video sequences. With an instance contrastive objective,\nwe learn an embedding to discriminate each instance from the others. We show\nthat even when only trained with images, the learned feature representation is\nrobust to instance appearance variations, and is thus able to track objects\nsteadily across frames. We further enhance the tracking capability of the\nembedding by learning correspondence from unlabeled videos in a self-supervised\nmanner. In addition, we integrate this module into single-stage instance\nsegmentation and pose estimation frameworks, which significantly reduce the\ncomputational complexity of tracking compared to two-stage networks. We conduct\nexperiments on the YouTube-VIS and PoseTrack datasets. Without any video\nannotation efforts, our proposed method can achieve comparable or even better\nperformance than most fully-supervised methods.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 06:47:41 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Fu", "Yang", ""], ["Liu", "Sifei", ""], ["Iqbal", "Umar", ""], ["De Mello", "Shalini", ""], ["Shi", "Humphrey", ""], ["Kautz", "Jan", ""]]}, {"id": "2104.00297", "submitter": "Xiufeng Jiang", "authors": "Xiufeng Jiang, Shugong Xu (Fellow, IEEE), Shunqing Zhang (Senior\n  Member, IEEE), and Shan Cao", "title": "Arbitrary-Shaped Text Detection withAdaptive Text Region Representation", "comments": "This is an article published in IEEE Access", "journal-ref": null, "doi": "10.1109/ACCESS.2020.2999069", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text detection/localization, as an important task in computer vision, has\nwitnessed substantialadvancements in methodology and performance with\nconvolutional neural networks. However, the vastmajority of popular methods use\nrectangles or quadrangles to describe text regions. These representationshave\ninherent drawbacks, especially relating to dense adjacent text and loose\nregional text boundaries,which usually cause difficulty detecting arbitrarily\nshaped text. In this paper, we propose a novel text regionrepresentation\nmethod, with a robust pipeline, which can precisely detect dense adjacent text\ninstances witharbitrary shapes. We consider a text instance to be composed of\nan adaptive central text region mask anda corresponding expanding ratio between\nthe central text region and the full text region. More specifically,our\npipeline generates adaptive central text regions and corresponding expanding\nratios with a proposedtraining strategy, followed by a new proposed\npost-processing algorithm which expands central text regionsto the complete\ntext instance with the corresponding expanding ratios. We demonstrated that our\nnew textregion representation is effective, and that the pipeline can precisely\ndetect closely adjacent text instances ofarbitrary shapes. Experimental results\non common datasets demonstrate superior performance o\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 07:06:34 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Jiang", "Xiufeng", "", "Fellow, IEEE"], ["Xu", "Shugong", "", "Fellow, IEEE"], ["Zhang", "Shunqing", "", "Senior\n  Member, IEEE"], ["Cao", "Shan", ""]]}, {"id": "2104.00298", "submitter": "Mingxing Tan", "authors": "Mingxing Tan, Quoc V. Le", "title": "EfficientNetV2: Smaller Models and Faster Training", "comments": "ICML 2021", "journal-ref": "International Conference on Machine Learning, 2021", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces EfficientNetV2, a new family of convolutional networks\nthat have faster training speed and better parameter efficiency than previous\nmodels. To develop this family of models, we use a combination of\ntraining-aware neural architecture search and scaling, to jointly optimize\ntraining speed and parameter efficiency. The models were searched from the\nsearch space enriched with new ops such as Fused-MBConv. Our experiments show\nthat EfficientNetV2 models train much faster than state-of-the-art models while\nbeing up to 6.8x smaller.\n  Our training can be further sped up by progressively increasing the image\nsize during training, but it often causes a drop in accuracy. To compensate for\nthis accuracy drop, we propose to adaptively adjust regularization (e.g.,\ndropout and data augmentation) as well, such that we can achieve both fast\ntraining and good accuracy.\n  With progressive learning, our EfficientNetV2 significantly outperforms\nprevious models on ImageNet and CIFAR/Cars/Flowers datasets. By pretraining on\nthe same ImageNet21k, our EfficientNetV2 achieves 87.3% top-1 accuracy on\nImageNet ILSVRC2012, outperforming the recent ViT by 2.0% accuracy while\ntraining 5x-11x faster using the same computing resources. Code will be\navailable at https://github.com/google/automl/tree/master/efficientnetv2.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 07:08:36 GMT"}, {"version": "v2", "created": "Thu, 13 May 2021 01:51:01 GMT"}, {"version": "v3", "created": "Wed, 23 Jun 2021 22:04:56 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Tan", "Mingxing", ""], ["Le", "Quoc V.", ""]]}, {"id": "2104.00299", "submitter": "Hojung Lee", "authors": "Hojung Lee, Jong-Seok Lee", "title": "Students are the Best Teacher: Exit-Ensemble Distillation with\n  Multi-Exits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper proposes a novel knowledge distillation-based learning method to\nimprove the classification performance of convolutional neural networks (CNNs)\nwithout a pre-trained teacher network, called exit-ensemble distillation. Our\nmethod exploits the multi-exit architecture that adds auxiliary classifiers\n(called exits) in the middle of a conventional CNN, through which early\ninference results can be obtained. The idea of our method is to train the\nnetwork using the ensemble of the exits as the distillation target, which\ngreatly improves the classification performance of the overall network. Our\nmethod suggests a new paradigm of knowledge distillation; unlike the\nconventional notion of distillation where teachers only teach students, we show\nthat students can also help other students and even the teacher to learn\nbetter. Experimental results demonstrate that our method achieves significant\nimprovement of classification performance on various popular CNN architectures\n(VGG, ResNet, ResNeXt, WideResNet, etc.). Furthermore, the proposed method can\nexpedite the convergence of learning with improved stability. Our code will be\navailable on Github.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 07:10:36 GMT"}, {"version": "v2", "created": "Mon, 5 Apr 2021 01:13:20 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Lee", "Hojung", ""], ["Lee", "Jong-Seok", ""]]}, {"id": "2104.00303", "submitter": "Jennifer Jang", "authors": "Jennifer Jang, Heinrich Jiang", "title": "MeanShift++: Extremely Fast Mode-Seeking With Applications to\n  Segmentation and Object Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  MeanShift is a popular mode-seeking clustering algorithm used in a wide range\nof applications in machine learning. However, it is known to be prohibitively\nslow, with quadratic runtime per iteration. We propose MeanShift++, an\nextremely fast mode-seeking algorithm based on MeanShift that uses a grid-based\napproach to speed up the mean shift step, replacing the computationally\nexpensive neighbors search with a density-weighted mean of adjacent grid cells.\nIn addition, we show that this grid-based technique for density estimation\ncomes with theoretical guarantees. The runtime is linear in the number of\npoints and exponential in dimension, which makes MeanShift++ ideal on\nlow-dimensional applications such as image segmentation and object tracking. We\nprovide extensive experimental analysis showing that MeanShift++ can be more\nthan 10,000x faster than MeanShift with competitive clustering results on\nbenchmark datasets and nearly identical image segmentations as MeanShift.\nFinally, we show promising results for object tracking.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 07:14:11 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Jang", "Jennifer", ""], ["Jiang", "Heinrich", ""]]}, {"id": "2104.00305", "submitter": "Dong Yao", "authors": "Dong Yao, Shengyu Zhang, Zhou Zhao, Wenyan Fan, Jieming Zhu, Xiuqiang\n  He, Fei Wu", "title": "Modeling High-order Interactions across Multi-interests for Micro-video\n  Recommendation", "comments": "accepted to AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Personalized recommendation system has become pervasive in various video\nplatform. Many effective methods have been proposed, but most of them didn't\ncapture the user's multi-level interest trait and dependencies between their\nviewed micro-videos well. To solve these problems, we propose a Self-over-Co\nAttention module to enhance user's interest representation. In particular, we\nfirst use co-attention to model correlation patterns across different levels\nand then use self-attention to model correlation patterns within a specific\nlevel. Experimental results on filtered public datasets verify that our\npresented module is useful.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 07:20:15 GMT"}, {"version": "v2", "created": "Mon, 10 May 2021 15:02:57 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Yao", "Dong", ""], ["Zhang", "Shengyu", ""], ["Zhao", "Zhou", ""], ["Fan", "Wenyan", ""], ["Zhu", "Jieming", ""], ["He", "Xiuqiang", ""], ["Wu", "Fei", ""]]}, {"id": "2104.00308", "submitter": "Rongjie Li", "authors": "Rongjie Li, Songyang Zhang, Bo Wan, Xuming He", "title": "Bipartite Graph Network with Adaptive Message Passing for Unbiased Scene\n  Graph Generation", "comments": "Accepted by CVPR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Scene graph generation is an important visual understanding task with a broad\nrange of vision applications. Despite recent tremendous progress, it remains\nchallenging due to the intrinsic long-tailed class distribution and large\nintra-class variation. To address these issues, we introduce a novel\nconfidence-aware bipartite graph neural network with adaptive message\npropagation mechanism for unbiased scene graph generation. In addition, we\npropose an efficient bi-level data resampling strategy to alleviate the\nimbalanced data distribution problem in training our graph network. Our\napproach achieves superior or competitive performance over previous methods on\nseveral challenging datasets, including Visual Genome, Open Images V4/V6,\ndemonstrating its effectiveness and generality.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 07:30:14 GMT"}, {"version": "v2", "created": "Thu, 29 Apr 2021 03:33:36 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Li", "Rongjie", ""], ["Zhang", "Songyang", ""], ["Wan", "Bo", ""], ["He", "Xuming", ""]]}, {"id": "2104.00315", "submitter": "Yan-Bo Lin", "authors": "Yan-Bo Lin, Hung-Yu Tseng, Hsin-Ying Lee, Yen-Yu Lin, Ming-Hsuan Yang", "title": "Unsupervised Sound Localization via Iterative Contrastive Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.SD eess.AS eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sound localization aims to find the source of the audio signal in the visual\nscene. However, it is labor-intensive to annotate the correlations between the\nsignals sampled from the audio and visual modalities, thus making it difficult\nto supervise the learning of a machine for this task. In this work, we propose\nan iterative contrastive learning framework that requires no data annotations.\nAt each iteration, the proposed method takes the 1) localization results in\nimages predicted in the previous iteration, and 2) semantic relationships\ninferred from the audio signals as the pseudo-labels. We then use the\npseudo-labels to learn the correlation between the visual and audio signals\nsampled from the same video (intra-frame sampling) as well as the association\nbetween those extracted across videos (inter-frame relation). Our iterative\nstrategy gradually encourages the localization of the sounding objects and\nreduces the correlation between the non-sounding regions and the reference\naudio. Quantitative and qualitative experimental results demonstrate that the\nproposed framework performs favorably against existing unsupervised and\nweakly-supervised methods on the sound localization task.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 07:48:29 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Lin", "Yan-Bo", ""], ["Tseng", "Hung-Yu", ""], ["Lee", "Hsin-Ying", ""], ["Lin", "Yen-Yu", ""], ["Yang", "Ming-Hsuan", ""]]}, {"id": "2104.00317", "submitter": "Phong Tran", "authors": "Phong Tran and Anh Tran and Quynh Phung and Minh Hoai", "title": "Explore Image Deblurring via Blur Kernel Space", "comments": "Accepted to CVPR'21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a method to encode the blur operators of an arbitrary\ndataset of sharp-blur image pairs into a blur kernel space. Assuming the\nencoded kernel space is close enough to in-the-wild blur operators, we propose\nan alternating optimization algorithm for blind image deblurring. It\napproximates an unseen blur operator by a kernel in the encoded space and\nsearches for the corresponding sharp image. Unlike recent deep-learning-based\nmethods, our system can handle unseen blur kernel, while avoiding using\ncomplicated handcrafted priors on the blur operator often found in classical\nmethods. Due to the method's design, the encoded kernel space is fully\ndifferentiable, thus can be easily adopted in deep neural network models.\nMoreover, our method can be used for blur synthesis by transferring existing\nblur operators from a given dataset into a new domain. Finally, we provide\nexperimental results to confirm the effectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 07:52:53 GMT"}, {"version": "v2", "created": "Sat, 3 Apr 2021 12:58:29 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Tran", "Phong", ""], ["Tran", "Anh", ""], ["Phung", "Quynh", ""], ["Hoai", "Minh", ""]]}, {"id": "2104.00319", "submitter": "Yoonhyung Kim", "authors": "Yoonhyung Kim and Changick Kim", "title": "Semi-Supervised Domain Adaptation via Selective Pseudo Labeling and\n  Progressive Self-Training", "comments": "Accepted at ICPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain adaptation (DA) is a representation learning methodology that\ntransfers knowledge from a label-sufficient source domain to a label-scarce\ntarget domain. While most of early methods are focused on unsupervised DA\n(UDA), several studies on semi-supervised DA (SSDA) are recently suggested. In\nSSDA, a small number of labeled target images are given for training, and the\neffectiveness of those data is demonstrated by the previous studies. However,\nthe previous SSDA approaches solely adopt those data for embedding ordinary\nsupervised losses, overlooking the potential usefulness of the few yet\ninformative clues. Based on this observation, in this paper, we propose a novel\nmethod that further exploits the labeled target images for SSDA. Specifically,\nwe utilize labeled target images to selectively generate pseudo labels for\nunlabeled target images. In addition, based on the observation that pseudo\nlabels are inevitably noisy, we apply a label noise-robust learning scheme,\nwhich progressively updates the network and the set of pseudo labels by turns.\nExtensive experimental results show that our proposed method outperforms other\nprevious state-of-the-art SSDA methods.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 07:56:41 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Kim", "Yoonhyung", ""], ["Kim", "Changick", ""]]}, {"id": "2104.00322", "submitter": "Matan Levi", "authors": "Matan Levi, Idan Attias, Aryeh Kontorovich", "title": "Domain Invariant Adversarial Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The phenomenon of adversarial examples illustrates one of the most basic\nvulnerabilities of deep neural networks. Among the variety of techniques\nintroduced to surmount this inherent weakness, adversarial training has emerged\nas the most common and efficient strategy to achieve robustness. Typically,\nthis is achieved by balancing robust and natural objectives. In this work, we\naim to achieve better trade-off between robust and natural performances by\nenforcing a domain-invariant feature representation. We present a new\nadversarial training method, Domain Invariant Adversarial Learning (DIAL),\nwhich learns a feature representation which is both robust and domain\ninvariant. DIAL uses a variant of Domain Adversarial Neural Network (DANN) on\nthe natural domain and its corresponding adversarial domain. In a case where\nthe source domain consists of natural examples and the target domain is the\nadversarially perturbed examples, our method learns a feature representation\nconstrained not to discriminate between the natural and adversarial examples,\nand can therefore achieve a more robust representation. Our experiments\nindicate that our method improves both robustness and natural accuracy, when\ncompared to current state-of-the-art adversarial training methods.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 08:04:10 GMT"}, {"version": "v2", "created": "Sun, 20 Jun 2021 14:23:20 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Levi", "Matan", ""], ["Attias", "Idan", ""], ["Kontorovich", "Aryeh", ""]]}, {"id": "2104.00323", "submitter": "Pengguang Chen", "authors": "Pengguang Chen, Shu Liu, Jiaya Jia", "title": "Jigsaw Clustering for Unsupervised Visual Representation Learning", "comments": "CVPR 2021 Oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised representation learning with contrastive learning achieved great\nsuccess. This line of methods duplicate each training batch to construct\ncontrastive pairs, making each training batch and its augmented version\nforwarded simultaneously and leading to additional computation. We propose a\nnew jigsaw clustering pretext task in this paper, which only needs to forward\neach training batch itself, and reduces the training cost. Our method makes use\nof information from both intra- and inter-images, and outperforms previous\nsingle-batch based ones by a large margin. It is even comparable to the\ncontrastive learning methods when only half of training batches are used.\n  Our method indicates that multiple batches during training are not necessary,\nand opens the door for future research of single-batch unsupervised methods.\nOur models trained on ImageNet datasets achieve state-of-the-art results with\nlinear classification, outperforming previous single-batch methods by 2.6%.\nModels transferred to COCO datasets outperform MoCo v2 by 0.4% with only half\nof the training batches. Our pretrained models outperform supervised ImageNet\npretrained models on CIFAR-10 and CIFAR-100 datasets by 0.9% and 4.1%\nrespectively. Code is available at\nhttps://github.com/Jia-Research-Lab/JigsawClustering\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 08:09:26 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Chen", "Pengguang", ""], ["Liu", "Shu", ""], ["Jia", "Jiaya", ""]]}, {"id": "2104.00324", "submitter": "Zhihong Fu", "authors": "Zhihong Fu, Qingjie Liu, Zehua Fu, Yunhong Wang", "title": "STMTrack: Template-free Visual Tracking with Space-time Memory Networks", "comments": "Accepted by CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Boosting performance of the offline trained siamese trackers is getting\nharder nowadays since the fixed information of the template cropped from the\nfirst frame has been almost thoroughly mined, but they are poorly capable of\nresisting target appearance changes. Existing trackers with template updating\nmechanisms rely on time-consuming numerical optimization and complex\nhand-designed strategies to achieve competitive performance, hindering them\nfrom real-time tracking and practical applications. In this paper, we propose a\nnovel tracking framework built on top of a space-time memory network that is\ncompetent to make full use of historical information related to the target for\nbetter adapting to appearance variations during tracking. Specifically, a novel\nmemory mechanism is introduced, which stores the historical information of the\ntarget to guide the tracker to focus on the most informative regions in the\ncurrent frame. Furthermore, the pixel-level similarity computation of the\nmemory network enables our tracker to generate much more accurate bounding\nboxes of the target. Extensive experiments and comparisons with many\ncompetitive trackers on challenging large-scale benchmarks, OTB-2015,\nTrackingNet, GOT-10k, LaSOT, UAV123, and VOT2018, show that, without bells and\nwhistles, our tracker outperforms all previous state-of-the-art real-time\nmethods while running at 37 FPS. The code is available at\nhttps://github.com/fzh0917/STMTrack.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 08:10:56 GMT"}, {"version": "v2", "created": "Fri, 2 Apr 2021 09:02:30 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Fu", "Zhihong", ""], ["Liu", "Qingjie", ""], ["Fu", "Zehua", ""], ["Wang", "Yunhong", ""]]}, {"id": "2104.00325", "submitter": "Shuo Wang", "authors": "Jingfeng Lu, Shuo Wang, Ping Li, Dong Ye", "title": "High-quality Low-dose CT Reconstruction Using Convolutional Neural\n  Networks with Spatial and Channel Squeeze and Excitation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Low-dose computed tomography (CT) allows the reduction of radiation risk in\nclinical applications at the expense of image quality, which deteriorates the\ndiagnosis accuracy of radiologists. In this work, we present a High-Quality\nImaging network (HQINet) for the CT image reconstruction from Low-dose computed\ntomography (CT) acquisitions. HQINet was a convolutional encoder-decoder\narchitecture, where the encoder was used to extract spatial and temporal\ninformation from three contiguous slices while the decoder was used to recover\nthe spacial information of the middle slice. We provide experimental results on\nthe real projection data from low-dose CT Image and Projection Data\n(LDCT-and-Projection-data), demonstrating that the proposed approach yielded a\nnotable improvement of the performance in terms of image quality, with a rise\nof 5.5dB in terms of peak signal-to-noise ratio (PSNR) and 0.29 in terms of\nmutual information (MI).\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 08:15:53 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Lu", "Jingfeng", ""], ["Wang", "Shuo", ""], ["Li", "Ping", ""], ["Ye", "Dong", ""]]}, {"id": "2104.00327", "submitter": "Shintaro Nishi", "authors": "Shintaro Nishi, Takeaki Kadota, Seiichi Uchida", "title": "Famous Companies Use More Letters in Logo:A Large-Scale Analysis of Text\n  Area in Logo", "comments": "Accepted at 14th International Workshop on Graphics Recognition\n  (GREC2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper analyzes a large number of logo images from the LLD-logo dataset,\nby recent deep learning-based techniques, to understand not only design trends\nof logo images and but also the correlation to their owner company. Especially,\nwe focus on three correlations between logo images and their text areas,\nbetween the text areas and the number of followers on Twitter, and between the\nlogo images and the number of followers. Various findings include the weak\npositive correlation between the text area ratio and the number of followers of\nthe company. In addition, deep regression and deep ranking methods can catch\ncorrelations between the logo images and the number of followers.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 08:19:29 GMT"}, {"version": "v2", "created": "Wed, 30 Jun 2021 05:31:04 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Nishi", "Shintaro", ""], ["Kadota", "Takeaki", ""], ["Uchida", "Seiichi", ""]]}, {"id": "2104.00332", "submitter": "Mingyang Zhou", "authors": "Mingyang Zhou, Luowei Zhou, Shuohang Wang, Yu Cheng, Linjie Li, Zhou\n  Yu, Jingjing Liu", "title": "UC2: Universal Cross-lingual Cross-modal Vision-and-Language\n  Pre-training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vision-and-language pre-training has achieved impressive success in learning\nmultimodal representations between vision and language. To generalize this\nsuccess to non-English languages, we introduce UC2, the first machine\ntranslation-augmented framework for cross-lingual cross-modal representation\nlearning. To tackle the scarcity problem of multilingual captions for image\ndatasets, we first augment existing English-only datasets with other languages\nvia machine translation (MT). Then we extend the standard Masked Language\nModeling and Image-Text Matching training objectives to multilingual setting,\nwhere alignment between different languages is captured through shared visual\ncontext (i.e, using image as pivot). To facilitate the learning of a joint\nembedding space of images and all languages of interest, we further propose two\nnovel pre-training tasks, namely Masked Region-to-Token Modeling (MRTM) and\nVisual Translation Language Modeling (VTLM), leveraging MT-enhanced translated\ndata. Evaluation on multilingual image-text retrieval and multilingual visual\nquestion answering benchmarks demonstrates that our proposed framework achieves\nnew state-of-the-art on diverse non-English benchmarks while maintaining\ncomparable performance to monolingual pre-trained models on English tasks.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 08:30:53 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Zhou", "Mingyang", ""], ["Zhou", "Luowei", ""], ["Wang", "Shuohang", ""], ["Cheng", "Yu", ""], ["Li", "Linjie", ""], ["Yu", "Zhou", ""], ["Liu", "Jingjing", ""]]}, {"id": "2104.00337", "submitter": "Yinlin Hu", "authors": "Yinlin Hu, Sebastien Speierer, Wenzel Jakob, Pascal Fua, Mathieu\n  Salzmann", "title": "Wide-Depth-Range 6D Object Pose Estimation in Space", "comments": "CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  6D pose estimation in space poses unique challenges that are not commonly\nencountered in the terrestrial setting. One of the most striking differences is\nthe lack of atmospheric scattering, allowing objects to be visible from a great\ndistance while complicating illumination conditions. Currently available\nbenchmark datasets do not place a sufficient emphasis on this aspect and mostly\ndepict the target in close proximity.\n  Prior work tackling pose estimation under large scale variations relies on a\ntwo-stage approach to first estimate scale, followed by pose estimation on a\nresized image patch. We instead propose a single-stage hierarchical end-to-end\ntrainable network that is more robust to scale variations. We demonstrate that\nit outperforms existing approaches not only on images synthesized to resemble\nimages taken in space but also on standard benchmarks.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 08:39:26 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Hu", "Yinlin", ""], ["Speierer", "Sebastien", ""], ["Jakob", "Wenzel", ""], ["Fua", "Pascal", ""], ["Salzmann", "Mathieu", ""]]}, {"id": "2104.00340", "submitter": "Qing Shuai", "authors": "Qi Fang, Qing Shuai, Junting Dong, Hujun Bao, Xiaowei Zhou", "title": "Reconstructing 3D Human Pose by Watching Humans in the Mirror", "comments": "CVPR 2021 (Oral), project page:\n  https://zju3dv.github.io/Mirrored-Human/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we introduce the new task of reconstructing 3D human pose from\na single image in which we can see the person and the person's image through a\nmirror. Compared to general scenarios of 3D pose estimation from a single view,\nthe mirror reflection provides an additional view for resolving the depth\nambiguity. We develop an optimization-based approach that exploits mirror\nsymmetry constraints for accurate 3D pose reconstruction. We also provide a\nmethod to estimate the surface normal of the mirror from vanishing points in\nthe single image. To validate the proposed approach, we collect a large-scale\ndataset named Mirrored-Human, which covers a large variety of human subjects,\nposes and backgrounds. The experiments demonstrate that, when trained on\nMirrored-Human with our reconstructed 3D poses as pseudo ground-truth, the\naccuracy and generalizability of existing single-view 3D pose estimators can be\nlargely improved.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 08:42:51 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Fang", "Qi", ""], ["Shuai", "Qing", ""], ["Dong", "Junting", ""], ["Bao", "Hujun", ""], ["Zhou", "Xiaowei", ""]]}, {"id": "2104.00341", "submitter": "Tanmay Chakraborty", "authors": "Tanmay Chakraborty and Utkarsh Trehan", "title": "SpectralNET: Exploring Spatial-Spectral WaveletCNN for Hyperspectral\n  Image Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Hyperspectral Image (HSI) classification using Convolutional Neural Networks\n(CNN) is widely found in the current literature. Approaches vary from using\nSVMs to 2D CNNs, 3D CNNs, 3D-2D CNNs. Besides 3D-2D CNNs and FuSENet, the other\napproaches do not consider both the spectral and spatial features together for\nHSI classification task, thereby resulting in poor performances. 3D CNNs are\ncomputationally heavy and are not widely used, while 2D CNNs do not consider\nmulti-resolution processing of images, and only limits itself to the spatial\nfeatures. Even though 3D-2D CNNs try to model the spectral and spatial features\ntheir performance seems limited when applied over multiple dataset. In this\narticle, we propose SpectralNET, a wavelet CNN, which is a variation of 2D CNN\nfor multi-resolution HSI classification. A wavelet CNN uses layers of wavelet\ntransform to bring out spectral features. Computing a wavelet transform is\nlighter than computing 3D CNN. The spectral features extracted are then\nconnected to the 2D CNN which bring out the spatial features, thereby creating\na spatial-spectral feature vector for classification. Overall a better model is\nachieved that can classify multi-resolution HSI data with high accuracy.\nExperiments performed with SpectralNET on benchmark dataset, i.e. Indian Pines,\nUniversity of Pavia, and Salinas Scenes confirm the superiority of proposed\nSpectralNET with respect to the state-of-the-art methods. The code is publicly\navailable in https://github.com/tanmay-ty/SpectralNET.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 08:45:15 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Chakraborty", "Tanmay", ""], ["Trehan", "Utkarsh", ""]]}, {"id": "2104.00351", "submitter": "Kacper Kania", "authors": "Kacper Kania, Marek Kowalski, Tomasz Trzci\\'nski", "title": "TrajeVAE -- Controllable Human Motion Generation from Trajectories", "comments": "Animations used in the paper can be found at\n  https://kacperkan.github.io/trajevae-supplementary/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The generation of plausible and controllable 3D human motion animations is a\nlong-standing problem that often requires a manual intervention of skilled\nartists. Existing machine learning approaches try to semi-automate this process\nby allowing the user to input partial information about the future movement.\nHowever, they are limited in two significant ways: they either base their pose\nprediction on past prior frames with no additional control over the future\nposes or allow the user to input only a single trajectory that precludes\nfine-grained control over the output. To mitigate these two issues, we\nreformulate the problem of future pose prediction into pose completion in space\nand time where trajectories are represented as poses with missing joints. We\nshow that such a framework can generalize to other neural networks designed for\nfuture pose prediction. Once trained in this framework, a model is capable of\npredicting sequences from any number of trajectories. To leverage this notion,\nwe propose a novel transformer-like architecture, TrajeVAE, that provides a\nversatile framework for 3D human animation. We demonstrate that TrajeVAE\noutperforms trajectory-based reference approaches and methods that base their\npredictions on past poses in terms of accuracy. We also show that it can\npredict reasonable future poses even if provided only with an initial pose.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 09:12:48 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Kania", "Kacper", ""], ["Kowalski", "Marek", ""], ["Trzci\u0144ski", "Tomasz", ""]]}, {"id": "2104.00356", "submitter": "Tianyu Hua", "authors": "Tianyu Hua, Hongdong Zheng, Yalong Bai, Wei Zhang, Xiao-Ping Zhang,\n  Tao Mei", "title": "Exploiting Relationship for Complex-scene Image Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The significant progress on Generative Adversarial Networks (GANs) has\nfacilitated realistic single-object image generation based on language input.\nHowever, complex-scene generation (with various interactions among multiple\nobjects) still suffers from messy layouts and object distortions, due to\ndiverse configurations in layouts and appearances. Prior methods are mostly\nobject-driven and ignore their inter-relations that play a significant role in\ncomplex-scene images. This work explores relationship-aware complex-scene image\ngeneration, where multiple objects are inter-related as a scene graph. With the\nhelp of relationships, we propose three major updates in the generation\nframework. First, reasonable spatial layouts are inferred by jointly\nconsidering the semantics and relationships among objects. Compared to standard\nlocation regression, we show relative scales and distances serve a more\nreliable target. Second, since the relations between objects significantly\ninfluence an object's appearance, we design a relation-guided generator to\ngenerate objects reflecting their relationships. Third, a novel scene graph\ndiscriminator is proposed to guarantee the consistency between the generated\nimage and the input scene graph. Our method tends to synthesize plausible\nlayouts and objects, respecting the interplay of multiple objects in an image.\nExperimental results on Visual Genome and HICO-DET datasets show that our\nproposed method significantly outperforms prior arts in terms of IS and FID\nmetrics. Based on our user study and visual inspection, our method is more\neffective in generating logical layout and appearance for complex-scenes.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 09:21:39 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Hua", "Tianyu", ""], ["Zheng", "Hongdong", ""], ["Bai", "Yalong", ""], ["Zhang", "Wei", ""], ["Zhang", "Xiao-Ping", ""], ["Mei", "Tao", ""]]}, {"id": "2104.00359", "submitter": "Marc Habermann", "authors": "Linjie Lyu, Marc Habermann, Lingjie Liu, Mallikarjun B R, Ayush\n  Tewari, Christian Theobalt", "title": "Efficient and Differentiable Shadow Computation for Inverse Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Differentiable rendering has received increasing interest for image-based\ninverse problems. It can benefit traditional optimization-based solutions to\ninverse problems, but also allows for self-supervision of learning-based\napproaches for which training data with ground truth annotation is hard to\nobtain. However, existing differentiable renderers either do not model\nvisibility of the light sources from the different points in the scene,\nresponsible for shadows in the images, or are too slow for being used to train\ndeep architectures over thousands of iterations. To this end, we propose an\naccurate yet efficient approach for differentiable visibility and soft shadow\ncomputation. Our approach is based on the spherical harmonics approximations of\nthe scene illumination and visibility, where the occluding surface is\napproximated with spheres. This allows for a significantly more efficient\nshadow computation compared to methods based on ray tracing. As our formulation\nis differentiable, it can be used to solve inverse problems such as texture,\nillumination, rigid pose, and geometric deformation recovery from images using\nanalysis-by-synthesis optimization.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 09:29:05 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Lyu", "Linjie", ""], ["Habermann", "Marc", ""], ["Liu", "Lingjie", ""], ["R", "Mallikarjun B", ""], ["Tewari", "Ayush", ""], ["Theobalt", "Christian", ""]]}, {"id": "2104.00365", "submitter": "Chenyou Fan", "authors": "Chenyou Fan and Jianwei Huang", "title": "Federated Few-Shot Learning with Adversarial Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We are interested in developing a unified machine learning model over many\nmobile devices for practical learning tasks, where each device only has very\nfew training data. This is a commonly encountered situation in mobile computing\nscenarios, where data is scarce and distributed while the tasks are distinct.\nIn this paper, we propose a federated few-shot learning (FedFSL) framework to\nlearn a few-shot classification model that can classify unseen data classes\nwith only a few labeled samples. With the federated learning strategy, FedFSL\ncan utilize many data sources while keeping data privacy and communication\nefficiency. There are two technical challenges: 1) directly using the existing\nfederated learning approach may lead to misaligned decision boundaries produced\nby client models, and 2) constraining the decision boundaries to be similar\nover clients would overfit to training tasks but not adapt well to unseen\ntasks. To address these issues, we propose to regularize local updates by\nminimizing the divergence of client models. We also formulate the training in\nan adversarial fashion and optimize the client models to produce a\ndiscriminative feature space that can better represent unseen data samples. We\ndemonstrate the intuitions and conduct experiments to show our approaches\noutperform baselines by more than 10% in learning vision tasks and 5% in\nlanguage tasks.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 09:44:57 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Fan", "Chenyou", ""], ["Huang", "Jianwei", ""]]}, {"id": "2104.00380", "submitter": "Song Guo", "authors": "Song Guo, Jingya Wang, Xinchao Wang, Dacheng Tao", "title": "Online Multiple Object Tracking with Cross-Task Synergy", "comments": "accepted by CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern online multiple object tracking (MOT) methods usually focus on two\ndirections to improve tracking performance. One is to predict new positions in\nan incoming frame based on tracking information from previous frames, and the\nother is to enhance data association by generating more discriminative identity\nembeddings. Some works combined both directions within one framework but\nhandled them as two individual tasks, thus gaining little mutual benefits. In\nthis paper, we propose a novel unified model with synergy between position\nprediction and embedding association. The two tasks are linked by\ntemporal-aware target attention and distractor attention, as well as\nidentity-aware memory aggregation model. Specifically, the attention modules\ncan make the prediction focus more on targets and less on distractors,\ntherefore more reliable embeddings can be extracted accordingly for\nassociation. On the other hand, such reliable embeddings can boost\nidentity-awareness through memory aggregation, hence strengthen attention\nmodules and suppress drifts. In this way, the synergy between position\nprediction and embedding association is achieved, which leads to strong\nrobustness to occlusions. Extensive experiments demonstrate the superiority of\nour proposed model over a wide range of existing methods on MOTChallenge\nbenchmarks. Our code and models are publicly available at\nhttps://github.com/songguocode/TADAM.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 10:19:40 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Guo", "Song", ""], ["Wang", "Jingya", ""], ["Wang", "Xinchao", ""], ["Tao", "Dacheng", ""]]}, {"id": "2104.00387", "submitter": "Agnese Chiatti", "authors": "Agnese Chiatti, Gianluca Bardaro, Enrico Motta, Enrico Daga", "title": "Commonsense Spatial Reasoning for Visually Intelligent Agents", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Service robots are expected to reliably make sense of complex, fast-changing\nenvironments. From a cognitive standpoint, they need the appropriate reasoning\ncapabilities and background knowledge required to exhibit human-like Visual\nIntelligence. In particular, our prior work has shown that the ability to\nreason about spatial relations between objects in the world is a key\nrequirement for the development of Visually Intelligent Agents. In this paper,\nwe present a framework for commonsense spatial reasoning which is tailored to\nreal-world robotic applications. Differently from prior approaches to\nqualitative spatial reasoning, the proposed framework is robust to variations\nin the robot's viewpoint and object orientation. The spatial relations in the\nproposed framework are also mapped to the types of commonsense predicates used\nto describe typical object configurations in English. In addition, we also show\nhow this formally-defined framework can be implemented in a concrete spatial\ndatabase.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 10:43:50 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Chiatti", "Agnese", ""], ["Bardaro", "Gianluca", ""], ["Motta", "Enrico", ""], ["Daga", "Enrico", ""]]}, {"id": "2104.00403", "submitter": "Yutao Cui", "authors": "Yutao Cui, Cheng Jiang, Limin Wang and Gangshan Wu", "title": "Target Transformed Regression for Accurate Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate tracking is still a challenging task due to appearance variations,\npose and view changes, and geometric deformations of target in videos. Recent\nanchor-free trackers provide an efficient regression mechanism but fail to\nproduce precise bounding box estimation. To address these issues, this paper\nrepurposes a Transformer-alike regression branch, termed as Target Transformed\nRegression (TREG), for accurate anchor-free tracking. The core to our TREG is\nto model pair-wise relation between elements in target template and search\nregion, and use the resulted target enhanced visual representation for accurate\nbounding box regression. This target contextualized representation is able to\nenhance the target relevant information to help precisely locate the box\nboundaries, and deal with the object deformation to some extent due to its\nlocal and dense matching mechanism. In addition, we devise a simple online\ntemplate update mechanism to select reliable templates, increasing the\nrobustness for appearance variations and geometric deformations of target in\ntime. Experimental results on visual tracking benchmarks including VOT2018,\nVOT2019, OTB100, GOT10k, NFS, UAV123, LaSOT and TrackingNet demonstrate that\nTREG obtains the state-of-the-art performance, achieving a success rate of\n0.640 on LaSOT, while running at around 30 FPS. The code and models will be\nmade available at https://github.com/MCG-NJU/TREG.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 11:25:23 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Cui", "Yutao", ""], ["Jiang", "Cheng", ""], ["Wang", "Limin", ""], ["Wu", "Gangshan", ""]]}, {"id": "2104.00405", "submitter": "Vincenzo Lomonaco PhD", "authors": "Vincenzo Lomonaco, Lorenzo Pellegrini, Andrea Cossu, Antonio Carta,\n  Gabriele Graffieti, Tyler L. Hayes, Matthias De Lange, Marc Masana, Jary\n  Pomponi, Gido van de Ven, Martin Mundt, Qi She, Keiland Cooper, Jeremy\n  Forest, Eden Belouadah, Simone Calderara, German I. Parisi, Fabio Cuzzolin,\n  Andreas Tolias, Simone Scardapane, Luca Antiga, Subutai Amhad, Adrian\n  Popescu, Christopher Kanan, Joost van de Weijer, Tinne Tuytelaars, Davide\n  Bacciu, Davide Maltoni", "title": "Avalanche: an End-to-End Library for Continual Learning", "comments": "Official Website: https://avalanche.continualai.org", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning continually from non-stationary data streams is a long-standing goal\nand a challenging problem in machine learning. Recently, we have witnessed a\nrenewed and fast-growing interest in continual learning, especially within the\ndeep learning community. However, algorithmic solutions are often difficult to\nre-implement, evaluate and port across different settings, where even results\non standard benchmarks are hard to reproduce. In this work, we propose\nAvalanche, an open-source end-to-end library for continual learning research\nbased on PyTorch. Avalanche is designed to provide a shared and collaborative\ncodebase for fast prototyping, training, and reproducible evaluation of\ncontinual learning algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 11:31:46 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Lomonaco", "Vincenzo", ""], ["Pellegrini", "Lorenzo", ""], ["Cossu", "Andrea", ""], ["Carta", "Antonio", ""], ["Graffieti", "Gabriele", ""], ["Hayes", "Tyler L.", ""], ["De Lange", "Matthias", ""], ["Masana", "Marc", ""], ["Pomponi", "Jary", ""], ["van de Ven", "Gido", ""], ["Mundt", "Martin", ""], ["She", "Qi", ""], ["Cooper", "Keiland", ""], ["Forest", "Jeremy", ""], ["Belouadah", "Eden", ""], ["Calderara", "Simone", ""], ["Parisi", "German I.", ""], ["Cuzzolin", "Fabio", ""], ["Tolias", "Andreas", ""], ["Scardapane", "Simone", ""], ["Antiga", "Luca", ""], ["Amhad", "Subutai", ""], ["Popescu", "Adrian", ""], ["Kanan", "Christopher", ""], ["van de Weijer", "Joost", ""], ["Tuytelaars", "Tinne", ""], ["Bacciu", "Davide", ""], ["Maltoni", "Davide", ""]]}, {"id": "2104.00411", "submitter": "Ashkan Khakzar", "authors": "Ashkan Khakzar, Yang Zhang, Wejdene Mansour, Yuezhi Cai, Yawei Li,\n  Yucheng Zhang, Seong Tae Kim, Nassir Navab", "title": "Explaining COVID-19 and Thoracic Pathology Model Predictions by\n  Identifying Informative Input Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks have demonstrated remarkable performance in classification\nand regression tasks on chest X-rays. In order to establish trust in the\nclinical routine, the networks' prediction mechanism needs to be interpretable.\nOne principal approach to interpretation is feature attribution. Feature\nattribution methods identify the importance of input features for the output\nprediction. Building on Information Bottleneck Attribution (IBA) method, for\neach prediction we identify the chest X-ray regions that have high mutual\ninformation with the network's output. Original IBA identifies input regions\nthat have sufficient predictive information. We propose Inverse IBA to identify\nall informative regions. Thus all predictive cues for pathologies are\nhighlighted on the X-rays, a desirable property for chest X-ray diagnosis.\nMoreover, we propose Regression IBA for explaining regression models. Using\nRegression IBA we observe that a model trained on cumulative severity score\nlabels implicitly learns the severity of different X-ray regions. Finally, we\npropose Multi-layer IBA to generate higher resolution and more detailed\nattribution/saliency maps. We evaluate our methods using both human-centric\n(ground-truth-based) interpretability metrics, and human-independent feature\nimportance metrics on NIH Chest X-ray8 and BrixIA datasets. The Code is\npublicly available.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 11:42:39 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Khakzar", "Ashkan", ""], ["Zhang", "Yang", ""], ["Mansour", "Wejdene", ""], ["Cai", "Yuezhi", ""], ["Li", "Yawei", ""], ["Zhang", "Yucheng", ""], ["Kim", "Seong Tae", ""], ["Navab", "Nassir", ""]]}, {"id": "2104.00416", "submitter": "Longguang Wang", "authors": "Longguang Wang, Yingqian Wang, Xiaoyu Dong, Qingyu Xu, Jungang Yang,\n  Wei An, Yulan Guo", "title": "Unsupervised Degradation Representation Learning for Blind\n  Super-Resolution", "comments": "Accepted by CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most existing CNN-based super-resolution (SR) methods are developed based on\nan assumption that the degradation is fixed and known (e.g., bicubic\ndownsampling). However, these methods suffer a severe performance drop when the\nreal degradation is different from their assumption. To handle various unknown\ndegradations in real-world applications, previous methods rely on degradation\nestimation to reconstruct the SR image. Nevertheless, degradation estimation\nmethods are usually time-consuming and may lead to SR failure due to large\nestimation errors. In this paper, we propose an unsupervised degradation\nrepresentation learning scheme for blind SR without explicit degradation\nestimation. Specifically, we learn abstract representations to distinguish\nvarious degradations in the representation space rather than explicit\nestimation in the pixel space. Moreover, we introduce a Degradation-Aware SR\n(DASR) network with flexible adaption to various degradations based on the\nlearned representations. It is demonstrated that our degradation representation\nlearning scheme can extract discriminative representations to obtain accurate\ndegradation information. Experiments on both synthetic and real images show\nthat our network achieves state-of-the-art performance for the blind SR task.\nCode is available at: https://github.com/LongguangWang/DASR.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 11:57:42 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Wang", "Longguang", ""], ["Wang", "Yingqian", ""], ["Dong", "Xiaoyu", ""], ["Xu", "Qingyu", ""], ["Yang", "Jungang", ""], ["An", "Wei", ""], ["Guo", "Yulan", ""]]}, {"id": "2104.00431", "submitter": "Guangming Wang", "authors": "Guangming Wang, Hesheng Wang, Yiling Liu and Weidong Chen", "title": "Unsupervised Learning of Monocular Depth and Ego-Motion Using Multiple\n  Masks", "comments": "Accepted to ICRA 2019", "journal-ref": "2019 International Conference on Robotics and Automation (ICRA).\n  IEEE, 2019, pp. 4724-4730", "doi": "10.1109/ICRA.2019.8793622", "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new unsupervised learning method of depth and ego-motion using multiple\nmasks from monocular video is proposed in this paper. The depth estimation\nnetwork and the ego-motion estimation network are trained according to the\nconstraints of depth and ego-motion without truth values. The main contribution\nof our method is to carefully consider the occlusion of the pixels generated\nwhen the adjacent frames are projected to each other, and the blank problem\ngenerated in the projection target imaging plane. Two fine masks are designed\nto solve most of the image pixel mismatch caused by the movement of the camera.\nIn addition, some relatively rare circumstances are considered, and repeated\nmasking is proposed. To some extent, the method is to use a geometric\nrelationship to filter the mismatched pixels for training, making unsupervised\nlearning more efficient and accurate. The experiments on KITTI dataset show our\nmethod achieves good performance in terms of depth and ego-motion. The\ngeneralization capability of our method is demonstrated by training on the\nlow-quality uncalibrated bike video dataset and evaluating on KITTI dataset,\nand the results are still good.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 12:29:23 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Wang", "Guangming", ""], ["Wang", "Hesheng", ""], ["Liu", "Yiling", ""], ["Chen", "Weidong", ""]]}, {"id": "2104.00432", "submitter": "Maxim Bonnaerens", "authors": "Maxim Bonnaerens, Matthias Freiberger, Joni Dambre", "title": "Anchor Pruning for Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper proposes anchor pruning for object detection in one-stage\nanchor-based detectors. While pruning techniques are widely used to reduce the\ncomputational cost of convolutional neural networks, they tend to focus on\noptimizing the backbone networks where often most computations are. In this\nwork we demonstrate an additional pruning technique, specifically for object\ndetection: anchor pruning. With more efficient backbone networks and a growing\ntrend of deploying object detectors on embedded systems where post-processing\nsteps such as non-maximum suppression can be a bottleneck, the impact of the\nanchors used in the detection head is becoming increasingly more important. In\nthis work, we show that many anchors in the object detection head can be\nremoved without any loss in accuracy. With additional retraining, anchor\npruning can even lead to improved accuracy. Extensive experiments on SSD and MS\nCOCO show that the detection head can be made up to 44% more efficient while\nsimultaneously increasing accuracy. Further experiments on RetinaNet and PASCAL\nVOC show the general effectiveness of our approach. We also introduce\n`overanchorized' models that can be used together with anchor pruning to\neliminate hyperparameters related to the initial shape of anchors.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 12:33:16 GMT"}, {"version": "v2", "created": "Tue, 20 Jul 2021 06:34:37 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Bonnaerens", "Maxim", ""], ["Freiberger", "Matthias", ""], ["Dambre", "Joni", ""]]}, {"id": "2104.00442", "submitter": "Sai Rajeswar Mudumba", "authors": "Sai Rajeswar, Cyril Ibrahim, Nitin Surya, Florian Golemo, David\n  Vazquez, Aaron Courville, Pedro O. Pinheiro", "title": "Touch-based Curiosity for Sparse-Reward Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Robots in many real-world settings have access to force/torque sensors in\ntheir gripper and tactile sensing is often necessary in tasks that involve\ncontact-rich motion. In this work, we leverage surprise from mismatches in\ntouch feedback to guide exploration in hard sparse-reward reinforcement\nlearning tasks. Our approach, Touch-based Curiosity (ToC), learns what visible\nobjects interactions are supposed to \"feel\" like. We encourage exploration by\nrewarding interactions where the expectation and the experience don't match. In\nour proposed method, an initial task-independent exploration phase is followed\nby an on-task learning phase, in which the original interactions are relabeled\nwith on-task rewards. We test our approach on a range of touch-intensive robot\narm tasks (e.g. pushing objects, opening doors), which we also release as part\nof this work. Across multiple experiments in a simulated setting, we\ndemonstrate that our method is able to learn these difficult tasks through\nsparse reward and curiosity alone. We compare our cross-modal approach to\nsingle-modality (touch- or vision-only) approaches as well as other\ncuriosity-based methods and find that our method performs better and is more\nsample-efficient.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 12:49:29 GMT"}, {"version": "v2", "created": "Sat, 26 Jun 2021 04:55:32 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Rajeswar", "Sai", ""], ["Ibrahim", "Cyril", ""], ["Surya", "Nitin", ""], ["Golemo", "Florian", ""], ["Vazquez", "David", ""], ["Courville", "Aaron", ""], ["Pinheiro", "Pedro O.", ""]]}, {"id": "2104.00447", "submitter": "Zhaoyang Lyu", "authors": "Zhaoyang Lyu, Minghao Guo, Tong Wu, Guodong Xu, Kehuan Zhang, Dahua\n  Lin", "title": "Towards Evaluating and Training Verifiably Robust Neural Networks", "comments": "Accepted to CVPR 2021 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent works have shown that interval bound propagation (IBP) can be used to\ntrain verifiably robust neural networks. Reseachers observe an intriguing\nphenomenon on these IBP trained networks: CROWN, a bounding method based on\ntight linear relaxation, often gives very loose bounds on these networks. We\nalso observe that most neurons become dead during the IBP training process,\nwhich could hurt the representation capability of the network. In this paper,\nwe study the relationship between IBP and CROWN, and prove that CROWN is always\ntighter than IBP when choosing appropriate bounding lines. We further propose a\nrelaxed version of CROWN, linear bound propagation (LBP), that can be used to\nverify large networks to obtain lower verified errors than IBP. We also design\na new activation function, parameterized ramp function (ParamRamp), which has\nmore diversity of neuron status than ReLU. We conduct extensive experiments on\nMNIST, CIFAR-10 and Tiny-ImageNet with ParamRamp activation and achieve\nstate-of-the-art verified robustness. Code and the appendix are available at\nhttps://github.com/ZhaoyangLyu/VerifiablyRobustNN.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 13:03:48 GMT"}, {"version": "v2", "created": "Mon, 5 Apr 2021 02:31:33 GMT"}, {"version": "v3", "created": "Wed, 16 Jun 2021 07:11:50 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Lyu", "Zhaoyang", ""], ["Guo", "Minghao", ""], ["Wu", "Tong", ""], ["Xu", "Guodong", ""], ["Zhang", "Kehuan", ""], ["Lin", "Dahua", ""]]}, {"id": "2104.00462", "submitter": "Tu Zheng", "authors": "Tu Zheng, Shuai Zhao, Yang Liu, Zili Liu, Deng Cai", "title": "SCALoss: Side and Corner Aligned Loss for Bounding Box Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bounding box regression is an important component in object detection. Recent\nwork has shown the promising performance by optimizing the Intersection over\nUnion (IoU) as loss. However, IoU-based loss has the gradient vanish problem in\nthe case of low overlapping bounding boxes, and the model could easily ignore\nthese simple cases. In this paper, we propose Side Overlap (SO) loss by\nmaximizing the side overlap of two bounding boxes, which puts more penalty for\nlow overlapping bounding box cases. Besides, to speed up the convergence, the\nCorner Distance (CD) is added into the objective function. Combining the Side\nOverlap and Corner Distance, we get a new regression objective function, Side\nand Corner Align Loss (SCALoss). The SCALoss is well-correlated with IoU loss,\nwhich also benefits the evaluation metric but produces more penalty for\nlow-overlapping cases. It can serve as a comprehensive similarity measure,\nleading the better localization performance and faster convergence speed.\nExperiments on COCO and PASCAL VOC benchmarks show that SCALoss can bring\nconsistent improvement and outperform $\\ell_n$ loss and IoU based loss with\npopular object detectors such as YOLOV3, SSD, Reppoints, Faster-RCNN.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 13:46:35 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Zheng", "Tu", ""], ["Zhao", "Shuai", ""], ["Liu", "Yang", ""], ["Liu", "Zili", ""], ["Cai", "Deng", ""]]}, {"id": "2104.00464", "submitter": "Roy Ganz", "authors": "Roy Ganz and Michael Elad", "title": "Improved Image Generation via Sparse Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The interest of the deep learning community in image synthesis has grown\nmassively in recent years. Nowadays, deep generative methods, and especially\nGenerative Adversarial Networks (GANs), are leading to state-of-the-art\nperformance, capable of synthesizing images that appear realistic. While the\nefforts for improving the quality of the generated images are extensive, most\nattempts still consider the generator part as an uncorroborated \"black-box\". In\nthis paper, we aim to provide a better understanding and design of the image\ngeneration process. We interpret existing generators as implicitly relying on\nsparsity-inspired models. More specifically, we show that generators can be\nviewed as manifestations of the Convolutional Sparse Coding (CSC) and its\nMulti-Layered version (ML-CSC) synthesis processes. We leverage this\nobservation by explicitly enforcing a sparsifying regularization on\nappropriately chosen activation layers in the generator, and demonstrate that\nthis leads to improved image synthesis. Furthermore, we show that the same\nrationale and benefits apply to generators serving inverse problems,\ndemonstrated on the Deep Image Prior (DIP) method.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 13:52:40 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Ganz", "Roy", ""], ["Elad", "Michael", ""]]}, {"id": "2104.00466", "submitter": "Zhisheng Zhong", "authors": "Zhisheng Zhong, Jiequan Cui, Shu Liu, Jiaya Jia", "title": "Improving Calibration for Long-Tailed Recognition", "comments": "Accepted by CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks may perform poorly when training datasets are heavily\nclass-imbalanced. Recently, two-stage methods decouple representation learning\nand classifier learning to improve performance. But there is still the vital\nissue of miscalibration. To address it, we design two methods to improve\ncalibration and performance in such scenarios. Motivated by the fact that\npredicted probability distributions of classes are highly related to the\nnumbers of class instances, we propose label-aware smoothing to deal with\ndifferent degrees of over-confidence for classes and improve classifier\nlearning. For dataset bias between these two stages due to different samplers,\nwe further propose shifted batch normalization in the decoupling framework. Our\nproposed methods set new records on multiple popular long-tailed recognition\nbenchmark datasets, including CIFAR-10-LT, CIFAR-100-LT, ImageNet-LT,\nPlaces-LT, and iNaturalist 2018. Code will be available at\nhttps://github.com/Jia-Research-Lab/MiSLAS.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 13:55:21 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Zhong", "Zhisheng", ""], ["Cui", "Jiequan", ""], ["Liu", "Shu", ""], ["Jia", "Jiaya", ""]]}, {"id": "2104.00476", "submitter": "Jan Bechtold", "authors": "Jan Bechtold, Maxim Tatarchenko, Volker Fischer, Thomas Brox", "title": "Fostering Generalization in Single-view 3D Reconstruction by Learning a\n  Hierarchy of Local and Global Shape Priors", "comments": "Accepted at CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single-view 3D object reconstruction has seen much progress, yet methods\nstill struggle generalizing to novel shapes unseen during training. Common\napproaches predominantly rely on learned global shape priors and, hence,\ndisregard detailed local observations. In this work, we address this issue by\nlearning a hierarchy of priors at different levels of locality from ground\ntruth input depth maps. We argue that exploiting local priors allows our method\nto efficiently use input observations, thus improving generalization in visible\nareas of novel shapes. At the same time, the combination of local and global\npriors enables meaningful hallucination of unobserved parts resulting in\nconsistent 3D shapes. We show that the hierarchical approach generalizes much\nbetter than the global approach. It generalizes not only between different\ninstances of a class but also across classes and to unseen arrangements of\nobjects.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 14:04:48 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Bechtold", "Jan", ""], ["Tatarchenko", "Maxim", ""], ["Fischer", "Volker", ""], ["Brox", "Thomas", ""]]}, {"id": "2104.00482", "submitter": "Benoit Guillard", "authors": "Benoit Guillard and Edoardo Remelli and Pierre Yvernay and Pascal Fua", "title": "Sketch2Mesh: Reconstructing and Editing 3D Shapes from Sketches", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reconstructing 3D shape from 2D sketches has long been an open problem\nbecause the sketches only provide very sparse and ambiguous information. In\nthis paper, we use an encoder/decoder architecture for the sketch to mesh\ntranslation. This enables us to leverage its latent parametrization to\nrepresent and refine a 3D mesh so that its projections match the external\ncontours outlined in the sketch. We will show that this approach is easy to\ndeploy, robust to style changes, and effective. Furthermore, it can be used for\nshape refinement given only single pen strokes. We compare our approach to\nstate-of-the-art methods on sketches -- both hand-drawn and synthesized -- and\ndemonstrate that we outperform them.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 14:10:59 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Guillard", "Benoit", ""], ["Remelli", "Edoardo", ""], ["Yvernay", "Pierre", ""], ["Fua", "Pascal", ""]]}, {"id": "2104.00483", "submitter": "Yu Yang", "authors": "Yu Yang, Hakan Bilen, Qiran Zou, Wing Yin Cheung, Xiangyang Ji", "title": "Unsupervised Foreground-Background Segmentation with Equivariant Layered\n  GANs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose an unsupervised foreground-background segmentation method via\ntraining a segmentation network on the synthetic pseudo segmentation dataset\ngenerated from GANs, which are trained from a collection of images without\nannotations to explicitly disentangle foreground and background. To efficiently\ngenerate foreground and background layers and overlay them to compose novel\nimages, the construction of such GANs is fulfilled by our proposed Equivariant\nLayered GAN, whose improvement, compared to the precedented layered GAN, is\nembodied in the following two aspects. (1) The disentanglement of foreground\nand background is improved by extending the previous perturbation strategy and\nintroducing private code recovery that reconstructs the private code of\nforeground from the composite image. (2) The latent space of the layered GANs\nis regularized by minimizing our proposed equivariance loss, resulting in\ninterpretable latent codes and better disentanglement of foreground and\nbackground. Our methods are evaluated on unsupervised object segmentation\ndatasets including Caltech-UCSD Birds and LSUN Car, achieving state-of-the-art\nperformance.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 14:13:25 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Yang", "Yu", ""], ["Bilen", "Hakan", ""], ["Zou", "Qiran", ""], ["Cheung", "Wing Yin", ""], ["Ji", "Xiangyang", ""]]}, {"id": "2104.00484", "submitter": "Longwen Zhang", "authors": "Longwen Zhang, Qixuan Zhang, Minye Wu, Jingyi Yu, Lan Xu", "title": "Neural Video Portrait Relighting in Real-time via Consistency Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Video portraits relighting is critical in user-facing human photography,\nespecially for immersive VR/AR experience. Recent advances still fail to\nrecover consistent relit result under dynamic illuminations from monocular RGB\nstream, suffering from the lack of video consistency supervision. In this\npaper, we propose a neural approach for real-time, high-quality and coherent\nvideo portrait relighting, which jointly models the semantic, temporal and\nlighting consistency using a new dynamic OLAT dataset. We propose a hybrid\nstructure and lighting disentanglement in an encoder-decoder architecture,\nwhich combines a multi-task and adversarial training strategy for\nsemantic-aware consistency modeling. We adopt a temporal modeling scheme via\nflow-based supervision to encode the conjugated temporal consistency in a cross\nmanner. We also propose a lighting sampling strategy to model the illumination\nconsistency and mutation for natural portrait light manipulation in real-world.\nExtensive experiments demonstrate the effectiveness of our approach for\nconsistent video portrait light-editing and relighting, even using mobile\ncomputing.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 14:13:28 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Zhang", "Longwen", ""], ["Zhang", "Qixuan", ""], ["Wu", "Minye", ""], ["Yu", "Jingyi", ""], ["Xu", "Lan", ""]]}, {"id": "2104.00487", "submitter": "Jianjin Xu", "authors": "Jianjin Xu, Changxi Zheng", "title": "Linear Semantics in Generative Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Generative Adversarial Networks (GANs) are able to generate high-quality\nimages, but it remains difficult to explicitly specify the semantics of\nsynthesized images. In this work, we aim to better understand the semantic\nrepresentation of GANs, and thereby enable semantic control in GAN's generation\nprocess. Interestingly, we find that a well-trained GAN encodes image semantics\nin its internal feature maps in a surprisingly simple way: a linear\ntransformation of feature maps suffices to extract the generated image\nsemantics. To verify this simplicity, we conduct extensive experiments on\nvarious GANs and datasets; and thanks to this simplicity, we are able to learn\na semantic segmentation model for a trained GAN from a small number (e.g., 8)\nof labeled images. Last but not least, leveraging our findings, we propose two\nfew-shot image editing approaches, namely Semantic-Conditional Sampling and\nSemantic Image Editing. Given a trained GAN and as few as eight semantic\nannotations, the user is able to generate diverse images subject to a\nuser-provided semantic layout, and control the synthesized image semantics. We\nhave made the code publicly available.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 14:18:48 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Xu", "Jianjin", ""], ["Zheng", "Changxi", ""]]}, {"id": "2104.00492", "submitter": "Yiye Chen", "authors": "Yiye Chen, Ruinian Xu, Yunzhi Lin, and Patricio A. Vela", "title": "A Joint Network for Grasp Detection Conditioned on Natural Language\n  Commands", "comments": "7 pages, 2 figures, Accepted to the ICRA2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the task of grasping a target object based on a natural language\ncommand query. Previous work primarily focused on localizing the object given\nthe query, which requires a separate grasp detection module to grasp it. The\ncascaded application of two pipelines incurs errors in overlapping multi-object\ncases due to ambiguity in the individual outputs. This work proposes a model\nnamed Command Grasping Network(CGNet) to directly output command satisficing\ngrasps from RGB image and textual command inputs. A dataset with ground truth\n(image, command, grasps) tuple is generated based on the VMRD dataset to train\nthe proposed network. Experimental results on the generated test set show that\nCGNet outperforms a cascaded object-retrieval and grasp detection baseline by a\nlarge margin. Three physical experiments demonstrate the functionality and\nperformance of CGNet.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 14:26:20 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Chen", "Yiye", ""], ["Xu", "Ruinian", ""], ["Lin", "Yunzhi", ""], ["Vela", "Patricio A.", ""]]}, {"id": "2104.00528", "submitter": "Alexander Wong", "authors": "Saad Abbasi, Mahmoud Famouri, Mohammad Javad Shafiee, and Alexander\n  Wong", "title": "OutlierNets: Highly Compact Deep Autoencoder Network Architectures for\n  On-Device Acoustic Anomaly Detection", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human operators often diagnose industrial machinery via anomalous sounds.\nAutomated acoustic anomaly detection can lead to reliable maintenance of\nmachinery. However, deep learning-driven anomaly detection methods often\nrequire an extensive amount of computational resources which prohibits their\ndeployment in factories. Here we explore a machine-driven design exploration\nstrategy to create OutlierNets, a family of highly compact deep convolutional\nautoencoder network architectures featuring as few as 686 parameters, model\nsizes as small as 2.7 KB, and as low as 2.8 million FLOPs, with a detection\naccuracy matching or exceeding published architectures with as many as 4\nmillion parameters. Furthermore, CPU-accelerated latency experiments show that\nthe OutlierNet architectures can achieve as much as 21x lower latency than\npublished networks.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 04:09:30 GMT"}, {"version": "v2", "created": "Mon, 19 Apr 2021 03:25:50 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Abbasi", "Saad", ""], ["Famouri", "Mahmoud", ""], ["Shafiee", "Mohammad Javad", ""], ["Wong", "Alexander", ""]]}, {"id": "2104.00531", "submitter": "Reza Pourreza", "authors": "Reza Pourreza and Taco S Cohen", "title": "Extending Neural P-frame Codecs for B-frame Coding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  While most neural video codecs address P-frame coding (predicting each frame\nfrom past ones), in this paper we address B-frame compression (predicting\nframes using both past and future reference frames). Our B-frame solution is\nbased on the existing P-frame methods. As a result, B-frame coding capability\ncan easily be added to an existing neural codec. The basic idea of our B-frame\ncoding method is to interpolate the two reference frames to generate a single\nreference frame and then use it together with an existing P-frame codec to\nencode the input B-frame. Our studies show that the interpolated frame is a\nmuch better reference for the P-frame codec compared to using the previous\nframe as is usually done. Our results show that using the proposed method with\nan existing P-frame codec can lead to 28.5%saving in bit-rate on the UVG\ndataset compared to the P-frame codec while generating the same video quality.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 21:25:35 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Pourreza", "Reza", ""], ["Cohen", "Taco S", ""]]}, {"id": "2104.00556", "submitter": "Yiran Zhong", "authors": "Jianyuan Wang, Yiran Zhong, Yuchao Dai, Stan Birchfield, Kaihao Zhang,\n  Nikolai Smolyanskiy, Hongdong Li", "title": "Deep Two-View Structure-from-Motion Revisited", "comments": "Accepted at CVPR 2021; Yiran Zhong and Jianyuan Wang contribute\n  equally to this work and the name listed in alphabetical order", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two-view structure-from-motion (SfM) is the cornerstone of 3D reconstruction\nand visual SLAM. Existing deep learning-based approaches formulate the problem\nby either recovering absolute pose scales from two consecutive frames or\npredicting a depth map from a single image, both of which are ill-posed\nproblems. In contrast, we propose to revisit the problem of deep two-view SfM\nby leveraging the well-posedness of the classic pipeline. Our method consists\nof 1) an optical flow estimation network that predicts dense correspondences\nbetween two frames; 2) a normalized pose estimation module that computes\nrelative camera poses from the 2D optical flow correspondences, and 3) a\nscale-invariant depth estimation network that leverages epipolar geometry to\nreduce the search space, refine the dense correspondences, and estimate\nrelative depth maps. Extensive experiments show that our method outperforms all\nstate-of-the-art two-view SfM methods by a clear margin on KITTI depth, KITTI\nVO, MVS, Scenes11, and SUN3D datasets in both relative pose and depth\nestimation.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 15:31:20 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Wang", "Jianyuan", ""], ["Zhong", "Yiran", ""], ["Dai", "Yuchao", ""], ["Birchfield", "Stan", ""], ["Zhang", "Kaihao", ""], ["Smolyanskiy", "Nikolai", ""], ["Li", "Hongdong", ""]]}, {"id": "2104.00562", "submitter": "Yihao Zhang", "authors": "Yihao Zhang and John J. Leonard", "title": "A Front-End for Dense Monocular SLAM using a Learned Outlier Mask Prior", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent achievements in depth prediction from a single RGB image have powered\nthe new research area of combining convolutional neural networks (CNNs) with\nclassical simultaneous localization and mapping (SLAM) algorithms. The depth\nprediction from a CNN provides a reasonable initial point in the optimization\nprocess in the traditional SLAM algorithms, while the SLAM algorithms further\nimprove the CNN prediction online. However, most of the current CNN-SLAM\napproaches have only taken advantage of the depth prediction but not yet other\nproducts from a CNN. In this work, we explore the use of the outlier mask, a\nby-product from unsupervised learning of depth from video, as a prior in a\nclassical probability model for depth estimate fusion to step up the\noutlier-resistant tracking performance of a SLAM front-end. On the other hand,\nsome of the previous CNN-SLAM work builds on feature-based sparse SLAM methods,\nwasting the per-pixel dense prediction from a CNN. In contrast to these sparse\nmethods, we devise a dense CNN-assisted SLAM front-end that is implementable\nwith TensorFlow and evaluate it on both indoor and outdoor datasets.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 15:43:28 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Zhang", "Yihao", ""], ["Leonard", "John J.", ""]]}, {"id": "2104.00563", "submitter": "Roger Girgis", "authors": "Roger Girgis, Florian Golemo, Felipe Codevilla, Jim Aldon D'Souza,\n  Martin Weiss, Samira Ebrahimi Kahou, Felix Heide, Christopher Pal", "title": "Autobots: Latent Variable Sequential Set Transformers", "comments": "21 pages, 15 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG cs.MA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Robust multi-agent trajectory prediction is essential for the safe control of\nrobots and vehicles that interact with humans. Many existing methods treat\nsocial and temporal information separately and therefore fall short of\nmodelling the joint future trajectories of all agents in a socially consistent\nway. To address this, we propose a new class of Latent Variable Sequential Set\nTransformers which autoregressively model multi-agent trajectories. We refer to\nthese architectures as \"AutoBots\". AutoBots model the contents of sets (e.g.\nrepresenting the properties of agents in a scene) over time and employ\nmulti-head self-attention blocks over these sequences of sets to encode the\nsociotemporal relationships between the different actors of a scene. This\nproduces either the trajectory of one ego-agent or a distribution over the\nfuture trajectories for all agents under consideration. Our approach works for\ngeneral sequences of sets and we provide illustrative experiments modelling the\nsequential structure of the multiple strokes that make up symbols in the\nOmniglot data. For the single-agent prediction case, we validate our model on\nthe NuScenes motion prediction task and achieve competitive results on the\nglobal leaderboard. In the multi-agent forecasting setting, we validate our\nmodel on TrajNet. We find that our method outperforms physical extrapolation\nand recurrent network baselines and generates scene-consistent trajectories.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2021 18:53:26 GMT"}, {"version": "v2", "created": "Wed, 16 Jun 2021 19:06:13 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Girgis", "Roger", ""], ["Golemo", "Florian", ""], ["Codevilla", "Felipe", ""], ["D'Souza", "Jim Aldon", ""], ["Weiss", "Martin", ""], ["Kahou", "Samira Ebrahimi", ""], ["Heide", "Felix", ""], ["Pal", "Christopher", ""]]}, {"id": "2104.00564", "submitter": "Mauro Martini", "authors": "Mauro Martini, Vittorio Mazzia, Aleem Khaliq, Marcello Chiaberge", "title": "Domain-Adversarial Training of Self-Attention Based Networks for Land\n  Cover Classification using Multi-temporal Sentinel-2 Satellite Imagery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The increasing availability of large-scale remote sensing labeled data has\nprompted researchers to develop increasingly precise and accurate data-driven\nmodels for land cover and crop classification (LC&CC). Moreover, with the\nintroduction of self-attention and introspection mechanisms, deep learning\napproaches have shown promising results in processing long temporal sequences\nin the multi-spectral domain with a contained computational request.\nNevertheless, most practical applications cannot rely on labeled data, and in\nthe field, surveys are a time consuming solution that poses strict limitations\nto the number of collected samples. Moreover, atmospheric conditions and\nspecific geographical region characteristics constitute a relevant domain gap\nthat does not allow direct applicability of a trained model on the available\ndataset to the area of interest. In this paper, we investigate adversarial\ntraining of deep neural networks to bridge the domain discrepancy between\ndistinct geographical zones. In particular, we perform a thorough analysis of\ndomain adaptation applied to challenging multi-spectral, multi-temporal data,\naccurately highlighting the advantages of adapting state-of-the-art\nself-attention based models for LC&CC to different target zones where labeled\ndata are not available. Extensive experimentation demonstrated significant\nperformance and generalization gain in applying domain-adversarial training to\nsource and target regions with marked dissimilarities between the distribution\nof extracted features.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 15:45:17 GMT"}, {"version": "v2", "created": "Wed, 30 Jun 2021 14:30:42 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Martini", "Mauro", ""], ["Mazzia", "Vittorio", ""], ["Khaliq", "Aleem", ""], ["Chiaberge", "Marcello", ""]]}, {"id": "2104.00567", "submitter": "Wentong Liao", "authors": "Kai Hu, Wentong Liao, Michael Ying Yang, Bodo Rosenhahn", "title": "Text to Image Generation with Semantic-Spatial Aware GAN", "comments": "code available", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A text to image generation (T2I) model aims to generate photo-realistic\nimages which are semantically consistent with the text descriptions. Built upon\nthe recent advances in generative adversarial networks (GANs), existing T2I\nmodels have made great progress. However, a close inspection of their generated\nimages reveals two major limitations: (1) The condition batch normalization\nmethods are applied on the whole image feature maps equally, ignoring the local\nsemantics; (2) The text encoder is fixed during training, which should be\ntrained with the image generator jointly to learn better text representations\nfor image generation. To address these limitations, we propose a novel\nframework Semantic-Spatial Aware GAN, which is trained in an end-to-end fashion\nso that the text encoder can exploit better text information. Concretely, we\nintroduce a novel Semantic-Spatial Aware Convolution Network, which (1) learns\nsemantic-adaptive transformation conditioned on text to effectively fuse text\nfeatures and image features, and (2) learns a mask map in a weakly-supervised\nway that depends on the current text-image fusion process in order to guide the\ntransformation spatially. Experiments on the challenging COCO and CUB bird\ndatasets demonstrate the advantage of our method over the recent\nstate-of-the-art approaches, regarding both visual fidelity and alignment with\ninput text description. Code is available at\nhttps://github.com/wtliao/text2image.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 15:48:01 GMT"}, {"version": "v2", "created": "Wed, 14 Apr 2021 14:36:40 GMT"}, {"version": "v3", "created": "Sat, 24 Apr 2021 20:24:38 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Hu", "Kai", ""], ["Liao", "Wentong", ""], ["Yang", "Michael Ying", ""], ["Rosenhahn", "Bodo", ""]]}, {"id": "2104.00568", "submitter": "Fu-En Wang", "authors": "Fu-En Wang, Yu-Hsuan Yeh, Min Sun, Wei-Chen Chiu, Yi-Hsuan Tsai", "title": "LED2-Net: Monocular 360 Layout Estimation via Differentiable Depth\n  Rendering", "comments": "CVPR 2021 Oral, see https://fuenwang.ml/project/led2net", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although significant progress has been made in room layout estimation, most\nmethods aim to reduce the loss in the 2D pixel coordinate rather than\nexploiting the room structure in the 3D space. Towards reconstructing the room\nlayout in 3D, we formulate the task of 360 layout estimation as a problem of\npredicting depth on the horizon line of a panorama. Specifically, we propose\nthe Differentiable Depth Rendering procedure to make the conversion from layout\nto depth prediction differentiable, thus making our proposed model end-to-end\ntrainable while leveraging the 3D geometric information, without the need of\nproviding the ground truth depth. Our method achieves state-of-the-art\nperformance on numerous 360 layout benchmark datasets. Moreover, our\nformulation enables a pre-training step on the depth dataset, which further\nimproves the generalizability of our layout estimation model.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 15:48:41 GMT"}, {"version": "v2", "created": "Sat, 3 Apr 2021 18:28:13 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Wang", "Fu-En", ""], ["Yeh", "Yu-Hsuan", ""], ["Sun", "Min", ""], ["Chiu", "Wei-Chen", ""], ["Tsai", "Yi-Hsuan", ""]]}, {"id": "2104.00597", "submitter": "Minghao Chen", "authors": "Minghao Chen, Houwen Peng, Jianlong Fu, Haibin Ling", "title": "One-Shot Neural Ensemble Architecture Search by Diversity-Guided Search\n  Space Shrinking", "comments": "Accepted to CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite remarkable progress achieved, most neural architecture search (NAS)\nmethods focus on searching for one single accurate and robust architecture. To\nfurther build models with better generalization capability and performance,\nmodel ensemble is usually adopted and performs better than stand-alone models.\nInspired by the merits of model ensemble, we propose to search for multiple\ndiverse models simultaneously as an alternative way to find powerful models.\nSearching for ensembles is non-trivial and has two key challenges: enlarged\nsearch space and potentially more complexity for the searched model. In this\npaper, we propose a one-shot neural ensemble architecture search (NEAS)\nsolution that addresses the two challenges. For the first challenge, we\nintroduce a novel diversity-based metric to guide search space shrinking,\nconsidering both the potentiality and diversity of candidate operators. For the\nsecond challenge, we enable a new search dimension to learn layer sharing among\ndifferent models for efficiency purposes. The experiments on ImageNet clearly\ndemonstrate that our solution can improve the supernet's capacity of ranking\nensemble architectures, and further lead to better search results. The\ndiscovered architectures achieve superior performance compared with\nstate-of-the-arts such as MobileNetV3 and EfficientNet families under aligned\nsettings. Moreover, we evaluate the generalization ability and robustness of\nour searched architecture on the COCO detection benchmark and achieve a 3.1%\nimprovement on AP compared with MobileNetV3. Codes and models are available at\nhttps://github.com/researchmm/NEAS.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 16:29:49 GMT"}, {"version": "v2", "created": "Fri, 16 Jul 2021 17:17:16 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Chen", "Minghao", ""], ["Peng", "Houwen", ""], ["Fu", "Jianlong", ""], ["Ling", "Haibin", ""]]}, {"id": "2104.00613", "submitter": "Vighnesh Birodkar", "authors": "Vighnesh Birodkar, Zhichao Lu, Siyang Li, Vivek Rathod, Jonathan Huang", "title": "The surprising impact of mask-head architecture on novel class\n  segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Instance segmentation models today are very accurate when trained on large\nannotated datasets, but collecting mask annotations at scale is prohibitively\nexpensive. We address the partially supervised instance segmentation problem in\nwhich one can train on (significantly cheaper) bounding boxes for all\ncategories but use masks only for a subset of categories. In this work, we\nfocus on a popular family of models which apply differentiable cropping to a\nfeature map and predict a mask based on the resulting crop. Within this family,\nwe show that the architecture of the mask-head plays a surprisingly important\nrole in generalization to classes for which we do not observe masks during\ntraining. While many architectures perform similarly when trained in fully\nsupervised mode, we show that they often generalize to novel classes in\ndramatically different ways. We call this phenomenon the strong mask\ngeneralization effect, which we exploit by replacing the typical mask-head of\n2-4 layers with significantly deeper off-the-shelf architectures (e.g. ResNet,\nHourglass models). We also show that the choice of mask-head architecture alone\ncan lead to SOTA results on the partially supervised COCO benchmark without the\nneed of specialty modules or losses proposed by prior literature. Finally, we\ndemonstrate that our effect is general, holding across underlying detection\nmethodologies, (e.g. both anchor-based or anchor free or no detector at all)\nand across different backbone networks. Code and pre-trained models are\navailable at https://git.io/deepmac.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 16:46:37 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Birodkar", "Vighnesh", ""], ["Lu", "Zhichao", ""], ["Li", "Siyang", ""], ["Rathod", "Vivek", ""], ["Huang", "Jonathan", ""]]}, {"id": "2104.00616", "submitter": "Arsha Nagrani", "authors": "Chen Sun, Arsha Nagrani, Yonglong Tian and Cordelia Schmid", "title": "Composable Augmentation Encoding for Video Representation Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We focus on contrastive methods for self-supervised video representation\nlearning. A common paradigm in contrastive learning is to construct positive\npairs by sampling different data views for the same instance, with different\ndata instances as negatives. These methods implicitly assume a set of\nrepresentational invariances to the view selection mechanism (eg, sampling\nframes with temporal shifts), which may lead to poor performance on downstream\ntasks which violate these invariances (fine-grained video action recognition\nthat would benefit from temporal information). To overcome this limitation, we\npropose an 'augmentation aware' contrastive learning framework, where we\nexplicitly provide a sequence of augmentation parameterisations (such as the\nvalues of the time shifts used to create data views) as composable augmentation\nencodings (CATE) to our model when projecting the video representations for\ncontrastive learning. We show that representations learned by our method encode\nvaluable information about specified spatial or temporal augmentation, and in\ndoing so also achieve state-of-the-art performance on a number of video\nbenchmarks.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 16:48:53 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Sun", "Chen", ""], ["Nagrani", "Arsha", ""], ["Tian", "Yonglong", ""], ["Schmid", "Cordelia", ""]]}, {"id": "2104.00619", "submitter": "Xiao Lin", "authors": "Xiao Lin, Meng Ye, Yunye Gong, Giedrius Buracas, Nikoletta Basiou,\n  Ajay Divakaran, Yi Yao", "title": "Modular Adaptation for Cross-Domain Few-Shot Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adapting pre-trained representations has become the go-to recipe for learning\nnew downstream tasks with limited examples. While literature has demonstrated\ngreat successes via representation learning, in this work, we show that\nsubstantial performance improvement of downstream tasks can also be achieved by\nappropriate designs of the adaptation process. Specifically, we propose a\nmodular adaptation method that selectively performs multiple state-of-the-art\n(SOTA) adaptation methods in sequence. As different downstream tasks may\nrequire different types of adaptation, our modular adaptation enables the\ndynamic configuration of the most suitable modules based on the downstream\ntask. Moreover, as an extension to existing cross-domain 5-way k-shot\nbenchmarks (e.g., miniImageNet -> CUB), we create a new high-way (~100) k-shot\nbenchmark with data from 10 different datasets. This benchmark provides a\ndiverse set of domains and allows the use of stronger representations learned\nfrom ImageNet. Experimental results show that by customizing adaptation process\ntowards downstream tasks, our modular adaptation pipeline (MAP) improves 3.1%\nin 5-shot classification accuracy over baselines of finetuning and Prototypical\nNetworks.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 16:50:43 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Lin", "Xiao", ""], ["Ye", "Meng", ""], ["Gong", "Yunye", ""], ["Buracas", "Giedrius", ""], ["Basiou", "Nikoletta", ""], ["Divakaran", "Ajay", ""], ["Yao", "Yi", ""]]}, {"id": "2104.00622", "submitter": "Luyang Zhu", "authors": "Luyang Zhu, Arsalan Mousavian, Yu Xiang, Hammad Mazhar, Jozef van\n  Eenbergen, Shoubhik Debnath, Dieter Fox", "title": "RGB-D Local Implicit Function for Depth Completion of Transparent\n  Objects", "comments": "CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Majority of the perception methods in robotics require depth information\nprovided by RGB-D cameras. However, standard 3D sensors fail to capture depth\nof transparent objects due to refraction and absorption of light. In this\npaper, we introduce a new approach for depth completion of transparent objects\nfrom a single RGB-D image. Key to our approach is a local implicit neural\nrepresentation built on ray-voxel pairs that allows our method to generalize to\nunseen objects and achieve fast inference speed. Based on this representation,\nwe present a novel framework that can complete missing depth given noisy RGB-D\ninput. We further improve the depth estimation iteratively using a\nself-correcting refinement model. To train the whole pipeline, we build a large\nscale synthetic dataset with transparent objects. Experiments demonstrate that\nour method performs significantly better than the current state-of-the-art\nmethods on both synthetic and real world data. In addition, our approach\nimproves the inference speed by a factor of 20 compared to the previous best\nmethod, ClearGrasp. Code and dataset will be released at\nhttps://research.nvidia.com/publication/2021-03_RGB-D-Local-Implicit.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 17:00:04 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Zhu", "Luyang", ""], ["Mousavian", "Arsalan", ""], ["Xiang", "Yu", ""], ["Mazhar", "Hammad", ""], ["van Eenbergen", "Jozef", ""], ["Debnath", "Shoubhik", ""], ["Fox", "Dieter", ""]]}, {"id": "2104.00633", "submitter": "Shun Iwase", "authors": "Shun Iwase, Xingyu Liu, Rawal Khirodkar, Rio Yokota, Kris M. Kitani", "title": "RePOSE: Real-Time Iterative Rendering and Refinement for 6D Object Pose\n  Estimation", "comments": "8 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The use of iterative pose refinement is a critical processing step for 6D\nobject pose estimation, and its performance depends greatly on one's choice of\nimage representation. Image representations learned via deep convolutional\nneural networks (CNN) are currently the method of choice as they are able to\nrobustly encode object keypoint locations. However, CNN-based image\nrepresentations are computational expensive to use for iterative pose\nrefinement, as they require that image features are extracted using a deep\nnetwork, once for the input image and multiple times for rendered images during\nthe refinement process. Instead of using a CNN to extract image features from a\nrendered RGB image, we propose to directly render a deep feature image. We call\nthis deep texture rendering, where a shallow multi-layer perceptron is used to\ndirectly regress a view invariant image representation of an object. Using an\nestimate of the pose and deep texture rendering, our system can render an image\nrepresentation in under 1ms. This image representation is optimized such that\nit makes it easier to perform nonlinear 6D pose estimation by adding a\ndifferentiable Levenberg-Marquardt optimization network and back-propagating\nthe 6D pose alignment error. We call our method, RePOSE, a Real-time Iterative\nRendering and Refinement algorithm for 6D POSE estimation. RePOSE runs at 71\nFPS and achieves state-of-the-art accuracy of 51.6% on the Occlusion LineMOD\ndataset - a 4.1% absolute improvement over the prior art, and comparable\nperformance on the YCB-Video dataset with a much faster runtime than the other\npose refinement methods.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 17:26:54 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Iwase", "Shun", ""], ["Liu", "Xingyu", ""], ["Khirodkar", "Rawal", ""], ["Yokota", "Rio", ""], ["Kitani", "Kris M.", ""]]}, {"id": "2104.00646", "submitter": "Tae Soo Kim", "authors": "Tae Soo Kim, Jonathan Jones, Gregory D. Hager", "title": "Motion Guided Attention Fusion to Recognize Interactions from Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a dual-pathway approach for recognizing fine-grained interactions\nfrom videos. We build on the success of prior dual-stream approaches, but make\na distinction between the static and dynamic representations of objects and\ntheir interactions explicit by introducing separate motion and object detection\npathways. Then, using our new Motion-Guided Attention Fusion module, we fuse\nthe bottom-up features in the motion pathway with features captured from object\ndetections to learn the temporal aspects of an action. We show that our\napproach can generalize across appearance effectively and recognize actions\nwhere an actor interacts with previously unseen objects. We validate our\napproach using the compositional action recognition task from the\nSomething-Something-v2 dataset where we outperform existing state-of-the-art\nmethods. We also show that our method can generalize well to real world tasks\nby showing state-of-the-art performance on recognizing humans assembling\nvarious IKEA furniture on the IKEA-ASM dataset.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 17:44:34 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Kim", "Tae Soo", ""], ["Jones", "Jonathan", ""], ["Hager", "Gregory D.", ""]]}, {"id": "2104.00650", "submitter": "Max Bain", "authors": "Max Bain, Arsha Nagrani, G\\\"ul Varol, Andrew Zisserman", "title": "Frozen in Time: A Joint Video and Image Encoder for End-to-End Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Our objective in this work is video-text retrieval - in particular a joint\nembedding that enables efficient text-to-video retrieval. The challenges in\nthis area include the design of the visual architecture and the nature of the\ntraining data, in that the available large scale video-text training datasets,\nsuch as HowTo100M, are noisy and hence competitive performance is achieved only\nat scale through large amounts of compute. We address both these challenges in\nthis paper. We propose an end-to-end trainable model that is designed to take\nadvantage of both large-scale image and video captioning datasets. Our model is\nan adaptation and extension of the recent ViT and Timesformer architectures,\nand consists of attention in both space and time. The model is flexible and can\nbe trained on both image and video text datasets, either independently or in\nconjunction. It is trained with a curriculum learning schedule that begins by\ntreating images as 'frozen' snapshots of video, and then gradually learns to\nattend to increasing temporal context when trained on video datasets. We also\nprovide a new video-text pretraining dataset WebVid-2M, comprised of over two\nmillion videos with weak captions scraped from the internet. Despite training\non datasets that are an order of magnitude smaller, we show that this approach\nyields state-of-the-art results on standard downstream video-retrieval\nbenchmarks including MSR-VTT, MSVD, DiDeMo and LSMDC.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 17:48:27 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Bain", "Max", ""], ["Nagrani", "Arsha", ""], ["Varol", "G\u00fcl", ""], ["Zisserman", "Andrew", ""]]}, {"id": "2104.00669", "submitter": "Nima Hatami", "authors": "Nima Hatami and Mohsin Bilal and Nasir Rajpoot", "title": "Deep Multi-Resolution Dictionary Learning for Histopathology Image\n  Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The problem of recognizing various types of tissues present in\nmulti-gigapixel histology images is an important fundamental pre-requisite for\ndownstream analysis of the tumor microenvironment in a bottom-up analysis\nparadigm for computational pathology. In this paper, we propose a deep\ndictionary learning approach to solve the problem of tissue phenotyping in\nhistology images. We propose deep Multi-Resolution Dictionary Learning\n(deepMRDL) in order to benefit from deep texture descriptors at multiple\ndifferent spatial resolutions. We show the efficacy of the proposed approach\nthrough extensive experiments on four benchmark histology image datasets from\ndifferent organs (colorectal cancer, breast cancer and breast lymphnodes) and\ntasks (namely, cancer grading, tissue phenotyping, tumor detection and tissue\ntype classification). We also show that the proposed framework can employ most\noff-the-shelf CNNs models to generate effective deep texture descriptors.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 17:58:18 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Hatami", "Nima", ""], ["Bilal", "Mohsin", ""], ["Rajpoot", "Nasir", ""]]}, {"id": "2104.00670", "submitter": "Miguel \\'Angel Bautista Martin", "authors": "Terrance DeVries, Miguel Angel Bautista, Nitish Srivastava, Graham W.\n  Taylor, Joshua M. Susskind", "title": "Unconstrained Scene Generation with Locally Conditioned Radiance Fields", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We tackle the challenge of learning a distribution over complex, realistic,\nindoor scenes. In this paper, we introduce Generative Scene Networks (GSN),\nwhich learns to decompose scenes into a collection of many local radiance\nfields that can be rendered from a free moving camera. Our model can be used as\na prior to generate new scenes, or to complete a scene given only sparse 2D\nobservations. Recent work has shown that generative models of radiance fields\ncan capture properties such as multi-view consistency and view-dependent\nlighting. However, these models are specialized for constrained viewing of\nsingle objects, such as cars or faces. Due to the size and complexity of\nrealistic indoor environments, existing models lack the representational\ncapacity to adequately capture them. Our decomposition scheme scales to larger\nand more complex scenes while preserving details and diversity, and the learned\nprior enables high-quality rendering from viewpoints that are significantly\ndifferent from observed viewpoints. When compared to existing models, GSN\nproduces quantitatively higher-quality scene renderings across several\ndifferent scene datasets.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 17:58:26 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["DeVries", "Terrance", ""], ["Bautista", "Miguel Angel", ""], ["Srivastava", "Nitish", ""], ["Taylor", "Graham W.", ""], ["Susskind", "Joshua M.", ""]]}, {"id": "2104.00674", "submitter": "Kai Zhang", "authors": "Kai Zhang, Fujun Luan, Qianqian Wang, Kavita Bala, Noah Snavely", "title": "PhySG: Inverse Rendering with Spherical Gaussians for Physics-based\n  Material Editing and Relighting", "comments": "Accepted to CVPR 2021; Project page:\n  https://kai-46.github.io/PhySG-website/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present PhySG, an end-to-end inverse rendering pipeline that includes a\nfully differentiable renderer and can reconstruct geometry, materials, and\nillumination from scratch from a set of RGB input images. Our framework\nrepresents specular BRDFs and environmental illumination using mixtures of\nspherical Gaussians, and represents geometry as a signed distance function\nparameterized as a Multi-Layer Perceptron. The use of spherical Gaussians\nallows us to efficiently solve for approximate light transport, and our method\nworks on scenes with challenging non-Lambertian reflectance captured under\nnatural, static illumination. We demonstrate, with both synthetic and real\ndata, that our reconstructions not only enable rendering of novel viewpoints,\nbut also physics-based appearance editing of materials and illumination.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 17:59:02 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Zhang", "Kai", ""], ["Luan", "Fujun", ""], ["Wang", "Qianqian", ""], ["Bala", "Kavita", ""], ["Snavely", "Noah", ""]]}, {"id": "2104.00675", "submitter": "Hsin-Ying Lee", "authors": "Yen-Chi Cheng, Chieh Hubert Lin, Hsin-Ying Lee, Jian Ren, Sergey\n  Tulyakov, Ming-Hsuan Yang", "title": "In&Out : Diverse Image Outpainting via GAN Inversion", "comments": "Project Page: https://yccyenchicheng.github.io/InOut/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image outpainting seeks for a semantically consistent extension of the input\nimage beyond its available content. Compared to inpainting -- filling in\nmissing pixels in a way coherent with the neighboring pixels -- outpainting can\nbe achieved in more diverse ways since the problem is less constrained by the\nsurrounding pixels. Existing image outpainting methods pose the problem as a\nconditional image-to-image translation task, often generating repetitive\nstructures and textures by replicating the content available in the input\nimage. In this work, we formulate the problem from the perspective of inverting\ngenerative adversarial networks. Our generator renders micro-patches\nconditioned on their joint latent code as well as their individual positions in\nthe image. To outpaint an image, we seek for multiple latent codes not only\nrecovering available patches but also synthesizing diverse outpainting by\npatch-based generation. This leads to richer structure and content in the\noutpainted regions. Furthermore, our formulation allows for outpainting\nconditioned on the categorical input, thereby enabling flexible user controls.\nExtensive experimental results demonstrate the proposed method performs\nfavorably against existing in- and outpainting methods, featuring higher visual\nquality and diversity.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 17:59:10 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Cheng", "Yen-Chi", ""], ["Lin", "Chieh Hubert", ""], ["Lee", "Hsin-Ying", ""], ["Ren", "Jian", ""], ["Tulyakov", "Sergey", ""], ["Yang", "Ming-Hsuan", ""]]}, {"id": "2104.00676", "submitter": "Zhiqiang Shen", "authors": "Zhiqiang Shen and Zechun Liu and Dejia Xu and Zitian Chen and\n  Kwang-Ting Cheng and Marios Savvides", "title": "Is Label Smoothing Truly Incompatible with Knowledge Distillation: An\n  Empirical Study", "comments": "ICLR 2021. Project page:\n  http://zhiqiangshen.com/projects/LS_and_KD/index.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work aims to empirically clarify a recently discovered perspective that\nlabel smoothing is incompatible with knowledge distillation. We begin by\nintroducing the motivation behind on how this incompatibility is raised, i.e.,\nlabel smoothing erases relative information between teacher logits. We provide\na novel connection on how label smoothing affects distributions of semantically\nsimilar and dissimilar classes. Then we propose a metric to quantitatively\nmeasure the degree of erased information in sample's representation. After\nthat, we study its one-sidedness and imperfection of the incompatibility view\nthrough massive analyses, visualizations and comprehensive experiments on Image\nClassification, Binary Networks, and Neural Machine Translation. Finally, we\nbroadly discuss several circumstances wherein label smoothing will indeed lose\nits effectiveness. Project page:\nhttp://zhiqiangshen.com/projects/LS_and_KD/index.html.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 17:59:12 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Shen", "Zhiqiang", ""], ["Liu", "Zechun", ""], ["Xu", "Dejia", ""], ["Chen", "Zitian", ""], ["Cheng", "Kwang-Ting", ""], ["Savvides", "Marios", ""]]}, {"id": "2104.00677", "submitter": "Ajay Jain", "authors": "Ajay Jain and Matthew Tancik and Pieter Abbeel", "title": "Putting NeRF on a Diet: Semantically Consistent Few-Shot View Synthesis", "comments": "Project website: https://www.ajayj.com/dietnerf", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We present DietNeRF, a 3D neural scene representation estimated from a few\nimages. Neural Radiance Fields (NeRF) learn a continuous volumetric\nrepresentation of a scene through multi-view consistency, and can be rendered\nfrom novel viewpoints by ray casting. While NeRF has an impressive ability to\nreconstruct geometry and fine details given many images, up to 100 for\nchallenging 360{\\deg} scenes, it often finds a degenerate solution to its image\nreconstruction objective when only a few input views are available. To improve\nfew-shot quality, we propose DietNeRF. We introduce an auxiliary semantic\nconsistency loss that encourages realistic renderings at novel poses. DietNeRF\nis trained on individual scenes to (1) correctly render given input views from\nthe same pose, and (2) match high-level semantic attributes across different,\nrandom poses. Our semantic loss allows us to supervise DietNeRF from arbitrary\nposes. We extract these semantics using a pre-trained visual encoder such as\nCLIP, a Vision Transformer trained on hundreds of millions of diverse\nsingle-view, 2D photographs mined from the web with natural language\nsupervision. In experiments, DietNeRF improves the perceptual quality of\nfew-shot view synthesis when learned from scratch, can render novel views with\nas few as one observed image when pre-trained on a multi-view dataset, and\nproduces plausible completions of completely unobserved regions.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 17:59:31 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Jain", "Ajay", ""], ["Tancik", "Matthew", ""], ["Abbeel", "Pieter", ""]]}, {"id": "2104.00678", "submitter": "Zheng Zhang", "authors": "Ze Liu, Zheng Zhang, Yue Cao, Han Hu, Xin Tong", "title": "Group-Free 3D Object Detection via Transformers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently, directly detecting 3D objects from 3D point clouds has received\nincreasing attention. To extract object representation from an irregular point\ncloud, existing methods usually take a point grouping step to assign the points\nto an object candidate so that a PointNet-like network could be used to derive\nobject features from the grouped points. However, the inaccurate point\nassignments caused by the hand-crafted grouping scheme decrease the performance\nof 3D object detection.\n  In this paper, we present a simple yet effective method for directly\ndetecting 3D objects from the 3D point cloud. Instead of grouping local points\nto each object candidate, our method computes the feature of an object from all\nthe points in the point cloud with the help of an attention mechanism in the\nTransformers \\cite{vaswani2017attention}, where the contribution of each point\nis automatically learned in the network training. With an improved attention\nstacking scheme, our method fuses object features in different stages and\ngenerates more accurate object detection results. With few bells and whistles,\nthe proposed method achieves state-of-the-art 3D object detection performance\non two widely used benchmarks, ScanNet V2 and SUN RGB-D. The code and models\nare publicly available at \\url{https://github.com/zeliu98/Group-Free-3D}\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 17:59:36 GMT"}, {"version": "v2", "created": "Fri, 23 Apr 2021 03:48:48 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Liu", "Ze", ""], ["Zhang", "Zheng", ""], ["Cao", "Yue", ""], ["Hu", "Han", ""], ["Tong", "Xin", ""]]}, {"id": "2104.00679", "submitter": "Jong-Chyi Su", "authors": "Jong-Chyi Su and Zezhou Cheng and Subhransu Maji", "title": "A Realistic Evaluation of Semi-Supervised Learning for Fine-Grained\n  Classification", "comments": "CVPR 2021 (oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We evaluate the effectiveness of semi-supervised learning (SSL) on a\nrealistic benchmark where data exhibits considerable class imbalance and\ncontains images from novel classes. Our benchmark consists of two fine-grained\nclassification datasets obtained by sampling classes from the Aves and Fungi\ntaxonomy. We find that recently proposed SSL methods provide significant\nbenefits, and can effectively use out-of-class data to improve performance when\ndeep networks are trained from scratch. Yet their performance pales in\ncomparison to a transfer learning baseline, an alternative approach for\nlearning from a few examples. Furthermore, in the transfer setting, while\nexisting SSL methods provide improvements, the presence of out-of-class is\noften detrimental. In this setting, standard fine-tuning followed by\ndistillation-based self-training is the most robust. Our work suggests that\nsemi-supervised learning with experts on realistic datasets may require\ndifferent strategies than those currently prevalent in the literature.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 17:59:41 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Su", "Jong-Chyi", ""], ["Cheng", "Zezhou", ""], ["Maji", "Subhransu", ""]]}, {"id": "2104.00680", "submitter": "Jiaming Sun", "authors": "Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, Xiaowei Zhou", "title": "LoFTR: Detector-Free Local Feature Matching with Transformers", "comments": "Accepted to CVPR 2021. Project page: https://zju3dv.github.io/loftr/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a novel method for local image feature matching. Instead of\nperforming image feature detection, description, and matching sequentially, we\npropose to first establish pixel-wise dense matches at a coarse level and later\nrefine the good matches at a fine level. In contrast to dense methods that use\na cost volume to search correspondences, we use self and cross attention layers\nin Transformer to obtain feature descriptors that are conditioned on both\nimages. The global receptive field provided by Transformer enables our method\nto produce dense matches in low-texture areas, where feature detectors usually\nstruggle to produce repeatable interest points. The experiments on indoor and\noutdoor datasets show that LoFTR outperforms state-of-the-art methods by a\nlarge margin. LoFTR also ranks first on two public benchmarks of visual\nlocalization among the published methods.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 17:59:42 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Sun", "Jiaming", ""], ["Shen", "Zehong", ""], ["Wang", "Yuang", ""], ["Bao", "Hujun", ""], ["Zhou", "Xiaowei", ""]]}, {"id": "2104.00681", "submitter": "Jiaming Sun", "authors": "Jiaming Sun, Yiming Xie, Linghao Chen, Xiaowei Zhou, Hujun Bao", "title": "NeuralRecon: Real-Time Coherent 3D Reconstruction from Monocular Video", "comments": "Accepted to CVPR 2021 as Oral Presentation. Project page:\n  https://zju3dv.github.io/neuralrecon/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a novel framework named NeuralRecon for real-time 3D scene\nreconstruction from a monocular video. Unlike previous methods that estimate\nsingle-view depth maps separately on each key-frame and fuse them later, we\npropose to directly reconstruct local surfaces represented as sparse TSDF\nvolumes for each video fragment sequentially by a neural network. A\nlearning-based TSDF fusion module based on gated recurrent units is used to\nguide the network to fuse features from previous fragments. This design allows\nthe network to capture local smoothness prior and global shape prior of 3D\nsurfaces when sequentially reconstructing the surfaces, resulting in accurate,\ncoherent, and real-time surface reconstruction. The experiments on ScanNet and\n7-Scenes datasets show that our system outperforms state-of-the-art methods in\nterms of both accuracy and speed. To the best of our knowledge, this is the\nfirst learning-based system that is able to reconstruct dense coherent 3D\ngeometry in real-time.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 17:59:46 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Sun", "Jiaming", ""], ["Xie", "Yiming", ""], ["Chen", "Linghao", ""], ["Zhou", "Xiaowei", ""], ["Bao", "Hujun", ""]]}, {"id": "2104.00682", "submitter": "Christoph Feichtenhofer", "authors": "Bo Xiong, Haoqi Fan, Kristen Grauman, Christoph Feichtenhofer", "title": "Multiview Pseudo-Labeling for Semi-supervised Learning from Video", "comments": "Technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a multiview pseudo-labeling approach to video learning, a novel\nframework that uses complementary views in the form of appearance and motion\ninformation for semi-supervised learning in video. The complementary views help\nobtain more reliable pseudo-labels on unlabeled video, to learn stronger video\nrepresentations than from purely supervised data. Though our method capitalizes\non multiple views, it nonetheless trains a model that is shared across\nappearance and motion input and thus, by design, incurs no additional\ncomputation overhead at inference time. On multiple video recognition datasets,\nour method substantially outperforms its supervised counterpart, and compares\nfavorably to previous work on standard benchmarks in self-supervised video\nrepresentation learning.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 17:59:48 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Xiong", "Bo", ""], ["Fan", "Haoqi", ""], ["Grauman", "Kristen", ""], ["Feichtenhofer", "Christoph", ""]]}, {"id": "2104.00683", "submitter": "Ye Yuan", "authors": "Ye Yuan, Shih-En Wei, Tomas Simon, Kris Kitani, Jason Saragih", "title": "SimPoE: Simulated Character Control for 3D Human Pose Estimation", "comments": "CVPR 2021 (Oral). Project page: https://www.ye-yuan.com/simpoe/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate estimation of 3D human motion from monocular video requires modeling\nboth kinematics (body motion without physical forces) and dynamics (motion with\nphysical forces). To demonstrate this, we present SimPoE, a Simulation-based\napproach for 3D human Pose Estimation, which integrates image-based kinematic\ninference and physics-based dynamics modeling. SimPoE learns a policy that\ntakes as input the current-frame pose estimate and the next image frame to\ncontrol a physically-simulated character to output the next-frame pose\nestimate. The policy contains a learnable kinematic pose refinement unit that\nuses 2D keypoints to iteratively refine its kinematic pose estimate of the next\nframe. Based on this refined kinematic pose, the policy learns to compute\ndynamics-based control (e.g., joint torques) of the character to advance the\ncurrent-frame pose estimate to the pose estimate of the next frame. This design\ncouples the kinematic pose refinement unit with the dynamics-based control\ngeneration unit, which are learned jointly with reinforcement learning to\nachieve accurate and physically-plausible pose estimation. Furthermore, we\npropose a meta-control mechanism that dynamically adjusts the character's\ndynamics parameters based on the character state to attain more accurate pose\nestimates. Experiments on large-scale motion datasets demonstrate that our\napproach establishes the new state of the art in pose accuracy while ensuring\nphysical plausibility.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 17:59:50 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Yuan", "Ye", ""], ["Wei", "Shih-En", ""], ["Simon", "Tomas", ""], ["Kitani", "Kris", ""], ["Saragih", "Jason", ""]]}, {"id": "2104.00702", "submitter": "Pablo Palafox", "authors": "Pablo Palafox, Alja\\v{z} Bo\\v{z}i\\v{c}, Justus Thies, Matthias\n  Nie{\\ss}ner, Angela Dai", "title": "NPMs: Neural Parametric Models for 3D Deformable Shapes", "comments": "Video: https://youtu.be/muZXXgkkMPY", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parametric 3D models have enabled a wide variety of tasks in computer\ngraphics and vision, such as modeling human bodies, faces, and hands. However,\nthe construction of these parametric models is often tedious, as it requires\nheavy manual tweaking, and they struggle to represent additional complexity and\ndetails such as wrinkles or clothing. To this end, we propose Neural Parametric\nModels (NPMs), a novel, learned alternative to traditional, parametric 3D\nmodels, which does not require hand-crafted, object-specific constraints. In\nparticular, we learn to disentangle 4D dynamics into latent-space\nrepresentations of shape and pose, leveraging the flexibility of recent\ndevelopments in learned implicit functions. Crucially, once learned, our neural\nparametric models of shape and pose enable optimization over the learned spaces\nto fit to new observations, similar to the fitting of a traditional parametric\nmodel, e.g., SMPL. This enables NPMs to achieve a significantly more accurate\nand detailed representation of observed deformable sequences. We show that NPMs\nimprove notably over both parametric and non-parametric state of the art in\nreconstruction and tracking of monocular depth sequences of clothed humans and\nhands. Latent-space interpolation as well as shape / pose transfer experiments\nfurther demonstrate the usefulness of NPMs.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 18:14:56 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Palafox", "Pablo", ""], ["Bo\u017ei\u010d", "Alja\u017e", ""], ["Thies", "Justus", ""], ["Nie\u00dfner", "Matthias", ""], ["Dai", "Angela", ""]]}, {"id": "2104.00704", "submitter": "Michael Schmitt", "authors": "Michael Schmitt, Yu-Lun Wu", "title": "Remote Sensing Image Classification with the SEN12MS Dataset", "comments": "accepted for publication in the ISPRS Annals of the Photogrammetry,\n  Remote Sensing and Spatial Information Sciences (online from July 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image classification is one of the main drivers of the rapid developments in\ndeep learning with convolutional neural networks for computer vision. So is the\nanalogous task of scene classification in remote sensing. However, in contrast\nto the computer vision community that has long been using well-established,\nlarge-scale standard datasets to train and benchmark high-capacity models, the\nremote sensing community still largely relies on relatively small and often\napplication-dependend datasets, thus lacking comparability. With this letter,\nwe present a classification-oriented conversion of the SEN12MS dataset. Using\nthat, we provide results for several baseline models based on two standard CNN\narchitectures and different input data configurations. Our results support the\nbenchmarking of remote sensing image classification and provide insights to the\nbenefit of multi-spectral data and multi-sensor data fusion over conventional\nRGB imagery.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 18:15:16 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Schmitt", "Michael", ""], ["Wu", "Yu-Lun", ""]]}, {"id": "2104.00706", "submitter": "Joseph Lambourne", "authors": "Joseph G. Lambourne, Karl D.D. Willis, Pradeep Kumar Jayaraman, Aditya\n  Sanghi, Peter Meltzer, Hooman Shayani", "title": "BRepNet: A topological message passing system for solid models", "comments": "CVPR 2021 Oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Boundary representation (B-rep) models are the standard way 3D shapes are\ndescribed in Computer-Aided Design (CAD) applications. They combine lightweight\nparametric curves and surfaces with topological information which connects the\ngeometric entities to describe manifolds. In this paper we introduce BRepNet, a\nneural network architecture designed to operate directly on B-rep data\nstructures, avoiding the need to approximate the model as meshes or point\nclouds. BRepNet defines convolutional kernels with respect to oriented coedges\nin the data structure. In the neighborhood of each coedge, a small collection\nof faces, edges and coedges can be identified and patterns in the feature\nvectors from these entities detected by specific learnable parameters. In\naddition, to encourage further deep learning research with B-reps, we publish\nthe Fusion 360 Gallery segmentation dataset. A collection of over 35,000 B-rep\nmodels annotated with information about the modeling operations which created\neach face. We demonstrate that BRepNet can segment these models with higher\naccuracy than methods working on meshes, and point clouds.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 18:16:03 GMT"}, {"version": "v2", "created": "Thu, 8 Apr 2021 14:46:06 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Lambourne", "Joseph G.", ""], ["Willis", "Karl D. D.", ""], ["Jayaraman", "Pradeep Kumar", ""], ["Sanghi", "Aditya", ""], ["Meltzer", "Peter", ""], ["Shayani", "Hooman", ""]]}, {"id": "2104.00742", "submitter": "Yunye Gong", "authors": "Yunye Gong, Xiao Lin, Yi Yao, Thomas G. Dietterich, Ajay Divakaran,\n  Melinda Gervasio", "title": "Confidence Calibration for Domain Generalization under Covariate Shift", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing calibration algorithms address the problem of covariate shift via\nunsupervised domain adaptation. However, these methods suffer from the\nfollowing limitations: 1) they require unlabeled data from the target domain,\nwhich may not be available at the stage of calibration in real-world\napplications and 2) their performances heavily depend on the disparity between\nthe distributions of the source and target domains. To address these two\nlimitations, we present novel calibration solutions via domain generalization\nwhich, to the best of our knowledge, are the first of their kind. Our core idea\nis to leverage multiple calibration domains to reduce the effective\ndistribution disparity between the target and calibration domains for improved\ncalibration transfer without needing any data from the target domain. We\nprovide theoretical justification and empirical experimental results to\ndemonstrate the effectiveness of our proposed algorithms. Compared against the\nstate-of-the-art calibration methods designed for domain adaptation, we observe\na decrease of 8.86 percentage points in expected calibration error,\nequivalently an increase of 35 percentage points in improvement ratio, for\nmulti-class classification on the Office-Home dataset.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 19:31:54 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Gong", "Yunye", ""], ["Lin", "Xiao", ""], ["Yao", "Yi", ""], ["Dietterich", "Thomas G.", ""], ["Divakaran", "Ajay", ""], ["Gervasio", "Melinda", ""]]}, {"id": "2104.00743", "submitter": "Tanmay Gupta", "authors": "Tanmay Gupta, Amita Kamath, Aniruddha Kembhavi and Derek Hoiem", "title": "Towards General Purpose Vision Systems", "comments": "Project page: https://prior.allenai.org/projects/gpv", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A special purpose learning system assumes knowledge of admissible tasks at\ndesign time. Adapting such a system to unforeseen tasks requires architecture\nmanipulation such as adding an output head for each new task or dataset. In\nthis work, we propose a task-agnostic vision-language system that accepts an\nimage and a natural language task description and outputs bounding boxes,\nconfidences, and text. The system supports a wide range of vision tasks such as\nclassification, localization, question answering, captioning, and more. We\nevaluate the system's ability to learn multiple skills simultaneously, to\nperform tasks with novel skill-concept combinations, and to learn new skills\nefficiently and without forgetting.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 19:35:21 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Gupta", "Tanmay", ""], ["Kamath", "Amita", ""], ["Kembhavi", "Aniruddha", ""], ["Hoiem", "Derek", ""]]}, {"id": "2104.00749", "submitter": "Zhuang Liu", "authors": "Zhuang Liu, Trevor Darrell, Evan Shelhamer", "title": "Confidence Adaptive Anytime Pixel-Level Recognition", "comments": "16 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anytime inference requires a model to make a progression of predictions which\nmight be halted at any time. Prior research on anytime visual recognition has\nmostly focused on image classification. We propose the first unified and\nend-to-end model approach for anytime pixel-level recognition. A cascade of\n\"exits\" is attached to the model to make multiple predictions and direct\nfurther computation. We redesign the exits to account for the depth and spatial\nresolution of the features for each exit. To reduce total computation, and make\nfull use of prior predictions, we develop a novel spatially adaptive approach\nto avoid further computation on regions where early predictions are already\nsufficiently confident. Our full model with redesigned exit architecture and\nspatial adaptivity enables anytime inference, achieves the same level of final\naccuracy, and even significantly reduces total computation. We evaluate our\napproach on semantic segmentation and human pose estimation. On Cityscapes\nsemantic segmentation and MPII human pose estimation, our approach enables\nanytime inference while also reducing the total FLOPs of its base models by\n44.4% and 59.1% without sacrificing accuracy. As a new anytime baseline, we\nmeasure the anytime capability of deep equilibrium networks, a recent class of\nmodel that is intrinsically iterative, and we show that the\naccuracy-computation curve of our architecture strictly dominates it.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 20:01:57 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Liu", "Zhuang", ""], ["Darrell", "Trevor", ""], ["Shelhamer", "Evan", ""]]}, {"id": "2104.00788", "submitter": "Kiran Mantripragada", "authors": "Kiran Mantripragada, Phuong D. Dao, Yuhong He, Faisal Z. Qureshi", "title": "A study on the effects of compression on hyperspectral image\n  classification", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents a systematic study the effects of compression on\nhyperspectral pixel classification task. We use five dimensionality reduction\nmethods -- PCA, KPCA, ICA, AE, and DAE -- to compress 301-dimensional\nhyperspectral pixels. Compressed pixels are subsequently used to perform\npixel-based classifications. Pixel classification accuracies together with\ncompression method, compression rates, and reconstruction errors provide a new\nlens to study the suitability of a compression method for the task of\npixel-based classification. We use three high-resolution hyperspectral image\ndatasets, representing three common landscape units (i.e. urban, transitional\nsuburban, and forests) collected by the Remote Sensing and Spatial Ecosystem\nModeling laboratory of the University of Toronto. We found that PCA, KPCA, and\nICA post greater signal reconstruction capability; however, when compression\nrate is more than 90\\% those methods showed lower classification scores. AE and\nDAE methods post better classification accuracy at 95\\% compression rate,\nhowever decreasing again at 97\\%, suggesting a sweet-spot at the 95\\% mark. Our\nresults demonstrate that the choice of a compression method with the\ncompression rate are important considerations when designing a hyperspectral\nimage classification pipeline.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 22:22:47 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Mantripragada", "Kiran", ""], ["Dao", "Phuong D.", ""], ["He", "Yuhong", ""], ["Qureshi", "Faisal Z.", ""]]}, {"id": "2104.00793", "submitter": "Saahil Jain", "authors": "Saahil Jain, Akshay Smit, Andrew Y. Ng, Pranav Rajpurkar", "title": "Effect of Radiology Report Labeler Quality on Deep Learning Models for\n  Chest X-Ray Interpretation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although deep learning models for chest X-ray interpretation are commonly\ntrained on labels generated by automatic radiology report labelers, the impact\nof improvements in report labeling on the performance of chest X-ray\nclassification models has not been systematically investigated. We first\ncompare the CheXpert, CheXbert, and VisualCheXbert labelers on the task of\nextracting accurate chest X-ray image labels from radiology reports, reporting\nthat the VisualCheXbert labeler outperforms the CheXpert and CheXbert labelers.\nNext, after training image classification models using labels generated from\nthe different radiology report labelers on one of the largest datasets of chest\nX-rays, we show that an image classification model trained on labels from the\nVisualCheXbert labeler outperforms image classification models trained on\nlabels from the CheXpert and CheXbert labelers. Our work suggests that recent\nimprovements in radiology report labeling can translate to the development of\nhigher performing chest X-ray classification models.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 22:37:29 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Jain", "Saahil", ""], ["Smit", "Akshay", ""], ["Ng", "Andrew Y.", ""], ["Rajpurkar", "Pranav", ""]]}, {"id": "2104.00795", "submitter": "Shyamgopal Karthik", "authors": "Shyamgopal Karthik, Ameya Prabhu, Puneet K. Dokania, Vineet Gandhi", "title": "No Cost Likelihood Manipulation at Test Time for Making Better Mistakes\n  in Deep Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  There has been increasing interest in building deep hierarchy-aware\nclassifiers that aim to quantify and reduce the severity of mistakes, and not\njust reduce the number of errors. The idea is to exploit the label hierarchy\n(e.g., the WordNet ontology) and consider graph distances as a proxy for\nmistake severity. Surprisingly, on examining mistake-severity distributions of\nthe top-1 prediction, we find that current state-of-the-art hierarchy-aware\ndeep classifiers do not always show practical improvement over the standard\ncross-entropy baseline in making better mistakes. The reason for the reduction\nin average mistake-severity can be attributed to the increase in low-severity\nmistakes, which may also explain the noticeable drop in their accuracy. To this\nend, we use the classical Conditional Risk Minimization (CRM) framework for\nhierarchy-aware classification. Given a cost matrix and a reliable estimate of\nlikelihoods (obtained from a trained network), CRM simply amends mistakes at\ninference time; it needs no extra hyperparameters and requires adding just a\nfew lines of code to the standard cross-entropy baseline. It significantly\noutperforms the state-of-the-art and consistently obtains large reductions in\nthe average hierarchical distance of top-$k$ predictions across datasets, with\nvery little loss in accuracy. CRM, because of its simplicity, can be used with\nany off-the-shelf trained model that provides reliable likelihood estimates.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 22:40:25 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Karthik", "Shyamgopal", ""], ["Prabhu", "Ameya", ""], ["Dokania", "Puneet K.", ""], ["Gandhi", "Vineet", ""]]}, {"id": "2104.00798", "submitter": "Jiahao Pang", "authors": "Haiyan Wang, Jiahao Pang, Muhammad A. Lodhi, Yingli Tian, Dong Tian", "title": "FESTA: Flow Estimation via Spatial-Temporal Attention for Scene Point\n  Clouds", "comments": "Accepted at CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scene flow depicts the dynamics of a 3D scene, which is critical for various\napplications such as autonomous driving, robot navigation, AR/VR, etc.\nConventionally, scene flow is estimated from dense/regular RGB video frames.\nWith the development of depth-sensing technologies, precise 3D measurements are\navailable via point clouds which have sparked new research in 3D scene flow.\nNevertheless, it remains challenging to extract scene flow from point clouds\ndue to the sparsity and irregularity in typical point cloud sampling patterns.\nOne major issue related to irregular sampling is identified as the randomness\nduring point set abstraction/feature extraction -- an elementary process in\nmany flow estimation scenarios. A novel Spatial Abstraction with Attention\n(SA^2) layer is accordingly proposed to alleviate the unstable abstraction\nproblem. Moreover, a Temporal Abstraction with Attention (TA^2) layer is\nproposed to rectify attention in temporal domain, leading to benefits with\nmotions scaled in a larger range. Extensive analysis and experiments verified\nthe motivation and significant performance gains of our method, dubbed as Flow\nEstimation via Spatial-Temporal Attention (FESTA), when compared to several\nstate-of-the-art benchmarks of scene flow estimation.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 23:04:04 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Wang", "Haiyan", ""], ["Pang", "Jiahao", ""], ["Lodhi", "Muhammad A.", ""], ["Tian", "Yingli", ""], ["Tian", "Dong", ""]]}, {"id": "2104.00805", "submitter": "Zoya Bylinskii", "authors": "Zoya Bylinskii, Lore Goetschalckx, Anelise Newman, Aude Oliva", "title": "Memorability: An image-computable measure of information utility", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The pixels in an image, and the objects, scenes, and actions that they\ncompose, determine whether an image will be memorable or forgettable. While\nmemorability varies by image, it is largely independent of an individual\nobserver. Observer independence is what makes memorability an image-computable\nmeasure of information, and eligible for automatic prediction. In this chapter,\nwe zoom into memorability with a computational lens, detailing the\nstate-of-the-art algorithms that accurately predict image memorability relative\nto human behavioral data, using image features at different scales from raw\npixels to semantic labels. We discuss the design of algorithms and\nvisualizations for face, object, and scene memorability, as well as algorithms\nthat generalize beyond static scenes to actions and videos. We cover the\nstate-of-the-art deep learning approaches that are the current front runners in\nthe memorability prediction space. Beyond prediction, we show how recent A.I.\napproaches can be used to create and modify visual memorability. Finally, we\npreview the computational applications that memorability can power, from\nfiltering visual streams to enhancing augmented reality interfaces.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 23:38:30 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Bylinskii", "Zoya", ""], ["Goetschalckx", "Lore", ""], ["Newman", "Anelise", ""], ["Oliva", "Aude", ""]]}, {"id": "2104.00807", "submitter": "Hilmi Enes Egilmez", "authors": "Ankitesh K. Singh, Hilmi E. Egilmez, Reza Pourreza, Muhammed Coban,\n  Marta Karczewicz, Taco S. Cohen", "title": "A Combined Deep Learning based End-to-End Video Coding Architecture for\n  YUV Color Space", "comments": "5 pages, submitted to as a conference paper. arXiv admin note: text\n  overlap with arXiv:2103.01760", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of the existing deep learning based end-to-end video coding (DLEC)\narchitectures are designed specifically for RGB color format, yet the video\ncoding standards, including H.264/AVC, H.265/HEVC and H.266/VVC developed over\npast few decades, have been designed primarily for YUV 4:2:0 format, where the\nchrominance (U and V) components are subsampled to achieve superior compression\nperformances considering the human visual system. While a broad number of\npapers on DLEC compare these two distinct coding schemes in RGB domain, it is\nideal to have a common evaluation framework in YUV 4:2:0 domain for a more fair\ncomparison. This paper introduces a new DLEC architecture for video coding to\neffectively support YUV 4:2:0 and compares its performance against the HEVC\nstandard under a common evaluation framework. The experimental results on YUV\n4:2:0 video sequences show that the proposed architecture can outperform HEVC\nin intra-frame coding, however inter-frame coding is not as efficient on\ncontrary to the RGB coding results reported in recent papers.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 23:41:06 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Singh", "Ankitesh K.", ""], ["Egilmez", "Hilmi E.", ""], ["Pourreza", "Reza", ""], ["Coban", "Muhammed", ""], ["Karczewicz", "Marta", ""], ["Cohen", "Taco S.", ""]]}, {"id": "2104.00808", "submitter": "Subhankar Roy", "authors": "Subhankar Roy, Evgeny Krivosheev, Zhun Zhong, Nicu Sebe, Elisa Ricci", "title": "Curriculum Graph Co-Teaching for Multi-Target Domain Adaptation", "comments": "CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper we address multi-target domain adaptation (MTDA), where given\none labeled source dataset and multiple unlabeled target datasets that differ\nin data distributions, the task is to learn a robust predictor for all the\ntarget domains. We identify two key aspects that can help to alleviate multiple\ndomain-shifts in the MTDA: feature aggregation and curriculum learning. To this\nend, we propose Curriculum Graph Co-Teaching (CGCT) that uses a dual classifier\nhead, with one of them being a graph convolutional network (GCN) which\naggregates features from similar samples across the domains. To prevent the\nclassifiers from over-fitting on its own noisy pseudo-labels we develop a\nco-teaching strategy with the dual classifier head that is assisted by\ncurriculum learning to obtain more reliable pseudo-labels. Furthermore, when\nthe domain labels are available, we propose Domain-aware Curriculum Learning\n(DCL), a sequential adaptation strategy that first adapts on the easier target\ndomains, followed by the harder ones. We experimentally demonstrate the\neffectiveness of our proposed frameworks on several benchmarks and advance the\nstate-of-the-art in the MTDA by large margins (e.g. +5.6% on the DomainNet).\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 23:41:41 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Roy", "Subhankar", ""], ["Krivosheev", "Evgeny", ""], ["Zhong", "Zhun", ""], ["Sebe", "Nicu", ""], ["Ricci", "Elisa", ""]]}, {"id": "2104.00816", "submitter": "Ali Sadeghian", "authors": "Mohammadreza Armandpour, Ali Sadeghian, Chunyuan Li, Mingyuan Zhou", "title": "Partition-Guided GANs", "comments": "Accepted for publication at CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Despite the success of Generative Adversarial Networks (GANs), their training\nsuffers from several well-known problems, including mode collapse and\ndifficulties learning a disconnected set of manifolds. In this paper, we break\ndown the challenging task of learning complex high dimensional distributions,\nsupporting diverse data samples, to simpler sub-tasks. Our solution relies on\ndesigning a partitioner that breaks the space into smaller regions, each having\na simpler distribution, and training a different generator for each partition.\nThis is done in an unsupervised manner without requiring any labels.\n  We formulate two desired criteria for the space partitioner that aid the\ntraining of our mixture of generators: 1) to produce connected partitions and\n2) provide a proxy of distance between partitions and data samples, along with\na direction for reducing that distance. These criteria are developed to avoid\nproducing samples from places with non-existent data density, and also\nfacilitate training by providing additional direction to the generators. We\ndevelop theoretical constraints for a space partitioner to satisfy the above\ncriteria. Guided by our theoretical analysis, we design an effective neural\narchitecture for the space partitioner that empirically assures these\nconditions. Experimental results on various standard benchmarks show that the\nproposed unsupervised model outperforms several recent methods.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 00:06:53 GMT"}, {"version": "v2", "created": "Fri, 18 Jun 2021 00:53:26 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Armandpour", "Mohammadreza", ""], ["Sadeghian", "Ali", ""], ["Li", "Chunyuan", ""], ["Zhou", "Mingyuan", ""]]}, {"id": "2104.00820", "submitter": "Enis Simsar", "authors": "O\\u{g}uz Kaan Y\\\"uksel, Enis Simsar, Ezgi G\\\"ulperi Er, Pinar Yanardag", "title": "LatentCLR: A Contrastive Learning Approach for Unsupervised Discovery of\n  Interpretable Directions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent research has shown great potential for finding interpretable\ndirections in the latent spaces of pre-trained Generative Adversarial Networks\n(GANs). These directions provide controllable generation and support a wide\nrange of semantic editing operations such as zoom or rotation. The discovery of\nsuch directions is often performed in a supervised or semi-supervised fashion\nand requires manual annotations, limiting their applications in practice. In\ncomparison, unsupervised discovery enables finding subtle directions a priori\nhard to recognize. In this work, we propose a contrastive-learning-based\napproach for discovering semantic directions in the latent space of pretrained\nGANs in a self-supervised manner. Our approach finds semantically meaningful\ndimensions compatible with state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 00:11:22 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Y\u00fcksel", "O\u011fuz Kaan", ""], ["Simsar", "Enis", ""], ["Er", "Ezgi G\u00fclperi", ""], ["Yanardag", "Pinar", ""]]}, {"id": "2104.00825", "submitter": "Andrew Hou", "authors": "Andrew Hou, Ze Zhang, Michel Sarkis, Ning Bi, Yiying Tong, Xiaoming\n  Liu", "title": "Towards High Fidelity Face Relighting with Realistic Shadows", "comments": "Accepted to CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing face relighting methods often struggle with two problems:\nmaintaining the local facial details of the subject and accurately removing and\nsynthesizing shadows in the relit image, especially hard shadows. We propose a\nnovel deep face relighting method that addresses both problems. Our method\nlearns to predict the ratio (quotient) image between a source image and the\ntarget image with the desired lighting, allowing us to relight the image while\nmaintaining the local facial details. During training, our model also learns to\naccurately modify shadows by using estimated shadow masks to emphasize on the\nhigh-contrast shadow borders. Furthermore, we introduce a method to use the\nshadow mask to estimate the ambient light intensity in an image, and are thus\nable to leverage multiple datasets during training with different global\nlighting intensities. With quantitative and qualitative evaluations on the\nMulti-PIE and FFHQ datasets, we demonstrate that our proposed method faithfully\nmaintains the local facial details of the subject and can accurately handle\nhard shadows while achieving state-of-the-art face relighting performance.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 00:28:40 GMT"}, {"version": "v2", "created": "Sat, 5 Jun 2021 20:55:04 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Hou", "Andrew", ""], ["Zhang", "Ze", ""], ["Sarkis", "Michel", ""], ["Bi", "Ning", ""], ["Tong", "Yiying", ""], ["Liu", "Xiaoming", ""]]}, {"id": "2104.00827", "submitter": "Jingxi Xu", "authors": "Jingxi Xu, Bruce Lee, Nikolai Matni, Dinesh Jayaraman", "title": "How Are Learned Perception-Based Controllers Impacted by the Limits of\n  Robust Control?", "comments": "Accepted to L4DC 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The difficulty of optimal control problems has classically been characterized\nin terms of system properties such as minimum eigenvalues of\ncontrollability/observability gramians. We revisit these characterizations in\nthe context of the increasing popularity of data-driven techniques like\nreinforcement learning (RL), and in control settings where input observations\nare high-dimensional images and transition dynamics are unknown. Specifically,\nwe ask: to what extent are quantifiable control and perceptual difficulty\nmetrics of a task predictive of the performance and sample complexity of\ndata-driven controllers? We modulate two different types of partial\nobservability in a cartpole \"stick-balancing\" problem -- (i) the height of one\nvisible fixation point on the cartpole, which can be used to tune fundamental\nlimits of performance achievable by any controller, and by (ii) the level of\nperception noise in the fixation point position inferred from depth or RGB\nimages of the cartpole. In these settings, we empirically study two popular\nfamilies of controllers: RL and system identification-based $H_\\infty$ control,\nusing visually estimated system state. Our results show that the fundamental\nlimits of robust control have corresponding implications for the\nsample-efficiency and performance of learned perception-based controllers.\nVisit our project website https://jxu.ai/rl-vs-control-web for more\ninformation.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 00:31:31 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Xu", "Jingxi", ""], ["Lee", "Bruce", ""], ["Matni", "Nikolai", ""], ["Jayaraman", "Dinesh", ""]]}, {"id": "2104.00829", "submitter": "Siyuan Cheng", "authors": "Siyuan Cheng, Bineng Zhong, Guorong Li, Xin Liu, Zhenjun Tang,\n  Xianxian Li, Jing Wang", "title": "Learning to Filter: Siamese Relation Network for Robust Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Despite the great success of Siamese-based trackers, their performance under\ncomplicated scenarios is still not satisfying, especially when there are\ndistractors. To this end, we propose a novel Siamese relation network, which\nintroduces two efficient modules, i.e. Relation Detector (RD) and Refinement\nModule (RM). RD performs in a meta-learning way to obtain a learning ability to\nfilter the distractors from the background while RM aims to effectively\nintegrate the proposed RD into the Siamese framework to generate accurate\ntracking result. Moreover, to further improve the discriminability and\nrobustness of the tracker, we introduce a contrastive training strategy that\nattempts not only to learn matching the same target but also to learn how to\ndistinguish the different objects. Therefore, our tracker can achieve accurate\ntracking results when facing background clutters, fast motion, and occlusion.\nExperimental results on five popular benchmarks, including VOT2018, VOT2019,\nOTB100, LaSOT, and UAV123, show that the proposed method is effective and can\nachieve state-of-the-art results. The code will be available at\nhttps://github.com/hqucv/siamrn\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 00:53:33 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Cheng", "Siyuan", ""], ["Zhong", "Bineng", ""], ["Li", "Guorong", ""], ["Liu", "Xin", ""], ["Tang", "Zhenjun", ""], ["Li", "Xianxian", ""], ["Wang", "Jing", ""]]}, {"id": "2104.00842", "submitter": "Aviral Joshi", "authors": "A Vinay, Aviral Joshi, Hardik Mahipal Surana, Harsh Garg, K N\n  BalasubramanyaMurthy, S Natarajan", "title": "Unconstrained Face Recognition using ASURF and Cloud-Forest Classifier\n  optimized with VLAD", "comments": "8 Pages, 3 Figures", "journal-ref": "Procedia computer science, 143, 570-578 (2018)", "doi": "10.1016/j.procs.2018.10.433", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The paper posits a computationally-efficient algorithm for multi-class facial\nimage classification in which images are constrained with translation,\nrotation, scale, color, illumination and affine distortion. The proposed method\nis divided into five main building blocks including Haar-Cascade for face\ndetection, Bilateral Filter for image preprocessing to remove unwanted noise,\nAffine Speeded-Up Robust Features (ASURF) for keypoint detection and\ndescription, Vector of Locally Aggregated Descriptors (VLAD) for feature\nquantization and Cloud Forest for image classification. The proposed method\naims at improving the accuracy and the time taken for face recognition systems.\nThe usage of the Cloud Forest algorithm as a classifier on three benchmark\ndatasets, namely the FACES95, FACES96 and ORL facial datasets, showed promising\nresults. The proposed methodology using Cloud Forest algorithm successfully\nimproves the recognition model by 2-12\\% when differentiated against other\nensemble techniques like the Random Forest classifier depending upon the\ndataset used.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 01:26:26 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Vinay", "A", ""], ["Joshi", "Aviral", ""], ["Surana", "Hardik Mahipal", ""], ["Garg", "Harsh", ""], ["BalasubramanyaMurthy", "K N", ""], ["Natarajan", "S", ""]]}, {"id": "2104.00845", "submitter": "Chuanxia Zheng", "authors": "Chuanxia Zheng, Tat-Jen Cham, Jianfei Cai", "title": "TFill: Image Completion via a Transformer-Based Architecture", "comments": "22 pages, 18 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Bridging distant context interactions is important for high quality image\ncompletion with large masks. Previous methods attempting this via deep or large\nreceptive field (RF) convolutions cannot escape from the dominance of nearby\ninteractions, which may be inferior. In this paper, we propose treating image\ncompletion as a directionless sequence-to-sequence prediction task, and deploy\na transformer to directly capture long-range dependence in the encoder in a\nfirst phase. Crucially, we employ a restrictive CNN with small and\nnon-overlapping RF for token representation, which allows the transformer to\nexplicitly model the long-range context relations with equal importance in all\nlayers, without implicitly confounding neighboring tokens when larger RFs are\nused. In a second phase, to improve appearance consistency between visible and\ngenerated regions, a novel attention-aware layer (AAL) is introduced to better\nexploit distantly related features and also avoid the insular effect of\nstandard attention. Overall, extensive experiments demonstrate superior\nperformance compared to state-of-the-art methods on several datasets.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 01:42:01 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Zheng", "Chuanxia", ""], ["Cham", "Tat-Jen", ""], ["Cai", "Jianfei", ""]]}, {"id": "2104.00848", "submitter": "Kangfu Mei", "authors": "Kangfu Mei, Shenglong Ye, Rui Huang", "title": "SDAN: Squared Deformable Alignment Network for Learning Misaligned\n  Optical Zoom", "comments": "ICME21. Code is available at https://github.com/MKFMIKU/SDAN", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Network (DNN) based super-resolution algorithms have greatly\nimproved the quality of the generated images. However, these algorithms often\nyield significant artifacts when dealing with real-world super-resolution\nproblems due to the difficulty in learning misaligned optical zoom. In this\npaper, we introduce a Squared Deformable Alignment Network (SDAN) to address\nthis issue. Our network learns squared per-point offsets for convolutional\nkernels, and then aligns features in corrected convolutional windows based on\nthe offsets. So the misalignment will be minimized by the extracted aligned\nfeatures. Different from the per-point offsets used in the vanilla Deformable\nConvolutional Network (DCN), our proposed squared offsets not only accelerate\nthe offset learning but also improve the generation quality with fewer\nparameters. Besides, we further propose an efficient cross packing attention\nlayer to boost the accuracy of the learned offsets. It leverages the packing\nand unpacking operations to enlarge the receptive field of the offset learning\nand to enhance the ability of extracting the spatial connection between the\nlow-resolution images and the referenced images. Comprehensive experiments show\nthe superiority of our method over other state-of-the-art methods in both\ncomputational efficiency and realistic details.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 01:58:00 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Mei", "Kangfu", ""], ["Ye", "Shenglong", ""], ["Huang", "Rui", ""]]}, {"id": "2104.00850", "submitter": "Loris Nanni", "authors": "Alessandra Lumini, Loris Nanni and Gianluca Maguolo", "title": "Deep ensembles based on Stochastic Activation Selection for Polyp\n  Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Semantic segmentation has a wide array of applications ranging from\nmedical-image analysis, scene understanding, autonomous driving and robotic\nnavigation. This work deals with medical image segmentation and in particular\nwith accurate polyp detection and segmentation during colonoscopy examinations.\nSeveral convolutional neural network architectures have been proposed to\neffectively deal with this task and with the problem of segmenting objects at\ndifferent scale input. The basic architecture in image segmentation consists of\nan encoder and a decoder: the first uses convolutional filters to extract\nfeatures from the image, the second is responsible for generating the final\noutput. In this work, we compare some variant of the DeepLab architecture\nobtained by varying the decoder backbone. We compare several decoder\narchitectures, including ResNet, Xception, EfficentNet, MobileNet and we\nperturb their layers by substituting ReLU activation layers with other\nfunctions. The resulting methods are used to create deep ensembles which are\nshown to be very effective. Our experimental evaluations show that our best\nensemble produces good segmentation results by achieving high evaluation scores\nwith a dice coefficient of 0.884, and a mean Intersection over Union (mIoU) of\n0.818 for the Kvasir-SEG dataset. To improve reproducibility and research\nefficiency the MATLAB source code used for this research is available at\nGitHub: https://github.com/LorisNanni.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 02:07:37 GMT"}, {"version": "v2", "created": "Wed, 7 Apr 2021 21:31:56 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Lumini", "Alessandra", ""], ["Nanni", "Loris", ""], ["Maguolo", "Gianluca", ""]]}, {"id": "2104.00851", "submitter": "Yang Zhao", "authors": "Yang Zhao and Hao Zhang", "title": "Analyzing and Quantifying Generalization in Convolutional Neural\n  Networks", "comments": "iccv submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Generalization is the key capability of convolutional neural networks (CNNs).\nHowever, it is still quite elusive for differentiating the CNNs with good or\npoor generalization. It results in the barrier for providing reliable\nquantitative measure of generalization ability. To this end, this paper aims to\nclarify the generalization status of individual units in typical CNNs and\nquantify the generalization ability of networks using image classification task\nwith multiple classes data. Firstly, we propose a feature quantity, role share,\nconsisting of four discriminate statuses for a certain unit based on its\ncontribution to generalization. The distribution of role shares across all\nunits provides a straightforward visualization for the generalization of a\nnetwork. Secondly, using only training sets, we propose a novel metric for\nquantifying the intrinsic generalization ability of networks. Lastly, a\npredictor of testing accuracy via only training accuracy of typical CNN is\ngiven. Empirical experiments using practical network model (VGG) and dataset\n(ImageNet) illustrate the rationality and effectiveness of our feature\nquantity, metric and predictor.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 02:10:32 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Zhao", "Yang", ""], ["Zhang", "Hao", ""]]}, {"id": "2104.00854", "submitter": "Chuanxia Zheng", "authors": "Chuanxia Zheng, Tat-Jen Cham, Jianfei Cai", "title": "The Spatially-Correlative Loss for Various Image Translation Tasks", "comments": "14 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We propose a novel spatially-correlative loss that is simple, efficient and\nyet effective for preserving scene structure consistency while supporting large\nappearance changes during unpaired image-to-image (I2I) translation. Previous\nmethods attempt this by using pixel-level cycle-consistency or feature-level\nmatching losses, but the domain-specific nature of these losses hinder\ntranslation across large domain gaps. To address this, we exploit the spatial\npatterns of self-similarity as a means of defining scene structure. Our\nspatially-correlative loss is geared towards only capturing spatial\nrelationships within an image rather than domain appearance. We also introduce\na new self-supervised learning method to explicitly learn spatially-correlative\nmaps for each specific translation task. We show distinct improvement over\nbaseline models in all three modes of unpaired I2I translation: single-modal,\nmulti-modal, and even single-image translation. This new loss can easily be\nintegrated into existing network architectures and thus allows wide\napplicability.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 02:13:30 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Zheng", "Chuanxia", ""], ["Cham", "Tat-Jen", ""], ["Cai", "Jianfei", ""]]}, {"id": "2104.00858", "submitter": "Feng Liu", "authors": "Feng Liu, Luan Tran, Xiaoming Liu", "title": "Fully Understanding Generic Objects: Modeling, Segmentation, and\n  Reconstruction", "comments": "To appear in CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Inferring 3D structure of a generic object from a 2D image is a long-standing\nobjective of computer vision. Conventional approaches either learn completely\nfrom CAD-generated synthetic data, which have difficulty in inference from real\nimages, or generate 2.5D depth image via intrinsic decomposition, which is\nlimited compared to the full 3D reconstruction. One fundamental challenge lies\nin how to leverage numerous real 2D images without any 3D ground truth. To\naddress this issue, we take an alternative approach with semi-supervised\nlearning. That is, for a 2D image of a generic object, we decompose it into\nlatent representations of category, shape and albedo, lighting and camera\nprojection matrix, decode the representations to segmented 3D shape and albedo\nrespectively, and fuse these components to render an image well approximating\nthe input image. Using a category-adaptive 3D joint occupancy field (JOF), we\nshow that the complete shape and albedo modeling enables us to leverage real 2D\nimages in both modeling and model fitting. The effectiveness of our approach is\ndemonstrated through superior 3D reconstruction from a single image, being\neither synthetic or real, and shape segmentation.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 02:39:29 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Liu", "Feng", ""], ["Tran", "Luan", ""], ["Liu", "Xiaoming", ""]]}, {"id": "2104.00862", "submitter": "Lianghua Huang Dr.", "authors": "Lianghua Huang, Yu Liu, Bin Wang, Pan Pan, Yinghui Xu, Rong Jin", "title": "Self-supervised Video Representation Learning by Context and Motion\n  Decoupling", "comments": "Accepted by CVPR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key challenge in self-supervised video representation learning is how to\neffectively capture motion information besides context bias. While most\nexisting works implicitly achieve this with video-specific pretext tasks (e.g.,\npredicting clip orders, time arrows, and paces), we develop a method that\nexplicitly decouples motion supervision from context bias through a carefully\ndesigned pretext task. Specifically, we take the keyframes and motion vectors\nin compressed videos (e.g., in H.264 format) as the supervision sources for\ncontext and motion, respectively, which can be efficiently extracted at over\n500 fps on the CPU. Then we design two pretext tasks that are jointly\noptimized: a context matching task where a pairwise contrastive loss is cast\nbetween video clip and keyframe features; and a motion prediction task where\nclip features, passed through an encoder-decoder network, are used to estimate\nmotion features in a near future. These two tasks use a shared video backbone\nand separate MLP heads. Experiments show that our approach improves the quality\nof the learned video representation over previous works, where we obtain\nabsolute gains of 16.0% and 11.1% in video retrieval recall on UCF101 and\nHMDB51, respectively. Moreover, we find the motion prediction to be a strong\nregularization for video networks, where using it as an auxiliary task improves\nthe accuracy of action recognition with a margin of 7.4%~13.8%.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 02:47:34 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Huang", "Lianghua", ""], ["Liu", "Yu", ""], ["Wang", "Bin", ""], ["Pan", "Pan", ""], ["Xu", "Yinghui", ""], ["Jin", "Rong", ""]]}, {"id": "2104.00868", "submitter": "Jaime Caballero", "authors": "Jaime Caballero, Francisco Vergara, Randal Miranda, Jos\\'e Serrac\\'in", "title": "Inference of Recyclable Objects with Convolutional Neural Networks", "comments": "11 pages, preprint version, comments are welcome!", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Population growth in the last decades has resulted in the production of about\n2.01 billion tons of municipal waste per year. The current waste management\nsystems are not capable of providing adequate solutions for the disposal and\nuse of these wastes. Recycling and reuse have proven to be a solution to the\nproblem, but large-scale waste segregation is a tedious task and on a small\nscale it depends on public awareness. This research used convolutional neural\nnetworks and computer vision to develop a tool for the automation of solid\nwaste sorting. The Fotini10k dataset was constructed, which has more than\n10,000 images divided into the categories of 'plastic bottles', 'aluminum cans'\nand 'paper and cardboard'. ResNet50, MobileNetV1 and MobileNetV2 were retrained\nwith ImageNet weights on the Fotini10k dataset. As a result, top-1 accuracy of\n99% was obtained in the test dataset with all three networks. To explore the\npossible use of these networks in mobile applications, the three nets were\nquantized in float16 weights. By doing so, it was possible to obtain inference\ntimes twice as low for Raspberry Pi and three times as low for computer\nprocessing units. It was also possible to reduce the size of the networks by\nhalf. When quantizing the top-1 accuracy of 99% was maintained with all three\nnetworks. When quantizing MobileNetV2 to int-8, it obtained a top-1 accuracy of\n97%.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 03:13:46 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Caballero", "Jaime", ""], ["Vergara", "Francisco", ""], ["Miranda", "Randal", ""], ["Serrac\u00edn", "Jos\u00e9", ""]]}, {"id": "2104.00875", "submitter": "Zilong Huang", "authors": "Zilong Huang, Wentian Hao, Xinggang Wang, Mingyuan Tao, Jianqiang\n  Huang, Wenyu Liu, Xian-Sheng Hua", "title": "Half-Real Half-Fake Distillation for Class-Incremental Semantic\n  Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Despite their success for semantic segmentation, convolutional neural\nnetworks are ill-equipped for incremental learning, \\ie, adapting the original\nsegmentation model as new classes are available but the initial training data\nis not retained. Actually, they are vulnerable to catastrophic forgetting\nproblem. We try to address this issue by \"inverting\" the trained segmentation\nnetwork to synthesize input images starting from random noise. To avoid setting\ndetailed pixel-wise segmentation maps as the supervision manually, we propose\nthe SegInversion to synthesize images using the image-level labels. To increase\nthe diversity of synthetic images, the Scale-Aware Aggregation module is\nintegrated into SegInversion for controlling the scale (the number of pixels)\nof synthetic objects. Along with real images of new classes, the synthesized\nimages will be fed into the distillation-based framework to train the new\nsegmentation model which retains the information about previously learned\nclasses, whilst updating the current model to learn the new ones. The proposed\nmethod significantly outperforms other incremental learning methods and obtains\nstate-of-the-art performance on the PASCAL VOC 2012 and ADE20K datasets. The\ncode and models will be made publicly available.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 03:47:16 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Huang", "Zilong", ""], ["Hao", "Wentian", ""], ["Wang", "Xinggang", ""], ["Tao", "Mingyuan", ""], ["Huang", "Jianqiang", ""], ["Liu", "Wenyu", ""], ["Hua", "Xian-Sheng", ""]]}, {"id": "2104.00877", "submitter": "Xiaotian Chen", "authors": "Xiaotian Chen, Yuwang Wang, Xuejin Chen, Wenjun Zeng", "title": "S2R-DepthNet: Learning a Generalizable Depth-specific Structural\n  Representation", "comments": "Accepted by CVPR2021(oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human can infer the 3D geometry of a scene from a sketch instead of a\nrealistic image, which indicates that the spatial structure plays a fundamental\nrole in understanding the depth of scenes. We are the first to explore the\nlearning of a depth-specific structural representation, which captures the\nessential feature for depth estimation and ignores irrelevant style\ninformation. Our S2R-DepthNet (Synthetic to Real DepthNet) can be well\ngeneralized to unseen real-world data directly even though it is only trained\non synthetic data. S2R-DepthNet consists of: a) a Structure Extraction (STE)\nmodule which extracts a domaininvariant structural representation from an image\nby disentangling the image into domain-invariant structure and domain-specific\nstyle components, b) a Depth-specific Attention (DSA) module, which learns\ntask-specific knowledge to suppress depth-irrelevant structures for better\ndepth estimation and generalization, and c) a depth prediction module (DP) to\npredict depth from the depth-specific representation. Without access of any\nreal-world images, our method even outperforms the state-of-the-art\nunsupervised domain adaptation methods which use real-world images of the\ntarget domain for training. In addition, when using a small amount of labeled\nreal-world data, we achieve the state-ofthe-art performance under the\nsemi-supervised setting. The code and trained models are available at\nhttps://github.com/microsoft/S2R-DepthNet.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 03:55:41 GMT"}, {"version": "v2", "created": "Tue, 15 Jun 2021 07:24:40 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Chen", "Xiaotian", ""], ["Wang", "Yuwang", ""], ["Chen", "Xuejin", ""], ["Zeng", "Wenjun", ""]]}, {"id": "2104.00885", "submitter": "Yousong Zhu", "authors": "Tong Wang, Yousong Zhu, Chaoyang Zhao, Wei Zeng, Jinqiao Wang and Ming\n  Tang", "title": "Adaptive Class Suppression Loss for Long-Tail Object Detection", "comments": "CVPR2021 camera ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To address the problem of long-tail distribution for the large vocabulary\nobject detection task, existing methods usually divide the whole categories\ninto several groups and treat each group with different strategies. These\nmethods bring the following two problems. One is the training inconsistency\nbetween adjacent categories of similar sizes, and the other is that the learned\nmodel is lack of discrimination for tail categories which are semantically\nsimilar to some of the head categories. In this paper, we devise a novel\nAdaptive Class Suppression Loss (ACSL) to effectively tackle the above problems\nand improve the detection performance of tail categories. Specifically, we\nintroduce a statistic-free perspective to analyze the long-tail distribution,\nbreaking the limitation of manual grouping. According to this perspective, our\nACSL adjusts the suppression gradients for each sample of each class\nadaptively, ensuring the training consistency and boosting the discrimination\nfor rare categories. Extensive experiments on long-tail datasets LVIS and Open\nImages show that the our ACSL achieves 5.18% and 5.2% improvements with\nResNet50-FPN, and sets a new state of the art. Code and models are available at\nhttps://github.com/CASIA-IVA-Lab/ACSL.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 05:12:31 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Wang", "Tong", ""], ["Zhu", "Yousong", ""], ["Zhao", "Chaoyang", ""], ["Zeng", "Wei", ""], ["Wang", "Jinqiao", ""], ["Tang", "Ming", ""]]}, {"id": "2104.00887", "submitter": "Sanghyuk Chun", "authors": "Song Park, Sanghyuk Chun, Junbum Cha, Bado Lee, Hyunjung Shim", "title": "Multiple Heads are Better than One: Few-shot Font Generation with\n  Multiple Localized Experts", "comments": "14 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A few-shot font generation (FFG) method has to satisfy two objectives: the\ngenerated images should preserve the underlying global structure of the target\ncharacter and present the diverse local reference style. Existing FFG methods\naim to disentangle content and style either by extracting a universal\nrepresentation style or extracting multiple component-wise style\nrepresentations. However, previous methods either fail to capture diverse local\nstyles or cannot be generalized to a character with unseen components, e.g.,\nunseen language systems. To mitigate the issues, we propose a novel FFG method,\nnamed Multiple Localized Experts Few-shot Font Generation Network (MX-Font).\nMX-Font extracts multiple style features not explicitly conditioned on\ncomponent labels, but automatically by multiple experts to represent different\nlocal concepts, e.g., left-side sub-glyph. Owing to the multiple experts,\nMX-Font can capture diverse local concepts and show the generalizability to\nunseen languages. During training, we utilize component labels as weak\nsupervision to guide each expert to be specialized for different local\nconcepts. We formulate the component assign problem to each expert as the graph\nmatching problem, and solve it by the Hungarian algorithm. We also employ the\nindependence loss and the content-style adversarial loss to impose the\ncontent-style disentanglement. In our experiments, MX-Font outperforms previous\nstate-of-the-art FFG methods in the Chinese generation and cross-lingual, e.g.,\nChinese to Korean, generation. Source code is available at\nhttps://github.com/clovaai/mxfont.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 05:20:51 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Park", "Song", ""], ["Chun", "Sanghyuk", ""], ["Cha", "Junbum", ""], ["Lee", "Bado", ""], ["Shim", "Hyunjung", ""]]}, {"id": "2104.00889", "submitter": "Mayank Patwari", "authors": "Wooram Kang, Mayank Patwari", "title": "Low Dose Helical CBCT denoising by using domain filtering with deep\n  reinforcement learning", "comments": "Research project report. 5 pages, 6 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Cone Beam Computed Tomography(CBCT) is a now known method to conduct CT\nimaging. Especially, The Low Dose CT imaging is one of possible options to\nprotect organs of patients when conducting CT imaging. Therefore Low Dose CT\nimaging can be an alternative instead of Standard dose CT imaging. However Low\nDose CT imaging has a fundamental issue with noises within results compared to\nStandard Dose CT imaging. Currently, there are lots of attempts to erase the\nnoises. Most of methods with artificial intelligence have many parameters and\nunexplained layers or a kind of black-box methods. Therefore, our research has\npurposes related to these issues. Our approach has less parameters than usual\nmethods by having Iterative learn-able bilateral filtering approach with Deep\nreinforcement learning. And we applied The Iterative learn-able filtering\napproach with deep reinforcement learning to sinograms and reconstructed volume\ndomains. The method and the results of the method can be much more explainable\nthan The other black box AI approaches. And we applied the method to Helical\nCone Beam Computed Tomography(CBCT), which is the recent CBCT trend. We tested\nthis method with on 2 abdominal scans(L004, L014) from Mayo Clinic TCIA\ndataset. The results and the performances of our approach overtake the results\nof the other previous methods.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 05:28:04 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Kang", "Wooram", ""], ["Patwari", "Mayank", ""]]}, {"id": "2104.00902", "submitter": "Jongyoun Noh", "authors": "Jongyoun Noh, Sanghoon Lee, Bumsub Ham", "title": "HVPR: Hybrid Voxel-Point Representation for Single-stage 3D Object\n  Detection", "comments": "Accepted to CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of 3D object detection, that is, estimating 3D object\nbounding boxes from point clouds. 3D object detection methods exploit either\nvoxel-based or point-based features to represent 3D objects in a scene.\nVoxel-based features are efficient to extract, while they fail to preserve\nfine-grained 3D structures of objects. Point-based features, on the other hand,\nrepresent the 3D structures more accurately, but extracting these features is\ncomputationally expensive. We introduce in this paper a novel single-stage 3D\ndetection method having the merit of both voxel-based and point-based features.\nTo this end, we propose a new convolutional neural network (CNN) architecture,\ndubbed HVPR, that integrates both features into a single 3D representation\neffectively and efficiently. Specifically, we augment the point-based features\nwith a memory module to reduce the computational cost. We then aggregate the\nfeatures in the memory, semantically similar to each voxel-based one, to obtain\na hybrid 3D representation in a form of a pseudo image, allowing to localize 3D\nobjects in a single stage efficiently. We also propose an Attentive Multi-scale\nFeature Module (AMFM) that extracts scale-aware features considering the sparse\nand irregular patterns of point clouds. Experimental results on the KITTI\ndataset demonstrate the effectiveness and efficiency of our approach, achieving\na better compromise in terms of speed and accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 06:34:49 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Noh", "Jongyoun", ""], ["Lee", "Sanghoon", ""], ["Ham", "Bumsub", ""]]}, {"id": "2104.00903", "submitter": "Junghyup Lee", "authors": "Junghyup Lee, Dohyung Kim, Bumsub Ham", "title": "Network Quantization with Element-wise Gradient Scaling", "comments": "Accepted to CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network quantization aims at reducing bit-widths of weights and/or\nactivations, particularly important for implementing deep neural networks with\nlimited hardware resources. Most methods use the straight-through estimator\n(STE) to train quantized networks, which avoids a zero-gradient problem by\nreplacing a derivative of a discretizer (i.e., a round function) with that of\nan identity function. Although quantized networks exploiting the STE have shown\ndecent performance, the STE is sub-optimal in that it simply propagates the\nsame gradient without considering discretization errors between inputs and\noutputs of the discretizer. In this paper, we propose an element-wise gradient\nscaling (EWGS), a simple yet effective alternative to the STE, training a\nquantized network better than the STE in terms of stability and accuracy. Given\na gradient of the discretizer output, EWGS adaptively scales up or down each\ngradient element, and uses the scaled gradient as the one for the discretizer\ninput to train quantized networks via backpropagation. The scaling is performed\ndepending on both the sign of each gradient element and an error between the\ncontinuous input and discrete output of the discretizer. We adjust a scaling\nfactor adaptively using Hessian information of a network. We show extensive\nexperimental results on the image classification datasets, including CIFAR-10\nand ImageNet, with diverse network architectures under a wide range of\nbit-width settings, demonstrating the effectiveness of our method.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 06:34:53 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Lee", "Junghyup", ""], ["Kim", "Dohyung", ""], ["Ham", "Bumsub", ""]]}, {"id": "2104.00905", "submitter": "Youngmin Oh", "authors": "Youngmin Oh, Beomjun Kim, Bumsub Ham", "title": "Background-Aware Pooling and Noise-Aware Loss for Weakly-Supervised\n  Semantic Segmentation", "comments": "Accepted to CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of weakly-supervised semantic segmentation (WSSS)\nusing bounding box annotations. Although object bounding boxes are good\nindicators to segment corresponding objects, they do not specify object\nboundaries, making it hard to train convolutional neural networks (CNNs) for\nsemantic segmentation. We find that background regions are perceptually\nconsistent in part within an image, and this can be leveraged to discriminate\nforeground and background regions inside object bounding boxes. To implement\nthis idea, we propose a novel pooling method, dubbed background-aware pooling\n(BAP), that focuses more on aggregating foreground features inside the bounding\nboxes using attention maps. This allows to extract high-quality pseudo\nsegmentation labels to train CNNs for semantic segmentation, but the labels\nstill contain noise especially at object boundaries. To address this problem,\nwe also introduce a noise-aware loss (NAL) that makes the networks less\nsusceptible to incorrect labels. Experimental results demonstrate that learning\nwith our pseudo labels already outperforms state-of-the-art weakly- and\nsemi-supervised methods on the PASCAL VOC 2012 dataset, and the NAL further\nboosts the performance.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 06:38:41 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Oh", "Youngmin", ""], ["Kim", "Beomjun", ""], ["Ham", "Bumsub", ""]]}, {"id": "2104.00912", "submitter": "Frederic Le Mouel", "authors": "Michael Puentes (UIS), Diana Novoa, John Delgado Nivia (UTS), Carlos\n  Barrios Hern\\'andez (UIS), Oscar Carrillo (DYNAMID, CPE), Fr\\'ed\\'eric Le\n  Mou\\\"el (DYNAMID)", "title": "Datacentric analysis to reduce pedestrians accidents: A case study in\n  Colombia", "comments": null, "journal-ref": "International Conference on Sustainable Smart Cities and\n  Territories (SSCt2021), Apr 2021, Doha, Qatar", "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since 2012, in a case-study in Bucaramanga-Colombia, 179 pedestrians died in\ncar accidents, and another 2873 pedestrians were injured. Each day, at least\none passerby is involved in a tragedy. Knowing the causes to decrease accidents\nis crucial, and using system-dynamics to reproduce the collisions' events is\ncritical to prevent further accidents. This work implements simulations to save\nlives by reducing the city's accidental rate and suggesting new safety policies\nto implement. Simulation's inputs are video recordings in some areas of the\ncity. Deep Learning analysis of the images results in the segmentation of the\ndifferent objects in the scene, and an interaction model identifies the primary\nreasons which prevail in the pedestrians or vehicles' behaviours. The first and\nmost efficient safety policy to implement-validated by our simulations-would be\nto build speed bumps in specific places before the crossings reducing the\naccident rate by 80%.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 06:59:50 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Puentes", "Michael", "", "UIS"], ["Novoa", "Diana", "", "UTS"], ["Nivia", "John Delgado", "", "UTS"], ["Hern\u00e1ndez", "Carlos Barrios", "", "UIS"], ["Carrillo", "Oscar", "", "DYNAMID, CPE"], ["Mou\u00ebl", "Fr\u00e9d\u00e9ric Le", "", "DYNAMID"]]}, {"id": "2104.00921", "submitter": "Kuan Zhu", "authors": "Kuan Zhu, Haiyun Guo, Shiliang Zhang, Yaowei Wang, Gaopan Huang,\n  Honglin Qiao, Jing Liu, Jinqiao Wang, Ming Tang", "title": "AAformer: Auto-Aligned Transformer for Person Re-Identification", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transformer is showing its superiority over convolutional architectures in\nmany vision tasks like image classification and object detection. However, the\nlacking of an explicit alignment mechanism limits its capability in person\nre-identification (re-ID), in which there are inevitable misalignment issues\ncaused by pose/viewpoints variations, etc. On the other hand, the alignment\nparadigm of convolutional neural networks does not perform well in Transformer\nin our experiments. To address this problem, we develop a novel alignment\nframework for Transformer through adding the learnable vectors of \"part tokens\"\nto learn the part representations and integrating the part alignment into the\nself-attention. A part token only interacts with a subset of patch embeddings\nand learns to represent this subset. Based on the framework, we design an\nonline Auto-Aligned Transformer (AAformer) to adaptively assign the patch\nembeddings of the same semantics to the identical part token in the running\ntime. The part tokens can be regarded as the part prototypes, and a fast\nvariant of Sinkhorn-Knopp algorithm is employed to cluster the patch embeddings\nto part tokens online. AAformer can be viewed as a new principled formulation\nfor simultaneously learning both part alignment and part representations.\nExtensive experiments validate the effectiveness of part tokens and the\nsuperiority of AAformer over various state-of-the-art CNN-based methods. Our\ncodes will be released.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 08:00:25 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Zhu", "Kuan", ""], ["Guo", "Haiyun", ""], ["Zhang", "Shiliang", ""], ["Wang", "Yaowei", ""], ["Huang", "Gaopan", ""], ["Qiao", "Honglin", ""], ["Liu", "Jing", ""], ["Wang", "Jinqiao", ""], ["Tang", "Ming", ""]]}, {"id": "2104.00924", "submitter": "Yong Man Ro", "authors": "Sangmin Lee, Hak Gu Kim, Dae Hwi Choi, Hyung-Il Kim, Yong Man Ro", "title": "Video Prediction Recalling Long-term Motion Context via Memory Alignment\n  Learning", "comments": "CVPR 2021 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our work addresses long-term motion context issues for predicting future\nframes. To predict the future precisely, it is required to capture which\nlong-term motion context (e.g., walking or running) the input motion (e.g., leg\nmovement) belongs to. The bottlenecks arising when dealing with the long-term\nmotion context are: (i) how to predict the long-term motion context naturally\nmatching input sequences with limited dynamics, (ii) how to predict the\nlong-term motion context with high-dimensionality (e.g., complex motion). To\naddress the issues, we propose novel motion context-aware video prediction. To\nsolve the bottleneck (i), we introduce a long-term motion context memory\n(LMC-Memory) with memory alignment learning. The proposed memory alignment\nlearning enables to store long-term motion contexts into the memory and to\nmatch them with sequences including limited dynamics. As a result, the\nlong-term context can be recalled from the limited input sequence. In addition,\nto resolve the bottleneck (ii), we propose memory query decomposition to store\nlocal motion context (i.e., low-dimensional dynamics) and recall the suitable\nlocal context for each local part of the input individually. It enables to\nboost the alignment effects of the memory. Experimental results show that the\nproposed method outperforms other sophisticated RNN-based methods, especially\nin long-term condition. Further, we validate the effectiveness of the proposed\nnetwork designs by conducting ablation studies and memory feature analysis. The\nsource code of this work is available.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 08:05:58 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Lee", "Sangmin", ""], ["Kim", "Hak Gu", ""], ["Choi", "Dae Hwi", ""], ["Kim", "Hyung-Il", ""], ["Ro", "Yong Man", ""]]}, {"id": "2104.00925", "submitter": "Dmitry V. Dylov", "authors": "Iaroslav Bespalov, Nazar Buzun, Oleg Kachan and Dmitry V. Dylov", "title": "Data Augmentation with Manifold Barycenters", "comments": "11 pages, 4 figures, 3 tables. I.B., N.B., O.K. contributed equally.\n  D.V.D. is the corresponding author", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The training of Generative Adversarial Networks (GANs) requires a large\namount of data, stimulating the development of new data augmentation methods to\nalleviate the challenge. Oftentimes, these methods either fail to produce\nenough new data or expand the dataset beyond the original knowledge domain. In\nthis paper, we propose a new way of representing the available knowledge in the\nmanifold of data barycenters. Such a representation allows performing data\naugmentation based on interpolation between the nearest data elements using\nWasserstein distance. The proposed method finds cliques in the\nnearest-neighbors graph and, at each sampling iteration, randomly draws one\nclique to compute the Wasserstein barycenter with random uniform weights. These\nbarycenters then become the new natural-looking elements that one could add to\nthe dataset. We apply this approach to the problem of landmarks detection and\naugment the available landmarks data within the dataset. Additionally, the idea\nis validated on cardiac data for the task of medical segmentation. Our approach\nreduces the overfitting and improves the quality metrics both beyond the\noriginal data outcome and beyond the result obtained with classical\naugmentation methods.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 08:07:21 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Bespalov", "Iaroslav", ""], ["Buzun", "Nazar", ""], ["Kachan", "Oleg", ""], ["Dylov", "Dmitry V.", ""]]}, {"id": "2104.00926", "submitter": "Theo Jaunet", "authors": "Theo Jaunet, Corentin Kervadec, Romain Vuillemot, Grigory Antipov,\n  Moez Baccouche and Christian Wolf", "title": "VisQA: X-raying Vision and Language Reasoning in Transformers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual Question Answering systems target answering open-ended textual\nquestions given input images. They are a testbed for learning high-level\nreasoning with a primary use in HCI, for instance assistance for the visually\nimpaired. Recent research has shown that state-of-the-art models tend to\nproduce answers exploiting biases and shortcuts in the training data, and\nsometimes do not even look at the input image, instead of performing the\nrequired reasoning steps. We present VisQA, a visual analytics tool that\nexplores this question of reasoning vs. bias exploitation. It exposes the key\nelement of state-of-the-art neural models -- attention maps in transformers.\nOur working hypothesis is that reasoning steps leading to model predictions are\nobservable from attention distributions, which are particularly useful for\nvisualization. The design process of VisQA was motivated by well-known bias\nexamples from the fields of deep learning and vision-language reasoning and\nevaluated in two ways. First, as a result of a collaboration of three fields,\nmachine learning, vision and language reasoning, and data analytics, the work\nlead to a better understanding of bias exploitation of neural models for VQA,\nwhich eventually resulted in an impact on its design and training through the\nproposition of a method for the transfer of reasoning patterns from an oracle\nmodel. Second, we also report on the design of VisQA, and a goal-oriented\nevaluation of VisQA targeting the analysis of a model decision process from\nmultiple experts, providing evidence that it makes the inner workings of models\naccessible to users.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 08:08:25 GMT"}, {"version": "v2", "created": "Tue, 20 Jul 2021 09:57:29 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Jaunet", "Theo", ""], ["Kervadec", "Corentin", ""], ["Vuillemot", "Romain", ""], ["Antipov", "Grigory", ""], ["Baccouche", "Moez", ""], ["Wolf", "Christian", ""]]}, {"id": "2104.00946", "submitter": "Tianjiao Li", "authors": "Tianjiao Li and Jun Liu and Wei Zhang and Yun Ni and Wenqian Wang and\n  Zhiheng Li", "title": "UAV-Human: A Large Benchmark for Human Behavior Understanding with\n  Unmanned Aerial Vehicles", "comments": "Accepted by CVPR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human behavior understanding with unmanned aerial vehicles (UAVs) is of great\nsignificance for a wide range of applications, which simultaneously brings an\nurgent demand of large, challenging, and comprehensive benchmarks for the\ndevelopment and evaluation of UAV-based models. However, existing benchmarks\nhave limitations in terms of the amount of captured data, types of data\nmodalities, categories of provided tasks, and diversities of subjects and\nenvironments. Here we propose a new benchmark - UAVHuman - for human behavior\nunderstanding with UAVs, which contains 67,428 multi-modal video sequences and\n119 subjects for action recognition, 22,476 frames for pose estimation, 41,290\nframes and 1,144 identities for person re-identification, and 22,263 frames for\nattribute recognition. Our dataset was collected by a flying UAV in multiple\nurban and rural districts in both daytime and nighttime over three months,\nhence covering extensive diversities w.r.t subjects, backgrounds,\nilluminations, weathers, occlusions, camera motions, and UAV flying attitudes.\nSuch a comprehensive and challenging benchmark shall be able to promote the\nresearch of UAV-based human behavior understanding, including action\nrecognition, pose estimation, re-identification, and attribute recognition.\nFurthermore, we propose a fisheye-based action recognition method that\nmitigates the distortions in fisheye videos via learning unbounded\ntransformations guided by flat RGB videos. Experiments show the efficacy of our\nmethod on the UAV-Human dataset. The project page:\nhttps://github.com/SUTDCV/UAV-Human\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 08:54:04 GMT"}, {"version": "v2", "created": "Mon, 12 Apr 2021 08:23:47 GMT"}, {"version": "v3", "created": "Sun, 11 Jul 2021 15:49:25 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Li", "Tianjiao", ""], ["Liu", "Jun", ""], ["Zhang", "Wei", ""], ["Ni", "Yun", ""], ["Wang", "Wenqian", ""], ["Li", "Zhiheng", ""]]}, {"id": "2104.00947", "submitter": "Xuelun Shen", "authors": "Xuelun Shen, Cheng Wang, Xin Li, Qian Hu, Jingyi Zhang", "title": "A Detector-oblivious Multi-arm Network for Keypoint Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a matching network to establish point correspondence\nbetween images. We propose a Multi-Arm Network (MAN) to learn region overlap\nand depth, which can greatly improve the keypoint matching robustness while\nbringing little computational cost during the inference stage. Another design\nthat makes this framework different from many existing learning based pipelines\nthat require re-training when a different keypoint detector is adopted, our\nnetwork can directly work with different keypoint detectors without such a\ntime-consuming re-training process. Comprehensive experiments conducted on\noutdoor and indoor datasets demonstrated that our proposed MAN outperforms\nstate-of-the-art methods. Code will be made publicly available.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 08:55:04 GMT"}, {"version": "v2", "created": "Mon, 5 Apr 2021 05:08:48 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Shen", "Xuelun", ""], ["Wang", "Cheng", ""], ["Li", "Xin", ""], ["Hu", "Qian", ""], ["Zhang", "Jingyi", ""]]}, {"id": "2104.00953", "submitter": "Yifan Yao", "authors": "Ze Ma, Yifan Yao, Pan Ji, Chao Ma", "title": "Learning Transferable Kinematic Dictionary for 3D Human Pose and Shape\n  Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating 3D human pose and shape from a single image is highly\nunder-constrained. To address this ambiguity, we propose a novel prior, namely\nkinematic dictionary, which explicitly regularizes the solution space of\nrelative 3D rotations of human joints in the kinematic tree. Integrated with a\nstatistical human model and a deep neural network, our method achieves\nend-to-end 3D reconstruction without the need of using any shape annotations\nduring the training of neural networks. The kinematic dictionary bridges the\ngap between in-the-wild images and 3D datasets, and thus facilitates end-to-end\ntraining across all types of datasets. The proposed method achieves competitive\nresults on large-scale datasets including Human3.6M, MPI-INF-3DHP, and LSP,\nwhile running in real-time given the human bounding boxes.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 09:24:29 GMT"}, {"version": "v2", "created": "Wed, 21 Apr 2021 00:52:19 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Ma", "Ze", ""], ["Yao", "Yifan", ""], ["Ji", "Pan", ""], ["Ma", "Chao", ""]]}, {"id": "2104.00965", "submitter": "Jae Woong Soh", "authors": "Jae Woong Soh and Nam Ik Cho", "title": "Variational Deep Image Denoising", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Convolutional neural networks (CNNs) have shown outstanding performance on\nimage denoising with the help of large-scale datasets. Earlier methods naively\ntrained a single CNN with many pairs of clean-noisy images. However, the\nconditional distribution of the clean image given a noisy one is too\ncomplicated and diverse, so that a single CNN cannot well learn such\ndistributions. Therefore, there have also been some methods that exploit\nadditional noise level parameters or train a separate CNN for a specific noise\nlevel parameter. These methods separate the original problem into easier\nsub-problems and thus have shown improved performance than the naively trained\nCNN. In this step, we raise two questions. The first one is whether it is an\noptimal approach to relate the conditional distribution only to noise level\nparameters. The second is what if we do not have noise level information, such\nas in a real-world scenario. To answer the questions and provide a better\nsolution, we propose a novel Bayesian framework based on the variational\napproximation of objective functions. This enables us to separate the\ncomplicated target distribution into simpler sub-distributions. Eventually, the\ndenoising CNN can conquer noise from each sub-distribution, which is generally\nan easier problem than the original. Experiments show that the proposed method\nprovides remarkable performance on additive white Gaussian noise (AWGN) and\nreal-noise denoising while requiring fewer parameters than recent\nstate-of-the-art denoisers.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 10:10:11 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Soh", "Jae Woong", ""], ["Cho", "Nam Ik", ""]]}, {"id": "2104.00969", "submitter": "Jiaojiao Zhao", "authors": "Jiaojiao Zhao, Xinyu Li, Chunhui Liu, Shuai Bing, Hao Chen, Cees G.M.\n  Snoek, Joseph Tighe", "title": "TubeR: Tube-Transformer for Action Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose TubeR: the first transformer based network for\nend-to-end action detection, with an encoder and decoder optimized for modeling\naction tubes with variable lengths and aspect ratios. TubeR does not rely on\nhand-designed tube structures, automatically links predicted action boxes over\ntime and learns a set of tube queries related to actions. By learning action\ntube embeddings, TubeR predicts more precise action tubes with flexible spatial\nand temporal extents. Our experiments demonstrate TubeR achieves\nstate-of-the-art among single-stream methods on UCF101-24 and J-HMDB. TubeR\noutperforms existing one-model methods on AVA and is even competitive with the\ntwo-model methods. Moreover, we observe TubeR has the potential on tracking\nactors with different actions, which will foster future research in long-range\nvideo understanding.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 10:21:22 GMT"}, {"version": "v2", "created": "Fri, 9 Apr 2021 12:22:14 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Zhao", "Jiaojiao", ""], ["Li", "Xinyu", ""], ["Liu", "Chunhui", ""], ["Bing", "Shuai", ""], ["Chen", "Hao", ""], ["Snoek", "Cees G. M.", ""], ["Tighe", "Joseph", ""]]}, {"id": "2104.00980", "submitter": "Mobarakol Islam", "authors": "Mobarakol Islam, V Jeya Maria Jose, Hongliang Ren", "title": "Glioma Prognosis: Segmentation of the Tumor and Survival Prediction\n  using Shape, Geometric and Clinical Information", "comments": "MICCAI-BrainLes Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Segmentation of brain tumor from magnetic resonance imaging (MRI) is a vital\nprocess to improve diagnosis, treatment planning and to study the difference\nbetween subjects with tumor and healthy subjects. In this paper, we exploit a\nconvolutional neural network (CNN) with hypercolumn technique to segment tumor\nfrom healthy brain tissue. Hypercolumn is the concatenation of a set of vectors\nwhich form by extracting convolutional features from multiple layers. Proposed\nmodel integrates batch normalization (BN) approach with hypercolumn. BN layers\nhelp to alleviate the internal covariate shift during stochastic gradient\ndescent (SGD) training by zero-mean and unit variance of each mini-batch.\nSurvival Prediction is done by first extracting features(Geometric, Fractal,\nand Histogram) from the segmented brain tumor data. Then, the number of days of\noverall survival is predicted by implementing regression on the extracted\nfeatures using an artificial neural network (ANN). Our model achieves a mean\ndice score of 89.78%, 82.53% and 76.54% for the whole tumor, tumor core and\nenhancing tumor respectively in segmentation task and 67.90% in overall\nsurvival prediction task with the validation set of BraTS 2018 challenge. It\nobtains a mean dice accuracy of 87.315%, 77.04% and 70.22% for the whole tumor,\ntumor core and enhancing tumor respectively in the segmentation task and a\n46.80% in overall survival prediction task in the BraTS 2018 test data set.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 10:49:05 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Islam", "Mobarakol", ""], ["Jose", "V Jeya Maria", ""], ["Ren", "Hongliang", ""]]}, {"id": "2104.00985", "submitter": "Mobarakol Islam", "authors": "Mobarakol Islam, Vibashan VS, V Jeya Maria Jose, Navodini Wijethilake,\n  Uppal Utkarsh, Hongliang Ren", "title": "Brain Tumor Segmentation and Survival Prediction using 3D Attention UNet", "comments": "MICCAI-BrainLes Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we develop an attention convolutional neural network (CNN) to\nsegment brain tumors from Magnetic Resonance Images (MRI). Further, we predict\nthe survival rate using various machine learning methods. We adopt a 3D UNet\narchitecture and integrate channel and spatial attention with the decoder\nnetwork to perform segmentation. For survival prediction, we extract some novel\nradiomic features based on geometry, location, the shape of the segmented tumor\nand combine them with clinical information to estimate the survival duration\nfor each patient. We also perform extensive experiments to show the effect of\neach feature for overall survival (OS) prediction. The experimental results\ninfer that radiomic features such as histogram, location, and shape of the\nnecrosis region and clinical features like age are the most critical parameters\nto estimate the OS.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 11:04:40 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Islam", "Mobarakol", ""], ["VS", "Vibashan", ""], ["Jose", "V Jeya Maria", ""], ["Wijethilake", "Navodini", ""], ["Utkarsh", "Uppal", ""], ["Ren", "Hongliang", ""]]}, {"id": "2104.00990", "submitter": "Arka Sadhu", "authors": "Arka Sadhu, Tanmay Gupta, Mark Yatskar, Ram Nevatia, Aniruddha\n  Kembhavi", "title": "Visual Semantic Role Labeling for Video Understanding", "comments": "CVPR21 camera-ready including appendix. Project Page at\n  https://vidsitu.org/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new framework for understanding and representing related salient\nevents in a video using visual semantic role labeling. We represent videos as a\nset of related events, wherein each event consists of a verb and multiple\nentities that fulfill various roles relevant to that event. To study the\nchallenging task of semantic role labeling in videos or VidSRL, we introduce\nthe VidSitu benchmark, a large-scale video understanding data source with $29K$\n$10$-second movie clips richly annotated with a verb and semantic-roles every\n$2$ seconds. Entities are co-referenced across events within a movie clip and\nevents are connected to each other via event-event relations. Clips in VidSitu\nare drawn from a large collection of movies (${\\sim}3K$) and have been chosen\nto be both complex (${\\sim}4.2$ unique verbs within a video) as well as diverse\n(${\\sim}200$ verbs have more than $100$ annotations each). We provide a\ncomprehensive analysis of the dataset in comparison to other publicly available\nvideo understanding benchmarks, several illustrative baselines and evaluate a\nrange of standard video recognition models. Our code and dataset is available\nat vidsitu.org.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 11:23:22 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Sadhu", "Arka", ""], ["Gupta", "Tanmay", ""], ["Yatskar", "Mark", ""], ["Nevatia", "Ram", ""], ["Kembhavi", "Aniruddha", ""]]}, {"id": "2104.00996", "submitter": "Jiaojiao Zhao", "authors": "Jiaojiao Zhao, Cees G.M. Snoek", "title": "LiftPool: Bidirectional ConvNet Pooling", "comments": "published on ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pooling is a critical operation in convolutional neural networks for\nincreasing receptive fields and improving robustness to input variations. Most\nexisting pooling operations downsample the feature maps, which is a lossy\nprocess. Moreover, they are not invertible: upsampling a downscaled feature map\ncan not recover the lost information in the downsampling. By adopting the\nphilosophy of the classical Lifting Scheme from signal processing, we propose\nLiftPool for bidirectional pooling layers, including LiftDownPool and\nLiftUpPool. LiftDownPool decomposes a feature map into various downsized\nsub-bands, each of which contains information with different frequencies. As\nthe pooling function in LiftDownPool is perfectly invertible, by performing\nLiftDownPool backward, a corresponding up-pooling layer LiftUpPool is able to\ngenerate a refined upsampled feature map using the detail sub-bands, which is\nuseful for image-to-image translation challenges. Experiments show the proposed\nmethods achieve better results on image classification and semantic\nsegmentation, using various backbones. Moreover, LiftDownPool offers better\nrobustness to input corruptions and perturbations.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 11:46:35 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Zhao", "Jiaojiao", ""], ["Snoek", "Cees G. M.", ""]]}, {"id": "2104.01032", "submitter": "Sheng Huang", "authors": "Zeyu Wang and Sheng Huang and Zhongxin Liu and Meng Yan and Xin Xia\n  and Bei Wang and Dan Yang", "title": "Plot2API: Recommending Graphic API from Plot via Semantic Parsing Guided\n  Neural Network", "comments": "Accepted by SANER2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Plot-based Graphic API recommendation (Plot2API) is an unstudied but\nmeaningful issue, which has several important applications in the context of\nsoftware engineering and data visualization, such as the plotting guidance of\nthe beginner, graphic API correlation analysis, and code conversion for\nplotting. Plot2API is a very challenging task, since each plot is often\nassociated with multiple APIs and the appearances of the graphics drawn by the\nsame API can be extremely varied due to the different settings of the\nparameters. Additionally, the samples of different APIs also suffer from\nextremely imbalanced. Considering the lack of technologies in Plot2API, we\npresent a novel deep multi-task learning approach named Semantic Parsing Guided\nNeural Network (SPGNN) which translates the Plot2API issue as a multi-label\nimage classification and an image semantic parsing tasks for the solution. In\nSPGNN, the recently advanced Convolutional Neural Network (CNN) named\nEfficientNet is employed as the backbone network for API recommendation.\nMeanwhile, a semantic parsing module is complemented to exploit the semantic\nrelevant visual information in feature learning and eliminate the\nappearance-relevant visual information which may confuse the\nvisual-information-based API recommendation. Moreover, the recent data\naugmentation technique named random erasing is also applied for alleviating the\nimbalance of API categories. We collect plots with the graphic APIs used to\ndrawn them from Stack Overflow, and release three new Plot2API datasets\ncorresponding to the graphic APIs of R and Python programming languages for\nevaluating the effectiveness of Plot2API techniques. Extensive experimental\nresults not only demonstrate the superiority of our method over the recent deep\nlearning baselines but also show the practicability of our method in the\nrecommendation of graphic APIs.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 13:08:56 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Wang", "Zeyu", ""], ["Huang", "Sheng", ""], ["Liu", "Zhongxin", ""], ["Yan", "Meng", ""], ["Xia", "Xin", ""], ["Wang", "Bei", ""], ["Yang", "Dan", ""]]}, {"id": "2104.01070", "submitter": "Minghang He", "authors": "Minghang He, Minghui Liao, Zhibo Yang, Humen Zhong, Jun Tang, Wenqing\n  Cheng, Cong Yao, Yongpan Wang, Xiang Bai", "title": "MOST: A Multi-Oriented Scene Text Detector with Localization Refinement", "comments": "Accepted by CVPR21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past few years, the field of scene text detection has progressed\nrapidly that modern text detectors are able to hunt text in various challenging\nscenarios. However, they might still fall short when handling text instances of\nextreme aspect ratios and varying scales. To tackle such difficulties, we\npropose in this paper a new algorithm for scene text detection, which puts\nforward a set of strategies to significantly improve the quality of text\nlocalization. Specifically, a Text Feature Alignment Module (TFAM) is proposed\nto dynamically adjust the receptive fields of features based on initial raw\ndetections; a Position-Aware Non-Maximum Suppression (PA-NMS) module is devised\nto selectively concentrate on reliable raw detections and exclude unreliable\nones; besides, we propose an Instance-wise IoU loss for balanced training to\ndeal with text instances of different scales. An extensive ablation study\ndemonstrates the effectiveness and superiority of the proposed strategies. The\nresulting text detection system, which integrates the proposed strategies with\na leading scene text detector EAST, achieves state-of-the-art or competitive\nperformance on various standard benchmarks for text detection while keeping a\nfast running speed.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 14:34:41 GMT"}, {"version": "v2", "created": "Mon, 5 Apr 2021 08:52:47 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["He", "Minghang", ""], ["Liao", "Minghui", ""], ["Yang", "Zhibo", ""], ["Zhong", "Humen", ""], ["Tang", "Jun", ""], ["Cheng", "Wenqing", ""], ["Yao", "Cong", ""], ["Wang", "Yongpan", ""], ["Bai", "Xiang", ""]]}, {"id": "2104.01071", "submitter": "Dennis N\\'u\\~nez Fern\\'andez", "authors": "Dennis N\\'u\\~nez-Fern\\'andez, Lamberto Ballan, Gabriel\n  Jim\\'enez-Avalos, Jorge Coronel, Patricia Sheen, Mirko Zimic", "title": "Prediction of Tuberculosis using U-Net and segmentation techniques", "comments": "AI for Public Health Workshop at ICLR 2021. arXiv admin note: text\n  overlap with arXiv:2007.02482", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the most serious public health problems in Peru and worldwide is\nTuberculosis (TB), which is produced by a bacterium known as Mycobacterium\ntuberculosis. The purpose of this work is to facilitate and automate the\ndiagnosis of tuberculosis using the MODS method and using lens-free microscopy,\nas it is easier to calibrate and easier to use by untrained personnel compared\nto lens microscopy. Therefore, we employed a U-Net network on our collected\ndata set to perform automatic segmentation of cord shape bacterial accumulation\nand then predict tuberculosis. Our results show promising evidence for\nautomatic segmentation of TB cords, and thus good accuracy for TB prediction.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 14:35:00 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["N\u00fa\u00f1ez-Fern\u00e1ndez", "Dennis", ""], ["Ballan", "Lamberto", ""], ["Jim\u00e9nez-Avalos", "Gabriel", ""], ["Coronel", "Jorge", ""], ["Sheen", "Patricia", ""], ["Zimic", "Mirko", ""]]}, {"id": "2104.01073", "submitter": "Guojia Hou", "authors": "Xinjie Li, Guojia Hou, Kunqian Li", "title": "Enhancing Underwater Image via Adaptive Color and Contrast Enhancement,\n  and Denoising", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Images captured underwater are often characterized by low contrast, color\ndistortion, and noise. To address these visual degradations, we propose a novel\nscheme by constructing an adaptive color and contrast enhancement, and\ndenoising (ACCE-D) framework for underwater image enhancement. In the proposed\nframework, Gaussian filter and Bilateral filter are respectively employed to\ndecompose the high-frequency and low-frequency components. Benefited from this\nseparation, we utilize soft-thresholding operation to suppress the noise in the\nhigh-frequency component. Accordingly, the low-frequency component is enhanced\nby using an adaptive color and contrast enhancement (ACCE) strategy. The\nproposed ACCE is a new adaptive variational framework implemented in the HSI\ncolor space, in which we design a Gaussian weight function and a Heaviside\nfunction to adaptively adjust the role of data item and regularized item.\nMoreover, we derive a numerical solution for ACCE, and adopt a pyramid-based\nstrategy to accelerate the solving procedure. Experimental results demonstrate\nthat our strategy is effective in color correction, visibility improvement, and\ndetail revealing. Comparison with state-of-the-art techniques also validate the\nsuperiority of propose method. Furthermore, we have verified the utility of our\nproposed ACCE-D for enhancing other types of degraded scenes, including foggy\nscene, sandstorm scene and low-light scene.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 14:37:20 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Li", "Xinjie", ""], ["Hou", "Guojia", ""], ["Li", "Kunqian", ""]]}, {"id": "2104.01085", "submitter": "Antoine Fond", "authors": "Antoine Fond, Luca Del Pero, Nikola Sivacki, Marco Paladini", "title": "End-to-end learning of keypoint detection and matching for relative pose\n  estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We propose a new method for estimating the relative pose between two images,\nwhere we jointly learn keypoint detection, description extraction, matching and\nrobust pose estimation. While our architecture follows the traditional pipeline\nfor pose estimation from geometric computer vision, all steps are learnt in an\nend-to-end fashion, including feature matching. We demonstrate our method for\nthe task of visual localization of a query image within a database of images\nwith known pose. Pairwise pose estimation has many practical applications for\nrobotic mapping, navigation, and AR. For example, the display of persistent AR\nobjects in the scene relies on a precise camera localization to make the\ndigital models appear anchored to the physical environment. We train our\npipeline end-to-end specifically for the problem of visual localization. We\nevaluate our proposed approach on localization accuracy, robustness and runtime\nspeed. Our method achieves state of the art localization accuracy on the 7\nScenes dataset.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 15:16:17 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Fond", "Antoine", ""], ["Del Pero", "Luca", ""], ["Sivacki", "Nikola", ""], ["Paladini", "Marco", ""]]}, {"id": "2104.01086", "submitter": "Dan Andrei Calian", "authors": "Dan A. Calian, Florian Stimberg, Olivia Wiles, Sylvestre-Alvise\n  Rebuffi, Andras Gyorgy, Timothy Mann, Sven Gowal", "title": "Defending Against Image Corruptions Through Adversarial Augmentations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern neural networks excel at image classification, yet they remain\nvulnerable to common image corruptions such as blur, speckle noise or fog.\nRecent methods that focus on this problem, such as AugMix and DeepAugment,\nintroduce defenses that operate in expectation over a distribution of image\ncorruptions. In contrast, the literature on $\\ell_p$-norm bounded perturbations\nfocuses on defenses against worst-case corruptions. In this work, we reconcile\nboth approaches by proposing AdversarialAugment, a technique which optimizes\nthe parameters of image-to-image models to generate adversarially corrupted\naugmented images. We theoretically motivate our method and give sufficient\nconditions for the consistency of its idealized version as well as that of\nDeepAugment. Our classifiers improve upon the state-of-the-art on common image\ncorruption benchmarks conducted in expectation on CIFAR-10-C and improve\nworst-case performance against $\\ell_p$-norm bounded perturbations on both\nCIFAR-10 and ImageNet.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 15:16:39 GMT"}, {"version": "v2", "created": "Tue, 20 Apr 2021 18:30:13 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Calian", "Dan A.", ""], ["Stimberg", "Florian", ""], ["Wiles", "Olivia", ""], ["Rebuffi", "Sylvestre-Alvise", ""], ["Gyorgy", "Andras", ""], ["Mann", "Timothy", ""], ["Gowal", "Sven", ""]]}, {"id": "2104.01102", "submitter": "Ziwen Ke", "authors": "Ziwen Ke, Zhuo-Xu Cui, Wenqi Huang, Jing Cheng, Sen Jia, Haifeng Wang,\n  Xin Liu, Hairong Zheng, Leslie Ying, Yanjie Zhu, Dong Liang", "title": "Deep Manifold Learning for Dynamic MR Imaging", "comments": "17 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Purpose: To develop a deep learning method on a nonlinear manifold to explore\nthe temporal redundancy of dynamic signals to reconstruct cardiac MRI data from\nhighly undersampled measurements.\n  Methods: Cardiac MR image reconstruction is modeled as general compressed\nsensing (CS) based optimization on a low-rank tensor manifold. The nonlinear\nmanifold is designed to characterize the temporal correlation of dynamic\nsignals. Iterative procedures can be obtained by solving the optimization model\non the manifold, including gradient calculation, projection of the gradient to\ntangent space, and retraction of the tangent space to the manifold. The\niterative procedures on the manifold are unrolled to a neural network, dubbed\nas Manifold-Net. The Manifold-Net is trained using in vivo data with a\nretrospective electrocardiogram (ECG)-gated segmented bSSFP sequence.\n  Results: Experimental results at high accelerations demonstrate that the\nproposed method can obtain improved reconstruction compared with a compressed\nsensing (CS) method k-t SLR and two state-of-the-art deep learning-based\nmethods, DC-CNN and CRNN.\n  Conclusion: This work represents the first study unrolling the optimization\non manifolds into neural networks. Specifically, the designed low-rank manifold\nprovides a new technical route for applying low-rank priors in dynamic MR\nimaging.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 02:18:08 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Ke", "Ziwen", ""], ["Cui", "Zhuo-Xu", ""], ["Huang", "Wenqi", ""], ["Cheng", "Jing", ""], ["Jia", "Sen", ""], ["Wang", "Haifeng", ""], ["Liu", "Xin", ""], ["Zheng", "Hairong", ""], ["Ying", "Leslie", ""], ["Zhu", "Yanjie", ""], ["Liang", "Dong", ""]]}, {"id": "2104.01103", "submitter": "Octave Mariotti", "authors": "Octave Mariotti, Hakan Bilen", "title": "Semi-supervised Viewpoint Estimation with Geometry-aware Conditional\n  Generation", "comments": null, "journal-ref": "ECCV 2020: Computer Vision - ECCV 2020 Workshops pp 631-647", "doi": "10.1007/978-3-030-66096-3_42", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  There is a growing interest in developing computer vision methods that can\nlearn from limited supervision. In this paper, we consider the problem of\nlearning to predict camera viewpoints, where obtaining ground-truth annotations\nare expensive and require special equipment, from a limited number of labeled\nimages. We propose a semi-supervised viewpoint estimation method that can learn\nto infer viewpoint information from unlabeled image pairs, where two images\ndiffer by a viewpoint change. In particular our method learns to synthesize the\nsecond image by combining the appearance from the first one and viewpoint from\nthe second one. We demonstrate that our method significantly improves the\nsupervised techniques, especially in the low-label regime and outperforms the\nstate-of-the-art semi-supervised methods.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 15:55:27 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Mariotti", "Octave", ""], ["Bilen", "Hakan", ""]]}, {"id": "2104.01106", "submitter": "Vlad Atanasiu", "authors": "Vlad Atanasiu, Isabelle Marthot-Santaniello", "title": "Legibility Enhancement of Papyri Using Color Processing and Visual\n  Illusions: A Case Study in Critical Vision", "comments": "Article accepted with minor revisions by the International Journal on\n  Document Analysis and Recognition (IJDAR) on 2021.03.11. Open Source software\n  accessible at https://hierax.ch", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DL cs.HC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Purpose: This article develops theoretical, algorithmic, perceptual, and\ninteraction aspects of script legibility enhancement in the visible light\nspectrum for the purpose of scholarly editing of papyri texts. - Methods: Novel\nlegibility enhancement algorithms based on color processing and visual\nillusions are proposed and compared to classic methods. A user experience\nexperiment was carried out to evaluate the solutions and better understand the\nproblem on an empirical basis. - Results: (1) The proposed methods outperformed\nthe comparison methods. (2) The methods that most successfully enhanced script\nlegibility were those that leverage human perception. (3) Users exhibited a\nbroad behavioral spectrum of text-deciphering strategies, under the influence\nof factors such as personality and social conditioning, tasks and application\ndomains, expertise level and image quality, and affordances of software,\nhardware, and interfaces. No single method satisfied all factor configurations.\nTherefore, using synergetically a range of enhancement methods and interaction\nmodalities is suggested for optimal results and user satisfaction. (4) A\nparadigm of legibility enhancement for critical applications is outlined,\ncomprising the following criteria: interpreting images skeptically; approaching\nenhancement as a system problem; considering all image structures as potential\ninformation; deriving interpretations from connections across distinct spatial\nlocations; and making uncertainty and alternative interpretations explicit,\nboth visually and numerically.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 23:48:17 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Atanasiu", "Vlad", ""], ["Marthot-Santaniello", "Isabelle", ""]]}, {"id": "2104.01107", "submitter": "Felix Ambellan", "authors": "Felix Ambellan, Stefan Zachow, Christoph von Tycowicz", "title": "Geodesic B-Score for Improved Assessment of Knee Osteoarthritis", "comments": "To be published in: Proc. International Conference on Information\n  Processing in Medical Imaging (IPMI) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV math.DG stat.AP stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Three-dimensional medical imaging enables detailed understanding of\nosteoarthritis structural status. However, there remains a vast need for\nautomatic, thus, reader-independent measures that provide reliable assessment\nof subject-specific clinical outcomes. To this end, we derive a consistent\ngeneralization of the recently proposed B-score to Riemannian shape spaces. We\nfurther present an algorithmic treatment yielding simple, yet efficient\ncomputations allowing for analysis of large shape populations with several\nthousand samples. Our intrinsic formulation exhibits improved discrimination\nability over its Euclidean counterpart, which we demonstrate for predictive\nvalidity on assessing risks of total knee replacement. This result highlights\nthe potential of the geodesic B-score to enable improved personalized\nassessment and stratification for interventions.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 12:16:21 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Ambellan", "Felix", ""], ["Zachow", "Stefan", ""], ["von Tycowicz", "Christoph", ""]]}, {"id": "2104.01108", "submitter": "Huazhu Fu", "authors": "Qi Fan, Deng-Ping Fan, Huazhu Fu, Chi Keung Tang, Ling Shao, Yu-Wing\n  Tai", "title": "Group Collaborative Learning for Co-Salient Object Detection", "comments": "Accepted to CVPR 2021. Project page: https://github.com/fanq15/GCoNet\n  Note: corrected Fig 9 in this version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a novel group collaborative learning framework (GCoNet) capable of\ndetecting co-salient objects in real time (16ms), by simultaneously mining\nconsensus representations at group level based on the two necessary criteria:\n1) intra-group compactness to better formulate the consistency among co-salient\nobjects by capturing their inherent shared attributes using our novel group\naffinity module; 2) inter-group separability to effectively suppress the\ninfluence of noisy objects on the output by introducing our new group\ncollaborating module conditioning the inconsistent consensus. To learn a better\nembedding space without extra computational overhead, we explicitly employ\nauxiliary classification supervision. Extensive experiments on three\nchallenging benchmarks, i.e., CoCA, CoSOD3k, and Cosal2015, demonstrate that\nour simple GCoNet outperforms 10 cutting-edge models and achieves the new\nstate-of-the-art. We demonstrate this paper's new technical contributions on a\nnumber of important downstream computer vision applications including content\naware co-segmentation, co-localization based automatic thumbnails, etc.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 13:16:03 GMT"}, {"version": "v2", "created": "Sun, 9 May 2021 12:05:51 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Fan", "Qi", ""], ["Fan", "Deng-Ping", ""], ["Fu", "Huazhu", ""], ["Tang", "Chi Keung", ""], ["Shao", "Ling", ""], ["Tai", "Yu-Wing", ""]]}, {"id": "2104.01109", "submitter": "Philippe Burlina", "authors": "Neil Joshi and Phil Burlina", "title": "AI Fairness via Domain Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  While deep learning (DL) approaches are reaching human-level performance for\nmany tasks, including for diagnostics AI, the focus is now on challenges\npossibly affecting DL deployment, including AI privacy, domain generalization,\nand fairness. This last challenge is addressed in this study. Here we look at a\nnovel method for ensuring AI fairness with respect to protected or sensitive\nfactors. This method uses domain adaptation via training set enhancement to\ntackle bias-causing training data imbalance. More specifically, it uses\ngenerative models that allow the generation of more synthetic training samples\nfor underrepresented populations. This paper applies this method to the use\ncase of detection of age related macular degeneration (AMD). Our experiments\nshow that starting with an originally biased AMD diagnostics model the method\nhas the ability to improve fairness.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 22:55:51 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Joshi", "Neil", ""], ["Burlina", "Phil", ""]]}, {"id": "2104.01110", "submitter": "Xiaojun Chang", "authors": "Pengzhen Ren, Gang Xiao, Xiaojun Chang, Yun Xiao, Zhihui Li, and\n  Xiaojiang Chen", "title": "NAS-TC: Neural Architecture Search on Temporal Convolutions for Complex\n  Action Recognition", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the field of complex action recognition in videos, the quality of the\ndesigned model plays a crucial role in the final performance. However,\nartificially designed network structures often rely heavily on the researchers'\nknowledge and experience. Accordingly, because of the automated design of its\nnetwork structure, Neural architecture search (NAS) has achieved great success\nin the image processing field and attracted substantial research attention in\nrecent years. Although some NAS methods have reduced the number of GPU search\ndays required to single digits in the image field, directly using 3D\nconvolution to extend NAS to the video field is still likely to produce a surge\nin computing volume. To address this challenge, we propose a new processing\nframework called Neural Architecture Search- Temporal Convolutional (NAS-TC).\nOur proposed framework is divided into two phases. In the first phase, the\nclassical CNN network is used as the backbone network to complete the\ncomputationally intensive feature extraction task. In the second stage, a\nsimple stitching search to the cell is used to complete the relatively\nlightweight long-range temporal-dependent information extraction. This ensures\nour method will have more reasonable parameter assignments and can handle\nminute-level videos. Finally, we conduct sufficient experiments on multiple\nbenchmark datasets and obtain competitive recognition accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 02:02:11 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Ren", "Pengzhen", ""], ["Xiao", "Gang", ""], ["Chang", "Xiaojun", ""], ["Xiao", "Yun", ""], ["Li", "Zhihui", ""], ["Chen", "Xiaojiang", ""]]}, {"id": "2104.01111", "submitter": "Xiaojun Chang", "authors": "Xiaojun Chang, Pengzhen Ren, Pengfei Xu, Zhihui Li, Xiaojiang Chen,\n  and Alex Hauptmann", "title": "Scene Graphs: A Survey of Generations and Applications", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Scene graph is a structured representation of a scene that can clearly\nexpress the objects, attributes, and relationships between objects in the\nscene. As computer vision technology continues to develop, people are no longer\nsatisfied with simply detecting and recognizing objects in images; instead,\npeople look forward to a higher level of understanding and reasoning about\nvisual scenes. For example, given an image, we want to not only detect and\nrecognize objects in the image, but also know the relationship between objects\n(visual relationship detection), and generate a text description (image\ncaptioning) based on the image content. Alternatively, we might want the\nmachine to tell us what the little girl in the image is doing (Visual Question\nAnswering (VQA)), or even remove the dog from the image and find similar images\n(image editing and retrieval), etc. These tasks require a higher level of\nunderstanding and reasoning for image vision tasks. The scene graph is just\nsuch a powerful tool for scene understanding. Therefore, scene graphs have\nattracted the attention of a large number of researchers, and related research\nis often cross-modal, complex, and rapidly developing. However, no relatively\nsystematic survey of scene graphs exists at present. To this end, this survey\nconducts a comprehensive investigation of the current scene graph research.\nMore specifically, we first summarized the general definition of the scene\ngraph, then conducted a comprehensive and systematic discussion on the\ngeneration method of the scene graph (SGG) and the SGG with the aid of prior\nknowledge. We then investigated the main applications of scene graphs and\nsummarized the most commonly used datasets. Finally, we provide some insights\ninto the future development of scene graphs. We believe this will be a very\nhelpful foundation for future research on scene graphs.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 04:24:20 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Chang", "Xiaojun", ""], ["Ren", "Pengzhen", ""], ["Xu", "Pengfei", ""], ["Li", "Zhihui", ""], ["Chen", "Xiaojiang", ""], ["Hauptmann", "Alex", ""]]}, {"id": "2104.01122", "submitter": "Tsu-Jui Fu", "authors": "Tsu-Jui Fu, Xin Eric Wang, Scott T. Grafton, Miguel P. Eckstein,\n  William Yang Wang", "title": "Language-based Video Editing via Multi-Modal Multi-Level Transformer", "comments": "11 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video editing tools are widely used nowadays for digital design. Although the\ndemand for these tools is high, the prior knowledge required makes it difficult\nfor novices to get started. Systems that could follow natural language\ninstructions to perform automatic editing would significantly improve\naccessibility. This paper introduces the language-based video editing (LBVE)\ntask, which allows the model to edit, guided by text instruction, a source\nvideo into a target video. LBVE contains two features: 1) the scenario of the\nsource video is preserved instead of generating a completely different video;\n2) the semantic is presented differently in the target video, and all changes\nare controlled by the given instruction. We propose a Multi-Modal Multi-Level\nTransformer (M$^3$L-Transformer) to carry out LBVE. The M$^3$L-Transformer\ndynamically learns the correspondence between video perception and language\nsemantic at different levels, which benefits both the video understanding and\nvideo frame synthesis. We build three new datasets for evaluation, including\ntwo diagnostic and one from natural videos with human-labeled text. Extensive\nexperimental results show that M$^3$L-Transformer is effective for video\nediting and that LBVE can lead to a new field toward vision-and-language\nresearch.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 15:59:52 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Fu", "Tsu-Jui", ""], ["Wang", "Xin Eric", ""], ["Grafton", "Scott T.", ""], ["Eckstein", "Miguel P.", ""], ["Wang", "William Yang", ""]]}, {"id": "2104.01136", "submitter": "Matthijs Douze", "authors": "Ben Graham and Alaaeldin El-Nouby and Hugo Touvron and Pierre Stock\n  and Armand Joulin and Herv\\'e J\\'egou and Matthijs Douze", "title": "LeViT: a Vision Transformer in ConvNet's Clothing for Faster Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We design a family of image classification architectures that optimize the\ntrade-off between accuracy and efficiency in a high-speed regime. Our work\nexploits recent findings in attention-based architectures, which are\ncompetitive on highly parallel processing hardware. We revisit principles from\nthe extensive literature on convolutional neural networks to apply them to\ntransformers, in particular activation maps with decreasing resolutions. We\nalso introduce the attention bias, a new way to integrate positional\ninformation in vision transformers. As a result, we propose LeVIT: a hybrid\nneural network for fast inference image classification. We consider different\nmeasures of efficiency on different hardware platforms, so as to best reflect a\nwide range of application scenarios. Our extensive experiments empirically\nvalidate our technical choices and show they are suitable to most\narchitectures. Overall, LeViT significantly outperforms existing convnets and\nvision transformers with respect to the speed/accuracy tradeoff. For example,\nat 80% ImageNet top-1 accuracy, LeViT is 5 times faster than EfficientNet on\nCPU. We release the code at https://github.com/facebookresearch/LeViT\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 16:29:57 GMT"}, {"version": "v2", "created": "Thu, 6 May 2021 15:25:03 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Graham", "Ben", ""], ["El-Nouby", "Alaaeldin", ""], ["Touvron", "Hugo", ""], ["Stock", "Pierre", ""], ["Joulin", "Armand", ""], ["J\u00e9gou", "Herv\u00e9", ""], ["Douze", "Matthijs", ""]]}, {"id": "2104.01137", "submitter": "Spencer He", "authors": "Spencer He and Ryan Liu", "title": "Developing a New Autism Diagnosis Process Based on a Hybrid Deep\n  Learning Architecture Through Analyzing Home Videos", "comments": "11 pages, 3 figures, 4 tables Accepted by International Conference on\n  Artificial Intelligence and Machine Learning for Healthcare Applications\n  (ICAIMLHA 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Currently, every 1 in 54 children have been diagnosed with Autism Spectrum\nDisorder (ASD), which is 178% higher than it was in 2000. An early diagnosis\nand treatment can significantly increase the chances of going off the spectrum\nand making a full recovery. With a multitude of physical and behavioral tests\nfor neurological and communication skills, diagnosing ASD is very complex,\nsubjective, time-consuming, and expensive. We hypothesize that the use of\nmachine learning analysis on facial features and social behavior can speed up\nthe diagnosis of ASD without compromising real-world performance. We propose to\ndevelop a hybrid architecture using both categorical data and image data to\nautomate traditional ASD pre-screening, which makes diagnosis a quicker and\neasier process. We created and tested a Logistic Regression model and a Linear\nSupport Vector Machine for Module 1, which classifies ADOS categorical data. A\nConvolutional Neural Network and a DenseNet network are used for module 2,\nwhich classifies video data. Finally, we combined the best performing models, a\nLinear SVM and DenseNet, using three data averaging strategies. We used a\nstandard average, weighted based on number of training data, and weighted based\non the number of ASD patients in the training data to average the results,\nthereby increasing accuracy in clinical applications. The results we obtained\nsupport our hypothesis. Our novel architecture is able to effectively automate\nASD pre-screening with a maximum weighted accuracy of 84%.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 17:30:35 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["He", "Spencer", ""], ["Liu", "Ryan", ""]]}, {"id": "2104.01148", "submitter": "Karl Stelzner", "authors": "Karl Stelzner, Kristian Kersting, Adam R. Kosiorek", "title": "Decomposing 3D Scenes into Objects via Unsupervised Volume Segmentation", "comments": "15 pages, 3 figures. For project page with videos, see\n  http://stelzner.github.io/obsurf/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present ObSuRF, a method which turns a single image of a scene into a 3D\nmodel represented as a set of Neural Radiance Fields (NeRFs), with each NeRF\ncorresponding to a different object. A single forward pass of an encoder\nnetwork outputs a set of latent vectors describing the objects in the scene.\nThese vectors are used independently to condition a NeRF decoder, defining the\ngeometry and appearance of each object. We make learning more computationally\nefficient by deriving a novel loss, which allows training NeRFs on RGB-D inputs\nwithout explicit ray marching. After confirming that the model performs equal\nor better than state of the art on three 2D image segmentation benchmarks, we\napply it to two multi-object 3D datasets: A multiview version of CLEVR, and a\nnovel dataset in which scenes are populated by ShapeNet models. We find that\nafter training ObSuRF on RGB-D views of training scenes, it is capable of not\nonly recovering the 3D geometry of a scene depicted in a single input image,\nbut also to segment it into objects, despite receiving no supervision in that\nregard.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 16:59:29 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Stelzner", "Karl", ""], ["Kersting", "Kristian", ""], ["Kosiorek", "Adam R.", ""]]}, {"id": "2104.01149", "submitter": "Navodini Wijethilake", "authors": "Mobarakol Islam, Navodini Wijethilake, Hongliang Ren", "title": "Glioblastoma Multiforme Prognosis: MRI Missing Modality Generation,\n  Segmentation and Radiogenomic Survival Prediction", "comments": "Accepted for a journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The accurate prognosis of Glioblastoma Multiforme (GBM) plays an essential\nrole in planning correlated surgeries and treatments. The conventional models\nof survival prediction rely on radiomic features using magnetic resonance\nimaging (MRI). In this paper, we propose a radiogenomic overall survival (OS)\nprediction approach by incorporating gene expression data with radiomic\nfeatures such as shape, geometry, and clinical information. We exploit TCGA\n(The Cancer Genomic Atlas) dataset and synthesize the missing MRI modalities\nusing a fully convolutional network (FCN) in a conditional Generative\nAdversarial Network (cGAN). Meanwhile, the same FCN architecture enables the\ntumor segmentation from the available and the synthesized MRI modalities. The\nproposed FCN architecture comprises octave convolution (OctConv) and a novel\ndecoder, with skip connections in spatial and channel squeeze & excitation\n(skip-scSE) block. The OctConv can process low and high-frequency features\nindividually and improve model efficiency by reducing channel-wise redundancy.\nSkip-scSE applies spatial and channel-wise excitation to signify the essential\nfeatures and reduces the sparsity in deeper layers learning parameters using\nskip connections. The proposed approaches are evaluated by comparative\nexperiments with state-of-the-art models in synthesis, segmentation, and\noverall survival (OS) prediction. We observe that adding missing MRI modality\nimproves the segmentation prediction, and expression levels of gene markers\nhave a high contribution in the GBM prognosis prediction, and fused\nradiogenomic features boost the OS estimation.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 10:11:51 GMT"}, {"version": "v2", "created": "Fri, 30 Apr 2021 14:28:26 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Islam", "Mobarakol", ""], ["Wijethilake", "Navodini", ""], ["Ren", "Hongliang", ""]]}, {"id": "2104.01198", "submitter": "Xitong Yang", "authors": "Xitong Yang, Haoqi Fan, Lorenzo Torresani, Larry Davis and Heng Wang", "title": "Beyond Short Clips: End-to-End Video-Level Learning with Collaborative\n  Memories", "comments": "CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The standard way of training video models entails sampling at each iteration\na single clip from a video and optimizing the clip prediction with respect to\nthe video-level label. We argue that a single clip may not have enough temporal\ncoverage to exhibit the label to recognize, since video datasets are often\nweakly labeled with categorical information but without dense temporal\nannotations. Furthermore, optimizing the model over brief clips impedes its\nability to learn long-term temporal dependencies. To overcome these\nlimitations, we introduce a collaborative memory mechanism that encodes\ninformation across multiple sampled clips of a video at each training\niteration. This enables the learning of long-range dependencies beyond a single\nclip. We explore different design choices for the collaborative memory to ease\nthe optimization difficulties. Our proposed framework is end-to-end trainable\nand significantly improves the accuracy of video classification at a negligible\ncomputational overhead. Through extensive experiments, we demonstrate that our\nframework generalizes to different video architectures and tasks, outperforming\nthe state of the art on both action recognition (e.g., Kinetics-400 & 700,\nCharades, Something-Something-V1) and action detection (e.g., AVA v2.1 & v2.2).\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 18:59:09 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Yang", "Xitong", ""], ["Fan", "Haoqi", ""], ["Torresani", "Lorenzo", ""], ["Davis", "Larry", ""], ["Wang", "Heng", ""]]}, {"id": "2104.01217", "submitter": "Loic Peter", "authors": "Loic Peter, Daniel C. Alexander, Caroline Magnain, Juan Eugenio\n  Iglesias", "title": "Uncertainty-Aware Annotation Protocol to Evaluate Deformable\n  Registration Algorithms", "comments": "Accepted at IEEE Transactions on Medical Imaging", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Landmark correspondences are a widely used type of gold standard in image\nregistration. However, the manual placement of corresponding points is subject\nto high inter-user variability in the chosen annotated locations and in the\ninterpretation of visual ambiguities. In this paper, we introduce a principled\nstrategy for the construction of a gold standard in deformable registration.\nOur framework: (i) iteratively suggests the most informative location to\nannotate next, taking into account its redundancy with previous annotations;\n(ii) extends traditional pointwise annotations by accounting for the spatial\nuncertainty of each annotation, which can either be directly specified by the\nuser, or aggregated from pointwise annotations from multiple experts; and (iii)\nnaturally provides a new strategy for the evaluation of deformable registration\nalgorithms. Our approach is validated on four different registration tasks. The\nexperimental results show the efficacy of suggesting annotations according to\ntheir informativeness, and an improved capacity to assess the quality of the\noutputs of registration algorithms. In addition, our approach yields, from\nsparse annotations only, a dense visualization of the errors made by a\nregistration method. The source code of our approach supporting both 2D and 3D\ndata is publicly available at\nhttps://github.com/LoicPeter/evaluation-deformable-registration.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 19:31:19 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Peter", "Loic", ""], ["Alexander", "Daniel C.", ""], ["Magnain", "Caroline", ""], ["Iglesias", "Juan Eugenio", ""]]}, {"id": "2104.01231", "submitter": "Theodoros Tsiligkaridis", "authors": "Athanasios Tsiligkaridis, Theodoros Tsiligkaridis", "title": "Misclassification-Aware Gaussian Smoothing improves Robustness against\n  Domain Shifts", "comments": "16 pages, submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep neural networks achieve high prediction accuracy when the train and test\ndistributions coincide. However, in practice various types of corruptions can\ndeviate from this setup and performance can be heavily degraded. There have\nbeen only a few methods to address generalization in presence of unexpected\ndomain shifts observed during deployment. In this paper, a\nmisclassification-aware Gaussian smoothing approach is presented to improve the\nrobustness of image classifiers against a variety of corruptions while\nmaintaining clean accuracy. The intuition behind our proposed\nmisclassification-aware objective is revealed through bounds on the local loss\ndeviation in the small-noise regime. When our method is coupled with additional\ndata augmentations, it is empirically shown to improve upon the\nstate-of-the-art in robustness and uncertainty calibration on several image\nclassification tasks.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 20:25:53 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Tsiligkaridis", "Athanasios", ""], ["Tsiligkaridis", "Theodoros", ""]]}, {"id": "2104.01257", "submitter": "Zhenzhen Weng", "authors": "Zhenzhen Weng, Mehmet Giray Ogut, Shai Limonchik, Serena Yeung", "title": "Unsupervised Discovery of the Long-Tail in Instance Segmentation Using\n  Hierarchical Self-Supervision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Instance segmentation is an active topic in computer vision that is usually\nsolved by using supervised learning approaches over very large datasets\ncomposed of object level masks. Obtaining such a dataset for any new domain can\nbe very expensive and time-consuming. In addition, models trained on certain\nannotated categories do not generalize well to unseen objects. The goal of this\npaper is to propose a method that can perform unsupervised discovery of\nlong-tail categories in instance segmentation, through learning instance\nembeddings of masked regions. Leveraging rich relationship and hierarchical\nstructure between objects in the images, we propose self-supervised losses for\nlearning mask embeddings. Trained on COCO dataset without additional\nannotations of the long-tail objects, our model is able to discover novel and\nmore fine-grained objects than the common categories in COCO. We show that the\nmodel achieves competitive quantitative results on LVIS as compared to the\nsupervised and partially supervised methods.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 22:05:03 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Weng", "Zhenzhen", ""], ["Ogut", "Mehmet Giray", ""], ["Limonchik", "Shai", ""], ["Yeung", "Serena", ""]]}, {"id": "2104.01260", "submitter": "Takuya Kiyokawa", "authors": "Takuya Kiyokawa, Hiroki Katayama, Yuya Tatsuta, Jun Takamatsu, Tsukasa\n  Ogasawara", "title": "Robotic Waste Sorter with Agile Manipulation and Quickly Trainable\n  Detector", "comments": "15 pages, 18 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Owing to human labor shortages, the automation of labor-intensive manual\nwaste-sorting is needed. The goal of automating the waste-sorting is to replace\nthe human role of robust detection and agile manipulation of the waste items by\nrobots. To achieve this, we propose three methods. First, we propose a combined\nmanipulation method using graspless push-and-drop and pick-and-release\nmanipulation. Second, we propose a robotic system that can automatically\ncollect object images to quickly train a deep neural network model. Third, we\npropose the method to mitigate the differences in the appearance of target\nobjects from two scenes: one for the dataset collection and the other for waste\nsorting in a recycling factory. If differences exist, the performance of a\ntrained waste detector could be decreased. We address differences in\nillumination and background by applying object scaling, histogram matching with\nhistogram equalization, and background synthesis to the source target-object\nimages. Via experiments in an indoor experimental workplace for waste-sorting,\nwe confirmed the proposed methods enable quickly collecting the training image\nsets for three classes of waste items, i.e., aluminum can, glass bottle, and\nplastic bottle and detecting them with higher performance than the methods that\ndo not consider the differences. We also confirmed that the proposed method\nenables the robot quickly manipulate them.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 22:19:34 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Kiyokawa", "Takuya", ""], ["Katayama", "Hiroki", ""], ["Tatsuta", "Yuya", ""], ["Takamatsu", "Jun", ""], ["Ogasawara", "Tsukasa", ""]]}, {"id": "2104.01263", "submitter": "Aatif Jiwani", "authors": "Aatif Jiwani, Shubhrakanti Ganguly, Chao Ding, Nan Zhou, and David M.\n  Chan", "title": "A Semantic Segmentation Network for Urban-Scale Building Footprint\n  Extraction Using RGB Satellite Imagery", "comments": "11 pages, 6 figures. Code available at\n  https://github.com/aatifjiwani/rgb-footprint-extract/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Urban areas consume over two-thirds of the world's energy and account for\nmore than 70 percent of global CO2 emissions. As stated in IPCC's Global\nWarming of 1.5C report, achieving carbon neutrality by 2050 requires a scalable\napproach that can be applied in a global context. Conventional methods of\ncollecting data on energy use and emissions of buildings are extremely\nexpensive and require specialized geometry information that not all cities have\nreadily available. High-quality building footprint generation from satellite\nimages can accelerate this predictive process and empower municipal\ndecision-making at scale. However, previous deep learning-based approaches use\nsupplemental data such as point cloud data, building height information, and\nmulti-band imagery - which has limited availability and is difficult to\nproduce. In this paper, we propose a modified DeeplabV3+ module with a Dilated\nResNet backbone to generate masks of building footprints from only\nthree-channel RGB satellite imagery. Furthermore, we introduce an F-Beta\nmeasure in our objective function to help the model account for skewed class\ndistributions. In addition to an F-Beta objective function, we incorporate an\nexponentially weighted boundary loss and use a cross-dataset training strategy\nto further increase the quality of predictions. As a result, we achieve\nstate-of-the-art performance across three standard benchmarks and demonstrate\nthat our RGB-only method is agnostic to the scale, resolution, and urban\ndensity of satellite imagery.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 22:32:04 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Jiwani", "Aatif", ""], ["Ganguly", "Shubhrakanti", ""], ["Ding", "Chao", ""], ["Zhou", "Nan", ""], ["Chan", "David M.", ""]]}, {"id": "2104.01268", "submitter": "Soumya Gupta", "authors": "Soumya Gupta, Sharib Ali, Louise Goldsmith, Ben Turney and Jens\n  Rittscher", "title": "Multi-class motion-based semantic segmentation for ureteroscopy and\n  laser lithotripsy", "comments": "18 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kidney stones represent a considerable burden for public health-care systems.\nUreteroscopy with laser lithotripsy has evolved as the most commonly used\ntechnique for the treatment of kidney stones. Automated segmentation of kidney\nstones and laser fiber is an important initial step to performing any automated\nquantitative analysis of the stones, particularly stone-size estimation, that\nhelps the surgeon decide if the stone requires more fragmentation. Factors such\nas turbid fluid inside the cavity, specularities, motion blur due to kidney\nmovements and camera motion, bleeding, and stone debris impact the quality of\nvision within the kidney and lead to extended operative times. To the best of\nour knowledge, this is the first attempt made towards multi-class segmentation\nin ureteroscopy and laser lithotripsy data. We propose an end-to-end CNN-based\nframework for the segmentation of stones and laser fiber. The proposed approach\nutilizes two sub-networks: HybResUNet, a version of residual U-Net, that uses\nresidual connections in the encoder path of U-Net and a DVFNet that generates\nDVF predictions which are then used to prune the prediction maps. We also\npresent ablation studies that combine dilated convolutions, recurrent and\nresidual connections, ASPP and attention gate. We propose a compound loss\nfunction that improves our segmentation performance. We have also provided an\nablation study to determine the optimal data augmentation strategy. Our\nqualitative and quantitative results illustrate that our proposed method\noutperforms SOTA methods such as UNet and DeepLabv3+ showing an improvement of\n5.2% and 15.93%, respectively, for the combined mean of DSC and JI in our\ninvivo test dataset. We also show that our proposed model generalizes better on\na new clinical dataset showing a mean improvement of 25.4%, 20%, and 11% over\nUNet, HybResUNet, and DeepLabv3+, respectively, for the same metric.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 22:47:21 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Gupta", "Soumya", ""], ["Ali", "Sharib", ""], ["Goldsmith", "Louise", ""], ["Turney", "Ben", ""], ["Rittscher", "Jens", ""]]}, {"id": "2104.01286", "submitter": "Astuti Sharma", "authors": "Astuti Sharma, Tarun Kalluri, Manmohan Chandraker", "title": "Instance Level Affinity-Based Transfer for Unsupervised Domain\n  Adaptation", "comments": "CVPR 2021 (Conference on Computer Vision and Pattern Recognition)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Domain adaptation deals with training models using large scale labeled data\nfrom a specific source domain and then adapting the knowledge to certain target\ndomains that have few or no labels. Many prior works learn domain agnostic\nfeature representations for this purpose using a global distribution alignment\nobjective which does not take into account the finer class specific structure\nin the source and target domains. We address this issue in our work and propose\nan instance affinity based criterion for source to target transfer during\nadaptation, called ILA-DA. We first propose a reliable and efficient method to\nextract similar and dissimilar samples across source and target, and utilize a\nmulti-sample contrastive loss to drive the domain alignment process. ILA-DA\nsimultaneously accounts for intra-class clustering as well as inter-class\nseparation among the categories, resulting in less noisy classifier boundaries,\nimproved transferability and increased accuracy. We verify the effectiveness of\nILA-DA by observing consistent improvements in accuracy over popular domain\nadaptation approaches on a variety of benchmark datasets and provide insights\ninto the proposed alignment approach. Code will be made publicly available at\nhttps://github.com/astuti/ILA-DA.\n", "versions": [{"version": "v1", "created": "Sat, 3 Apr 2021 01:33:14 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Sharma", "Astuti", ""], ["Kalluri", "Tarun", ""], ["Chandraker", "Manmohan", ""]]}, {"id": "2104.01291", "submitter": "Bowen Shi", "authors": "Bowen Shi, Diane Brentari, Greg Shakhnarovich, Karen Livescu", "title": "Fingerspelling Detection in American Sign Language", "comments": "CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fingerspelling, in which words are signed letter by letter, is an important\ncomponent of American Sign Language. Most previous work on automatic\nfingerspelling recognition has assumed that the boundaries of fingerspelling\nregions in signing videos are known beforehand. In this paper, we consider the\ntask of fingerspelling detection in raw, untrimmed sign language videos. This\nis an important step towards building real-world fingerspelling recognition\nsystems. We propose a benchmark and a suite of evaluation metrics, some of\nwhich reflect the effect of detection on the downstream fingerspelling\nrecognition task. In addition, we propose a new model that learns to detect\nfingerspelling via multi-task training, incorporating pose estimation and\nfingerspelling recognition (transcription) along with detection, and compare\nthis model to several alternatives. The model outperforms all alternative\napproaches across all metrics, establishing a state of the art on the\nbenchmark.\n", "versions": [{"version": "v1", "created": "Sat, 3 Apr 2021 02:11:09 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Shi", "Bowen", ""], ["Brentari", "Diane", ""], ["Shakhnarovich", "Greg", ""], ["Livescu", "Karen", ""]]}, {"id": "2104.01301", "submitter": "Palak Tiwary", "authors": "Palak Tiwary and Sanjida Ahmed", "title": "Multimedia Technology Applications and Algorithms: A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multimedia related research and development has evolved rapidly in the last\nfew years with advancements in hardware, software and network infrastructures.\nAs a result, multimedia has been integrated into domains like Healthcare and\nMedicine, Human facial feature extraction and tracking, pose recognition,\ndisparity estimation, etc. This survey gives an overview of the various\nmultimedia technologies and algorithms developed in the domains mentioned.\n", "versions": [{"version": "v1", "created": "Sat, 3 Apr 2021 03:23:06 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Tiwary", "Palak", ""], ["Ahmed", "Sanjida", ""]]}, {"id": "2104.01318", "submitter": "Zhuyu Yao", "authors": "Zhuyu Yao, Jiangbo Ai, Boxun Li, Chi Zhang", "title": "Efficient DETR: Improving End-to-End Object Detector with Dense Prior", "comments": "10 pages, 5 figures, 10 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recently proposed end-to-end transformer detectors, such as DETR and\nDeformable DETR, have a cascade structure of stacking 6 decoder layers to\nupdate object queries iteratively, without which their performance degrades\nseriously. In this paper, we investigate that the random initialization of\nobject containers, which include object queries and reference points, is mainly\nresponsible for the requirement of multiple iterations. Based on our findings,\nwe propose Efficient DETR, a simple and efficient pipeline for end-to-end\nobject detection. By taking advantage of both dense detection and sparse set\ndetection, Efficient DETR leverages dense prior to initialize the object\ncontainers and brings the gap of the 1-decoder structure and 6-decoder\nstructure. Experiments conducted on MS COCO show that our method, with only 3\nencoder layers and 1 decoder layer, achieves competitive performance with\nstate-of-the-art object detection methods. Efficient DETR is also robust in\ncrowded scenes. It outperforms modern detectors on CrowdHuman dataset by a\nlarge margin.\n", "versions": [{"version": "v1", "created": "Sat, 3 Apr 2021 06:14:24 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Yao", "Zhuyu", ""], ["Ai", "Jiangbo", ""], ["Li", "Boxun", ""], ["Zhang", "Chi", ""]]}, {"id": "2104.01325", "submitter": "Joy Hsu", "authors": "Joy Hsu, Wah Chiu, Serena Yeung", "title": "DARCNN: Domain Adaptive Region-based Convolutional Neural Network for\n  Unsupervised Instance Segmentation in Biomedical Images", "comments": "To appear at CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the biomedical domain, there is an abundance of dense, complex data where\nobjects of interest may be challenging to detect or constrained by limits of\nhuman knowledge. Labelled domain specific datasets for supervised tasks are\noften expensive to obtain, and furthermore discovery of novel distinct objects\nmay be desirable for unbiased scientific discovery. Therefore, we propose\nleveraging the wealth of annotations in benchmark computer vision datasets to\nconduct unsupervised instance segmentation for diverse biomedical datasets. The\nkey obstacle is thus overcoming the large domain shift from common to\nbiomedical images. We propose a Domain Adaptive Region-based Convolutional\nNeural Network (DARCNN), that adapts knowledge of object definition from COCO,\na large labelled vision dataset, to multiple biomedical datasets. We introduce\na domain separation module, a self-supervised representation consistency loss,\nand an augmented pseudo-labelling stage within DARCNN to effectively perform\ndomain adaptation across such large domain shifts. We showcase DARCNN's\nperformance for unsupervised instance segmentation on numerous biomedical\ndatasets.\n", "versions": [{"version": "v1", "created": "Sat, 3 Apr 2021 06:54:33 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Hsu", "Joy", ""], ["Chiu", "Wah", ""], ["Yeung", "Serena", ""]]}, {"id": "2104.01328", "submitter": "Niko S\\\"underhauf", "authors": "Dimity Miller, Niko S\\\"underhauf, Michael Milford and Feras Dayoub", "title": "Uncertainty for Identifying Open-Set Errors in Visual Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deployed into an open world, object detectors are prone to a type of false\npositive detection termed open-set errors. We propose GMM-Det, a real-time\nmethod for extracting epistemic uncertainty from object detectors to identify\nand reject open-set errors. GMM-Det trains the detector to produce a structured\nlogit space that is modelled with class-specific Gaussian Mixture Models. At\ntest time, open-set errors are identified by their low log-probability under\nall Gaussian Mixture Models. We test two common detector architectures, Faster\nR-CNN and RetinaNet, across three varied datasets spanning robotics and\ncomputer vision. Our results show that GMM-Det consistently outperforms\nexisting uncertainty techniques for identifying and rejecting open-set\ndetections, especially at the low-error-rate operating point required for\nsafety-critical applications. GMM-Det maintains object detection performance,\nand introduces only minimal computational overhead. We also introduce a\nmethodology for converting existing object detection datasets into specific\nopen-set datasets to consistently evaluate open-set performance in object\ndetection. Code for GMM-Det and the dataset methodology will be made publicly\navailable.\n", "versions": [{"version": "v1", "created": "Sat, 3 Apr 2021 07:12:31 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Miller", "Dimity", ""], ["S\u00fcnderhauf", "Niko", ""], ["Milford", "Michael", ""], ["Dayoub", "Feras", ""]]}, {"id": "2104.01329", "submitter": "Leonardo Rossi", "authors": "Leonardo Rossi, Akbar Karimi, Andrea Prati", "title": "Recursively Refined R-CNN: Instance Segmentation with Self-RoI\n  Rebalancing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Within the field of instance segmentation, most of the state-of-the-art deep\nlearning networks rely nowadays on cascade architectures, where multiple object\ndetectors are trained sequentially, re-sampling the ground truth at each step.\nThis offers a solution to the problem of exponentially vanishing positive\nsamples. However, it also translates into an increase in network complexity in\nterms of the number of parameters. To address this issue, we propose\nRecursively Refined R-CNN ($R^3$-CNN) which avoids duplicates by introducing a\nloop mechanism instead. At the same time, it achieves a quality boost using a\nrecursive re-sampling technique, where a specific IoU quality is utilized in\neach recursion to eventually equally cover the positive spectrum. Our\nexperiments highlight the specific encoding of the loop mechanism in the\nweights, requiring its usage at inference time. The $R^3$-CNN architecture is\nable to surpass the recently proposed HTC model, while reducing the number of\nparameters significantly. Experiments on COCO minival 2017 dataset show\nperformance boost independently from the utilized baseline model. The code is\navailable online at https://github.com/IMPLabUniPr/mmdetection/tree/r3_cnn.\n", "versions": [{"version": "v1", "created": "Sat, 3 Apr 2021 07:25:33 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Rossi", "Leonardo", ""], ["Karimi", "Akbar", ""], ["Prati", "Andrea", ""]]}, {"id": "2104.01350", "submitter": "Masaki Kitayama", "authors": "Masaki Kitayama, Hitoshi Kiya", "title": "Generation of Gradient-Preserving Images allowing HOG Feature Extraction", "comments": "Accepted for publication in IEEE International Conference on Consumer\n  Electronics - Taiwan, 2021(ICCE-TW 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a method for generating visually protected images,\nreferred to as gradient-preserving images. The protected images allow us to\ndirectly extract Histogram-of-Oriented-Gradients (HOG) features for\nprivacy-preserving machine learning. In an experiment, HOG features extracted\nfrom gradient-preserving images are applied to a face recognition algorithm to\ndemonstrate the effectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Sat, 3 Apr 2021 09:06:58 GMT"}, {"version": "v2", "created": "Sat, 22 May 2021 06:46:37 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Kitayama", "Masaki", ""], ["Kiya", "Hitoshi", ""]]}, {"id": "2104.01353", "submitter": "YoungJin Heo", "authors": "Young-Jin Heo, Young-Ju Choi, Young-Woon Lee, Byung-Gyu Kim", "title": "Deepfake Detection Scheme Based on Vision Transformer and Distillation", "comments": "7 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deepfake is the manipulated video made with a generative deep learning\ntechnique such as Generative Adversarial Networks (GANs) or Auto Encoder that\nanyone can utilize. Recently, with the increase of Deepfake videos, some\nclassifiers consisting of the convolutional neural network that can distinguish\nfake videos as well as deepfake datasets have been actively created. However,\nthe previous studies based on the CNN structure have the problem of not only\noverfitting, but also considerable misjudging fake video as real ones. In this\npaper, we propose a Vision Transformer model with distillation methodology for\ndetecting fake videos. We design that a CNN features and patch-based\npositioning model learns to interact with all positions to find the artifact\nregion for solving false negative problem. Through comparative analysis on\nDeepfake Detection (DFDC) Dataset, we verify that the proposed scheme with\npatch embedding as input outperforms the state-of-the-art using the combined\nCNN features. Without ensemble technique, our model obtains 0.978 of AUC and\n91.9 of f1 score, while previous SOTA model yields 0.972 of AUC and 90.6 of f1\nscore on the same condition.\n", "versions": [{"version": "v1", "created": "Sat, 3 Apr 2021 09:13:05 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Heo", "Young-Jin", ""], ["Choi", "Young-Ju", ""], ["Lee", "Young-Woon", ""], ["Kim", "Byung-Gyu", ""]]}, {"id": "2104.01374", "submitter": "Florian Jug", "authors": "Mangal Prakash, Mauricio Delbracio, Peyman Milanfar, Florian Jug", "title": "Removing Pixel Noises and Spatial Artifacts with Generative Diversity\n  Denoising Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG q-bio.QM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Image denoising and artefact removal are complex inverse problems admitting\nmany potential solutions. Variational Autoencoders (VAEs) can be used to learn\na whole distribution of sensible solutions, from which one can sample\nefficiently. However, such a generative approach to image restoration is only\nstudied in the context of pixel-wise noise removal (e.g. Poisson or Gaussian\nnoise). While important, a plethora of application domains suffer from imaging\nartefacts (structured noises) that alter groups of pixels in correlated ways.\nIn this work we show, for the first time, that generative diversity denoising\n(GDD) approaches can learn to remove structured noises without supervision. To\nthis end, we investigate two existing GDD architectures, introduce a new one\nbased on hierarchical VAEs, and compare their performances against a total of\nseven state-of-the-art baseline methods on five sources of structured noise\n(including tomography reconstruction artefacts and microscopy artefacts). We\nfind that GDD methods outperform all unsupervised baselines and in many cases\nnot lagging far behind supervised results (in some occasions even superseding\nthem). In addition to structured noise removal, we also show that our new GDD\nmethod produces new state-of-the-art (SOTA) results on seven out of eight\nbenchmark datasets for pixel-noise removal. Finally, we offer insights into the\ndaunting question of how GDD methods distinguish structured noise, which we\nlike to see removed, from image signals, which we want to see retained.\n", "versions": [{"version": "v1", "created": "Sat, 3 Apr 2021 11:00:21 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Prakash", "Mangal", ""], ["Delbracio", "Mauricio", ""], ["Milanfar", "Peyman", ""], ["Jug", "Florian", ""]]}, {"id": "2104.01375", "submitter": "Ioannis Kakogeorgiou", "authors": "Ioannis Kakogeorgiou and Konstantinos Karantzalos", "title": "Evaluating Explainable Artificial Intelligence Methods for Multi-label\n  Deep Learning Classification Tasks in Remote Sensing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Although deep neural networks hold the state-of-the-art in several remote\nsensing tasks, their black-box operation hinders the understanding of their\ndecisions, concealing any bias and other shortcomings in datasets and model\nperformance. To this end, we have applied explainable artificial intelligence\n(XAI) methods in remote sensing multi-label classification tasks towards\nproducing human-interpretable explanations and improve transparency. In\nparticular, we developed deep learning models with state-of-the-art performance\nin the benchmark BigEarthNet and SEN12MS datasets. Ten XAI methods were\nemployed towards understanding and interpreting models' predictions, along with\nquantitative metrics to assess and compare their performance. Numerous\nexperiments were performed to assess the overall performance of XAI methods for\nstraightforward prediction cases, competing multiple labels, as well as\nmisclassification cases. According to our findings, Occlusion, Grad-CAM and\nLime were the most interpretable and reliable XAI methods. However, none\ndelivers high-resolution outputs, while apart from Grad-CAM, both Lime and\nOcclusion are computationally expensive. We also highlight different aspects of\nXAI performance and elaborate with insights on black-box decisions in order to\nimprove transparency, understand their behavior and reveal, as well, datasets'\nparticularities.\n", "versions": [{"version": "v1", "created": "Sat, 3 Apr 2021 11:13:14 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Kakogeorgiou", "Ioannis", ""], ["Karantzalos", "Konstantinos", ""]]}, {"id": "2104.01381", "submitter": "Tsubasa Murate", "authors": "Tsubasa Murate, Takashi Watanabe, Masaki Yamada", "title": "Learning Mobile CNN Feature Extraction Toward Fast Computation of Visual\n  Object Tracking", "comments": "9 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we construct a lightweight, high-precision and high-speed\nobject tracking using a trained CNN. Conventional methods with trained CNNs use\nVGG16 network which requires powerful computational resources. Therefore, there\nis a problem that it is difficult to apply in low computation resources\nenvironments. To solve this problem, we use MobileNetV3, which is a CNN for\nmobile terminals.Based on Feature Map Selection Tracking, we propose a new\narchitecture that extracts effective features of MobileNet for object tracking.\nThe architecture requires no online learning but only offline learning. In\naddition, by using features of objects other than tracking target, the features\nof tracking target are extracted more efficiently. We measure the tracking\naccuracy with Visual Tracker Benchmark and confirm that the proposed method can\nperform high-precision and high-speed calculation even in low computation\nresource environments.\n", "versions": [{"version": "v1", "created": "Sat, 3 Apr 2021 11:49:54 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Murate", "Tsubasa", ""], ["Watanabe", "Takashi", ""], ["Yamada", "Masaki", ""]]}, {"id": "2104.01394", "submitter": "Viraj Bagal", "authors": "Yash Khare, Viraj Bagal, Minesh Mathew, Adithi Devi, U Deva\n  Priyakumar, CV Jawahar", "title": "MMBERT: Multimodal BERT Pretraining for Improved Medical VQA", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Images in the medical domain are fundamentally different from the general\ndomain images. Consequently, it is infeasible to directly employ general domain\nVisual Question Answering (VQA) models for the medical domain. Additionally,\nmedical images annotation is a costly and time-consuming process. To overcome\nthese limitations, we propose a solution inspired by self-supervised\npretraining of Transformer-style architectures for NLP, Vision and Language\ntasks. Our method involves learning richer medical image and text semantic\nrepresentations using Masked Language Modeling (MLM) with image features as the\npretext task on a large medical image+caption dataset. The proposed solution\nachieves new state-of-the-art performance on two VQA datasets for radiology\nimages -- VQA-Med 2019 and VQA-RAD, outperforming even the ensemble models of\nprevious best solutions. Moreover, our solution provides attention maps which\nhelp in model interpretability. The code is available at\nhttps://github.com/VirajBagal/MMBERT\n", "versions": [{"version": "v1", "created": "Sat, 3 Apr 2021 13:01:19 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Khare", "Yash", ""], ["Bagal", "Viraj", ""], ["Mathew", "Minesh", ""], ["Devi", "Adithi", ""], ["Priyakumar", "U Deva", ""], ["Jawahar", "CV", ""]]}, {"id": "2104.01405", "submitter": "Yi Zhang", "authors": "Tao Wang, Wenjun Xia, Zexin Lu, Huaiqiang Sun, Yan Liu, Hu Chen, Jiliu\n  Zhou, Yi Zhang", "title": "IDOL-Net: An Interactive Dual-Domain Parallel Network for CT Metal\n  Artifact Reduction", "comments": "10 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.med-ph cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Due to the presence of metallic implants, the imaging quality of computed\ntomography (CT) would be heavily degraded. With the rapid development of deep\nlearning, several network models have been proposed for metal artifact\nreduction (MAR). Since the dual-domain MAR methods can leverage the hybrid\ninformation from both sinogram and image domains, they have significantly\nimproved the performance compared to single-domain methods. However,current\ndual-domain methods usually operate on both domains in a specific order, which\nimplicitly imposes a certain priority prior into MAR and may ignore the latent\ninformation interaction between both domains. To address this problem, in this\npaper, we propose a novel interactive dualdomain parallel network for CT MAR,\ndubbed as IDOLNet. Different from existing dual-domain methods, the proposed\nIDOL-Net is composed of two modules. The disentanglement module is utilized to\ngenerate high-quality prior sinogram and image as the complementary inputs. The\nfollow-up refinement module consists of two parallel and interactive branches\nthat simultaneously operate on image and sinogram domain, fully exploiting the\nlatent information interaction between both domains. The simulated and clinical\nresults demonstrate that the proposed IDOL-Net outperforms several\nstate-of-the-art models in both qualitative and quantitative aspects.\n", "versions": [{"version": "v1", "created": "Sat, 3 Apr 2021 13:50:34 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Wang", "Tao", ""], ["Xia", "Wenjun", ""], ["Lu", "Zexin", ""], ["Sun", "Huaiqiang", ""], ["Liu", "Yan", ""], ["Chen", "Hu", ""], ["Zhou", "Jiliu", ""], ["Zhang", "Yi", ""]]}, {"id": "2104.01429", "submitter": "Huasong Zhong", "authors": "Huasong Zhong, Jianlong Wu, Chong Chen, Jianqiang Huang, Minghua Deng,\n  Liqiang Nie, Zhouchen Lin, Xian-Sheng Hua", "title": "Graph Contrastive Clustering", "comments": "10", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, some contrastive learning methods have been proposed to\nsimultaneously learn representations and clustering assignments, achieving\nsignificant improvements. However, these methods do not take the category\ninformation and clustering objective into consideration, thus the learned\nrepresentations are not optimal for clustering and the performance might be\nlimited. Towards this issue, we first propose a novel graph contrastive\nlearning framework, which is then applied to the clustering task and we come up\nwith the Graph Constrastive Clustering~(GCC) method. Different from basic\ncontrastive clustering that only assumes an image and its augmentation should\nshare similar representation and clustering assignments, we lift the\ninstance-level consistency to the cluster-level consistency with the assumption\nthat samples in one cluster and their augmentations should all be similar.\nSpecifically, on the one hand, the graph Laplacian based contrastive loss is\nproposed to learn more discriminative and clustering-friendly features. On the\nother hand, a novel graph-based contrastive learning strategy is proposed to\nlearn more compact clustering assignments. Both of them incorporate the latent\ncategory information to reduce the intra-cluster variance while increasing the\ninter-cluster variance. Experiments on six commonly used datasets demonstrate\nthe superiority of our proposed approach over the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sat, 3 Apr 2021 15:32:49 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Zhong", "Huasong", ""], ["Wu", "Jianlong", ""], ["Chen", "Chong", ""], ["Huang", "Jianqiang", ""], ["Deng", "Minghua", ""], ["Nie", "Liqiang", ""], ["Lin", "Zhouchen", ""], ["Hua", "Xian-Sheng", ""]]}, {"id": "2104.01431", "submitter": "Yanhong Zeng", "authors": "Yanhong Zeng, Jianlong Fu, Hongyang Chao, Baining Guo", "title": "Aggregated Contextual Transformations for High-Resolution Image\n  Inpainting", "comments": "This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  State-of-the-art image inpainting approaches can suffer from generating\ndistorted structures and blurry textures in high-resolution images (e.g.,\n512x512). The challenges mainly drive from (1) image content reasoning from\ndistant contexts, and (2) fine-grained texture synthesis for a large missing\nregion. To overcome these two challenges, we propose an enhanced GAN-based\nmodel, named Aggregated COntextual-Transformation GAN (AOT-GAN), for\nhigh-resolution image inpainting. Specifically, to enhance context reasoning,\nwe construct the generator of AOT-GAN by stacking multiple layers of a proposed\nAOT block. The AOT blocks aggregate contextual transformations from various\nreceptive fields, allowing to capture both informative distant image contexts\nand rich patterns of interest for context reasoning. For improving texture\nsynthesis, we enhance the discriminator of AOT-GAN by training it with a\ntailored mask-prediction task. Such a training objective forces the\ndiscriminator to distinguish the detailed appearances of real and synthesized\npatches, and in turn, facilitates the generator to synthesize clear textures.\nExtensive comparisons on Places2, the most challenging benchmark with 1.8\nmillion high-resolution images of 365 complex scenes, show that our model\noutperforms the state-of-the-art by a significant margin in terms of FID with\n38.60% relative improvement. A user study including more than 30 subjects\nfurther validates the superiority of AOT-GAN. We further evaluate the proposed\nAOT-GAN in practical applications, e.g., logo removal, face editing, and object\nremoval. Results show that our model achieves promising completions in the real\nworld. We release code and models in\nhttps://github.com/researchmm/AOT-GAN-for-Inpainting.\n", "versions": [{"version": "v1", "created": "Sat, 3 Apr 2021 15:50:17 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Zeng", "Yanhong", ""], ["Fu", "Jianlong", ""], ["Chao", "Hongyang", ""], ["Guo", "Baining", ""]]}, {"id": "2104.01449", "submitter": "Jonas Denck", "authors": "Jonas Denck, Jens Guehring, Andreas Maier, Eva Rothgang", "title": "MR-Contrast-Aware Image-to-Image Translations with Generative\n  Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Purpose\n  A Magnetic Resonance Imaging (MRI) exam typically consists of several\nsequences that yield different image contrasts. Each sequence is parameterized\nthrough multiple acquisition parameters that influence image contrast,\nsignal-to-noise ratio, acquisition time, and/or resolution. Depending on the\nclinical indication, different contrasts are required by the radiologist to\nmake a diagnosis. As MR sequence acquisition is time consuming and acquired\nimages may be corrupted due to motion, a method to synthesize MR images with\nadjustable contrast properties is required.\n  Methods\n  Therefore, we trained an image-to-image generative adversarial network\nconditioned on the MR acquisition parameters repetition time and echo time. Our\napproach is motivated by style transfer networks, whereas the \"style\" for an\nimage is explicitly given in our case, as it is determined by the MR\nacquisition parameters our network is conditioned on.\n  Results\n  This enables us to synthesize MR images with adjustable image contrast. We\nevaluated our approach on the fastMRI dataset, a large set of publicly\navailable MR knee images, and show that our method outperforms a benchmark\npix2pix approach in the translation of non-fat-saturated MR images to\nfat-saturated images. Our approach yields a peak signal-to-noise ratio and\nstructural similarity of 24.48 and 0.66, surpassing the pix2pix benchmark model\nsignificantly.\n  Conclusion\n  Our model is the first that enables fine-tuned contrast synthesis, which can\nbe used to synthesize missing MR contrasts or as a data augmentation technique\nfor AI training in MRI.\n", "versions": [{"version": "v1", "created": "Sat, 3 Apr 2021 17:05:13 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Denck", "Jonas", ""], ["Guehring", "Jens", ""], ["Maier", "Andreas", ""], ["Rothgang", "Eva", ""]]}, {"id": "2104.01478", "submitter": "Habtamu Fanta Mr", "authors": "Habtamu Fanta, Zhiwen Shao, Lizhuang Ma", "title": "\"Forget\" the Forget Gate: Estimating Anomalies in Videos using\n  Self-contained Long Short-Term Memory Networks", "comments": "16 pages, 7 figures, Computer Graphics International (CGI) 2020", "journal-ref": null, "doi": "10.1007/978-3-030-61864-3_15", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Abnormal event detection is a challenging task that requires effectively\nhandling intricate features of appearance and motion. In this paper, we present\nan approach of detecting anomalies in videos by learning a novel LSTM based\nself-contained network on normal dense optical flow. Due to their sigmoid\nimplementations, standard LSTM's forget gate is susceptible to overlooking and\ndismissing relevant content in long sequence tasks like abnormality detection.\nThe forget gate mitigates participation of previous hidden state for\ncomputation of cell state prioritizing current input. In addition, the\nhyperbolic tangent activation of standard LSTMs sacrifices performance when a\nnetwork gets deeper. To tackle these two limitations, we introduce a bi-gated,\nlight LSTM cell by discarding the forget gate and introducing sigmoid\nactivation. Specifically, the LSTM architecture we come up with fully sustains\ncontent from previous hidden state thereby enabling the trained model to be\nrobust and make context-independent decision during evaluation. Removing the\nforget gate results in a simplified and undemanding LSTM cell with improved\nperformance effectiveness and computational efficiency. Empirical evaluations\nshow that the proposed bi-gated LSTM based network outperforms various LSTM\nbased models verifying its effectiveness for abnormality detection and\ngeneralization tasks on CUHK Avenue and UCSD datasets.\n", "versions": [{"version": "v1", "created": "Sat, 3 Apr 2021 20:43:49 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Fanta", "Habtamu", ""], ["Shao", "Zhiwen", ""], ["Ma", "Lizhuang", ""]]}, {"id": "2104.01482", "submitter": "Edgar A. Bernal", "authors": "Edgar A. Bernal", "title": "Training Deep Normalizing Flow Models in Highly Incomplete Data\n  Scenarios with Prior Regularization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep generative frameworks including GANs and normalizing flow models have\nproven successful at filling in missing values in partially observed data\nsamples by effectively learning -- either explicitly or implicitly -- complex,\nhigh-dimensional statistical distributions. In tasks where the data available\nfor learning is only partially observed, however, their performance decays\nmonotonically as a function of the data missingness rate. In high missing data\nrate regimes (e.g., 60% and above), it has been observed that state-of-the-art\nmodels tend to break down and produce unrealistic and/or semantically\ninaccurate data. We propose a novel framework to facilitate the learning of\ndata distributions in high paucity scenarios that is inspired by traditional\nformulations of solutions to ill-posed problems. The proposed framework\nnaturally stems from posing the process of learning from incomplete data as a\njoint optimization task of the parameters of the model being learned and the\nmissing data values. The method involves enforcing a prior regularization term\nthat seamlessly integrates with objectives used to train explicit and tractable\ndeep generative frameworks such as deep normalizing flow models. We demonstrate\nvia extensive experimental validation that the proposed framework outperforms\ncompeting techniques, particularly as the rate of data paucity approaches\nunity.\n", "versions": [{"version": "v1", "created": "Sat, 3 Apr 2021 20:57:57 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Bernal", "Edgar A.", ""]]}, {"id": "2104.01495", "submitter": "Yang Gao", "authors": "Yang Gao, Yi-Fan Li, Swarup Chandra, Latifur Khan, Bhavani\n  Thuraisingham", "title": "Towards Self-Adaptive Metric Learning On the Fly", "comments": "Accepted by WWW 2019 (Long Paper, Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Good quality similarity metrics can significantly facilitate the performance\nof many large-scale, real-world applications. Existing studies have proposed\nvarious solutions to learn a Mahalanobis or bilinear metric in an online\nfashion by either restricting distances between similar (dissimilar) pairs to\nbe smaller (larger) than a given lower (upper) bound or requiring similar\ninstances to be separated from dissimilar instances with a given margin.\nHowever, these linear metrics learned by leveraging fixed bounds or margins may\nnot perform well in real-world applications, especially when data distributions\nare complex. We aim to address the open challenge of \"Online Adaptive Metric\nLearning\" (OAML) for learning adaptive metric functions on the fly. Unlike\ntraditional online metric learning methods, OAML is significantly more\nchallenging since the learned metric could be non-linear and the model has to\nbe self-adaptive as more instances are observed. In this paper, we present a\nnew online metric learning framework that attempts to tackle the challenge by\nlearning an ANN-based metric with adaptive model complexity from a stream of\nconstraints. In particular, we propose a novel Adaptive-Bound Triplet Loss\n(ABTL) to effectively utilize the input constraints and present a novel\nAdaptive Hedge Update (AHU) method for online updating the model parameters. We\nempirically validate the effectiveness and efficacy of our framework on various\napplications such as real-world image classification, facial verification, and\nimage retrieval.\n", "versions": [{"version": "v1", "created": "Sat, 3 Apr 2021 23:11:52 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Gao", "Yang", ""], ["Li", "Yi-Fan", ""], ["Chandra", "Swarup", ""], ["Khan", "Latifur", ""], ["Thuraisingham", "Bhavani", ""]]}, {"id": "2104.01508", "submitter": "Yaxuan Zhu", "authors": "Yaxuan Zhu, Ruiqi Gao, Siyuan Huang, Song-Chun Zhu, Ying Nian Wu", "title": "Learning Neural Representation of Camera Pose with Matrix Representation\n  of Pose Shift via View Synthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How to effectively represent camera pose is an essential problem in 3D\ncomputer vision, especially in tasks such as camera pose regression and novel\nview synthesis. Traditionally, 3D position of the camera is represented by\nCartesian coordinate and the orientation is represented by Euler angle or\nquaternions. These representations are manually designed, which may not be the\nmost effective representation for downstream tasks. In this work, we propose an\napproach to learn neural representations of camera poses and 3D scenes, coupled\nwith neural representations of local camera movements. Specifically, the camera\npose and 3D scene are represented as vectors and the local camera movement is\nrepresented as a matrix operating on the vector of the camera pose. We\ndemonstrate that the camera movement can further be parametrized by a matrix\nLie algebra that underlies a rotation system in the neural space. The vector\nrepresentations are then concatenated and generate the posed 2D image through a\ndecoder network. The model is learned from only posed 2D images and\ncorresponding camera poses, without access to depths or shapes. We conduct\nextensive experiments on synthetic and real datasets. The results show that\ncompared with other camera pose representations, our learned representation is\nmore robust to noise in novel view synthesis and more effective in camera pose\nregression.\n", "versions": [{"version": "v1", "created": "Sun, 4 Apr 2021 00:40:53 GMT"}, {"version": "v2", "created": "Thu, 15 Apr 2021 03:40:28 GMT"}, {"version": "v3", "created": "Sun, 9 May 2021 00:04:40 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Zhu", "Yaxuan", ""], ["Gao", "Ruiqi", ""], ["Huang", "Siyuan", ""], ["Zhu", "Song-Chun", ""], ["Wu", "Ying Nian", ""]]}, {"id": "2104.01509", "submitter": "Dennis N\\'u\\~nez Fern\\'andez", "authors": "Carlos Rojas-Azabache, Karen Vilca-Janampa, Renzo Guerrero-Huayta,\n  Dennis N\\'u\\~nez-Fern\\'andez", "title": "Detection of COVID-19 Disease using Deep Neural Networks with Ultrasound\n  Imaging", "comments": "Practical ML for Developing Countries Workshop at ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The new coronavirus 2019 (COVID-2019) has rapidly become a pandemic and has\nhad a devastating effect on both everyday life, public health and the global\neconomy. It is critical to detect positive cases as early as possible to\nprevent the further spread of this epidemic and to treat affected patients\nquickly. The need for auxiliary diagnostic tools has increased as accurate\nautomated tool kits are not available. This paper presents a work in progress\nthat proposes the analysis of images of lung ultrasound scans using a\nconvolutional neural network. The trained model will be used on a Raspberry Pi\nto predict on new images.\n", "versions": [{"version": "v1", "created": "Sun, 4 Apr 2021 00:53:06 GMT"}, {"version": "v2", "created": "Sun, 11 Apr 2021 06:50:25 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Rojas-Azabache", "Carlos", ""], ["Vilca-Janampa", "Karen", ""], ["Guerrero-Huayta", "Renzo", ""], ["N\u00fa\u00f1ez-Fern\u00e1ndez", "Dennis", ""]]}, {"id": "2104.01517", "submitter": "Zhiqi Chen", "authors": "Zhiqi Chen, Ran Wang, Haojie Liu and Yao Wang", "title": "PDWN: Pyramid Deformable Warping Network for Video Interpolation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video interpolation aims to generate a non-existent intermediate frame given\nthe past and future frames. Many state-of-the-art methods achieve promising\nresults by estimating the optical flow between the known frames and then\ngenerating the backward flows between the middle frame and the known frames.\nHowever, these methods usually suffer from the inaccuracy of estimated optical\nflows and require additional models or information to compensate for flow\nestimation errors. Following the recent development in using deformable\nconvolution (DConv) for video interpolation, we propose a light but effective\nmodel, called Pyramid Deformable Warping Network (PDWN). PDWN uses a pyramid\nstructure to generate DConv offsets of the unknown middle frame with respect to\nthe known frames through coarse-to-fine successive refinements. Cost volumes\nbetween warped features are calculated at every pyramid level to help the\noffset inference. At the finest scale, the two warped frames are adaptively\nblended to generate the middle frame. Lastly, a context enhancement network\nfurther enhances the contextual detail of the final output. Ablation studies\ndemonstrate the effectiveness of the coarse-to-fine offset refinement, cost\nvolumes, and DConv. Our method achieves better or on-par accuracy compared to\nstate-of-the-art models on multiple datasets while the number of model\nparameters and the inference time are substantially less than previous models.\nMoreover, we present an extension of the proposed framework to use four input\nframes, which can achieve significant improvement over using only two input\nframes, with only a slight increase in the model size and inference time.\n", "versions": [{"version": "v1", "created": "Sun, 4 Apr 2021 02:08:57 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Chen", "Zhiqi", ""], ["Wang", "Ran", ""], ["Liu", "Haojie", ""], ["Wang", "Yao", ""]]}, {"id": "2104.01525", "submitter": "Benyamin Ghojogh", "authors": "Benyamin Ghojogh, Ali Ghodsi, Fakhri Karray, Mark Crowley", "title": "Generative Locally Linear Embedding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Locally Linear Embedding (LLE) is a nonlinear spectral dimensionality\nreduction and manifold learning method. It has two main steps which are linear\nreconstruction and linear embedding of points in the input space and embedding\nspace, respectively. In this work, we propose two novel generative versions of\nLLE, named Generative LLE (GLLE), whose linear reconstruction steps are\nstochastic rather than deterministic. GLLE assumes that every data point is\ncaused by its linear reconstruction weights as latent factors. The proposed\nGLLE algorithms can generate various LLE embeddings stochastically while all\nthe generated embeddings relate to the original LLE embedding. We propose two\nversions for stochastic linear reconstruction, one using expectation\nmaximization and another with direct sampling from a derived distribution by\noptimization. The proposed GLLE methods are closely related to and inspired by\nvariational inference, factor analysis, and probabilistic principal component\nanalysis. Our simulations show that the proposed GLLE methods work effectively\nin unfolding and generating submanifolds of data.\n", "versions": [{"version": "v1", "created": "Sun, 4 Apr 2021 02:59:39 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Ghojogh", "Benyamin", ""], ["Ghodsi", "Ali", ""], ["Karray", "Fakhri", ""], ["Crowley", "Mark", ""]]}, {"id": "2104.01526", "submitter": "Xinggang Wang", "authors": "Xinggang Wang and Jiapei Feng and Bin Hu and Qi Ding and Longjin Ran\n  and Xiaoxin Chen and Wenyu Liu", "title": "Weakly-supervised Instance Segmentation via Class-agnostic Learning with\n  Salient Images", "comments": null, "journal-ref": "CVPR 2021", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Humans have a strong class-agnostic object segmentation ability and can\noutline boundaries of unknown objects precisely, which motivates us to propose\na box-supervised class-agnostic object segmentation (BoxCaseg) based solution\nfor weakly-supervised instance segmentation. The BoxCaseg model is jointly\ntrained using box-supervised images and salient images in a multi-task learning\nmanner. The fine-annotated salient images provide class-agnostic and precise\nobject localization guidance for box-supervised images. The object masks\npredicted by a pretrained BoxCaseg model are refined via a novel merged and\ndropped strategy as proxy ground truth to train a Mask R-CNN for\nweakly-supervised instance segmentation. Only using $7991$ salient images, the\nweakly-supervised Mask R-CNN is on par with fully-supervised Mask R-CNN on\nPASCAL VOC and significantly outperforms previous state-of-the-art\nbox-supervised instance segmentation methods on COCO. The source code,\npretrained models and datasets are available at\n\\url{https://github.com/hustvl/BoxCaseg}.\n", "versions": [{"version": "v1", "created": "Sun, 4 Apr 2021 03:01:52 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Wang", "Xinggang", ""], ["Feng", "Jiapei", ""], ["Hu", "Bin", ""], ["Ding", "Qi", ""], ["Ran", "Longjin", ""], ["Chen", "Xiaoxin", ""], ["Liu", "Wenyu", ""]]}, {"id": "2104.01528", "submitter": "Liushuai Shi", "authors": "Liushuai Shi, Le Wang, Chengjiang Long, Sanping Zhou, Mo Zhou,\n  Zhenxing Niu, Gang Hua", "title": "SGCN:Sparse Graph Convolution Network for Pedestrian Trajectory\n  Prediction", "comments": "Accepted by CVPR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pedestrian trajectory prediction is a key technology in autopilot, which\nremains to be very challenging due to complex interactions between pedestrians.\nHowever, previous works based on dense undirected interaction suffer from\nmodeling superfluous interactions and neglect of trajectory motion tendency,\nand thus inevitably result in a considerable deviance from the reality. To cope\nwith these issues, we present a Sparse Graph Convolution Network~(SGCN) for\npedestrian trajectory prediction. Specifically, the SGCN explicitly models the\nsparse directed interaction with a sparse directed spatial graph to capture\nadaptive interaction pedestrians. Meanwhile, we use a sparse directed temporal\ngraph to model the motion tendency, thus to facilitate the prediction based on\nthe observed direction. Finally, parameters of a bi-Gaussian distribution for\ntrajectory prediction are estimated by fusing the above two sparse graphs. We\nevaluate our proposed method on the ETH and UCY datasets, and the experimental\nresults show our method outperforms comparative state-of-the-art methods by 9%\nin Average Displacement Error(ADE) and 13% in Final Displacement Error(FDE).\nNotably, visualizations indicate that our method can capture adaptive\ninteractions between pedestrians and their effective motion tendencies.\n", "versions": [{"version": "v1", "created": "Sun, 4 Apr 2021 03:17:42 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Shi", "Liushuai", ""], ["Wang", "Le", ""], ["Long", "Chengjiang", ""], ["Zhou", "Sanping", ""], ["Zhou", "Mo", ""], ["Niu", "Zhenxing", ""], ["Hua", "Gang", ""]]}, {"id": "2104.01530", "submitter": "Zhiwei Zhong", "authors": "Zhiwei Zhong, Xianming Liu, Junjun Jiang, Debin Zhao, Zhiwen Chen and\n  Xiangyang Ji", "title": "High-resolution Depth Maps Imaging via Attention-based Hierarchical\n  Multi-modal Fusion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Depth map records distance between the viewpoint and objects in the scene,\nwhich plays a critical role in many real-world applications. However, depth map\ncaptured by consumer-grade RGB-D cameras suffers from low spatial resolution.\nGuided depth map super-resolution (DSR) is a popular approach to address this\nproblem, which attempts to restore a high-resolution (HR) depth map from the\ninput low-resolution (LR) depth and its coupled HR RGB image that serves as the\nguidance.\n  The most challenging problems for guided DSR are how to correctly select\nconsistent structures and propagate them, and properly handle inconsistent\nones. In this paper, we propose a novel attention-based hierarchical\nmulti-modal fusion (AHMF) network for guided DSR. Specifically, to effectively\nextract and combine relevant information from LR depth and HR guidance, we\npropose a multi-modal attention based fusion (MMAF) strategy for hierarchical\nconvolutional layers, including a feature enhance block to select valuable\nfeatures and a feature recalibration block to unify the similarity metrics of\nmodalities with different appearance characteristics. Furthermore, we propose a\nbi-directional hierarchical feature collaboration (BHFC) module to fully\nleverage low-level spatial information and high-level structure information\namong multi-scale features. Experimental results show that our approach\noutperforms state-of-the-art methods in terms of reconstruction accuracy,\nrunning speed and memory efficiency.\n", "versions": [{"version": "v1", "created": "Sun, 4 Apr 2021 03:28:33 GMT"}, {"version": "v2", "created": "Tue, 13 Apr 2021 01:33:17 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Zhong", "Zhiwei", ""], ["Liu", "Xianming", ""], ["Jiang", "Junjun", ""], ["Zhao", "Debin", ""], ["Chen", "Zhiwen", ""], ["Ji", "Xiangyang", ""]]}, {"id": "2104.01534", "submitter": "Yuanbin Fu", "authors": "Fu Yuanbin and Guoxiaojie and Hu Qiming and Lin Di and Ma Jiayi and\n  Ling Haibin", "title": "Hierarchical Image Peeling: A Flexible Scale-space Filtering Framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The importance of hierarchical image organization has been witnessed by a\nwide spectrum of applications in computer vision and graphics. Different from\nimage segmentation with the spatial whole-part consideration, this work designs\na modern framework for disassembling an image into a family of derived signals\nfrom a scale-space perspective. Specifically, we first offer a formal\ndefinition of image disassembly. Then, by concerning desired properties, such\nas peeling hierarchy and structure preservation, we convert the original\ncomplex problem into a series of two-component separation sub-problems,\nsignificantly reducing the complexity. The proposed framework is flexible to\nboth supervised and unsupervised settings. A compact recurrent network, namely\nhierarchical image peeling net, is customized to efficiently and effectively\nfulfill the task, which is about 3.5Mb in size, and can handle 1080p images in\nmore than 60 fps per recurrence on a GTX 2080Ti GPU, making it attractive for\npractical use. Both theoretical findings and experimental results are provided\nto demonstrate the efficacy of the proposed framework, reveal its superiority\nover other state-of-the-art alternatives, and show its potential to various\napplicable scenarios. Our code is available at\n\\url{https://github.com/ForawardStar/HIPe}.\n", "versions": [{"version": "v1", "created": "Sun, 4 Apr 2021 04:08:14 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Yuanbin", "Fu", ""], ["Guoxiaojie", "", ""], ["Qiming", "Hu", ""], ["Di", "Lin", ""], ["Jiayi", "Ma", ""], ["Haibin", "Ling", ""]]}, {"id": "2104.01536", "submitter": "Jiashu He", "authors": "Jiashu He", "title": "Performance analysis of facial recognition: A critical review through\n  glass factor", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  COVID-19 pandemic and social distancing urge a reliable human face\nrecognition system in different abnormal situations. However, there is no\nresearch which studies the influence of glass factor in facial recognition\nsystem. This paper provides a comprehensive review of glass factor. The study\ncontains two steps: data collection and accuracy test. Data collection includes\ncollecting human face images through different situations, such as clear\nglasses, glass with water and glass with mist. Based on the collected data, an\nexisting state-of-the-art face detection and recognition system built upon\nMTCNN and Inception V1 deep nets is tested for further analysis. Experimental\ndata supports that 1) the system is robust for classification when comparing\nreal-time images and 2) it fails at determining if two images are of same\nperson by comparing real-time disturbed image with the frontal ones.\n", "versions": [{"version": "v1", "created": "Sun, 4 Apr 2021 05:02:04 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["He", "Jiashu", ""]]}, {"id": "2104.01538", "submitter": "Juhong Min", "authors": "Juhong Min, Dahyun Kang, Minsu Cho", "title": "Hypercorrelation Squeeze for Few-Shot Segmentation", "comments": "24 pages, 23 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Few-shot semantic segmentation aims at learning to segment a target object\nfrom a query image using only a few annotated support images of the target\nclass. This challenging task requires to understand diverse levels of visual\ncues and analyze fine-grained correspondence relations between the query and\nthe support images. To address the problem, we propose Hypercorrelation Squeeze\nNetworks (HSNet) that leverages multi-level feature correlation and efficient\n4D convolutions. It extracts diverse features from different levels of\nintermediate convolutional layers and constructs a collection of 4D correlation\ntensors, i.e., hypercorrelations. Using efficient center-pivot 4D convolutions\nin a pyramidal architecture, the method gradually squeezes high-level semantic\nand low-level geometric cues of the hypercorrelation into precise segmentation\nmasks in coarse-to-fine manner. The significant performance improvements on\nstandard few-shot segmentation benchmarks of PASCAL-5i, COCO-20i, and FSS-1000\nverify the efficacy of the proposed method.\n", "versions": [{"version": "v1", "created": "Sun, 4 Apr 2021 05:27:13 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Min", "Juhong", ""], ["Kang", "Dahyun", ""], ["Cho", "Minsu", ""]]}, {"id": "2104.01539", "submitter": "Jian Liang", "authors": "Jian Liang and Dapeng Hu and Ran He and Jiashi Feng", "title": "Distill and Fine-tune: Effective Adaptation from a Black-box Source\n  Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To alleviate the burden of labeling, unsupervised domain adaptation (UDA)\naims to transfer knowledge in previous related labeled datasets (source) to a\nnew unlabeled dataset (target). Despite impressive progress, prior methods\nalways need to access the raw source data and develop data-dependent alignment\napproaches to recognize the target samples in a transductive learning manner,\nwhich may raise privacy concerns from source individuals. Several recent\nstudies resort to an alternative solution by exploiting the well-trained\nwhite-box model instead of the raw data from the source domain, however, it may\nleak the raw data through generative adversarial training. This paper studies a\npractical and interesting setting for UDA, where only a black-box source model\n(i.e., only network predictions are available) is provided during adaptation in\nthe target domain. Besides, different neural networks are even allowed to be\nemployed for different domains. For this new problem, we propose a novel\ntwo-step adaptation framework called Distill and Fine-tune (Dis-tune).\nSpecifically, Dis-tune first structurally distills the knowledge from the\nsource model to a customized target model, then unsupervisedly fine-tunes the\ndistilled model to fit the target domain. To verify the effectiveness, we\nconsider two UDA scenarios (\\ie, closed-set and partial-set), and discover that\nDis-tune achieves highly competitive performance to state-of-the-art\napproaches.\n", "versions": [{"version": "v1", "created": "Sun, 4 Apr 2021 05:29:05 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Liang", "Jian", ""], ["Hu", "Dapeng", ""], ["He", "Ran", ""], ["Feng", "Jiashi", ""]]}, {"id": "2104.01542", "submitter": "Zhenyu Jiang", "authors": "Zhenyu Jiang, Yifeng Zhu, Maxwell Svetlik, Kuan Fang, Yuke Zhu", "title": "Synergies Between Affordance and Geometry: 6-DoF Grasp Detection via\n  Implicit Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Grasp detection in clutter requires the robot to reason about the 3D scene\nfrom incomplete and noisy perception. In this work, we draw insight that 3D\nreconstruction and grasp learning are two intimately connected tasks, both of\nwhich require a fine-grained understanding of local geometry details. We thus\npropose to utilize the synergies between grasp affordance and 3D reconstruction\nthrough multi-task learning of a shared representation. Our model takes\nadvantage of deep implicit functions, a continuous and memory-efficient\nrepresentation, to enable differentiable training of both tasks. We train the\nmodel on self-supervised grasp trials data in simulation. Evaluation is\nconducted on a clutter removal task, where the robot clears cluttered objects\nby grasping them one at a time. The experimental results in simulation and on\nthe real robot have demonstrated that the use of implicit neural\nrepresentations and joint learning of grasp affordance and 3D reconstruction\nhave led to state-of-the-art grasping results. Our method outperforms baselines\nby over 10% in terms of grasp success rate. Additional results and videos can\nbe found at https://sites.google.com/view/rpl-giga2021\n", "versions": [{"version": "v1", "created": "Sun, 4 Apr 2021 05:46:37 GMT"}, {"version": "v2", "created": "Wed, 21 Jul 2021 14:39:06 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Jiang", "Zhenyu", ""], ["Zhu", "Yifeng", ""], ["Svetlik", "Maxwell", ""], ["Fang", "Kuan", ""], ["Zhu", "Yuke", ""]]}, {"id": "2104.01546", "submitter": "Shengcai Liao", "authors": "Shengcai Liao and Ling Shao", "title": "Graph Sampling Based Deep Metric Learning for Generalizable Person\n  Re-Identification", "comments": "V2 is only an update of the abstract in metadata, which is completely\n  wrong in V1", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalizable person re-identification has recently got increasing attention\ndue to its research values as well as practical values. However, the efficiency\nof learning from large-scale data has not yet been much studied. In this paper,\nwe argue that the most popular random sampling method, the well-known PK\nsampler, is not informative and efficient for deep metric learning. Though\nonline hard example mining improves the learning efficiency to some extent, the\nmining in mini batches after random sampling is still limited. Therefore, this\ninspires us that the hard example mining should be shifted backward to the data\nsampling stage. To address this, in this paper, we propose an efficient mini\nbatch sampling method called Graph Sampling (GS) for large-scale metric\nlearning. The basic idea is to build a nearest neighbor relationship graph for\nall classes at the beginning of each epoch. Then, each mini batch is composed\nof a randomly selected class and its nearest neighboring classes so as to\nprovide informative and challenging examples for learning. Together with an\nadapted competitive baseline, we improve the previous state of the arts in\ngeneralizable person re-identification significantly, by up to 22.3% in Rank-1\nand 15% in mAP. Besides, the proposed method also outperforms the competitive\nbaseline by up to 4%, with the training time significantly reduced by up to\nx6.6, from 12.2 hours to 1.8 hours in training a large-scale dataset RandPerson\nwith 8,000 IDs. Code is available at\n\\url{https://github.com/ShengcaiLiao/QAConv}.\n", "versions": [{"version": "v1", "created": "Sun, 4 Apr 2021 06:44:15 GMT"}, {"version": "v2", "created": "Tue, 6 Apr 2021 05:26:26 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Liao", "Shengcai", ""], ["Shao", "Ling", ""]]}, {"id": "2104.01552", "submitter": "Hao Wang", "authors": "Hao Wang, Xiang Bai, Mingkun Yang, Shenggao Zhu, Jing Wang, Wenyu Liu", "title": "Scene Text Retrieval via Joint Text Detection and Similarity Learning", "comments": "Accepted to CVPR 2021. Code is available at:\n  https://github.com/lanfeng4659/STR-TDSL", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scene text retrieval aims to localize and search all text instances from an\nimage gallery, which are the same or similar to a given query text. Such a task\nis usually realized by matching a query text to the recognized words, outputted\nby an end-to-end scene text spotter. In this paper, we address this problem by\ndirectly learning a cross-modal similarity between a query text and each text\ninstance from natural images. Specifically, we establish an end-to-end\ntrainable network, jointly optimizing the procedures of scene text detection\nand cross-modal similarity learning. In this way, scene text retrieval can be\nsimply performed by ranking the detected text instances with the learned\nsimilarity. Experiments on three benchmark datasets demonstrate our method\nconsistently outperforms the state-of-the-art scene text spotting/retrieval\napproaches. In particular, the proposed framework of joint detection and\nsimilarity learning achieves significantly better performance than separated\nmethods. Code is available at: https://github.com/lanfeng4659/STR-TDSL.\n", "versions": [{"version": "v1", "created": "Sun, 4 Apr 2021 07:18:38 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Wang", "Hao", ""], ["Bai", "Xiang", ""], ["Yang", "Mingkun", ""], ["Zhu", "Shenggao", ""], ["Wang", "Jing", ""], ["Liu", "Wenyu", ""]]}, {"id": "2104.01592", "submitter": "Chen Chao", "authors": "Chao Chen, Catalina Raymond, Bill Speier, Xinyu Jin, Timothy F.\n  Cloughesy, Dieter Enzmann, Benjamin M. Ellingson, Corey W. Arnold", "title": "Synthesizing MR Image Contrast Enhancement Using 3D High-resolution\n  ConvNets", "comments": "Technical paper submitted to IEEE TMI, Code is available at\n  \\url{https://github.com/chenchao666/Contrast-enhanced-MRI-Synthesis}", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Gadolinium-based contrast agents (GBCAs) have been widely used to better\nvisualize disease in brain magnetic resonance imaging (MRI). However,\ngadolinium deposition within the brain and body has raised safety concerns\nabout the use of GBCAs. Therefore, the development of novel approaches that can\ndecrease or even eliminate GBCA exposure while providing similar contrast\ninformation would be of significant use clinically. For brain tumor patients,\nstandard-of-care includes repeated MRI with gadolinium-based contrast for\ndisease monitoring, increasing the risk of gadolinium deposition. In this work,\nwe present a deep learning based approach for contrast-enhanced T1 synthesis on\nbrain tumor patients. A 3D high-resolution fully convolutional network (FCN),\nwhich maintains high resolution information through processing and aggregates\nmulti-scale information in parallel, is designed to map pre-contrast MRI\nsequences to contrast-enhanced MRI sequences. Specifically, three pre-contrast\nMRI sequences, T1, T2 and apparent diffusion coefficient map (ADC), are\nutilized as inputs and the post-contrast T1 sequences are utilized as target\noutput. To alleviate the data imbalance problem between normal tissues and the\ntumor regions, we introduce a local loss to improve the contribution of the\ntumor regions, which leads to better enhancement results on tumors. Extensive\nquantitative and visual assessments are performed, with our proposed model\nachieving a PSNR of 28.24dB in the brain and 21.2dB in tumor regions. Our\nresults suggests the potential of substituting GBCAs with synthetic contrast\nimages generated via deep learning.\n", "versions": [{"version": "v1", "created": "Sun, 4 Apr 2021 11:54:15 GMT"}, {"version": "v2", "created": "Tue, 13 Apr 2021 03:45:52 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Chen", "Chao", ""], ["Raymond", "Catalina", ""], ["Speier", "Bill", ""], ["Jin", "Xinyu", ""], ["Cloughesy", "Timothy F.", ""], ["Enzmann", "Dieter", ""], ["Ellingson", "Benjamin M.", ""], ["Arnold", "Corey W.", ""]]}, {"id": "2104.01601", "submitter": "Zhihang Zhong", "authors": "Zhihang Zhong, Yinqiang Zheng and Imari Sato", "title": "Towards Rolling Shutter Correction and Deblurring in Dynamic Scenes", "comments": "To be published in CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Joint rolling shutter correction and deblurring (RSCD) techniques are\ncritical for the prevalent CMOS cameras. However, current approaches are still\nbased on conventional energy optimization and are developed for static scenes.\nTo enable learning-based approaches to address real-world RSCD problem, we\ncontribute the first dataset, BS-RSCD, which includes both ego-motion and\nobject-motion in dynamic scenes. Real distorted and blurry videos with\ncorresponding ground truth are recorded simultaneously via a\nbeam-splitter-based acquisition system.\n  Since direct application of existing individual rolling shutter correction\n(RSC) or global shutter deblurring (GSD) methods on RSCD leads to undesirable\nresults due to inherent flaws in the network architecture, we further present\nthe first learning-based model (JCD) for RSCD. The key idea is that we adopt\nbi-directional warping streams for displacement compensation, while also\npreserving the non-warped deblurring stream for details restoration. The\nexperimental results demonstrate that JCD achieves state-of-the-art performance\non the realistic RSCD dataset (BS-RSCD) and the synthetic RSC dataset\n(Fastec-RS). The dataset and code are available at\nhttps://github.com/zzh-tech/RSCD.\n", "versions": [{"version": "v1", "created": "Sun, 4 Apr 2021 12:36:48 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Zhong", "Zhihang", ""], ["Zheng", "Yinqiang", ""], ["Sato", "Imari", ""]]}, {"id": "2104.01622", "submitter": "Andreea Danielescu", "authors": "Andreea Danielescu", "title": "OnTarget: An Electronic Archery Scoring", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  There are several challenges in creating an electronic archery scoring system\nusing computer vision techniques. Variability of light, reconstruction of the\ntarget from several images, variability of target configuration, and filtering\nnoise were significant challenges during the creation of this scoring system.\nThis paper discusses the approach used to determine where an arrow hits a\ntarget, for any possible single or set of targets and provides an algorithm\nthat balances the difficulty of robust arrow detection while retaining the\nrequired accuracy.\n", "versions": [{"version": "v1", "created": "Sun, 4 Apr 2021 15:02:32 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Danielescu", "Andreea", ""]]}, {"id": "2104.01633", "submitter": "Jia-Chang Feng", "authors": "Jia-Chang Feng, Fa-Ting Hong, Wei-Shi Zheng", "title": "MIST: Multiple Instance Self-Training Framework for Video Anomaly\n  Detection", "comments": "Accepted by CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Weakly supervised video anomaly detection (WS-VAD) is to distinguish\nanomalies from normal events based on discriminative representations. Most\nexisting works are limited in insufficient video representations. In this work,\nwe develop a multiple instance self-training framework (MIST)to efficiently\nrefine task-specific discriminative representations with only video-level\nannotations. In particular, MIST is composed of 1) a multiple instance pseudo\nlabel generator, which adapts a sparse continuous sampling strategy to produce\nmore reliable clip-level pseudo labels, and 2) a self-guided attention boosted\nfeature encoder that aims to automatically focus on anomalous regions in frames\nwhile extracting task-specific representations. Moreover, we adopt a\nself-training scheme to optimize both components and finally obtain a\ntask-specific feature encoder. Extensive experiments on two public datasets\ndemonstrate the efficacy of our method, and our method performs comparably to\nor even better than existing supervised and weakly supervised methods,\nspecifically obtaining a frame-level AUC 94.83% on ShanghaiTech.\n", "versions": [{"version": "v1", "created": "Sun, 4 Apr 2021 15:47:14 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Feng", "Jia-Chang", ""], ["Hong", "Fa-Ting", ""], ["Zheng", "Wei-Shi", ""]]}, {"id": "2104.01641", "submitter": "Duy Minh Ho Nguyen", "authors": "Duy M. H. Nguyen, Thu T. Nguyen, Huong Vu, Quang Pham, Manh-Duy\n  Nguyen, Binh T. Nguyen, Daniel Sonntag", "title": "TATL: Task Agnostic Transfer Learning for Skin Attributes Detection", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Existing skin attributes detection methods usually initialize with a\npre-trained Imagenet network and then fine-tune the medical target task.\nHowever, we argue that such approaches are suboptimal because medical datasets\nare largely different from ImageNet and often contain limited training samples.\nIn this work, we propose Task Agnostic Transfer Learning (TATL), a novel\nframework motivated by dermatologists' behaviors in the skincare context. TATL\nlearns an attribute-agnostic segmenter that detects lesion skin regions and\nthen transfers this knowledge to a set of attribute-specific classifiers to\ndetect each particular region's attributes. Since TATL's attribute-agnostic\nsegmenter only detects abnormal skin regions, it enjoys ample data from all\nattributes, allows transferring knowledge among features, and compensates for\nthe lack of training data from rare attributes. We extensively evaluate TATL on\ntwo popular skin attributes detection benchmarks and show that TATL outperforms\nstate-of-the-art methods while enjoying minimal model and computational\ncomplexity. We also provide theoretical insights and explanations for why TATL\nworks well in practice.\n", "versions": [{"version": "v1", "created": "Sun, 4 Apr 2021 16:24:51 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Nguyen", "Duy M. H.", ""], ["Nguyen", "Thu T.", ""], ["Vu", "Huong", ""], ["Pham", "Quang", ""], ["Nguyen", "Manh-Duy", ""], ["Nguyen", "Binh T.", ""], ["Sonntag", "Daniel", ""]]}, {"id": "2104.01681", "submitter": "Grigor Gatchev", "authors": "Grigor Gatchev, Valentin Mollov", "title": "Faster Convolution Inference Through Using Pre-Calculated Lookup Tables", "comments": "11 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Low-cardinality activations permit an algorithm based on fetching the\ninference values from pre-calculated lookup tables instead of calculating them\nevery time. This algorithm can have extensions, some of which offer abilities\nbeyond those of the currently used algorithms. It also allows for a simpler and\nmore effective CNN-specialized hardware.\n", "versions": [{"version": "v1", "created": "Sun, 4 Apr 2021 20:09:20 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Gatchev", "Grigor", ""], ["Mollov", "Valentin", ""]]}, {"id": "2104.01687", "submitter": "Roman Solovyev A", "authors": "Roman Solovyev, Alexandr A. Kalinin, Tatiana Gabruseva", "title": "3D Convolutional Neural Networks for Stalled Brain Capillary Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adequate blood supply is critical for normal brain function. Brain\nvasculature dysfunctions such as stalled blood flow in cerebral capillaries are\nassociated with cognitive decline and pathogenesis in Alzheimer's disease.\nRecent advances in imaging technology enabled generation of high-quality 3D\nimages that can be used to visualize stalled blood vessels. However,\nlocalization of stalled vessels in 3D images is often required as the first\nstep for downstream analysis, which can be tedious, time-consuming and\nerror-prone, when done manually. Here, we describe a deep learning-based\napproach for automatic detection of stalled capillaries in brain images based\non 3D convolutional neural networks. Our networks employed custom 3D data\naugmentations and were used weight transfer from pre-trained 2D models for\ninitialization. We used an ensemble of several 3D models to produce the winning\nsubmission to the Clog Loss: Advance Alzheimer's Research with Stall Catchers\nmachine learning competition that challenged the participants with classifying\nblood vessels in 3D image stacks as stalled or flowing. In this setting, our\napproach outperformed other methods and demonstrated state-of-the-art results,\nachieving 0.85 Matthews correlation coefficient, 85% sensitivity, and 99.3%\nspecificity. The source code for our solution is made publicly available.\n", "versions": [{"version": "v1", "created": "Sun, 4 Apr 2021 20:30:14 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Solovyev", "Roman", ""], ["Kalinin", "Alexandr A.", ""], ["Gabruseva", "Tatiana", ""]]}, {"id": "2104.01703", "submitter": "Hyounghun Kim", "authors": "Hyounghun Kim, Abhay Zala, Graham Burri, Mohit Bansal", "title": "FixMyPose: Pose Correctional Captioning and Retrieval", "comments": "AAAI 2021 (18 pages, 16 figures; webpage:\n  https://fixmypose-unc.github.io/)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interest in physical therapy and individual exercises such as yoga/dance has\nincreased alongside the well-being trend. However, such exercises are hard to\nfollow without expert guidance (which is impossible to scale for personalized\nfeedback to every trainee remotely). Thus, automated pose correction systems\nare required more than ever, and we introduce a new captioning dataset named\nFixMyPose to address this need. We collect descriptions of correcting a\n\"current\" pose to look like a \"target\" pose (in both English and Hindi). The\ncollected descriptions have interesting linguistic properties such as\negocentric relations to environment objects, analogous references, etc.,\nrequiring an understanding of spatial relations and commonsense knowledge about\npostures. Further, to avoid ML biases, we maintain a balance across characters\nwith diverse demographics, who perform a variety of movements in several\ninterior environments (e.g., homes, offices). From our dataset, we introduce\nthe pose-correctional-captioning task and its reverse target-pose-retrieval\ntask. During the correctional-captioning task, models must generate\ndescriptions of how to move from the current to target pose image, whereas in\nthe retrieval task, models should select the correct target pose given the\ninitial pose and correctional description. We present strong cross-attention\nbaseline models (uni/multimodal, RL, multilingual) and also show that our\nbaselines are competitive with other models when evaluated on other\nimage-difference datasets. We also propose new task-specific metrics\n(object-match, body-part-match, direction-match) and conduct human evaluation\nfor more reliable evaluation, and we demonstrate a large human-model\nperformance gap suggesting room for promising future work. To verify the\nsim-to-real transfer of our FixMyPose dataset, we collect a set of real images\nand show promising performance on these images.\n", "versions": [{"version": "v1", "created": "Sun, 4 Apr 2021 21:45:44 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Kim", "Hyounghun", ""], ["Zala", "Abhay", ""], ["Burri", "Graham", ""], ["Bansal", "Mohit", ""]]}, {"id": "2104.01730", "submitter": "Yuanwei Wu", "authors": "Yuanwei Wu, Ziming Zhang and Guanghui Wang", "title": "Branch-and-Pruning Optimization Towards Global Optimality in Deep\n  Learning", "comments": "30 pages, 18 figures. arXiv admin note: substantial text overlap with\n  arXiv:1711.06959", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  It has been attracting more and more attention to understand the global\noptimality in deep learning (DL) recently. However, conventional DL solvers,\nhave not been developed intentionally to seek for such global optimality. In\nthis paper, we propose a novel approximation algorithm, {\\em BPGrad}, towards\noptimizing deep models globally via branch and pruning. The proposed BPGrad\nalgorithm is based on the assumption of Lipschitz continuity in DL, and as a\nresult, it can adaptively determine the step size for the current gradient\ngiven the history of previous updates, wherein theoretically no smaller steps\ncan achieve the global optimality. We prove that, by repeating such a\nbranch-and-pruning procedure, we can locate the global optimality within finite\niterations. Empirically an efficient adaptive solver based on BPGrad for DL is\nproposed as well, and it outperforms conventional DL solvers such as Adagrad,\nAdadelta, RMSProp, and Adam in the tasks of object recognition, detection, and\nsegmentation. The code is available at \\url{https://github.com/RyanCV/BPGrad}.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 00:43:03 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Wu", "Yuanwei", ""], ["Zhang", "Ziming", ""], ["Wang", "Guanghui", ""]]}, {"id": "2104.01732", "submitter": "Chuhua Wang", "authors": "Zhenhua Chen, Chuhua Wang, David J. Crandall", "title": "Adversarial Attack in the Context of Self-driving", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we propose a model that can attack segmentation models with\nsemantic and dynamic targets in the context of self-driving. Specifically, our\nmodel is designed to map an input image as well as its corresponding label to\nperturbations. After adding the perturbation to the input image, the\nadversarial example can manipulate the labels of the pixels in a semantically\nmeaningful way on dynamic targets. In this way, we can make a potential attack\nsubtle and stealthy. To evaluate the stealthiness of our attacking model, we\ndesign three types of tasks, including hiding true labels in the context,\ngenerating fake labels, and displacing labels that belong to some category. The\nexperiments show that our model can attack segmentation models efficiently with\na relatively high success rate on Cityscapes, Mapillary, and BDD100K. We also\nevaluate the generalization of our model across different datasets. Finally, we\npropose a new metric to evaluate the parameter-wise efficiency of attacking\nmodels by comparing the number of parameters used by both the attacking models\nand the target models.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 00:56:45 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Chen", "Zhenhua", ""], ["Wang", "Chuhua", ""], ["Crandall", "David J.", ""]]}, {"id": "2104.01734", "submitter": "Fakai Wang", "authors": "Fakai Wang, Kang Zheng, Yirui Wang, Xiaoyun Zhou, Le Lu, Jing Xiao,\n  Min Wu, Chang-Fu Kuo, Shun Miao", "title": "Opportunistic Screening of Osteoporosis Using Plain Film Chest X-ray", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Osteoporosis is a common chronic metabolic bone disease that is often\nunder-diagnosed and under-treated due to the limited access to bone mineral\ndensity (BMD) examinations, Dual-energy X-ray Absorptiometry (DXA). In this\npaper, we propose a method to predict BMD from Chest X-ray (CXR), one of the\nmost common, accessible, and low-cost medical image examinations. Our method\nfirst automatically detects Regions of Interest (ROIs) of local and global bone\nstructures from the CXR. Then a multi-ROI model is developed to exploit both\nlocal and global information in the chest X-ray image for accurate BMD\nestimation. Our method is evaluated on 329 CXR cases with ground truth BMD\nmeasured by DXA. The model predicted BMD has a strong correlation with the gold\nstandard DXA BMD (Pearson correlation coefficient 0.840). When applied for\nosteoporosis screening, it achieves a high classification performance (AUC\n0.936). As the first effort in the field to use CXR scans to predict the spine\nBMD, the proposed algorithm holds strong potential in enabling early\nosteoporosis screening through routine chest X-rays and contributing to the\nenhancement of public health.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 01:25:23 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Wang", "Fakai", ""], ["Zheng", "Kang", ""], ["Wang", "Yirui", ""], ["Zhou", "Xiaoyun", ""], ["Lu", "Le", ""], ["Xiao", "Jing", ""], ["Wu", "Min", ""], ["Kuo", "Chang-Fu", ""], ["Miao", "Shun", ""]]}, {"id": "2104.01735", "submitter": "Cheng-You Lu", "authors": "Yung-Han Ho, Guo-Lun Jin, Yun Liang, Wen-Hsiao Peng, Xiaobo Li", "title": "A Dual-Critic Reinforcement Learning Framework for Frame-level Bit\n  Allocation in HEVC/H.265", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper introduces a dual-critic reinforcement learning (RL) framework to\naddress the problem of frame-level bit allocation in HEVC/H.265. The objective\nis to minimize the distortion of a group of pictures (GOP) under a rate\nconstraint. Previous RL-based methods tackle such a constrained optimization\nproblem by maximizing a single reward function that often combines a distortion\nand a rate reward. However, the way how these rewards are combined is usually\nad hoc and may not generalize well to various coding conditions and video\nsequences. To overcome this issue, we adapt the deep deterministic policy\ngradient (DDPG) reinforcement learning algorithm for use with two critics, with\none learning to predict the distortion reward and the other the rate reward. In\nparticular, the distortion critic works to update the agent when the rate\nconstraint is satisfied. By contrast, the rate critic makes the rate constraint\na priority when the agent goes over the bit budget. Experimental results on\ncommonly used datasets show that our method outperforms the bit allocation\nscheme in x265 and the single-critic baseline by a significant margin in terms\nof rate-distortion performance while offering fairly precise rate control.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 01:26:52 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Ho", "Yung-Han", ""], ["Jin", "Guo-Lun", ""], ["Liang", "Yun", ""], ["Peng", "Wen-Hsiao", ""], ["Li", "Xiaobo", ""]]}, {"id": "2104.01742", "submitter": "Robin M. Schmidt", "authors": "Robin M. Schmidt", "title": "Explainability-aided Domain Generalization for Image Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Traditionally, for most machine learning settings, gaining some degree of\nexplainability that tries to give users more insights into how and why the\nnetwork arrives at its predictions, restricts the underlying model and hinders\nperformance to a certain degree. For example, decision trees are thought of as\nbeing more explainable than deep neural networks but they lack performance on\nvisual tasks. In this work, we empirically demonstrate that applying methods\nand architectures from the explainability literature can, in fact, achieve\nstate-of-the-art performance for the challenging task of domain generalization\nwhile offering a framework for more insights into the prediction and training\nprocess. For that, we develop a set of novel algorithms including DivCAM, an\napproach where the network receives guidance during training via gradient based\nclass activation maps to focus on a diverse set of discriminative features, as\nwell as ProDrop and D-Transformers which apply prototypical networks to the\ndomain generalization task, either with self-challenging or attention\nalignment. Since these methods offer competitive performance on top of\nexplainability, we argue that the proposed methods can be used as a tool to\nimprove the robustness of deep neural network architectures.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 02:27:01 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Schmidt", "Robin M.", ""]]}, {"id": "2104.01745", "submitter": "Pingping Zhang Dr", "authors": "Xuehu Liu and Pingping Zhang and Chenyang Yu and Huchuan Lu and\n  Xuesheng Qian and Xiaoyun Yang", "title": "A Video Is Worth Three Views: Trigeminal Transformers for Video-based\n  Person Re-identification", "comments": "This work includes 10 pages, 5 figures and 4 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video-based person re-identification (Re-ID) aims to retrieve video sequences\nof the same person under non-overlapping cameras. Previous methods usually\nfocus on limited views, such as spatial, temporal or spatial-temporal view,\nwhich lack of the observations in different feature domains. To capture richer\nperceptions and extract more comprehensive video representations, in this paper\nwe propose a novel framework named Trigeminal Transformers (TMT) for\nvideo-based person Re-ID. More specifically, we design a trigeminal feature\nextractor to jointly transform raw video data into spatial, temporal and\nspatial-temporal domain. Besides, inspired by the great success of vision\ntransformer, we introduce the transformer structure for video-based person\nRe-ID. In our work, three self-view transformers are proposed to exploit the\nrelationships between local features for information enhancement in spatial,\ntemporal and spatial-temporal domains. Moreover, a cross-view transformer is\nproposed to aggregate the multi-view features for comprehensive video\nrepresentations. The experimental results indicate that our approach can\nachieve better performance than other state-of-the-art approaches on public\nRe-ID benchmarks. We will release the code for model reproduction.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 02:50:16 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Liu", "Xuehu", ""], ["Zhang", "Pingping", ""], ["Yu", "Chenyang", ""], ["Lu", "Huchuan", ""], ["Qian", "Xuesheng", ""], ["Yang", "Xiaoyun", ""]]}, {"id": "2104.01753", "submitter": "Jia-Wei Chen", "authors": "Jia-Wei Chen, Li-Ju Chen, Chia-Mu Yu, Chun-Shien Lu", "title": "Perceptual Indistinguishability-Net (PI-Net): Facial Image Obfuscation\n  with Manipulable Semantics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the growing use of camera devices, the industry has many image datasets\nthat provide more opportunities for collaboration between the machine learning\ncommunity and industry. However, the sensitive information in the datasets\ndiscourages data owners from releasing these datasets. Despite recent research\ndevoted to removing sensitive information from images, they provide neither\nmeaningful privacy-utility trade-off nor provable privacy guarantees. In this\nstudy, with the consideration of the perceptual similarity, we propose\nperceptual indistinguishability (PI) as a formal privacy notion particularly\nfor images. We also propose PI-Net, a privacy-preserving mechanism that\nachieves image obfuscation with PI guarantee. Our study shows that PI-Net\nachieves significantly better privacy utility trade-off through public image\ndata.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 03:40:07 GMT"}, {"version": "v2", "created": "Wed, 7 Apr 2021 09:06:15 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Chen", "Jia-Wei", ""], ["Chen", "Li-Ju", ""], ["Yu", "Chia-Mu", ""], ["Lu", "Chun-Shien", ""]]}, {"id": "2104.01754", "submitter": "Jun Li", "authors": "Dengsheng Chen and Haowen Deng and Jun Li and Duo Li and Yao Duan and\n  Kai Xu", "title": "Potential Convolution: Embedding Point Clouds into Potential Fields", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, various convolutions based on continuous or discrete kernels for\npoint cloud processing have been widely studied, and achieve impressive\nperformance in many applications, such as shape classification, scene\nsegmentation and so on. However, they still suffer from some drawbacks. For\ncontinuous kernels, the inaccurate estimation of the kernel weights constitutes\na bottleneck for further improving the performance; while for discrete ones,\nthe kernels represented as the points located in the 3D space are lack of rich\ngeometry information. In this work, rather than defining a continuous or\ndiscrete kernel, we directly embed convolutional kernels into the learnable\npotential fields, giving rise to potential convolution. It is convenient for us\nto define various potential functions for potential convolution which can\ngeneralize well to a wide range of tasks. Specifically, we provide two simple\nyet effective potential functions via point-wise convolution operations.\nComprehensive experiments demonstrate the effectiveness of our method, which\nachieves superior performance on the popular 3D shape classification and scene\nsegmentation benchmarks compared with other state-of-the-art point convolution\nmethods.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 03:46:09 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Chen", "Dengsheng", ""], ["Deng", "Haowen", ""], ["Li", "Jun", ""], ["Li", "Duo", ""], ["Duan", "Yao", ""], ["Xu", "Kai", ""]]}, {"id": "2104.01762", "submitter": "Yanhong Zeng", "authors": "Yanhong Zeng, Jianlong Fu, Hongyang Chao", "title": "3D Human Body Reshaping with Anthropometric Modeling", "comments": "ICIMCS 2017(oral). The final publication is available at Springer via\n  https://doi.org/10.1007/978-981-10-8530-7_10", "journal-ref": "In International Conference on Internet Multimedia Computing and\n  Service (pp. 96-107). Springer, Singapore (2017)", "doi": "10.1007/978-981-10-8530-7_10", "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Reshaping accurate and realistic 3D human bodies from anthropometric\nparameters (e.g., height, chest size, etc.) poses a fundamental challenge for\nperson identification, online shopping and virtual reality. Existing approaches\nfor creating such 3D shapes often suffer from complex measurement by range\ncameras or high-end scanners, which either involve heavy expense cost or result\nin low quality. However, these high-quality equipments limit existing\napproaches in real applications, because the equipments are not easily\naccessible for common users. In this paper, we have designed a 3D human body\nreshaping system by proposing a novel feature-selection-based local mapping\ntechnique, which enables automatic anthropometric parameter modeling for each\nbody facet. Note that the proposed approach can leverage limited anthropometric\nparameters (i.e., 3-5 measurements) as input, which avoids complex measurement,\nand thus better user-friendly experience can be achieved in real scenarios.\nSpecifically, the proposed reshaping model consists of three steps. First, we\ncalculate full-body anthropometric parameters from limited user inputs by\nimputation technique, and thus essential anthropometric parameters for 3D body\nreshaping can be obtained. Second, we select the most relevant anthropometric\nparameters for each facet by adopting relevance masks, which are learned\noffline by the proposed local mapping technique. Third, we generate the 3D body\nmeshes by mapping matrices, which are learned by linear regression from the\nselected parameters to mesh-based body representation. We conduct experiments\nby anthropomorphic evaluation and a user study from 68 volunteers. Experiments\nshow the superior results of the proposed system in terms of mean\nreconstruction error against the state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 04:09:39 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Zeng", "Yanhong", ""], ["Fu", "Jianlong", ""], ["Chao", "Hongyang", ""]]}, {"id": "2104.01766", "submitter": "Dong He", "authors": "Dong He, Jie Cheng, Jong-Hwan Kim", "title": "GSECnet: Ground Segmentation of Point Clouds for Edge Computing", "comments": "6 pages, 5 figures, code is available at\n  https://github.com/SAMMiCA/GSECnet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Ground segmentation of point clouds remains challenging because of the sparse\nand unordered data structure. This paper proposes the GSECnet - Ground\nSegmentation network for Edge Computing, an efficient ground segmentation\nframework of point clouds specifically designed to be deployable on a low-power\nedge computing unit. First, raw point clouds are converted into a\ndiscretization representation by pillarization. Afterward, features of points\nwithin pillars are fed into PointNet to get the corresponding pillars feature\nmap. Then, a depthwise-separable U-Net with the attention module learns the\nclassification from the pillars feature map with an enormously diminished model\nparameter size. Our proposed framework is evaluated on SemanticKITTI against\nboth point-based and discretization-based state-of-the-art learning approaches,\nand achieves an excellent balance between high accuracy and low computing\ncomplexity. Remarkably, our framework achieves the inference runtime of 135.2\nHz on a desktop platform. Moreover, experiments verify that it is deployable on\na low-power edge computing unit powered 10 watts only.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 04:29:28 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["He", "Dong", ""], ["Cheng", "Jie", ""], ["Kim", "Jong-Hwan", ""]]}, {"id": "2104.01769", "submitter": "Wei-Lun Chao", "authors": "Han-Jia Ye, De-Chuan Zhan, Wei-Lun Chao", "title": "Procrustean Training for Imbalanced Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Neural networks trained with class-imbalanced data are known to perform\npoorly on minor classes of scarce training data. Several recent works attribute\nthis to over-fitting to minor classes. In this paper, we provide a novel\nexplanation of this issue. We found that a neural network tends to first\nunder-fit the minor classes by classifying most of their data into the major\nclasses in early training epochs. To correct these wrong predictions, the\nneural network then must focus on pushing features of minor class data across\nthe decision boundaries between major and minor classes, leading to much larger\ngradients for features of minor classes. We argue that such an under-fitting\nphase over-emphasizes the competition between major and minor classes, hinders\nthe neural network from learning the discriminative knowledge that can be\ngeneralized to test data, and eventually results in over-fitting. To address\nthis issue, we propose a novel learning strategy to equalize the training\nprogress across classes. We mix features of the major class data with those of\nother data in a mini-batch, intentionally weakening their features to prevent a\nneural network from fitting them first. We show that this strategy can largely\nbalance the training accuracy and feature gradients across classes, effectively\nmitigating the under-fitting then over-fitting problem for minor class data. On\nseveral benchmark datasets, our approach achieves the state-of-the-art\naccuracy, especially for the challenging step-imbalanced cases.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 04:44:01 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Ye", "Han-Jia", ""], ["Zhan", "De-Chuan", ""], ["Chao", "Wei-Lun", ""]]}, {"id": "2104.01771", "submitter": "Yunhe Gao", "authors": "Yunhe Gao, Rui Huang, Yiwei Yang, Jie Zhang, Kainan Shao, Changjuan\n  Tao, Yuanyuan Chen, Dimitris N. Metaxas, Hongsheng Li, Ming Chen", "title": "FocusNetv2: Imbalanced Large and Small Organ Segmentation with\n  Adversarial Shape Constraint for Head and Neck CT Images", "comments": "Accepted by Medical Image Analysis", "journal-ref": null, "doi": "10.1016/j.media.2020.101831", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Radiotherapy is a treatment where radiation is used to eliminate cancer\ncells. The delineation of organs-at-risk (OARs) is a vital step in radiotherapy\ntreatment planning to avoid damage to healthy organs. For nasopharyngeal\ncancer, more than 20 OARs are needed to be precisely segmented in advance. The\nchallenge of this task lies in complex anatomical structure, low-contrast organ\ncontours, and the extremely imbalanced size between large and small organs.\nCommon segmentation methods that treat them equally would generally lead to\ninaccurate small-organ labeling. We propose a novel two-stage deep neural\nnetwork, FocusNetv2, to solve this challenging problem by automatically\nlocating, ROI-pooling, and segmenting small organs with specifically designed\nsmall-organ localization and segmentation sub-networks while maintaining the\naccuracy of large organ segmentation. In addition to our original FocusNet, we\nemploy a novel adversarial shape constraint on small organs to ensure the\nconsistency between estimated small-organ shapes and organ shape prior\nknowledge. Our proposed framework is extensively tested on both self-collected\ndataset of 1,164 CT scans and the MICCAI Head and Neck Auto Segmentation\nChallenge 2015 dataset, which shows superior performance compared with\nstate-of-the-art head and neck OAR segmentation methods.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 04:45:31 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Gao", "Yunhe", ""], ["Huang", "Rui", ""], ["Yang", "Yiwei", ""], ["Zhang", "Jie", ""], ["Shao", "Kainan", ""], ["Tao", "Changjuan", ""], ["Chen", "Yuanyuan", ""], ["Metaxas", "Dimitris N.", ""], ["Li", "Hongsheng", ""], ["Chen", "Ming", ""]]}, {"id": "2104.01772", "submitter": "Haimin Luo", "authors": "Haimin Luo, Anpei Chen, Qixuan Zhang, Bai Pang, Minye Wu, Lan Xu, and\n  Jingyi Yu", "title": "Convolutional Neural Opacity Radiance Fields", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Photo-realistic modeling and rendering of fuzzy objects with complex opacity\nare critical for numerous immersive VR/AR applications, but it suffers from\nstrong view-dependent brightness, color. In this paper, we propose a novel\nscheme to generate opacity radiance fields with a convolutional neural renderer\nfor fuzzy objects, which is the first to combine both explicit opacity\nsupervision and convolutional mechanism into the neural radiance field\nframework so as to enable high-quality appearance and global consistent alpha\nmattes generation in arbitrary novel views. More specifically, we propose an\nefficient sampling strategy along with both the camera rays and image plane,\nwhich enables efficient radiance field sampling and learning in a patch-wise\nmanner, as well as a novel volumetric feature integration scheme that generates\nper-patch hybrid feature embeddings to reconstruct the view-consistent\nfine-detailed appearance and opacity output. We further adopt a patch-wise\nadversarial training scheme to preserve both high-frequency appearance and\nopacity details in a self-supervised framework. We also introduce an effective\nmulti-view image capture system to capture high-quality color and alpha maps\nfor challenging fuzzy objects. Extensive experiments on existing and our new\nchallenging fuzzy object dataset demonstrate that our method achieves\nphoto-realistic, globally consistent, and fined detailed appearance and opacity\nfree-viewpoint rendering for various fuzzy objects.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 04:46:46 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Luo", "Haimin", ""], ["Chen", "Anpei", ""], ["Zhang", "Qixuan", ""], ["Pang", "Bai", ""], ["Wu", "Minye", ""], ["Xu", "Lan", ""], ["Yu", "Jingyi", ""]]}, {"id": "2104.01781", "submitter": "Astuti Sharma", "authors": "Apoorva Gokhale, Astuti Sharma, Kaustav Datta, Savyasachi", "title": "Reducing Racial Bias in Facial Age Prediction using Unsupervised Domain\n  Adaptation in Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We propose an approach for unsupervised domain adaptation for the task of\nestimating someone's age from a given face image. In order to avoid the\npropagation of racial bias in most publicly available face image datasets into\nthe inefficacy of models trained on them, we perform domain adaptation to\nmotivate the predictor to learn features that are invariant to ethnicity,\nenhancing the generalization performance across faces of people from different\nethnic backgrounds. Exploiting the ordinality of age, we also impose ranking\nconstraints on the prediction of the model and design our model such that it\ntakes as input a pair of images, and outputs both the relative age difference\nand the rank of the first identity with respect to the other in terms of their\nages. Furthermore, we implement Multi-Dimensional Scaling to retrieve absolute\nages from the predicted age differences from as few as two labeled images from\nthe domain to be adapted to. We experiment with a publicly available dataset\nwith age labels, dividing it into subsets based on the ethnicity labels, and\nevaluating the performance of our approach on the data from an ethnicity\ndifferent from the one that the model is trained on. Additionally, we impose a\nconstraint to preserve the sanity of the predictions with respect to relative\nand absolute ages, and another to ensure the smoothness of the predictions with\nrespect to the input. We experiment extensively and compare various domain\nadaptation approaches for the task of regression.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 05:31:12 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Gokhale", "Apoorva", ""], ["Sharma", "Astuti", ""], ["Datta", "Kaustav", ""], ["Savyasachi", "", ""]]}, {"id": "2104.01784", "submitter": "Wenbo Zhang", "authors": "Wenbo Zhang, Yao Jiang, Keren Fu, Qijun Zhao", "title": "BTS-Net: Bi-directional Transfer-and-Selection Network For RGB-D Salient\n  Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Depth information has been proved beneficial in RGB-D salient object\ndetection (SOD). However, depth maps obtained often suffer from low quality and\ninaccuracy. Most existing RGB-D SOD models have no cross-modal interactions or\nonly have unidirectional interactions from depth to RGB in their encoder\nstages, which may lead to inaccurate encoder features when facing low quality\ndepth. To address this limitation, we propose to conduct progressive\nbi-directional interactions as early in the encoder stage, yielding a novel\nbi-directional transfer-and-selection network named BTS-Net, which adopts a set\nof bi-directional transfer-and-selection (BTS) modules to purify features\nduring encoding. Based on the resulting robust encoder features, we also design\nan effective light-weight group decoder to achieve accurate final saliency\nprediction. Comprehensive experiments on six widely used datasets demonstrate\nthat BTS-Net surpasses 16 latest state-of-the-art approaches in terms of four\nkey metrics.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 05:58:43 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Zhang", "Wenbo", ""], ["Jiang", "Yao", ""], ["Fu", "Keren", ""], ["Zhao", "Qijun", ""]]}, {"id": "2104.01789", "submitter": "Guannan Lou", "authors": "Yao Deng, Tiehua Zhang, Guannan Lou, Xi Zheng, Jiong Jin, Qing-Long\n  Han", "title": "Deep Learning-Based Autonomous Driving Systems: A Survey of Attacks and\n  Defenses", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rapid development of artificial intelligence, especially deep learning\ntechnology, has advanced autonomous driving systems (ADSs) by providing precise\ncontrol decisions to counterpart almost any driving event, spanning from\nanti-fatigue safe driving to intelligent route planning. However, ADSs are\nstill plagued by increasing threats from different attacks, which could be\ncategorized into physical attacks, cyberattacks and learning-based adversarial\nattacks. Inevitably, the safety and security of deep learning-based autonomous\ndriving are severely challenged by these attacks, from which the\ncountermeasures should be analyzed and studied comprehensively to mitigate all\npotential risks. This survey provides a thorough analysis of different attacks\nthat may jeopardize ADSs, as well as the corresponding state-of-the-art defense\nmechanisms. The analysis is unrolled by taking an in-depth overview of each\nstep in the ADS workflow, covering adversarial attacks for various deep\nlearning models and attacks in both physical and cyber context. Furthermore,\nsome promising research directions are suggested in order to improve deep\nlearning-based autonomous driving safety, including model robustness training,\nmodel testing and verification, and anomaly detection based on cloud/edge\nservers.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 06:31:47 GMT"}, {"version": "v2", "created": "Sat, 10 Apr 2021 02:28:02 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Deng", "Yao", ""], ["Zhang", "Tiehua", ""], ["Lou", "Guannan", ""], ["Zheng", "Xi", ""], ["Jin", "Jiong", ""], ["Han", "Qing-Long", ""]]}, {"id": "2104.01792", "submitter": "Hiroaki Aizawa", "authors": "Hiroaki Aizawa, Yukihiro Domae, Kunihito Kato", "title": "Hierarchical Pyramid Representations for Semantic Segmentation", "comments": "Project page: https://aizawan.github.io/hdca/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the context of complex and cluttered scenes is a challenging\nproblem for semantic segmentation. However, it is difficult to model the\ncontext without prior and additional supervision because the scene's factors,\nsuch as the scale, shape, and appearance of objects, vary considerably in these\nscenes. To solve this, we propose to learn the structures of objects and the\nhierarchy among objects because context is based on these intrinsic properties.\nIn this study, we design novel hierarchical, contextual, and multiscale\npyramidal representations to capture the properties from an input image. Our\nkey idea is the recursive segmentation in different hierarchical regions based\non a predefined number of regions and the aggregation of the context in these\nregions. The aggregated contexts are used to predict the contextual\nrelationship between the regions and partition the regions in the following\nhierarchical level. Finally, by constructing the pyramid representations from\nthe recursively aggregated context, multiscale and hierarchical properties are\nattained. In the experiments, we confirmed that our proposed method achieves\nstate-of-the-art performance in PASCAL Context.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 06:39:12 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Aizawa", "Hiroaki", ""], ["Domae", "Yukihiro", ""], ["Kato", "Kunihito", ""]]}, {"id": "2104.01797", "submitter": "Yu Cheng", "authors": "Yu Cheng, Bo Wang, Bo Yang, Robby T. Tan", "title": "Monocular 3D Multi-Person Pose Estimation by Integrating Top-Down and\n  Bottom-Up Networks", "comments": "Accepted to CVPR 2021. Code is available at:\n  https://github.com/3dpose/3D-Multi-Person-Pose", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In monocular video 3D multi-person pose estimation, inter-person occlusion\nand close interactions can cause human detection to be erroneous and\nhuman-joints grouping to be unreliable. Existing top-down methods rely on human\ndetection and thus suffer from these problems. Existing bottom-up methods do\nnot use human detection, but they process all persons at once at the same\nscale, causing them to be sensitive to multiple-persons scale variations. To\naddress these challenges, we propose the integration of top-down and bottom-up\napproaches to exploit their strengths. Our top-down network estimates human\njoints from all persons instead of one in an image patch, making it robust to\npossible erroneous bounding boxes. Our bottom-up network incorporates\nhuman-detection based normalized heatmaps, allowing the network to be more\nrobust in handling scale variations. Finally, the estimated 3D poses from the\ntop-down and bottom-up networks are fed into our integration network for final\n3D poses. Besides the integration of top-down and bottom-up networks, unlike\nexisting pose discriminators that are designed solely for single person, and\nconsequently cannot assess natural inter-person interactions, we propose a\ntwo-person pose discriminator that enforces natural two-person interactions.\nLastly, we also apply a semi-supervised method to overcome the 3D ground-truth\ndata scarcity. Our quantitative and qualitative evaluations show the\neffectiveness of our method compared to the state-of-the-art baselines.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 07:05:21 GMT"}, {"version": "v2", "created": "Wed, 7 Apr 2021 06:22:10 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Cheng", "Yu", ""], ["Wang", "Bo", ""], ["Yang", "Bo", ""], ["Tan", "Robby T.", ""]]}, {"id": "2104.01832", "submitter": "Chaoqun Wang", "authors": "Chaoqun Wang, Xuejin Chen, Shaobo Min, Xiaoyan Sun, Houqiang Li", "title": "Task-Independent Knowledge Makes for Transferable Representations for\n  Generalized Zero-Shot Learning", "comments": "Accepted at AAAI2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Generalized Zero-Shot Learning (GZSL) targets recognizing new categories by\nlearning transferable image representations. Existing methods find that, by\naligning image representations with corresponding semantic labels, the\nsemantic-aligned representations can be transferred to unseen categories.\nHowever, supervised by only seen category labels, the learned semantic\nknowledge is highly task-specific, which makes image representations biased\ntowards seen categories. In this paper, we propose a novel Dual-Contrastive\nEmbedding Network (DCEN) that simultaneously learns task-specific and\ntask-independent knowledge via semantic alignment and instance discrimination.\nFirst, DCEN leverages task labels to cluster representations of the same\nsemantic category by cross-modal contrastive learning and exploring\nsemantic-visual complementarity. Besides task-specific knowledge, DCEN then\nintroduces task-independent knowledge by attracting representations of\ndifferent views of the same image and repelling representations of different\nimages. Compared to high-level seen category supervision, this instance\ndiscrimination supervision encourages DCEN to capture low-level visual\nknowledge, which is less biased toward seen categories and alleviates the\nrepresentation bias. Consequently, the task-specific and task-independent\nknowledge jointly make for transferable representations of DCEN, which obtains\naveraged 4.1% improvement on four public benchmarks.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 10:05:48 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Wang", "Chaoqun", ""], ["Chen", "Xuejin", ""], ["Min", "Shaobo", ""], ["Sun", "Xiaoyan", ""], ["Li", "Houqiang", ""]]}, {"id": "2104.01845", "submitter": "Sk Miraj Ahmed", "authors": "Sk Miraj Ahmed, Dripta S. Raychaudhuri, Sujoy Paul, Samet Oymak, Amit\n  K. Roy-Chowdhury", "title": "Unsupervised Multi-source Domain Adaptation Without Access to Source\n  Data", "comments": "This paper will appear at CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Unsupervised Domain Adaptation (UDA) aims to learn a predictor model for an\nunlabeled domain by transferring knowledge from a separate labeled source\ndomain. However, most of these conventional UDA approaches make the strong\nassumption of having access to the source data during training, which may not\nbe very practical due to privacy, security and storage concerns. A recent line\nof work addressed this problem and proposed an algorithm that transfers\nknowledge to the unlabeled target domain from a single source model without\nrequiring access to the source data. However, for adaptation purposes, if there\nare multiple trained source models available to choose from, this method has to\ngo through adapting each and every model individually, to check for the best\nsource. Thus, we ask the question: can we find the optimal combination of\nsource models, with no source data and without target labels, whose performance\nis no worse than the single best source? To answer this, we propose a novel and\nefficient algorithm which automatically combines the source models with\nsuitable weights in such a way that it performs at least as good as the best\nsource model. We provide intuitive theoretical insights to justify our claim.\nFurthermore, extensive experiments are conducted on several benchmark datasets\nto show the effectiveness of our algorithm, where in most cases, our method not\nonly reaches best source accuracy but also outperforms it.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 10:45:12 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Ahmed", "Sk Miraj", ""], ["Raychaudhuri", "Dripta S.", ""], ["Paul", "Sujoy", ""], ["Oymak", "Samet", ""], ["Roy-Chowdhury", "Amit K.", ""]]}, {"id": "2104.01854", "submitter": "Mohammad Azangoo", "authors": "Seppo Sierla (1), Mohammad Azangoo (1), Alexander Fay (2), Valeriy\n  Vyatkin (1 and 3), and Nikolaos Papakonstantinou (4) ((1) Department of\n  Electrical Engineering and Automation, Aalto University, Espoo, Finland, (2)\n  Department of Automation Engineering, Helmut Schmidt University, Hamburg,\n  Germany, (3) Department of Computer Science, Electrical and Space\n  Engineering, Lule{\\aa} University of Technology, Lule{\\aa}, Sweden, (4) VTT\n  Technical Research Centre of Finland Ltd, Espoo, Finland)", "title": "Integrating 2D and 3D Digital Plant Information Towards Automatic\n  Generation of Digital Twins", "comments": "8 pages, 13 figures", "journal-ref": null, "doi": "10.1109/ISIE45063.2020.9152371", "report-no": null, "categories": "eess.SY cs.AI cs.CV cs.IT cs.SE cs.SY math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ongoing standardization in Industry 4.0 supports tool vendor neutral\nrepresentations of Piping and Instrumentation diagrams as well as 3D pipe\nrouting. However, a complete digital plant model requires combining these two\nrepresentations. 3D pipe routing information is essential for building any\naccurate first-principles process simulation model. Piping and instrumentation\ndiagrams are the primary source for control loops. In order to automatically\nintegrate these information sources to a unified digital plant model, it is\nnecessary to develop algorithms for identifying corresponding elements such as\ntanks and pumps from piping and instrumentation diagrams and 3D CAD models. One\napproach is to raise these two information sources to a common level of\nabstraction and to match them at this level of abstraction. Graph matching is a\npotential technique for this purpose. This article focuses on automatic\ngeneration of the graphs as a prerequisite to graph matching. Algorithms for\nthis purpose are proposed and validated with a case study. The paper concludes\nwith a discussion of further research needed to reprocess the generated graphs\nin order to enable effective matching.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 11:07:05 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Sierla", "Seppo", "", "1 and 3"], ["Azangoo", "Mohammad", "", "1 and 3"], ["Fay", "Alexander", "", "1 and 3"], ["Vyatkin", "Valeriy", "", "1 and 3"], ["Papakonstantinou", "Nikolaos", ""]]}, {"id": "2104.01867", "submitter": "Thao Nguyen", "authors": "Thao Nguyen, Anh Tran, Minh Hoai", "title": "Lipstick ain't enough: Beyond Color Matching for In-the-Wild Makeup\n  Transfer", "comments": "Accepted to CVPR'21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Makeup transfer is the task of applying on a source face the makeup style\nfrom a reference image. Real-life makeups are diverse and wild, which cover not\nonly color-changing but also patterns, such as stickers, blushes, and\njewelries. However, existing works overlooked the latter components and\nconfined makeup transfer to color manipulation, focusing only on light makeup\nstyles. In this work, we propose a holistic makeup transfer framework that can\nhandle all the mentioned makeup components. It consists of an improved color\ntransfer branch and a novel pattern transfer branch to learn all makeup\nproperties, including color, shape, texture, and location. To train and\nevaluate such a system, we also introduce new makeup datasets for real and\nsynthetic extreme makeup. Experimental results show that our framework achieves\nthe state of the art performance on both light and extreme makeup styles. Code\nis available at https://github.com/VinAIResearch/CPM.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 12:12:56 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Nguyen", "Thao", ""], ["Tran", "Anh", ""], ["Hoai", "Minh", ""]]}, {"id": "2104.01876", "submitter": "Ayan Kumar Bhunia", "authors": "Ayan Kumar Bhunia, Shuvozit Ghose, Amandeep Kumar, Pinaki Nath\n  Chowdhury, Aneeshan Sain, Yi-Zhe Song", "title": "MetaHTR: Towards Writer-Adaptive Handwritten Text Recognition", "comments": "IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Handwritten Text Recognition (HTR) remains a challenging problem to date,\nlargely due to the varying writing styles that exist amongst us. Prior works\nhowever generally operate with the assumption that there is a limited number of\nstyles, most of which have already been captured by existing datasets. In this\npaper, we take a completely different perspective -- we work on the assumption\nthat there is always a new style that is drastically different, and that we\nwill only have very limited data during testing to perform adaptation. This\nresults in a commercially viable solution -- the model has the best shot at\nadaptation being exposed to the new style, and the few samples nature makes it\npractical to implement. We achieve this via a novel meta-learning framework\nwhich exploits additional new-writer data through a support set, and outputs a\nwriter-adapted model via single gradient step update, all during inference. We\ndiscover and leverage on the important insight that there exists few key\ncharacters per writer that exhibit relatively larger style discrepancies. For\nthat, we additionally propose to meta-learn instance specific weights for a\ncharacter-wise cross-entropy loss, which is specifically designed to work with\nthe sequential nature of text data. Our writer-adaptive MetaHTR framework can\nbe easily implemented on the top of most state-of-the-art HTR models.\nExperiments show an average performance gain of 5-7% can be obtained by\nobserving very few new style data. We further demonstrate via a set of ablative\nstudies the advantage of our meta design when compared with alternative\nadaption mechanisms.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 12:35:39 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Bhunia", "Ayan Kumar", ""], ["Ghose", "Shuvozit", ""], ["Kumar", "Amandeep", ""], ["Chowdhury", "Pinaki Nath", ""], ["Sain", "Aneeshan", ""], ["Song", "Yi-Zhe", ""]]}, {"id": "2104.01888", "submitter": "Haoran Wei", "authors": "Haoran Wei, Qingbo Wu, Hui Li, King Ngi Ngan, Hongliang Li, Fanman\n  Meng, and Linfeng Xu", "title": "Non-Homogeneous Haze Removal via Artificial Scene Prior and\n  Bidimensional Graph Reasoning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Due to the lack of natural scene and haze prior information, it is greatly\nchallenging to completely remove the haze from single image without distorting\nits visual content. Fortunately, the real-world haze usually presents\nnon-homogeneous distribution, which provides us with many valuable clues in\npartial well-preserved regions. In this paper, we propose a Non-Homogeneous\nHaze Removal Network (NHRN) via artificial scene prior and bidimensional graph\nreasoning. Firstly, we employ the gamma correction iteratively to simulate\nartificial multiple shots under different exposure conditions, whose haze\ndegrees are different and enrich the underlying scene prior. Secondly, beyond\nutilizing the local neighboring relationship, we build a bidimensional graph\nreasoning module to conduct non-local filtering in the spatial and channel\ndimensions of feature maps, which models their long-range dependency and\npropagates the natural scene prior between the well-preserved nodes and the\nnodes contaminated by haze. We evaluate our method on different benchmark\ndatasets. The results demonstrate that our method achieves superior performance\nover many state-of-the-art algorithms for both the single image dehazing and\nhazy image understanding tasks.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 13:04:44 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Wei", "Haoran", ""], ["Wu", "Qingbo", ""], ["Li", "Hui", ""], ["Ngan", "King Ngi", ""], ["Li", "Hongliang", ""], ["Meng", "Fanman", ""], ["Xu", "Linfeng", ""]]}, {"id": "2104.01889", "submitter": "Itzik Malkiel", "authors": "Itzik Malkiel, Sangtae Ahn, Valentina Taviani, Anne Menini, Lior Wolf,\n  Christopher J. Hardy", "title": "Adaptive Gradient Balancing for Undersampled MRI Reconstruction and\n  Image-to-Image Translation", "comments": "arXiv admin note: substantial text overlap with arXiv:1905.00985", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent accelerated MRI reconstruction models have used Deep Neural Networks\n(DNNs) to reconstruct relatively high-quality images from highly undersampled\nk-space data, enabling much faster MRI scanning. However, these techniques\nsometimes struggle to reconstruct sharp images that preserve fine detail while\nmaintaining a natural appearance. In this work, we enhance the image quality by\nusing a Conditional Wasserstein Generative Adversarial Network combined with a\nnovel Adaptive Gradient Balancing (AGB) technique that automates the process of\ncombining the adversarial and pixel-wise terms and streamlines hyperparameter\ntuning. In addition, we introduce a Densely Connected Iterative Network, which\nis an undersampled MRI reconstruction network that utilizes dense connections.\nIn MRI, our method minimizes artifacts, while maintaining a high-quality\nreconstruction that produces sharper images than other techniques. To\ndemonstrate the general nature of our method, it is further evaluated on a\nbattery of image-to-image translation experiments, demonstrating an ability to\nrecover from sub-optimal weighting in multi-term adversarial training.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 13:05:22 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Malkiel", "Itzik", ""], ["Ahn", "Sangtae", ""], ["Taviani", "Valentina", ""], ["Menini", "Anne", ""], ["Wolf", "Lior", ""], ["Hardy", "Christopher J.", ""]]}, {"id": "2104.01893", "submitter": "Gen Li", "authors": "Gen Li, Varun Jampani, Laura Sevilla-Lara, Deqing Sun, Jonghyun Kim,\n  Joongkyu Kim", "title": "Adaptive Prototype Learning and Allocation for Few-Shot Segmentation", "comments": "Accepted to CVPR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prototype learning is extensively used for few-shot segmentation. Typically,\na single prototype is obtained from the support feature by averaging the global\nobject information. However, using one prototype to represent all the\ninformation may lead to ambiguities. In this paper, we propose two novel\nmodules, named superpixel-guided clustering (SGC) and guided prototype\nallocation (GPA), for multiple prototype extraction and allocation.\nSpecifically, SGC is a parameter-free and training-free approach, which\nextracts more representative prototypes by aggregating similar feature vectors,\nwhile GPA is able to select matched prototypes to provide more accurate\nguidance. By integrating the SGC and GPA together, we propose the Adaptive\nSuperpixel-guided Network (ASGNet), which is a lightweight model and adapts to\nobject scale and shape variation. In addition, our network can easily\ngeneralize to k-shot segmentation with substantial improvement and no\nadditional computational cost. In particular, our evaluations on COCO\ndemonstrate that ASGNet surpasses the state-of-the-art method by 5% in 5-shot\nsegmentation.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 13:10:50 GMT"}, {"version": "v2", "created": "Sun, 16 May 2021 12:49:14 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Li", "Gen", ""], ["Jampani", "Varun", ""], ["Sevilla-Lara", "Laura", ""], ["Sun", "Deqing", ""], ["Kim", "Jonghyun", ""], ["Kim", "Joongkyu", ""]]}, {"id": "2104.01894", "submitter": "Ramon Sanabria", "authors": "Ramon Sanabria, Austin Waters, Jason Baldridge", "title": "Talk, Don't Write: A Study of Direct Speech-Based Image Retrieval", "comments": "Accepted to INTERSPEECH 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Speech-based image retrieval has been studied as a proxy for joint\nrepresentation learning, usually without emphasis on retrieval itself. As such,\nit is unclear how well speech-based retrieval can work in practice -- both in\nan absolute sense and versus alternative strategies that combine automatic\nspeech recognition (ASR) with strong text encoders. In this work, we\nextensively study and expand choices of encoder architectures, training\nmethodology (including unimodal and multimodal pretraining), and other factors.\nOur experiments cover different types of speech in three datasets: Flickr\nAudio, Places Audio, and Localized Narratives. Our best model configuration\nachieves large gains over state of the art, e.g., pushing recall-at-one from\n21.8% to 33.2% for Flickr Audio and 27.6% to 53.4% for Places Audio. We also\nshow our best speech-based models can match or exceed cascaded ASR-to-text\nencoding when speech is spontaneous, accented, or otherwise hard to\nautomatically transcribe.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 13:11:40 GMT"}, {"version": "v2", "created": "Thu, 8 Apr 2021 10:16:17 GMT"}, {"version": "v3", "created": "Tue, 15 Jun 2021 17:03:38 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Sanabria", "Ramon", ""], ["Waters", "Austin", ""], ["Baldridge", "Jason", ""]]}, {"id": "2104.01896", "submitter": "Cheng Xue", "authors": "Cheng Xue, Lei Zhu, Huazhu Fu, Xiaowei Hu, Xiaomeng Li, Hai Zhang,\n  Pheng Ann Heng", "title": "Global Guidance Network for Breast Lesion Segmentation in Ultrasound\n  Images", "comments": "16page,10 figures. Accepted by medical image analysis", "journal-ref": null, "doi": "10.1016/j.media.2021.101989", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Automatic breast lesion segmentation in ultrasound helps to diagnose breast\ncancer, which is one of the dreadful diseases that affect women globally.\nSegmenting breast regions accurately from ultrasound image is a challenging\ntask due to the inherent speckle artifacts, blurry breast lesion boundaries,\nand inhomogeneous intensity distributions inside the breast lesion regions.\nRecently, convolutional neural networks (CNNs) have demonstrated remarkable\nresults in medical image segmentation tasks. However, the convolutional\noperations in a CNN often focus on local regions, which suffer from limited\ncapabilities in capturing long-range dependencies of the input ultrasound\nimage, resulting in degraded breast lesion segmentation accuracy. In this\npaper, we develop a deep convolutional neural network equipped with a global\nguidance block (GGB) and breast lesion boundary detection (BD) modules for\nboosting the breast ultrasound lesion segmentation. The GGB utilizes the\nmulti-layer integrated feature map as a guidance information to learn the\nlong-range non-local dependencies from both spatial and channel domains. The BD\nmodules learn additional breast lesion boundary map to enhance the boundary\nquality of a segmentation result refinement. Experimental results on a public\ndataset and a collected dataset show that our network outperforms other medical\nimage segmentation methods and the recent semantic segmentation methods on\nbreast ultrasound lesion segmentation. Moreover, we also show the application\nof our network on the ultrasound prostate segmentation, in which our method\nbetter identifies prostate regions than state-of-the-art networks.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 13:15:22 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Xue", "Cheng", ""], ["Zhu", "Lei", ""], ["Fu", "Huazhu", ""], ["Hu", "Xiaowei", ""], ["Li", "Xiaomeng", ""], ["Zhang", "Hai", ""], ["Heng", "Pheng Ann", ""]]}, {"id": "2104.01928", "submitter": "Dingwen Zhang", "authors": "Dingwen Zhang, Haibin Tian, and Jungong Han", "title": "Few-Cost Salient Object Detection with Adversarial-Paced Learning", "comments": null, "journal-ref": "34th Conference on Neural Information Processing Systems (NeurIPS\n  2020)", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Detecting and segmenting salient objects from given image scenes has received\ngreat attention in recent years. A fundamental challenge in training the\nexisting deep saliency detection models is the requirement of large amounts of\nannotated data. While gathering large quantities of training data becomes cheap\nand easy, annotating the data is an expensive process in terms of time, labor\nand human expertise. To address this problem, this paper proposes to learn the\neffective salient object detection model based on the manual annotation on a\nfew training images only, thus dramatically alleviating human labor in training\nmodels. To this end, we name this task as the few-cost salient object detection\nand propose an adversarial-paced learning (APL)-based framework to facilitate\nthe few-cost learning scenario. Essentially, APL is derived from the self-paced\nlearning (SPL) regime but it infers the robust learning pace through the\ndata-driven adversarial learning mechanism rather than the heuristic design of\nthe learning regularizer. Comprehensive experiments on four widely-used\nbenchmark datasets demonstrate that the proposed method can effectively\napproach to the existing supervised deep salient object detection models with\nonly 1k human-annotated training images. The project page is available at\nhttps://github.com/hb-stone/FC-SOD.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 14:15:49 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Zhang", "Dingwen", ""], ["Tian", "Haibin", ""], ["Han", "Jungong", ""]]}, {"id": "2104.01948", "submitter": "Dmitrii Marin", "authors": "Dmitrii Marin and Yuri Boykov", "title": "Robust Trust Region for Weakly Supervised Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Acquisition of training data for the standard semantic segmentation is\nexpensive if requiring that each pixel is labeled. Yet, current methods\nsignificantly deteriorate in weakly supervised settings, e.g. where a fraction\nof pixels is labeled or when only image-level tags are available. It has been\nshown that regularized losses - originally developed for unsupervised low-level\nsegmentation and representing geometric priors on pixel labels - can\nconsiderably improve the quality of weakly supervised training. However, many\ncommon priors require optimization stronger than gradient descent. Thus, such\nregularizers have limited applicability in deep learning. We propose a new\nrobust trust region approach for regularized losses improving the\nstate-of-the-art results. Our approach can be seen as a higher-order\ngeneralization of the classic chain rule. It allows neural network optimization\nto use strong low-level solvers for the corresponding regularizers, including\ndiscrete ones.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 15:11:29 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Marin", "Dmitrii", ""], ["Boykov", "Yuri", ""]]}, {"id": "2104.01960", "submitter": "Maria A. Zuluaga", "authors": "Maria A. Zuluaga and Alex F. Mendelson and M. Jorge Cardoso and Andrew\n  M. Taylor and S\\'ebastien Ourselin", "title": "Multi-Atlas Based Pathological Stratification of d-TGA Congenital Heart\n  Disease", "comments": "In: IEEE International Symposium on Biomedical Imaging 2014", "journal-ref": null, "doi": "10.1109/ISBI.2014.6867821", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the main sources of error in multi-atlas segmentation propagation\napproaches comes from the use of atlas databases that are morphologically\ndissimilar to the target image. In this work, we exploit the segmentation\nerrors associated with poor atlas selection to build a computer aided diagnosis\n(CAD) system for pathological classification in post-operative\ndextro-transposition of the great arteries (d-TGA). The proposed approach\nextracts a set of features, which describe the quality of a segmentation, and\nintroduces them into a logical decision tree that provides the final diagnosis.\nWe have validated our method on a set of 60 whole heart MR images containing\nhealthy cases and two different forms of post-operative d-TGA. The reported\noverall CAD system accuracy was of 93.33%.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 15:28:39 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Zuluaga", "Maria A.", ""], ["Mendelson", "Alex F.", ""], ["Cardoso", "M. Jorge", ""], ["Taylor", "Andrew M.", ""], ["Ourselin", "S\u00e9bastien", ""]]}, {"id": "2104.01975", "submitter": "Cheng Xue", "authors": "Cheng Xue, Qiao Deng, Xiaomeng Li, Qi Dou, Pheng Ann Heng", "title": "Cascaded Robust Learning at Imperfect Labels for Chest X-ray\n  Segmentation", "comments": "9pages, 4 figures. MICCAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The superior performance of CNN on medical image analysis heavily depends on\nthe annotation quality, such as the number of labeled image, the source of\nimage, and the expert experience. The annotation requires great expertise and\nlabour. To deal with the high inter-rater variability, the study of imperfect\nlabel has great significance in medical image segmentation tasks. In this\npaper, we present a novel cascaded robust learning framework for chest X-ray\nsegmentation with imperfect annotation. Our model consists of three independent\nnetwork, which can effectively learn useful information from the peer networks.\nThe framework includes two stages. In the first stage, we select the clean\nannotated samples via a model committee setting, the networks are trained by\nminimizing a segmentation loss using the selected clean samples. In the second\nstage, we design a joint optimization framework with label correction to\ngradually correct the wrong annotation and improve the network performance. We\nconduct experiments on the public chest X-ray image datasets collected by\nShenzhen Hospital. The results show that our methods could achieve a\nsignificant improvement on the accuracy in segmentation tasks compared to the\nprevious methods.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 15:50:16 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Xue", "Cheng", ""], ["Deng", "Qiao", ""], ["Li", "Xiaomeng", ""], ["Dou", "Qi", ""], ["Heng", "Pheng Ann", ""]]}, {"id": "2104.01984", "submitter": "Wenjing Wang", "authors": "Wenjing Wang, Wenhan Yang, Jiaying Liu", "title": "HLA-Face: Joint High-Low Adaptation for Low Light Face Detection", "comments": "Accepted to CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face detection in low light scenarios is challenging but vital to many\npractical applications, e.g., surveillance video, autonomous driving at night.\nMost existing face detectors heavily rely on extensive annotations, while\ncollecting data is time-consuming and laborious. To reduce the burden of\nbuilding new datasets for low light conditions, we make full use of existing\nnormal light data and explore how to adapt face detectors from normal light to\nlow light. The challenge of this task is that the gap between normal and low\nlight is too huge and complex for both pixel-level and object-level. Therefore,\nmost existing low-light enhancement and adaptation methods do not achieve\ndesirable performance. To address the issue, we propose a joint High-Low\nAdaptation (HLA) framework. Through a bidirectional low-level adaptation and\nmulti-task high-level adaptation scheme, our HLA-Face outperforms\nstate-of-the-art methods even without using dark face labels for training. Our\nproject is publicly available at https://daooshee.github.io/HLA-Face-Website/\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 16:20:57 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Wang", "Wenjing", ""], ["Yang", "Wenhan", ""], ["Liu", "Jiaying", ""]]}, {"id": "2104.01985", "submitter": "Jorge F. Lazo", "authors": "Jorge F. Lazo, Aldo Marzullo, Sara Moccia, Michele Catellani, Benoit\n  Rosa, Michel de Mathelin, Elena De Momi", "title": "Using spatial-temporal ensembles of convolutional neural networks for\n  lumen segmentation in ureteroscopy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Purpose: Ureteroscopy is an efficient endoscopic minimally invasive technique\nfor the diagnosis and treatment of upper tract urothelial carcinoma (UTUC).\nDuring ureteroscopy, the automatic segmentation of the hollow lumen is of\nprimary importance, since it indicates the path that the endoscope should\nfollow. In order to obtain an accurate segmentation of the hollow lumen, this\npaper presents an automatic method based on Convolutional Neural Networks\n(CNNs).\n  Methods: The proposed method is based on an ensemble of 4 parallel CNNs to\nsimultaneously process single and multi-frame information. Of these, two\narchitectures are taken as core-models, namely U-Net based in residual\nblocks($m_1$) and Mask-RCNN($m_2$), which are fed with single still-frames\n$I(t)$. The other two models ($M_1$, $M_2$) are modifications of the former\nones consisting on the addition of a stage which makes use of 3D Convolutions\nto process temporal information. $M_1$, $M_2$ are fed with triplets of frames\n($I(t-1)$, $I(t)$, $I(t+1)$) to produce the segmentation for $I(t)$.\n  Results: The proposed method was evaluated using a custom dataset of 11\nvideos (2,673 frames) which were collected and manually annotated from 6\npatients. We obtain a Dice similarity coefficient of 0.80, outperforming\nprevious state-of-the-art methods.\n  Conclusion: The obtained results show that spatial-temporal information can\nbe effectively exploited by the ensemble model to improve hollow lumen\nsegmentation in ureteroscopic images. The method is effective also in presence\nof poor visibility, occasional bleeding, or specular reflections.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 16:24:32 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Lazo", "Jorge F.", ""], ["Marzullo", "Aldo", ""], ["Moccia", "Sara", ""], ["Catellani", "Michele", ""], ["Rosa", "Benoit", ""], ["de Mathelin", "Michel", ""], ["De Momi", "Elena", ""]]}, {"id": "2104.02000", "submitter": "Yapeng Tian", "authors": "Yapeng Tian and Chenliang Xu", "title": "Can audio-visual integration strengthen robustness under multimodal\n  attacks?", "comments": "CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we propose to make a systematic study on machines multisensory\nperception under attacks. We use the audio-visual event recognition task\nagainst multimodal adversarial attacks as a proxy to investigate the robustness\nof audio-visual learning. We attack audio, visual, and both modalities to\nexplore whether audio-visual integration still strengthens perception and how\ndifferent fusion mechanisms affect the robustness of audio-visual models. For\ninterpreting the multimodal interactions under attacks, we learn a\nweakly-supervised sound source visual localization model to localize sounding\nregions in videos. To mitigate multimodal attacks, we propose an audio-visual\ndefense approach based on an audio-visual dissimilarity constraint and external\nfeature memory banks. Extensive experiments demonstrate that audio-visual\nmodels are susceptible to multimodal adversarial attacks; audio-visual\nintegration could decrease the model robustness rather than strengthen under\nmultimodal attacks; even a weakly-supervised sound source visual localization\nmodel can be successfully fooled; our defense method can improve the\ninvulnerability of audio-visual networks without significantly sacrificing\nclean model performance.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 16:46:45 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Tian", "Yapeng", ""], ["Xu", "Chenliang", ""]]}, {"id": "2104.02008", "submitter": "Kaiyang Zhou", "authors": "Kaiyang Zhou and Yongxin Yang and Yu Qiao and Tao Xiang", "title": "Domain Generalization with MixStyle", "comments": "ICLR 2021; Code is available at\n  https://github.com/KaiyangZhou/mixstyle-release", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Though convolutional neural networks (CNNs) have demonstrated remarkable\nability in learning discriminative features, they often generalize poorly to\nunseen domains. Domain generalization aims to address this problem by learning\nfrom a set of source domains a model that is generalizable to any unseen\ndomain. In this paper, a novel approach is proposed based on probabilistically\nmixing instance-level feature statistics of training samples across source\ndomains. Our method, termed MixStyle, is motivated by the observation that\nvisual domain is closely related to image style (e.g., photo vs.~sketch\nimages). Such style information is captured by the bottom layers of a CNN where\nour proposed style-mixing takes place. Mixing styles of training instances\nresults in novel domains being synthesized implicitly, which increase the\ndomain diversity of the source domains, and hence the generalizability of the\ntrained model. MixStyle fits into mini-batch training perfectly and is\nextremely easy to implement. The effectiveness of MixStyle is demonstrated on a\nwide range of tasks including category classification, instance retrieval and\nreinforcement learning.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 16:58:09 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Zhou", "Kaiyang", ""], ["Yang", "Yongxin", ""], ["Qiao", "Yu", ""], ["Xiang", "Tao", ""]]}, {"id": "2104.02026", "submitter": "Yapeng Tian", "authors": "Yapeng Tian, Di Hu, Chenliang Xu", "title": "Cyclic Co-Learning of Sounding Object Visual Grounding and Sound\n  Separation", "comments": "CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  There are rich synchronized audio and visual events in our daily life. Inside\nthe events, audio scenes are associated with the corresponding visual objects;\nmeanwhile, sounding objects can indicate and help to separate their individual\nsounds in the audio track. Based on this observation, in this paper, we propose\na cyclic co-learning (CCoL) paradigm that can jointly learn sounding object\nvisual grounding and audio-visual sound separation in a unified framework.\nConcretely, we can leverage grounded object-sound relations to improve the\nresults of sound separation. Meanwhile, benefiting from discriminative\ninformation from separated sounds, we improve training example sampling for\nsounding object grounding, which builds a co-learning cycle for the two tasks\nand makes them mutually beneficial. Extensive experiments show that the\nproposed framework outperforms the compared recent approaches on both tasks,\nand they can benefit from each other with our cyclic co-learning.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 17:30:41 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Tian", "Yapeng", ""], ["Hu", "Di", ""], ["Xu", "Chenliang", ""]]}, {"id": "2104.02042", "submitter": "Hossein Arabi", "authors": "Faeze Gholamiankhah, Samaneh Mostafapour, Nouraddin Abdi Goushbolagh,\n  Seyedjafar Shojaerazavi, Parvaneh Layegh, Seyyed Mohammad Tabatabaei, Hossein\n  Arabi", "title": "Automated lung segmentation from CT images of normal and COVID-19\n  pneumonia patients", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated semantic image segmentation is an essential step in quantitative\nimage analysis and disease diagnosis. This study investigates the performance\nof a deep learning-based model for lung segmentation from CT images for normal\nand COVID-19 patients. Chest CT images and corresponding lung masks of 1200\nconfirmed COVID-19 cases were used for training a residual neural network. The\nreference lung masks were generated through semi-automated/manual segmentation\nof the CT images. The performance of the model was evaluated on two distinct\nexternal test datasets including 120 normal and COVID-19 subjects, and the\nresults of these groups were compared to each other. Different evaluation\nmetrics such as dice coefficient (DSC), mean absolute error (MAE), relative\nmean HU difference, and relative volume difference were calculated to assess\nthe accuracy of the predicted lung masks. The proposed deep learning method\nachieved DSC of 0.980 and 0.971 for normal and COVID-19 subjects, respectively,\ndemonstrating significant overlap between predicted and reference lung masks.\nMoreover, MAEs of 0.037 HU and 0.061 HU, relative mean HU difference of -2.679%\nand -4.403%, and relative volume difference of 2.405% and 5.928% were obtained\nfor normal and COVID-19 subjects, respectively. The comparable performance in\nlung segmentation of the normal and COVID-19 patients indicates the accuracy of\nthe model for the identification of the lung tissue in the presence of the\nCOVID-19 induced infections (though slightly better performance was observed\nfor normal patients). The promising results achieved by the proposed deep\nlearning-based model demonstrated its reliability in COVID-19 lung\nsegmentation. This prerequisite step would lead to a more efficient and robust\npneumonia lesion analysis.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 17:46:12 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Gholamiankhah", "Faeze", ""], ["Mostafapour", "Samaneh", ""], ["Goushbolagh", "Nouraddin Abdi", ""], ["Shojaerazavi", "Seyedjafar", ""], ["Layegh", "Parvaneh", ""], ["Tabatabaei", "Seyyed Mohammad", ""], ["Arabi", "Hossein", ""]]}, {"id": "2104.02052", "submitter": "Utkarsh Ojha", "authors": "Utkarsh Ojha, Krishna Kumar Singh, Yong Jae Lee", "title": "Generating Furry Cars: Disentangling Object Shape & Appearance across\n  Multiple Domains", "comments": "Camera ready version for ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  We consider the novel task of learning disentangled representations of object\nshape and appearance across multiple domains (e.g., dogs and cars). The goal is\nto learn a generative model that learns an intermediate distribution, which\nborrows a subset of properties from each domain, enabling the generation of\nimages that did not exist in any domain exclusively. This challenging problem\nrequires an accurate disentanglement of object shape, appearance, and\nbackground from each domain, so that the appearance and shape factors from the\ntwo domains can be interchanged. We augment an existing approach that can\ndisentangle factors within a single domain but struggles to do so across\ndomains. Our key technical contribution is to represent object appearance with\na differentiable histogram of visual features, and to optimize the generator so\nthat two images with the same latent appearance factor but different latent\nshape factors produce similar histograms. On multiple multi-domain datasets, we\ndemonstrate our method leads to accurate and consistent appearance and shape\ntransfer across domains.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 17:59:15 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Ojha", "Utkarsh", ""], ["Singh", "Krishna Kumar", ""], ["Lee", "Yong Jae", ""]]}, {"id": "2104.02057", "submitter": "Xinlei Chen", "authors": "Xinlei Chen and Saining Xie and Kaiming He", "title": "An Empirical Study of Training Self-Supervised Vision Transformers", "comments": "Technical report, 10 pages. v2: corrected \"Visual Transformer\" to\n  \"Vision Transformer\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper does not describe a novel method. Instead, it studies a\nstraightforward, incremental, yet must-know baseline given the recent progress\nin computer vision: self-supervised learning for Vision Transformers (ViT).\nWhile the training recipes for standard convolutional networks have been highly\nmature and robust, the recipes for ViT are yet to be built, especially in the\nself-supervised scenarios where training becomes more challenging. In this\nwork, we go back to basics and investigate the effects of several fundamental\ncomponents for training self-supervised ViT. We observe that instability is a\nmajor issue that degrades accuracy, and it can be hidden by apparently good\nresults. We reveal that these results are indeed partial failure, and they can\nbe improved when training is made more stable. We benchmark ViT results in MoCo\nv3 and several other self-supervised frameworks, with ablations in various\naspects. We discuss the currently positive evidence as well as challenges and\nopen questions. We hope that this work will provide useful data points and\nexperience for future research.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 17:59:40 GMT"}, {"version": "v2", "created": "Thu, 8 Apr 2021 20:16:36 GMT"}, {"version": "v3", "created": "Wed, 5 May 2021 06:35:38 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Chen", "Xinlei", ""], ["Xie", "Saining", ""], ["He", "Kaiming", ""]]}, {"id": "2104.02060", "submitter": "Jayalakshmi Mangalagiri", "authors": "Jayalakshmi Mangalagiri, David Chapman, Aryya Gangopadhyay, Yaacov\n  Yesha, Joshua Galita, Sumeet Menon, Yelena Yesha, Babak Saboury, Michael\n  Morris, Phuong Nguyen", "title": "Toward Generating Synthetic CT Volumes using a 3D-Conditional Generative\n  Adversarial Network", "comments": "It is a short paper accepted in CSCI 2020 conference and is accepted\n  to publication in the IEEE CPS proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel conditional Generative Adversarial Network (cGAN)\narchitecture that is capable of generating 3D Computed Tomography scans in\nvoxels from noisy and/or pixelated approximations and with the potential to\ngenerate full synthetic 3D scan volumes. We believe conditional cGAN to be a\ntractable approach to generate 3D CT volumes, even though the problem of\ngenerating full resolution deep fakes is presently impractical due to GPU\nmemory limitations. We present results for autoencoder, denoising, and\ndepixelating tasks which are trained and tested on two novel COVID19 CT\ndatasets. Our evaluation metrics, Peak Signal to Noise ratio (PSNR) range from\n12.53 - 46.46 dB, and the Structural Similarity index ( SSIM) range from 0.89\nto 1.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 12:25:37 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Mangalagiri", "Jayalakshmi", ""], ["Chapman", "David", ""], ["Gangopadhyay", "Aryya", ""], ["Yesha", "Yaacov", ""], ["Galita", "Joshua", ""], ["Menon", "Sumeet", ""], ["Yesha", "Yelena", ""], ["Saboury", "Babak", ""], ["Morris", "Michael", ""], ["Nguyen", "Phuong", ""]]}, {"id": "2104.02066", "submitter": "Jun-En Ding", "authors": "Jun-En Ding, Chi-Hsiang Chu, Mong-Na Lo Huang, Chien-Ching Hsu", "title": "Dopamine Transporter SPECT Image Classification for Neurodegenerative\n  Parkinsonism via Diffusion Maps and Machine Learning Classifiers", "comments": null, "journal-ref": "24th Annual Conference, MIUA 2021, Oxford, UK, July 12-14, 2021,\n  Proceedings", "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Neurodegenerative parkinsonism can be assessed by dopamine transporter single\nphoton emission computed tomography (DaT-SPECT). Although generating images is\ntime consuming, these images can show interobserver variability and they have\nbeen visually interpreted by nuclear medicine physicians to date. Accordingly,\nthis study aims to provide an automatic and robust method based on Diffusion\nMaps and machine learning classifiers to classify the SPECT images into two\ntypes, namely Normal and Abnormal DaT-SPECT image groups. In the proposed\nmethod, the 3D images of N patients are mapped to an N by N pairwise distance\nmatrix and are visualized in Diffusion Maps coordinates. The images of the\ntraining set are embedded into a low-dimensional space by using diffusion maps.\nMoreover, we use Nystr\\\"om's out-of-sample extension, which embeds new sample\npoints as the testing set in the reduced space. Testing samples in the embedded\nspace are then classified into two types through the ensemble classifier with\nLinear Discriminant Analysis (LDA) and voting procedure through\ntwenty-five-fold cross-validation results. The feasibility of the method is\ndemonstrated via Parkinsonism Progression Markers Initiative (PPMI) dataset of\n1097 subjects and a clinical cohort from Kaohsiung Chang Gung Memorial Hospital\n(KCGMH-TW) of 630 patients. We compare performances using Diffusion Maps with\nthose of three alternative manifold methods for dimension reduction, namely\nLocally Linear Embedding (LLE), Isomorphic Mapping Algorithm (Isomap), and\nKernel Principal Component Analysis (Kernel PCA). We also compare results using\n2D and 3D CNN methods. The diffusion maps method has an average accuracy of 98%\nfor the PPMI and 90% for the KCGMH-TW dataset with twenty-five fold\ncross-validation results. It outperforms the other three methods concerning the\noverall accuracy and the robustness in the training and testing samples.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 06:30:15 GMT"}, {"version": "v2", "created": "Fri, 7 May 2021 15:47:56 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Ding", "Jun-En", ""], ["Chu", "Chi-Hsiang", ""], ["Huang", "Mong-Na Lo", ""], ["Hsu", "Chien-Ching", ""]]}, {"id": "2104.02096", "submitter": "Zhiyuan Fang", "authors": "Zhiyuan Fang, Jianfeng Wang, Xiaowei Hu, Lijuan Wang, Yezhou Yang,\n  Zicheng Liu", "title": "Compressing Visual-linguistic Model via Knowledge Distillation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Despite exciting progress in pre-training for visual-linguistic (VL)\nrepresentations, very few aspire to a small VL model. In this paper, we study\nknowledge distillation (KD) to effectively compress a transformer-based large\nVL model into a small VL model. The major challenge arises from the\ninconsistent regional visual tokens extracted from different detectors of\nTeacher and Student, resulting in the misalignment of hidden representations\nand attention distributions. To address the problem, we retrain and adapt the\nTeacher by using the same region proposals from Student's detector while the\nfeatures are from Teacher's own object detector. With aligned network inputs,\nthe adapted Teacher is capable of transferring the knowledge through the\nintermediate representations. Specifically, we use the mean square error loss\nto mimic the attention distribution inside the transformer block and present a\ntoken-wise noise contrastive loss to align the hidden state by contrasting with\nnegative representations stored in a sample queue. To this end, we show that\nour proposed distillation significantly improves the performance of small VL\nmodels on image captioning and visual question answering tasks. It reaches\n120.8 in CIDEr score on COCO captioning, an improvement of 5.1 over its\nnon-distilled counterpart; and an accuracy of 69.8 on VQA 2.0, a 0.8 gain from\nthe baseline. Our extensive experiments and ablations confirm the effectiveness\nof VL distillation in both pre-training and fine-tuning stages.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 18:02:17 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Fang", "Zhiyuan", ""], ["Wang", "Jianfeng", ""], ["Hu", "Xiaowei", ""], ["Wang", "Lijuan", ""], ["Yang", "Yezhou", ""], ["Liu", "Zicheng", ""]]}, {"id": "2104.02107", "submitter": "Neal Mangaokar", "authors": "Neal Mangaokar, Jiameng Pu, Parantapa Bhattacharya, Chandan K. Reddy,\n  Bimal Viswanath", "title": "Jekyll: Attacking Medical Image Diagnostics using Deep Generative Models", "comments": "Published in proceedings of the 5th European Symposium on Security\n  and Privacy (EuroS&P '20)", "journal-ref": null, "doi": "10.1109/EuroSP48549.2020.00017", "report-no": null, "categories": "cs.CR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advances in deep neural networks (DNNs) have shown tremendous promise in the\nmedical domain. However, the deep learning tools that are helping the domain,\ncan also be used against it. Given the prevalence of fraud in the healthcare\ndomain, it is important to consider the adversarial use of DNNs in manipulating\nsensitive data that is crucial to patient healthcare. In this work, we present\nthe design and implementation of a DNN-based image translation attack on\nbiomedical imagery. More specifically, we propose Jekyll, a neural style\ntransfer framework that takes as input a biomedical image of a patient and\ntranslates it to a new image that indicates an attacker-chosen disease\ncondition. The potential for fraudulent claims based on such generated 'fake'\nmedical images is significant, and we demonstrate successful attacks on both\nX-rays and retinal fundus image modalities. We show that these attacks manage\nto mislead both medical professionals and algorithmic detection schemes.\nLastly, we also investigate defensive measures based on machine learning to\ndetect images generated by Jekyll.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 18:23:36 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Mangaokar", "Neal", ""], ["Pu", "Jiameng", ""], ["Bhattacharya", "Parantapa", ""], ["Reddy", "Chandan K.", ""], ["Viswanath", "Bimal", ""]]}, {"id": "2104.02113", "submitter": "Jun Li", "authors": "Jun Li, Sinisa Todorovic", "title": "Anchor-Constrained Viterbi for Set-Supervised Action Segmentation", "comments": "CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is about action segmentation under weak supervision in training,\nwhere the ground truth provides only a set of actions present, but neither\ntheir temporal ordering nor when they occur in a training video. We use a\nHidden Markov Model (HMM) grounded on a multilayer perceptron (MLP) to label\nvideo frames, and thus generate a pseudo-ground truth for the subsequent\npseudo-supervised training. In testing, a Monte Carlo sampling of action sets\nseen in training is used to generate candidate temporal sequences of actions,\nand select the maximum posterior sequence. Our key contribution is a new\nanchor-constrained Viterbi algorithm (ACV) for generating the pseudo-ground\ntruth, where anchors are salient action parts estimated for each action from a\ngiven ground-truth set. Our evaluation on the tasks of action segmentation and\nalignment on the benchmark Breakfast, MPII Cooking2, Hollywood Extended\ndatasets demonstrates our superior performance relative to that of prior work.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 18:50:21 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Li", "Jun", ""], ["Todorovic", "Sinisa", ""]]}, {"id": "2104.02116", "submitter": "Jun Li", "authors": "Jun Li, Sinisa Todorovic", "title": "Action Shuffle Alternating Learning for Unsupervised Action Segmentation", "comments": "CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses unsupervised action segmentation. Prior work captures\nthe frame-level temporal structure of videos by a feature embedding that\nencodes time locations of frames in the video. We advance prior work with a new\nself-supervised learning (SSL) of a feature embedding that accounts for both\nframe- and action-level structure of videos. Our SSL trains an RNN to recognize\npositive and negative action sequences, and the RNN's hidden layer is taken as\nour new action-level feature embedding. The positive and negative sequences\nconsist of action segments sampled from videos, where in the former the sampled\naction segments respect their time ordering in the video, and in the latter\nthey are shuffled. As supervision of actions is not available and our SSL\nrequires access to action segments, we specify an HMM that explicitly models\naction lengths, and infer a MAP action segmentation with the Viterbi algorithm.\nThe resulting action segmentation is used as pseudo-ground truth for estimating\nour action-level feature embedding and updating the HMM. We alternate the above\nsteps within the Generalized EM framework, which ensures convergence. Our\nevaluation on the Breakfast, YouTube Instructions, and 50Salads datasets gives\nsuperior results to those of the state of the art.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 18:58:57 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Li", "Jun", ""], ["Todorovic", "Sinisa", ""]]}, {"id": "2104.02149", "submitter": "Mahmood Mohassel Feghhi", "authors": "Vida Esmaeili, Mahmood Mohassel Feghhi, Seyed Omid Shahdi", "title": "Automatic Micro-Expression Apex Frame Spotting using Local Binary\n  Pattern from Six Intersection Planes", "comments": "6 pages, 7 figures, Presented at the 11 Iranian and the first\n  International Conference on Machine Vision and Image Processing (MVIP), 19-20\n  February 2020, https://mvip2020.ut.ac.ir/paper?manu=39079", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Facial expressions are one of the most effective ways for non-verbal\ncommunications, which can be expressed as the Micro-Expression (ME) in the\nhigh-stake situations. The MEs are involuntary, rapid, and, subtle, and they\ncan reveal real human intentions. However, their feature extraction is very\nchallenging due to their low intensity and very short duration. Although Local\nBinary Pattern from Three Orthogonal Plane (LBP-TOP) feature extractor is\nuseful for the ME analysis, it does not consider essential information. To\naddress this problem, we propose a new feature extractor called Local Binary\nPattern from Six Intersection Planes (LBP-SIPl). This method extracts LBP code\non six intersection planes, and then it combines them. Results show that the\nproposed method has superior performance in apex frame spotting automatically\nin comparison with the relevant methods on the CASME database. Simulation\nresults show that, using the proposed method, the apex frame has been spotted\nin 43% of subjects in the CASME database, automatically. Also, the mean\nabsolute error of 1.76 is achieved, using our novel proposed method.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 20:42:57 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Esmaeili", "Vida", ""], ["Feghhi", "Mahmood Mohassel", ""], ["Shahdi", "Seyed Omid", ""]]}, {"id": "2104.02155", "submitter": "Samuel Henrique Silva", "authors": "Samuel Henrique Silva, Arun Das, Ian Scarff, Peyman Najafirad", "title": "Adaptive Clustering of Robust Semantic Representations for Adversarial\n  Image Purification", "comments": "11 pages, 5 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Deep Learning models are highly susceptible to adversarial manipulations that\ncan lead to catastrophic consequences. One of the most effective methods to\ndefend against such disturbances is adversarial training but at the cost of\ngeneralization of unseen attacks and transferability across models. In this\npaper, we propose a robust defense against adversarial attacks, which is model\nagnostic and generalizable to unseen adversaries. Initially, with a baseline\nmodel, we extract the latent representations for each class and adaptively\ncluster the latent representations that share a semantic similarity. We obtain\nthe distributions for the clustered latent representations and from their\noriginating images, we learn semantic reconstruction dictionaries (SRD). We\nadversarially train a new model constraining the latent space representation to\nminimize the distance between the adversarial latent representation and the\ntrue cluster distribution. To purify the image, we decompose the input into low\nand high-frequency components. The high-frequency component is reconstructed\nbased on the most adequate SRD from the clean dataset. In order to evaluate the\nmost adequate SRD, we rely on the distance between robust latent\nrepresentations and semantic cluster distributions. The output is a purified\nimage with no perturbation. Image purification on CIFAR-10 and ImageNet-10\nusing our proposed method improved the accuracy by more than 10% compared to\nstate-of-the-art results.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 21:07:04 GMT"}, {"version": "v2", "created": "Wed, 7 Apr 2021 15:22:42 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Silva", "Samuel Henrique", ""], ["Das", "Arun", ""], ["Scarff", "Ian", ""], ["Najafirad", "Peyman", ""]]}, {"id": "2104.02156", "submitter": "Debayan Deb", "authors": "Debayan Deb, Xiaoming Liu, Anil K. Jain", "title": "Unified Detection of Digital and Physical Face Attacks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art defense mechanisms against face attacks achieve near perfect\naccuracies within one of three attack categories, namely adversarial, digital\nmanipulation, or physical spoofs, however, they fail to generalize well when\ntested across all three categories. Poor generalization can be attributed to\nlearning incoherent attacks jointly. To overcome this shortcoming, we propose a\nunified attack detection framework, namely UniFAD, that can automatically\ncluster 25 coherent attack types belonging to the three categories. Using a\nmulti-task learning framework along with k-means clustering, UniFAD learns\njoint representations for coherent attacks, while uncorrelated attack types are\nlearned separately. Proposed UniFAD outperforms prevailing defense methods and\ntheir fusion with an overall TDR = 94.73% @ 0.2% FDR on a large fake face\ndataset consisting of 341K bona fide images and 448K attack images of 25 types\nacross all 3 categories. Proposed method can detect an attack within 3\nmilliseconds on a Nvidia 2080Ti. UniFAD can also identify the attack types and\ncategories with 75.81% and 97.37% accuracies, respectively.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 21:08:28 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Deb", "Debayan", ""], ["Liu", "Xiaoming", ""], ["Jain", "Anil K.", ""]]}, {"id": "2104.02166", "submitter": "Shihao Jiang", "authors": "Shihao Jiang, Yao Lu, Hongdong Li, Richard Hartley", "title": "Learning Optical Flow from a Few Matches", "comments": "Accepted to CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art neural network models for optical flow estimation require a\ndense correlation volume at high resolutions for representing per-pixel\ndisplacement. Although the dense correlation volume is informative for accurate\nestimation, its heavy computation and memory usage hinders the efficient\ntraining and deployment of the models. In this paper, we show that the dense\ncorrelation volume representation is redundant and accurate flow estimation can\nbe achieved with only a fraction of elements in it. Based on this observation,\nwe propose an alternative displacement representation, named Sparse Correlation\nVolume, which is constructed directly by computing the k closest matches in one\nfeature map for each feature vector in the other feature map and stored in a\nsparse data structure. Experiments show that our method can reduce\ncomputational cost and memory use significantly, while maintaining high\naccuracy compared to previous approaches with dense correlation volumes. Code\nis available at https://github.com/zacjiang/scv .\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 21:44:00 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Jiang", "Shihao", ""], ["Lu", "Yao", ""], ["Li", "Hongdong", ""], ["Hartley", "Richard", ""]]}, {"id": "2104.02173", "submitter": "AKM Bahalul Haque", "authors": "A K M Bahalul Haque, Tahmid Hasan Pranto, Abdulla All Noman and Atik\n  Mahmood", "title": "Insight about Detection, Prediction and Weather Impact of Coronavirus\n  (Covid-19) using Neural Network", "comments": "15 Pages, 13 Figures and 4 Tables", "journal-ref": "International Journal of Artificial Intelligence & Applications\n  11(4):67-81, July. 2020", "doi": "10.5121/ijaia.2020.11406", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The world is facing a tough situation due to the catastrophic pandemic caused\nby novel coronavirus (COVID-19). The number people affected by this virus are\nincreasing exponentially day by day and the number has already crossed 6.4\nmillion. As no vaccine has been discovered yet, the early detection of patients\nand isolation is the only and most effective way to reduce the spread of the\nvirus. Detecting infected persons from chest X-Ray by using Deep Neural\nNetworks, can be applied as a time and laborsaving solution. In this study, we\ntried to detect Covid-19 by classification of Covid-19, pneumonia and normal\nchest X-Rays. We used five different Convolutional Pre-Trained Neural Network\nmodels (VGG16, VGG19, Xception, InceptionV3 and Resnet50) and compared their\nperformance. VGG16 and VGG19 shows precise performance in classification. Both\nmodels can classify between three kinds of X-Rays with an accuracy over 92%.\nAnother part of our study was to find the impact of weather factors\n(temperature, humidity, sun hour and wind speed) on this pandemic using\nDecision Tree Regressor. We found that temperature, humidity and sun-hour\njointly hold 85.88% impact on escalation of Covid-19 and 91.89% impact on death\ndue to Covid-19 where humidity has 8.09% impact on death. We also tried to\npredict the death of an individual based on age, gender, country, and location\ndue to COVID-19 using the LogisticRegression, which can predict death of an\nindividual with a model accuracy of 94.40%.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 22:18:57 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Haque", "A K M Bahalul", ""], ["Pranto", "Tahmid Hasan", ""], ["Noman", "Abdulla All", ""], ["Mahmood", "Atik", ""]]}, {"id": "2104.02206", "submitter": "Mengmi Zhang", "authors": "Mengmi Zhang, Rohil Badkundri, Morgan B. Talbot, Gabriel Kreiman", "title": "Hypothesis-driven Stream Learning with Augmented Memory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stream learning refers to the ability to acquire and transfer knowledge\nacross a continuous stream of data without forgetting and without repeated\npasses over the data. A common way to avoid catastrophic forgetting is to\nintersperse new examples with replays of old examples stored as image pixels or\nreproduced by generative models. Here, we considered stream learning in image\nclassification tasks and proposed a novel hypotheses-driven Augmented Memory\nNetwork, which efficiently consolidates previous knowledge with a limited\nnumber of hypotheses in the augmented memory and replays relevant hypotheses to\navoid catastrophic forgetting. The advantages of hypothesis-driven replay over\nimage pixel replay and generative replay are two-fold. First, hypothesis-based\nknowledge consolidation avoids redundant information in the image pixel space\nand makes memory usage more efficient. Second, hypotheses in the augmented\nmemory can be re-used for learning new tasks, improving generalization and\ntransfer learning ability. We evaluated our method on three stream learning\nobject recognition datasets. Our method performs comparably well or better than\nSOTA methods, while offering more efficient memory usage. All source code and\ndata are publicly available https://github.com/kreimanlab/AugMem.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 00:53:01 GMT"}, {"version": "v2", "created": "Wed, 7 Apr 2021 01:25:33 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Zhang", "Mengmi", ""], ["Badkundri", "Rohil", ""], ["Talbot", "Morgan B.", ""], ["Kreiman", "Gabriel", ""]]}, {"id": "2104.02215", "submitter": "Mengmi Zhang", "authors": "Philipp Bomatter, Mengmi Zhang, Dimitar Karev, Spandan Madan, Claire\n  Tseng, Gabriel Kreiman", "title": "When Pigs Fly: Contextual Reasoning in Synthetic and Natural Scenes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Context is of fundamental importance to both human and machine vision -- an\nobject in the air is more likely to be an airplane, than a pig. The rich notion\nof context incorporates several aspects including physics rules, statistical\nco-occurrences, and relative object sizes, among others. While previous works\nhave crowd-sourced out-of-context photographs from the web to study scene\ncontext, controlling the nature and extent of contextual violations has been an\nextremely daunting task. Here we introduce a diverse, synthetic Out-of-Context\nDataset (OCD) with fine-grained control over scene context. By leveraging a 3D\nsimulation engine, we systematically control the gravity, object co-occurrences\nand relative sizes across 36 object categories in a virtual household\nenvironment. We then conduct a series of experiments to gain insights into the\nimpact of contextual cues on both human and machine vision using OCD. First, we\nconduct psycho-physics experiments to establish a human benchmark for\nout-of-context recognition, and then compare it with state-of-the-art computer\nvision models to quantify the gap between the two. Finally, we propose a\ncontext-aware recognition transformer model, fusing object and contextual\ninformation via multi-head attention. Our model captures useful information for\ncontextual reasoning, enabling human-level performance and significantly better\nrobustness in out-of-context conditions compared to baseline models across OCD\nand other existing out-of-context natural image datasets. All source code and\ndata are publicly available https://github.com/kreimanlab/WhenPigsFlyContext.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 01:05:34 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Bomatter", "Philipp", ""], ["Zhang", "Mengmi", ""], ["Karev", "Dimitar", ""], ["Madan", "Spandan", ""], ["Tseng", "Claire", ""], ["Kreiman", "Gabriel", ""]]}, {"id": "2104.02226", "submitter": "Boyuan Chen", "authors": "Boyuan Chen, Yu Li, Sunand Raghupathi, Hod Lipson", "title": "Beyond Categorical Label Representations for Image Classification", "comments": "International Conference on Learning Representations (ICLR 2021).\n  Project page is at\n  \\url{https://www.creativemachineslab.com/label-representation.html}", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We find that the way we choose to represent data labels can have a profound\neffect on the quality of trained models. For example, training an image\nclassifier to regress audio labels rather than traditional categorical\nprobabilities produces a more reliable classification. This result is\nsurprising, considering that audio labels are more complex than simpler\nnumerical probabilities or text. We hypothesize that high dimensional, high\nentropy label representations are generally more useful because they provide a\nstronger error signal. We support this hypothesis with evidence from various\nlabel representations including constant matrices, spectrograms, shuffled\nspectrograms, Gaussian mixtures, and uniform random matrices of various\ndimensionalities. Our experiments reveal that high dimensional, high entropy\nlabels achieve comparable accuracy to text (categorical) labels on the standard\nimage classification task, but features learned through our label\nrepresentations exhibit more robustness under various adversarial attacks and\nbetter effectiveness with a limited amount of training data. These results\nsuggest that label representation may play a more important role than\npreviously thought. The project website is at\n\\url{https://www.creativemachineslab.com/label-representation.html}.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 01:31:04 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Chen", "Boyuan", ""], ["Li", "Yu", ""], ["Raghupathi", "Sunand", ""], ["Lipson", "Hod", ""]]}, {"id": "2104.02230", "submitter": "Pinhao Song", "authors": "Pinhao Song, Linhui Dai, Peipei Yuan, Hong Liu and Runwei Ding", "title": "Achieving Domain Generalization in Underwater Object Detection by Image\n  Stylization and Domain Mixup", "comments": "some experimental results are wrong", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance of existing underwater object detection methods degrades\nseriously when facing domain shift problem caused by complicated underwater\nenvironments. Due to the limitation of the number of domains in the dataset,\ndeep detectors easily just memorize a few seen domain, which leads to low\ngeneralization ability. Ulteriorly, it can be inferred that the detector\ntrained on as many domains as possible is domain-invariant. Based on this\nviewpoint, we propose a domain generalization method from the aspect of data\naugmentation. First, the style transfer model transforms images from one source\ndomain to another, enriching the domain diversity of the training data. Second,\ninterpolating different domains on feature level, new domains can be sampled on\nthe domain manifold. With our method, detectors will be robust to domain shift.\nComprehensive experiments on S-UODAC2020 datasets demonstrate that the proposed\nmethod is able to learn domain-invariant representations, and outperforms other\ndomain generalization methods. The source code is available at\nhttps://github.com/mousecpn.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 01:45:07 GMT"}, {"version": "v2", "created": "Sun, 9 May 2021 13:20:51 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Song", "Pinhao", ""], ["Dai", "Linhui", ""], ["Yuan", "Peipei", ""], ["Liu", "Hong", ""], ["Ding", "Runwei", ""]]}, {"id": "2104.02236", "submitter": "Guanjie Huang", "authors": "Shaowei Wang, Guanjie Huang, Xiangyu Luo", "title": "Hippocampus-heuristic Character Recognition Network for Zero-shot\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recognition of Chinese characters has always been a challenging task due\nto their huge variety and complex structures. The latest research proves that\nsuch an enormous character set can be decomposed into a collection of about 500\nfundamental Chinese radicals, and based on which this problem can be solved\neffectively. While with the constant advent of novel Chinese characters, the\nnumber of basic radicals is also expanding. The current methods that entirely\nrely on existing radicals are not flexible for identifying these novel\ncharacters and fail to recognize these Chinese characters without learning all\nof their radicals in the training stage. To this end, this paper proposes a\nnovel Hippocampus-heuristic Character Recognition Network (HCRN), which\nreferences the way of hippocampus thinking, and can recognize unseen Chinese\ncharacters (namely zero-shot learning) only by training part of radicals. More\nspecifically, the network architecture of HCRN is a new pseudo-siamese network\ndesigned by us, which can learn features from pairs of input training character\nsamples and use them to predict unseen Chinese characters. The experimental\nresults show that HCRN is robust and effective. It can accurately predict about\n16,330 unseen testing Chinese characters relied on only 500 trained Chinese\ncharacters. The recognition accuracy of HCRN outperforms the state-of-the-art\nChinese radical recognition approach by 15% (from 85.1% to 99.9%) for\nrecognizing unseen Chinese characters.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 01:57:20 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Wang", "Shaowei", ""], ["Huang", "Guanjie", ""], ["Luo", "Xiangyu", ""]]}, {"id": "2104.02238", "submitter": "Alexandrea Ramnarine", "authors": "Alexandrea K. Ramnarine", "title": "In-Line Image Transformations for Imbalanced, Multiclass Computer Vision\n  Classification of Lung Chest X-Rays", "comments": "8 article pages, 4 article figures, 1 article table. 14 supplemental\n  pages with figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Artificial intelligence (AI) is disrupting the medical field as advances in\nmodern technology allow common household computers to learn anatomical and\npathological features that distinguish between healthy and disease with the\naccuracy of highly specialized, trained physicians. Computer vision AI\napplications use medical imaging, such as lung chest X-Rays (LCXRs), to\nfacilitate diagnoses by providing second-opinions in addition to a physician's\nor radiologist's interpretation. Considering the advent of the current\nCoronavirus disease (COVID-19) pandemic, LCXRs may provide rapid insights to\nindirectly aid in infection containment, however generating a reliably labeled\nimage dataset for a novel disease is not an easy feat, nor is it of highest\npriority when combating a global pandemic. Deep learning techniques such as\nconvolutional neural networks (CNNs) are able to select features that\ndistinguish between healthy and disease states for other lung pathologies; this\nstudy aims to leverage that body of literature in order to apply image\ntransformations that would serve to balance the lack of COVID-19 LCXR data.\nFurthermore, this study utilizes a simple CNN architecture for high-performance\nmulticlass LCXR classification at 94 percent accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 02:01:43 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Ramnarine", "Alexandrea K.", ""]]}, {"id": "2104.02239", "submitter": "Jinsu Kim", "authors": "Sunpill Kim, Yunseong Jeong, Jinsu Kim, Jungkon Kim, Hyung Tae Lee and\n  Jae Hong Seo", "title": "IronMask: Modular Architecture for Protecting Deep Face Template", "comments": "The submission is a 13 pages of paper which consists of 3 figures, 3\n  tables. It is the full version of CVPR '21 paper (The Conference on Computer\n  Vision and Patter Recognition)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Convolutional neural networks have made remarkable progress in the face\nrecognition field. The more the technology of face recognition advances, the\ngreater discriminative features into a face template. However, this increases\nthe threat to user privacy in case the template is exposed.\n  In this paper, we present a modular architecture for face template\nprotection, called IronMask, that can be combined with any face recognition\nsystem using angular distance metric. We circumvent the need for binarization,\nwhich is the main cause of performance degradation in most existing face\ntemplate protections, by proposing a new real-valued error-correcting-code that\nis compatible with real-valued templates and can therefore, minimize\nperformance degradation. We evaluate the efficacy of IronMask by extensive\nexperiments on two face recognitions, ArcFace and CosFace with three datasets,\nCMU-Multi-PIE, FEI, and Color-FERET. According to our experimental results,\nIronMask achieves a true accept rate (TAR) of 99.79% at a false accept rate\n(FAR) of 0.0005% when combined with ArcFace, and 95.78% TAR at 0% FAR with\nCosFace, while providing at least 115-bit security against known attacks.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 02:07:12 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Kim", "Sunpill", ""], ["Jeong", "Yunseong", ""], ["Kim", "Jinsu", ""], ["Kim", "Jungkon", ""], ["Lee", "Hyung Tae", ""], ["Seo", "Jae Hong", ""]]}, {"id": "2104.02243", "submitter": "Zhengzhe Liu", "authors": "Zhengzhe Liu, Xiaojuan Qi, Chi-Wing Fu", "title": "3D-to-2D Distillation for Indoor Scene Parsing", "comments": "Accepted by CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Indoor scene semantic parsing from RGB images is very challenging due to\nocclusions, object distortion, and viewpoint variations. Going beyond prior\nworks that leverage geometry information, typically paired depth maps, we\npresent a new approach, a 3D-to-2D distillation framework, that enables us to\nleverage 3D features extracted from large-scale 3D data repository (e.g.,\nScanNet-v2) to enhance 2D features extracted from RGB images. Our work has\nthree novel contributions. First, we distill 3D knowledge from a pretrained 3D\nnetwork to supervise a 2D network to learn simulated 3D features from 2D\nfeatures during the training, so the 2D network can infer without requiring 3D\ndata. Second, we design a two-stage dimension normalization scheme to calibrate\nthe 2D and 3D features for better integration. Third, we design a\nsemantic-aware adversarial training model to extend our framework for training\nwith unpaired 3D data. Extensive experiments on various datasets, ScanNet-V2,\nS3DIS, and NYU-v2, demonstrate the superiority of our approach. Also,\nexperimental results show that our 3D-to-2D distillation improves the model\ngeneralization.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 02:22:24 GMT"}, {"version": "v2", "created": "Wed, 7 Apr 2021 06:04:14 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Liu", "Zhengzhe", ""], ["Qi", "Xiaojuan", ""], ["Fu", "Chi-Wing", ""]]}, {"id": "2104.02244", "submitter": "Yuchen Liu", "authors": "Yuchen Liu, Zhixin Shu, Yijun Li, Zhe Lin, Federico Perazzi, S.Y. Kung", "title": "Content-Aware GAN Compression", "comments": "Published in CVPR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative adversarial networks (GANs), e.g., StyleGAN2, play a vital role in\nvarious image generation and synthesis tasks, yet their notoriously high\ncomputational cost hinders their efficient deployment on edge devices. Directly\napplying generic compression approaches yields poor results on GANs, which\nmotivates a number of recent GAN compression works. While prior works mainly\naccelerate conditional GANs, e.g., pix2pix and CycleGAN, compressing\nstate-of-the-art unconditional GANs has rarely been explored and is more\nchallenging. In this paper, we propose novel approaches for unconditional GAN\ncompression. We first introduce effective channel pruning and knowledge\ndistillation schemes specialized for unconditional GANs. We then propose a\nnovel content-aware method to guide the processes of both pruning and\ndistillation. With content-awareness, we can effectively prune channels that\nare unimportant to the contents of interest, e.g., human faces, and focus our\ndistillation on these regions, which significantly enhances the distillation\nquality. On StyleGAN2 and SN-GAN, we achieve a substantial improvement over the\nstate-of-the-art compression method. Notably, we reduce the FLOPs of StyleGAN2\nby 11x with visually negligible image quality loss compared to the full-size\nmodel. More interestingly, when applied to various image manipulation tasks,\nour compressed model forms a smoother and better disentangled latent manifold,\nmaking it more effective for image editing.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 02:23:56 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Liu", "Yuchen", ""], ["Shu", "Zhixin", ""], ["Li", "Yijun", ""], ["Lin", "Zhe", ""], ["Perazzi", "Federico", ""], ["Kung", "S. Y.", ""]]}, {"id": "2104.02245", "submitter": "Wang Xin", "authors": "Xin Wang, Yang Zhao, Tangwen Yang, Qiuqi Ruan", "title": "Multi-Scale Context Aggregation Network with Attention-Guided for Crowd\n  Counting", "comments": null, "journal-ref": "ICSP2020", "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Crowd counting aims to predict the number of people and generate the density\nmap in the image. There are many challenges, including varying head scales, the\ndiversity of crowd distribution across images and cluttered backgrounds. In\nthis paper, we propose a multi-scale context aggregation network (MSCANet)\nbased on single-column encoder-decoder architecture for crowd counting, which\nconsists of an encoder based on a dense context-aware module (DCAM) and a\nhierarchical attention-guided decoder. To handle the issue of scale variation,\nwe construct the DCAM to aggregate multi-scale contextual information by\ndensely connecting the dilated convolution with varying receptive fields. The\nproposed DCAM can capture rich contextual information of crowd areas due to its\nlong-range receptive fields and dense scale sampling. Moreover, to suppress the\nbackground noise and generate a high-quality density map, we adopt a\nhierarchical attention-guided mechanism in the decoder. This helps to integrate\nmore useful spatial information from shallow feature maps of the encoder by\nintroducing multiple supervision based on semantic attention module (SAM).\nExtensive experiments demonstrate that the proposed approach achieves better\nperformance than other similar state-of-the-art methods on three challenging\nbenchmark datasets for crowd counting. The code is available at\nhttps://github.com/KingMV/MSCANet\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 02:24:06 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Wang", "Xin", ""], ["Zhao", "Yang", ""], ["Yang", "Tangwen", ""], ["Ruan", "Qiuqi", ""]]}, {"id": "2104.02246", "submitter": "Zhengzhe Liu", "authors": "Zhengzhe Liu, Xiaojuan Qi, Chi-Wing Fu", "title": "One Thing One Click: A Self-Training Approach for Weakly Supervised 3D\n  Semantic Segmentation", "comments": "Accepted by CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point cloud semantic segmentation often requires largescale annotated\ntraining data, but clearly, point-wise labels are too tedious to prepare. While\nsome recent methods propose to train a 3D network with small percentages of\npoint labels, we take the approach to an extreme and propose \"One Thing One\nClick,\" meaning that the annotator only needs to label one point per object. To\nleverage these extremely sparse labels in network training, we design a novel\nself-training approach, in which we iteratively conduct the training and label\npropagation, facilitated by a graph propagation module. Also, we adopt a\nrelation network to generate per-category prototype and explicitly model the\nsimilarity among graph nodes to generate pseudo labels to guide the iterative\ntraining. Experimental results on both ScanNet-v2 and S3DIS show that our\nself-training approach, with extremely-sparse annotations, outperforms all\nexisting weakly supervised methods for 3D semantic segmentation by a large\nmargin, and our results are also comparable to those of the fully supervised\ncounterparts.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 02:27:25 GMT"}, {"version": "v2", "created": "Wed, 7 Apr 2021 05:58:22 GMT"}, {"version": "v3", "created": "Fri, 16 Apr 2021 13:29:56 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Liu", "Zhengzhe", ""], ["Qi", "Xiaojuan", ""], ["Fu", "Chi-Wing", ""]]}, {"id": "2104.02253", "submitter": "Saif Imran", "authors": "Saif Imran, Xiaoming Liu and Daniel Morris", "title": "Depth Completion with Twin Surface Extrapolation at Occlusion Boundaries", "comments": "Accepted in Intl. Conf. on Computer Vision and Pattern Recognition\n  (CVPR) 2021 (Supplementary Included)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Depth completion starts from a sparse set of known depth values and estimates\nthe unknown depths for the remaining image pixels. Most methods model this as\ndepth interpolation and erroneously interpolate depth pixels into the empty\nspace between spatially distinct objects, resulting in depth-smearing across\nocclusion boundaries. Here we propose a multi-hypothesis depth representation\nthat explicitly models both foreground and background depths in the difficult\nocclusion-boundary regions. Our method can be thought of as performing\ntwin-surface extrapolation, rather than interpolation, in these regions. Next\nour method fuses these extrapolated surfaces into a single depth image\nleveraging the image data. Key to our method is the use of an asymmetric loss\nfunction that operates on a novel twin-surface representation. This enables us\nto train a network to simultaneously do surface extrapolation and surface\nfusion. We characterize our loss function and compare with other common losses.\nFinally, we validate our method on three different datasets; KITTI, an outdoor\nreal-world dataset, NYU2, indoor real-world depth dataset and Virtual KITTI, a\nphoto-realistic synthetic dataset with dense groundtruth, and demonstrate\nimprovement over the state of the art.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 02:36:35 GMT"}, {"version": "v2", "created": "Wed, 7 Apr 2021 14:12:49 GMT"}, {"version": "v3", "created": "Sun, 25 Jul 2021 16:55:44 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Imran", "Saif", ""], ["Liu", "Xiaoming", ""], ["Morris", "Daniel", ""]]}, {"id": "2104.02256", "submitter": "Huy Hieu Pham", "authors": "Ngoc Huy Nguyen, Ha Quy Nguyen, Nghia Trung Nguyen, Thang Viet Nguyen,\n  Hieu Huy Pham, Tuan Ngoc-Minh Nguyen", "title": "A clinical validation of VinDr-CXR, an AI system for detecting abnormal\n  chest radiographs", "comments": "This is a preprint which has been submitted and under review by PLOS\n  One journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Computer-Aided Diagnosis (CAD) systems for chest radiographs using artificial\nintelligence (AI) have recently shown a great potential as a second opinion for\nradiologists. The performances of such systems, however, were mostly evaluated\non a fixed dataset in a retrospective manner and, thus, far from the real\nperformances in clinical practice. In this work, we demonstrate a mechanism for\nvalidating an AI-based system for detecting abnormalities on X-ray scans,\nVinDr-CXR, at the Phu Tho General Hospital - a provincial hospital in the North\nof Vietnam. The AI system was directly integrated into the Picture Archiving\nand Communication System (PACS) of the hospital after being trained on a fixed\nannotated dataset from other sources. The performance of the system was\nprospectively measured by matching and comparing the AI results with the\nradiology reports of 6,285 chest X-ray examinations extracted from the Hospital\nInformation System (HIS) over the last two months of 2020. The normal/abnormal\nstatus of a radiology report was determined by a set of rules and served as the\nground truth. Our system achieves an F1 score - the harmonic average of the\nrecall and the precision - of 0.653 (95% CI 0.635, 0.671) for detecting any\nabnormalities on chest X-rays. Despite a significant drop from the in-lab\nperformance, this result establishes a high level of confidence in applying\nsuch a system in real-life situations.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 02:53:35 GMT"}, {"version": "v2", "created": "Wed, 7 Apr 2021 02:22:01 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Nguyen", "Ngoc Huy", ""], ["Nguyen", "Ha Quy", ""], ["Nguyen", "Nghia Trung", ""], ["Nguyen", "Thang Viet", ""], ["Pham", "Hieu Huy", ""], ["Nguyen", "Tuan Ngoc-Minh", ""]]}, {"id": "2104.02260", "submitter": "Bin Li", "authors": "Panpan Zhang, Bin Li, Jinye Peng, Wei Jiang", "title": "Multi-hierarchical Convolutional Network for Efficient Remote\n  Photoplethysmograph Signal and Heart Rate Estimation from Face Video Clips", "comments": "33 pages,9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Heart beat rhythm and heart rate (HR) are important physiological parameters\nof the human body. This study presents an efficient multi-hierarchical\nspatio-temporal convolutional network that can quickly estimate remote\nphysiological (rPPG) signal and HR from face video clips. First, the facial\ncolor distribution characteristics are extracted using a low-level face feature\nGeneration (LFFG) module. Then, the three-dimensional (3D) spatio-temporal\nstack convolution module (STSC) and multi-hierarchical feature fusion module\n(MHFF) are used to strengthen the spatio-temporal correlation of multi-channel\nfeatures. In the MHFF, sparse optical flow is used to capture the tiny motion\ninformation of faces between frames and generate a self-adaptive region of\ninterest (ROI) skin mask. Finally, the signal prediction module (SP) is used to\nextract the estimated rPPG signal. The experimental results on the three\ndatasets show that the proposed network outperforms the state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 03:04:27 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Zhang", "Panpan", ""], ["Li", "Bin", ""], ["Peng", "Jinye", ""], ["Jiang", "Wei", ""]]}, {"id": "2104.02265", "submitter": "Suncheng Xiang", "authors": "Suncheng Xiang, Yuzhuo Fu, Mengyuan Guan, Ting Liu", "title": "Learning from Self-Discrepancy via Multiple Co-teaching for Cross-Domain\n  Person Re-Identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Employing clustering strategy to assign unlabeled target images with pseudo\nlabels has become a trend for person re-identification (re-ID) algorithms in\ndomain adaptation. A potential limitation of these clustering-based methods is\nthat they always tend to introduce noisy labels, which will undoubtedly hamper\nthe performance of our re-ID system. To handle this limitation, an intuitive\nsolution is to utilize collaborative training to purify the pseudo label\nquality. However, there exists a challenge that the complementarity of two\nnetworks, which inevitably share a high similarity, becomes weakened gradually\nas training process goes on; worse still, these approaches typically ignore to\nconsider the self-discrepancy of intra-class relations. To address this issue,\nin this paper, we propose a multiple co-teaching framework for domain adaptive\nperson re-ID, opening up a promising direction about self-discrepancy problem\nunder unsupervised condition. On top of that, a mean-teaching mechanism is\nleveraged to enlarge the difference and discover more complementary features.\nComprehensive experiments conducted on several large-scale datasets show that\nour method achieves competitive performance compared with the\nstate-of-the-arts.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 03:12:11 GMT"}, {"version": "v2", "created": "Sat, 10 Jul 2021 12:24:02 GMT"}, {"version": "v3", "created": "Tue, 13 Jul 2021 02:06:45 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Xiang", "Suncheng", ""], ["Fu", "Yuzhuo", ""], ["Guan", "Mengyuan", ""], ["Liu", "Ting", ""]]}, {"id": "2104.02273", "submitter": "Jiahao Lin", "authors": "Jiahao Lin, Gim Hee Lee", "title": "Multi-View Multi-Person 3D Pose Estimation with Plane Sweep Stereo", "comments": "10 pages, 5 figures. Accepted in CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing approaches for multi-view multi-person 3D pose estimation explicitly\nestablish cross-view correspondences to group 2D pose detections from multiple\ncamera views and solve for the 3D pose estimation for each person. Establishing\ncross-view correspondences is challenging in multi-person scenes, and incorrect\ncorrespondences will lead to sub-optimal performance for the multi-stage\npipeline. In this work, we present our multi-view 3D pose estimation approach\nbased on plane sweep stereo to jointly address the cross-view fusion and 3D\npose reconstruction in a single shot. Specifically, we propose to perform depth\nregression for each joint of each 2D pose in a target camera view. Cross-view\nconsistency constraints are implicitly enforced by multiple reference camera\nviews via the plane sweep algorithm to facilitate accurate depth regression. We\nadopt a coarse-to-fine scheme to first regress the person-level depth followed\nby a per-person joint-level relative depth estimation. 3D poses are obtained\nfrom a simple back-projection given the estimated depths. We evaluate our\napproach on benchmark datasets where it outperforms previous state-of-the-arts\nwhile being remarkably efficient. Our code is available at\nhttps://github.com/jiahaoLjh/PlaneSweepPose.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 03:49:35 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Lin", "Jiahao", ""], ["Lee", "Gim Hee", ""]]}, {"id": "2104.02281", "submitter": "Boyu Yang", "authors": "Boyu Yang, Mingbao Lin, Binghao Liu, Mengying Fu, Chang Liu, Rongrong\n  Ji and Qixiang Ye", "title": "Learnable Expansion-and-Compression Network for Few-shot\n  Class-Incremental Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Few-shot class-incremental learning (FSCIL), which targets at continuously\nexpanding model's representation capacity under few supervisions, is an\nimportant yet challenging problem. On the one hand, when fitting new tasks\n(novel classes), features trained on old tasks (old classes) could\nsignificantly drift, causing catastrophic forgetting. On the other hand,\ntraining the large amount of model parameters with few-shot novel-class\nexamples leads to model over-fitting. In this paper, we propose a learnable\nexpansion-and-compression network (LEC-Net), with the aim to simultaneously\nsolve catastrophic forgetting and model over-fitting problems in a unified\nframework. By tentatively expanding network nodes, LEC-Net enlarges the\nrepresentation capacity of features, alleviating feature drift of old network\nfrom the perspective of model regularization. By compressing the expanded\nnetwork nodes, LEC-Net purses minimal increase of model parameters, alleviating\nover-fitting of the expanded network from a perspective of compact\nrepresentation. Experiments on the CUB/CIFAR-100 datasets show that LEC-Net\nimproves the baseline by 5~7% while outperforms the state-of-the-art by 5~6%.\nLEC-Net also demonstrates the potential to be a general incremental learning\napproach with dynamic model expansion capability.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 04:34:21 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Yang", "Boyu", ""], ["Lin", "Mingbao", ""], ["Liu", "Binghao", ""], ["Fu", "Mengying", ""], ["Liu", "Chang", ""], ["Ji", "Rongrong", ""], ["Ye", "Qixiang", ""]]}, {"id": "2104.02290", "submitter": "Wuyang Chen", "authors": "Wuyang Chen, Zhiding Yu, Shalini De Mello, Sifei Liu, Jose M. Alvarez,\n  Zhangyang Wang, Anima Anandkumar", "title": "Contrastive Syn-to-Real Generalization", "comments": "Accepted in ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training on synthetic data can be beneficial for label or data-scarce\nscenarios. However, synthetically trained models often suffer from poor\ngeneralization in real domains due to domain gaps. In this work, we make a key\nobservation that the diversity of the learned feature embeddings plays an\nimportant role in the generalization performance. To this end, we propose\ncontrastive synthetic-to-real generalization (CSG), a novel framework that\nleverages the pre-trained ImageNet knowledge to prevent overfitting to the\nsynthetic domain, while promoting the diversity of feature embeddings as an\ninductive bias to improve generalization. In addition, we enhance the proposed\nCSG framework with attentional pooling (A-pool) to let the model focus on\nsemantically important regions and further improve its generalization. We\ndemonstrate the effectiveness of CSG on various synthetic training tasks,\nexhibiting state-of-the-art performance on zero-shot domain generalization.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 05:10:29 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Chen", "Wuyang", ""], ["Yu", "Zhiding", ""], ["De Mello", "Shalini", ""], ["Liu", "Sifei", ""], ["Alvarez", "Jose M.", ""], ["Wang", "Zhangyang", ""], ["Anandkumar", "Anima", ""]]}, {"id": "2104.02299", "submitter": "Feng Gao", "authors": "Junjie Wang, Feng Gao, Junyu Dong", "title": "Change Detection from SAR Images Based on Deformable Residual\n  Convolutional Neural Networks", "comments": "Accepted by ACM Multimedia Asia 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Convolutional neural networks (CNN) have made great progress for synthetic\naperture radar (SAR) images change detection. However, sampling locations of\ntraditional convolutional kernels are fixed and cannot be changed according to\nthe actual structure of the SAR images. Besides, objects may appear with\ndifferent sizes in natural scenes, which requires the network to have stronger\nmulti-scale representation ability. In this paper, a novel\n\\underline{D}eformable \\underline{R}esidual Convolutional Neural\n\\underline{N}etwork (DRNet) is designed for SAR images change detection. First,\nthe proposed DRNet introduces the deformable convolutional sampling locations,\nand the shape of convolutional kernel can be adaptively adjusted according to\nthe actual structure of ground objects. To create the deformable sampling\nlocations, 2-D offsets are calculated for each pixel according to the spatial\ninformation of the input images. Then the sampling location of pixels can\nadaptively reflect the spatial structure of the input images. Moreover, we\nproposed a novel pooling module replacing the vanilla pooling to utilize\nmulti-scale information effectively, by constructing hierarchical residual-like\nconnections within one pooling layer, which improve the multi-scale\nrepresentation ability at a granular level. Experimental results on three real\nSAR datasets demonstrate the effectiveness of the proposed DRNet.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 05:52:25 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Wang", "Junjie", ""], ["Gao", "Feng", ""], ["Dong", "Junyu", ""]]}, {"id": "2104.02300", "submitter": "Zigang Geng", "authors": "Zigang Geng, Ke Sun, Bin Xiao, Zhaoxiang Zhang, Jingdong Wang", "title": "Bottom-Up Human Pose Estimation Via Disentangled Keypoint Regression", "comments": "Accepted by CVPR2021. arXiv admin note: text overlap with\n  arXiv:2006.15480", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we are interested in the bottom-up paradigm of estimating\nhuman poses from an image. We study the dense keypoint regression framework\nthat is previously inferior to the keypoint detection and grouping framework.\nOur motivation is that regressing keypoint positions accurately needs to learn\nrepresentations that focus on the keypoint regions.\n  We present a simple yet effective approach, named disentangled keypoint\nregression (DEKR). We adopt adaptive convolutions through pixel-wise spatial\ntransformer to activate the pixels in the keypoint regions and accordingly\nlearn representations from them. We use a multi-branch structure for separate\nregression: each branch learns a representation with dedicated adaptive\nconvolutions and regresses one keypoint. The resulting disentangled\nrepresentations are able to attend to the keypoint regions, respectively, and\nthus the keypoint regression is spatially more accurate. We empirically show\nthat the proposed direct regression method outperforms keypoint detection and\ngrouping methods and achieves superior bottom-up pose estimation results on two\nbenchmark datasets, COCO and CrowdPose. The code and models are available at\nhttps://github.com/HRNet/DEKR.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 05:54:46 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Geng", "Zigang", ""], ["Sun", "Ke", ""], ["Xiao", "Bin", ""], ["Zhang", "Zhaoxiang", ""], ["Wang", "Jingdong", ""]]}, {"id": "2104.02301", "submitter": "Feng Gao", "authors": "Min Feng, Feng Gao, Jian Fang, Junyu Dong", "title": "Hyperspectral and LiDAR data classification based on linear\n  self-attention", "comments": "Accepted for publication in the International Geoscience and Remote\n  Sensing Symposium (IGARSS 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  An efficient linear self-attention fusion model is proposed in this paper for\nthe task of hyperspectral image (HSI) and LiDAR data joint classification. The\nproposed method is comprised of a feature extraction module, an attention\nmodule, and a fusion module. The attention module is a plug-and-play linear\nself-attention module that can be extensively used in any model. The proposed\nmodel has achieved the overall accuracy of 95.40\\% on the Houston dataset. The\nexperimental results demonstrate the superiority of the proposed method over\nother state-of-the-art models.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 05:57:41 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Feng", "Min", ""], ["Gao", "Feng", ""], ["Fang", "Jian", ""], ["Dong", "Junyu", ""]]}, {"id": "2104.02303", "submitter": "Tomasz Kryjak", "authors": "Dominika Przewlocka-Rus, Marcin Kowalczyk, Tomasz Kryjak", "title": "Exploration of Hardware Acceleration Methods for an XNOR Traffic Signs\n  Classifier", "comments": "12 pages, 2 figures, 6 tables. Submitted for the CORES 2021\n  conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning algorithms are a key component of many state-of-the-art vision\nsystems, especially as Convolutional Neural Networks (CNN) outperform most\nsolutions in the sense of accuracy. To apply such algorithms in real-time\napplications, one has to address the challenges of memory and computational\ncomplexity. To deal with the first issue, we use networks with reduced\nprecision, specifically a binary neural network (also known as XNOR). To\nsatisfy the computational requirements, we propose to use highly parallel and\nlow-power FPGA devices. In this work, we explore the possibility of\naccelerating XNOR networks for traffic sign classification. The trained binary\nnetworks are implemented on the ZCU 104 development board, equipped with a Zynq\nUltraScale+ MPSoC device using two different approaches. Firstly, we propose a\ncustom HDL accelerator for XNOR networks, which enables the inference with\nalmost 450 fps. Even better results are obtained with the second method - the\nXilinx FINN accelerator - enabling to process input images with around 550\nframe rate. Both approaches provide over 96% accuracy on the test set.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 06:01:57 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Przewlocka-Rus", "Dominika", ""], ["Kowalczyk", "Marcin", ""], ["Kryjak", "Tomasz", ""]]}, {"id": "2104.02322", "submitter": "Mehrdad Khani", "authors": "Mehrdad Khani, Vibhaalakshmi Sivaraman, Mohammad Alizadeh", "title": "Efficient Video Compression via Content-Adaptive Super-Resolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video compression is a critical component of Internet video delivery. Recent\nwork has shown that deep learning techniques can rival or outperform\nhuman-designed algorithms, but these methods are significantly less compute and\npower-efficient than existing codecs. This paper presents a new approach that\naugments existing codecs with a small, content-adaptive super-resolution model\nthat significantly boosts video quality. Our method, SRVC, encodes video into\ntwo bitstreams: (i) a content stream, produced by compressing downsampled\nlow-resolution video with the existing codec, (ii) a model stream, which\nencodes periodic updates to a lightweight super-resolution neural network\ncustomized for short segments of the video. SRVC decodes the video by passing\nthe decompressed low-resolution video frames through the (time-varying)\nsuper-resolution model to reconstruct high-resolution video frames. Our results\nshow that to achieve the same PSNR, SRVC requires 16% of the bits-per-pixel of\nH.265 in slow mode, and 2% of the bits-per-pixel of DVC, a recent deep\nlearning-based video compression scheme. SRVC runs at 90 frames per second on a\nNVIDIA V100 GPU.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 07:01:06 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Khani", "Mehrdad", ""], ["Sivaraman", "Vibhaalakshmi", ""], ["Alizadeh", "Mohammad", ""]]}, {"id": "2104.02323", "submitter": "Yunpeng Zhang", "authors": "Yunpeng Zhang, Jiwen Lu, Jie Zhou", "title": "Objects are Different: Flexible Monocular 3D Object Detection", "comments": "Accepted in CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The precise localization of 3D objects from a single image without depth\ninformation is a highly challenging problem. Most existing methods adopt the\nsame approach for all objects regardless of their diverse distributions,\nleading to limited performance for truncated objects. In this paper, we propose\na flexible framework for monocular 3D object detection which explicitly\ndecouples the truncated objects and adaptively combines multiple approaches for\nobject depth estimation. Specifically, we decouple the edge of the feature map\nfor predicting long-tail truncated objects so that the optimization of normal\nobjects is not influenced. Furthermore, we formulate the object depth\nestimation as an uncertainty-guided ensemble of directly regressed object depth\nand solved depths from different groups of keypoints. Experiments demonstrate\nthat our method outperforms the state-of-the-art method by relatively 27\\% for\nthe moderate level and 30\\% for the hard level in the test set of KITTI\nbenchmark while maintaining real-time efficiency. Code will be available at\n\\url{https://github.com/zhangyp15/MonoFlex}.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 07:01:28 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Zhang", "Yunpeng", ""], ["Lu", "Jiwen", ""], ["Zhou", "Jie", ""]]}, {"id": "2104.02324", "submitter": "Tianning Yuan", "authors": "Tianning Yuan (1), Fang Wan (1), Mengying Fu (1), Jianzhuang Liu (2),\n  Songcen Xu (2), Xiangyang Ji (3), Qixiang Ye (1) ((1) University of Chinese\n  Academy of Sciences, Beijing, China, (2) Noah's Ark Lab, Huawei Technologies,\n  Shenzhen, China, (3) Tsinghua University, Beijing, China)", "title": "Multiple instance active learning for object detection", "comments": "10 pages, 7 figures, 5 tables. Code is available at\n  https://github.com/yuantn/MI-AOD", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the substantial progress of active learning for image recognition,\nthere still lacks an instance-level active learning method specified for object\ndetection. In this paper, we propose Multiple Instance Active Object Detection\n(MI-AOD), to select the most informative images for detector training by\nobserving instance-level uncertainty. MI-AOD defines an instance uncertainty\nlearning module, which leverages the discrepancy of two adversarial instance\nclassifiers trained on the labeled set to predict instance uncertainty of the\nunlabeled set. MI-AOD treats unlabeled images as instance bags and feature\nanchors in images as instances, and estimates the image uncertainty by\nre-weighting instances in a multiple instance learning (MIL) fashion. Iterative\ninstance uncertainty learning and re-weighting facilitate suppressing noisy\ninstances, toward bridging the gap between instance uncertainty and image-level\nuncertainty. Experiments validate that MI-AOD sets a solid baseline for\ninstance-level active learning. On commonly used object detection datasets,\nMI-AOD outperforms state-of-the-art methods with significant margins,\nparticularly when the labeled sets are small. Code is available at\nhttps://github.com/yuantn/MI-AOD.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 07:03:38 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Yuan", "Tianning", ""], ["Wan", "Fang", ""], ["Fu", "Mengying", ""], ["Liu", "Jianzhuang", ""], ["Xu", "Songcen", ""], ["Ji", "Xiangyang", ""], ["Ye", "Qixiang", ""]]}, {"id": "2104.02326", "submitter": "Dongkyu Won", "authors": "Dongkyu Won, Euijin Jung, Sion An, Philip Chikontwe, Sang Hyun Park", "title": "Self-Supervised Learning based CT Denoising using Pseudo-CT Image Pairs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, Self-supervised learning methods able to perform image denoising\nwithout ground truth labels have been proposed. These methods create\nlow-quality images by adding random or Gaussian noise to images and then train\na model for denoising. Ideally, it would be beneficial if one can generate\nhigh-quality CT images with only a few training samples via self-supervision.\nHowever, the performance of CT denoising is generally limited due to the\ncomplexity of CT noise. To address this problem, we propose a novel\nself-supervised learning-based CT denoising method. In particular, we train\npre-train CT denoising and noise models that can predict CT noise from Low-dose\nCT (LDCT) using available LDCT and Normal-dose CT (NDCT) pairs. For a given\ntest LDCT, we generate Pseudo-LDCT and NDCT pairs using the pre-trained\ndenoising and noise models and then update the parameters of the denoising\nmodel using these pairs to remove noise in the test LDCT. To make realistic\nPseudo LDCT, we train multiple noise models from individual images and generate\nthe noise using the ensemble of noise models. We evaluate our method on the\n2016 AAPM Low-Dose CT Grand Challenge dataset. The proposed ensemble noise\nmodel can generate realistic CT noise, and thus our method significantly\nimproves the denoising performance existing denoising models trained by\nsupervised- and self-supervised learning.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 07:11:46 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Won", "Dongkyu", ""], ["Jung", "Euijin", ""], ["An", "Sion", ""], ["Chikontwe", "Philip", ""], ["Park", "Sang Hyun", ""]]}, {"id": "2104.02330", "submitter": "Yuecong Min", "authors": "Yuecong Min, Aiming Hao, Xiujuan Chai, Xilin Chen", "title": "Visual Alignment Constraint for Continuous Sign Language Recognition", "comments": "The code will be released: https://github.com/Blueprintf/VAC_CSLR", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vision-based Continuous Sign Language Recognition (CSLR) aims to recognize\nunsegmented gestures from image sequences. To better train CSLR models, the\niterative training scheme is widely adopted to alleviate the overfitting of the\nalignment model. Although the iterative training scheme can improve\nperformance, it will also increase the training time. In this work, we revisit\nthe overfitting problem in recent CTC-based CSLR works and attribute it to the\ninsufficient training of the feature extractor. To solve this problem, we\npropose a Visual Alignment Constraint (VAC) to enhance the feature extractor\nwith more alignment supervision. Specifically, the proposed VAC is composed of\ntwo auxiliary losses: one makes predictions based on visual features only, and\nthe other aligns short-term visual and long-term contextual features. Moreover,\nwe further propose two metrics to evaluate the contributions of the feature\nextractor and the alignment model, which provide evidence for the overfitting\nproblem. The proposed VAC achieves competitive performance on two challenging\nCSLR datasets and experimental results show its effectiveness.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 07:24:58 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Min", "Yuecong", ""], ["Hao", "Aiming", ""], ["Chai", "Xiujuan", ""], ["Chen", "Xilin", ""]]}, {"id": "2104.02331", "submitter": "Yuhao Zhang", "authors": "Yuhao Zhang, Shuhang Wang, Haoxiang Wu, Kejia Hu, Shufan Ji", "title": "Brain Tumors Classification for MR images based on Attention Guided Deep\n  Learning Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the clinical diagnosis and treatment of brain tumors, manual image reading\nconsumes a lot of energy and time. In recent years, the automatic tumor\nclassification technology based on deep learning has entered people's field of\nvision. Brain tumors can be divided into primary and secondary intracranial\ntumors according to their source. However, to our best knowledge, most existing\nresearch on brain tumors are limited to primary intracranial tumor images and\ncannot classify the source of the tumor. In order to solve the task of tumor\nsource type classification, we analyze the existing technology and propose an\nattention guided deep convolution neural network (CNN) model. Meanwhile, the\nmethod proposed in this paper also effectively improves the accuracy of\nclassifying the presence or absence of tumor. For the brain MR dataset, our\nmethod can achieve the average accuracy of 99.18% under ten-fold\ncross-validation for identifying the presence or absence of tumor, and 83.38%\nfor classifying the source of tumor. Experimental results show that our method\nis consistent with the method of medical experts. It can assist doctors in\nachieving efficient clinical diagnosis of brain tumors.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 07:25:52 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Zhang", "Yuhao", ""], ["Wang", "Shuhang", ""], ["Wu", "Haoxiang", ""], ["Hu", "Kejia", ""], ["Ji", "Shufan", ""]]}, {"id": "2104.02333", "submitter": "Jiawei Zhang", "authors": "Jiawei Zhang, Yanchun Zhang, Xiaowei Xu", "title": "Pyramid U-Net for Retinal Vessel Segmentation", "comments": "10 pages, 5 figures, Accepted by ICASSP2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Retinal blood vessel can assist doctors in diagnosis of eye-related diseases\nsuch as diabetes and hypertension, and its segmentation is particularly\nimportant for automatic retinal image analysis. However, it is challenging to\nsegment these vessels structures, especially the thin capillaries from the\ncolor retinal image due to low contrast and ambiguousness. In this paper, we\npropose pyramid U-Net for accurate retinal vessel segmentation. In pyramid\nU-Net, the proposed pyramid-scale aggregation blocks (PSABs) are employed in\nboth the encoder and decoder to aggregate features at higher, current and lower\nlevels. In this way, coarse-to-fine context information is shared and\naggregated in each block thus to improve the location of capillaries. To\nfurther improve performance, two optimizations including pyramid inputs\nenhancement and deep pyramid supervision are applied to PSABs in the encoder\nand decoder, respectively. For PSABs in the encoder, scaled input images are\nadded as extra inputs. While for PSABs in the decoder, scaled intermediate\noutputs are supervised by the scaled segmentation labels. Extensive evaluations\nshow that our pyramid U-Net outperforms the current state-of-the-art methods on\nthe public DRIVE and CHASE-DB1 datasets.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 07:33:52 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Zhang", "Jiawei", ""], ["Zhang", "Yanchun", ""], ["Xu", "Xiaowei", ""]]}, {"id": "2104.02357", "submitter": "Chen Ju", "authors": "Chen Ju, Peisen Zhao, Siheng Chen, Ya Zhang, Xiaoyun Zhang, Qi Tian", "title": "Adaptive Mutual Supervision for Weakly-Supervised Temporal Action\n  Localization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weakly-supervised temporal action localization aims to localize actions in\nuntrimmed videos with only video-level action category labels. Most of previous\nmethods ignore the incompleteness issue of Class Activation Sequences (CAS),\nsuffering from trivial localization results. To solve this issue, we introduce\nan adaptive mutual supervision framework (AMS) with two branches, where the\nbase branch adopts CAS to localize the most discriminative action regions,\nwhile the supplementary branch localizes the less discriminative action regions\nthrough a novel adaptive sampler. The adaptive sampler dynamically updates the\ninput of the supplementary branch with a sampling weight sequence negatively\ncorrelated with the CAS from the base branch, thereby prompting the\nsupplementary branch to localize the action regions underestimated by the base\nbranch. To promote mutual enhancement between these two branches, we construct\nmutual location supervision. Each branch leverages location pseudo-labels\ngenerated from the other branch as localization supervision. By alternately\noptimizing the two branches in multiple iterations, we progressively complete\naction regions. Extensive experiments on THUMOS14 and ActivityNet1.2\ndemonstrate that the proposed AMS method significantly outperforms the\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 08:31:10 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Ju", "Chen", ""], ["Zhao", "Peisen", ""], ["Chen", "Siheng", ""], ["Zhang", "Ya", ""], ["Zhang", "Xiaoyun", ""], ["Tian", "Qi", ""]]}, {"id": "2104.02361", "submitter": "Yiming Li", "authors": "Yiming Li, Tongqing Zhai, Yong Jiang, Zhifeng Li, Shu-Tao Xia", "title": "Backdoor Attack in the Physical World", "comments": "This work was done when Yiming Li was an intern at Tencent AI Lab,\n  supported by the Tencent Rhino-Bird Elite Training Program (2020). This is a\n  6-pages short version of our ongoing work, `Rethinking the Trigger of\n  Backdoor Attack' (arXiv:2004.04692). It is accepted by the non-archival ICLR\n  2021 workshop on Robust and Reliable Machine Learning in the Real World.\n  arXiv admin note: substantial text overlap with arXiv:2004.04692", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Backdoor attack intends to inject hidden backdoor into the deep neural\nnetworks (DNNs), such that the prediction of infected models will be\nmaliciously changed if the hidden backdoor is activated by the attacker-defined\ntrigger. Currently, most existing backdoor attacks adopted the setting of\nstatic trigger, $i.e.,$ triggers across the training and testing images follow\nthe same appearance and are located in the same area. In this paper, we revisit\nthis attack paradigm by analyzing trigger characteristics. We demonstrate that\nthis attack paradigm is vulnerable when the trigger in testing images is not\nconsistent with the one used for training. As such, those attacks are far less\neffective in the physical world, where the location and appearance of the\ntrigger in the digitized image may be different from that of the one used for\ntraining. Moreover, we also discuss how to alleviate such vulnerability. We\nhope that this work could inspire more explorations on backdoor properties, to\nhelp the design of more advanced backdoor attack and defense methods.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 08:37:33 GMT"}, {"version": "v2", "created": "Sat, 24 Apr 2021 16:40:13 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Li", "Yiming", ""], ["Zhai", "Tongqing", ""], ["Jiang", "Yong", ""], ["Li", "Zhifeng", ""], ["Xia", "Shu-Tao", ""]]}, {"id": "2104.02366", "submitter": "Lin Wan", "authors": "Yehansen Chen, Lin Wan, Zhihang Li, Qianyan Jing, Zongyuan Sun", "title": "Neural Feature Search for RGB-Infrared Person Re-Identification", "comments": "13 pages, 7 figures, accepted by CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  RGB-Infrared person re-identification (RGB-IR ReID) is a challenging\ncross-modality retrieval problem, which aims at matching the person-of-interest\nover visible and infrared camera views. Most existing works achieve performance\ngains through manually-designed feature selection modules, which often require\nsignificant domain knowledge and rich experience. In this paper, we study a\ngeneral paradigm, termed Neural Feature Search (NFS), to automate the process\nof feature selection. Specifically, NFS combines a dual-level feature search\nspace and a differentiable search strategy to jointly select identity-related\ncues in coarse-grained channels and fine-grained spatial pixels. This\ncombination allows NFS to adaptively filter background noises and concentrate\non informative parts of human bodies in a data-driven manner. Moreover, a\ncross-modality contrastive optimization scheme further guides NFS to search\nfeatures that can minimize modality discrepancy whilst maximizing inter-class\ndistance. Extensive experiments on mainstream benchmarks demonstrate that our\nmethod outperforms state-of-the-arts, especially achieving better performance\non the RegDB dataset with significant improvement of 11.20% and 8.64% in Rank-1\nand mAP, respectively.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 08:40:44 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Chen", "Yehansen", ""], ["Wan", "Lin", ""], ["Li", "Zhihang", ""], ["Jing", "Qianyan", ""], ["Sun", "Zongyuan", ""]]}, {"id": "2104.02381", "submitter": "Paridhi Maheshwari", "authors": "Paridhi Maheshwari, Ritwick Chaudhry, Vishwa Vinay", "title": "Scene Graph Embeddings Using Relative Similarity Supervision", "comments": "Accepted to AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scene graphs are a powerful structured representation of the underlying\ncontent of images, and embeddings derived from them have been shown to be\nuseful in multiple downstream tasks. In this work, we employ a graph\nconvolutional network to exploit structure in scene graphs and produce image\nembeddings useful for semantic image retrieval. Different from\nclassification-centric supervision traditionally available for learning image\nrepresentations, we address the task of learning from relative similarity\nlabels in a ranking context. Rooted within the contrastive learning paradigm,\nwe propose a novel loss function that operates on pairs of similar and\ndissimilar images and imposes relative ordering between them in embedding\nspace. We demonstrate that this Ranking loss, coupled with an intuitive triple\nsampling strategy, leads to robust representations that outperform well-known\ncontrastive losses on the retrieval task. In addition, we provide qualitative\nevidence of how retrieved results that utilize structured scene information\ncapture the global context of the scene, different from visual similarity\nsearch.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 09:13:05 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Maheshwari", "Paridhi", ""], ["Chaudhry", "Ritwick", ""], ["Vinay", "Vishwa", ""]]}, {"id": "2104.02385", "submitter": "Jiahao Lin", "authors": "Jiahao Lin, Gim Hee Lee", "title": "Learning Spatial Context with Graph Neural Network for Multi-Person Pose\n  Grouping", "comments": "7 pages, 4 figures. Accepted in ICRA 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bottom-up approaches for image-based multi-person pose estimation consist of\ntwo stages: (1) keypoint detection and (2) grouping of the detected keypoints\nto form person instances. Current grouping approaches rely on learned embedding\nfrom only visual features that completely ignore the spatial configuration of\nhuman poses. In this work, we formulate the grouping task as a graph\npartitioning problem, where we learn the affinity matrix with a Graph Neural\nNetwork (GNN). More specifically, we design a Geometry-aware Association GNN\nthat utilizes spatial information of the keypoints and learns local affinity\nfrom the global context. The learned geometry-based affinity is further fused\nwith appearance-based affinity to achieve robust keypoint association. Spectral\nclustering is used to partition the graph for the formation of the pose\ninstances. Experimental results on two benchmark datasets show that our\nproposed method outperforms existing appearance-only grouping frameworks, which\nshows the effectiveness of utilizing spatial context for robust grouping.\nSource code is available at: https://github.com/jiahaoLjh/PoseGrouping.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 09:21:14 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Lin", "Jiahao", ""], ["Lee", "Gim Hee", ""]]}, {"id": "2104.02391", "submitter": "Jing Zhang", "authors": "Wangbo Zhao and Jing Zhang and Long Li and Nick Barnes and Nian Liu\n  and Junwei Han", "title": "Weakly Supervised Video Salient Object Detection", "comments": null, "journal-ref": "2021 IEEE/CVF Conference on Computer Vision and Pattern\n  Recognition (CVPR)", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Significant performance improvement has been achieved for fully-supervised\nvideo salient object detection with the pixel-wise labeled training datasets,\nwhich are time-consuming and expensive to obtain. To relieve the burden of data\nannotation, we present the first weakly supervised video salient object\ndetection model based on relabeled \"fixation guided scribble annotations\".\nSpecifically, an \"Appearance-motion fusion module\" and bidirectional ConvLSTM\nbased framework are proposed to achieve effective multi-modal learning and\nlong-term temporal context modeling based on our new weak annotations. Further,\nwe design a novel foreground-background similarity loss to further explore the\nlabeling similarity across frames. A weak annotation boosting strategy is also\nintroduced to boost our model performance with a new pseudo-label generation\ntechnique. Extensive experimental results on six benchmark video saliency\ndetection datasets illustrate the effectiveness of our solution.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 09:48:38 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Zhao", "Wangbo", ""], ["Zhang", "Jing", ""], ["Li", "Long", ""], ["Barnes", "Nick", ""], ["Liu", "Nian", ""], ["Han", "Junwei", ""]]}, {"id": "2104.02395", "submitter": "M Tanveer PhD", "authors": "M.A. Ganaie (1) and Minghui Hu (2) and M. Tanveer*(1) and P.N.\n  Suganthan*(2) (* Corresponding Author (1) Department of Mathematics, Indian\n  Institute of Technology Indore, Simrol, Indore, 453552, India (2) School of\n  Electrical & Electronic Engineering, Nanyang Technological University,\n  Singapore)", "title": "Ensemble deep learning: A review", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ensemble learning combines several individual models to obtain better\ngeneralization performance. Currently, deep learning models with multilayer\nprocessing architecture is showing better performance as compared to the\nshallow or traditional classification models. Deep ensemble learning models\ncombine the advantages of both the deep learning models as well as the ensemble\nlearning such that the final model has better generalization performance. This\npaper reviews the state-of-art deep ensemble models and hence serves as an\nextensive summary for the researchers. The ensemble models are broadly\ncategorised into ensemble models like bagging, boosting and stacking, negative\ncorrelation based deep ensemble models, explicit/implicit ensembles,\nhomogeneous /heterogeneous ensemble, decision fusion strategies, unsupervised,\nsemi-supervised, reinforcement learning and online/incremental, multilabel\nbased deep ensemble models. Application of deep ensemble models in different\ndomains is also briefly discussed. Finally, we conclude this paper with some\nfuture recommendations and research directions.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 09:56:29 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Ganaie", "M. A.", ""], ["Hu", "Minghui", ""], ["Tanveer*", "M.", ""], ["Suganthan*", "P. N.", ""]]}, {"id": "2104.02409", "submitter": "Shihao Jiang", "authors": "Shihao Jiang, Dylan Campbell, Yao Lu, Hongdong Li, Richard Hartley", "title": "Learning to Estimate Hidden Motions with Global Motion Aggregation", "comments": "Accepted by ICCV 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Occlusions pose a significant challenge to optical flow algorithms that rely\non local evidences. We consider an occluded point to be one that is imaged in\nthe first frame but not in the next, a slight overloading of the standard\ndefinition since it also includes points that move out-of-frame. Estimating the\nmotion of these points is extremely difficult, particularly in the two-frame\nsetting. Previous work relies on CNNs to learn occlusions, without much\nsuccess, or requires multiple frames to reason about occlusions using temporal\nsmoothness. In this paper, we argue that the occlusion problem can be better\nsolved in the two-frame case by modelling image self-similarities. We introduce\na global motion aggregation module, a transformer-based approach to find\nlong-range dependencies between pixels in the first image, and perform global\naggregation on the corresponding motion features. We demonstrate that the\noptical flow estimates in the occluded regions can be significantly improved\nwithout damaging the performance in non-occluded regions. This approach obtains\nnew state-of-the-art results on the challenging Sintel dataset, improving the\naverage end-point error by 13.6% on Sintel Final and 13.7% on Sintel Clean. At\nthe time of submission, our method ranks first on these benchmarks among all\npublished and unpublished approaches. Code is available at\nhttps://github.com/zacjiang/GMA\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 10:32:03 GMT"}, {"version": "v2", "created": "Mon, 26 Jul 2021 22:56:24 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Jiang", "Shihao", ""], ["Campbell", "Dylan", ""], ["Lu", "Yao", ""], ["Li", "Hongdong", ""], ["Hartley", "Richard", ""]]}, {"id": "2104.02416", "submitter": "Diego Martin Arroyo", "authors": "Diego Martin Arroyo, Janis Postels and Federico Tombari", "title": "Variational Transformer Networks for Layout Generation", "comments": "To be published in CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative models able to synthesize layouts of different kinds (e.g.\ndocuments, user interfaces or furniture arrangements) are a useful tool to aid\ndesign processes and as a first step in the generation of synthetic data, among\nother tasks. We exploit the properties of self-attention layers to capture high\nlevel relationships between elements in a layout, and use these as the building\nblocks of the well-known Variational Autoencoder (VAE) formulation. Our\nproposed Variational Transformer Network (VTN) is capable of learning margins,\nalignments and other global design rules without explicit supervision. Layouts\nsampled from our model have a high degree of resemblance to the training data,\nwhile demonstrating appealing diversity. In an extensive evaluation on publicly\navailable benchmarks for different layout types VTNs achieve state-of-the-art\ndiversity and perceptual quality. Additionally, we show the capabilities of\nthis method as part of a document layout detection pipeline.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 10:45:53 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Arroyo", "Diego Martin", ""], ["Postels", "Janis", ""], ["Tombari", "Federico", ""]]}, {"id": "2104.02424", "submitter": "Hardik Uppal", "authors": "Hardik Uppal, Alireza Sepas-Moghaddam, Michael Greenspan, Ali Etemad", "title": "Teacher-Student Adversarial Depth Hallucination to Improve Face\n  Recognition", "comments": "10 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present the Teacher-Student Generative Adversarial Network (TS-GAN) to\ngenerate depth images from a single RGB image in order to boost the recognition\naccuracy of face recognition (FR) systems. For our method to generalize well\nacross unseen datasets, we design two components in the architecture, a teacher\nand a student. The teacher, which itself consists of a generator and a\ndiscriminator, learns a latent mapping between input RGB and paired depth\nimages in a supervised fashion. The student, which consists of two generators\n(one shared with the teacher) and a discriminator, learns from new RGB data\nwith no available paired depth information, for improved generalization. The\nfully trained shared generator can then be used in runtime to hallucinate depth\nfrom RGB for downstream applications such as face recognition. We perform\nrigorous experiments to show the superiority of TS-GAN over other methods in\ngenerating synthetic depth images. Moreover, face recognition experiments\ndemonstrate that our hallucinated depth along with the input RGB images boosts\nperformance across various architectures when compared to a single RGB modality\nby average values of +1.2%, +2.6%, and +2.6% for IIIT-D, EURECOM, and LFW\ndatasets respectively.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 11:07:02 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Uppal", "Hardik", ""], ["Sepas-Moghaddam", "Alireza", ""], ["Greenspan", "Michael", ""], ["Etemad", "Ali", ""]]}, {"id": "2104.02429", "submitter": "Zhe Ma", "authors": "Jianfeng Dong, Zhe Ma, Xiaofeng Mao, Xun Yang, Yuan He, Richang Hong,\n  Shouling Ji", "title": "Fine-Grained Fashion Similarity Prediction by Attribute-Specific\n  Embedding Learning", "comments": "arXiv admin note: substantial text overlap with arXiv:2002.02814", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper strives to predict fine-grained fashion similarity. In this\nsimilarity paradigm, one should pay more attention to the similarity in terms\nof a specific design/attribute between fashion items. For example, whether the\ncollar designs of the two clothes are similar. It has potential value in many\nfashion related applications, such as fashion copyright protection. To this\nend, we propose an Attribute-Specific Embedding Network (ASEN) to jointly learn\nmultiple attribute-specific embeddings, thus measure the fine-grained\nsimilarity in the corresponding space. The proposed ASEN is comprised of a\nglobal branch and a local branch. The global branch takes the whole image as\ninput to extract features from a global perspective, while the local branch\ntakes as input the zoomed-in region-of-interest (RoI) w.r.t. the specified\nattribute thus able to extract more fine-grained features. As the global branch\nand the local branch extract the features from different perspectives, they are\ncomplementary to each other. Additionally, in each branch, two attention\nmodules, i.e., Attribute-aware Spatial Attention and Attribute-aware Channel\nAttention, are integrated to make ASEN be able to locate the related regions\nand capture the essential patterns under the guidance of the specified\nattribute, thus make the learned attribute-specific embeddings better reflect\nthe fine-grained similarity. Extensive experiments on three fashion-related\ndatasets, i.e., FashionAI, DARN, and DeepFashion, show the effectiveness of\nASEN for fine-grained fashion similarity prediction and its potential for\nfashion reranking. Code and data are available at\nhttps://github.com/maryeon/asenpp .\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 11:26:38 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Dong", "Jianfeng", ""], ["Ma", "Zhe", ""], ["Mao", "Xiaofeng", ""], ["Yang", "Xun", ""], ["He", "Yuan", ""], ["Hong", "Richang", ""], ["Ji", "Shouling", ""]]}, {"id": "2104.02439", "submitter": "PengWan Yang", "authors": "Pengwan Yang and Pascal Mettes and Cees G. M. Snoek", "title": "Few-Shot Transformation of Common Actions into Time and Space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces the task of few-shot common action localization in time\nand space. Given a few trimmed support videos containing the same but unknown\naction, we strive for spatio-temporal localization of that action in a long\nuntrimmed query video. We do not require any class labels, interval bounds, or\nbounding boxes. To address this challenging task, we introduce a novel few-shot\ntransformer architecture with a dedicated encoder-decoder structure optimized\nfor joint commonality learning and localization prediction, without the need\nfor proposals. Experiments on our reorganizations of the AVA and UCF101-24\ndatasets show the effectiveness of our approach for few-shot common action\nlocalization, even when the support videos are noisy. Although we are not\nspecifically designed for common localization in time only, we also compare\nfavorably against the few-shot and one-shot state-of-the-art in this setting.\nLastly, we demonstrate that the few-shot transformer is easily extended to\ncommon action localization per pixel.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 11:55:08 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Yang", "Pengwan", ""], ["Mettes", "Pascal", ""], ["Snoek", "Cees G. M.", ""]]}, {"id": "2104.02471", "submitter": "Khalil Khan", "authors": "Khalil Khan, Jehad Ali, Irfan Uddin, Sahib Khan, and Byeong-hee Roh", "title": "A Facial Feature Discovery Framework for Race Classification Using Deep\n  Learning", "comments": "Number of pages in the paper are 15", "journal-ref": "Under review in Computer, Material, and Continua, 2021", "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Race classification is a long-standing challenge in the field of face image\nanalysis. The investigation of salient facial features is an important task to\navoid processing all face parts. Face segmentation strongly benefits several\nface analysis tasks, including ethnicity and race classification. We propose a\nraceclassification algorithm using a prior face segmentation framework. A deep\nconvolutional neural network (DCNN) was used to construct a face segmentation\nmodel. For training the DCNN, we label face images according to seven different\nclasses, that is, nose, skin, hair, eyes, brows, back, and mouth. The DCNN\nmodel developed in the first phase was used to create segmentation results. The\nprobabilistic classification method is used, and probability maps (PMs) are\ncreated for each semantic class. We investigated five salient facial features\nfrom among seven that help in race classification. Features are extracted from\nthe PMs of five classes, and a new model is trained based on the DCNN. We\nassessed the performance of the proposed race classification method on four\nstandard face datasets, reporting superior results compared with previous\nstudies.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 06:33:04 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Khan", "Khalil", ""], ["Ali", "Jehad", ""], ["Uddin", "Irfan", ""], ["Khan", "Sahib", ""], ["Roh", "Byeong-hee", ""]]}, {"id": "2104.02481", "submitter": "Ashkan Khakzar", "authors": "Ashkan Khakzar, Sabrina Musatian, Jonas Buchberger, Icxel Valeriano\n  Quiroz, Nikolaus Pinger, Soroosh Baselizadeh, Seong Tae Kim, Nassir Navab", "title": "Towards Semantic Interpretation of Thoracic Disease and COVID-19\n  Diagnosis Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks are showing promise in the automatic diagnosis\nof thoracic pathologies on chest x-rays. Their black-box nature has sparked\nmany recent works to explain the prediction via input feature attribution\nmethods (aka saliency methods). However, input feature attribution methods\nmerely identify the importance of input regions for the prediction and lack\nsemantic interpretation of model behavior. In this work, we first identify the\nsemantics associated with internal units (feature maps) of the network. We\nproceed to investigate the following questions; Does a regression model that is\nonly trained with COVID-19 severity scores implicitly learn visual patterns\nassociated with thoracic pathologies? Does a network that is trained on weakly\nlabeled data (e.g. healthy, unhealthy) implicitly learn pathologies? Moreover,\nwe investigate the effect of pretraining and data imbalance on the\ninterpretability of learned features. In addition to the analysis, we propose\nsemantic attribution to semantically explain each prediction. We present our\nfindings using publicly available chest pathologies (CheXpert, NIH ChestX-ray8)\nand COVID-19 datasets (BrixIA, and COVID-19 chest X-ray segmentation dataset).\nThe Code is publicly available.\n", "versions": [{"version": "v1", "created": "Sun, 4 Apr 2021 17:35:13 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Khakzar", "Ashkan", ""], ["Musatian", "Sabrina", ""], ["Buchberger", "Jonas", ""], ["Quiroz", "Icxel Valeriano", ""], ["Pinger", "Nikolaus", ""], ["Baselizadeh", "Soroosh", ""], ["Kim", "Seong Tae", ""], ["Navab", "Nassir", ""]]}, {"id": "2104.02486", "submitter": "Jiabin Zhang", "authors": "Jiabin Zhang, Zheng Zhu, Jiwen Lu, Junjie Huang, Guan Huang, Jie Zhou", "title": "SIMPLE: SIngle-network with Mimicking and Point Learning for Bottom-up\n  Human Pose Estimation", "comments": "Accepted by AAAI2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The practical application requests both accuracy and efficiency on\nmulti-person pose estimation algorithms. But the high accuracy and fast\ninference speed are dominated by top-down methods and bottom-up methods\nrespectively. To make a better trade-off between accuracy and efficiency, we\npropose a novel multi-person pose estimation framework, SIngle-network with\nMimicking and Point Learning for Bottom-up Human Pose Estimation (SIMPLE).\nSpecifically, in the training process, we enable SIMPLE to mimic the pose\nknowledge from the high-performance top-down pipeline, which significantly\npromotes SIMPLE's accuracy while maintaining its high efficiency during\ninference. Besides, SIMPLE formulates human detection and pose estimation as a\nunified point learning framework to complement each other in single-network.\nThis is quite different from previous works where the two tasks may interfere\nwith each other. To the best of our knowledge, both mimicking strategy between\ndifferent method types and unified point learning are firstly proposed in pose\nestimation. In experiments, our approach achieves the new state-of-the-art\nperformance among bottom-up methods on the COCO, MPII and PoseTrack datasets.\nCompared with the top-down approaches, SIMPLE has comparable accuracy and\nfaster inference speed.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 13:12:51 GMT"}, {"version": "v2", "created": "Wed, 7 Apr 2021 07:41:43 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Zhang", "Jiabin", ""], ["Zhu", "Zheng", ""], ["Lu", "Jiwen", ""], ["Huang", "Junjie", ""], ["Huang", "Guan", ""], ["Zhou", "Jie", ""]]}, {"id": "2104.02488", "submitter": "Jose Dolz", "authors": "Gaurav Patel and Jose Dolz", "title": "Weakly supervised segmentation with cross-modality equivariant\n  constraints", "comments": "Submitted to TMI. Code available", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weakly supervised learning has emerged as an appealing alternative to\nalleviate the need for large labeled datasets in semantic segmentation. Most\ncurrent approaches exploit class activation maps (CAMs), which can be generated\nfrom image-level annotations. Nevertheless, resulting maps have been\ndemonstrated to be highly discriminant, failing to serve as optimal proxy\npixel-level labels. We present a novel learning strategy that leverages\nself-supervision in a multi-modal image scenario to significantly enhance\noriginal CAMs. In particular, the proposed method is based on two observations.\nFirst, the learning of fully-supervised segmentation networks implicitly\nimposes equivariance by means of data augmentation, whereas this implicit\nconstraint disappears on CAMs generated with image tags. And second, the\ncommonalities between image modalities can be employed as an efficient\nself-supervisory signal, correcting the inconsistency shown by CAMs obtained\nacross multiple modalities. To effectively train our model, we integrate a\nnovel loss function that includes a within-modality and a cross-modality\nequivariant term to explicitly impose these constraints during training. In\naddition, we add a KL-divergence on the class prediction distributions to\nfacilitate the information exchange between modalities, which, combined with\nthe equivariant regularizers further improves the performance of our model.\nExhaustive experiments on the popular multi-modal BRATS dataset demonstrate\nthat our approach outperforms relevant recent literature under the same\nlearning conditions.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 13:14:20 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Patel", "Gaurav", ""], ["Dolz", "Jose", ""]]}, {"id": "2104.02495", "submitter": "Li Siyao", "authors": "Li Siyao, Shiyu Zhao, Weijiang Yu, Wenxiu Sun, Dimitris N. Metaxas,\n  Chen Change Loy, Ziwei Liu", "title": "Deep Animation Video Interpolation in the Wild", "comments": "Accepted by CVPR21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In the animation industry, cartoon videos are usually produced at low frame\nrate since hand drawing of such frames is costly and time-consuming. Therefore,\nit is desirable to develop computational models that can automatically\ninterpolate the in-between animation frames. However, existing video\ninterpolation methods fail to produce satisfying results on animation data.\nCompared to natural videos, animation videos possess two unique characteristics\nthat make frame interpolation difficult: 1) cartoons comprise lines and smooth\ncolor pieces. The smooth areas lack textures and make it difficult to estimate\naccurate motions on animation videos. 2) cartoons express stories via\nexaggeration. Some of the motions are non-linear and extremely large. In this\nwork, we formally define and study the animation video interpolation problem\nfor the first time. To address the aforementioned challenges, we propose an\neffective framework, AnimeInterp, with two dedicated modules in a\ncoarse-to-fine manner. Specifically, 1) Segment-Guided Matching resolves the\n\"lack of textures\" challenge by exploiting global matching among color pieces\nthat are piece-wise coherent. 2) Recurrent Flow Refinement resolves the\n\"non-linear and extremely large motion\" challenge by recurrent predictions\nusing a transformer-like architecture. To facilitate comprehensive training and\nevaluations, we build a large-scale animation triplet dataset, ATD-12K, which\ncomprises 12,000 triplets with rich annotations. Extensive experiments\ndemonstrate that our approach outperforms existing state-of-the-art\ninterpolation methods for animation videos. Notably, AnimeInterp shows\nfavorable perceptual quality and robustness for animation scenarios in the\nwild. The proposed dataset and code are available at\nhttps://github.com/lisiyao21/AnimeInterp/.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 13:26:49 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Siyao", "Li", ""], ["Zhao", "Shiyu", ""], ["Yu", "Weijiang", ""], ["Sun", "Wenxiu", ""], ["Metaxas", "Dimitris N.", ""], ["Loy", "Chen Change", ""], ["Liu", "Ziwei", ""]]}, {"id": "2104.02525", "submitter": "Qian Ning", "authors": "Qian Ning, Weisheng Dong, Xin Li, Jinjian Wu, Leida Li, Guangming Shi", "title": "Searching Efficient Model-guided Deep Network for Image Denoising", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural architecture search (NAS) has recently reshaped our understanding on\nvarious vision tasks. Similar to the success of NAS in high-level vision tasks,\nit is possible to find a memory and computationally efficient solution via NAS\nwith highly competent denoising performance. However, the optimization gap\nbetween the super-network and the sub-architectures has remained an open issue\nin both low-level and high-level vision. In this paper, we present a novel\napproach to filling in this gap by connecting model-guided design with NAS\n(MoD-NAS) and demonstrate its application into image denoising. Specifically,\nwe propose to construct a new search space under model-guided framework and\ndevelop more stable and efficient differential search strategies. MoD-NAS\nemploys a highly reusable width search strategy and a densely connected search\nblock to automatically select the operations of each layer as well as network\nwidth and depth via gradient descent. During the search process, the proposed\nMoG-NAS is capable of avoiding mode collapse due to the smoother search space\ndesigned under the model-guided framework. Experimental results on several\npopular datasets show that our MoD-NAS has achieved even better PSNR\nperformance than current state-of-the-art methods with fewer parameters, lower\nnumber of flops, and less amount of testing time.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 14:03:01 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Ning", "Qian", ""], ["Dong", "Weisheng", ""], ["Li", "Xin", ""], ["Wu", "Jinjian", ""], ["Li", "Leida", ""], ["Shi", "Guangming", ""]]}, {"id": "2104.02527", "submitter": "Yangzheng Wu", "authors": "Yangzheng Wu, Mohsen Zand, Ali Etemad, Michael Greenspan", "title": "Vote from the Center: 6 DoF Pose Estimation in RGB-D Images by Radial\n  Keypoint Voting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel keypoint voting scheme based on intersecting spheres, that\nis more accurate than existing schemes and allows for a smaller set of more\ndisperse keypoints. The scheme forms the basis of the proposed RCVPose method\nfor 6 DoF pose estimation of 3D objects in RGB-D data, which is particularly\neffective at handling occlusions. A CNN is trained to estimate the distance\nbetween the 3D point corresponding to the depth mode of each RGB pixel, and a\nset of 3 disperse keypoints defined in the object frame. At inference, a sphere\nof radius equal to this estimated distance is generated, centered at each 3D\npoint. The surface of these spheres votes to increment a 3D accumulator space,\nthe peaks of which indicate keypoint locations. The proposed radial voting\nscheme is more accurate than previous vector or offset schemes, and robust to\ndisperse keypoints. Experiments demonstrate RCVPose to be highly accurate and\ncompetitive, achieving state-of-the-art results on LINEMOD 99.7%, YCB-Video\n97.2% datasets, and notably scoring +7.9% higher than previous methods on the\nchallenging Occlusion LINEMOD 71.1% dataset.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 14:06:08 GMT"}, {"version": "v2", "created": "Wed, 7 Apr 2021 21:29:19 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Wu", "Yangzheng", ""], ["Zand", "Mohsen", ""], ["Etemad", "Ali", ""], ["Greenspan", "Michael", ""]]}, {"id": "2104.02533", "submitter": "Yifu Liu", "authors": "Yifu Liu, Chenfeng Xu and Xinyu Jin", "title": "DCANet: Dense Context-Aware Network for Semantic Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the superiority of context information gradually manifests in advanced\nsemantic segmentation, learning to capture the compact context relationship can\nhelp to understand the complex scenes. In contrast to some previous works\nutilizing the multi-scale context fusion, we propose a novel module, named\nDense Context-Aware (DCA) module, to adaptively integrate local detail\ninformation with global dependencies. Driven by the contextual relationship,\nthe DCA module can better achieve the aggregation of context information to\ngenerate more powerful features. Furthermore, we deliberately design two\nextended structures based on the DCA modules to further capture the long-range\ncontextual dependency information. By combining the DCA modules in cascade or\nparallel, our networks use a progressive strategy to improve multi-scale\nfeature representations for robust segmentation. We empirically demonstrate the\npromising performance of our approach (DCANet) with extensive experiments on\nthree challenging datasets, including PASCAL VOC 2012, Cityscapes, and ADE20K.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 14:12:22 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Liu", "Yifu", ""], ["Xu", "Chenfeng", ""], ["Jin", "Xinyu", ""]]}, {"id": "2104.02538", "submitter": "Mehmet Ozgur Turkoglu", "authors": "Mehmet Ozgur Turkoglu, Eric Brachmann, Konrad Schindler, Gabriel\n  Brostow, Aron Monszpart", "title": "Visual Camera Re-Localization Using Graph Neural Networks and Relative\n  Pose Supervision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual re-localization means using a single image as input to estimate the\ncamera's location and orientation relative to a pre-recorded environment. The\nhighest-scoring methods are \"structure based,\" and need the query camera's\nintrinsics as an input to the model, with careful geometric optimization. When\nintrinsics are absent, methods vie for accuracy by making various other\nassumptions. This yields fairly good localization scores, but the models are\n\"narrow\" in some way, eg., requiring costly test-time computations, or depth\nsensors, or multiple query frames. In contrast, our proposed method makes few\nspecial assumptions, and is fairly lightweight in training and testing.\n  Our pose regression network learns from only relative poses of training\nscenes. For inference, it builds a graph connecting the query image to training\ncounterparts and uses a graph neural network (GNN) with image representations\non nodes and image-pair representations on edges. By efficiently passing\nmessages between them, both representation types are refined to produce a\nconsistent camera pose estimate. We validate the effectiveness of our approach\non both standard indoor (7-Scenes) and outdoor (Cambridge Landmarks) camera\nre-localization benchmarks. Our relative pose regression method matches the\naccuracy of absolute pose regression networks, while retaining the\nrelative-pose models' test-time speed and ability to generalize to non-training\nscenes.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 14:29:03 GMT"}, {"version": "v2", "created": "Mon, 12 Apr 2021 14:38:14 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Turkoglu", "Mehmet Ozgur", ""], ["Brachmann", "Eric", ""], ["Schindler", "Konrad", ""], ["Brostow", "Gabriel", ""], ["Monszpart", "Aron", ""]]}, {"id": "2104.02541", "submitter": "Nicoletta Risi", "authors": "Nicoletta Risi, Enrico Calabrese, Giacomo Indiveri", "title": "Instantaneous Stereo Depth Estimation of Real-World Stimuli with a\n  Neuromorphic Stereo-Vision Setup", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The stereo-matching problem, i.e., matching corresponding features in two\ndifferent views to reconstruct depth, is efficiently solved in biology. Yet, it\nremains the computational bottleneck for classical machine vision approaches.\nBy exploiting the properties of event cameras, recently proposed Spiking Neural\nNetwork (SNN) architectures for stereo vision have the potential of simplifying\nthe stereo-matching problem. Several solutions that combine event cameras with\nspike-based neuromorphic processors already exist. However, they are either\nsimulated on digital hardware or tested on simplified stimuli. In this work, we\nuse the Dynamic Vision Sensor 3D Human Pose Dataset (DHP19) to validate a\nbrain-inspired event-based stereo-matching architecture implemented on a\nmixed-signal neuromorphic processor with real-world data. Our experiments show\nthat this SNN architecture, composed of coincidence detectors and disparity\nsensitive neurons, is able to provide a coarse estimate of the input disparity\ninstantaneously, thereby detecting the presence of a stimulus moving in depth\nin real-time.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 14:31:23 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Risi", "Nicoletta", ""], ["Calabrese", "Enrico", ""], ["Indiveri", "Giacomo", ""]]}, {"id": "2104.02548", "submitter": "Meghna P Ayyar", "authors": "Meghna P Ayyar, Jenny Benois-Pineau, Akka Zemmari", "title": "White Box Methods for Explanations of Convolutional Neural Networks in\n  Image Classification Tasks", "comments": "Submitted to Journal of Electronic Imaging (JEI)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In recent years, deep learning has become prevalent to solve applications\nfrom multiple domains. Convolutional Neural Networks (CNNs) particularly have\ndemonstrated state of the art performance for the task of image classification.\nHowever, the decisions made by these networks are not transparent and cannot be\ndirectly interpreted by a human. Several approaches have been proposed to\nexplain to understand the reasoning behind a prediction made by a network. In\nthis paper, we propose a topology of grouping these methods based on their\nassumptions and implementations. We focus primarily on white box methods that\nleverage the information of the internal architecture of a network to explain\nits decision. Given the task of image classification and a trained CNN, this\nwork aims to provide a comprehensive and detailed overview of a set of methods\nthat can be used to create explanation maps for a particular image, that assign\nan importance score to each pixel of the image based on its contribution to the\ndecision of the network. We also propose a further classification of the white\nbox methods based on their implementations to enable better comparisons and\nhelp researchers find methods best suited for different scenarios.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 14:40:00 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Ayyar", "Meghna P", ""], ["Benois-Pineau", "Jenny", ""], ["Zemmari", "Akka", ""]]}, {"id": "2104.02555", "submitter": "Tim-Oliver Buchholz", "authors": "Tim-Oliver Buchholz and Florian Jug", "title": "Fourier Image Transformer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Transformer architectures show spectacular performance on NLP tasks and have\nrecently also been used for tasks such as image completion or image\nclassification. Here we propose to use a sequential image representation, where\neach prefix of the complete sequence describes the whole image at reduced\nresolution. Using such Fourier Domain Encodings (FDEs), an auto-regressive\nimage completion task is equivalent to predicting a higher resolution output\ngiven a low-resolution input. Additionally, we show that an encoder-decoder\nsetup can be used to query arbitrary Fourier coefficients given a set of\nFourier domain observations. We demonstrate the practicality of this approach\nin the context of computed tomography (CT) image reconstruction. In summary, we\nshow that Fourier Image Transformer (FIT) can be used to solve relevant image\nanalysis tasks in Fourier space, a domain inherently inaccessible to\nconvolutional architectures.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 14:48:57 GMT"}, {"version": "v2", "created": "Mon, 3 May 2021 10:29:54 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Buchholz", "Tim-Oliver", ""], ["Jug", "Florian", ""]]}, {"id": "2104.02571", "submitter": "Themos Stafylakis", "authors": "Themos Stafylakis, Johan Rohdin, Lukas Burget", "title": "Speaker embeddings by modeling channel-wise correlations", "comments": "Accepted at Interspeech 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Speaker embeddings extracted with deep 2D convolutional neural networks are\ntypically modeled as projections of first and second order statistics of\nchannel-frequency pairs onto a linear layer, using either average or attentive\npooling along the time axis. In this paper we examine an alternative pooling\nmethod, where pairwise correlations between channels for given frequencies are\nused as statistics. The method is inspired by style-transfer methods in\ncomputer vision, where the style of an image, modeled by the matrix of\nchannel-wise correlations, is transferred to another image, in order to produce\na new image having the style of the first and the content of the second. By\ndrawing analogies between image style and speaker characteristics, and between\nimage content and phonetic sequence, we explore the use of such channel-wise\ncorrelations features to train a ResNet architecture in an end-to-end fashion.\nOur experiments on VoxCeleb demonstrate the effectiveness of the proposed\npooling method in speaker recognition.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 15:10:14 GMT"}, {"version": "v2", "created": "Wed, 7 Jul 2021 14:00:22 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Stafylakis", "Themos", ""], ["Rohdin", "Johan", ""], ["Burget", "Lukas", ""]]}, {"id": "2104.02576", "submitter": "Jiaolong Xu", "authors": "Chen Min and Jiaolong Xu and Liang Xiao and Dawei Zhao and Yiming Nie\n  and Bin Dai", "title": "Attentional Graph Neural Network for Parking-slot Detection", "comments": "Accepted by RAL", "journal-ref": "IEEE Robotics and Automation Letters, vol.6, pp. 3445-3450, 2021", "doi": "10.1109/LRA.2021.3064270", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has recently demonstrated its promising performance for\nvision-based parking-slot detection. However, very few existing methods\nexplicitly take into account learning the link information of the\nmarking-points, resulting in complex post-processing and erroneous detection.\nIn this paper, we propose an attentional graph neural network based\nparking-slot detection method, which refers the marking-points in an\naround-view image as graph-structured data and utilize graph neural network to\naggregate the neighboring information between marking-points. Without any\nmanually designed post-processing, the proposed method is end-to-end trainable.\nExtensive experiments have been conducted on public benchmark dataset, where\nthe proposed method achieves state-of-the-art accuracy. Code is publicly\navailable at \\url{https://github.com/Jiaolong/gcn-parking-slot}.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 15:14:39 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Min", "Chen", ""], ["Xu", "Jiaolong", ""], ["Xiao", "Liang", ""], ["Zhao", "Dawei", ""], ["Nie", "Yiming", ""], ["Dai", "Bin", ""]]}, {"id": "2104.02598", "submitter": "Dima Kagan", "authors": "Dima Kagan, Galit Fuhrmann Alpert, Michael Fire", "title": "Automatic Large Scale Detection of Red Palm Weevil Infestation using\n  Aerial and Street View Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The spread of the Red Palm Weevil has dramatically affected date growers,\nhomeowners and governments, forcing them to deal with a constant threat to\ntheir palm trees. Early detection of palm tree infestation has been proven to\nbe critical in order to allow treatment that may save trees from irreversible\ndamage, and is most commonly performed by local physical access for individual\ntree monitoring. Here, we present a novel method for surveillance of Red Palm\nWeevil infested palm trees utilizing state-of-the-art deep learning algorithms,\nwith aerial and street-level imagery data. To detect infested palm trees we\nanalyzed over 100,000 aerial and street-images, mapping the location of palm\ntrees in urban areas. Using this procedure, we discovered and verified infested\npalm trees at various locations.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 15:35:26 GMT"}, {"version": "v2", "created": "Fri, 9 Apr 2021 05:35:50 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Kagan", "Dima", ""], ["Alpert", "Galit Fuhrmann", ""], ["Fire", "Michael", ""]]}, {"id": "2104.02600", "submitter": "Eliya Nachmani", "authors": "Robin San-Roman, Eliya Nachmani, Lior Wolf", "title": "Noise Estimation for Generative Diffusion Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Generative diffusion models have emerged as leading models in speech and\nimage generation. However, in order to perform well with a small number of\ndenoising steps, a costly tuning of the set of noise parameters is needed. In\nthis work, we present a simple and versatile learning scheme that can\nstep-by-step adjust those noise parameters, for any given number of steps,\nwhile the previous work needs to retune for each number separately.\nFurthermore, without modifying the weights of the diffusion model, we are able\nto significantly improve the synthesis results, for a small number of steps.\nOur approach comes at a negligible computation cost.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 15:46:16 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["San-Roman", "Robin", ""], ["Nachmani", "Eliya", ""], ["Wolf", "Lior", ""]]}, {"id": "2104.02602", "submitter": "Li Xiao", "authors": "Li Xiao, Yinhao Li, Luxi Qv, Xinxia Tian, Yijie Peng, S.Kevin Zhou", "title": "Pathological Image Segmentation with Noisy Labels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Segmentation of pathological images is essential for accurate disease\ndiagnosis. The quality of manual labels plays a critical role in segmentation\naccuracy; yet, in practice, the labels between pathologists could be\ninconsistent, thus confusing the training process. In this work, we propose a\nnovel label re-weighting framework to account for the reliability of different\nexperts' labels on each pixel according to its surrounding features. We further\ndevise a new attention heatmap, which takes roughness as prior knowledge to\nguide the model to focus on important regions. Our approach is evaluated on the\npublic Gleason 2019 datasets. The results show that our approach effectively\nimproves the model's robustness against noisy labels and outperforms\nstate-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Sat, 20 Mar 2021 03:36:06 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Xiao", "Li", ""], ["Li", "Yinhao", ""], ["Qv", "Luxi", ""], ["Tian", "Xinxia", ""], ["Peng", "Yijie", ""], ["Zhou", "S. Kevin", ""]]}, {"id": "2104.02605", "submitter": "Zejun Li", "authors": "Zejun Li, Zhongyu Wei, Zhihao Fan, Haijun Shan, Xuanjing Huang", "title": "An Unsupervised Sampling Approach for Image-Sentence Matching Using\n  Document-Level Structural Information", "comments": "To be published in AAAI2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we focus on the problem of unsupervised image-sentence\nmatching. Existing research explores to utilize document-level structural\ninformation to sample positive and negative instances for model training.\nAlthough the approach achieves positive results, it introduces a sampling bias\nand fails to distinguish instances with high semantic similarity. To alleviate\nthe bias, we propose a new sampling strategy to select additional\nintra-document image-sentence pairs as positive or negative samples.\nFurthermore, to recognize the complex pattern in intra-document samples, we\npropose a Transformer based model to capture fine-grained features and\nimplicitly construct a graph for each document, where concepts in a document\nare introduced to bridge the representation learning of images and sentences in\nthe context of a document. Experimental results show the effectiveness of our\napproach to alleviate the bias and learn well-aligned multimodal\nrepresentations.\n", "versions": [{"version": "v1", "created": "Sun, 21 Mar 2021 05:43:29 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Li", "Zejun", ""], ["Wei", "Zhongyu", ""], ["Fan", "Zhihao", ""], ["Shan", "Haijun", ""], ["Huang", "Xuanjing", ""]]}, {"id": "2104.02606", "submitter": "Tanzila Rahman", "authors": "Tanzila Rahman, Leonid Sigal", "title": "Weakly-supervised Audio-visual Sound Source Detection and Separation", "comments": "4 figures, 6 pages", "journal-ref": "IEEE International Conference on Multimedia and Expo (ICME) 2021", "doi": null, "report-no": null, "categories": "cs.CV cs.SD eess.AS eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning how to localize and separate individual object sounds in the audio\nchannel of the video is a difficult task. Current state-of-the-art methods\npredict audio masks from artificially mixed spectrograms, known as\nMix-and-Separate framework. We propose an audio-visual co-segmentation, where\nthe network learns both what individual objects look and sound like, from\nvideos labeled with only object labels. Unlike other recent visually-guided\naudio source separation frameworks, our architecture can be learned in an\nend-to-end manner and requires no additional supervision or bounding box\nproposals. Specifically, we introduce weakly-supervised object segmentation in\nthe context of sound separation. We also formulate spectrogram mask prediction\nusing a set of learned mask bases, which combine using coefficients conditioned\non the output of object segmentation , a design that facilitates separation.\nExtensive experiments on the MUSIC dataset show that our proposed approach\noutperforms state-of-the-art methods on visually guided sound source separation\nand sound denoising.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 10:17:55 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Rahman", "Tanzila", ""], ["Sigal", "Leonid", ""]]}, {"id": "2104.02607", "submitter": "Ziyu Wang", "authors": "Ziyu Wang, Liao Wang, Fuqiang Zhao, Minye Wu, Lan Xu, Jingyi Yu", "title": "MirrorNeRF: One-shot Neural Portrait Radiance Field from Multi-mirror\n  Catadioptric Imaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Photo-realistic neural reconstruction and rendering of the human portrait are\ncritical for numerous VR/AR applications. Still, existing solutions inherently\nrely on multi-view capture settings, and the one-shot solution to get rid of\nthe tedious multi-view synchronization and calibration remains extremely\nchallenging. In this paper, we propose MirrorNeRF - a one-shot neural portrait\nfree-viewpoint rendering approach using a catadioptric imaging system with\nmultiple sphere mirrors and a single high-resolution digital camera, which is\nthe first to combine neural radiance field with catadioptric imaging so as to\nenable one-shot photo-realistic human portrait reconstruction and rendering, in\na low-cost and casual capture setting. More specifically, we propose a\nlight-weight catadioptric system design with a sphere mirror array to enable\ndiverse ray sampling in the continuous 3D space as well as an effective online\ncalibration for the camera and the mirror array. Our catadioptric imaging\nsystem can be easily deployed with a low budget and the casual capture ability\nfor convenient daily usages. We introduce a novel neural warping radiance field\nrepresentation to learn a continuous displacement field that implicitly\ncompensates for the misalignment due to our flexible system setting. We further\npropose a density regularization scheme to leverage the inherent geometry\ninformation from the catadioptric data in a self-supervision manner, which not\nonly improves the training efficiency but also provides more effective density\nsupervision for higher rendering quality. Extensive experiments demonstrate the\neffectiveness and robustness of our scheme to achieve one-shot photo-realistic\nand high-quality appearance free-viewpoint rendering for human portrait scenes.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 15:48:47 GMT"}, {"version": "v2", "created": "Fri, 14 May 2021 05:43:00 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Wang", "Ziyu", ""], ["Wang", "Liao", ""], ["Zhao", "Fuqiang", ""], ["Wu", "Minye", ""], ["Xu", "Lan", ""], ["Yu", "Jingyi", ""]]}, {"id": "2104.02609", "submitter": "Nooshin Mojab", "authors": "Nooshin Mojab, Vahid Noroozi, Abdullah Aleem, Manoj P. Nallabothula,\n  Joseph Baker, Dimitri T. Azar, Mark Rosenblatt, RV Paul Chan, Darvin Yi,\n  Philip S. Yu, Joelle A. Hallak", "title": "I-ODA, Real-World Multi-modal Longitudinal Data for\n  OphthalmicApplications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data from clinical real-world settings is characterized by variability in\nquality, machine-type, setting, and source. One of the primary goals of medical\ncomputer vision is to develop and validate artificial intelligence (AI) based\nalgorithms on real-world data enabling clinical translations. However, despite\nthe exponential growth in AI based applications in healthcare, specifically in\nophthalmology, translations to clinical settings remain challenging. Limited\naccess to adequate and diverse real-world data inhibits the development and\nvalidation of translatable algorithms. In this paper, we present a new\nmulti-modal longitudinal ophthalmic imaging dataset, the Illinois Ophthalmic\nDatabase Atlas (I-ODA), with the goal of advancing state-of-the-art computer\nvision applications in ophthalmology, and improving upon the translatable\ncapacity of AI based applications across different clinical settings. We\npresent the infrastructure employed to collect, annotate, and anonymize images\nfrom multiple sources, demonstrating the complexity of real-world retrospective\ndata and its limitations. I-ODA includes 12 imaging modalities with a total of\n3,668,649 ophthalmic images of 33,876 individuals from the Department of\nOphthalmology and Visual Sciences at the Illinois Eye and Ear Infirmary of the\nUniversity of Illinois Chicago (UIC) over the course of 12 years.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 00:59:25 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Mojab", "Nooshin", ""], ["Noroozi", "Vahid", ""], ["Aleem", "Abdullah", ""], ["Nallabothula", "Manoj P.", ""], ["Baker", "Joseph", ""], ["Azar", "Dimitri T.", ""], ["Rosenblatt", "Mark", ""], ["Chan", "RV Paul", ""], ["Yi", "Darvin", ""], ["Yu", "Philip S.", ""], ["Hallak", "Joelle A.", ""]]}, {"id": "2104.02610", "submitter": "Kaleel Mahmood", "authors": "Kaleel Mahmood, Rigel Mahmood, Marten van Dijk", "title": "On the Robustness of Vision Transformers to Adversarial Examples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in attention-based networks have shown that Vision\nTransformers can achieve state-of-the-art or near state-of-the-art results on\nmany image classification tasks. This puts transformers in the unique position\nof being a promising alternative to traditional convolutional neural networks\n(CNNs). While CNNs have been carefully studied with respect to adversarial\nattacks, the same cannot be said of Vision Transformers. In this paper, we\nstudy the robustness of Vision Transformers to adversarial examples. Our\nanalyses of transformer security is divided into three parts. First, we test\nthe transformer under standard white-box and black-box attacks. Second, we\nstudy the transferability of adversarial examples between CNNs and\ntransformers. We show that adversarial examples do not readily transfer between\nCNNs and transformers. Based on this finding, we analyze the security of a\nsimple ensemble defense of CNNs and transformers. By creating a new attack, the\nself-attention blended gradient attack, we show that such an ensemble is not\nsecure under a white-box adversary. However, under a black-box adversary, we\nshow that an ensemble can achieve unprecedented robustness without sacrificing\nclean accuracy. Our analysis for this work is done using six types of white-box\nattacks and two types of black-box attacks. Our study encompasses multiple\nVision Transformers, Big Transfer Models and CNN architectures trained on\nCIFAR-10, CIFAR-100 and ImageNet.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 00:29:12 GMT"}, {"version": "v2", "created": "Sat, 5 Jun 2021 00:31:29 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Mahmood", "Kaleel", ""], ["Mahmood", "Rigel", ""], ["van Dijk", "Marten", ""]]}, {"id": "2104.02611", "submitter": "Linchao He", "authors": "Linchao He, Mengting Luo, Dejun Zhang, Xiao Yang, Hu Chen and Yi Zhang", "title": "PointShuffleNet: Learning Non-Euclidean Features with Homotopy\n  Equivalence and Mutual Information", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point cloud analysis is still a challenging task due to the disorder and\nsparsity of samplings of their geometric structures from 3D sensors. In this\npaper, we introduce the homotopy equivalence relation (HER) to make the neural\nnetworks learn the data distribution from a high-dimension manifold. A shuffle\noperation is adopted to construct HER for its randomness and zero-parameter. In\naddition, inspired by prior works, we propose a local mutual information\nregularizer (LMIR) to cut off the trivial path that leads to a classification\nerror from HER. LMIR utilizes mutual information to measure the distance\nbetween the original feature and HER transformed feature and learns common\nfeatures in a contrastive learning scheme. Thus, we combine HER and LMIR to\ngive our model the ability to learn non-Euclidean features from a\nhigh-dimension manifold. This is named the non-Euclidean feature learner.\nFurthermore, we propose a new heuristics and efficiency point sampling\nalgorithm named ClusterFPS to obtain approximate uniform sampling but at faster\nspeed. ClusterFPS uses a cluster algorithm to divide a point cloud into several\nclusters and deploy the farthest point sampling algorithm on each cluster in\nparallel. By combining the above methods, we propose a novel point cloud\nanalysis neural network called PointShuffleNet (PSN), which shows great promise\nin point cloud classification and segmentation. Extensive experiments show that\nour PSN achieves state-of-the-art results on ModelNet40, ShapeNet and S3DIS\nwith high efficiency. Theoretically, we provide mathematical analysis toward\nunderstanding of what the data distribution HER has developed and why LMIR can\ndrop the trivial path by maximizing mutual information implicitly.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 03:01:16 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["He", "Linchao", ""], ["Luo", "Mengting", ""], ["Zhang", "Dejun", ""], ["Yang", "Xiao", ""], ["Chen", "Hu", ""], ["Zhang", "Yi", ""]]}, {"id": "2104.02613", "submitter": "Qiang Zhai", "authors": "Qiang Zhai, Xin Li, Fan Yang, Chenglizhao Chen, Hong Cheng, Deng-Ping\n  Fan", "title": "Mutual Graph Learning for Camouflaged Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatically detecting/segmenting object(s) that blend in with their\nsurroundings is difficult for current models. A major challenge is that the\nintrinsic similarities between such foreground objects and background\nsurroundings make the features extracted by deep model indistinguishable. To\novercome this challenge, an ideal model should be able to seek valuable, extra\nclues from the given scene and incorporate them into a joint learning framework\nfor representation co-enhancement. With this inspiration, we design a novel\nMutual Graph Learning (MGL) model, which generalizes the idea of conventional\nmutual learning from regular grids to the graph domain. Specifically, MGL\ndecouples an image into two task-specific feature maps -- one for roughly\nlocating the target and the other for accurately capturing its boundary details\n-- and fully exploits the mutual benefits by recurrently reasoning their\nhigh-order relations through graphs. Importantly, in contrast to most mutual\nlearning approaches that use a shared function to model all between-task\ninteractions, MGL is equipped with typed functions for handling different\ncomplementary relations to maximize information interactions. Experiments on\nchallenging datasets, including CHAMELEON, CAMO and COD10K, demonstrate the\neffectiveness of our MGL with superior performance to existing state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Sat, 3 Apr 2021 10:14:39 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Zhai", "Qiang", ""], ["Li", "Xin", ""], ["Yang", "Fan", ""], ["Chen", "Chenglizhao", ""], ["Cheng", "Hong", ""], ["Fan", "Deng-Ping", ""]]}, {"id": "2104.02615", "submitter": "Adrian W\\\"alchli", "authors": "Adrian W\\\"alchli and Paolo Favaro", "title": "Optical Flow Dataset Synthesis from Unpaired Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The estimation of optical flow is an ambiguous task due to the lack of\ncorrespondence at occlusions, shadows, reflections, lack of texture and changes\nin illumination over time. Thus, unsupervised methods face major challenges as\nthey need to tune complex cost functions with several terms designed to handle\neach of these sources of ambiguity. In contrast, supervised methods avoid these\nchallenges altogether by relying on explicit ground truth optical flow obtained\ndirectly from synthetic or real data. In the case of synthetic data, the ground\ntruth provides an exact and explicit description of what optical flow to assign\nto a given scene. However, the domain gap between synthetic data and real data\noften limits the ability of a trained network to generalize. In the case of\nreal data, the ground truth is obtained through multiple sensors and additional\ndata processing, which might introduce persistent errors and contaminate it. As\na solution to these issues, we introduce a novel method to build a training set\nof pseudo-real images that can be used to train optical flow in a supervised\nmanner. Our dataset uses two unpaired frames from real data and creates pairs\nof frames by simulating random warps, occlusions with super-pixels, shadows and\nillumination changes, and associates them to their corresponding exact optical\nflow. We thus obtain the benefit of directly training on real data while having\naccess to an exact ground truth. Training with our datasets on the Sintel and\nKITTI benchmarks is straightforward and yields models on par or with state of\nthe art performance compared to much more sophisticated training approaches.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 22:19:47 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["W\u00e4lchli", "Adrian", ""], ["Favaro", "Paolo", ""]]}, {"id": "2104.02617", "submitter": "Diego Gragnaniello", "authors": "Diego Gragnaniello, Davide Cozzolino, Francesco Marra, Giovanni Poggi,\n  Luisa Verdoliva", "title": "Are GAN generated images easy to detect? A critical analysis of the\n  state-of-the-art", "comments": "7 pages, 5 figures, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The advent of deep learning has brought a significant improvement in the\nquality of generated media. However, with the increased level of photorealism,\nsynthetic media are becoming hardly distinguishable from real ones, raising\nserious concerns about the spread of fake or manipulated information over the\nInternet. In this context, it is important to develop automated tools to\nreliably and timely detect synthetic media. In this work, we analyze the\nstate-of-the-art methods for the detection of synthetic images, highlighting\nthe key ingredients of the most successful approaches, and comparing their\nperformance over existing generative architectures. We will devote special\nattention to realistic and challenging scenarios, like media uploaded on social\nnetworks or generated by new and unseen architectures, analyzing the impact of\nsuitable augmentation and training strategies on the detectors' generalization\nability.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 15:54:26 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Gragnaniello", "Diego", ""], ["Cozzolino", "Davide", ""], ["Marra", "Francesco", ""], ["Poggi", "Giovanni", ""], ["Verdoliva", "Luisa", ""]]}, {"id": "2104.02628", "submitter": "Yuchao Dai Dr.", "authors": "Aixuan Li and Jing Zhang and Yunqiu Lv and Bowen Liu and Tong Zhang\n  and Yuchao Dai", "title": "Uncertainty-aware Joint Salient Object and Camouflaged Object Detection", "comments": "Accepted to IEEE/CVF Conference on Computer Vision and Pattern\n  Recognition (CVPR) 2021. Aixuan Li and Jing Zhang contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Visual salient object detection (SOD) aims at finding the salient object(s)\nthat attract human attention, while camouflaged object detection (COD) on the\ncontrary intends to discover the camouflaged object(s) that hidden in the\nsurrounding. In this paper, we propose a paradigm of leveraging the\ncontradictory information to enhance the detection ability of both salient\nobject detection and camouflaged object detection. We start by exploiting the\neasy positive samples in the COD dataset to serve as hard positive samples in\nthe SOD task to improve the robustness of the SOD model. Then, we introduce a\nsimilarity measure module to explicitly model the contradicting attributes of\nthese two tasks. Furthermore, considering the uncertainty of labeling in both\ntasks' datasets, we propose an adversarial learning network to achieve both\nhigher order similarity measure and network confidence estimation. Experimental\nresults on benchmark datasets demonstrate that our solution leads to\nstate-of-the-art (SOTA) performance for both tasks.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 16:05:10 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Li", "Aixuan", ""], ["Zhang", "Jing", ""], ["Lv", "Yunqiu", ""], ["Liu", "Bowen", ""], ["Zhang", "Tong", ""], ["Dai", "Yuchao", ""]]}, {"id": "2104.02631", "submitter": "Jack Valmadre", "authors": "Jack Valmadre, Alex Bewley, Jonathan Huang, Chen Sun, Cristian\n  Sminchisescu, Cordelia Schmid", "title": "Local Metrics for Multi-Object Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces temporally local metrics for Multi-Object Tracking.\nThese metrics are obtained by restricting existing metrics based on track\nmatching to a finite temporal horizon, and provide new insight into the ability\nof trackers to maintain identity over time. Moreover, the horizon parameter\noffers a novel, meaningful mechanism by which to define the relative importance\nof detection and association, a common dilemma in applications where imperfect\nassociation is tolerable. It is shown that the historical Average Tracking\nAccuracy (ATA) metric exhibits superior sensitivity to association, enabling\nits proposed local variant, ALTA, to capture a wide range of characteristics.\nIn particular, ALTA is better equipped to identify advances in association\nindependent of detection. The paper further presents an error decomposition for\nATA that reveals the impact of four distinct error types and is equally\napplicable to ALTA. The diagnostic capabilities of ALTA are demonstrated on the\nMOT 2017 and Waymo Open Dataset benchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 16:07:04 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Valmadre", "Jack", ""], ["Bewley", "Alex", ""], ["Huang", "Jonathan", ""], ["Sun", "Chen", ""], ["Sminchisescu", "Cristian", ""], ["Schmid", "Cordelia", ""]]}, {"id": "2104.02633", "submitter": "Francesco Barbato", "authors": "Francesco Barbato, Marco Toldo, Umberto Michieli, Pietro Zanuttigh", "title": "Latent Space Regularization for Unsupervised Domain Adaptation in\n  Semantic Segmentation", "comments": "Accepted at CVPR-WAD 2021, 11 pages, 7 figures, 1 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks for semantic segmentation achieve\noutstanding accuracy, however they also have a couple of major drawbacks:\nfirst, they do not generalize well to distributions slightly different from the\none of the training data; second, they require a huge amount of labeled data\nfor their optimization. In this paper, we introduce feature-level space-shaping\nregularization strategies to reduce the domain discrepancy in semantic\nsegmentation. In particular, for this purpose we jointly enforce a clustering\nobjective, a perpendicularity constraint and a norm alignment goal on the\nfeature vectors corresponding to source and target samples. Additionally, we\npropose a novel measure able to capture the relative efficacy of an adaptation\nstrategy compared to supervised training. We verify the effectiveness of such\nmethods in the autonomous driving setting achieving state-of-the-art results in\nmultiple synthetic-to-real road scenes benchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 16:07:22 GMT"}, {"version": "v2", "created": "Wed, 21 Apr 2021 05:25:31 GMT"}, {"version": "v3", "created": "Wed, 7 Jul 2021 11:43:45 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Barbato", "Francesco", ""], ["Toldo", "Marco", ""], ["Michieli", "Umberto", ""], ["Zanuttigh", "Pietro", ""]]}, {"id": "2104.02638", "submitter": "Neil Houlsby", "authors": "Vincent Dumoulin, Neil Houlsby, Utku Evci, Xiaohua Zhai, Ross\n  Goroshin, Sylvain Gelly, Hugo Larochelle", "title": "Comparing Transfer and Meta Learning Approaches on a Unified Few-Shot\n  Classification Benchmark", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Meta and transfer learning are two successful families of approaches to\nfew-shot learning. Despite highly related goals, state-of-the-art advances in\neach family are measured largely in isolation of each other. As a result of\ndiverging evaluation norms, a direct or thorough comparison of different\napproaches is challenging. To bridge this gap, we perform a cross-family study\nof the best transfer and meta learners on both a large-scale meta-learning\nbenchmark (Meta-Dataset, MD), and a transfer learning benchmark (Visual Task\nAdaptation Benchmark, VTAB). We find that, on average, large-scale transfer\nmethods (Big Transfer, BiT) outperform competing approaches on MD, even when\ntrained only on ImageNet. In contrast, meta-learning approaches struggle to\ncompete on VTAB when trained and validated on MD. However, BiT is not without\nlimitations, and pushing for scale does not improve performance on highly\nout-of-distribution MD tasks. In performing this study, we reveal a number of\ndiscrepancies in evaluation norms and study some of these in light of the\nperformance gap. We hope that this work facilitates sharing of insights from\neach community, and accelerates progress on few-shot learning.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 16:17:51 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Dumoulin", "Vincent", ""], ["Houlsby", "Neil", ""], ["Evci", "Utku", ""], ["Zhai", "Xiaohua", ""], ["Goroshin", "Ross", ""], ["Gelly", "Sylvain", ""], ["Larochelle", "Hugo", ""]]}, {"id": "2104.02646", "submitter": "Krishna Murthy Jatavallabhula", "authors": "Krishna Murthy Jatavallabhula and Miles Macklin and Florian Golemo and\n  Vikram Voleti and Linda Petrini and Martin Weiss and Breandan Considine and\n  Jerome Parent-Levesque and Kevin Xie and Kenny Erleben and Liam Paull and\n  Florian Shkurti and Derek Nowrouzezahrai and Sanja Fidler", "title": "gradSim: Differentiable simulation for system identification and\n  visuomotor control", "comments": "ICLR 2021. Project page (and a dynamic web version of the article):\n  https://gradsim.github.io", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.RO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We consider the problem of estimating an object's physical properties such as\nmass, friction, and elasticity directly from video sequences. Such a system\nidentification problem is fundamentally ill-posed due to the loss of\ninformation during image formation. Current solutions require precise 3D labels\nwhich are labor-intensive to gather, and infeasible to create for many systems\nsuch as deformable solids or cloth. We present gradSim, a framework that\novercomes the dependence on 3D supervision by leveraging differentiable\nmultiphysics simulation and differentiable rendering to jointly model the\nevolution of scene dynamics and image formation. This novel combination enables\nbackpropagation from pixels in a video sequence through to the underlying\nphysical attributes that generated them. Moreover, our unified computation\ngraph -- spanning from the dynamics and through the rendering process --\nenables learning in challenging visuomotor control tasks, without relying on\nstate-based (3D) supervision, while obtaining performance competitive to or\nbetter than techniques that rely on precise 3D labels.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 16:32:01 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Jatavallabhula", "Krishna Murthy", ""], ["Macklin", "Miles", ""], ["Golemo", "Florian", ""], ["Voleti", "Vikram", ""], ["Petrini", "Linda", ""], ["Weiss", "Martin", ""], ["Considine", "Breandan", ""], ["Parent-Levesque", "Jerome", ""], ["Xie", "Kevin", ""], ["Erleben", "Kenny", ""], ["Paull", "Liam", ""], ["Shkurti", "Florian", ""], ["Nowrouzezahrai", "Derek", ""], ["Fidler", "Sanja", ""]]}, {"id": "2104.02651", "submitter": "Erico Tjoa", "authors": "Erico Tjoa", "title": "A Modified Convolutional Network for Auto-encoding based on Pattern\n  Theory Growth Function", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This brief paper reports the shortcoming of a variant of convolutional neural\nnetwork whose components are developed based on the pattern theory framework.\n", "versions": [{"version": "v1", "created": "Sun, 4 Apr 2021 04:23:36 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Tjoa", "Erico", ""]]}, {"id": "2104.02652", "submitter": "Meng Xia", "authors": "Meng Xia, Meenal K. Kheterpal, Samantha C. Wong, Christine Park,\n  William Ratliff, Lawrence Carin, Ricardo Henao", "title": "Malignancy Prediction and Lesion Identification from Clinical\n  Dermatological Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider machine-learning-based malignancy prediction and lesion\nidentification from clinical dermatological images, which can be indistinctly\nacquired via smartphone or dermoscopy capture. Additionally, we do not assume\nthat images contain single lesions, thus the framework supports both focal or\nwide-field images. Specifically, we propose a two-stage approach in which we\nfirst identify all lesions present in the image regardless of sub-type or\nlikelihood of malignancy, then it estimates their likelihood of malignancy, and\nthrough aggregation, it also generates an image-level likelihood of malignancy\nthat can be used for high-level screening processes. Further, we consider\naugmenting the proposed approach with clinical covariates (from electronic\nhealth records) and publicly available data (the ISIC dataset). Comprehensive\nexperiments validated on an independent test dataset demonstrate that i) the\nproposed approach outperforms alternative model architectures; ii) the model\nbased on images outperforms a pure clinical model by a large margin, and the\ncombination of images and clinical data does not significantly improves over\nthe image-only model; and iii) the proposed framework offers comparable\nperformance in terms of malignancy classification relative to three board\ncertified dermatologists with different levels of experience.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 20:52:05 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Xia", "Meng", ""], ["Kheterpal", "Meenal K.", ""], ["Wong", "Samantha C.", ""], ["Park", "Christine", ""], ["Ratliff", "William", ""], ["Carin", "Lawrence", ""], ["Henao", "Ricardo", ""]]}, {"id": "2104.02653", "submitter": "Ad\\'in Ram\\'irez Rivera", "authors": "Miguel Rodr\\'iguez Santander, Juan Hern\\'andez Albarrac\\'in, Ad\\'in\n  Ram\\'irez Rivera", "title": "On the Pitfalls of Learning with Limited Data: A Facial Expression\n  Recognition Case Study", "comments": "To appear in Expert Systems with Applications", "journal-ref": "Expert Syst. Appl. 2021, 18 (1) 114991", "doi": "10.1016/j.eswa.2021.114991", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Deep learning models need large amounts of data for training. In video\nrecognition and classification, significant advances were achieved with the\nintroduction of new large databases. However, the creation of large-databases\nfor training is infeasible in several scenarios. Thus, existing or small\ncollected databases are typically joined and amplified to train these models.\nNevertheless, training neural networks on limited data is not straightforward\nand comes with a set of problems. In this paper, we explore the effects of\nstacking databases, model initialization, and data amplification techniques\nwhen training with limited data on deep learning models' performance. We\nfocused on the problem of Facial Expression Recognition from videos. We\nperformed an extensive study with four databases at a different complexity and\nnine deep-learning architectures for video classification. We found that (i)\ncomplex training sets translate better to more stable test sets when trained\nwith transfer learning and synthetically generated data, but their performance\nyields a high variance; (ii) training with more detailed data translates to\nmore stable performance on novel scenarios (albeit with lower performance);\n(iii) merging heterogeneous data is not a straightforward improvement, as the\ntype of augmentation and initialization is crucial; (iv) classical data\naugmentation cannot fill the holes created by joining largely separated\ndatasets; and (v) inductive biases help to bridge the gap when paired with\nsynthetic data, but this data is not enough when working with standard\ninitialization techniques.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 18:53:41 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Santander", "Miguel Rodr\u00edguez", ""], ["Albarrac\u00edn", "Juan Hern\u00e1ndez", ""], ["Rivera", "Ad\u00edn Ram\u00edrez", ""]]}, {"id": "2104.02655", "submitter": "Tao Li", "authors": "Tao Li and Min Soo Choi", "title": "DeepBlur: A Simple and Effective Method for Natural Image Obfuscation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a growing privacy concern due to the popularity of social media and\nsurveillance systems, along with advances in face recognition software.\nHowever, established image obfuscation techniques are either vulnerable to\nre-identification attacks by human or deep learning models, insufficient in\npreserving image fidelity, or too computationally intensive to be practical. To\ntackle these issues, we present DeepBlur, a simple yet effective method for\nimage obfuscation by blurring in the latent space of an unconditionally\npre-trained generative model that is able to synthesize photo-realistic facial\nimages. We compare it with existing methods by efficiency and image quality,\nand evaluate against both state-of-the-art deep learning models and industrial\nproducts (e.g., Face++, Microsoft face service). Experiments show that our\nmethod produces high quality outputs and is the strongest defense for most test\ncases.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 19:31:26 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Li", "Tao", ""], ["Choi", "Min Soo", ""]]}, {"id": "2104.02656", "submitter": "Vinod Kumar Kurmi", "authors": "Vinod K Kurmi, Vipul Bajaj, Badri N Patro, K S Venkatesh, Vinay P\n  Namboodiri, Preethi Jyothi", "title": "Collaborative Learning to Generate Audio-Video Jointly", "comments": "ICASSP 2021 (Accepted)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR cs.MM cs.SD eess.AS eess.IV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  There have been a number of techniques that have demonstrated the generation\nof multimedia data for one modality at a time using GANs, such as the ability\nto generate images, videos, and audio. However, so far, the task of multi-modal\ngeneration of data, specifically for audio and videos both, has not been\nsufficiently well-explored. Towards this, we propose a method that demonstrates\nthat we are able to generate naturalistic samples of video and audio data by\nthe joint correlated generation of audio and video modalities. The proposed\nmethod uses multiple discriminators to ensure that the audio, video, and the\njoint output are also indistinguishable from real-world samples. We present a\ndataset for this task and show that we are able to generate realistic samples.\nThis method is validated using various standard metrics such as Inception\nScore, Frechet Inception Distance (FID) and through human evaluation.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 01:00:51 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Kurmi", "Vinod K", ""], ["Bajaj", "Vipul", ""], ["Patro", "Badri N", ""], ["Venkatesh", "K S", ""], ["Namboodiri", "Vinay P", ""], ["Jyothi", "Preethi", ""]]}, {"id": "2104.02663", "submitter": "Mohammad Saeed Rad", "authors": "Mohammad Saeed Rad, Thomas Yu, Behzad Bozorgtabar, Jean-Philippe\n  Thiran", "title": "Test-Time Adaptation for Super-Resolution: You Only Need to Overfit on a\n  Few More Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing reference (RF)-based super-resolution (SR) models try to improve\nperceptual quality in SR under the assumption of the availability of\nhigh-resolution RF images paired with low-resolution (LR) inputs at testing. As\nthe RF images should be similar in terms of content, colors, contrast, etc. to\nthe test image, this hinders the applicability in a real scenario. Other\napproaches to increase the perceptual quality of images, including perceptual\nloss and adversarial losses, tend to dramatically decrease fidelity to the\nground-truth through significant decreases in PSNR/SSIM. Addressing both\nissues, we propose a simple yet universal approach to improve the perceptual\nquality of the HR prediction from a pre-trained SR network on a given LR input\nby further fine-tuning the SR network on a subset of images from the training\ndataset with similar patterns of activation as the initial HR prediction, with\nrespect to the filters of a feature extractor. In particular, we show the\neffects of fine-tuning on these images in terms of the perceptual quality and\nPSNR/SSIM values. Contrary to perceptually driven approaches, we demonstrate\nthat the fine-tuned network produces a HR prediction with both greater\nperceptual quality and minimal changes to the PSNR/SSIM with respect to the\ninitial HR prediction. Further, we present novel numerical experiments\nconcerning the filters of SR networks, where we show through filter\ncorrelation, that the filters of the fine-tuned network from our method are\ncloser to \"ideal\" filters, than those of the baseline network or a network\nfine-tuned on random images.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 16:50:52 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Rad", "Mohammad Saeed", ""], ["Yu", "Thomas", ""], ["Bozorgtabar", "Behzad", ""], ["Thiran", "Jean-Philippe", ""]]}, {"id": "2104.02680", "submitter": "Benjamin McLaughlin", "authors": "Benjamin McLaughlin, Sung Ha Kang", "title": "A New Parallel Adaptive Clustering and its Application to Streaming Data", "comments": "This work was funded by NAVSEA. Distribution Statement A: Approved\n  for Public Release, Distribution is Unlimited", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a parallel adaptive clustering (PAC) algorithm to\nautomatically classify data while simultaneously choosing a suitable number of\nclasses. Clustering is an important tool for data analysis and understanding in\na broad set of areas including data reduction, pattern analysis, and\nclassification. However, the requirement to specify the number of clusters in\nadvance and the computational burden associated with clustering large sets of\ndata persist as challenges in clustering. We propose a new parallel adaptive\nclustering (PAC) algorithm that addresses these challenges by adaptively\ncomputing the number of clusters and leveraging the power of parallel\ncomputing. The algorithm clusters disjoint subsets of the data on parallel\ncomputation threads. We develop regularized set \\mi{k}-means to efficiently\ncluster the results from the parallel threads. A refinement step further\nimproves the clusters. The PAC algorithm offers the capability to adaptively\ncluster data sets which change over time by reusing the information from\nprevious time steps to decrease computation. We provide theoretical analysis\nand numerical experiments to characterize the performance of the method,\nvalidate its properties, and demonstrate the computational efficiency of the\nmethod.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 17:18:56 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["McLaughlin", "Benjamin", ""], ["Kang", "Sung Ha", ""]]}, {"id": "2104.02687", "submitter": "Medhini Narasimhan", "authors": "Medhini Narasimhan, Shiry Ginosar, Andrew Owens, Alexei A. Efros,\n  Trevor Darrell", "title": "Strumming to the Beat: Audio-Conditioned Contrastive Video Textures", "comments": "Project website at https://medhini.github.io/audio_video_textures/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce a non-parametric approach for infinite video texture synthesis\nusing a representation learned via contrastive learning. We take inspiration\nfrom Video Textures, which showed that plausible new videos could be generated\nfrom a single one by stitching its frames together in a novel yet consistent\norder. This classic work, however, was constrained by its use of hand-designed\ndistance metrics, limiting its use to simple, repetitive videos. We draw on\nrecent techniques from self-supervised learning to learn this distance metric,\nallowing us to compare frames in a manner that scales to more challenging\ndynamics, and to condition on other data, such as audio. We learn\nrepresentations for video frames and frame-to-frame transition probabilities by\nfitting a video-specific model trained using contrastive learning. To\nsynthesize a texture, we randomly sample frames with high transition\nprobabilities to generate diverse temporally smooth videos with novel sequences\nand transitions. The model naturally extends to an audio-conditioned setting\nwithout requiring any finetuning. Our model outperforms baselines on human\nperceptual scores, can handle a diverse range of input videos, and can combine\nsemantic and audio-visual cues in order to synthesize videos that synchronize\nwell with an audio signal.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 17:24:57 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Narasimhan", "Medhini", ""], ["Ginosar", "Shiry", ""], ["Owens", "Andrew", ""], ["Efros", "Alexei A.", ""], ["Darrell", "Trevor", ""]]}, {"id": "2104.02691", "submitter": "Honglie Chen", "authors": "Honglie Chen, Weidi Xie, Triantafyllos Afouras, Arsha Nagrani, Andrea\n  Vedaldi, Andrew Zisserman", "title": "Localizing Visual Sounds the Hard Way", "comments": "CVPR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.AS eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The objective of this work is to localize sound sources that are visible in a\nvideo without using manual annotations. Our key technical contribution is to\nshow that, by training the network to explicitly discriminate challenging image\nfragments, even for images that do contain the object emitting the sound, we\ncan significantly boost the localization performance. We do so elegantly by\nintroducing a mechanism to mine hard samples and add them to a contrastive\nlearning formulation automatically. We show that our algorithm achieves\nstate-of-the-art performance on the popular Flickr SoundNet dataset.\nFurthermore, we introduce the VGG-Sound Source (VGG-SS) benchmark, a new set of\nannotations for the recently-introduced VGG-Sound dataset, where the sound\nsources visible in each video clip are explicitly marked with bounding box\nannotations. This dataset is 20 times larger than analogous existing ones,\ncontains 5K videos spanning over 200 categories, and, differently from Flickr\nSoundNet, is video-based. On VGG-SS, we also show that our algorithm achieves\nstate-of-the-art performance against several baselines.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 17:38:18 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Chen", "Honglie", ""], ["Xie", "Weidi", ""], ["Afouras", "Triantafyllos", ""], ["Nagrani", "Arsha", ""], ["Vedaldi", "Andrea", ""], ["Zisserman", "Andrew", ""]]}, {"id": "2104.02699", "submitter": "Yuval Alaluf", "authors": "Yuval Alaluf, Or Patashnik, Daniel Cohen-Or", "title": "ReStyle: A Residual-Based StyleGAN Encoder via Iterative Refinement", "comments": "Project page available at\n  https://yuval-alaluf.github.io/restyle-encoder/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the power of unconditional image synthesis has significantly\nadvanced through the use of Generative Adversarial Networks (GANs). The task of\ninverting an image into its corresponding latent code of the trained GAN is of\nutmost importance as it allows for the manipulation of real images, leveraging\nthe rich semantics learned by the network. Recognizing the limitations of\ncurrent inversion approaches, in this work we present a novel inversion scheme\nthat extends current encoder-based inversion methods by introducing an\niterative refinement mechanism. Instead of directly predicting the latent code\nof a given real image using a single pass, the encoder is tasked with\npredicting a residual with respect to the current estimate of the inverted\nlatent code in a self-correcting manner. Our residual-based encoder, named\nReStyle, attains improved accuracy compared to current state-of-the-art\nencoder-based methods with a negligible increase in inference time. We analyze\nthe behavior of ReStyle to gain valuable insights into its iterative nature. We\nthen evaluate the performance of our residual encoder and analyze its\nrobustness compared to optimization-based inversion and state-of-the-art\nencoders.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 17:47:13 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Alaluf", "Yuval", ""], ["Patashnik", "Or", ""], ["Cohen-Or", "Daniel", ""]]}, {"id": "2104.02703", "submitter": "Tong Wu", "authors": "Tong Wu, Ziwei Liu, Qingqiu Huang, Yu Wang and Dahua Lin", "title": "Adversarial Robustness under Long-Tailed Distribution", "comments": "Accepted to CVPR 2021 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial robustness has attracted extensive studies recently by revealing\nthe vulnerability and intrinsic characteristics of deep networks. However,\nexisting works on adversarial robustness mainly focus on balanced datasets,\nwhile real-world data usually exhibits a long-tailed distribution. To push\nadversarial robustness towards more realistic scenarios, in this work we\ninvestigate the adversarial vulnerability as well as defense under long-tailed\ndistributions. In particular, we first reveal the negative impacts induced by\nimbalanced data on both recognition performance and adversarial robustness,\nuncovering the intrinsic challenges of this problem. We then perform a\nsystematic study on existing long-tailed recognition methods in conjunction\nwith the adversarial training framework. Several valuable observations are\nobtained: 1) natural accuracy is relatively easy to improve, 2) fake gain of\nrobust accuracy exists under unreliable evaluation, and 3) boundary error\nlimits the promotion of robustness. Inspired by these observations, we propose\na clean yet effective framework, RoBal, which consists of two dedicated\nmodules, a scale-invariant classifier and data re-balancing via both margin\nengineering at training stage and boundary adjustment during inference.\nExtensive experiments demonstrate the superiority of our approach over other\nstate-of-the-art defense methods. To our best knowledge, we are the first to\ntackle adversarial robustness under long-tailed distributions, which we believe\nwould be a significant step towards real-world robustness. Our code is\navailable at: https://github.com/wutong16/Adversarial_Long-Tail .\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 17:53:08 GMT"}, {"version": "v2", "created": "Mon, 14 Jun 2021 12:22:54 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Wu", "Tong", ""], ["Liu", "Ziwei", ""], ["Huang", "Qingqiu", ""], ["Wang", "Yu", ""], ["Lin", "Dahua", ""]]}, {"id": "2104.02710", "submitter": "Jennifer J. Sun", "authors": "Jennifer J. Sun, Tomomi Karigo, Dipam Chakraborty, Sharada P. Mohanty,\n  Benjamin Wild, Quan Sun, Chen Chen, David J. Anderson, Pietro Perona, Yisong\n  Yue, Ann Kennedy", "title": "The Multi-Agent Behavior Dataset: Mouse Dyadic Social Interactions", "comments": "Dataset: https://data.caltech.edu/records/1991, Website:\n  https://sites.google.com/view/computational-behavior/calms21-dataset", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-agent behavior modeling aims to understand the interactions that occur\nbetween agents. We present a multi-agent dataset from behavioral neuroscience,\nthe Caltech Mouse Social Interactions (CalMS21) Dataset. Our dataset consists\nof trajectory data of social interactions, recorded from videos of freely\nbehaving mice in a standard resident-intruder assay. To help accelerate\nbehavioral studies, the CalMS21 dataset provides benchmarks to evaluate the\nperformance of automated behavior classification methods in three settings: (1)\nfor training on large behavioral datasets all annotated by a single annotator,\n(2) for style transfer to learn inter-annotator differences in behavior\ndefinitions, and (3) for learning of new behaviors of interest given limited\ntraining data. The dataset consists of 6 million frames of unlabeled tracked\nposes of interacting mice, as well as over 1 million frames with tracked poses\nand corresponding frame-level behavior annotations. The challenge of our\ndataset is to be able to classify behaviors accurately using both labeled and\nunlabeled tracking data, as well as being able to generalize to new settings.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 17:58:47 GMT"}, {"version": "v2", "created": "Wed, 7 Apr 2021 16:16:29 GMT"}, {"version": "v3", "created": "Thu, 10 Jun 2021 19:56:59 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Sun", "Jennifer J.", ""], ["Karigo", "Tomomi", ""], ["Chakraborty", "Dipam", ""], ["Mohanty", "Sharada P.", ""], ["Wild", "Benjamin", ""], ["Sun", "Quan", ""], ["Chen", "Chen", ""], ["Anderson", "David J.", ""], ["Perona", "Pietro", ""], ["Yue", "Yisong", ""], ["Kennedy", "Ann", ""]]}, {"id": "2104.02735", "submitter": "Berthy Feng", "authors": "Berthy Feng, Alexander C. Ogren, Chiara Daraio, Katherine L. Bouman", "title": "Visual Vibration Tomography: Estimating Interior Material Properties\n  from Monocular Video", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An object's interior material properties, while invisible to the human eye,\ndetermine motion observed on its surface. We propose an approach that estimates\nheterogeneous material properties of an object directly from a monocular video\nof its surface vibrations. Specifically, we estimate Young's modulus and\ndensity throughout a 3D object with known geometry. Knowledge of how these\nvalues change across the object is useful for characterizing defects and\nsimulating how the object will interact with different environments.\nTraditional non-destructive testing approaches, which generally estimate\nhomogenized material properties or the presence of defects, are expensive and\nuse specialized instruments. We propose an approach that leverages monocular\nvideo to (1) measure and object's sub-pixel motion and decompose this motion\ninto image-space modes, and (2) directly infer spatially-varying Young's\nmodulus and density values from the observed image-space modes. On both\nsimulated and real videos, we demonstrate that our approach is able to image\nmaterial properties simply by analyzing surface motion. In particular, our\nmethod allows us to identify unseen defects on a 2D drum head from real,\nhigh-speed video.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 18:05:27 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Feng", "Berthy", ""], ["Ogren", "Alexander C.", ""], ["Daraio", "Chiara", ""], ["Bouman", "Katherine L.", ""]]}, {"id": "2104.02745", "submitter": "Shubhankar Mangesh Borse", "authors": "Shubhankar Borse, Ying Wang, Yizhe Zhang, Fatih Porikli", "title": "InverseForm: A Loss Function for Structured Boundary-Aware Segmentation", "comments": "Accepted to CVPR 2021 as an oral presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a novel boundary-aware loss term for semantic segmentation using\nan inverse-transformation network, which efficiently learns the degree of\nparametric transformations between estimated and target boundaries. This\nplug-in loss term complements the cross-entropy loss in capturing boundary\ntransformations and allows consistent and significant performance improvement\non segmentation backbone models without increasing their size and computational\ncomplexity. We analyze the quantitative and qualitative effects of our loss\nfunction on three indoor and outdoor segmentation benchmarks, including\nCityscapes, NYU-Depth-v2, and PASCAL, integrating it into the training phase of\nseveral backbone networks in both single-task and multi-task settings. Our\nextensive experiments show that the proposed method consistently outperforms\nbaselines, and even sets the new state-of-the-art on two datasets.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 18:52:45 GMT"}, {"version": "v2", "created": "Thu, 8 Apr 2021 01:19:22 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Borse", "Shubhankar", ""], ["Wang", "Ying", ""], ["Zhang", "Yizhe", ""], ["Porikli", "Fatih", ""]]}, {"id": "2104.02749", "submitter": "Pranjal Singh Rajput", "authors": "Pranjal Singh Rajput, Yeshwanth Napolean, Jan van Gemert", "title": "Heuristics2Annotate: Efficient Annotation of Large-Scale Marathon\n  Dataset For Bounding Box Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Annotating a large-scale in-the-wild person re-identification dataset\nespecially of marathon runners is a challenging task. The variations in the\nscenarios such as camera viewpoints, resolution, occlusion, and illumination\nmake the problem non-trivial. Manually annotating bounding boxes in such\nlarge-scale datasets is cost-inefficient. Additionally, due to crowdedness and\nocclusion in the videos, aligning the identity of runners across multiple\ndisjoint cameras is a challenge. We collected a novel large-scale in-the-wild\nvideo dataset of marathon runners. The dataset consists of hours of recording\nof thousands of runners captured using 42 hand-held smartphone cameras and\ncovering real-world scenarios. Due to the presence of crowdedness and occlusion\nin the videos, the annotation of runners becomes a challenging task. We propose\na new scheme for tackling the challenges in the annotation of such large\ndataset. Our technique reduces the overall cost of annotation in terms of time\nas well as budget. We demonstrate performing fps analysis to reduce the effort\nand time of annotation. We investigate several annotation methods for\nefficiently generating tight bounding boxes. Our results prove that\ninterpolating bounding boxes between keyframes is the most efficient method of\nbounding box generation amongst several other methods and is 3x times faster\nthan the naive baseline method. We introduce a novel way of aligning the\nidentity of runners in disjoint cameras. Our inter-camera alignment tool\nintegrated with the state-of-the-art person re-id system proves to be\nsufficient and effective in the alignment of the runners across multiple\ncameras with non-overlapping views. Our proposed framework of annotation\nreduces the annotation cost of the dataset by a factor of 16x, also effectively\naligning 93.64% of the runners in the cross-camera setting.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 19:08:31 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Rajput", "Pranjal Singh", ""], ["Napolean", "Yeshwanth", ""], ["van Gemert", "Jan", ""]]}, {"id": "2104.02761", "submitter": "Victor Amblard", "authors": "Victor Amblard, Timothy P. Osedach, Arnaud Croux, Andrew Speck and\n  John J. Leonard", "title": "Lidar-Monocular Surface Reconstruction Using Line Segments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structure from Motion (SfM) often fails to estimate accurate poses in\nenvironments that lack suitable visual features. In such cases, the quality of\nthe final 3D mesh, which is contingent on the accuracy of those estimates, is\nreduced. One way to overcome this problem is to combine data from a monocular\ncamera with that of a LIDAR. This allows fine details and texture to be\ncaptured while still accurately representing featureless subjects. However,\nfusing these two sensor modalities is challenging due to their fundamentally\ndifferent characteristics. Rather than directly fusing image features and LIDAR\npoints, we propose to leverage common geometric features that are detected in\nboth the LIDAR scans and image data, allowing data from the two sensors to be\nprocessed in a higher-level space. In particular, we propose to find\ncorrespondences between 3D lines extracted from LIDAR scans and 2D lines\ndetected in images before performing a bundle adjustment to refine poses. We\nalso exploit the detected and optimized line segments to improve the quality of\nthe final mesh. We test our approach on the recently published dataset, Newer\nCollege Dataset. We compare the accuracy and the completeness of the 3D mesh to\na ground truth obtained with a survey-grade 3D scanner. We show that our method\ndelivers results that are comparable to a state-of-the-art LIDAR survey while\nnot requiring highly accurate ground truth pose estimates.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 19:49:53 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Amblard", "Victor", ""], ["Osedach", "Timothy P.", ""], ["Croux", "Arnaud", ""], ["Speck", "Andrew", ""], ["Leonard", "John J.", ""]]}, {"id": "2104.02768", "submitter": "Jacob Pfau", "authors": "Jacob Pfau, Albert T. Young, Jerome Wei, Maria L. Wei, Michael J.\n  Keiser", "title": "Robust Semantic Interpretability: Revisiting Concept Activation Vectors", "comments": "ICML WHI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interpretability methods for image classification assess model\ntrustworthiness by attempting to expose whether the model is systematically\nbiased or attending to the same cues as a human would. Saliency methods for\nfeature attribution dominate the interpretability literature, but these methods\ndo not address semantic concepts such as the textures, colors, or genders of\nobjects within an image. Our proposed Robust Concept Activation Vectors (RCAV)\nquantifies the effects of semantic concepts on individual model predictions and\non model behavior as a whole. RCAV calculates a concept gradient and takes a\ngradient ascent step to assess model sensitivity to the given concept. By\ngeneralizing previous work on concept activation vectors to account for model\nnon-linearity, and by introducing stricter hypothesis testing, we show that\nRCAV yields interpretations which are both more accurate at the image level and\nrobust at the dataset level. RCAV, like saliency methods, supports the\ninterpretation of individual predictions. To evaluate the practical use of\ninterpretability methods as debugging tools, and the scientific use of\ninterpretability methods for identifying inductive biases (e.g. texture over\nshape), we construct two datasets and accompanying metrics for realistic\nbenchmarking of semantic interpretability methods. Our benchmarks expose the\nimportance of counterfactual augmentation and negative controls for quantifying\nthe practical usability of interpretability methods.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 20:14:59 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Pfau", "Jacob", ""], ["Young", "Albert T.", ""], ["Wei", "Jerome", ""], ["Wei", "Maria L.", ""], ["Keiser", "Michael J.", ""]]}, {"id": "2104.02773", "submitter": "Loc Huynh", "authors": "Loc Huynh, Bipin Kishore, Paul Debevec", "title": "A New Dimension in Testimony: Relighting Video with Reflectance Field\n  Exemplars", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a learning-based method for estimating 4D reflectance field of a\nperson given video footage illuminated under a flat-lit environment of the same\nsubject. For training data, we use one light at a time to illuminate the\nsubject and capture the reflectance field data in a variety of poses and\nviewpoints. We estimate the lighting environment of the input video footage and\nuse the subject's reflectance field to create synthetic images of the subject\nilluminated by the input lighting environment. We then train a deep\nconvolutional neural network to regress the reflectance field from the\nsynthetic images. We also use a differentiable renderer to provide feedback for\nthe network by matching the relit images with the input video frames. This\nsemi-supervised training scheme allows the neural network to handle unseen\nposes in the dataset as well as compensate for the lighting estimation error.\nWe evaluate our method on the video footage of the real Holocaust survivors and\nshow that our method outperforms the state-of-the-art methods in both realism\nand speed.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 20:29:06 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Huynh", "Loc", ""], ["Kishore", "Bipin", ""], ["Debevec", "Paul", ""]]}, {"id": "2104.02775", "submitter": "Jiyoung Lee", "authors": "Jiyoung Lee, Soo-Whan Chung, Sunok Kim, Hong-Goo Kang, Kwanghoon Sohn", "title": "Looking into Your Speech: Learning Cross-modal Affinity for Audio-visual\n  Speech Separation", "comments": "CVPR 2021. The first two authors contributed equally to this work.\n  Project page: https://caffnet.github.io", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.SD eess.AS eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the problem of separating individual speech signals\nfrom videos using audio-visual neural processing. Most conventional approaches\nutilize frame-wise matching criteria to extract shared information between\nco-occurring audio and video. Thus, their performance heavily depends on the\naccuracy of audio-visual synchronization and the effectiveness of their\nrepresentations. To overcome the frame discontinuity problem between two\nmodalities due to transmission delay mismatch or jitter, we propose a\ncross-modal affinity network (CaffNet) that learns global correspondence as\nwell as locally-varying affinities between audio and visual streams. Given that\nthe global term provides stability over a temporal sequence at the\nutterance-level, this resolves the label permutation problem characterized by\ninconsistent assignments. By extending the proposed cross-modal affinity on the\ncomplex network, we further improve the separation performance in the complex\nspectral domain. Experimental results verify that the proposed methods\noutperform conventional ones on various datasets, demonstrating their\nadvantages in real-world scenarios.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 15:39:12 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Lee", "Jiyoung", ""], ["Chung", "Soo-Whan", ""], ["Kim", "Sunok", ""], ["Kang", "Hong-Goo", ""], ["Sohn", "Kwanghoon", ""]]}, {"id": "2104.02785", "submitter": "Sara Zahedian", "authors": "Sara Zahedian, Kaveh Farokhi Sadabadi, Amir Nohekhan", "title": "Localization of Autonomous Vehicles: Proof of Concept for A Computer\n  Vision Approach", "comments": "2019 ITS America Annual Meeting", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper introduces a visual-based localization method for autonomous\nvehicles (AVs) that operate in the absence of any complicated hardware system\nbut a single camera. Visual localization refers to techniques that aim to find\nthe location of an object based on visual information of its surrounding area.\nThe problem of localization has been of interest for many years. However,\nvisual localization is a relatively new subject in the literature of\ntransportation. Moreover, the inevitable application of this type of\nlocalization in the context of autonomous vehicles demands special attention\nfrom the transportation community to this problem. This study proposes a\ntwo-step localization method that requires a database of geotagged images and a\ncamera mounted on a vehicle that can take pictures while the car is moving. The\nfirst step which is image retrieval uses SIFT local feature descriptor to find\nan initial location for the vehicle using image matching. The next step is to\nutilize the Kalman filter to estimate a more accurate location for the vehicle\nas it is moving. All stages of the introduced method are implemented as a\ncomplete system using different Python libraries. The proposed system is tested\non the KITTI dataset and has shown an average accuracy of 2 meters in finding\nthe final location of the vehicle.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 21:09:47 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Zahedian", "Sara", ""], ["Sadabadi", "Kaveh Farokhi", ""], ["Nohekhan", "Amir", ""]]}, {"id": "2104.02793", "submitter": "Asmaa Haja", "authors": "Asmaa Haja and Lambert R.B. Schomaker", "title": "A fully automated end-to-end process for fluorescence microscopy images\n  of yeast cells: From segmentation to detection and classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, an enormous amount of fluorescence microscopy images were\ncollected in high-throughput lab settings. Analyzing and extracting relevant\ninformation from all images in a short time is almost impossible. Detecting\ntiny individual cell compartments is one of many challenges faced by\nbiologists. This paper aims at solving this problem by building an end-to-end\nprocess that employs methods from the deep learning field to automatically\nsegment, detect and classify cell compartments of fluorescence microscopy\nimages of yeast cells. With this intention we used Mask R-CNN to automatically\nsegment and label a large amount of yeast cell data, and YOLOv4 to\nautomatically detect and classify individual yeast cell compartments from these\nimages. This fully automated end-to-end process is intended to be integrated\ninto an interactive e-Science server in the PerICo1 project, which can be used\nby biologists with minimized human effort in training and operation to complete\ntheir various classification tasks. In addition, we evaluated the detection and\nclassification performance of state-of-the-art YOLOv4 on data from the\nNOP1pr-GFP-SWAT yeast-cell data library. Experimental results show that by\ndividing original images into 4 quadrants YOLOv4 outputs good detection and\nclassification results with an F1-score of 98% in terms of accuracy and speed,\nwhich is optimally suited for the native resolution of the microscope and\ncurrent GPU memory sizes. Although the application domain is optical microscopy\nin yeast cells, the method is also applicable to multiple-cell images in\nmedical applications\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 21:24:50 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Haja", "Asmaa", ""], ["Schomaker", "Lambert R. B.", ""]]}, {"id": "2104.02805", "submitter": "Pengyu Yuan", "authors": "Pengyu Yuan, Wenyi Hu, Xuqing Wu, Jiefu Chen, Hien Van Nguyen", "title": "First arrival picking using U-net with Lovasz loss and nearest point\n  picking method", "comments": null, "journal-ref": null, "doi": "10.1190/segam2019-3214404.1", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We proposed a robust segmentation and picking workflow to solve the first\narrival picking problem for seismic signal processing. Unlike traditional\nclassification algorithm, image segmentation method can utilize the location\ninformation by outputting a prediction map which has the same size of the input\nimage. A parameter-free nearest point picking algorithm is proposed to further\nimprove the accuracy of the first arrival picking. The algorithm is test on\nsynthetic clean data, synthetic noisy data, synthetic picking-disconnected data\nand field data. It performs well on all of them and the picking deviation\nreaches as low as 4.8ms per receiver. The first arrival picking problem is\nformulated as the contour detection problem. Similar to \\cite{wu2019semi}, we\nuse U-net to perform the segmentation as it is proven to be state-of-the-art in\nmany image segmentation tasks. Particularly, a Lovasz loss instead of the\ntraditional cross-entropy loss is used to train the network for a better\nsegmentation performance. Lovasz loss is a surrogate loss for Jaccard index or\nthe so-called intersection-over-union (IoU) score, which is often one of the\nmost used metrics for segmentation tasks. In the picking part, we use a novel\nnearest point picking (NPP) method to take the advantage of the coherence of\nthe first arrival picking among adjacent receivers. Our model is tested and\nvalidated on both synthetic and field data with harmonic noises. The main\ncontributions of this paper are as follows: 1. Used Lovasz loss to directly\noptimize the IoU for segmentation task. Improvement over the cross-entropy loss\nwith regard to the segmentation accuracy is verified by the test result. 2.\nProposed a nearest point picking post processing method to overcome any defects\nleft by the segmentation output. 3. Conducted noise analysis and verified the\nmodel with both noisy synthetic and field datasets.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 21:46:53 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Yuan", "Pengyu", ""], ["Hu", "Wenyi", ""], ["Wu", "Xuqing", ""], ["Chen", "Jiefu", ""], ["Van Nguyen", "Hien", ""]]}, {"id": "2104.02811", "submitter": "Steven Grosz Mr.", "authors": "Steven A. Grosz, Joshua J. Engelsma, and Anil K. Jain", "title": "C2CL: Contact to Contactless Fingerprint Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matching contactless fingerprints or finger photos to contact-based\nfingerprint impressions has received increased attention in the wake of\nCOVID-19 due to the superior hygiene of the contactless acquisition and the\nwidespread availability of low cost mobile phones capable of capturing photos\nof fingerprints with sufficient resolution for verification purposes. This\npaper presents an end-to-end automated system, called C2CL, comprised of a\nmobile finger photo capture app, preprocessing, and matching algorithms to\nhandle the challenges inhibiting previous cross-matching methods; namely i) low\nridge-valley contrast of contactless fingerprints, ii) varying roll, pitch,\nyaw, and distance of the finger to the camera, iii) non-linear distortion of\ncontact-based fingerprints, and vi) different image qualities of smartphone\ncameras. Our preprocessing algorithm segments, enhances, scales, and unwarps\ncontactless fingerprints, while our matching algorithm extracts both minutiae\nand texture representations. A sequestered dataset of 9,888 contactless 2D\nfingerprints and corresponding contact-based fingerprints from 206 subjects (2\nthumbs and 2 index fingers for each subject) acquired using our mobile capture\napp is used to evaluate the cross-database performance of our proposed\nalgorithm. Furthermore, additional experimental results on 3 publicly available\ndatasets demonstrate, for the first time, contact to contactless fingerprint\nmatching accuracy that is comparable to existing contact to contact fingerprint\nmatching systems (TAR in the range of 96.67% to 98.15% at FAR=0.01%).\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 21:52:46 GMT"}, {"version": "v2", "created": "Thu, 8 Apr 2021 20:43:51 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Grosz", "Steven A.", ""], ["Engelsma", "Joshua J.", ""], ["Jain", "Anil K.", ""]]}, {"id": "2104.02815", "submitter": "Haoyu Zhang", "authors": "Haoyu Zhang, Marcel Grimmer, Raghavendra Ramachandra, Kiran Raja,\n  Christoph Busch", "title": "On the Applicability of Synthetic Data for Face Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face verification has come into increasing focus in various applications\nincluding the European Entry/Exit System, which integrates face recognition\nmechanisms. At the same time, the rapid advancement of biometric authentication\nrequires extensive performance tests in order to inhibit the discriminatory\ntreatment of travellers due to their demographic background. However, the use\nof face images collected as part of border controls is restricted by the\nEuropean General Data Protection Law to be processed for no other reason than\nits original purpose. Therefore, this paper investigates the suitability of\nsynthetic face images generated with StyleGAN and StyleGAN2 to compensate for\nthe urgent lack of publicly available large-scale test data. Specifically, two\ndeep learning-based (SER-FIQ, FaceQnet v1) and one standard-based (ISO/IEC TR\n29794-5) face image quality assessment algorithm is utilized to compare the\napplicability of synthetic face images compared to real face images extracted\nfrom the FRGC dataset. Finally, based on the analysis of impostor score\ndistributions and utility score distributions, our experiments reveal\nnegligible differences between StyleGAN vs. StyleGAN2, and further also minor\ndiscrepancies compared to real face images.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 22:12:30 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Zhang", "Haoyu", ""], ["Grimmer", "Marcel", ""], ["Ramachandra", "Raghavendra", ""], ["Raja", "Kiran", ""], ["Busch", "Christoph", ""]]}, {"id": "2104.02820", "submitter": "Julien Martel", "authors": "Edwin Vargas, Julien N.P. Martel, Gordon Wetzstein, Henry Arguello", "title": "Time-Multiplexed Coded Aperture Imaging: Learned Coded Aperture and\n  Pixel Exposures for Compressive Imaging Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compressive imaging using coded apertures (CA) is a powerful technique that\ncan be used to recover depth, light fields, hyperspectral images and other\nquantities from a single snapshot. The performance of compressive imaging\nsystems based on CAs mostly depends on two factors: the properties of the\nmask's attenuation pattern, that we refer to as \"codification\" and the\ncomputational techniques used to recover the quantity of interest from the\ncoded snapshot. In this work, we introduce the idea of using time-varying CAs\nsynchronized with spatially varying pixel shutters. We divide the exposure of a\nsensor into sub-exposures at the beginning of which the CA mask changes and at\nwhich the sensor's pixels are simultaneously and individually switched \"on\" or\n\"off\". This is a practically appealing codification as it does not introduce\nadditional optical components other than the already present CA but uses a\nchange in the pixel shutter that can be easily realized electronically. We show\nthat our proposed time multiplexed coded aperture (TMCA) can be optimized\nend-to-end and induces better coded snapshots enabling superior reconstructions\nin two different applications: compressive light field imaging and\nhyperspectral imaging. We demonstrate both in simulation and on real captures\n(taken with prototypes we built) that this codification outperforms the\nstate-of-the-art compressive imaging systems by more than 4dB in those\napplications.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 22:42:34 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Vargas", "Edwin", ""], ["Martel", "Julien N. P.", ""], ["Wetzstein", "Gordon", ""], ["Arguello", "Henry", ""]]}, {"id": "2104.02821", "submitter": "Cristian Canton Ferrer", "authors": "Caner Hazirbas, Joanna Bitton, Brian Dolhansky, Jacqueline Pan, Albert\n  Gordo, Cristian Canton Ferrer", "title": "Towards measuring fairness in AI: the Casual Conversations dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a novel dataset to help researchers evaluate their\ncomputer vision and audio models for accuracy across a diverse set of age,\ngenders, apparent skin tones and ambient lighting conditions. Our dataset is\ncomposed of 3,011 subjects and contains over 45,000 videos, with an average of\n15 videos per person. The videos were recorded in multiple U.S. states with a\ndiverse set of adults in various age, gender and apparent skin tone groups. A\nkey feature is that each subject agreed to participate for their likenesses to\nbe used. Additionally, our age and gender annotations are provided by the\nsubjects themselves. A group of trained annotators labeled the subjects'\napparent skin tone using the Fitzpatrick skin type scale. Moreover, annotations\nfor videos recorded in low ambient lighting are also provided. As an\napplication to measure robustness of predictions across certain attributes, we\nprovide a comprehensive study on the top five winners of the DeepFake Detection\nChallenge (DFDC). Experimental evaluation shows that the winning models are\nless performant on some specific groups of people, such as subjects with darker\nskin tones and thus may not generalize to all people. In addition, we also\nevaluate the state-of-the-art apparent age and gender classification methods.\nOur experiments provides a through analysis on these models in terms of fair\ntreatment of people from various backgrounds.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 22:48:22 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Hazirbas", "Caner", ""], ["Bitton", "Joanna", ""], ["Dolhansky", "Brian", ""], ["Pan", "Jacqueline", ""], ["Gordo", "Albert", ""], ["Ferrer", "Cristian Canton", ""]]}, {"id": "2104.02830", "submitter": "Shivangi Aneja Ms", "authors": "Pranjal Singh Rajput, Shivangi Aneja", "title": "IndoFashion : Apparel Classification for Indian Ethnic Clothes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloth categorization is an important research problem that is used by\ne-commerce websites for displaying correct products to the end-users. Indian\nclothes have a large number of clothing categories both for men and women. The\ntraditional Indian clothes like \"Saree\" and \"Dhoti\" are worn very differently\nfrom western clothes like t-shirts and jeans. Moreover, the style and patterns\nof ethnic clothes have a very different distribution from western outfits. Thus\nthe models trained on standard cloth datasets fail miserably on ethnic outfits.\nTo address these challenges, we introduce the first large-scale ethnic dataset\nof over 106k images with 15 different categories for fine-grained\nclassification of Indian ethnic clothes. We gathered a diverse dataset from a\nlarge number of Indian e-commerce websites. We then evaluate several baselines\nfor the cloth classification task on our dataset. In the end, we obtain 88.43%\nclassification accuracy. We hope that our dataset would foster research in the\ndevelopment of several algorithms such as cloth classification, landmark\ndetection, especially for ethnic clothes.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 23:59:23 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Rajput", "Pranjal Singh", ""], ["Aneja", "Shivangi", ""]]}, {"id": "2104.02832", "submitter": "Syed Talha Bukhari", "authors": "Syed Talha Bukhari, Abdul Wahab Amin, Muhammad Abdullah Naveed,\n  Muhammad Rzi Abbas", "title": "ARC: A Vision-based Automatic Retail Checkout System", "comments": "Work was done during the academic year 2017-2018 as a Senior Year\n  (undergraduate) Project (thesis)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Retail checkout systems employed at supermarkets primarily rely on barcode\nscanners, with some utilizing QR codes, to identify the items being purchased.\nThese methods are time-consuming in practice, require a certain level of human\nsupervision, and involve waiting in long queues. In this regard, we propose a\nsystem, that we call ARC, which aims at making the process of check-out at\nretail store counters faster, autonomous, and more convenient, while reducing\ndependency on a human operator. The approach makes use of a computer\nvision-based system, with a Convolutional Neural Network at its core, which\nscans objects placed beneath a webcam for identification. To evaluate the\nproposed system, we curated an image dataset of one-hundred local retail items\nof various categories. Within the given assumptions and considerations, the\nsystem achieves a reasonable test-time accuracy, pointing towards an ambitious\nfuture for the proposed setup. The project code and the dataset are made\npublicly available.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 00:07:53 GMT"}, {"version": "v2", "created": "Mon, 17 May 2021 04:20:00 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Bukhari", "Syed Talha", ""], ["Amin", "Abdul Wahab", ""], ["Naveed", "Muhammad Abdullah", ""], ["Abbas", "Muhammad Rzi", ""]]}, {"id": "2104.02841", "submitter": "Lifeng Fan", "authors": "Lifeng Fan, Shuwen Qiu, Zilong Zheng, Tao Gao, Song-Chun Zhu, Yixin\n  Zhu", "title": "Learning Triadic Belief Dynamics in Nonverbal Communication from Videos", "comments": "CVPR2021, Oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans possess a unique social cognition capability; nonverbal communication\ncan convey rich social information among agents. In contrast, such crucial\nsocial characteristics are mostly missing in the existing scene understanding\nliterature. In this paper, we incorporate different nonverbal communication\ncues (e.g., gaze, human poses, and gestures) to represent, model, learn, and\ninfer agents' mental states from pure visual inputs. Crucially, such a mental\nrepresentation takes the agent's belief into account so that it represents what\nthe true world state is and infers the beliefs in each agent's mental state,\nwhich may differ from the true world states. By aggregating different beliefs\nand true world states, our model essentially forms \"five minds\" during the\ninteractions between two agents. This \"five minds\" model differs from prior\nworks that infer beliefs in an infinite recursion; instead, agents' beliefs are\nconverged into a \"common mind\". Based on this representation, we further devise\na hierarchical energy-based model that jointly tracks and predicts all five\nminds. From this new perspective, a social event is interpreted by a series of\nnonverbal communication and belief dynamics, which transcends the classic\nkeyframe video summary. In the experiments, we demonstrate that using such a\nsocial account provides a better video summary on videos with rich social\ninteractions compared with state-of-the-art keyframe video summary methods.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 00:52:04 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Fan", "Lifeng", ""], ["Qiu", "Shuwen", ""], ["Zheng", "Zilong", ""], ["Gao", "Tao", ""], ["Zhu", "Song-Chun", ""], ["Zhu", "Yixin", ""]]}, {"id": "2104.02846", "submitter": "Yuansheng Hua", "authors": "Yuansheng Hua, Lichao Mou, Pu Jin, Xiao Xiang Zhu", "title": "MultiScene: A Large-scale Dataset and Benchmark for Multi-scene\n  Recognition in Single Aerial Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aerial scene recognition is a fundamental research problem in interpreting\nhigh-resolution aerial imagery. Over the past few years, most studies focus on\nclassifying an image into one scene category, while in real-world scenarios, it\nis more often that a single image contains multiple scenes. Therefore, in this\npaper, we investigate a more practical yet underexplored task -- multi-scene\nrecognition in single images. To this end, we create a large-scale dataset,\ncalled MultiScene, composed of 100,000 unconstrained high-resolution aerial\nimages. Considering that manually labeling such images is extremely arduous, we\nresort to low-cost annotations from crowdsourcing platforms, e.g.,\nOpenStreetMap (OSM). However, OSM data might suffer from incompleteness and\nincorrectness, which introduce noise into image labels. To address this issue,\nwe visually inspect 14,000 images and correct their scene labels, yielding a\nsubset of cleanly-annotated images, named MultiScene-Clean. With it, we can\ndevelop and evaluate deep networks for multi-scene recognition using clean\ndata. Moreover, we provide crowdsourced annotations of all images for the\npurpose of studying network learning with noisy labels. We conduct experiments\nwith extensive baseline models on both MultiScene-Clean and MultiScene to offer\nbenchmarks for multi-scene recognition in single images and learning from noisy\nlabels for this task, respectively. To facilitate progress, we will make our\ndataset and pre-trained models available.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 01:09:12 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Hua", "Yuansheng", ""], ["Mou", "Lichao", ""], ["Jin", "Pu", ""], ["Zhu", "Xiao Xiang", ""]]}, {"id": "2104.02847", "submitter": "Ashwin Raju", "authors": "Ashwin Raju, Shun Miao, Chi-Tung Cheng, Le Lu, Mei Han, Jing Xiao,\n  Chien-Hung Liao, Junzhou Huang and Adam P. Harrison", "title": "Deep Implicit Statistical Shape Models for 3D Medical Image Delineation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  3D delineation of anatomical structures is a cardinal goal in medical imaging\nanalysis. Prior to deep learning, statistical shape models that imposed\nanatomical constraints and produced high quality surfaces were a core\ntechnology. Prior to deep learning, statistical shape models that imposed\nanatomical constraints and produced high quality surfaces were a core\ntechnology. Today fully-convolutional networks (FCNs), while dominant, do not\noffer these capabilities. We present deep implicit statistical shape models\n(DISSMs), a new approach to delineation that marries the representation power\nof convolutional neural networks (CNNs) with the robustness of SSMs. DISSMs use\na deep implicit surface representation to produce a compact and descriptive\nshape latent space that permits statistical models of anatomical variance. To\nreliably fit anatomically plausible shapes to an image, we introduce a novel\nrigid and non-rigid pose estimation pipeline that is modelled as a Markov\ndecision process(MDP). We outline a training regime that includes inverted\nepisodic training and a deep realization of marginal space learning (MSL).\nIntra-dataset experiments on the task of pathological liver segmentation\ndemonstrate that DISSMs can perform more robustly than three leading FCN\nmodels, including nnU-Net: reducing the mean Hausdorff distance (HD) by\n7.7-14.3mm and improving the worst case Dice-Sorensen coefficient (DSC) by\n1.2-2.3%. More critically, cross-dataset experiments on a dataset directly\nreflecting clinical deployment scenarios demonstrate that DISSMs improve the\nmean DSC and HD by 3.5-5.9% and 12.3-24.5mm, respectively, and the worst-case\nDSC by 5.4-7.3%. These improvements are over and above any benefits from\nrepresenting delineations with high-quality surface.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 01:15:06 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Raju", "Ashwin", ""], ["Miao", "Shun", ""], ["Cheng", "Chi-Tung", ""], ["Lu", "Le", ""], ["Han", "Mei", ""], ["Xiao", "Jing", ""], ["Liao", "Chien-Hung", ""], ["Huang", "Junzhou", ""], ["Harrison", "Adam P.", ""]]}, {"id": "2104.02850", "submitter": "Jin Liu", "authors": "Jin Liu, Peng Chen, Tao Liang, Zhaoxing Li, Cai Yu, Shuqiao Zou, Jiao\n  Dai, Jizhong Han", "title": "LI-Net: Large-Pose Identity-Preserving Face Reenactment Network", "comments": "IEEE International Conference on Multimedia and Expo(ICME) 2021 Oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face reenactment is a challenging task, as it is difficult to maintain\naccurate expression, pose and identity simultaneously. Most existing methods\ndirectly apply driving facial landmarks to reenact source faces and ignore the\nintrinsic gap between two identities, resulting in the identity mismatch issue.\nBesides, they neglect the entanglement of expression and pose features when\nencoding driving faces, leading to inaccurate expressions and visual artifacts\non large-pose reenacted faces. To address these problems, we propose a\nLarge-pose Identity-preserving face reenactment network, LI-Net. Specifically,\nthe Landmark Transformer is adopted to adjust driving landmark images, which\naims to narrow the identity gap between driving and source landmark images.\nThen the Face Rotation Module and the Expression Enhancing Generator decouple\nthe transformed landmark image into pose and expression features, and reenact\nthose attributes separately to generate identity-preserving faces with accurate\nexpressions and poses. Both qualitative and quantitative experimental results\ndemonstrate the superiority of our method.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 01:41:21 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Liu", "Jin", ""], ["Chen", "Peng", ""], ["Liang", "Tao", ""], ["Li", "Zhaoxing", ""], ["Yu", "Cai", ""], ["Zou", "Shuqiao", ""], ["Dai", "Jiao", ""], ["Han", "Jizhong", ""]]}, {"id": "2104.02857", "submitter": "Guang Li", "authors": "Guang Li, Ren Togo, Takahiro Ogawa, Miki Haseyama", "title": "Soft-Label Anonymous Gastric X-ray Image Distillation", "comments": "Published as a conference paper at ICIP 2020", "journal-ref": null, "doi": "10.1109/ICIP40778.2020.9191357", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a soft-label anonymous gastric X-ray image distillation\nmethod based on a gradient descent approach. The sharing of medical data is\ndemanded to construct high-accuracy computer-aided diagnosis (CAD) systems.\nHowever, the large size of the medical dataset and privacy protection are\nremaining problems in medical data sharing, which hindered the research of CAD\nsystems. The idea of our distillation method is to extract the valid\ninformation of the medical dataset and generate a tiny distilled dataset that\nhas a different data distribution. Different from model distillation, our\nmethod aims to find the optimal distilled images, distilled labels and the\noptimized learning rate. Experimental results show that the proposed method can\nnot only effectively compress the medical dataset but also anonymize medical\nimages to protect the patient's private information. The proposed approach can\nimprove the efficiency and security of medical data sharing.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 02:04:12 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Li", "Guang", ""], ["Togo", "Ren", ""], ["Ogawa", "Takahiro", ""], ["Haseyama", "Miki", ""]]}, {"id": "2104.02862", "submitter": "Xudong Tian", "authors": "Xudong Tian, Zhizhong Zhang, Shaohui Lin, Yanyun Qu, Yuan Xie,\n  Lizhuang Ma", "title": "Farewell to Mutual Information: Variational Distillation for Cross-Modal\n  Person Re-Identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Information Bottleneck (IB) provides an information theoretic principle\nfor representation learning, by retaining all information relevant for\npredicting label while minimizing the redundancy. Though IB principle has been\napplied to a wide range of applications, its optimization remains a challenging\nproblem which heavily relies on the accurate estimation of mutual information.\nIn this paper, we present a new strategy, Variational Self-Distillation (VSD),\nwhich provides a scalable, flexible and analytic solution to essentially\nfitting the mutual information but without explicitly estimating it. Under\nrigorously theoretical guarantee, VSD enables the IB to grasp the intrinsic\ncorrelation between representation and label for supervised training.\nFurthermore, by extending VSD to multi-view learning, we introduce two other\nstrategies, Variational Cross-Distillation (VCD) and Variational\nMutual-Learning (VML), which significantly improve the robustness of\nrepresentation to view-changes by eliminating view-specific and task-irrelevant\ninformation. To verify our theoretically grounded strategies, we apply our\napproaches to cross-modal person Re-ID, and conduct extensive experiments,\nwhere the superior performance against state-of-the-art methods are\ndemonstrated. Our intriguing findings highlight the need to rethink the way to\nestimate mutual\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 02:19:41 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Tian", "Xudong", ""], ["Zhang", "Zhizhong", ""], ["Lin", "Shaohui", ""], ["Qu", "Yanyun", ""], ["Xie", "Yuan", ""], ["Ma", "Lizhuang", ""]]}, {"id": "2104.02864", "submitter": "Guang Li", "authors": "Guang Li, Ren Togo, Takahiro Ogawa, Miki Haseyama", "title": "Self-Supervised Learning for Gastritis Detection with Gastric X-ray\n  Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background and Objective: Manually annotating gastric X-ray images for\ngastritis detection is time-consuming and expensive because it typically\nrequires expert knowledge. This paper proposes a self-supervised learning\nmethod to solve this problem. This study aims to verify the effectiveness of\nthe proposed self-supervised learning method in gastritis detection using a few\nannotated gastric X-ray images. Methods: In this paper, we propose a novel\nself-supervised learning method that can perform explicit self-supervised\nlearning and learn discriminative representations from gastric X-ray images.\nModels trained with the proposed method were fine-tuned on datasets with a few\nannotated gastric X-ray images. For comparison, several state-of-the-art\nself-supervised learning methods, i.e., containing SimSiam, BYOL, PIRL-jigsaw,\nPIRL-rotation, and SimCLR, were compared with the proposed method. Furthermore,\ntwo baseline methods, one pretrained on ImageNet and the other trained from\nscratch, were compared with the proposed method. Results: The proposed method's\nharmonic mean score of sensitivity and specificity after fine-tuning with the\nannotated data of 10, 20, 30, and 40 patients were 0.875, 0.911, 0.915, and\n0.931, respectively. The proposed method outperformed all comparative methods,\nincluding the five state-of-the-art self-supervised learning and two baseline\nmethods. Experimental results showed the effectiveness of the proposed method\nin gastritis detection with a few annotated gastric X-ray images. Conclusions:\nThe proposed self-supervised learning method shows potential for clinical use\nin gastritis detection using a few annotated gastric X-ray images.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 02:32:06 GMT"}, {"version": "v2", "created": "Mon, 21 Jun 2021 01:27:43 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Li", "Guang", ""], ["Togo", "Ren", ""], ["Ogawa", "Takahiro", ""], ["Haseyama", "Miki", ""]]}, {"id": "2104.02866", "submitter": "Xinkai Zhao", "authors": "Xinkai Zhao, Chaowei Fang, Feng Gao, De-Jun Fan, Xutao Lin, Guanbin Li", "title": "Deep Transformers for Fast Small Intestine Grounding in Capsule\n  Endoscope Video", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Capsule endoscopy is an evolutional technique for examining and diagnosing\nintractable gastrointestinal diseases. Because of the huge amount of data,\nanalyzing capsule endoscope videos is very time-consuming and labor-intensive\nfor gastrointestinal medicalists. The development of intelligent long video\nanalysis algorithms for regional positioning and analysis of capsule endoscopic\nvideo is therefore essential to reduce the workload of clinicians and assist in\nimproving the accuracy of disease diagnosis. In this paper, we propose a deep\nmodel to ground shooting range of small intestine from a capsule endoscope\nvideo which has duration of tens of hours. This is the first attempt to attack\nthe small intestine grounding task using deep neural network method. We model\nthe task as a 3-way classification problem, in which every video frame is\ncategorized into esophagus/stomach, small intestine or colorectum. To explore\nlong-range temporal dependency, a transformer module is built to fuse features\nof multiple neighboring frames. Based on the classification model, we devise an\nefficient search algorithm to efficiently locate the starting and ending\nshooting boundaries of the small intestine. Without searching the small\nintestine exhaustively in the full video, our method is implemented via\niteratively separating the video segment along the direction to the target\nboundary in the middle. We collect 113 videos from a local hospital to validate\nour method. In the 5-fold cross validation, the average IoU between the small\nintestine segments located by our method and the ground-truths annotated by\nbroad-certificated gastroenterologists reaches 0.945.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 02:35:18 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Zhao", "Xinkai", ""], ["Fang", "Chaowei", ""], ["Gao", "Feng", ""], ["Fan", "De-Jun", ""], ["Lin", "Xutao", ""], ["Li", "Guanbin", ""]]}, {"id": "2104.02867", "submitter": "Zhi Hou", "authors": "Zhi Hou, Baosheng Yu, Yu Qiao, Xiaojiang Peng, Dacheng Tao", "title": "Affordance Transfer Learning for Human-Object Interaction Detection", "comments": "Accepted to CVPR2021; add a new but important ablated experiment in\n  appendix(union box verb representation);", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reasoning the human-object interactions (HOI) is essential for deeper scene\nunderstanding, while object affordances (or functionalities) are of great\nimportance for human to discover unseen HOIs with novel objects. Inspired by\nthis, we introduce an affordance transfer learning approach to jointly detect\nHOIs with novel objects and recognize affordances. Specifically, HOI\nrepresentations can be decoupled into a combination of affordance and object\nrepresentations, making it possible to compose novel interactions by combining\naffordance representations and novel object representations from additional\nimages, i.e. transferring the affordance to novel objects. With the proposed\naffordance transfer learning, the model is also capable of inferring the\naffordances of novel objects from known affordance representations. The\nproposed method can thus be used to 1) improve the performance of HOI\ndetection, especially for the HOIs with unseen objects; and 2) infer the\naffordances of novel objects. Experimental results on two datasets, HICO-DET\nand HOI-COCO (from V-COCO), demonstrate significant improvements over recent\nstate-of-the-art methods for HOI detection and object affordance detection.\nCode is available at https://github.com/zhihou7/HOI-CL\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 02:37:04 GMT"}, {"version": "v2", "created": "Wed, 9 Jun 2021 06:02:11 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Hou", "Zhi", ""], ["Yu", "Baosheng", ""], ["Qiao", "Yu", ""], ["Peng", "Xiaojiang", ""], ["Tao", "Dacheng", ""]]}, {"id": "2104.02869", "submitter": "Ugur Demir", "authors": "Ugur Demir, Ismail Irmakci, Elif Keles, Ahmet Topcu, Ziyue Xu,\n  Concetto Spampinato, Sachin Jambawalikar, Evrim Turkbey, Baris Turkbey, Ulas\n  Bagci", "title": "Information Bottleneck Attribution for Visual Explanations of Diagnosis\n  and Prognosis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual explanation methods have an important role in the prognosis of the\npatients where the annotated data is limited or unavailable. There have been\nseveral attempts to use gradient-based attribution methods to localize\npathology from medical scans without using segmentation labels. This research\ndirection has been impeded by the lack of robustness and reliability. These\nmethods are highly sensitive to the network parameters. In this study, we\nintroduce a robust visual explanation method to address this problem for\nmedical applications. We provide an innovative visual explanation algorithm for\ngeneral purpose and as an example application, we demonstrate its effectiveness\nfor quantifying lesions in the lungs caused by the Covid-19 with high accuracy\nand robustness without using dense segmentation labels. This approach overcomes\nthe drawbacks of commonly used Grad-CAM and its extended versions. The premise\nbehind our proposed strategy is that the information flow is minimized while\nensuring the classifier prediction stays similar. Our findings indicate that\nthe bottleneck condition provides a more stable severity estimation than the\nsimilar attribution methods.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 02:43:52 GMT"}, {"version": "v2", "created": "Wed, 23 Jun 2021 01:13:39 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Demir", "Ugur", ""], ["Irmakci", "Ismail", ""], ["Keles", "Elif", ""], ["Topcu", "Ahmet", ""], ["Xu", "Ziyue", ""], ["Spampinato", "Concetto", ""], ["Jambawalikar", "Sachin", ""], ["Turkbey", "Evrim", ""], ["Turkbey", "Baris", ""], ["Bagci", "Ulas", ""]]}, {"id": "2104.02873", "submitter": "Yuchen He", "authors": "Yuchen He, Sihong Duan, Jianxing Li, Hui Chen, Huaibin Zheng, Jianbin\n  Liu, Shitao Zhu, Zhuo Xu", "title": "Speckles-Training-Based Denoising Convolutional Neural Network Ghost\n  Imaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Ghost imaging (GI) has been paid attention gradually because of its lens-less\nimaging capability, turbulence-free imaging and high detection sensitivity.\nHowever, low image quality and slow imaging speed restrict the application\nprocess of GI. In this paper, we propose a improved GI method based on\nDenoising Convolutional Neural Networks (DnCNN). Inspired by the corresponding\nbetween input (noisy image) and output (residual image) in DnCNN, we construct\nthe mapping between speckles sequence and the corresponding noise distribution\nin GI through training. Then, the same speckles sequence is employed to\nilluminate unknown targets, and a de-noising target image will be obtained. The\nproposed method can be regarded as a general method for GI. Under two sampling\nrates, extensive experiments are carried out to compare with traditional GI\nmethod (basic correlation and compressed sensing) and DnCNN method on three\ndata sets. Moreover, we set up a physical GI experiment system to verify the\nproposed method. The results show that the proposed method achieves promising\nperformance.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 02:56:57 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["He", "Yuchen", ""], ["Duan", "Sihong", ""], ["Li", "Jianxing", ""], ["Chen", "Hui", ""], ["Zheng", "Huaibin", ""], ["Liu", "Jianbin", ""], ["Zhu", "Shitao", ""], ["Xu", "Zhuo", ""]]}, {"id": "2104.02874", "submitter": "XingJiao Wu", "authors": "Xingjiao Wu, Ziling Hu, Xiangcheng Du, Jing Yang, Liang He", "title": "Document Layout Analysis via Dynamic Residual Feature Fusion", "comments": "7 pages, 6 figures", "journal-ref": "IEEE ICME 2021", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The document layout analysis (DLA) aims to split the document image into\ndifferent interest regions and understand the role of each region, which has\nwide application such as optical character recognition (OCR) systems and\ndocument retrieval. However, it is a challenge to build a DLA system because\nthe training data is very limited and lacks an efficient model. In this paper,\nwe propose an end-to-end united network named Dynamic Residual Fusion Network\n(DRFN) for the DLA task. Specifically, we design a dynamic residual feature\nfusion module which can fully utilize low-dimensional information and maintain\nhigh-dimensional category information. Besides, to deal with the model\noverfitting problem that is caused by lacking enough data, we propose the\ndynamic select mechanism for efficient fine-tuning in limited train data. We\nexperiment with two challenging datasets and demonstrate the effectiveness of\nthe proposed module.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 02:57:09 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Wu", "Xingjiao", ""], ["Hu", "Ziling", ""], ["Du", "Xiangcheng", ""], ["Yang", "Jing", ""], ["He", "Liang", ""]]}, {"id": "2104.02894", "submitter": "Zhaoyi Wan", "authors": "Zhaoyi Wan, Haoran Chen, Jielei Zhang, Wentao Jiang, Cong Yao, Jiebo\n  Luo", "title": "Facial Attribute Transformers for Precise and Robust Makeup Transfer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we address the problem of makeup transfer, which aims at\ntransplanting the makeup from the reference face to the source face while\npreserving the identity of the source. Existing makeup transfer methods have\nmade notable progress in generating realistic makeup faces, but do not perform\nwell in terms of color fidelity and spatial transformation. To tackle these\nissues, we propose a novel Facial Attribute Transformer (FAT) and its variant\nSpatial FAT for high-quality makeup transfer. Drawing inspirations from the\nTransformer in NLP, FAT is able to model the semantic correspondences and\ninteractions between the source face and reference face, and then precisely\nestimate and transfer the facial attributes. To further facilitate shape\ndeformation and transformation of facial parts, we also integrate thin plate\nsplines (TPS) into FAT, thus creating Spatial FAT, which is the first method\nthat can transfer geometric attributes in addition to color and texture.\nExtensive qualitative and quantitative experiments demonstrate the\neffectiveness and superiority of our proposed FATs in the following aspects:\n(1) ensuring high-fidelity color transfer; (2) allowing for geometric\ntransformation of facial parts; (3) handling facial variations (such as poses\nand shadows) and (4) supporting high-resolution face generation.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 03:39:02 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Wan", "Zhaoyi", ""], ["Chen", "Haoran", ""], ["Zhang", "Jielei", ""], ["Jiang", "Wentao", ""], ["Yao", "Cong", ""], ["Luo", "Jiebo", ""]]}, {"id": "2104.02895", "submitter": "Jong Chul Ye", "authors": "Byung-Hoon Kim, Joonyoung Song, Jong Chul Ye, JaeHyun Baek", "title": "PyNET-CA: Enhanced PyNET with Channel Attention for End-to-End Mobile\n  Image Signal Processing", "comments": "ECCV 2020 AIM workshop accepted version", "journal-ref": null, "doi": "10.1007/978-3-030-67070-2_12", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reconstructing RGB image from RAW data obtained with a mobile device is\nrelated to a number of image signal processing (ISP) tasks, such as\ndemosaicing, denoising, etc. Deep neural networks have shown promising results\nover hand-crafted ISP algorithms on solving these tasks separately, or even\nreplacing the whole reconstruction process with one model. Here, we propose\nPyNET-CA, an end-to-end mobile ISP deep learning algorithm for RAW to RGB\nreconstruction. The model enhances PyNET, a recently proposed state-of-the-art\nmodel for mobile ISP, and improve its performance with channel attention and\nsubpixel reconstruction module. We demonstrate the performance of the proposed\nmethod with comparative experiments and results from the AIM 2020 learned\nsmartphone ISP challenge. The source code of our implementation is available at\nhttps://github.com/egyptdj/skyb-aim2020-public\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 03:40:11 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Kim", "Byung-Hoon", ""], ["Song", "Joonyoung", ""], ["Ye", "Jong Chul", ""], ["Baek", "JaeHyun", ""]]}, {"id": "2104.02904", "submitter": "Shu Kong", "authors": "Yi-Ting Chen, Jinghao Shi, Christoph Mertz, Shu Kong, Deva Ramanan", "title": "Multimodal Object Detection via Bayesian Fusion", "comments": "https://mscvprojects.ri.cmu.edu/2020teamc", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detection with multimodal inputs can improve many safety-critical\nperception systems such as autonomous vehicles (AVs). Motivated by AVs that\noperate in both day and night, we study multimodal object detection with RGB\nand thermal cameras, since the latter can provide much stronger object\nsignatures under poor illumination. We explore strategies for fusing\ninformation from different modalities. Our key contribution is a non-learned\nlate-fusion method that fuses together bounding box detections from different\nmodalities via a simple probabilistic model derived from first principles. Our\nsimple approach, which we call Bayesian Fusion, is readily derived from\nconditional independence assumptions across different modalities. We apply our\napproach to benchmarks containing both aligned (KAIST) and unaligned (FLIR)\nmultimodal sensor data. Our Bayesian Fusion outperforms prior work by more than\n13% in relative performance.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 04:03:20 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Chen", "Yi-Ting", ""], ["Shi", "Jinghao", ""], ["Mertz", "Christoph", ""], ["Kong", "Shu", ""], ["Ramanan", "Deva", ""]]}, {"id": "2104.02921", "submitter": "Xudong Wang", "authors": "Xudong Wang, Long Lian, Stella X. Yu", "title": "Unsupervised Visual Attention and Invariance for Reinforcement Learning", "comments": "Accepted at CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vision-based reinforcement learning (RL) is successful, but how to generalize\nit to unknown test environments remains challenging. Existing methods focus on\ntraining an RL policy that is universal to changing visual domains, whereas we\nfocus on extracting visual foreground that is universal, feeding clean\ninvariant vision to the RL policy learner. Our method is completely\nunsupervised, without manual annotations or access to environment internals.\n  Given videos of actions in a training environment, we learn how to extract\nforegrounds with unsupervised keypoint detection, followed by unsupervised\nvisual attention to automatically generate a foreground mask per video frame.\nWe can then introduce artificial distractors and train a model to reconstruct\nthe clean foreground mask from noisy observations. Only this learned model is\nneeded during test to provide distraction-free visual input to the RL policy\nlearner.\n  Our Visual Attention and Invariance (VAI) method significantly outperforms\nthe state-of-the-art on visual domain generalization, gaining 15 to 49% (61 to\n229%) more cumulative rewards per episode on DeepMind Control (our DrawerWorld\nManipulation) benchmarks. Our results demonstrate that it is not only possible\nto learn domain-invariant vision without any supervision, but freeing RL from\nvisual distractions also makes the policy more focused and thus far better.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 05:28:01 GMT"}, {"version": "v2", "created": "Fri, 16 Apr 2021 18:56:48 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Wang", "Xudong", ""], ["Lian", "Long", ""], ["Yu", "Stella X.", ""]]}, {"id": "2104.02922", "submitter": "Suryabhan Singh Hada", "authors": "Suryabhan Singh Hada and Miguel \\'A. Carreira-Perpi\\~n\\'an and Arman\n  Zharmagambetov", "title": "Sparse Oblique Decision Trees: A Tool to Understand and Manipulate\n  Neural Net Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The widespread deployment of deep nets in practical applications has lead to\na growing desire to understand how and why such black-box methods perform\nprediction. Much work has focused on understanding what part of the input\npattern (an image, say) is responsible for a particular class being predicted,\nand how the input may be manipulated to predict a different class. We focus\ninstead on understanding which of the internal features computed by the neural\nnet are responsible for a particular class. We achieve this by mimicking part\nof the neural net with an oblique decision tree having sparse weight vectors at\nthe decision nodes. Using the recently proposed Tree Alternating Optimization\n(TAO) algorithm, we are able to learn trees that are both highly accurate and\ninterpretable. Such trees can faithfully mimic the part of the neural net they\nreplaced, and hence they can provide insights into the deep net black box.\nFurther, we show we can easily manipulate the neural net features in order to\nmake the net predict, or not predict, a given class, thus showing that it is\npossible to carry out adversarial attacks at the level of the features. These\ninsights and manipulations apply globally to the entire training and test set,\nnot just at a local (single-instance) level. We demonstrate this robustly in\nthe MNIST and ImageNet datasets with LeNet5 and VGG networks.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 05:31:08 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Hada", "Suryabhan Singh", ""], ["Carreira-Perpi\u00f1\u00e1n", "Miguel \u00c1.", ""], ["Zharmagambetov", "Arman", ""]]}, {"id": "2104.02925", "submitter": "Alexandre Thiery", "authors": "Rahul Rahaman, Atin Ghosh and Alexandre H. Thiery", "title": "Pretrained equivariant features improve unsupervised landmark discovery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Locating semantically meaningful landmark points is a crucial component of a\nlarge number of computer vision pipelines. Because of the small number of\navailable datasets with ground truth landmark annotations, it is important to\ndesign robust unsupervised and semi-supervised methods for landmark detection.\n  Many of the recent unsupervised learning methods rely on the equivariance\nproperties of landmarks to synthetic image deformations. Our work focuses on\nsuch widely used methods and sheds light on its core problem, its inability to\nproduce equivariant intermediate convolutional features. This finding leads us\nto formulate a two-step unsupervised approach that overcomes this challenge by\nfirst learning powerful pixel-based features and then use the pre-trained\nfeatures to learn a landmark detector by the traditional equivariance method.\nOur method produces state-of-the-art results in several challenging landmark\ndetection datasets such as the BBC Pose dataset and the Cat-Head dataset. It\nperforms comparably on a range of other benchmarks.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 05:42:11 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Rahaman", "Rahul", ""], ["Ghosh", "Atin", ""], ["Thiery", "Alexandre H.", ""]]}, {"id": "2104.02939", "submitter": "Shu Kong", "authors": "Shu Kong, Deva Ramanan", "title": "OpenGAN: Open-Set Recognition via Open Data Generation", "comments": "reformatting fig.1 (https://github.com/aimerykong/OpenGAN)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-world machine learning systems need to analyze novel testing data that\ndiffers from the training data. In K-way classification, this is crisply\nformulated as open-set recognition, core to which is the ability to\ndiscriminate open-set data outside the K closed-set classes. Two conceptually\nelegant ideas for open-set discrimination are: 1) discriminatively learning an\nopen-vs-closed binary discriminator by exploiting some outlier data as the\nopen-set, and 2) unsupervised learning the closed-set data distribution with a\nGAN and using its discriminator as the open-set likelihood function. However,\nthe former generalizes poorly to diverse open test data due to overfitting to\nthe training outliers, which unlikely exhaustively span the open-world. The\nlatter does not work well, presumably due to the instable training of GANs.\nMotivated by the above, we propose OpenGAN, which addresses the limitation of\neach approach by combining them with several technical insights. First, we show\nthat a carefully selected GAN-discriminator on some real outlier data already\nachieves the state-of-the-art. Second, we augment the available set of real\nopen training examples with adversarially synthesized \"fake\" data. Third and\nmost importantly, we build the discriminator over the features computed by the\nclosed-world K-way networks. Extensive experiments show that OpenGAN\nsignificantly outperforms prior open-set methods.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 06:19:24 GMT"}, {"version": "v2", "created": "Fri, 9 Apr 2021 02:55:27 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Kong", "Shu", ""], ["Ramanan", "Deva", ""]]}, {"id": "2104.02963", "submitter": "Jinlai Zhang", "authors": "Jinlai Zhang, Binbin Liu, Lyvjie Chen, Bo Ouyang, Jihong Zhu, Minchi\n  Kuang, Houqing Wang, Yanmei Meng", "title": "The art of defense: letting networks fool the attacker", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Some deep neural networks are invariant to some input transformations, such\nas Pointnet is permutation invariant to the input point cloud. In this paper,\nwe demonstrated this property could be powerful in defense of gradient-based\nattacks. Specifically, we apply random input transformation which is invariant\nto the networks we want to defend. Extensive experiments demonstrate that the\nproposed scheme defeats various gradient-based attackers in the targeted attack\nsetting, and breaking the attack accuracy into nearly zero. Our code is\navailable at: {\\footnotesize{\\url{https://github.com/cuge1995/IT-Defense}}}.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 07:28:46 GMT"}, {"version": "v2", "created": "Mon, 31 May 2021 13:15:53 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Zhang", "Jinlai", ""], ["Liu", "Binbin", ""], ["Chen", "Lyvjie", ""], ["Ouyang", "Bo", ""], ["Zhu", "Jihong", ""], ["Kuang", "Minchi", ""], ["Wang", "Houqing", ""], ["Meng", "Yanmei", ""]]}, {"id": "2104.02967", "submitter": "Sanqing Qu", "authors": "Sanqing Qu, Guang Chen, Zhijun Li, Lijun Zhang, Fan Lu, Alois Knoll", "title": "ACM-Net: Action Context Modeling Network for Weakly-Supervised Temporal\n  Action Localization", "comments": "Submitted to TIP. Code is available at\n  https://github.com/ispc-lab/ACM-Net", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Weakly-supervised temporal action localization aims to localize action\ninstances temporal boundary and identify the corresponding action category with\nonly video-level labels. Traditional methods mainly focus on foreground and\nbackground frames separation with only a single attention branch and class\nactivation sequence. However, we argue that apart from the distinctive\nforeground and background frames there are plenty of semantically ambiguous\naction context frames. It does not make sense to group those context frames to\nthe same background class since they are semantically related to a specific\naction category. Consequently, it is challenging to suppress action context\nframes with only a single class activation sequence. To address this issue, in\nthis paper, we propose an action-context modeling network termed ACM-Net, which\nintegrates a three-branch attention module to measure the likelihood of each\ntemporal point being action instance, context, or non-action background,\nsimultaneously. Then based on the obtained three-branch attention values, we\nconstruct three-branch class activation sequences to represent the action\ninstances, contexts, and non-action backgrounds, individually. To evaluate the\neffectiveness of our ACM-Net, we conduct extensive experiments on two benchmark\ndatasets, THUMOS-14 and ActivityNet-1.3. The experiments show that our method\ncan outperform current state-of-the-art methods, and even achieve comparable\nperformance with fully-supervised methods. Code can be found at\nhttps://github.com/ispc-lab/ACM-Net\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 07:39:57 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Qu", "Sanqing", ""], ["Chen", "Guang", ""], ["Li", "Zhijun", ""], ["Zhang", "Lijun", ""], ["Lu", "Fan", ""], ["Knoll", "Alois", ""]]}, {"id": "2104.02971", "submitter": "Jiashuo Yu", "authors": "Jiashuo Yu, Ying Cheng, Rui Feng", "title": "MPN: Multimodal Parallel Network for Audio-Visual Event Localization", "comments": "IEEE International Conference on Multimedia and Expo (ICME) 2021 Oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Audio-visual event localization aims to localize an event that is both\naudible and visible in the wild, which is a widespread audio-visual scene\nanalysis task for unconstrained videos. To address this task, we propose a\nMultimodal Parallel Network (MPN), which can perceive global semantics and\nunmixed local information parallelly. Specifically, our MPN framework consists\nof a classification subnetwork to predict event categories and a localization\nsubnetwork to predict event boundaries. The classification subnetwork is\nconstructed by the Multimodal Co-attention Module (MCM) and obtains global\ncontexts. The localization subnetwork consists of Multimodal Bottleneck\nAttention Module (MBAM), which is designed to extract fine-grained\nsegment-level contents. Extensive experiments demonstrate that our framework\nachieves the state-of-the-art performance both in fully supervised and weakly\nsupervised settings on the Audio-Visual Event (AVE) dataset.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 07:44:22 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Yu", "Jiashuo", ""], ["Cheng", "Ying", ""], ["Feng", "Rui", ""]]}, {"id": "2104.02972", "submitter": "Jiayu Yang", "authors": "Jiayu Yang, Jose M. Alvarez, Miaomiao Liu", "title": "Self-supervised Learning of Depth Inference for Multi-view Stereo", "comments": "CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent supervised multi-view depth estimation networks have achieved\npromising results. Similar to all supervised approaches, these networks require\nground-truth data during training. However, collecting a large amount of\nmulti-view depth data is very challenging. Here, we propose a self-supervised\nlearning framework for multi-view stereo that exploit pseudo labels from the\ninput data. We start by learning to estimate depth maps as initial pseudo\nlabels under an unsupervised learning framework relying on image reconstruction\nloss as supervision. We then refine the initial pseudo labels using a carefully\ndesigned pipeline leveraging depth information inferred from higher resolution\nimages and neighboring views. We use these high-quality pseudo labels as the\nsupervision signal to train the network and improve, iteratively, its\nperformance by self-training. Extensive experiments on the DTU dataset show\nthat our proposed self-supervised learning framework outperforms existing\nunsupervised multi-view stereo networks by a large margin and performs on par\ncompared to the supervised counterpart. Code is available at\nhttps://github.com/JiayuYANG/Self-supervised-CVP-MVSNet.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 07:45:02 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Yang", "Jiayu", ""], ["Alvarez", "Jose M.", ""], ["Liu", "Miaomiao", ""]]}, {"id": "2104.02973", "submitter": "Pierre Gutierrez", "authors": "Antoine Cordier, Deepan Das, and Pierre Gutierrez", "title": "Active learning using weakly supervised signals for quality inspection", "comments": "8 pages, 3 Figures, QCAV 2021 conference (proceedings published in\n  SPIE)", "journal-ref": null, "doi": "10.1117/12.2586595", "report-no": null, "categories": "cs.CV cs.AI cs.LO stat.ML", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Because manufacturing processes evolve fast, and since production visual\naspect can vary significantly on a daily basis, the ability to rapidly update\nmachine vision based inspection systems is paramount. Unfortunately, supervised\nlearning of convolutional neural networks requires a significant amount of\nannotated images for being able to learn effectively from new data.\nAcknowledging the abundance of continuously generated images coming from the\nproduction line and the cost of their annotation, we demonstrate it is possible\nto prioritize and accelerate the annotation process. In this work, we develop a\nmethodology for learning actively, from rapidly mined, weakly (i.e. partially)\nannotated data, enabling a fast, direct feedback from the operators on the\nproduction line and tackling a big machine vision weakness: false positives. We\nalso consider the problem of covariate shift, which arises inevitably due to\nchanging conditions during data acquisition. In that regard, we show\ndomain-adversarial training to be an efficient way to address this issue.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 07:49:07 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Cordier", "Antoine", ""], ["Das", "Deepan", ""], ["Gutierrez", "Pierre", ""]]}, {"id": "2104.02980", "submitter": "Pierre Gutierrez", "authors": "Pierre Gutierrez, Maria Luschkova, Antoine Cordier, Mustafa Shukor,\n  Mona Schappert, and Tim Dahmen", "title": "Synthetic training data generation for deep learning based quality\n  inspection", "comments": "8 pages, 4 figures, to be published in QCAV 2021 conference,\n  proceedings will by published by SPIE", "journal-ref": null, "doi": "10.1117/12.2586824", "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Deep learning is now the gold standard in computer vision-based quality\ninspection systems. In order to detect defects, supervised learning is often\nutilized, but necessitates a large amount of annotated images, which can be\ncostly: collecting, cleaning, and annotating the data is tedious and limits the\nspeed at which a system can be deployed as everything the system must detect\nneeds to be observed first. This can impede the inspection of rare defects,\nsince very few samples can be collected by the manufacturer. In this work, we\nfocus on simulations to solve this issue. We first present a generic simulation\npipeline to render images of defective or healthy (non defective) parts. As\nmetallic parts can be highly textured with small defects like holes, we design\na texture scanning and generation method. We assess the quality of the\ngenerated images by training deep learning networks and by testing them on real\ndata from a manufacturer. We demonstrate that we can achieve encouraging\nresults on real defect detection using purely simulated data. Additionally, we\nare able to improve global performances by concatenating simulated and real\ndata, showing that simulations can complement real images to boost\nperformances. Lastly, using domain adaptation techniques helps improving\nslightly our final results.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 08:07:57 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Gutierrez", "Pierre", ""], ["Luschkova", "Maria", ""], ["Cordier", "Antoine", ""], ["Shukor", "Mustafa", ""], ["Schappert", "Mona", ""], ["Dahmen", "Tim", ""]]}, {"id": "2104.02984", "submitter": "Joel Frank", "authors": "Joel Frank, Thorsten Holz", "title": "[RE] CNN-generated images are surprisingly easy to spot...for now", "comments": "Code available: https://github.com/Joool/ReproducabilityCNNEasyToSpot", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This work evaluates the reproducibility of the paper \"CNN-generated images\nare surprisingly easy to spot... for now\" by Wang et al. published at CVPR\n2020. The paper addresses the challenge of detecting CNN-generated imagery,\nwhich has reached the potential to even fool humans. The authors propose two\nmethods which help an image classifier to generalize from being trained on one\nspecific CNN to detecting imagery produced by unseen architectures, training\nmethods, or data sets. The paper proposes two methods to help a classifier\ngeneralize: (i) utilizing different kinds of data augmentations and (ii) using\na diverse data set. This report focuses on assessing if these techniques indeed\nhelp the generalization process. Furthermore, we perform additional experiments\nto study the limitations of the proposed techniques.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 08:26:35 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Frank", "Joel", ""], ["Holz", "Thorsten", ""]]}, {"id": "2104.03000", "submitter": "Philipp Benz", "authors": "Philipp Benz, Chaoning Zhang, Adil Karjauv, In So Kweon", "title": "Universal Adversarial Training with Class-Wise Perturbations", "comments": "Accepted to ICME 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite their overwhelming success on a wide range of applications,\nconvolutional neural networks (CNNs) are widely recognized to be vulnerable to\nadversarial examples. This intriguing phenomenon led to a competition between\nadversarial attacks and defense techniques. So far, adversarial training is the\nmost widely used method for defending against adversarial attacks. It has also\nbeen extended to defend against universal adversarial perturbations (UAPs). The\nSOTA universal adversarial training (UAT) method optimizes a single\nperturbation for all training samples in the mini-batch. In this work, we find\nthat a UAP does not attack all classes equally. Inspired by this observation,\nwe identify it as the source of the model having unbalanced robustness. To this\nend, we improve the SOTA UAT by proposing to utilize class-wise UAPs during\nadversarial training. On multiple benchmark datasets, our class-wise UAT leads\nsuperior performance for both clean accuracy and adversarial robustness against\nuniversal attack.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 09:05:49 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Benz", "Philipp", ""], ["Zhang", "Chaoning", ""], ["Karjauv", "Adil", ""], ["Kweon", "In So", ""]]}, {"id": "2104.03002", "submitter": "Luca Tomasetti", "authors": "Luca Tomasetti, Kjersti Engan, Mahdieh Khanmohammadi, and Kathinka\n  D{\\ae}hli Kurz", "title": "CNN Based Segmentation of Infarcted Regions in Acute Cerebral Stroke\n  Patients From Computed Tomography Perfusion Imaging", "comments": null, "journal-ref": null, "doi": "10.1145/3388440.3412470", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  More than 13 million people suffer from ischemic cerebral stroke worldwide\neach year. Thrombolytic treatment can reduce brain damage but has a narrow\ntreatment window. Computed Tomography Perfusion imaging is a commonly used\nprimary assessment tool for stroke patients, and typically the radiologists\nwill evaluate resulting parametric maps to estimate the affected areas, dead\ntissue (core), and the surrounding tissue at risk (penumbra), to decide further\ntreatments. Different work has been reported, suggesting thresholds, and\nsemi-automated methods, and in later years deep neural networks, for segmenting\ninfarction areas based on the parametric maps. However, there is no consensus\nin terms of which thresholds to use, or how to combine the information from the\nparametric maps, and the presented methods all have limitations in terms of\nboth accuracy and reproducibility.\n  We propose a fully automated convolutional neural network based segmentation\nmethod that uses the full four-dimensional computed tomography perfusion\ndataset as input, rather than the pre-filtered parametric maps. The suggested\nnetwork is tested on an available dataset as a proof-of-concept, with very\nencouraging results. Cross-validated results show averaged Dice score of 0.78\nand 0.53, and an area under the receiver operating characteristic curve of 0.97\nand 0.94 for penumbra and core respectively\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 09:09:13 GMT"}, {"version": "v2", "created": "Wed, 21 Apr 2021 14:25:34 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Tomasetti", "Luca", ""], ["Engan", "Kjersti", ""], ["Khanmohammadi", "Mahdieh", ""], ["Kurz", "Kathinka D\u00e6hli", ""]]}, {"id": "2104.03008", "submitter": "Divyansh Aggarwal", "authors": "Divyansh Aggarwal, Jiayu Zhou and Anil K. Jain", "title": "FedFace: Collaborative Learning of Face Recognition Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  DNN-based face recognition models require large centrally aggregated face\ndatasets for training. However, due to the growing data privacy concerns and\nlegal restrictions, accessing and sharing face datasets has become exceedingly\ndifficult. We propose FedFace, a federated learning (FL) framework for\ncollaborative learning of face recognition models in a privacy-aware manner.\nFedFace utilizes the face images available on multiple clients to learn an\naccurate and generalizable face recognition model where the face images stored\nat each client are neither shared with other clients nor the central host and\neach client is a mobile device containing face images pertaining to only the\nowner of the device (one identity per client). Our experiments show the\neffectiveness of FedFace in enhancing the verification performance of\npre-trained face recognition system on standard face verification benchmarks\nnamely LFW, IJB-A, and IJB-C.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 09:25:32 GMT"}, {"version": "v2", "created": "Thu, 24 Jun 2021 13:34:14 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Aggarwal", "Divyansh", ""], ["Zhou", "Jiayu", ""], ["Jain", "Anil K.", ""]]}, {"id": "2104.03015", "submitter": "Minchul Shin", "authors": "Minchul Shin, Yoonjae Cho, Byungsoo Ko, Geonmo Gu", "title": "RTIC: Residual Learning for Text and Image Composition using Graph\n  Convolutional Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we study the compositional learning of images and texts for\nimage retrieval. The query is given in the form of an image and text that\ndescribes the desired modifications to the image; the goal is to retrieve the\ntarget image that satisfies the given modifications and resembles the query by\ncomposing information in both the text and image modalities. To accomplish this\ntask, we propose a simple new architecture using skip connections that can\neffectively encode the errors between the source and target images in the\nlatent space. Furthermore, we introduce a novel method that combines the graph\nconvolutional network (GCN) with existing composition methods. We find that the\ncombination consistently improves the performance in a plug-and-play manner. We\nperform thorough and exhaustive experiments on several widely used datasets,\nand achieve state-of-the-art scores on the task with our model. To ensure\nfairness in comparison, we suggest a strict standard for the evaluation because\na small difference in the training conditions can significantly affect the\nfinal performance. We release our implementation, including that of all the\ncompared methods, for reproducibility.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 09:41:52 GMT"}, {"version": "v2", "created": "Thu, 8 Apr 2021 23:28:15 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Shin", "Minchul", ""], ["Cho", "Yoonjae", ""], ["Ko", "Byungsoo", ""], ["Gu", "Geonmo", ""]]}, {"id": "2104.03020", "submitter": "Wenjie Yin", "authors": "Wenjie Yin, Hang Yin, Danica Kragic, M{\\aa}rten Bj\\\"orkman", "title": "Graph-based Normalizing Flow for Human Motion Generation and\n  Reconstruction", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data-driven approaches for modeling human skeletal motion have found various\napplications in interactive media and social robotics. Challenges remain in\nthese fields for generating high-fidelity samples and robustly reconstructing\nmotion from imperfect input data, due to e.g. missed marker detection. In this\npaper, we propose a probabilistic generative model to synthesize and\nreconstruct long horizon motion sequences conditioned on past information and\ncontrol signals, such as the path along which an individual is moving. Our\nmethod adapts the existing work MoGlow by introducing a new graph-based model.\nThe model leverages the spatial-temporal graph convolutional network (ST-GCN)\nto effectively capture the spatial structure and temporal correlation of\nskeletal motion data at multiple scales. We evaluate the models on a mixture of\nmotion capture datasets of human locomotion with foot-step and bone-length\nanalysis. The results demonstrate the advantages of our model in reconstructing\nmissing markers and achieving comparable results on generating realistic future\nposes. When the inputs are imperfect, our model shows improvements on\nrobustness of generation.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 09:51:15 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Yin", "Wenjie", ""], ["Yin", "Hang", ""], ["Kragic", "Danica", ""], ["Bj\u00f6rkman", "M\u00e5rten", ""]]}, {"id": "2104.03046", "submitter": "Ant\\'onio Farinhas", "authors": "Ant\\'onio Farinhas, Andr\\'e F. T. Martins, Pedro M. Q. Aguiar", "title": "Multimodal Continuous Visual Attention Mechanisms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual attention mechanisms are a key component of neural network models for\ncomputer vision. By focusing on a discrete set of objects or image regions,\nthese mechanisms identify the most relevant features and use them to build more\npowerful representations. Recently, continuous-domain alternatives to discrete\nattention models have been proposed, which exploit the continuity of images.\nThese approaches model attention as simple unimodal densities (e.g. a\nGaussian), making them less suitable to deal with images whose region of\ninterest has a complex shape or is composed of multiple non-contiguous patches.\nIn this paper, we introduce a new continuous attention mechanism that produces\nmultimodal densities, in the form of mixtures of Gaussians. We use the EM\nalgorithm to obtain a clustering of relevant regions in the image, and a\ndescription length penalty to select the number of components in the mixture.\nOur densities decompose as a linear combination of unimodal attention\nmechanisms, enabling closed-form Jacobians for the backpropagation step.\nExperiments on visual question answering in the VQA-v2 dataset show competitive\naccuracies and a selection of regions that mimics human attention more closely\nin VQA-HAT. We present several examples that suggest how multimodal attention\nmaps are naturally more interpretable than their unimodal counterparts, showing\nthe ability of our model to automatically segregate objects from ground in\ncomplex scenes.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 10:47:51 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Farinhas", "Ant\u00f3nio", ""], ["Martins", "Andr\u00e9 F. T.", ""], ["Aguiar", "Pedro M. Q.", ""]]}, {"id": "2104.03047", "submitter": "Chi Zhang", "authors": "Chi Zhang, Nan Song, Guosheng Lin, Yun Zheng, Pan Pan, Yinghui Xu", "title": "Few-Shot Incremental Learning with Continually Evolved Classifiers", "comments": "Accpeted to CVPR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Few-shot class-incremental learning (FSCIL) aims to design machine learning\nalgorithms that can continually learn new concepts from a few data points,\nwithout forgetting knowledge of old classes. The difficulty lies in that\nlimited data from new classes not only lead to significant overfitting issues\nbut also exacerbate the notorious catastrophic forgetting problems. Moreover,\nas training data come in sequence in FSCIL, the learned classifier can only\nprovide discriminative information in individual sessions, while FSCIL requires\nall classes to be involved for evaluation. In this paper, we address the FSCIL\nproblem from two aspects. First, we adopt a simple but effective decoupled\nlearning strategy of representations and classifiers that only the classifiers\nare updated in each incremental session, which avoids knowledge forgetting in\nthe representations. By doing so, we demonstrate that a pre-trained backbone\nplus a non-parametric class mean classifier can beat state-of-the-art methods.\nSecond, to make the classifiers learned on individual sessions applicable to\nall classes, we propose a Continually Evolved Classifier (CEC) that employs a\ngraph model to propagate context information between classifiers for\nadaptation. To enable the learning of CEC, we design a pseudo incremental\nlearning paradigm that episodically constructs a pseudo incremental learning\ntask to optimize the graph parameters by sampling data from the base dataset.\nExperiments on three popular benchmark datasets, including CIFAR100,\nminiImageNet, and Caltech-USCD Birds-200-2011 (CUB200), show that our method\nsignificantly outperforms the baselines and sets new state-of-the-art results\nwith remarkable advantages.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 10:54:51 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Zhang", "Chi", ""], ["Song", "Nan", ""], ["Lin", "Guosheng", ""], ["Zheng", "Yun", ""], ["Pan", "Pan", ""], ["Xu", "Yinghui", ""]]}, {"id": "2104.03054", "submitter": "Immanuel Weber", "authors": "Immanuel Weber, Jens Bongartz, Ribana Roscher", "title": "Artificial and beneficial -- Exploiting artificial images for aerial\n  vehicle detection", "comments": "14 pages, 13 figures, 4 tables", "journal-ref": "ISPRS Journal of Photogrammetry and Remote Sensing, Volume 175,\n  May 2021, Pages 158-170", "doi": "10.1016/j.isprsjprs.2021.02.015", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Object detection in aerial images is an important task in environmental,\neconomic, and infrastructure-related tasks. One of the most prominent\napplications is the detection of vehicles, for which deep learning approaches\nare increasingly used. A major challenge in such approaches is the limited\namount of data that arises, for example, when more specialized and rarer\nvehicles such as agricultural machinery or construction vehicles are to be\ndetected. This lack of data contrasts with the enormous data hunger of deep\nlearning methods in general and object recognition in particular. In this\narticle, we address this issue in the context of the detection of road vehicles\nin aerial images. To overcome the lack of annotated data, we propose a\ngenerative approach that generates top-down images by overlaying artificial\nvehicles created from 2D CAD drawings on artificial or real backgrounds. Our\nexperiments with a modified RetinaNet object detection network show that adding\nthese images to small real-world datasets significantly improves detection\nperformance. In cases of very limited or even no real-world images, we observe\nan improvement in average precision of up to 0.70 points. We address the\nremaining performance gap to real-world datasets by analyzing the effect of the\nimage composition of background and objects and give insights into the\nimportance of background.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 11:06:15 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Weber", "Immanuel", ""], ["Bongartz", "Jens", ""], ["Roscher", "Ribana", ""]]}, {"id": "2104.03059", "submitter": "Thomas Unterthiner", "authors": "Jean-Baptiste Cordonnier, Aravindh Mahendran, Alexey Dosovitskiy, Dirk\n  Weissenborn, Jakob Uszkoreit, Thomas Unterthiner", "title": "Differentiable Patch Selection for Image Recognition", "comments": "Accepted to IEEE/CVF Conference on Computer Vision and Pattern\n  Recognition (CVPR) 2021. Code available at\n  https://github.com/google-research/google-research/tree/master/ptopk_patch_selection/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural Networks require large amounts of memory and compute to process high\nresolution images, even when only a small part of the image is actually\ninformative for the task at hand. We propose a method based on a differentiable\nTop-K operator to select the most relevant parts of the input to efficiently\nprocess high resolution images. Our method may be interfaced with any\ndownstream neural network, is able to aggregate information from different\npatches in a flexible way, and allows the whole model to be trained end-to-end\nusing backpropagation. We show results for traffic sign recognition,\ninter-patch relationship reasoning, and fine-grained recognition without using\nobject/part bounding box annotations during training.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 11:15:51 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Cordonnier", "Jean-Baptiste", ""], ["Mahendran", "Aravindh", ""], ["Dosovitskiy", "Alexey", ""], ["Weissenborn", "Dirk", ""], ["Uszkoreit", "Jakob", ""], ["Unterthiner", "Thomas", ""]]}, {"id": "2104.03061", "submitter": "Linsen Song", "authors": "Linsen Song, Wayne Wu, Chaoyou Fu, Chen Qian, Chen Change Loy, Ran He", "title": "Everything's Talkin': Pareidolia Face Reenactment", "comments": "Accepted by CVPR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new application direction named Pareidolia Face Reenactment,\nwhich is defined as animating a static illusory face to move in tandem with a\nhuman face in the video. For the large differences between pareidolia face\nreenactment and traditional human face reenactment, two main challenges are\nintroduced, i.e., shape variance and texture variance. In this work, we propose\na novel Parametric Unsupervised Reenactment Algorithm to tackle these two\nchallenges. Specifically, we propose to decompose the reenactment into three\ncatenate processes: shape modeling, motion transfer and texture synthesis. With\nthe decomposition, we introduce three crucial components, i.e., Parametric\nShape Modeling, Expansionary Motion Transfer and Unsupervised Texture\nSynthesizer, to overcome the problems brought by the remarkably variances on\npareidolia faces. Extensive experiments show the superior performance of our\nmethod both qualitatively and quantitatively. Code, model and data are\navailable on our project page.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 11:19:13 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Song", "Linsen", ""], ["Wu", "Wayne", ""], ["Fu", "Chaoyou", ""], ["Qian", "Chen", ""], ["Loy", "Chen Change", ""], ["He", "Ran", ""]]}, {"id": "2104.03064", "submitter": "Yangchen Xie", "authors": "Yangchen Xie and Xinyuan Chen and Li Sun and Yue Lu", "title": "DG-Font: Deformable Generative Networks for Unsupervised Font Generation", "comments": "Accepted by CVPR-2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Font generation is a challenging problem especially for some writing systems\nthat consist of a large number of characters and has attracted a lot of\nattention in recent years. However, existing methods for font generation are\noften in supervised learning. They require a large number of paired data, which\nis labor-intensive and expensive to collect. Besides, common image-to-image\ntranslation models often define style as the set of textures and colors, which\ncannot be directly applied to font generation. To address these problems, we\npropose novel deformable generative networks for unsupervised font generation\n(DGFont). We introduce a feature deformation skip connection (FDSC) which\npredicts pairs of displacement maps and employs the predicted maps to apply\ndeformable convolution to the low-level feature maps from the content encoder.\nThe outputs of FDSC are fed into a mixer to generate the final results. Taking\nadvantage of FDSC, the mixer outputs a high-quality character with a complete\nstructure. To further improve the quality of generated images, we use three\ndeformable convolution layers in the content encoder to learn style-invariant\nfeature representations. Experiments demonstrate that our model generates\ncharacters in higher quality than state-of-art methods. The source code is\navailable at https://github.com/ecnuycxie/DG-Font.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 11:32:32 GMT"}, {"version": "v2", "created": "Thu, 8 Apr 2021 14:19:26 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Xie", "Yangchen", ""], ["Chen", "Xinyuan", ""], ["Sun", "Li", ""], ["Lu", "Yue", ""]]}, {"id": "2104.03066", "submitter": "Dvir Samuel", "authors": "Dvir Samuel and Gal Chechik", "title": "Distributional Robustness Loss for Long-tail Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Real-world data is often unbalanced and long-tailed, but deep models struggle\nto recognize rare classes in the presence of frequent classes. To address\nunbalanced data, most studies try balancing the data, the loss, or the\nclassifier to reduce classification bias towards head classes. Far less\nattention has been given to the latent representations learned with unbalanced\ndata. We show that the feature extractor part of deep networks suffers greatly\nfrom this bias. We propose a new loss based on robustness theory, which\nencourages the model to learn high-quality representations for both head and\ntail classes. While the general form of the robustness loss may be hard to\ncompute, we further derive an easy-to-compute upper bound that can be minimized\nefficiently. This procedure reduces representation bias towards head classes in\nthe feature space and achieves new SOTA results on CIFAR100-LT, ImageNet-LT,\nand iNaturalist long-tail benchmarks. We find that training with robustness\nincreases recognition accuracy of tail classes while largely maintaining the\naccuracy of head classes. The new robustness loss can be combined with various\nclassifier balancing techniques and can be applied to representations at\nseveral layers of the deep model.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 11:34:04 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Samuel", "Dvir", ""], ["Chechik", "Gal", ""]]}, {"id": "2104.03068", "submitter": "Moi Hoon Yap", "authors": "Moi Hoon Yap and Bill Cassidy and Joseph M. Pappachan and Claire\n  O'Shea and David Gillespie and Neil Reeves", "title": "Analysis Towards Classification of Infection and Ischaemia of Diabetic\n  Foot Ulcers", "comments": "4 pages, 6 figures and 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces the Diabetic Foot Ulcers dataset (DFUC2021) for\nanalysis of pathology, focusing on infection and ischaemia. We describe the\ndata preparation of DFUC2021 for ground truth annotation, data curation and\ndata analysis. The final release of DFUC2021 consists of 15,683 DFU patches,\nwith 5,955 training, 5,734 for testing and 3,994 unlabeled DFU patches. The\nground truth labels are four classes, i.e. control, infection, ischaemia and\nboth conditions. We curate the dataset using image hashing techniques and\nanalyse the separability using UMAP projection. We benchmark the performance of\nfive key backbones of deep learning, i.e. VGG16, ResNet101, InceptionV3,\nDenseNet121 and EfficientNet on DFUC2021. We report the optimised results of\nthese key backbones with different strategies. Based on our observations, we\nconclude that EfficientNetB0 with data augmentation and transfer learning\nprovided the best results for multi-class (4-class) classification with\nmacro-average Precision, Recall and F1-score of 0.57, 0.62 and 0.55,\nrespectively. In ischaemia and infection recognition, when trained on\none-versus-all, EfficientNetB0 achieved comparable results with the state of\nthe art. Finally, we interpret the results with statistical analysis and\nGrad-CAM visualisation.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 11:38:57 GMT"}, {"version": "v2", "created": "Mon, 21 Jun 2021 06:49:30 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Yap", "Moi Hoon", ""], ["Cassidy", "Bill", ""], ["Pappachan", "Joseph M.", ""], ["O'Shea", "Claire", ""], ["Gillespie", "David", ""], ["Reeves", "Neil", ""]]}, {"id": "2104.03078", "submitter": "Xiu Li", "authors": "Xiu Li, Jinli Suo, Weihang Zhang, Xin Yuan, Qionghai Dai", "title": "Universal and Flexible Optical Aberration Correction Using Deep-Prior\n  Based Deconvolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  High quality imaging usually requires bulky and expensive lenses to\ncompensate geometric and chromatic aberrations. This poses high constraints on\nthe optical hash or low cost applications. Although one can utilize algorithmic\nreconstruction to remove the artifacts of low-end lenses, the degeneration from\noptical aberrations is spatially varying and the computation has to trade off\nefficiency for performance. For example, we need to conduct patch-wise\noptimization or train a large set of local deep neural networks to achieve high\nreconstruction performance across the whole image. In this paper, we propose a\nPSF aware plug-and-play deep network, which takes the aberrant image and PSF\nmap as input and produces the latent high quality version via incorporating\nlens-specific deep priors, thus leading to a universal and flexible optical\naberration correction method. Specifically, we pre-train a base model from a\nset of diverse lenses and then adapt it to a given lens by quickly refining the\nparameters, which largely alleviates the time and memory consumption of model\nlearning. The approach is of high efficiency in both training and testing\nstages. Extensive results verify the promising applications of our proposed\napproach for compact low-end cameras.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 12:00:38 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Li", "Xiu", ""], ["Suo", "Jinli", ""], ["Zhang", "Weihang", ""], ["Yuan", "Xin", ""], ["Dai", "Qionghai", ""]]}, {"id": "2104.03097", "submitter": "Zhaoyang Huang", "authors": "Zhaoyang Huang, Xiaokun Pan, Runsen Xu, Yan Xu, Ka chun Cheung,\n  Guofeng Zhang, Hongsheng Li", "title": "LIFE: Lighting Invariant Flow Estimation", "comments": "Project page: https://drinkingcoder.github.io/publication/life/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We tackle the problem of estimating flow between two images with large\nlighting variations. Recent learning-based flow estimation frameworks have\nshown remarkable performance on image pairs with small displacement and\nconstant illuminations, but cannot work well on cases with large viewpoint\nchange and lighting variations because of the lack of pixel-wise flow\nannotations for such cases. We observe that via the Structure-from-Motion (SfM)\ntechniques, one can easily estimate relative camera poses between image pairs\nwith large viewpoint change and lighting variations. We propose a novel weakly\nsupervised framework LIFE to train a neural network for estimating accurate\nlighting-invariant flows between image pairs. Sparse correspondences are\nconventionally established via feature matching with descriptors encoding local\nimage contents. However, local image contents are inevitably ambiguous and\nerror-prone during the cross-image feature matching process, which hinders\ndownstream tasks. We propose to guide feature matching with the flows predicted\nby LIFE, which addresses the ambiguous matching by utilizing abundant context\ninformation in the image pairs. We show that LIFE outperforms previous flow\nlearning frameworks by large margins in challenging scenarios, consistently\nimproves feature matching, and benefits downstream tasks.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 12:43:40 GMT"}, {"version": "v2", "created": "Mon, 19 Apr 2021 09:13:32 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Huang", "Zhaoyang", ""], ["Pan", "Xiaokun", ""], ["Xu", "Runsen", ""], ["Xu", "Yan", ""], ["Cheung", "Ka chun", ""], ["Zhang", "Guofeng", ""], ["Li", "Hongsheng", ""]]}, {"id": "2104.03100", "submitter": "Xing Lan", "authors": "Xing Lan, Qinghao Hu, Jian Cheng", "title": "HIH: Towards More Accurate Face Alignment via Heatmap in Heatmap", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, heatmap regression models have become the mainstream in locating\nfacial landmarks. To keep computation affordable and reduce memory usage, the\nwhole procedure involves downsampling from the raw image to the output heatmap.\nHowever, how much impact will the quantization error introduced by downsampling\nbring? The problem is hardly systematically investigated among previous works.\nThis work fills the blank and we are the first to quantitatively analyze the\nnegative gain. The statistical results show the NME generated by quantization\nerror is even larger than 1/3 of the SOTA item, which is a serious obstacle for\nmaking a new breakthrough in face alignment. To compensate the impact of\nquantization effect, we propose a novel method, called Heatmap In Heatmap(HIH),\nwhich leverages two categories of heatmaps as label representation to encode\ncoordinate. And in HIH, the range of one heatmap represents a pixel of the\nother category of heatmap. Also, we even combine the face alignment with\nsolutions of other fields to make a comparison. Extensive experiments on\nvarious benchmarks show the feasibility of HIH and the superior performance\nthan other solutions. Moreover, the mean error reaches to 4.18 on WFLW, which\nexceeds SOTA a lot. Our source code are made publicly available at\nsupplementary material.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 12:53:37 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Lan", "Xing", ""], ["Hu", "Qinghao", ""], ["Cheng", "Jian", ""]]}, {"id": "2104.03106", "submitter": "Mingyang Shang", "authors": "Mingyang Shang and Dawei Xiang and Zhicheng Wang and Erjin Zhou", "title": "V2F-Net: Explicit Decomposition of Occluded Pedestrian Detection", "comments": "11 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Occlusion is very challenging in pedestrian detection. In this paper, we\npropose a simple yet effective method named V2F-Net, which explicitly\ndecomposes occluded pedestrian detection into visible region detection and full\nbody estimation. V2F-Net consists of two sub-networks: Visible region Detection\nNetwork (VDN) and Full body Estimation Network (FEN). VDN tries to localize\nvisible regions and FEN estimates full-body box on the basis of the visible\nbox. Moreover, to further improve the estimation of full body, we propose a\nnovel Embedding-based Part-aware Module (EPM). By supervising the visibility\nfor each part, the network is encouraged to extract features with essential\npart information. We experimentally show the effectiveness of V2F-Net by\nconducting several experiments on two challenging datasets. V2F-Net achieves\n5.85% AP gains on CrowdHuman and 2.24% MR-2 improvements on CityPersons\ncompared to FPN baseline. Besides, the consistent gain on both one-stage and\ntwo-stage detector validates the generalizability of our method.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 13:12:16 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Shang", "Mingyang", ""], ["Xiang", "Dawei", ""], ["Wang", "Zhicheng", ""], ["Zhou", "Erjin", ""]]}, {"id": "2104.03109", "submitter": "Yilin Liu", "authors": "Yilin Liu, Ke Xie, and Hui Huang", "title": "VGF-Net: Visual-Geometric Fusion Learning for Simultaneous Drone\n  Navigation and Height Mapping", "comments": "Accepted by CVM 2021", "journal-ref": "Graphical Models 2021", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The drone navigation requires the comprehensive understanding of both visual\nand geometric information in the 3D world. In this paper, we present a\nVisual-Geometric Fusion Network(VGF-Net), a deep network for the fusion\nanalysis of visual/geometric data and the construction of 2.5D height maps for\nsimultaneous drone navigation in novel environments. Given an initial rough\nheight map and a sequence of RGB images, our VGF-Net extracts the visual\ninformation of the scene, along with a sparse set of 3D keypoints that capture\nthe geometric relationship between objects in the scene. Driven by the data,\nVGF-Net adaptively fuses visual and geometric information, forming a unified\nVisual-Geometric Representation. This representation is fed to a new\nDirectional Attention Model(DAM), which helps enhance the visual-geometric\nobject relationship and propagates the informative data to dynamically refine\nthe height map and the corresponding keypoints. An entire end-to-end\ninformation fusion and mapping system is formed, demonstrating remarkable\nrobustness and high accuracy on the autonomous drone navigation across complex\nindoor and large-scale outdoor scenes. The dataset can be found in\nhttp://vcc.szu.edu.cn/research/2021/VGFNet.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 13:18:40 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Liu", "Yilin", ""], ["Xie", "Ke", ""], ["Huang", "Hui", ""]]}, {"id": "2104.03110", "submitter": "Atsuhiro Noguchi", "authors": "Atsuhiro Noguchi, Xiao Sun, Stephen Lin, Tatsuya Harada", "title": "Neural Articulated Radiance Field", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present Neural Articulated Radiance Field (NARF), a novel deformable 3D\nrepresentation for articulated objects learned from images. While recent\nadvances in 3D implicit representation have made it possible to learn models of\ncomplex objects, learning pose-controllable representations of articulated\nobjects remains a challenge, as current methods require 3D shape supervision\nand are unable to render appearance. In formulating an implicit representation\nof 3D articulated objects, our method considers only the rigid transformation\nof the most relevant object part in solving for the radiance field at each 3D\nlocation. In this way, the proposed method represents pose-dependent changes\nwithout significantly increasing the computational complexity. NARF is fully\ndifferentiable and can be trained from images with pose annotations. Moreover,\nthrough the use of an autoencoder, it can learn appearance variations over\nmultiple instances of an object class. Experiments show that the proposed\nmethod is efficient and can generalize well to novel poses. We make the code,\nmodel and demo available for research purposes at\nhttps://github.com/nogu-atsu/NARF\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 13:23:14 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Noguchi", "Atsuhiro", ""], ["Sun", "Xiao", ""], ["Lin", "Stephen", ""], ["Harada", "Tatsuya", ""]]}, {"id": "2104.03114", "submitter": "Shuiwang Li", "authors": "Shuiwang Li, Yuting Liu, Qijun Zhao, Ziliang Feng", "title": "Learning Residue-Aware Correlation Filters and Refining Scale Estimates\n  with the GrabCut for Real-Time UAV Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Unmanned aerial vehicle (UAV)-based tracking is attracting increasing\nattention and developing rapidly in applications such as agriculture, aviation,\nnavigation, transportation and public security. Recently, discriminative\ncorrelation filters (DCF)-based trackers have stood out in UAV tracking\ncommunity for their high efficiency and appealing robustness on a single CPU.\nHowever, due to limited onboard computation resources and other challenges the\nefficiency and accuracy of existing DCF-based approaches is still not\nsatisfying. In this paper, we explore using segmentation by the GrabCut to\nimprove the wildly adopted discriminative scale estimation in DCF-based\ntrackers, which, as a mater of fact, greatly impacts the precision and accuracy\nof the trackers since accumulated scale error degrades the appearance model as\nonline updating goes on. Meanwhile, inspired by residue representation, we\nexploit the residue nature inherent to videos and propose residue-aware\ncorrelation filters that show better convergence properties in filter learning.\nExtensive experiments are conducted on four UAV benchmarks, namely,\nUAV123@10fps, DTB70, UAVDT and Vistrone2018 (VisDrone2018-test-dev). The\nresults show that our method achieves state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 13:35:01 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Li", "Shuiwang", ""], ["Liu", "Yuting", ""], ["Zhao", "Qijun", ""], ["Feng", "Ziliang", ""]]}, {"id": "2104.03117", "submitter": "Soumya Tripathy", "authors": "Soumya Tripathy, Juho Kannala, Esa Rahtu", "title": "Single Source One Shot Reenactment using Weighted motion From Paired\n  Feature Points", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image reenactment is a task where the target object in the source image\nimitates the motion represented in the driving image. One of the most common\nreenactment tasks is face image animation. The major challenge in the current\nface reenactment approaches is to distinguish between facial motion and\nidentity. For this reason, the previous models struggle to produce high-quality\nanimations if the driving and source identities are different (cross-person\nreenactment). We propose a new (face) reenactment model that learns\nshape-independent motion features in a self-supervised setup. The motion is\nrepresented using a set of paired feature points extracted from the source and\ndriving images simultaneously. The model is generalised to multiple reenactment\ntasks including faces and non-face objects using only a single source image.\nThe extensive experiments show that the model faithfully transfers the driving\nmotion to the source while retaining the source identity intact.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 13:45:34 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Tripathy", "Soumya", ""], ["Kannala", "Juho", ""], ["Rahtu", "Esa", ""]]}, {"id": "2104.03130", "submitter": "Steven Guan", "authors": "Steven Guan, Ko-Tsung Hsu, Matthias Eyassu, and Parag V. Chitnis", "title": "Dense Dilated UNet: Deep Learning for 3D Photoacoustic Tomography Image\n  Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In photoacoustic tomography (PAT), the acoustic pressure waves produced by\noptical excitation are measured by an array of detectors and used to\nreconstruct an image. Sparse spatial sampling and limited-view detection are\ntwo common challenges faced in PAT. Reconstructing from incomplete data using\nstandard methods results in severe streaking artifacts and blurring. We propose\na modified convolutional neural network (CNN) architecture termed Dense\nDilation UNet (DD-UNet) for correcting artifacts in 3D PAT. The DD-Net\nleverages the benefits of dense connectivity and dilated convolutions to\nimprove CNN performance. We compare the proposed CNN in terms of image quality\nas measured by the multiscale structural similarity index metric to the Fully\nDense UNet (FD-UNet). Results demonstrate that the DD-Net consistently\noutperforms the FD-UNet and is able to more reliably reconstruct smaller image\nfeatures.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 14:01:48 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Guan", "Steven", ""], ["Hsu", "Ko-Tsung", ""], ["Eyassu", "Matthias", ""], ["Chitnis", "Parag V.", ""]]}, {"id": "2104.03133", "submitter": "Bo Zhang", "authors": "Bo Zhang and Li Niu and Liqing Zhang", "title": "Image Composition Assessment with Saliency-augmented Multi-pattern\n  Pooling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image composition assessment is crucial in aesthetic assessment, which aims\nto assess the overall composition quality of a given image. However, to the\nbest of our knowledge, there is neither dataset nor method specifically\nproposed for this task. In this paper, we contribute the first composition\nassessment dataset CADB with composition scores for each image provided by\nmultiple professional raters. Besides, we propose a composition assessment\nnetwork SAMP-Net with a novel Saliency-Augmented Multi-pattern Pooling (SAMP)\nmodule, which analyses visual layout from the perspectives of multiple\ncomposition patterns. We also leverage composition-relevant attributes to\nfurther boost the performance, and extend Earth Mover's Distance (EMD) loss to\nweighted EMD loss to eliminate the content bias. The experimental results show\nthat our SAMP-Net can perform more favorably than previous aesthetic assessment\napproaches and offer constructive composition suggestions.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 14:07:17 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Zhang", "Bo", ""], ["Niu", "Li", ""], ["Zhang", "Liqing", ""]]}, {"id": "2104.03135", "submitter": "Zhicheng Huang", "authors": "Zhicheng Huang, Zhaoyang Zeng, Yupan Huang, Bei Liu, Dongmei Fu,\n  Jianlong Fu", "title": "Seeing Out of tHe bOx: End-to-End Pre-training for Vision-Language\n  Representation Learning", "comments": "Accepted by CVPR2021 oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study joint learning of Convolutional Neural Network (CNN) and Transformer\nfor vision-language pre-training (VLPT) which aims to learn cross-modal\nalignments from millions of image-text pairs. State-of-the-art approaches\nextract salient image regions and align regions with words step-by-step. As\nregion-based visual features usually represent parts of an image, it is\nchallenging for existing vision-language models to fully understand the\nsemantics from paired natural languages. In this paper, we propose SOHO to \"See\nOut of tHe bOx\" that takes a whole image as input, and learns vision-language\nrepresentation in an end-to-end manner. SOHO does not require bounding box\nannotations which enables inference 10 times faster than region-based\napproaches. In particular, SOHO learns to extract comprehensive yet compact\nimage features through a visual dictionary (VD) that facilitates cross-modal\nunderstanding. VD is designed to represent consistent visual abstractions of\nsimilar semantics. It is updated on-the-fly and utilized in our proposed\npre-training task Masked Visual Modeling (MVM). We conduct experiments on four\nwell-established vision-language tasks by following standard VLPT settings. In\nparticular, SOHO achieves absolute gains of 2.0% R@1 score on MSCOCO text\nretrieval 5k test split, 1.5% accuracy on NLVR$^2$ test-P split, 6.7% accuracy\non SNLI-VE test split, respectively.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 14:07:20 GMT"}, {"version": "v2", "created": "Thu, 8 Apr 2021 01:03:43 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Huang", "Zhicheng", ""], ["Zeng", "Zhaoyang", ""], ["Huang", "Yupan", ""], ["Liu", "Bei", ""], ["Fu", "Dongmei", ""], ["Fu", "Jianlong", ""]]}, {"id": "2104.03149", "submitter": "Corentin Dancette", "authors": "Corentin Dancette, Remi Cadene, Damien Teney, Matthieu Cord", "title": "Beyond Question-Based Biases: Assessing Multimodal Shortcut Learning in\n  Visual Question Answering", "comments": "Code is available at https://github.com/cdancette/detect-shortcuts", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce an evaluation methodology for visual question answering (VQA) to\nbetter diagnose cases of shortcut learning. These cases happen when a model\nexploits spurious statistical regularities to produce correct answers but does\nnot actually deploy the desired behavior. There is a need to identify possible\nshortcuts in a dataset and assess their use before deploying a model in the\nreal world. The research community in VQA has focused exclusively on\nquestion-based shortcuts, where a model might, for example, answer \"What is the\ncolor of the sky\" with \"blue\" by relying mostly on the question-conditional\ntraining prior and give little weight to visual evidence. We go a step further\nand consider multimodal shortcuts that involve both questions and images. We\nfirst identify potential shortcuts in the popular VQA v2 training set by mining\ntrivial predictive rules such as co-occurrences of words and visual elements.\nWe then create VQA-CE, a new evaluation set made of CounterExamples i.e.\nquestions where the mined rules lead to incorrect answers. We use this new\nevaluation in a large-scale study of existing models. We demonstrate that even\nstate-of-the-art models perform poorly and that existing techniques to reduce\nbiases are largely ineffective in this context. Our findings suggest that past\nwork on question-based biases in VQA has only addressed one facet of a complex\nissue. The code for our method is available at\nhttps://github.com/cdancette/detect-shortcuts\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 14:28:22 GMT"}, {"version": "v2", "created": "Wed, 14 Apr 2021 09:53:36 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Dancette", "Corentin", ""], ["Cadene", "Remi", ""], ["Teney", "Damien", ""], ["Cord", "Matthieu", ""]]}, {"id": "2104.03164", "submitter": "Xin Ding", "authors": "Xin Ding and Yongwei Wang and Zuheng Xu and Z. Jane Wang and William\n  J. Welch", "title": "Distilling and Transferring Knowledge via cGAN-generated Samples for\n  Image Classification and Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Knowledge distillation (KD) has been actively studied for image\nclassification tasks in deep learning, aiming to improve the performance of a\nstudent model based on the knowledge from a teacher model. However, there have\nbeen very few efforts for applying KD in image regression with a scalar\nresponse, and there is no KD method applicable to both tasks. Moreover,\nexisting KD methods often require a practitioner to carefully choose or adjust\nthe teacher and student architectures, making these methods less scalable in\npractice. Furthermore, although KD is usually conducted in scenarios with\nlimited labeled data, very few techniques are developed to alleviate such data\ninsufficiency. To solve the above problems in an all-in-one manner, we propose\nin this paper a unified KD framework based on conditional generative\nadversarial networks (cGANs), termed cGAN-KD. Fundamentally different from\nexisting KD methods, cGAN-KD distills and transfers knowledge from a teacher\nmodel to a student model via cGAN-generated samples. This unique mechanism\nmakes cGAN-KD suitable for both classification and regression tasks, compatible\nwith other KD methods, and insensitive to the teacher and student\narchitectures. Also, benefiting from the recent advances in cGAN methodology\nand our specially designed subsampling and filtering procedures, cGAN-KD also\nperforms well when labeled data are scarce. An error bound of a student model\ntrained in the cGAN-KD framework is derived in this work, which theoretically\nexplains why cGAN-KD takes effect and guides the implementation of cGAN-KD in\npractice. Extensive experiments on CIFAR-10 and Tiny-ImageNet show that we can\nincorporate state-of-the-art KD methods into the cGAN-KD framework to reach a\nnew state of the art. Also, experiments on RC-49 and UTKFace demonstrate the\neffectiveness of cGAN-KD in image regression tasks, where existing KD methods\nare inapplicable.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 14:52:49 GMT"}, {"version": "v2", "created": "Sat, 1 May 2021 04:50:06 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Ding", "Xin", ""], ["Wang", "Yongwei", ""], ["Xu", "Zuheng", ""], ["Wang", "Z. Jane", ""], ["Welch", "William J.", ""]]}, {"id": "2104.03165", "submitter": "Alexander Wong", "authors": "Alexander Wong, James Ren Hou Lee, Hadi Rahmat-Khah, Ali Sabri, and\n  Amer Alaref", "title": "TB-Net: A Tailored, Self-Attention Deep Convolutional Neural Network\n  Design for Detection of Tuberculosis Cases from Chest X-ray Images", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tuberculosis (TB) remains a global health problem, and is the leading cause\nof death from an infectious disease. A crucial step in the treatment of\ntuberculosis is screening high risk populations and the early detection of the\ndisease, with chest x-ray (CXR) imaging being the most widely-used imaging\nmodality. As such, there has been significant recent interest in artificial\nintelligence-based TB screening solutions for use in resource-limited scenarios\nwhere there is a lack of trained healthcare workers with expertise in CXR\ninterpretation. Motivated by this pressing need and the recent recommendation\nby the World Health Organization (WHO) for the use of computer-aided diagnosis\nof TB, we introduce TB-Net, a self-attention deep convolutional neural network\ntailored for TB case screening. More specifically, we leveraged machine-driven\ndesign exploration to build a highly customized deep neural network\narchitecture with attention condensers. We conducted an explainability-driven\nperformance validation process to validate TB-Net's decision-making behaviour.\nExperiments on CXR data from a multi-national patient cohort showed that the\nproposed TB-Net is able to achieve accuracy/sensitivity/specificity of\n99.86%/100.0%/99.71%. Radiologist validation was conducted on select cases by\ntwo board-certified radiologists with over 10 and 19 years of experience,\nrespectively, and showed consistency between radiologist interpretation and\ncritical factors leveraged by TB-Net for TB case detection for the case where\nradiologists identified anomalies. While not a production-ready solution, we\nhope that the open-source release of TB-Net as part of the COVID-Net initiative\nwill support researchers, clinicians, and citizen data scientists in advancing\nthis field in the fight against this global public health crisis.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 14:09:05 GMT"}, {"version": "v2", "created": "Wed, 14 Apr 2021 00:09:11 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Wong", "Alexander", ""], ["Lee", "James Ren Hou", ""], ["Rahmat-Khah", "Hadi", ""], ["Sabri", "Ali", ""], ["Alaref", "Amer", ""]]}, {"id": "2104.03176", "submitter": "Lea M\\\"uller", "authors": "Lea M\\\"uller and Ahmed A. A. Osman and Siyu Tang and Chun-Hao P. Huang\n  and Michael J. Black", "title": "On Self-Contact and Human Pose", "comments": "Accepted in CVPR'21 (oral). Project page: https://tuch.is.tue.mpg.de/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  People touch their face 23 times an hour, they cross their arms and legs, put\ntheir hands on their hips, etc. While many images of people contain some form\nof self-contact, current 3D human pose and shape (HPS) regression methods\ntypically fail to estimate this contact. To address this, we develop new\ndatasets and methods that significantly improve human pose estimation with\nself-contact. First, we create a dataset of 3D Contact Poses (3DCP) containing\nSMPL-X bodies fit to 3D scans as well as poses from AMASS, which we refine to\nensure good contact. Second, we leverage this to create the Mimic-The-Pose\n(MTP) dataset of images, collected via Amazon Mechanical Turk, containing\npeople mimicking the 3DCP poses with selfcontact. Third, we develop a novel HPS\noptimization method, SMPLify-XMC, that includes contact constraints and uses\nthe known 3DCP body pose during fitting to create near ground-truth poses for\nMTP images. Fourth, for more image variety, we label a dataset of in-the-wild\nimages with Discrete Self-Contact (DSC) information and use another new\noptimization method, SMPLify-DC, that exploits discrete contacts during pose\noptimization. Finally, we use our datasets during SPIN training to learn a new\n3D human pose regressor, called TUCH (Towards Understanding Contact in Humans).\nWe show that the new self-contact training data significantly improves 3D human\npose estimates on withheld test data and existing datasets like 3DPW. Not only\ndoes our method improve results for self-contact poses, but it also improves\naccuracy for non-contact poses. The code and data are available for research\npurposes at https://tuch.is.tue.mpg.de.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 15:10:38 GMT"}, {"version": "v2", "created": "Thu, 8 Apr 2021 07:29:50 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["M\u00fcller", "Lea", ""], ["Osman", "Ahmed A. A.", ""], ["Tang", "Siyu", ""], ["Huang", "Chun-Hao P.", ""], ["Black", "Michael J.", ""]]}, {"id": "2104.03178", "submitter": "Vivek Singh Bawa", "authors": "Vivek Singh Bawa, Gurkirt Singh, Francis KapingA, Inna\n  Skarga-Bandurova, Elettra Oleari, Alice Leporini, Carmela Landolfo, Pengfei\n  Zhao, Xi Xiang, Gongning Luo, Kuanquan Wang, Liangzhi Li, Bowen Wang, Shang\n  Zhao, Li Li, Armando Stabile, Francesco Setti, Riccardo Muradore, Fabio\n  Cuzzolin", "title": "The SARAS Endoscopic Surgeon Action Detection (ESAD) dataset: Challenges\n  and methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  For an autonomous robotic system, monitoring surgeon actions and assisting\nthe main surgeon during a procedure can be very challenging. The challenges\ncome from the peculiar structure of the surgical scene, the greater similarity\nin appearance of actions performed via tools in a cavity compared to, say,\nhuman actions in unconstrained environments, as well as from the motion of the\nendoscopic camera. This paper presents ESAD, the first large-scale dataset\ndesigned to tackle the problem of surgeon action detection in endoscopic\nminimally invasive surgery. ESAD aims at contributing to increase the\neffectiveness and reliability of surgical assistant robots by realistically\ntesting their awareness of the actions performed by a surgeon. The dataset\nprovides bounding box annotation for 21 action classes on real endoscopic video\nframes captured during prostatectomy, and was used as the basis of a recent\nMIDL 2020 challenge. We also present an analysis of the dataset conducted using\nthe baseline model which was released as part of the challenge, and a\ndescription of the top performing models submitted to the challenge together\nwith the results they obtained. This study provides significant insight into\nwhat approaches can be effective and can be extended further. We believe that\nESAD will serve in the future as a useful benchmark for all researchers active\nin surgeon action detection and assistive robotics at large.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 15:11:51 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Bawa", "Vivek Singh", ""], ["Singh", "Gurkirt", ""], ["KapingA", "Francis", ""], ["Skarga-Bandurova", "Inna", ""], ["Oleari", "Elettra", ""], ["Leporini", "Alice", ""], ["Landolfo", "Carmela", ""], ["Zhao", "Pengfei", ""], ["Xiang", "Xi", ""], ["Luo", "Gongning", ""], ["Wang", "Kuanquan", ""], ["Li", "Liangzhi", ""], ["Wang", "Bowen", ""], ["Zhao", "Shang", ""], ["Li", "Li", ""], ["Stabile", "Armando", ""], ["Setti", "Francesco", ""], ["Muradore", "Riccardo", ""], ["Cuzzolin", "Fabio", ""]]}, {"id": "2104.03207", "submitter": "Soheyla Amirian", "authors": "Soheyla Amirian, Abolfazl Farahani, Hamid R. Arabnia, Khaled Rasheed,\n  Thiab R. Taha", "title": "The Use of Video Captioning for Fostering Physical Activity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Video Captioning is considered to be one of the most challenging problems in\nthe field of computer vision. Video Captioning involves the combination of\ndifferent deep learning models to perform object detection, action detection,\nand localization by processing a sequence of image frames. It is crucial to\nconsider the sequence of actions in a video in order to generate a meaningful\ndescription of the overall action event. A reliable, accurate, and real-time\nvideo captioning method can be used in many applications. However, this paper\nfocuses on one application: video captioning for fostering and facilitating\nphysical activities. In broad terms, the work can be considered to be assistive\ntechnology. Lack of physical activity appears to be increasingly widespread in\nmany nations due to many factors, the most important being the convenience that\ntechnology has provided in workplaces. The adopted sedentary lifestyle is\nbecoming a significant public health issue. Therefore, it is essential to\nincorporate more physical movements into our daily lives. Tracking one's daily\nphysical activities would offer a base for comparison with activities performed\nin subsequent days. With the above in mind, this paper proposes a video\ncaptioning framework that aims to describe the activities in a video and\nestimate a person's daily physical activity level. This framework could\npotentially help people trace their daily movements to reduce an inactive\nlifestyle's health risks. The work presented in this paper is still in its\ninfancy. The initial steps of the application are outlined in this paper. Based\non our preliminary research, this project has great merit.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 15:52:48 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Amirian", "Soheyla", ""], ["Farahani", "Abolfazl", ""], ["Arabnia", "Hamid R.", ""], ["Rasheed", "Khaled", ""], ["Taha", "Thiab R.", ""]]}, {"id": "2104.03214", "submitter": "Xiang Wang", "authors": "Xiang Wang, Shiwei Zhang, Zhiwu Qing, Yuanjie Shao, Changxin Gao and\n  Nong Sang", "title": "Self-Supervised Learning for Semi-Supervised Temporal Action Proposal", "comments": "Accepted by CVPR-2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Self-supervised learning presents a remarkable performance to utilize\nunlabeled data for various video tasks. In this paper, we focus on applying the\npower of self-supervised methods to improve semi-supervised action proposal\ngeneration. Particularly, we design an effective Self-supervised\nSemi-supervised Temporal Action Proposal (SSTAP) framework. The SSTAP contains\ntwo crucial branches, i.e., temporal-aware semi-supervised branch and\nrelation-aware self-supervised branch. The semi-supervised branch improves the\nproposal model by introducing two temporal perturbations, i.e., temporal\nfeature shift and temporal feature flip, in the mean teacher framework. The\nself-supervised branch defines two pretext tasks, including masked feature\nreconstruction and clip-order prediction, to learn the relation of temporal\nclues. By this means, SSTAP can better explore unlabeled videos, and improve\nthe discriminative abilities of learned action features. We extensively\nevaluate the proposed SSTAP on THUMOS14 and ActivityNet v1.3 datasets. The\nexperimental results demonstrate that SSTAP significantly outperforms\nstate-of-the-art semi-supervised methods and even matches fully-supervised\nmethods. Code is available at https://github.com/wangxiang1230/SSTAP.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 16:03:25 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Wang", "Xiang", ""], ["Zhang", "Shiwei", ""], ["Qing", "Zhiwu", ""], ["Shao", "Yuanjie", ""], ["Gao", "Changxin", ""], ["Sang", "Nong", ""]]}, {"id": "2104.03218", "submitter": "Luyang Luo", "authors": "Luyang Luo, Hao Chen, Yanning Zhou, Huangjing Lin, Pheng-Ann Pheng", "title": "OXnet: Omni-supervised Thoracic Disease Detection from Chest X-rays", "comments": "Accepted to MICCAI2021. Code is available at\n  https://github.com/LLYXC/OXnet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Chest X-ray (CXR) is the most typical diagnostic X-ray examination for\nscreening various thoracic diseases. Automatically localizing lesions from CXR\nis promising for alleviating radiologists' reading burden. However, CXR\ndatasets are often with massive image-level annotations and scarce lesion-level\nannotations, and more often, without annotations. Thus far, unifying different\nsupervision granularities to develop thoracic disease detection algorithms has\nnot been comprehensively addressed. In this paper, we present OXnet, the first\ndeep omni-supervised thoracic disease detection network to our best knowledge\nthat uses as much available supervision as possible for CXR diagnosis. We first\nintroduce supervised learning via a one-stage detection model. Then, we inject\na global classification head to the detection model and propose dual attention\nalignment to guide the global gradient to the local detection branch, which\nenables learning lesion detection from image-level annotations. We also impose\nintra-class compactness and inter-class separability with global prototype\nalignment to further enhance the global information learning. Moreover, we\nleverage a soft focal loss to distill the soft pseudo-labels of unlabeled data\ngenerated by a teacher model. Extensive experiments on a large-scale chest\nX-ray dataset show the proposed OXnet outperforms competitive methods with\nsignificant margins. Further, we investigate omni-supervision under various\nannotation granularities and corroborate OXnet is a promising choice to\nmitigate the plight of annotation shortage for medical image diagnosis.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 16:12:31 GMT"}, {"version": "v2", "created": "Thu, 8 Jul 2021 08:38:30 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Luo", "Luyang", ""], ["Chen", "Hao", ""], ["Zhou", "Yanning", ""], ["Lin", "Huangjing", ""], ["Pheng", "Pheng-Ann", ""]]}, {"id": "2104.03225", "submitter": "Luyang Luo", "authors": "Yanwen Li, Luyang Luo, Huangjing Lin, Hao Chen, Pheng-Ann Heng", "title": "Dual-Consistency Semi-Supervised Learning with Uncertainty\n  Quantification for COVID-19 Lesion Segmentation from CT Images", "comments": "Accepted to MICCAI2021. The first two authors contributed equally.\n  Code is available at https://github.com/poiuohke/UDC-Net", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The novel coronavirus disease 2019 (COVID-19) characterized by atypical\npneumonia has caused millions of deaths worldwide. Automatically segmenting\nlesions from chest Computed Tomography (CT) is a promising way to assist\ndoctors in COVID-19 screening, treatment planning, and follow-up monitoring.\nHowever, voxel-wise annotations are extremely expert-demanding and scarce,\nespecially when it comes to novel diseases, while an abundance of unlabeled\ndata could be available. To tackle the challenge of limited annotations, in\nthis paper, we propose an uncertainty-guided dual-consistency learning network\n(UDC-Net) for semi-supervised COVID-19 lesion segmentation from CT images.\nSpecifically, we present a dual-consistency learning scheme that simultaneously\nimposes image transformation equivalence and feature perturbation invariance to\neffectively harness the knowledge from unlabeled data. We then quantify the\nsegmentation uncertainty in two forms and employ them together to guide the\nconsistency regularization for more reliable unsupervised learning. Extensive\nexperiments showed that our proposed UDC-Net improves the fully supervised\nmethod by 6.3% in Dice and outperforms other competitive semi-supervised\napproaches by significant margins, demonstrating high potential in real-world\nclinical practice.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 16:23:35 GMT"}, {"version": "v2", "created": "Thu, 8 Jul 2021 08:47:49 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Li", "Yanwen", ""], ["Luo", "Luyang", ""], ["Lin", "Huangjing", ""], ["Chen", "Hao", ""], ["Heng", "Pheng-Ann", ""]]}, {"id": "2104.03255", "submitter": "Additya Popli", "authors": "Additya Popli, Saraansh Tandon, Joshua J. Engelsma, Naoyuki Onoe,\n  Atsushi Okubo, Anoop Namboodiri", "title": "A Unified Model for Fingerprint Authentication and Presentation Attack\n  Detection", "comments": "Accepted at IJCB2021; 12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Typical fingerprint recognition systems are comprised of a spoof detection\nmodule and a subsequent recognition module, running one after the other. In\nthis paper, we reformulate the workings of a typical fingerprint recognition\nsystem. In particular, we posit that both spoof detection and fingerprint\nrecognition are correlated tasks. Therefore, rather than performing the two\ntasks separately, we propose a joint model for spoof detection and matching to\nsimultaneously perform both tasks without compromising the accuracy of either\ntask. We demonstrate the capability of our joint model to obtain an\nauthentication accuracy (1:1 matching) of TAR = 100% @ FAR = 0.1% on the FVC\n2006 DB2A dataset while achieving a spoof detection ACE of 1.44% on the LiveDet\n2015 dataset, both maintaining the performance of stand-alone methods. In\npractice, this reduces the time and memory requirements of the fingerprint\nrecognition system by 50% and 40%, respectively; a significant advantage for\nrecognition systems running on resource-constrained devices and communication\nchannels. The project page for our work is available at\nhttps://www.bit.ly/ijcb2021-unified .\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 16:57:38 GMT"}, {"version": "v2", "created": "Sun, 4 Jul 2021 11:04:34 GMT"}, {"version": "v3", "created": "Fri, 23 Jul 2021 16:36:44 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Popli", "Additya", ""], ["Tandon", "Saraansh", ""], ["Engelsma", "Joshua J.", ""], ["Onoe", "Naoyuki", ""], ["Okubo", "Atsushi", ""], ["Namboodiri", "Anoop", ""]]}, {"id": "2104.03265", "submitter": "Luyang Luo", "authors": "Zhizhong Chai, Luyang Luo, Huangjing Lin, Hao Chen, Pheng-Ann Heng", "title": "Deep Semi-supervised Metric Learning with Dual Alignment for Cervical\n  Cancer Cell Detection", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  With availability of huge amounts of labeled data, deep learning has achieved\nunprecedented success in various object detection tasks. However, large-scale\nannotations for medical images are extremely challenging to be acquired due to\nthe high demand of labour and expertise. To address this difficult issue, in\nthis paper we propose a novel semi-supervised deep metric learning method to\neffectively leverage both labeled and unlabeled data with application to\ncervical cancer cell detection. Different from previous methods, our model\nlearns an embedding metric space and conducts dual alignment of semantic\nfeatures on both the proposal and prototype levels. First, on the proposal\nlevel, we generate pseudo labels for the unlabeled data to align the proposal\nfeatures with learnable class proxies derived from the labeled data.\nFurthermore, we align the prototypes generated from each mini-batch of labeled\nand unlabeled data to alleviate the influence of possibly noisy pseudo labels.\nMoreover, we adopt a memory bank to store the labeled prototypes and hence\nsignificantly enrich the metric learning information from larger batches. To\ncomprehensively validate the method, we construct a large-scale dataset for\nsemi-supervised cervical cancer cell detection for the first time, consisting\nof 240,860 cervical cell images in total. Extensive experiments show our\nproposed method outperforms other state-of-the-art semi-supervised approaches\nconsistently, demonstrating efficacy of deep semi-supervised metric learning\nwith dual alignment on improving cervical cancer cell detection performance.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 17:11:27 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Chai", "Zhizhong", ""], ["Luo", "Luyang", ""], ["Lin", "Huangjing", ""], ["Chen", "Hao", ""], ["Heng", "Pheng-Ann", ""]]}, {"id": "2104.03304", "submitter": "Hanwen Jiang", "authors": "Hanwen Jiang, Shaowei Liu, Jiashun Wang and Xiaolong Wang", "title": "Hand-Object Contact Consistency Reasoning for Human Grasps Generation", "comments": "Project page: https://hwjiang1510.github.io/GraspTTA/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  While predicting robot grasps with parallel jaw grippers have been well\nstudied and widely applied in robot manipulation tasks, the study on natural\nhuman grasp generation with a multi-finger hand remains a very challenging\nproblem. In this paper, we propose to generate human grasps given a 3D object\nin the world. Our key observation is that it is crucial to model the\nconsistency between the hand contact points and object contact regions. That\nis, we encourage the prior hand contact points to be close to the object\nsurface and the object common contact regions to be touched by the hand at the\nsame time. Based on the hand-object contact consistency, we design novel\nobjectives in training the human grasp generation model and also a new\nself-supervised task which allows the grasp generation network to be adjusted\neven during test time. Our experiments show significant improvement in human\ngrasp generation over state-of-the-art approaches by a large margin. More\ninterestingly, by optimizing the model during test time with the\nself-supervised task, it helps achieve larger gain on unseen and out-of-domain\nobjects. Project page: https://hwjiang1510.github.io/GraspTTA/\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 17:57:14 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Jiang", "Hanwen", ""], ["Liu", "Shaowei", ""], ["Wang", "Jiashun", ""], ["Wang", "Xiaolong", ""]]}, {"id": "2104.03308", "submitter": "Prune Truong", "authors": "Prune Truong and Martin Danelljan and Fisher Yu and Luc Van Gool", "title": "Warp Consistency for Unsupervised Learning of Dense Correspondences", "comments": "code: https://github.com/PruneTruong/DenseMatching", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The key challenge in learning dense correspondences lies in the lack of\nground-truth matches for real image pairs. While photometric consistency losses\nprovide unsupervised alternatives, they struggle with large appearance changes,\nwhich are ubiquitous in geometric and semantic matching tasks. Moreover,\nmethods relying on synthetic training pairs often suffer from poor\ngeneralisation to real data.\n  We propose Warp Consistency, an unsupervised learning objective for dense\ncorrespondence regression. Our objective is effective even in settings with\nlarge appearance and view-point changes. Given a pair of real images, we first\nconstruct an image triplet by applying a randomly sampled warp to one of the\noriginal images. We derive and analyze all flow-consistency constraints arising\nbetween the triplet. From our observations and empirical results, we design a\ngeneral unsupervised objective employing two of the derived constraints. We\nvalidate our warp consistency loss by training three recent dense\ncorrespondence networks for the geometric and semantic matching tasks. Our\napproach sets a new state-of-the-art on several challenging benchmarks,\nincluding MegaDepth, RobotCar and TSS. Code and models will be released at\nhttps://github.com/PruneTruong/DenseMatching.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 17:58:22 GMT"}, {"version": "v2", "created": "Thu, 8 Apr 2021 13:06:59 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Truong", "Prune", ""], ["Danelljan", "Martin", ""], ["Yu", "Fisher", ""], ["Van Gool", "Luc", ""]]}, {"id": "2104.03309", "submitter": "Aayush Bansal", "authors": "Zhiqiu Lin and Deva Ramanan and Aayush Bansal", "title": "Streaming Self-Training via Domain-Agnostic Unlabeled Images", "comments": "Project Page: https://www.cs.cmu.edu/~aayushb/SST/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present streaming self-training (SST) that aims to democratize the process\nof learning visual recognition models such that a non-expert user can define a\nnew task depending on their needs via a few labeled examples and minimal domain\nknowledge. Key to SST are two crucial observations: (1) domain-agnostic\nunlabeled images enable us to learn better models with a few labeled examples\nwithout any additional knowledge or supervision; and (2) learning is a\ncontinuous process and can be done by constructing a schedule of learning\nupdates that iterates between pre-training on novel segments of the streams of\nunlabeled data, and fine-tuning on the small and fixed labeled dataset. This\nallows SST to overcome the need for a large number of domain-specific labeled\nand unlabeled examples, exorbitant computational resources, and\ndomain/task-specific knowledge. In this setting, classical semi-supervised\napproaches require a large amount of domain-specific labeled and unlabeled\nexamples, immense resources to process data, and expert knowledge of a\nparticular task. Due to these reasons, semi-supervised learning has been\nrestricted to a few places that can house required computational and human\nresources. In this work, we overcome these challenges and demonstrate our\nfindings for a wide range of visual recognition tasks including fine-grained\nimage classification, surface normal estimation, and semantic segmentation. We\nalso demonstrate our findings for diverse domains including medical, satellite,\nand agricultural imagery, where there does not exist a large amount of labeled\nor unlabeled data.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 17:58:39 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Lin", "Zhiqiu", ""], ["Ramanan", "Deva", ""], ["Bansal", "Aayush", ""]]}, {"id": "2104.03310", "submitter": "Hung-Yu Tseng", "authors": "Hung-Yu Tseng, Lu Jiang, Ce Liu, Ming-Hsuan Yang, Weilong Yang", "title": "Regularizing Generative Adversarial Networks under Limited Data", "comments": "CVPR 2021. Project Page: https://hytseng0509.github.io/lecam-gan\n  Code: https://github.com/google/lecam-gan", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have witnessed the rapid progress of generative adversarial\nnetworks (GANs). However, the success of the GAN models hinges on a large\namount of training data. This work proposes a regularization approach for\ntraining robust GAN models on limited data. We theoretically show a connection\nbetween the regularized loss and an f-divergence called LeCam-divergence, which\nwe find is more robust under limited training data. Extensive experiments on\nseveral benchmark datasets demonstrate that the proposed regularization scheme\n1) improves the generalization performance and stabilizes the learning dynamics\nof GAN models under limited training data, and 2) complements the recent data\naugmentation methods. These properties facilitate training GAN models to\nachieve state-of-the-art performance when only limited training data of the\nImageNet benchmark is available.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 17:59:06 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Tseng", "Hung-Yu", ""], ["Jiang", "Lu", ""], ["Liu", "Ce", ""], ["Yang", "Ming-Hsuan", ""], ["Yang", "Weilong", ""]]}, {"id": "2104.03311", "submitter": "Chuang Gan", "authors": "Zhiao Huang, Yuanming Hu, Tao Du, Siyuan Zhou, Hao Su, Joshua B.\n  Tenenbaum, Chuang Gan", "title": "PlasticineLab: A Soft-Body Manipulation Benchmark with Differentiable\n  Physics", "comments": "Accepted to ICLR 2021 as a spotlight presentation. Project page:\n  http://plasticinelab.csail.mit.edu/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.GR cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Simulated virtual environments serve as one of the main driving forces behind\ndeveloping and evaluating skill learning algorithms. However, existing\nenvironments typically only simulate rigid body physics. Additionally, the\nsimulation process usually does not provide gradients that might be useful for\nplanning and control optimizations. We introduce a new differentiable physics\nbenchmark called PasticineLab, which includes a diverse collection of soft body\nmanipulation tasks. In each task, the agent uses manipulators to deform the\nplasticine into the desired configuration. The underlying physics engine\nsupports differentiable elastic and plastic deformation using the DiffTaichi\nsystem, posing many under-explored challenges to robotic agents. We evaluate\nseveral existing reinforcement learning (RL) methods and gradient-based methods\non this benchmark. Experimental results suggest that 1) RL-based approaches\nstruggle to solve most of the tasks efficiently; 2) gradient-based approaches,\nby optimizing open-loop control sequences with the built-in differentiable\nphysics engine, can rapidly find a solution within tens of iterations, but\nstill fall short on multi-stage tasks that require long-term planning. We\nexpect that PlasticineLab will encourage the development of novel algorithms\nthat combine differentiable physics and RL for more complex physics-based skill\nlearning tasks.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 17:59:23 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Huang", "Zhiao", ""], ["Hu", "Yuanming", ""], ["Du", "Tao", ""], ["Zhou", "Siyuan", ""], ["Su", "Hao", ""], ["Tenenbaum", "Joshua B.", ""], ["Gan", "Chuang", ""]]}, {"id": "2104.03313", "submitter": "Jinlong Yang", "authors": "Shunsuke Saito, Jinlong Yang, Qianli Ma, Michael J. Black", "title": "SCANimate: Weakly Supervised Learning of Skinned Clothed Avatar Networks", "comments": "CVPR 2021 (oral). Project page: https://scanimate.is.tue.mpg.de", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present SCANimate, an end-to-end trainable framework that takes raw 3D\nscans of a clothed human and turns them into an animatable avatar. These\navatars are driven by pose parameters and have realistic clothing that moves\nand deforms naturally. SCANimate does not rely on a customized mesh template or\nsurface mesh registration. We observe that fitting a parametric 3D body model,\nlike SMPL, to a clothed human scan is tractable while surface registration of\nthe body topology to the scan is often not, because clothing can deviate\nsignificantly from the body shape. We also observe that articulated\ntransformations are invertible, resulting in geometric cycle consistency in the\nposed and unposed shapes. These observations lead us to a weakly supervised\nlearning method that aligns scans into a canonical pose by disentangling\narticulated deformations without template-based surface registration.\nFurthermore, to complete missing regions in the aligned scans while modeling\npose-dependent deformations, we introduce a locally pose-aware implicit\nfunction that learns to complete and model geometry with learned pose\ncorrectives. In contrast to commonly used global pose embeddings, our local\npose conditioning significantly reduces long-range spurious correlations and\nimproves generalization to unseen poses, especially when training data is\nlimited. Our method can be applied to pose-aware appearance modeling to\ngenerate a fully textured avatar. We demonstrate our approach on various\nclothing types with different amounts of training data, outperforming existing\nsolutions and other variants in terms of fidelity and generality in every\nsetting. The code is available at https://scanimate.is.tue.mpg.de.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 17:59:58 GMT"}, {"version": "v2", "created": "Thu, 8 Apr 2021 06:31:02 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Saito", "Shunsuke", ""], ["Yang", "Jinlong", ""], ["Ma", "Qianli", ""], ["Black", "Michael J.", ""]]}, {"id": "2104.03337", "submitter": "Soheyla Amirian", "authors": "Soheyla Amirian, Khaled Rasheed, Thiab R. Taha, Hamid R. Arabnia", "title": "Automatic Generation of Descriptive Titles for Video Clips Using Deep\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Over the last decade, the use of Deep Learning in many applications produced\nresults that are comparable to and in some cases surpassing human expert\nperformance. The application domains include diagnosing diseases, finance,\nagriculture, search engines, robot vision, and many others. In this paper, we\nare proposing an architecture that utilizes image/video captioning methods and\nNatural Language Processing systems to generate a title and a concise abstract\nfor a video. Such a system can potentially be utilized in many application\ndomains, including, the cinema industry, video search engines, security\nsurveillance, video databases/warehouses, data centers, and others. The\nproposed system functions and operates as followed: it reads a video;\nrepresentative image frames are identified and selected; the image frames are\ncaptioned; NLP is applied to all generated captions together with text\nsummarization; and finally, a title and an abstract are generated for the\nvideo. All functions are performed automatically. Preliminary results are\nprovided in this paper using publicly available datasets. This paper is not\nconcerned about the efficiency of the system at the execution time. We hope to\nbe able to address execution efficiency issues in our subsequent publications.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 18:14:18 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Amirian", "Soheyla", ""], ["Rasheed", "Khaled", ""], ["Taha", "Thiab R.", ""], ["Arabnia", "Hamid R.", ""]]}, {"id": "2104.03344", "submitter": "Kuniaki Saito", "authors": "Kuniaki Saito and Kate Saenko", "title": "OVANet: One-vs-All Network for Universal Domain Adaptation", "comments": "https://github.com/VisionLearningGroup/OVANet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Universal Domain Adaptation (UNDA) aims to handle both domain-shift and\ncategory-shift between two datasets, where the main challenge is to transfer\nknowledge while rejecting unknown classes which are absent in the labeled\nsource data but present in the unlabeled target data. Existing methods manually\nset a threshold to reject unknown samples based on validation or a pre-defined\nratio of unknown samples, but this strategy is not practical. In this paper, we\npropose a method to learn the threshold using source samples and to adapt it to\nthe target domain. Our idea is that a minimum inter-class distance in the\nsource domain should be a good threshold to decide between known or unknown in\nthe target. To learn the inter-and intra-class distance, we propose to train a\none-vs-all classifier for each class using labeled source data. Then, we adapt\nthe open-set classifier to the target domain by minimizing class entropy. The\nresulting framework is the simplest of all baselines of UNDA and is insensitive\nto the value of a hyper-parameter yet outperforms baselines with a large\nmargin.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 18:36:31 GMT"}, {"version": "v2", "created": "Sun, 18 Apr 2021 20:48:39 GMT"}, {"version": "v3", "created": "Tue, 27 Apr 2021 18:35:12 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Saito", "Kuniaki", ""], ["Saenko", "Kate", ""]]}, {"id": "2104.03361", "submitter": "Javier Gonzalez-Trejo", "authors": "Javier A. Gonz\\'alez-Trejo, Diego A. Mercado-Ravell", "title": "Monitoring Social-distance in Wide Areas during Pandemics: a Density Map\n  and Segmentation Approach", "comments": "Video: https://youtu.be/TwzBMKg7h_U", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the relaxation of the containment measurements around the globe,\nmonitoring the social distancing in crowded public places is of grate\nimportance to prevent a new massive wave of COVID-19 infections. Recent works\nin that matter have limited themselves by detecting social distancing in\ncorridors up to small crowds by detecting each person individually considering\nthe full body in the image. In this work, we propose a new framework for\nmonitoring the social-distance using end-to-end Deep Learning, to detect crowds\nviolating the social-distance in wide areas where important occlusions may be\npresent. Our framework consists in the creation of a new ground truth based on\nthe ground truth density maps and the proposal of two different solutions, a\ndensity-map-based and a segmentation-based, to detect the crowds violating the\nsocial-distance constrain. We assess the results of both approaches by using\nthe generated ground truth from the PET2009 and CityStreet datasets. We show\nthat our framework performs well at providing the zones where people are not\nfollowing the social-distance even when heavily occluded or far away from one\ncamera.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 19:26:26 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Gonz\u00e1lez-Trejo", "Javier A.", ""], ["Mercado-Ravell", "Diego A.", ""]]}, {"id": "2104.03362", "submitter": "Juan-Ting Lin", "authors": "R\\'emi Pautrat, Juan-Ting Lin, Viktor Larsson, Martin R. Oswald, Marc\n  Pollefeys", "title": "SOLD2: Self-supervised Occlusion-aware Line Description and Detection", "comments": "17 pages, Accepted at CVPR 2021 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compared to feature point detection and description, detecting and matching\nline segments offer additional challenges. Yet, line features represent a\npromising complement to points for multi-view tasks. Lines are indeed\nwell-defined by the image gradient, frequently appear even in poorly textured\nareas and offer robust structural cues. We thus hereby introduce the first\njoint detection and description of line segments in a single deep network.\nThanks to a self-supervised training, our method does not require any annotated\nline labels and can therefore generalize to any dataset. Our detector offers\nrepeatable and accurate localization of line segments in images, departing from\nthe wireframe parsing approach. Leveraging the recent progresses in descriptor\nlearning, our proposed line descriptor is highly discriminative, while\nremaining robust to viewpoint changes and occlusions. We evaluate our approach\nagainst previous line detection and description methods on several multi-view\ndatasets created with homographic warps as well as real-world viewpoint\nchanges. Our full pipeline yields higher repeatability, localization accuracy\nand matching metrics, and thus represents a first step to bridge the gap with\nlearned feature points methods. Code and trained weights are available at\nhttps://github.com/cvg/SOLD2.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 19:27:17 GMT"}, {"version": "v2", "created": "Fri, 9 Apr 2021 07:38:38 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Pautrat", "R\u00e9mi", ""], ["Lin", "Juan-Ting", ""], ["Larsson", "Viktor", ""], ["Oswald", "Martin R.", ""], ["Pollefeys", "Marc", ""]]}, {"id": "2104.03366", "submitter": "Md Imran Hossen", "authors": "Md Imran Hossen, Yazhou Tu, Md Fazle Rabby, Md Nazmul Islam, Hui Cao\n  and Xiali Hei", "title": "An Object Detection based Solver for Google's Image reCAPTCHA v2", "comments": "Accepted at the 23rd International Symposium on Research in Attacks,\n  Intrusions and Defenses (RAID 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Previous work showed that reCAPTCHA v2's image challenges could be solved by\nautomated programs armed with Deep Neural Network (DNN) image classifiers and\nvision APIs provided by off-the-shelf image recognition services. In response\nto emerging threats, Google has made significant updates to its image reCAPTCHA\nv2 challenges that can render the prior approaches ineffective to a great\nextent. In this paper, we investigate the robustness of the latest version of\nreCAPTCHA v2 against advanced object detection based solvers. We propose a\nfully automated object detection based system that breaks the most advanced\nchallenges of reCAPTCHA v2 with an online success rate of 83.25%, the highest\nsuccess rate to date, and it takes only 19.93 seconds (including network\ndelays) on average to crack a challenge. We also study the updated security\nfeatures of reCAPTCHA v2, such as anti-recognition mechanisms, improved\nanti-bot detection techniques, and adjustable security preferences. Our\nextensive experiments show that while these security features can provide some\nresistance against automated attacks, adversaries can still bypass most of\nthem. Our experimental findings indicate that the recent advances in object\ndetection technologies pose a severe threat to the security of image captcha\ndesigns relying on simple object detection as their underlying AI problem.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 19:35:33 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Hossen", "Md Imran", ""], ["Tu", "Yazhou", ""], ["Rabby", "Md Fazle", ""], ["Islam", "Md Nazmul", ""], ["Cao", "Hui", ""], ["Hei", "Xiali", ""]]}, {"id": "2104.03393", "submitter": "Eric Upschulte", "authors": "Eric Upschulte, Stefan Harmeling, Katrin Amunts and Timo Dickscheid", "title": "Contour Proposal Networks for Biomedical Instance Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We present a conceptually simple framework for object instance segmentation\ncalled Contour Proposal Network (CPN), which detects possibly overlapping\nobjects in an image while simultaneously fitting closed object contours using\nan interpretable, fixed-sized representation based on Fourier Descriptors. The\nCPN can incorporate state of the art object detection architectures as backbone\nnetworks into a single-stage instance segmentation model that can be trained\nend-to-end. We construct CPN models with different backbone networks, and apply\nthem to instance segmentation of cells in datasets from different modalities.\nIn our experiments, we show CPNs that outperform U-Nets and Mask R-CNNs in\ninstance segmentation accuracy, and present variants with execution times\nsuitable for real-time applications. The trained models generalize well across\ndifferent domains of cell types. Since the main assumption of the framework are\nclosed object contours, it is applicable to a wide range of detection problems\nalso outside the biomedical domain. An implementation of the model architecture\nin PyTorch is freely available.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 21:00:45 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Upschulte", "Eric", ""], ["Harmeling", "Stefan", ""], ["Amunts", "Katrin", ""], ["Dickscheid", "Timo", ""]]}, {"id": "2104.03414", "submitter": "Youngeun Kim", "authors": "Youngeun Kim, Yeshwanth Venkatesha and Priyadarshini Panda", "title": "PrivateSNN: Fully Privacy-Preserving Spiking Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  How can we bring both privacy and energy-efficiency to a neural system on\nedge devices? In this paper, we propose PrivateSNN, which aims to build\nlow-power Spiking Neural Networks (SNNs) from a pre-trained ANN model without\nleaking sensitive information contained in a dataset. Here, we tackle two types\nof leakage problems: 1) Data leakage caused when the networks access real\ntraining data during an ANN-SNN conversion process. 2) Class leakage is the\nconcept of leakage caused when class-related features can be reconstructed from\nnetwork parameters. In order to address the data leakage issue, we generate\nsynthetic images from the pre-trained ANNs and convert ANNs to SNNs using\ngenerated images. However, converted SNNs are still vulnerable with respect to\nthe class leakage since the weight parameters have the same (or scaled) value\nwith respect to ANN parameters. Therefore, we encrypt SNN weights by training\nSNNs with a temporal spike-based learning rule. Updating weight parameters with\ntemporal data makes networks difficult to be interpreted in the spatial domain.\nWe observe that the encrypted PrivateSNN can be implemented not only without\nthe huge performance drop (less than ~5%) but also with significant\nenergy-efficiency gain (about x60 compared to the standard ANN). We conduct\nextensive experiments on various datasets including CIFAR10, CIFAR100, and\nTinyImageNet, highlighting the importance of privacy-preserving SNN training.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 22:14:02 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Kim", "Youngeun", ""], ["Venkatesha", "Yeshwanth", ""], ["Panda", "Priyadarshini", ""]]}, {"id": "2104.03419", "submitter": "Ali Almadan", "authors": "Ali Almadan and Ajita Rattani", "title": "Towards On-Device Face Recognition in Body-worn Cameras", "comments": "6 pages", "journal-ref": "IEEE International Workshop on Biometrics and Forensics (IWBF)\n  2021", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Face recognition technology related to recognizing identities is widely\nadopted in intelligence gathering, law enforcement, surveillance, and consumer\napplications. Recently, this technology has been ported to smartphones and\nbody-worn cameras (BWC). Face recognition technology in body-worn cameras is\nused for surveillance, situational awareness, and keeping the officer safe.\nOnly a handful of academic studies exist in face recognition using the\nbody-worn camera. A recent study has assembled BWCFace facial image dataset\nacquired using a body-worn camera and evaluated the ResNet-50 model for face\nidentification. However, for real-time inference in resource constraint\nbody-worn cameras and privacy concerns involving facial images, on-device face\nrecognition is required. To this end, this study evaluates lightweight\nMobileNet-V2, EfficientNet-B0, LightCNN-9 and LightCNN-29 models for face\nidentification using body-worn camera. Experiments are performed on a publicly\navailable BWCface dataset. The real-time inference is evaluated on three mobile\ndevices. The comparative analysis is done with heavy-weight VGG-16 and\nResNet-50 models along with six hand-crafted features to evaluate the trade-off\nbetween the performance and model size. Experimental results suggest the\ndifference in maximum rank-1 accuracy of lightweight LightCNN-29 over\nbest-performing ResNet-50 is \\textbf{1.85\\%} and the reduction in model\nparameters is \\textbf{23.49M}. Most of the deep models obtained similar\nperformances at rank-5 and rank-10. The inference time of LightCNNs is 2.1x\nfaster than other models on mobile devices. The least performance difference of\n\\textbf{14\\%} is noted between LightCNN-29 and Local Phase Quantization (LPQ)\ndescriptor at rank-1. In most of the experimental settings, lightweight\nLightCNN models offered the best trade-off between accuracy and the model size\nin comparison to most of the models.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 22:24:57 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Almadan", "Ali", ""], ["Rattani", "Ajita", ""]]}, {"id": "2104.03424", "submitter": "Adam Harley", "authors": "Adam W. Harley, Yiming Zuo, Jing Wen, Ayush Mangal, Shubhankar Potdar,\n  Ritwick Chaudhry, Katerina Fragkiadaki", "title": "Track, Check, Repeat: An EM Approach to Unsupervised Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an unsupervised method for detecting and tracking moving objects\nin 3D, in unlabelled RGB-D videos. The method begins with classic handcrafted\ntechniques for segmenting objects using motion cues: we estimate optical flow\nand camera motion, and conservatively segment regions that appear to be moving\nindependently of the background. Treating these initial segments as\npseudo-labels, we learn an ensemble of appearance-based 2D and 3D detectors,\nunder heavy data augmentation. We use this ensemble to detect new instances of\nthe \"moving\" type, even if they are not moving, and add these as new\npseudo-labels. Our method is an expectation-maximization algorithm, where in\nthe expectation step we fire all modules and look for agreement among them, and\nin the maximization step we re-train the modules to improve this agreement. The\nconstraint of ensemble agreement helps combat contamination of the generated\npseudo-labels (during the E step), and data augmentation helps the modules\ngeneralize to yet-unlabelled data (during the M step). We compare against\nexisting unsupervised object discovery and tracking methods, using challenging\nvideos from CATER and KITTI, and show strong improvements over the\nstate-of-the-art.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 22:51:39 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Harley", "Adam W.", ""], ["Zuo", "Yiming", ""], ["Wen", "Jing", ""], ["Mangal", "Ayush", ""], ["Potdar", "Shubhankar", ""], ["Chaudhry", "Ritwick", ""], ["Fragkiadaki", "Katerina", ""]]}, {"id": "2104.03427", "submitter": "Chaitanya Kaul", "authors": "Chaitanya Kaul, Nick Pears, Suresh Manandhar", "title": "FatNet: A Feature-attentive Network for 3D Point Cloud Processing", "comments": "Published at ICPR 2020 (Oral). arXiv admin note: substantial text\n  overlap with arXiv:1905.07650", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The application of deep learning to 3D point clouds is challenging due to its\nlack of order. Inspired by the point embeddings of PointNet and the edge\nembeddings of DGCNNs, we propose three improvements to the task of point cloud\nanalysis. First, we introduce a novel feature-attentive neural network layer, a\nFAT layer, that combines both global point-based features and local edge-based\nfeatures in order to generate better embeddings. Second, we find that applying\nthe same attention mechanism across two different forms of feature map\naggregation, max pooling and average pooling, gives better performance than\neither alone. Third, we observe that residual feature reuse in this setting\npropagates information more effectively between the layers, and makes the\nnetwork easier to train. Our architecture achieves state-of-the-art results on\nthe task of point cloud classification, as demonstrated on the ModelNet40\ndataset, and an extremely competitive performance on the ShapeNet part\nsegmentation challenge.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 23:13:56 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Kaul", "Chaitanya", ""], ["Pears", "Nick", ""], ["Manandhar", "Suresh", ""]]}, {"id": "2104.03435", "submitter": "Sethuraman Sankaran", "authors": "Sethuraman Sankaran, David Yang, Ser-Nam Lim", "title": "Multimodal Fusion Refiner Networks", "comments": "11 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Tasks that rely on multi-modal information typically include a fusion module\nthat combines information from different modalities. In this work, we develop a\nRefiner Fusion Network (ReFNet) that enables fusion modules to combine strong\nunimodal representation with strong multimodal representations. ReFNet combines\nthe fusion network with a decoding/defusing module, which imposes a\nmodality-centric responsibility condition. This approach addresses a big gap in\nexisting multimodal fusion frameworks by ensuring that both unimodal and fused\nrepresentations are strongly encoded in the latent fusion space. We demonstrate\nthat the Refiner Fusion Network can improve upon performance of powerful\nbaseline fusion modules such as multimodal transformers. The refiner network\nenables inducing graphical representations of the fused embeddings in the\nlatent space, which we prove under certain conditions and is supported by\nstrong empirical results in the numerical experiments. These graph structures\nare further strengthened by combining the ReFNet with a Multi-Similarity\ncontrastive loss function. The modular nature of Refiner Fusion Network lends\nitself to be combined with different fusion architectures easily, and in\naddition, the refiner step can be applied for pre-training on unlabeled\ndatasets, thus leveraging unsupervised data towards improving performance. We\ndemonstrate the power of Refiner Fusion Networks on three datasets, and further\nshow that they can maintain performance with only a small fraction of labeled\ndata.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 00:02:01 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Sankaran", "Sethuraman", ""], ["Yang", "David", ""], ["Lim", "Ser-Nam", ""]]}, {"id": "2104.03437", "submitter": "He Wang", "authors": "Yijia Weng, He Wang, Qiang Zhou, Yuzhe Qin, Yueqi Duan, Qingnan Fan,\n  Baoquan Chen, Hao Su, Leonidas J. Guibas", "title": "CAPTRA: CAtegory-level Pose Tracking for Rigid and Articulated Objects\n  from Point Clouds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work, we tackle the problem of category-level online pose tracking of\nobjects from point cloud sequences. For the first time, we propose a unified\nframework that can handle 9DoF pose tracking for novel rigid object instances\nas well as per-part pose tracking for articulated objects from known\ncategories. Here the 9DoF pose, comprising 6D pose and 3D size, is equivalent\nto a 3D amodal bounding box representation with free 6D pose. Given the depth\npoint cloud at the current frame and the estimated pose from the last frame,\nour novel end-to-end pipeline learns to accurately update the pose. Our\npipeline is composed of three modules: 1) a pose canonicalization module that\nnormalizes the pose of the input depth point cloud; 2) RotationNet, a module\nthat directly regresses small interframe delta rotations; and 3) CoordinateNet,\na module that predicts the normalized coordinates and segmentation, enabling\nanalytical computation of the 3D size and translation. Leveraging the small\npose regime in the pose-canonicalized point clouds, our method integrates the\nbest of both worlds by combining dense coordinate prediction and direct\nrotation regression, thus yielding an end-to-end differentiable pipeline\noptimized for 9DoF pose accuracy (without using non-differentiable RANSAC). Our\nextensive experiments demonstrate that our method achieves new state-of-the-art\nperformance on category-level rigid object pose (NOCS-REAL275) and articulated\nobject pose benchmarks (SAPIEN , BMVC) at the fastest FPS ~12.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 00:14:58 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Weng", "Yijia", ""], ["Wang", "He", ""], ["Zhou", "Qiang", ""], ["Qin", "Yuzhe", ""], ["Duan", "Yueqi", ""], ["Fan", "Qingnan", ""], ["Chen", "Baoquan", ""], ["Su", "Hao", ""], ["Guibas", "Leonidas J.", ""]]}, {"id": "2104.03438", "submitter": "Zi Wang", "authors": "Zi Wang, Chengcheng Li, Xiangyang Wang", "title": "Convolutional Neural Network Pruning with Structural Redundancy\n  Reduction", "comments": "Accepted to CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Convolutional neural network (CNN) pruning has become one of the most\nsuccessful network compression approaches in recent years. Existing works on\nnetwork pruning usually focus on removing the least important filters in the\nnetwork to achieve compact architectures. In this study, we claim that\nidentifying structural redundancy plays a more essential role than finding\nunimportant filters, theoretically and empirically. We first statistically\nmodel the network pruning problem in a redundancy reduction perspective and\nfind that pruning in the layer(s) with the most structural redundancy\noutperforms pruning the least important filters across all layers. Based on\nthis finding, we then propose a network pruning approach that identifies\nstructural redundancy of a CNN and prunes filters in the selected layer(s) with\nthe most redundancy. Experiments on various benchmark network architectures and\ndatasets show that our proposed approach significantly outperforms the previous\nstate-of-the-art.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 00:16:24 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Wang", "Zi", ""], ["Li", "Chengcheng", ""], ["Wang", "Xiangyang", ""]]}, {"id": "2104.03488", "submitter": "Loris Nanni", "authors": "Loris Nanni, Stefano Ghidoni, Sheryl Brahnam", "title": "Deep Features for training Support Vector Machine", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Features play a crucial role in computer vision. Initially designed to detect\nsalient elements by means of handcrafted algorithms, features are now often\nlearned by different layers in Convolutional Neural Networks (CNNs). This paper\ndevelops a generic computer vision system based on features extracted from\ntrained CNNs. Multiple learned features are combined into a single structure to\nwork on different image classification tasks. The proposed system was\nexperimentally derived by testing several approaches for extracting features\nfrom the inner layers of CNNs and using them as inputs to SVMs that are then\ncombined by sum rule. Dimensionality reduction techniques are used to reduce\nthe high dimensionality of inner layers. The resulting vision system is shown\nto significantly boost the performance of standard CNNs across a large and\ndiverse collection of image data sets. An ensemble of different topologies\nusing the same approach obtains state-of-the-art results on a virus data set.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 03:13:09 GMT"}, {"version": "v2", "created": "Mon, 28 Jun 2021 22:29:51 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Nanni", "Loris", ""], ["Ghidoni", "Stefano", ""], ["Brahnam", "Sheryl", ""]]}, {"id": "2104.03493", "submitter": "Ziqian Bai", "authors": "Ziqian Bai, Zhaopeng Cui, Xiaoming Liu, Ping Tan", "title": "Riggable 3D Face Reconstruction via In-Network Optimization", "comments": "CVPR2021. Code: https://github.com/zqbai-jeremy/INORig Camera Ready\n  Paper: https://zqbai-jeremy.github.io/files/INORig.pdf Camera Ready Supp:\n  https://zqbai-jeremy.github.io/files/INORig_supp.pdf", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a method for riggable 3D face reconstruction from\nmonocular images, which jointly estimates a personalized face rig and per-image\nparameters including expressions, poses, and illuminations. To achieve this\ngoal, we design an end-to-end trainable network embedded with a differentiable\nin-network optimization. The network first parameterizes the face rig as a\ncompact latent code with a neural decoder, and then estimates the latent code\nas well as per-image parameters via a learnable optimization. By estimating a\npersonalized face rig, our method goes beyond static reconstructions and\nenables downstream applications such as video retargeting. In-network\noptimization explicitly enforces constraints derived from the first principles,\nthus introduces additional priors than regression-based methods. Finally,\ndata-driven priors from deep learning are utilized to constrain the ill-posed\nmonocular setting and ease the optimization difficulty. Experiments demonstrate\nthat our method achieves SOTA reconstruction accuracy, reasonable robustness\nand generalization ability, and supports standard face rig applications.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 03:53:20 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Bai", "Ziqian", ""], ["Cui", "Zhaopeng", ""], ["Liu", "Xiaoming", ""], ["Tan", "Ping", ""]]}, {"id": "2104.03496", "submitter": "Elliott Skomski", "authors": "Elliott Skomski, Aaron Tuor, Andrew Avila, Lauren Phillips, Zachary\n  New, Henry Kvinge, Courtney D. Corley, and Nathan Hodas", "title": "Prototypical Region Proposal Networks for Few-Shot Localization and\n  Classification", "comments": "9 pages, 1 figure. Submitted to 4th Workshop on Meta-Learning at\n  NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently proposed few-shot image classification methods have generally\nfocused on use cases where the objects to be classified are the central subject\nof images. Despite success on benchmark vision datasets aligned with this use\ncase, these methods typically fail on use cases involving densely-annotated,\nbusy images: images common in the wild where objects of relevance are not the\ncentral subject, instead appearing potentially occluded, small, or among other\nincidental objects belonging to other classes of potential interest. To\nlocalize relevant objects, we employ a prototype-based few-shot segmentation\nmodel which compares the encoded features of unlabeled query images with\nsupport class centroids to produce region proposals indicating the presence and\nlocation of support set classes in a query image. These region proposals are\nthen used as additional conditioning input to few-shot image classifiers. We\ndevelop a framework to unify the two stages (segmentation and classification)\ninto an end-to-end classification model -- PRoPnet -- and empirically\ndemonstrate that our methods improve accuracy on image datasets with natural\nscenes containing multiple object classes.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 04:03:30 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Skomski", "Elliott", ""], ["Tuor", "Aaron", ""], ["Avila", "Andrew", ""], ["Phillips", "Lauren", ""], ["New", "Zachary", ""], ["Kvinge", "Henry", ""], ["Corley", "Courtney D.", ""], ["Hodas", "Nathan", ""]]}, {"id": "2104.03501", "submitter": "Jiaxin Li", "authors": "Jiaxin Li, Gim Hee Lee", "title": "DeepI2P: Image-to-Point Cloud Registration via Deep Classification", "comments": "CVPR 2021. Main paper and supplementary materials", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents DeepI2P: a novel approach for cross-modality registration\nbetween an image and a point cloud. Given an image (e.g. from a rgb-camera) and\na general point cloud (e.g. from a 3D Lidar scanner) captured at different\nlocations in the same scene, our method estimates the relative rigid\ntransformation between the coordinate frames of the camera and Lidar. Learning\ncommon feature descriptors to establish correspondences for the registration is\ninherently challenging due to the lack of appearance and geometric correlations\nacross the two modalities. We circumvent the difficulty by converting the\nregistration problem into a classification and inverse camera projection\noptimization problem. A classification neural network is designed to label\nwhether the projection of each point in the point cloud is within or beyond the\ncamera frustum. These labeled points are subsequently passed into a novel\ninverse camera projection solver to estimate the relative pose. Extensive\nexperimental results on Oxford Robotcar and KITTI datasets demonstrate the\nfeasibility of our approach. Our source code is available at\nhttps://github.com/lijx10/DeepI2P\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 04:27:32 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Li", "Jiaxin", ""], ["Lee", "Gim Hee", ""]]}, {"id": "2104.03507", "submitter": "Xueyan Zou", "authors": "Xueyan Zou, Linjie Yang, Ding Liu, Yong Jae Lee", "title": "Progressive Temporal Feature Alignment Network for Video Inpainting", "comments": "Accepted in CVPR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video inpainting aims to fill spatio-temporal \"corrupted\" regions with\nplausible content. To achieve this goal, it is necessary to find\ncorrespondences from neighbouring frames to faithfully hallucinate the unknown\ncontent. Current methods achieve this goal through attention, flow-based\nwarping, or 3D temporal convolution. However, flow-based warping can create\nartifacts when optical flow is not accurate, while temporal convolution may\nsuffer from spatial misalignment. We propose 'Progressive Temporal Feature\nAlignment Network', which progressively enriches features extracted from the\ncurrent frame with the feature warped from neighbouring frames using optical\nflow. Our approach corrects the spatial misalignment in the temporal feature\npropagation stage, greatly improving visual quality and temporal consistency of\nthe inpainted videos. Using the proposed architecture, we achieve\nstate-of-the-art performance on the DAVIS and FVI datasets compared to existing\ndeep learning approaches. Code is available at\nhttps://github.com/MaureenZOU/TSAM.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 04:50:33 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Zou", "Xueyan", ""], ["Yang", "Linjie", ""], ["Liu", "Ding", ""], ["Lee", "Yong Jae", ""]]}, {"id": "2104.03509", "submitter": "Jin Hyun Cheong", "authors": "Jin Hyun Cheong, Tiankang Xie, Sophie Byrne, Luke J. Chang", "title": "Py-Feat: Python Facial Expression Analysis Toolbox", "comments": "25 pages, 3 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Studying facial expressions is a notoriously difficult endeavor. Recent\nadvances in the field of affective computing have yielded impressive progress\nin automatically detecting facial expressions from pictures and videos.\nHowever, much of this work has yet to be widely disseminated in social science\ndomains such as psychology. Current state of the art models require\nconsiderable domain expertise that is not traditionally incorporated into\nsocial science training programs. Furthermore, there is a notable absence of\nuser-friendly and open-source software that provides a comprehensive set of\ntools and functions that support facial expression research. In this paper, we\nintroduce Py-Feat, an open-source Python toolbox that provides support for\ndetecting, preprocessing, analyzing, and visualizing facial expression data.\nPy-Feat makes it easy for domain experts to disseminate and benchmark computer\nvision models and also for end users to quickly process, analyze, and visualize\nface expression data. We hope this platform will facilitate increased use of\nfacial expression data in human behavior research.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 04:52:21 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Cheong", "Jin Hyun", ""], ["Xie", "Tiankang", ""], ["Byrne", "Sophie", ""], ["Chang", "Luke J.", ""]]}, {"id": "2104.03510", "submitter": "Abu Md Niamul Taufique", "authors": "Abu Md Niamul Taufique, Andreas Savakis, Michael Braun, Daniel\n  Kubacki, Ethan Dell, Lei Qian, Sean M. O'Rourke", "title": "SiamReID: Confuser Aware Siamese Tracker with Re-identification Feature", "comments": "6 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Siamese deep-network trackers have received significant attention in recent\nyears due to their real-time speed and state-of-the-art performance. However,\nSiamese trackers suffer from similar looking confusers, that are prevalent in\naerial imagery and create challenging conditions due to prolonged occlusions\nwhere the tracker object re-appears under different pose and illumination. Our\nwork proposes SiamReID, a novel re-identification framework for Siamese\ntrackers, that incorporates confuser rejection during prolonged occlusions and\nis well-suited for aerial tracking. The re-identification feature is trained\nusing both triplet loss and a class balanced loss. Our approach achieves\nstate-of-the-art performance in the UAVDT single object tracking benchmark.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 04:58:34 GMT"}, {"version": "v2", "created": "Sun, 11 Apr 2021 00:05:07 GMT"}, {"version": "v3", "created": "Thu, 15 Apr 2021 16:43:48 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Taufique", "Abu Md Niamul", ""], ["Savakis", "Andreas", ""], ["Braun", "Michael", ""], ["Kubacki", "Daniel", ""], ["Dell", "Ethan", ""], ["Qian", "Lei", ""], ["O'Rourke", "Sean M.", ""]]}, {"id": "2104.03515", "submitter": "Diqiong Jiang", "authors": "Diqiong Jiang, Yiwei Jin, Risheng Deng, Ruofeng Tong, Fanglue Zhang,\n  Yukun Yai, Ming Tang", "title": "Reconstructing Recognizable 3D Face Shapes based on 3D Morphable Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many recent works have reconstructed distinctive 3D face shapes by\naggregating shape parameters of the same identity and separating those of\ndifferent people based on parametric models (e.g., 3D morphable models\n(3DMMs)). However, despite the high accuracy in the face recognition task using\nthese shape parameters, the visual discrimination of face shapes reconstructed\nfrom those parameters is unsatisfactory. The following research question has\nnot been answered in previous works: Do discriminative shape parameters\nguarantee visual discrimination in represented 3D face shapes? This paper\nanalyzes the relationship between shape parameters and reconstructed shape\ngeometry and proposes a novel shape identity-aware regularization(SIR) loss for\nshape parameters, aiming at increasing discriminability in both the shape\nparameter and shape geometry domains. Moreover, to cope with the lack of\ntraining data containing both landmark and identity annotations, we propose a\nnetwork structure and an associated training strategy to leverage mixed data\ncontaining either identity or landmark labels. We compare our method with\nexisting methods in terms of the reconstruction error, visual\ndistinguishability, and face recognition accuracy of the shape parameters.\nExperimental results show that our method outperforms the state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 05:11:48 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Jiang", "Diqiong", ""], ["Jin", "Yiwei", ""], ["Deng", "Risheng", ""], ["Tong", "Ruofeng", ""], ["Zhang", "Fanglue", ""], ["Yai", "Yukun", ""], ["Tang", "Ming", ""]]}, {"id": "2104.03516", "submitter": "Yanjie Li", "authors": "Yanjie Li, Shoukui Zhang, Zhicheng Wang, Sen Yang, Wankou Yang,\n  Shu-Tao Xia, Erjin Zhou", "title": "TokenPose: Learning Keypoint Tokens for Human Pose Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human pose estimation deeply relies on visual clues and anatomical\nconstraints between parts to locate keypoints. Most existing CNN-based methods\ndo well in visual representation, however, lacking in the ability to explicitly\nlearn the constraint relationships between keypoints. In this paper, we propose\na novel approach based on Token representation for human Pose\nestimation~(TokenPose). In detail, each keypoint is explicitly embedded as a\ntoken to simultaneously learn constraint relationships and appearance cues from\nimages. Extensive experiments show that the small and large TokenPose models\nare on par with state-of-the-art CNN-based counterparts while being more\nlightweight. Specifically, our TokenPose-S and TokenPose-L achieve 72.5 AP and\n75.8 AP on COCO validation dataset respectively, with significant reduction in\nparameters ($\\downarrow80.6\\%$ ; $\\downarrow$ $56.8\\%$) and GFLOPs\n($\\downarrow$$ 75.3\\%$; $\\downarrow$ $24.7\\%$).\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 05:12:38 GMT"}, {"version": "v2", "created": "Fri, 9 Apr 2021 15:28:13 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Li", "Yanjie", ""], ["Zhang", "Shoukui", ""], ["Wang", "Zhicheng", ""], ["Yang", "Sen", ""], ["Yang", "Wankou", ""], ["Xia", "Shu-Tao", ""], ["Zhou", "Erjin", ""]]}, {"id": "2104.03520", "submitter": "Fangneng Zhan", "authors": "Changgong Zhang, Fangneng Zhan, Yuan Chang", "title": "Deep Monocular 3D Human Pose Estimation via Cascaded Dimension-Lifting", "comments": "3 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The 3D pose estimation from a single image is a challenging problem due to\ndepth ambiguity. One type of the previous methods lifts 2D joints, obtained by\nresorting to external 2D pose detectors, to the 3D space. However, this type of\napproaches discards the contextual information of images which are strong cues\nfor 3D pose estimation. Meanwhile, some other methods predict the joints\ndirectly from monocular images but adopt a 2.5D output representation $P^{2.5D}\n= (u,v,z^{r}) $ where both $u$ and $v$ are in the image space but $z^{r}$ in\nroot-relative 3D space. Thus, the ground-truth information (e.g., the depth of\nroot joint from the camera) is normally utilized to transform the 2.5D output\nto the 3D space, which limits the applicability in practice. In this work, we\npropose a novel end-to-end framework that not only exploits the contextual\ninformation but also produces the output directly in the 3D space via cascaded\ndimension-lifting. Specifically, we decompose the task of lifting pose from 2D\nimage space to 3D spatial space into several sequential sub-tasks, 1) kinematic\nskeletons \\& individual joints estimation in 2D space, 2) root-relative depth\nestimation, and 3) lifting to the 3D space, each of which employs direct\nsupervisions and contextual image features to guide the learning process.\nExtensive experiments show that the proposed framework achieves\nstate-of-the-art performance on two widely used 3D human pose datasets\n(Human3.6M, MuPoTS-3D).\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 05:44:02 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Zhang", "Changgong", ""], ["Zhan", "Fangneng", ""], ["Chang", "Yuan", ""]]}, {"id": "2104.03531", "submitter": "Zhao Kang", "authors": "Juncheng Lv and Zhao Kang and Xiao Lu and Zenglin Xu", "title": "Pseudo-supervised Deep Subspace Clustering", "comments": null, "journal-ref": "IEEE Transactions on Image Processing 2021", "doi": "10.1109/TIP.2021.3079800", "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Auto-Encoder (AE)-based deep subspace clustering (DSC) methods have achieved\nimpressive performance due to the powerful representation extracted using deep\nneural networks while prioritizing categorical separability. However,\nself-reconstruction loss of an AE ignores rich useful relation information and\nmight lead to indiscriminative representation, which inevitably degrades the\nclustering performance. It is also challenging to learn high-level similarity\nwithout feeding semantic labels. Another unsolved problem facing DSC is the\nhuge memory cost due to $n\\times n$ similarity matrix, which is incurred by the\nself-expression layer between an encoder and decoder. To tackle these problems,\nwe use pairwise similarity to weigh the reconstruction loss to capture local\nstructure information, while a similarity is learned by the self-expression\nlayer. Pseudo-graphs and pseudo-labels, which allow benefiting from uncertain\nknowledge acquired during network training, are further employed to supervise\nsimilarity learning. Joint learning and iterative training facilitate to obtain\nan overall optimal solution. Extensive experiments on benchmark datasets\ndemonstrate the superiority of our approach. By combining with the $k$-nearest\nneighbors algorithm, we further show that our method can address the\nlarge-scale and out-of-sample problems.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 06:25:47 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Lv", "Juncheng", ""], ["Kang", "Zhao", ""], ["Lu", "Xiao", ""], ["Xu", "Zenglin", ""]]}, {"id": "2104.03535", "submitter": "Makoto Takamoto", "authors": "Makoto Takamoto and Yusuke Morishita", "title": "An Empirical Study of the Effects of Sample-Mixing Methods for Efficient\n  Training of Generative Adversarial Networks", "comments": "draft version, accepted by IEEE 4th International Conference on\n  Multimedia Information Processing and Retrieval (IEEE MIPR 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  It is well-known that training of generative adversarial networks (GANs)\nrequires huge iterations before the generator's providing good-quality samples.\nAlthough there are several studies to tackle this problem, there is still no\nuniversal solution. In this paper, we investigated the effect of sample mixing\nmethods, that is, Mixup, CutMix, and newly proposed Smoothed Regional Mix\n(SRMix), to alleviate this problem. The sample-mixing methods are known to\nenhance the accuracy and robustness in the wide range of classification\nproblems, and can naturally be applicable to GANs because the role of the\ndiscriminator can be interpreted as the classification between real and fake\nsamples. We also proposed a new formalism applying the sample-mixing methods to\nGANs with the saturated losses which do not have a clear \"label\" of real and\nfake. We performed a vast amount of numerical experiments using LSUN and CelebA\ndatasets. The results showed that Mixup and SRMix improved the quality of the\ngenerated images in terms of FID in most cases, in particular, SRMix showed the\nbest improvement in most cases. Our analysis indicates that the mixed-samples\ncan provide different properties from the vanilla fake samples, and the mixing\npattern strongly affects the decision of the discriminators. The generated\nimages of Mixup have good high-level feature but low-level feature is not so\nimpressible. On the other hand, CutMix showed the opposite tendency. Our SRMix\nshowed the middle tendency, that is, showed good high and low level features.\nWe believe that our finding provides a new perspective to accelerate the GANs\nconvergence and improve the quality of generated samples.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 06:40:23 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Takamoto", "Makoto", ""], ["Morishita", "Yusuke", ""]]}, {"id": "2104.03541", "submitter": "Qiang Wang", "authors": "Qiang Wang, Yun Zheng, Pan Pan, Yinghui Xu", "title": "Multiple Object Tracking with Correlation Learning", "comments": "11 pages, 5 figures, Accepted to CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent works have shown that convolutional networks have substantially\nimproved the performance of multiple object tracking by simultaneously learning\ndetection and appearance features. However, due to the local perception of the\nconvolutional network structure itself, the long-range dependencies in both the\nspatial and temporal cannot be obtained efficiently. To incorporate the spatial\nlayout, we propose to exploit the local correlation module to model the\ntopological relationship between targets and their surrounding environment,\nwhich can enhance the discriminative power of our model in crowded scenes.\nSpecifically, we establish dense correspondences of each spatial location and\nits context, and explicitly constrain the correlation volumes through\nself-supervised learning. To exploit the temporal context, existing approaches\ngenerally utilize two or more adjacent frames to construct an enhanced feature\nrepresentation, but the dynamic motion scene is inherently difficult to depict\nvia CNNs. Instead, our paper proposes a learnable correlation operator to\nestablish frame-to-frame matches over convolutional feature maps in the\ndifferent layers to align and propagate temporal context. With extensive\nexperimental results on the MOT datasets, our approach demonstrates the\neffectiveness of correlation learning with the superior performance and obtains\nstate-of-the-art MOTA of 76.5% and IDF1 of 73.6% on MOT17.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 06:48:02 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Wang", "Qiang", ""], ["Zheng", "Yun", ""], ["Pan", "Pan", ""], ["Xu", "Yinghui", ""]]}, {"id": "2104.03544", "submitter": "Qiyao Wang", "authors": "Qiyao Wang, Pengfei Li, Li Zhu, Yi Niu", "title": "1st Place Solution to ICDAR 2021 RRC-ICTEXT End-to-end Text Spotting and\n  Aesthetic Assessment on Integrated Circuit", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents our proposed methods to ICDAR 2021 Robust Reading\nChallenge - Integrated Circuit Text Spotting and Aesthetic Assessment (ICDAR\nRRC-ICTEXT 2021). For the text spotting task, we detect the characters on\nintegrated circuit and classify them based on yolov5 detection model. We\nbalance the lowercase and non-lowercase by using SynthText, generated data and\ndata sampler. We adopt semi-supervised algorithm and distillation to furtherly\nimprove the model's accuracy. For the aesthetic assessment task, we add a\nclassification branch of 3 classes to differentiate the aesthetic classes of\neach character. Finally, we make model deployment to accelerate inference speed\nand reduce memory consumption based on NVIDIA Tensorrt. Our methods achieve\n59.1 mAP on task 3.1 with 31 FPS and 306M memory (rank 1), 78.7\\% F2 score on\ntask 3.2 with 30 FPS and 306M memory (rank 1).\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 06:52:49 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Wang", "Qiyao", ""], ["Li", "Pengfei", ""], ["Zhu", "Li", ""], ["Niu", "Yi", ""]]}, {"id": "2104.03549", "submitter": "Maleeha Khalid Khan", "authors": "Maleeha Khalid Khan (1) Syed Muhammad Anwar (2)", "title": "M-Net with Bidirectional ConvLSTM for Cup and Disc Segmentation in\n  Fundus Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Glaucoma is a severe eye disease that is known to deteriorate optic never\nfibers, causing cup size to increase, which could result in permanent loss of\nvision. Glaucoma is the second leading cause of blindness after cataract, but\nglaucoma being more dangerous as it is not curable. Early diagnoses and\ntreatment of glaucoma can help to slow the progression of glaucoma and its\ndamages. For the detection of glaucoma, the Cup to Disc ratio (CDR) provides\nsignificant information. The CDR depends heavily on the accurate segmentation\nof cup and disc regions. In this paper, we have proposed a modified M-Net with\nbidirectional convolution long short-term memory (LSTM), based on joint cup and\ndisc segmentation. The proposed network combines features of encoder and\ndecoder, with bidirectional LSTM. Our proposed model segments cup and disc\nregions based on which the abnormalities in cup to disc ratio can be observed.\nThe proposed model is tested on REFUGE2 data, where our model achieves a dice\nscore of 0.92 for optic disc and an accuracy of 98.99% in segmenting cup and\ndisc regions\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 07:01:42 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Khan", "Maleeha Khalid", ""], ["Anwar", "Syed Muhammad", ""]]}, {"id": "2104.03560", "submitter": "Kunming Luo", "authors": "Kunming Luo, Ao Luo, Chuan Wang, Haoqiang Fan, Shuaicheng Liu", "title": "ASFlow: Unsupervised Optical Flow Learning with Adaptive Pyramid\n  Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an unsupervised optical flow estimation method by proposing an\nadaptive pyramid sampling in the deep pyramid network. Specifically, in the\npyramid downsampling, we propose an Content Aware Pooling (CAP) module, which\npromotes local feature gathering by avoiding cross region pooling, so that the\nlearned features become more representative. In the pyramid upsampling, we\npropose an Adaptive Flow Upsampling (AFU) module, where cross edge\ninterpolation can be avoided, producing sharp motion boundaries. Equipped with\nthese two modules, our method achieves the best performance for unsupervised\noptical flow estimation on multiple leading benchmarks, including MPI-SIntel,\nKITTI 2012 and KITTI 2015. Particuarlly, we achieve EPE=1.5 on KITTI 2012 and\nF1=9.67% KITTI 2015, which outperform the previous state-of-the-art methods by\n16.7% and 13.1%, respectively.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 07:22:35 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Luo", "Kunming", ""], ["Luo", "Ao", ""], ["Wang", "Chuan", ""], ["Fan", "Haoqiang", ""], ["Liu", "Shuaicheng", ""]]}, {"id": "2104.03577", "submitter": "Daniel Franco-Barranco", "authors": "Daniel Franco-Barranco and Arrate Mu\\~noz-Barrutia and Ignacio\n  Arganda-Carreras", "title": "Stable deep neural network architectures for mitochondria segmentation\n  on electron microscopy volumes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Electron microscopy (EM) allows the identification of intracellular\norganelles such as mitochondria, providing insights for clinical and scientific\nstudies. In recent years, a number of novel deep learning architectures have\nbeen published reporting superior performance, or even human-level accuracy,\ncompared to previous approaches on public mitochondria segmentation datasets.\nUnfortunately, many of these publications do not make neither the code nor the\nfull training details public to support the results obtained, leading to\nreproducibility issues and dubious model comparisons. For that reason, and\nfollowing a recent code of best practices for reporting experimental results,\nwe present an extensive study of the state-of-the-art deep learning\narchitectures for the segmentation of mitochondria on EM volumes, and evaluate\nthe impact in performance of different variations of 2D and 3D U-Net-like\nmodels for this task. To better understand the contribution of each component,\na common set of pre- and post-processing operations has been implemented and\ntested with each approach. Moreover, an exhaustive sweep of hyperparameters\nvalues for all architectures have been performed and each configuration has\nbeen run multiple times to report the mean and standard deviation values of the\nevaluation metrics. Using this methodology, we found very stable architectures\nand hyperparameter configurations that consistently obtain state-of-the-art\nresults in the well-known EPFL Hippocampus mitochondria segmentation dataset.\nFurthermore, we have benchmarked our proposed models on two other available\ndatasets, Lucchi++ and Kasthuri++, where they outperform all previous works.\nThe code derived from this research and its documentation are publicly\navailable.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 07:41:13 GMT"}, {"version": "v2", "created": "Tue, 27 Jul 2021 19:26:30 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Franco-Barranco", "Daniel", ""], ["Mu\u00f1oz-Barrutia", "Arrate", ""], ["Arganda-Carreras", "Ignacio", ""]]}, {"id": "2104.03584", "submitter": "Zhengyang Shen", "authors": "Zhengyang Shen, Tiancheng Shen, Zhouchen Lin, Jinwen Ma", "title": "PDO-e$\\text{S}^\\text{2}$CNNs: Partial Differential Operator Based\n  Equivariant Spherical CNNs", "comments": "Accepted by AAAI2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spherical signals exist in many applications, e.g., planetary data, LiDAR\nscans and digitalization of 3D objects, calling for models that can process\nspherical data effectively. It does not perform well when simply projecting\nspherical data into the 2D plane and then using planar convolution neural\nnetworks (CNNs), because of the distortion from projection and ineffective\ntranslation equivariance. Actually, good principles of designing spherical CNNs\nare avoiding distortions and converting the shift equivariance property in\nplanar CNNs to rotation equivariance in the spherical domain. In this work, we\nuse partial differential operators (PDOs) to design a spherical equivariant\nCNN, PDO-e$\\text{S}^\\text{2}$CNN, which is exactly rotation equivariant in the\ncontinuous domain. We then discretize PDO-e$\\text{S}^\\text{2}$CNNs, and analyze\nthe equivariance error resulted from discretization. This is the first time\nthat the equivariance error is theoretically analyzed in the spherical domain.\nIn experiments, PDO-e$\\text{S}^\\text{2}$CNNs show greater parameter efficiency\nand outperform other spherical CNNs significantly on several tasks.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 07:54:50 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Shen", "Zhengyang", ""], ["Shen", "Tiancheng", ""], ["Lin", "Zhouchen", ""], ["Ma", "Jinwen", ""]]}, {"id": "2104.03589", "submitter": "Yonggang Qi", "authors": "Yonggang Qi, Kai Zhang, Aneeshan Sain, Yi-Zhe Song", "title": "PQA: Perceptual Question Answering", "comments": "Accepted by CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Perceptual organization remains one of the very few established theories on\nthe human visual system. It underpinned many pre-deep seminal works on\nsegmentation and detection, yet research has seen a rapid decline since the\npreferential shift to learning deep models. Of the limited attempts, most aimed\nat interpreting complex visual scenes using perceptual organizational rules.\nThis has however been proven to be sub-optimal, since models were unable to\neffectively capture the visual complexity in real-world imagery. In this paper,\nwe rejuvenate the study of perceptual organization, by advocating two\npositional changes: (i) we examine purposefully generated synthetic data,\ninstead of complex real imagery, and (ii) we ask machines to synthesize novel\nperceptually-valid patterns, instead of explaining existing data. Our overall\nanswer lies with the introduction of a novel visual challenge -- the challenge\nof perceptual question answering (PQA). Upon observing example perceptual\nquestion-answer pairs, the goal for PQA is to solve similar questions by\ngenerating answers entirely from scratch (see Figure 1). Our first contribution\nis therefore the first dataset of perceptual question-answer pairs, each\ngenerated specifically for a particular Gestalt principle. We then borrow\ninsights from human psychology to design an agent that casts perceptual\norganization as a self-attention problem, where a proposed grid-to-grid mapping\nnetwork directly generates answer patterns from scratch. Experiments show our\nagent to outperform a selection of naive and strong baselines. A human study\nhowever indicates that ours uses astronomically more data to learn when\ncompared to an average human, necessitating future research (with or without\nour dataset).\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 08:06:21 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Qi", "Yonggang", ""], ["Zhang", "Kai", ""], ["Sain", "Aneeshan", ""], ["Song", "Yi-Zhe", ""]]}, {"id": "2104.03602", "submitter": "Sara Atito", "authors": "Sara Atito and Muhammad Awais and Josef Kittler", "title": "SiT: Self-supervised vIsion Transformer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-supervised learning methods are gaining increasing traction in computer\nvision due to their recent success in reducing the gap with supervised\nlearning. In natural language processing (NLP) self-supervised learning and\ntransformers are already the methods of choice. The recent literature suggests\nthat the transformers are becoming increasingly popular also in computer\nvision. So far, the vision transformers have been shown to work well when\npretrained either using a large scale supervised data or with some kind of\nco-supervision, e.g. in terms of teacher network. These supervised pretrained\nvision transformers achieve very good results in downstream tasks with minimal\nchanges. In this work we investigate the merits of self-supervised learning for\npretraining image/vision transformers and then using them for downstream\nclassification tasks. We propose Self-supervised vIsion Transformers (SiT) and\ndiscuss several self-supervised training mechanisms to obtain a pretext model.\nThe architectural flexibility of SiT allows us to use it as an autoencoder and\nwork with multiple self-supervised tasks seamlessly. We show that a pretrained\nSiT can be finetuned for a downstream classification task on small scale\ndatasets, consisting of a few thousand images rather than several millions. The\nproposed approach is evaluated on standard datasets using common protocols. The\nresults demonstrate the strength of the transformers and their suitability for\nself-supervised learning. We outperformed existing self-supervised learning\nmethods by large margin. We also observed that SiT is good for few shot\nlearning and also showed that it is learning useful representation by simply\ntraining a linear classifier on top of the learned features from SiT.\nPretraining, finetuning, and evaluation codes will be available under:\nhttps://github.com/Sara-Ahmed/SiT.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 08:34:04 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Atito", "Sara", ""], ["Awais", "Muhammad", ""], ["Kittler", "Josef", ""]]}, {"id": "2104.03618", "submitter": "Hao Guan", "authors": "Hao Guan (1), Chaoyue Wang (1), Dacheng Tao (1) ((1) School of\n  Computer Science, The University of Sydney)", "title": "MRI-based Alzheimer's disease prediction via distilling the knowledge in\n  multi-modal data", "comments": "19 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mild cognitive impairment (MCI) conversion prediction, i.e., identifying MCI\npatients of high risks converting to Alzheimer's disease (AD), is essential for\npreventing or slowing the progression of AD. Although previous studies have\nshown that the fusion of multi-modal data can effectively improve the\nprediction accuracy, their applications are largely restricted by the limited\navailability or high cost of multi-modal data. Building an effective prediction\nmodel using only magnetic resonance imaging (MRI) remains a challenging\nresearch topic. In this work, we propose a multi-modal multi-instance\ndistillation scheme, which aims to distill the knowledge learned from\nmulti-modal data to an MRI-based network for MCI conversion prediction. In\ncontrast to existing distillation algorithms, the proposed multi-instance\nprobabilities demonstrate a superior capability of representing the complicated\natrophy distributions, and can guide the MRI-based network to better explore\nthe input MRI. To our best knowledge, this is the first study that attempts to\nimprove an MRI-based prediction model by leveraging extra supervision distilled\nfrom multi-modal information. Experiments demonstrate the advantage of our\nframework, suggesting its potentials in the data-limited clinical settings.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 09:06:39 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Guan", "Hao", ""], ["Wang", "Chaoyue", ""], ["Tao", "Dacheng", ""]]}, {"id": "2104.03620", "submitter": "Yang Shu", "authors": "Yang Shu, Zhangjie Cao, Chenyu Wang, Jianmin Wang, Mingsheng Long", "title": "Open Domain Generalization with Domain-Augmented Meta-Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Leveraging datasets available to learn a model with high generalization\nability to unseen domains is important for computer vision, especially when the\nunseen domain's annotated data are unavailable. We study a novel and practical\nproblem of Open Domain Generalization (OpenDG), which learns from different\nsource domains to achieve high performance on an unknown target domain, where\nthe distributions and label sets of each individual source domain and the\ntarget domain can be different. The problem can be generally applied to diverse\nsource domains and widely applicable to real-world applications. We propose a\nDomain-Augmented Meta-Learning framework to learn open-domain generalizable\nrepresentations. We augment domains on both feature-level by a new Dirichlet\nmixup and label-level by distilled soft-labeling, which complements each domain\nwith missing classes and other domain knowledge. We conduct meta-learning over\ndomains by designing new meta-learning tasks and losses to preserve domain\nunique knowledge and generalize knowledge across domains simultaneously.\nExperiment results on various multi-domain datasets demonstrate that the\nproposed Domain-Augmented Meta-Learning (DAML) outperforms prior methods for\nunseen domain recognition.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 09:12:24 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Shu", "Yang", ""], ["Cao", "Zhangjie", ""], ["Wang", "Chenyu", ""], ["Wang", "Jianmin", ""], ["Long", "Mingsheng", ""]]}, {"id": "2104.03638", "submitter": "Linh K\\\"astner", "authors": "Zhengcheng Shen, Linh K\\\"astner and Jens Lambrecht", "title": "Spatial Imagination With Semantic Cognition for Mobile Robots", "comments": "7 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The imagination of the surrounding environment based on experience and\nsemantic cognition has great potential to extend the limited observations and\nprovide more information for mapping, collision avoidance, and path planning.\nThis paper provides a training-based algorithm for mobile robots to perform\nspatial imagination based on semantic cognition and evaluates the proposed\nmethod for the mapping task. We utilize a photo-realistic simulation\nenvironment, Habitat, for training and evaluation. The trained model is\ncomposed of Resent-18 as encoder and Unet as the backbone. We demonstrate that\nthe algorithm can perform imagination for unseen parts of the object\nuniversally, by recalling the images and experience and compare our approach\nwith traditional semantic mapping methods. It is found that our approach will\nimprove the efficiency and accuracy of semantic mapping.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 09:44:49 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Shen", "Zhengcheng", ""], ["K\u00e4stner", "Linh", ""], ["Lambrecht", "Jens", ""]]}, {"id": "2104.03640", "submitter": "Yingjie Cai", "authors": "Yingjie Cai, Xuesong Chen, Chao Zhang, Kwan-Yee Lin, Xiaogang Wang,\n  Hongsheng Li", "title": "Semantic Scene Completion via Integrating Instances and Scene\n  in-the-Loop", "comments": "CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Semantic Scene Completion aims at reconstructing a complete 3D scene with\nprecise voxel-wise semantics from a single-view depth or RGBD image. It is a\ncrucial but challenging problem for indoor scene understanding. In this work,\nwe present a novel framework named Scene-Instance-Scene Network\n(\\textit{SISNet}), which takes advantages of both instance and scene level\nsemantic information. Our method is capable of inferring fine-grained shape\ndetails as well as nearby objects whose semantic categories are easily\nmixed-up. The key insight is that we decouple the instances from a coarsely\ncompleted semantic scene instead of a raw input image to guide the\nreconstruction of instances and the overall scene. SISNet conducts iterative\nscene-to-instance (SI) and instance-to-scene (IS) semantic completion.\nSpecifically, the SI is able to encode objects' surrounding context for\neffectively decoupling instances from the scene and each instance could be\nvoxelized into higher resolution to capture finer details. With IS,\nfine-grained instance information can be integrated back into the 3D scene and\nthus leads to more accurate semantic scene completion. Utilizing such an\niterative mechanism, the scene and instance completion benefits each other to\nachieve higher completion accuracy. Extensively experiments show that our\nproposed method consistently outperforms state-of-the-art methods on both real\nNYU, NYUCAD and synthetic SUNCG-RGBD datasets. The code and the supplementary\nmaterial will be available at \\url{https://github.com/yjcaimeow/SISNet}.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 09:50:30 GMT"}, {"version": "v2", "created": "Sun, 6 Jun 2021 13:53:01 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Cai", "Yingjie", ""], ["Chen", "Xuesong", ""], ["Zhang", "Chao", ""], ["Lin", "Kwan-Yee", ""], ["Wang", "Xiaogang", ""], ["Li", "Hongsheng", ""]]}, {"id": "2104.03642", "submitter": "Huy Hoang Nguyen", "authors": "Huy Hoang Nguyen, Simo Saarakkala, Matthew B. Blaschko, Aleksei\n  Tiulpin", "title": "DeepProg: A Transformer-based Framework for Predicting Disease Prognosis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A vast majority of deep learning methods are built to automate diagnostic\ntasks. However, in clinical practice, a more advanced question is how to\npredict the course of a disease. Current methods for this problem are\ncomplicated, and often require domain knowledge, making them difficult for\npractitioners to use. In this paper, we formulate the prognosis prediction task\nas a one-to-many sequence prediction problem. Inspired by a clinical decision\nmaking process with two agents -- a radiologist and a general practitioner --\nwe propose a generic end-to-end transformer-based framework to estimate disease\nprognosis from images and auxiliary data. The effectiveness and validation of\nthe developed method are shown on synthetic data, and in the task of predicting\nthe development of structural osteoarthritic changes in knee joints.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 09:53:18 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Nguyen", "Huy Hoang", ""], ["Saarakkala", "Simo", ""], ["Blaschko", "Matthew B.", ""], ["Tiulpin", "Aleksei", ""]]}, {"id": "2104.03643", "submitter": "Juan Pablo Zuluaga-Gomez", "authors": "Juan Zuluaga-Gomez and Iuliia Nigmatulina and Amrutha Prasad and Petr\n  Motlicek and Karel Vesel\\'y and Martin Kocour and Igor Sz\\\"oke", "title": "Contextual Semi-Supervised Learning: An Approach To Leverage\n  Air-Surveillance and Untranscribed ATC Data in ASR Systems", "comments": "Submitted to: Interspeech conference 2021 (Brno, Czechia, August 30 -\n  September 3)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.LG eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Air traffic management and specifically air-traffic control (ATC) rely mostly\non voice communications between Air Traffic Controllers (ATCos) and pilots. In\nmost cases, these voice communications follow a well-defined grammar that could\nbe leveraged in Automatic Speech Recognition (ASR) technologies. The callsign\nused to address an airplane is an essential part of all ATCo-pilot\ncommunications. We propose a two-steps approach to add contextual knowledge\nduring semi-supervised training to reduce the ASR system error rates at\nrecognizing the part of the utterance that contains the callsign. Initially, we\nrepresent in a WFST the contextual knowledge (i.e. air-surveillance data) of an\nATCo-pilot communication. Then, during Semi-Supervised Learning (SSL) the\ncontextual knowledge is added by second-pass decoding (i.e. lattice\nre-scoring). Results show that `unseen domains' (e.g. data from airports not\npresent in the supervised training data) are further aided by contextual SSL\nwhen compared to standalone SSL. For this task, we introduce the Callsign Word\nError Rate (CA-WER) as an evaluation metric, which only assesses ASR\nperformance of the spoken callsign in an utterance. We obtained a 32.1% CA-WER\nrelative improvement applying SSL with an additional 17.5% CA-WER improvement\nby adding contextual knowledge during SSL on a challenging ATC-based test set\ngathered from LiveATC.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 09:53:54 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Zuluaga-Gomez", "Juan", ""], ["Nigmatulina", "Iuliia", ""], ["Prasad", "Amrutha", ""], ["Motlicek", "Petr", ""], ["Vesel\u00fd", "Karel", ""], ["Kocour", "Martin", ""], ["Sz\u00f6ke", "Igor", ""]]}, {"id": "2104.03656", "submitter": "Corentin Kervadec", "authors": "Corentin Kervadec, Theo Jaunet, Grigory Antipov, Moez Baccouche,\n  Romain Vuillemot and Christian Wolf", "title": "How Transferable are Reasoning Patterns in VQA?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since its inception, Visual Question Answering (VQA) is notoriously known as\na task, where models are prone to exploit biases in datasets to find shortcuts\ninstead of performing high-level reasoning. Classical methods address this by\nremoving biases from training data, or adding branches to models to detect and\nremove biases. In this paper, we argue that uncertainty in vision is a\ndominating factor preventing the successful learning of reasoning in vision and\nlanguage problems. We train a visual oracle and in a large scale study provide\nexperimental evidence that it is much less prone to exploiting spurious dataset\nbiases compared to standard models. We propose to study the attention\nmechanisms at work in the visual oracle and compare them with a SOTA\nTransformer-based model. We provide an in-depth analysis and visualizations of\nreasoning patterns obtained with an online visualization tool which we make\npublicly available (https://reasoningpatterns.github.io). We exploit these\ninsights by transferring reasoning patterns from the oracle to a SOTA\nTransformer-based VQA model taking standard noisy visual inputs via\nfine-tuning. In experiments we report higher overall accuracy, as well as\naccuracy on infrequent answers for each question type, which provides evidence\nfor improved generalization and a decrease of the dependency on dataset biases.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 10:18:45 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Kervadec", "Corentin", ""], ["Jaunet", "Theo", ""], ["Antipov", "Grigory", ""], ["Baccouche", "Moez", ""], ["Vuillemot", "Romain", ""], ["Wolf", "Christian", ""]]}, {"id": "2104.03658", "submitter": "Zongxin Yang", "authors": "Zongxin Yang, Xin Yu, Yi Yang", "title": "DSC-PoseNet: Learning 6DoF Object Pose Estimation via Dual-scale\n  Consistency", "comments": "CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compared to 2D object bounding-box labeling, it is very difficult for humans\nto annotate 3D object poses, especially when depth images of scenes are\nunavailable. This paper investigates whether we can estimate the object poses\neffectively when only RGB images and 2D object annotations are given. To this\nend, we present a two-step pose estimation framework to attain 6DoF object\nposes from 2D object bounding-boxes. In the first step, the framework learns to\nsegment objects from real and synthetic data in a weakly-supervised fashion,\nand the segmentation masks will act as a prior for pose estimation. In the\nsecond step, we design a dual-scale pose estimation network, namely\nDSC-PoseNet, to predict object poses by employing a differential renderer. To\nbe specific, our DSC-PoseNet firstly predicts object poses in the original\nimage scale by comparing the segmentation masks and the rendered visible object\nmasks. Then, we resize object regions to a fixed scale to estimate poses once\nagain. In this fashion, we eliminate large scale variations and focus on\nrotation estimation, thus facilitating pose estimation. Moreover, we exploit\nthe initial pose estimation to generate pseudo ground-truth to train our\nDSC-PoseNet in a self-supervised manner. The estimation results in these two\nscales are ensembled as our final pose estimation. Extensive experiments on\nwidely-used benchmarks demonstrate that our method outperforms state-of-the-art\nmodels trained on synthetic data by a large margin and even is on par with\nseveral fully-supervised methods.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 10:19:35 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Yang", "Zongxin", ""], ["Yu", "Xin", ""], ["Yang", "Yi", ""]]}, {"id": "2104.03668", "submitter": "Olivier Rukundo", "authors": "Olivier Rukundo, Marius Pedersen, {\\O}istein Hovde", "title": "Advanced Image Enhancement Method for Distant Vessels and Structures in\n  Capsule Endoscopy", "comments": "8 pages, 12 figures, 4 tables", "journal-ref": "Computational and Mathematical Methods in Medicine (CMMM), Volume\n  2017, Article ID 9813165", "doi": "10.1155/2017/9813165", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper proposes an advanced method for contrast enhancement of capsule\nendoscopic images, with the main objective to obtain sufficient information\nabout the vessels and structures in more distant (or darker) parts of capsule\nendoscopic images. The proposed method (PM) combines two algorithms for the\nenhancement of darker and brighter areas of capsule endoscopic images,\nrespectively. The half-unit weighted bilinear algorithm (HWB) proposed in our\nprevious work is used to enhance darker areas according to the darker map\ncontent of its HSV's component V. Enhancement of brighter areas is achieved\nthanks to the novel thresholded weighted-bilinear algorithm (TWB) developed to\navoid overexposure and enlargement of specular highlight spots while preserving\nthe hue, in such areas. The TWB performs enhancement operations following a\ngradual increment of the brightness of the brighter map content of its HSV's\ncomponent V. In other words, the TWB decreases its averaged-weights as the\nintensity content of the component V increases. Extensive experimental\ndemonstrations were conducted, and based on evaluation of the reference and PM\nenhanced images, a gastroenterologist ({\\O}H) concluded that the PM enhanced\nimages were the best ones based on the information about the vessels, contrast\nin the images, and the view or visibility of the structures in more distant\nparts of the capsule endoscopy images.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 10:37:36 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Rukundo", "Olivier", ""], ["Pedersen", "Marius", ""], ["Hovde", "\u00d8istein", ""]]}, {"id": "2104.03670", "submitter": "Linqi Zhou", "authors": "Linqi Zhou, Yilun Du, Jiajun Wu", "title": "3D Shape Generation and Completion through Point-Voxel Diffusion", "comments": "Project page: https://alexzhou907.github.io/pvd", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a novel approach for probabilistic generative modeling of 3D\nshapes. Unlike most existing models that learn to deterministically translate a\nlatent vector to a shape, our model, Point-Voxel Diffusion (PVD), is a unified,\nprobabilistic formulation for unconditional shape generation and conditional,\nmulti-modal shape completion. PVD marries denoising diffusion models with the\nhybrid, point-voxel representation of 3D shapes. It can be viewed as a series\nof denoising steps, reversing the diffusion process from observed point cloud\ndata to Gaussian noise, and is trained by optimizing a variational lower bound\nto the (conditional) likelihood function. Experiments demonstrate that PVD is\ncapable of synthesizing high-fidelity shapes, completing partial point clouds,\nand generating multiple completion results from single-view depth scans of real\nobjects.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 10:38:03 GMT"}, {"version": "v2", "created": "Sun, 11 Apr 2021 22:11:25 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Zhou", "Linqi", ""], ["Du", "Yilun", ""], ["Wu", "Jiajun", ""]]}, {"id": "2104.03693", "submitter": "Zhao Zhong", "authors": "Yucong Zhou, Zezhou Zhu, Zhao Zhong", "title": "Learning specialized activation functions with the Piecewise Linear Unit", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The choice of activation functions is crucial for modern deep neural\nnetworks. Popular hand-designed activation functions like Rectified Linear\nUnit(ReLU) and its variants show promising performance in various tasks and\nmodels. Swish, the automatically discovered activation function, has been\nproposed and outperforms ReLU on many challenging datasets. However, it has two\nmain drawbacks. First, the tree-based search space is highly discrete and\nrestricted, which is difficult for searching. Second, the sample-based\nsearching method is inefficient, making it infeasible to find specialized\nactivation functions for each dataset or neural architecture. To tackle these\ndrawbacks, we propose a new activation function called Piecewise Linear\nUnit(PWLU), which incorporates a carefully designed formulation and learning\nmethod. It can learn specialized activation functions and achieves SOTA\nperformance on large-scale datasets like ImageNet and COCO. For example, on\nImageNet classification dataset, PWLU improves 0.9%/0.53%/1.0%/1.7%/1.0% top-1\naccuracy over Swish for\nResNet-18/ResNet-50/MobileNet-V2/MobileNet-V3/EfficientNet-B0. PWLU is also\neasy to implement and efficient at inference, which can be widely applied in\nreal-world applications.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 11:29:11 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Zhou", "Yucong", ""], ["Zhu", "Zezhou", ""], ["Zhong", "Zhao", ""]]}, {"id": "2104.03715", "submitter": "Wenqiang Li", "authors": "Wenqiang Li, YM Tang, Ziyang Wang, KM Yu, Sandy To", "title": "Atrous Residual Interconnected Encoder to Attention Decoder Framework\n  for Vertebrae Segmentation via 3D Volumetric CT Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic medical image segmentation based on Computed Tomography (CT) has\nbeen widely applied for computer-aided surgery as a prerequisite. With the\ndevelopment of deep learning technologies, deep convolutional neural networks\n(DCNNs) have shown robust performance in automated semantic segmentation of\nmedical images. However, semantic segmentation algorithms based on DCNNs still\nmeet the challenges of feature loss between encoder and decoder, multi-scale\nobject, restricted field of view of filters, and lack of medical image data.\nThis paper proposes a novel algorithm for automated vertebrae segmentation via\n3D volumetric spine CT images. The proposed model is based on the structure of\nencoder to decoder, using layer normalization to optimize mini-batch training\nperformance. To address the concern of the information loss between encoder and\ndecoder, we designed an Atrous Residual Path to pass more features from encoder\nto decoder instead of an easy shortcut connection. The proposed model also\napplied the attention module in the decoder part to extract features from\nvariant scales. The proposed model is evaluated on a publicly available dataset\nby a variety of metrics. The experimental results show that our model achieves\ncompetitive performance compared with other state-of-the-art medical semantic\nsegmentation methods.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 12:09:16 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Li", "Wenqiang", ""], ["Tang", "YM", ""], ["Wang", "Ziyang", ""], ["Yu", "KM", ""], ["To", "Sandy", ""]]}, {"id": "2104.03722", "submitter": "Muhammad AbdurRafae", "authors": "Muhammad AbdurRafae", "title": "HindSight: A Graph-Based Vision Model Architecture For Representing\n  Part-Whole Hierarchies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents a model architecture for encoding the representations of\npart-whole hierarchies in images in form of a graph. The idea is to divide the\nimage into patches of different levels and then treat all of these patches as\nnodes for a fully connected graph. A dynamic feature extraction module is used\nto extract feature representations from these patches in each graph iteration.\nThis enables us to learn a rich graph representation of the image that\nencompasses the inherent part-whole hierarchical information. Utilizing proper\nself-supervised training techniques, such a model can be trained as a general\npurpose vision encoder model which can then be used for various vision related\ndownstream tasks (e.g., Image Classification, Object Detection, Image\nCaptioning, etc.).\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 12:17:54 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["AbdurRafae", "Muhammad", ""]]}, {"id": "2104.03725", "submitter": "Joan Serr\\`a", "authors": "Joan Serr\\`a, Santiago Pascual, Jordi Pons", "title": "On tuning consistent annealed sampling for denoising score matching", "comments": "3 pages and 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Score-based generative models provide state-of-the-art quality for image and\naudio synthesis. Sampling from these models is performed iteratively, typically\nemploying a discretized series of noise levels and a predefined scheme. In this\nnote, we first overview three common sampling schemes for models trained with\ndenoising score matching. Next, we focus on one of them, consistent annealed\nsampling, and study its hyper-parameter boundaries. We then highlight a\npossible formulation of such hyper-parameter that explicitly considers those\nboundaries and facilitates tuning when using few or a variable number of steps.\nFinally, we highlight some connections of the formulation with other sampling\nschemes.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 12:19:10 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Serr\u00e0", "Joan", ""], ["Pascual", "Santiago", ""], ["Pons", "Jordi", ""]]}, {"id": "2104.03736", "submitter": "Su Lu", "authors": "Su Lu, Han-Jia Ye, Le Gan, De-Chuan Zhan", "title": "Towards Enabling Meta-Learning from Target Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Meta-learning can extract an inductive bias from previous learning experience\nand assist the training processes of new tasks. It is often realized through\noptimizing a meta-model with the evaluation loss of a series of task-specific\nsolvers. Most existing algorithms sample non-overlapping $\\mathit{support}$\nsets and $\\mathit{query}$ sets to train and evaluate the solvers respectively\ndue to simplicity ($\\mathcal{S}/\\mathcal{Q}$ protocol). However, another\nevaluation method that assesses the discrepancy between the solver and a target\nmodel is short of research ($\\mathcal{S}/\\mathcal{T}$ protocol).\n$\\mathcal{S}/\\mathcal{T}$ protocol has unique advantages such as offering more\ninformative supervision, but it is computationally expensive. This paper looks\ninto this special evaluation method and takes a step towards putting it into\npractice. We find that with a small ratio of tasks armed with target models,\nclassic meta-learning algorithms can be improved a lot without consuming many\nresources. Furthermore, we empirically verify the effectiveness of\n$\\mathcal{S}/\\mathcal{T}$ protocol in a typical application of meta-learning,\n$\\mathit{i.e.}$, few-shot learning. In detail, after constructing target models\nby fine-tuning the pre-trained network on those hard tasks, we match the\ntask-specific solvers to target models via knowledge distillation. Experiments\ndemonstrate the superiority of our proposal.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 12:41:33 GMT"}, {"version": "v2", "created": "Sat, 5 Jun 2021 07:29:24 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Lu", "Su", ""], ["Ye", "Han-Jia", ""], ["Gan", "Le", ""], ["Zhan", "De-Chuan", ""]]}, {"id": "2104.03737", "submitter": "Su Lu", "authors": "Su Lu, Han-Jia Ye, De-Chuan Zhan", "title": "Few-Shot Action Recognition with Compromised Metric via Optimal\n  Transport", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Although vital to computer vision systems, few-shot action recognition is\nstill not mature despite the wide research of few-shot image classification.\nPopular few-shot learning algorithms extract a transferable embedding from seen\nclasses and reuse it on unseen classes by constructing a metric-based\nclassifier. One main obstacle to applying these algorithms in action\nrecognition is the complex structure of videos. Some existing solutions sample\nframes from a video and aggregate their embeddings to form a video-level\nrepresentation, neglecting important temporal relations. Others perform an\nexplicit sequence matching between two videos and define their distance as\nmatching cost, imposing too strong restrictions on sequence ordering. In this\npaper, we propose Compromised Metric via Optimal Transport (CMOT) to combine\nthe advantages of these two solutions. CMOT simultaneously considers semantic\nand temporal information in videos under Optimal Transport framework, and is\ndiscriminative for both content-sensitive and ordering-sensitive tasks. In\ndetail, given two videos, we sample segments from them and cast the calculation\nof their distance as an optimal transport problem between two segment\nsequences. To preserve the inherent temporal ordering information, we\nadditionally amend the ground cost matrix by penalizing it with the positional\ndistance between a pair of segments. Empirical results on benchmark datasets\ndemonstrate the superiority of CMOT.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 12:42:05 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Lu", "Su", ""], ["Ye", "Han-Jia", ""], ["Zhan", "De-Chuan", ""]]}, {"id": "2104.03739", "submitter": "Mostafa Mehdipour Ghazi", "authors": "Mostafa Mehdipour Ghazi, Lauge S{\\o}rensen, S\\'ebastien Ourselin, Mads\n  Nielsen", "title": "CARRNN: A Continuous Autoregressive Recurrent Neural Network for Deep\n  Representation Learning from Sporadic Temporal Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning temporal patterns from multivariate longitudinal data is challenging\nespecially in cases when data is sporadic, as often seen in, e.g., healthcare\napplications where the data can suffer from irregularity and asynchronicity as\nthe time between consecutive data points can vary across features and samples,\nhindering the application of existing deep learning models that are constructed\nfor complete, evenly spaced data with fixed sequence lengths. In this paper, a\nnovel deep learning-based model is developed for modeling multiple temporal\nfeatures in sporadic data using an integrated deep learning architecture based\non a recurrent neural network (RNN) unit and a continuous-time autoregressive\n(CAR) model. The proposed model, called CARRNN, uses a generalized\ndiscrete-time autoregressive model that is trainable end-to-end using neural\nnetworks modulated by time lags to describe the changes caused by the\nirregularity and asynchronicity. It is applied to multivariate time-series\nregression tasks using data provided for Alzheimer's disease progression\nmodeling and intensive care unit (ICU) mortality rate prediction, where the\nproposed model based on a gated recurrent unit (GRU) achieves the lowest\nprediction errors among the proposed RNN-based models and state-of-the-art\nmethods using GRUs and long short-term memory (LSTM) networks in their\narchitecture.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 12:43:44 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Ghazi", "Mostafa Mehdipour", ""], ["S\u00f8rensen", "Lauge", ""], ["Ourselin", "S\u00e9bastien", ""], ["Nielsen", "Mads", ""]]}, {"id": "2104.03762", "submitter": "Arka Sadhu", "authors": "Arka Sadhu, Kan Chen, Ram Nevatia", "title": "Video Question Answering with Phrases via Semantic Roles", "comments": "NAACL21 Camera Ready including appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video Question Answering (VidQA) evaluation metrics have been limited to a\nsingle-word answer or selecting a phrase from a fixed set of phrases. These\nmetrics limit the VidQA models' application scenario. In this work, we leverage\nsemantic roles derived from video descriptions to mask out certain phrases, to\nintroduce VidQAP which poses VidQA as a fill-in-the-phrase task. To enable\nevaluation of answer phrases, we compute the relative improvement of the\npredicted answer compared to an empty string. To reduce the influence of\nlanguage bias in VidQA datasets, we retrieve a video having a different answer\nfor the same question. To facilitate research, we construct ActivityNet-SRL-QA\nand Charades-SRL-QA and benchmark them by extending three vision-language\nmodels. We further perform extensive analysis and ablative studies to guide\nfuture work.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 13:27:43 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Sadhu", "Arka", ""], ["Chen", "Kan", ""], ["Nevatia", "Ram", ""]]}, {"id": "2104.03765", "submitter": "Yonghao Xu", "authors": "Yonghao Xu, Bo Du, and Liangpei Zhang", "title": "Robust Self-Ensembling Network for Hyperspectral Image Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent research has shown the great potential of deep learning algorithms in\nthe hyperspectral image (HSI) classification task. Nevertheless, training these\nmodels usually requires a large amount of labeled data. Since the collection of\npixel-level annotations for HSI is laborious and time-consuming, developing\nalgorithms that can yield good performance in the small sample size situation\nis of great significance. In this study, we propose a robust self-ensembling\nnetwork (RSEN) to address this problem. The proposed RSEN consists of two\nsubnetworks including a base network and an ensemble network. With the\nconstraint of both the supervised loss from the labeled data and the\nunsupervised loss from the unlabeled data, the base network and the ensemble\nnetwork can learn from each other, achieving the self-ensembling mechanism. To\nthe best of our knowledge, the proposed method is the first attempt to\nintroduce the self-ensembling technique into the HSI classification task, which\nprovides a different view on how to utilize the unlabeled data in HSI to assist\nthe network training. We further propose a novel consistency filter to increase\nthe robustness of self-ensembling learning. Extensive experiments on three\nbenchmark HSI datasets demonstrate that the proposed algorithm can yield\ncompetitive performance compared with the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 13:33:14 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Xu", "Yonghao", ""], ["Du", "Bo", ""], ["Zhang", "Liangpei", ""]]}, {"id": "2104.03768", "submitter": "Mo Zhang", "authors": "Mo Zhang, Fei Yu, Jie Zhao, Li Zhang, Quanzheng Li", "title": "BEFD: Boundary Enhancement and Feature Denoising for Vessel Segmentation", "comments": "MICCAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Blood vessel segmentation is crucial for many diagnostic and research\napplications. In recent years, CNN-based models have leaded to breakthroughs in\nthe task of segmentation, however, such methods usually lose high-frequency\ninformation like object boundaries and subtle structures, which are vital to\nvessel segmentation. To tackle this issue, we propose Boundary Enhancement and\nFeature Denoising (BEFD) module to facilitate the network ability of extracting\nboundary information in semantic segmentation, which can be integrated into\narbitrary encoder-decoder architecture in an end-to-end way. By introducing\nSobel edge detector, the network is able to acquire additional edge prior, thus\nenhancing boundary in an unsupervised manner for medical image segmentation. In\naddition, we also utilize a denoising block to reduce the noise hidden in the\nlow-level features. Experimental results on retinal vessel dataset and\nangiocarpy dataset demonstrate the superior performance of the new BEFD module.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 13:44:47 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Zhang", "Mo", ""], ["Yu", "Fei", ""], ["Zhao", "Jie", ""], ["Zhang", "Li", ""], ["Li", "Quanzheng", ""]]}, {"id": "2104.03775", "submitter": "Xuepeng Shi", "authors": "Xuepeng Shi, Qi Ye, Xiaozhi Chen, Chuangrong Chen, Zhixiang Chen,\n  Tae-Kyun Kim", "title": "Geometry-based Distance Decomposition for Monocular 3D Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monocular 3D object detection is of great significance for autonomous driving\nbut remains challenging. The core challenge is to predict the distance of\nobjects in the absence of explicit depth information. Unlike regressing the\ndistance as a single variable in most existing methods, we propose a novel\ngeometry-based distance decomposition to recover the distance by its factors.\nThe decomposition factors the distance of objects into the most representative\nand stable variables, i.e. the physical height and the projected visual height\nin the image plane. Moreover, the decomposition maintains the self-consistency\nbetween the two heights, leading to the robust distance prediction when both\npredicted heights are inaccurate. The decomposition also enables us to trace\nthe cause of the distance uncertainty for different scenarios. Such\ndecomposition makes the distance prediction interpretable, accurate, and\nrobust. Our method directly predicts 3D bounding boxes from RGB images with a\ncompact architecture, making the training and inference simple and efficient.\nThe experimental results show that our method achieves the state-of-the-art\nperformance on the monocular 3D Object detection and Birds Eye View tasks on\nthe KITTI dataset, and can generalize to images with different camera\nintrinsics.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 13:57:30 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Shi", "Xuepeng", ""], ["Ye", "Qi", ""], ["Chen", "Xiaozhi", ""], ["Chen", "Chuangrong", ""], ["Chen", "Zhixiang", ""], ["Kim", "Tae-Kyun", ""]]}, {"id": "2104.03777", "submitter": "Liheng Bian", "authors": "Daoyu Li, Liheng Bian, and Jun Zhang", "title": "Affine-modeled video extraction from a single motion blurred image", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A motion-blurred image is the temporal average of multiple sharp frames over\nthe exposure time. Recovering these sharp video frames from a single blurred\nimage is nontrivial, due to not only its strong ill-posedness, but also various\ntypes of complex motion in reality such as rotation and motion in depth. In\nthis work, we report a generalized video extraction method using the affine\nmotion modeling, enabling to tackle multiple types of complex motion and their\nmixing. In its workflow, the moving objects are first segemented in the alpha\nchannel. This allows separate recovery of different objects with different\nmotion. Then, we reduce the variable space by modeling each video clip as a\nseries of affine transformations of a reference frame, and introduce the\n$l0$-norm total variation regularization to attenuate the ringing artifact. The\ndifferentiable affine operators are employed to realize gradient-descent\noptimization of the affine model, which follows a novel coarse-to-fine strategy\nto further reduce artifacts. As a result, both the affine parameters and sharp\nreference image are retrieved. They are finally input into stepwise affine\ntransformation to recover the sharp video frames. The stepwise retrieval\nmaintains the nature to bypass the frame order ambiguity. Experiments on both\npublic datasets and real captured data validate the state-of-the-art\nperformance of the reported technique.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 13:59:14 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Li", "Daoyu", ""], ["Bian", "Liheng", ""], ["Zhang", "Jun", ""]]}, {"id": "2104.03778", "submitter": "Chuong Huynh", "authors": "Chuong Huynh, Anh Tran, Khoa Luu, Minh Hoai", "title": "Progressive Semantic Segmentation", "comments": "Accepted to CVPR'21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The objective of this work is to segment high-resolution images without\noverloading GPU memory usage or losing the fine details in the output\nsegmentation map. The memory constraint means that we must either downsample\nthe big image or divide the image into local patches for separate processing.\nHowever, the former approach would lose the fine details, while the latter can\nbe ambiguous due to the lack of a global picture. In this work, we present\nMagNet, a multi-scale framework that resolves local ambiguity by looking at the\nimage at multiple magnification levels. MagNet has multiple processing stages,\nwhere each stage corresponds to a magnification level, and the output of one\nstage is fed into the next stage for coarse-to-fine information propagation.\nEach stage analyzes the image at a higher resolution than the previous stage,\nrecovering the previously lost details due to the lossy downsampling step, and\nthe segmentation output is progressively refined through the processing stages.\nExperiments on three high-resolution datasets of urban views, aerial scenes,\nand medical images show that MagNet consistently outperforms the\nstate-of-the-art methods by a significant margin.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 13:59:27 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Huynh", "Chuong", ""], ["Tran", "Anh", ""], ["Luu", "Khoa", ""], ["Hoai", "Minh", ""]]}, {"id": "2104.03807", "submitter": "Zahra Gharaee", "authors": "Zahra Gharaee and Karl Holmquist and Linbo He and Michael Felsberg", "title": "A Bayesian Approach to Reinforcement Learning of Vision-Based Vehicular\n  Control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we present a state-of-the-art reinforcement learning method\nfor autonomous driving. Our approach employs temporal difference learning in a\nBayesian framework to learn vehicle control signals from sensor data. The agent\nhas access to images from a forward facing camera, which are preprocessed to\ngenerate semantic segmentation maps. We trained our system using both ground\ntruth and estimated semantic segmentation input. Based on our observations from\na large set of experiments, we conclude that training the system on ground\ntruth input data leads to better performance than training the system on\nestimated input even if estimated input is used for evaluation. The system is\ntrained and evaluated in a realistic simulated urban environment using the\nCARLA simulator. The simulator also contains a benchmark that allows for\ncomparing to other systems and methods. The required training time of the\nsystem is shown to be lower and the performance on the benchmark superior to\ncompeting approaches.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 14:34:57 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Gharaee", "Zahra", ""], ["Holmquist", "Karl", ""], ["He", "Linbo", ""], ["Felsberg", "Michael", ""]]}, {"id": "2104.03821", "submitter": "Wei Wang", "authors": "Wei Wang, Zheng Dang, Yinlin Hu, Pascal Fua and Mathieu Salzmann", "title": "Robust Differentiable SVD", "comments": "IEEE Transactions on Pattern Analysis and Machine Intelligence\n  (TPAMI) PREPRINT 2021", "journal-ref": "IEEE Transactions on Pattern Analysis and Machine Intelligence\n  (TPAMI) 2021", "doi": "10.1109/TPAMI.2021.3072422", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Eigendecomposition of symmetric matrices is at the heart of many computer\nvision algorithms. However, the derivatives of the eigenvectors tend to be\nnumerically unstable, whether using the SVD to compute them analytically or\nusing the Power Iteration (PI) method to approximate them. This instability\narises in the presence of eigenvalues that are close to each other. This makes\nintegrating eigendecomposition into deep networks difficult and often results\nin poor convergence, particularly when dealing with large matrices.\n  While this can be mitigated by partitioning the data into small arbitrary\ngroups, doing so has no theoretical basis and makes it impossible to exploit\nthe full power of eigendecomposition. In previous work, we mitigated this using\nSVD during the forward pass and PI to compute the gradients during the backward\npass. However, the iterative deflation procedure required to compute multiple\neigenvectors using PI tends to accumulate errors and yield inaccurate\ngradients. Here, we show that the Taylor expansion of the SVD gradient is\ntheoretically equivalent to the gradient obtained using PI without relying in\npractice on an iterative process and thus yields more accurate gradients. We\ndemonstrate the benefits of this increased accuracy for image classification\nand style transfer.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 15:04:15 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Wang", "Wei", ""], ["Dang", "Zheng", ""], ["Hu", "Yinlin", ""], ["Fua", "Pascal", ""], ["Salzmann", "Mathieu", ""]]}, {"id": "2104.03829", "submitter": "Abhijit Guha Roy", "authors": "Abhijit Guha Roy, Jie Ren, Shekoofeh Azizi, Aaron Loh, Vivek\n  Natarajan, Basil Mustafa, Nick Pawlowski, Jan Freyberg, Yuan Liu, Zach\n  Beaver, Nam Vo, Peggy Bui, Samantha Winter, Patricia MacWilliams, Greg S.\n  Corrado, Umesh Telang, Yun Liu, Taylan Cemgil, Alan Karthikesalingam, Balaji\n  Lakshminarayanan, Jim Winkens", "title": "Does Your Dermatology Classifier Know What It Doesn't Know? Detecting\n  the Long-Tail of Unseen Conditions", "comments": "Under Review, 19 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We develop and rigorously evaluate a deep learning based system that can\naccurately classify skin conditions while detecting rare conditions for which\nthere is not enough data available for training a confident classifier. We\nframe this task as an out-of-distribution (OOD) detection problem. Our novel\napproach, hierarchical outlier detection (HOD) assigns multiple abstention\nclasses for each training outlier class and jointly performs a coarse\nclassification of inliers vs. outliers, along with fine-grained classification\nof the individual classes. We demonstrate the effectiveness of the HOD loss in\nconjunction with modern representation learning approaches (BiT, SimCLR, MICLe)\nand explore different ensembling strategies for further improving the results.\nWe perform an extensive subgroup analysis over conditions of varying risk\nlevels and different skin types to investigate how the OOD detection\nperformance changes over each subgroup and demonstrate the gains of our\nframework in comparison to baselines. Finally, we introduce a cost metric to\napproximate downstream clinical impact. We use this cost metric to compare the\nproposed method against a baseline system, thereby making a stronger case for\nthe overall system effectiveness in a real-world deployment scenario.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 15:15:22 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Roy", "Abhijit Guha", ""], ["Ren", "Jie", ""], ["Azizi", "Shekoofeh", ""], ["Loh", "Aaron", ""], ["Natarajan", "Vivek", ""], ["Mustafa", "Basil", ""], ["Pawlowski", "Nick", ""], ["Freyberg", "Jan", ""], ["Liu", "Yuan", ""], ["Beaver", "Zach", ""], ["Vo", "Nam", ""], ["Bui", "Peggy", ""], ["Winter", "Samantha", ""], ["MacWilliams", "Patricia", ""], ["Corrado", "Greg S.", ""], ["Telang", "Umesh", ""], ["Liu", "Yun", ""], ["Cemgil", "Taylan", ""], ["Karthikesalingam", "Alan", ""], ["Lakshminarayanan", "Balaji", ""], ["Winkens", "Jim", ""]]}, {"id": "2104.03840", "submitter": "Anneke Meyer", "authors": "Anneke Meyer, Suhita Ghosh, Daniel Schindele, Martin Schostak,\n  Sebastian Stober, Christian Hansen, Marko Rak", "title": "Uncertainty-Aware Temporal Self-Learning (UATS): Semi-Supervised\n  Learning for Segmentation of Prostate Zones and Beyond", "comments": "Accepted manuscript in Elsevier Artificial Intelligence in Medicine.\n  Anneke Meyer and Suhita Ghosh contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Various convolutional neural network (CNN) based concepts have been\nintroduced for the prostate's automatic segmentation and its coarse subdivision\ninto transition zone (TZ) and peripheral zone (PZ). However, when targeting a\nfine-grained segmentation of TZ, PZ, distal prostatic urethra (DPU) and the\nanterior fibromuscular stroma (AFS), the task becomes more challenging and has\nnot yet been solved at the level of human performance. One reason might be the\ninsufficient amount of labeled data for supervised training. Therefore, we\npropose to apply a semi-supervised learning (SSL) technique named\nuncertainty-aware temporal self-learning (UATS) to overcome the expensive and\ntime-consuming manual ground truth labeling. We combine the SSL techniques\ntemporal ensembling and uncertainty-guided self-learning to benefit from\nunlabeled images, which are often readily available. Our method significantly\noutperforms the supervised baseline and obtained a Dice coefficient (DC) of up\nto 78.9% , 87.3%, 75.3%, 50.6% for TZ, PZ, DPU and AFS, respectively. The\nobtained results are in the range of human inter-rater performance for all\nstructures. Moreover, we investigate the method's robustness against noise and\ndemonstrate the generalization capability for varying ratios of labeled data\nand on other challenging tasks, namely the hippocampus and skin lesion\nsegmentation. UATS achieved superiority segmentation quality compared to the\nsupervised baseline, particularly for minimal amounts of labeled data.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 15:31:57 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Meyer", "Anneke", ""], ["Ghosh", "Suhita", ""], ["Schindele", "Daniel", ""], ["Schostak", "Martin", ""], ["Stober", "Sebastian", ""], ["Hansen", "Christian", ""], ["Rak", "Marko", ""]]}, {"id": "2104.03841", "submitter": "Daniela Massiceti", "authors": "Daniela Massiceti, Luisa Zintgraf, John Bronskill, Lida Theodorou,\n  Matthew Tobias Harris, Edward Cutrell, Cecily Morrison, Katja Hofmann, Simone\n  Stumpf", "title": "ORBIT: A Real-World Few-Shot Dataset for Teachable Object Recognition", "comments": null, "journal-ref": null, "doi": "10.25383/city.14294597", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Object recognition has made great advances in the last decade, but\npredominately still relies on many high-quality training examples per object\ncategory. In contrast, learning new objects from only a few examples could\nenable many impactful applications from robotics to user personalization. Most\nfew-shot learning research, however, has been driven by benchmark datasets that\nlack the high variation that these applications will face when deployed in the\nreal-world. To close this gap, we present the ORBIT dataset and benchmark,\ngrounded in a real-world application of teachable object recognizers for people\nwho are blind/low-vision. The dataset contains 3,822 videos of 486 objects\nrecorded by people who are blind/low-vision on their mobile phones, and the\nbenchmark reflects a realistic, highly challenging recognition problem,\nproviding a rich playground to drive research in robustness to few-shot,\nhigh-variation conditions. We set the first state-of-the-art on the benchmark\nand show that there is massive scope for further innovation, holding the\npotential to impact a broad range of real-world vision applications including\ntools for the blind/low-vision community. The dataset is available at\nhttps://bit.ly/2OyElCj and the code to run the benchmark at\nhttps://bit.ly/39YgiUW.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 15:32:01 GMT"}, {"version": "v2", "created": "Fri, 9 Apr 2021 16:56:43 GMT"}, {"version": "v3", "created": "Thu, 10 Jun 2021 14:50:34 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Massiceti", "Daniela", ""], ["Zintgraf", "Luisa", ""], ["Bronskill", "John", ""], ["Theodorou", "Lida", ""], ["Harris", "Matthew Tobias", ""], ["Cutrell", "Edward", ""], ["Morrison", "Cecily", ""], ["Hofmann", "Katja", ""], ["Stumpf", "Simone", ""]]}, {"id": "2104.03843", "submitter": "Moab Arar", "authors": "Moab Arar, Ariel Shamir, Amit Bermano", "title": "InAugment: Improving Classifiers via Internal Augmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Image augmentation techniques apply transformation functions such as\nrotation, shearing, or color distortion on an input image. These augmentations\nwere proven useful in improving neural networks' generalization ability. In\nthis paper, we present a novel augmentation operation, InAugment, that exploits\nimage internal statistics. The key idea is to copy patches from the image\nitself, apply augmentation operations on them, and paste them back at random\npositions on the same image. This method is simple and easy to implement and\ncan be incorporated with existing augmentation techniques. We test InAugment on\ntwo popular datasets -- CIFAR and ImageNet. We show improvement over\nstate-of-the-art augmentation techniques. Incorporating InAugment with Auto\nAugment yields a significant improvement over other augmentation techniques\n(e.g., +1% improvement over multiple architectures trained on the CIFAR\ndataset). We also demonstrate an increase for ResNet50 and EfficientNet-B3\ntop-1's accuracy on the ImageNet dataset compared to prior augmentation\nmethods. Finally, our experiments suggest that training convolutional neural\nnetwork using InAugment not only improves the model's accuracy and confidence\nbut its performance on out-of-distribution images.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 15:37:21 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Arar", "Moab", ""], ["Shamir", "Ariel", ""], ["Bermano", "Amit", ""]]}, {"id": "2104.03851", "submitter": "Shamit Lal", "authors": "Shamit Lal, Mihir Prabhudesai, Ishita Mediratta, Adam W. Harley,\n  Katerina Fragkiadaki", "title": "CoCoNets: Continuous Contrastive 3D Scene Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores self-supervised learning of amodal 3D feature\nrepresentations from RGB and RGB-D posed images and videos, agnostic to object\nand scene semantic content, and evaluates the resulting scene representations\nin the downstream tasks of visual correspondence, object tracking, and object\ndetection. The model infers a latent3D representation of the scene in the form\nof 3D feature points, where each continuous world 3D point is mapped to its\ncorresponding feature vector. The model is trained for contrastive view\nprediction by rendering 3D feature clouds in queried viewpoints and matching\nagainst the 3D feature point cloud predicted from the query view. Notably, the\nrepresentation can be queried for any 3D location, even if it is not visible\nfrom the input view. Our model brings together three powerful ideas of recent\nexciting research work: 3D feature grids as a neural bottleneck for view\nprediction, implicit functions for handling resolution limitations of 3D grids,\nand contrastive learning for unsupervised training of feature representations.\nWe show the resulting 3D visual feature representations effectively scale\nacross objects and scenes, imagine information occluded or missing from the\ninput viewpoints, track objects over time, align semantically related objects\nin 3D, and improve 3D object detection. We outperform many existing\nstate-of-the-art methods for 3D feature learning and view prediction, which are\neither limited by 3D grid spatial resolution, do not attempt to build amodal 3D\nrepresentations, or do not handle combinatorial scene variability due to their\nnon-convolutional bottlenecks.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 15:50:47 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Lal", "Shamit", ""], ["Prabhudesai", "Mihir", ""], ["Mediratta", "Ishita", ""], ["Harley", "Adam W.", ""], ["Fragkiadaki", "Katerina", ""]]}, {"id": "2104.03854", "submitter": "Viktor Varkarakis", "authors": "Viktor Varkarakis, Wang Yao, Peter Corcoran", "title": "Towards End-to-End Neural Face Authentication in the Wild -- Quantifying\n  and Compensating for Directional Lighting Effects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The recent availability of low-power neural accelerator hardware, combined\nwith improvements in end-to-end neural facial recognition algorithms provides,\nenabling technology for on-device facial authentication. The present research\nwork examines the effects of directional lighting on a State-of-Art(SoA) neural\nface recognizer. A synthetic re-lighting technique is used to augment data\nsamples due to the lack of public data-sets with sufficient directional\nlighting variations. Top lighting and its variants (top-left, top-right) are\nfound to have minimal effect on accuracy, while bottom-left or bottom-right\ndirectional lighting has the most pronounced effects. Following the fine-tuning\nof network weights, the face recognition model is shown to achieve close to the\noriginal Receiver Operating Characteristic curve (ROC)performance across all\nlighting conditions and demonstrates an ability to generalize beyond the\nlighting augmentations used in the fine-tuning data-set. This work shows that\nan SoA neural face recognition model can be tuned to compensate for directional\nlighting effects, removing the need for a pre-processing step before applying\nfacial recognition.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 15:58:09 GMT"}], "update_date": "2021-04-10", "authors_parsed": [["Varkarakis", "Viktor", ""], ["Yao", "Wang", ""], ["Corcoran", "Peter", ""]]}, {"id": "2104.03856", "submitter": "Haoyang Ye", "authors": "Haoyang Ye, Huaiyang Huang, Marco Hutter, Timothy Sandy, Ming Liu", "title": "3D Surfel Map-Aided Visual Relocalization with Learned Descriptors", "comments": "To appear in ICRA 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we introduce a method for visual relocalization using the\ngeometric information from a 3D surfel map. A visual database is first built by\nglobal indices from the 3D surfel map rendering, which provides associations\nbetween image points and 3D surfels. Surfel reprojection constraints are\nutilized to optimize the keyframe poses and map points in the visual database.\nA hierarchical camera relocalization algorithm then utilizes the visual\ndatabase to estimate 6-DoF camera poses. Learned descriptors are further used\nto improve the performance in challenging cases. We present evaluation under\nreal-world conditions and simulation to show the effectiveness and efficiency\nof our method, and make the final camera poses consistently well aligned with\nthe 3D environment.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 15:59:57 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Ye", "Haoyang", ""], ["Huang", "Huaiyang", ""], ["Hutter", "Marco", ""], ["Sandy", "Timothy", ""], ["Liu", "Ming", ""]]}, {"id": "2104.03864", "submitter": "Deblina Bhattacharjee", "authors": "Bahar Aydemir, Deblina Bhattacharjee, Seungryong Kim, Tong Zhang,\n  Mathieu Salzmann and Sabine S\\\"usstrunk", "title": "Modeling Object Dissimilarity for Deep Saliency Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Saliency prediction has made great strides over the past two decades, with\ncurrent techniques modeling low-level information, such as color, intensity and\nsize contrasts, and high-level one, such as attention and gaze direction for\nentire objects. Despite this, these methods fail to account for the\ndissimilarity between objects, which humans naturally do. In this paper, we\nintroduce a detection-guided saliency prediction network that explicitly models\nthe differences between multiple objects, such as their appearance and size\ndissimilarities. Our approach is general, allowing us to fuse our object\ndissimilarities with features extracted by any deep saliency prediction\nnetwork. As evidenced by our experiments, this consistently boosts the accuracy\nof the baseline networks, enabling us to outperform the state-of-the-art models\non three saliency benchmarks, namely SALICON, MIT300 and CAT2000.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 16:10:37 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Aydemir", "Bahar", ""], ["Bhattacharjee", "Deblina", ""], ["Kim", "Seungryong", ""], ["Zhang", "Tong", ""], ["Salzmann", "Mathieu", ""], ["S\u00fcsstrunk", "Sabine", ""]]}, {"id": "2104.03866", "submitter": "Fabio Tosi", "authors": "Fabio Tosi, Yiyi Liao, Carolin Schmitt, Andreas Geiger", "title": "SMD-Nets: Stereo Mixture Density Networks", "comments": "CVPR 2021. Project Page: https://github.com/fabiotosi92/SMD-Nets", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite stereo matching accuracy has greatly improved by deep learning in the\nlast few years, recovering sharp boundaries and high-resolution outputs\nefficiently remains challenging. In this paper, we propose Stereo Mixture\nDensity Networks (SMD-Nets), a simple yet effective learning framework\ncompatible with a wide class of 2D and 3D architectures which ameliorates both\nissues. Specifically, we exploit bimodal mixture densities as output\nrepresentation and show that this allows for sharp and precise disparity\nestimates near discontinuities while explicitly modeling the aleatoric\nuncertainty inherent in the observations. Moreover, we formulate disparity\nestimation as a continuous problem in the image domain, allowing our model to\nquery disparities at arbitrary spatial precision. We carry out comprehensive\nexperiments on a new high-resolution and highly realistic synthetic stereo\ndataset, consisting of stereo pairs at 8Mpx resolution, as well as on\nreal-world stereo datasets. Our experiments demonstrate increased depth\naccuracy near object boundaries and prediction of ultra high-resolution\ndisparity maps on standard GPUs. We demonstrate the flexibility of our\ntechnique by improving the performance of a variety of stereo backbones.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 16:15:46 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Tosi", "Fabio", ""], ["Liao", "Yiyi", ""], ["Schmitt", "Carolin", ""], ["Geiger", "Andreas", ""]]}, {"id": "2104.03888", "submitter": "Manuel Carranza-Garc\\'ia", "authors": "Manuel Carranza-Garc\\'ia, Pedro Lara-Ben\\'itez, Jorge\n  Garc\\'ia-Guti\\'errez, Jos\\'e C. Riquelme", "title": "Enhancing Object Detection for Autonomous Driving by Optimizing Anchor\n  Generation and Addressing Class Imbalance", "comments": null, "journal-ref": "Neurocomputing, 2021", "doi": "10.1016/j.neucom.2021.04.001", "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Object detection has been one of the most active topics in computer vision\nfor the past years. Recent works have mainly focused on pushing the\nstate-of-the-art in the general-purpose COCO benchmark. However, the use of\nsuch detection frameworks in specific applications such as autonomous driving\nis yet an area to be addressed. This study presents an enhanced 2D object\ndetector based on Faster R-CNN that is better suited for the context of\nautonomous vehicles. Two main aspects are improved: the anchor generation\nprocedure and the performance drop in minority classes. The default uniform\nanchor configuration is not suitable in this scenario due to the perspective\nprojection of the vehicle cameras. Therefore, we propose a perspective-aware\nmethodology that divides the image into key regions via clustering and uses\nevolutionary algorithms to optimize the base anchors for each of them.\nFurthermore, we add a module that enhances the precision of the second-stage\nheader network by including the spatial information of the candidate regions\nproposed in the first stage. We also explore different re-weighting strategies\nto address the foreground-foreground class imbalance, showing that the use of a\nreduced version of focal loss can significantly improve the detection of\ndifficult and underrepresented objects in two-stage detectors. Finally, we\ndesign an ensemble model to combine the strengths of the different learning\nstrategies. Our proposal is evaluated with the Waymo Open Dataset, which is the\nmost extensive and diverse up to date. The results demonstrate an average\naccuracy improvement of 6.13% mAP when using the best single model, and of\n9.69% mAP with the ensemble. The proposed modifications over the Faster R-CNN\ndo not increase computational cost and can easily be extended to optimize other\nanchor-based detection frameworks.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 16:58:31 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Carranza-Garc\u00eda", "Manuel", ""], ["Lara-Ben\u00edtez", "Pedro", ""], ["Garc\u00eda-Guti\u00e9rrez", "Jorge", ""], ["Riquelme", "Jos\u00e9 C.", ""]]}, {"id": "2104.03893", "submitter": "Mehrshad Zandigohar", "authors": "Mehrshad Zandigohar, Mo Han, Mohammadreza Sharif, Sezen Yagmur Gunay,\n  Mariusz P. Furmanek, Mathew Yarossi, Paolo Bonato, Cagdas Onal, Taskin Padir,\n  Deniz Erdogmus, Gunar Schirner", "title": "Multimodal Fusion of EMG and Vision for Human Grasp Intent Inference in\n  Prosthetic Hand Control", "comments": "This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.HC eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For lower arm amputees, robotic prosthetic hands offer the promise to regain\nthe capability to perform fine object manipulation in activities of daily\nliving. Current control methods based on physiological signals such as EEG and\nEMG are prone to poor inference outcomes due to motion artifacts, variability\nof skin electrode junction impedance over time, muscle fatigue, and other\nfactors. Visual evidence is also susceptible to its own artifacts, most often\ndue to object occlusion, lighting changes, variable shapes of objects depending\non view-angle, among other factors. Multimodal evidence fusion using\nphysiological and vision sensor measurements is a natural approach due to the\ncomplementary strengths of these modalities.\n  In this paper, we present a Bayesian evidence fusion framework for grasp\nintent inference using eye-view video, gaze, and EMG from the forearm processed\nby neural network models. We analyze individual and fused performance as a\nfunction of time as the hand approaches the object to grasp it. For this\npurpose, we have also developed novel data processing and augmentation\ntechniques to train neural network components. Our experimental data analyses\ndemonstrate that EMG and visual evidence show complementary strengths, and as a\nconsequence, fusion of multimodal evidence can outperform each individual\nevidence modality at any given time. Specifically, results indicate that, on\naverage, fusion improves the instantaneous upcoming grasp type classification\naccuracy while in the reaching phase by 13.66% and 14.8%, relative to EMG and\nvisual evidence individually. An overall fusion accuracy of 95.3% among 13\nlabels (compared to a chance level of 7.7%) is achieved, and more detailed\nanalysis indicate that the correct grasp is inferred sufficiently early and\nwith high confidence compared to the top contender, in order to allow\nsuccessful robot actuation to close the loop.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 17:01:19 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Zandigohar", "Mehrshad", ""], ["Han", "Mo", ""], ["Sharif", "Mohammadreza", ""], ["Gunay", "Sezen Yagmur", ""], ["Furmanek", "Mariusz P.", ""], ["Yarossi", "Mathew", ""], ["Bonato", "Paolo", ""], ["Onal", "Cagdas", ""], ["Padir", "Taskin", ""], ["Erdogmus", "Deniz", ""], ["Schirner", "Gunar", ""]]}, {"id": "2104.03916", "submitter": "Thomas Mitchel", "authors": "Thomas W. Mitchel, Vladimir G. Kim, Michael Kazhdan", "title": "Field Convolutions for Surface CNNs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel surface convolution operator acting on vector fields that\nis based on a simple observation: instead of combining neighboring features\nwith respect to a single coordinate parameterization defined at a given point,\nwe have every neighbor describe the position of the point within its own\ncoordinate frame. This formulation combines intrinsic spatial convolution with\nparallel transport in a scattering operation while placing no constraints on\nthe filters themselves, providing a definition of convolution that commutes\nwith the action of isometries, has increased descriptive potential, and is\nrobust to noise and other nuisance factors. The result is a rich notion of\nconvolution which we call field convolution, well-suited for CNNs on surfaces.\nField convolutions are flexible and straight-forward to implement, and their\nhighly discriminating nature has cascading effects throughout the learning\npipeline. Using simple networks constructed from residual field convolution\nblocks, we achieve state-of-the-art results on standard benchmarks in\nfundamental geometry processing tasks, such as shape classification,\nsegmentation, correspondence, and sparse matching.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 17:11:14 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Mitchel", "Thomas W.", ""], ["Kim", "Vladimir G.", ""], ["Kazhdan", "Michael", ""]]}, {"id": "2104.03926", "submitter": "Guanghao Yin", "authors": "Guanghao Yin, Wei Wang, Zehuan Yuan, Dongdong Yu, Shouqian Sun,\n  Changhu Wang", "title": "Conditional Meta-Network for Blind Super-Resolution with Multiple\n  Degradations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although single-image super-resolution (SISR) methods have achieved great\nsuccess on single degradation, they still suffer performance drop with multiple\ndegrading effects in real scenarios. Recently, some blind and non-blind models\nfor multiple degradations have been explored. However, those methods usually\ndegrade significantly for distribution shifts between the training and test\ndata. Towards this end, we propose a conditional meta-network framework (named\nCMDSR) for the first time, which helps SR framework learn how to adapt to\nchanges in input distribution. We extract degradation prior at task-level with\nthe proposed ConditionNet, which will be used to adapt the parameters of the\nbasic SR network (BaseNet). Specifically, the ConditionNet of our framework\nfirst learns the degradation prior from a support set, which is composed of a\nseries of degraded image patches from the same task. Then the adaptive BaseNet\nrapidly shifts its parameters according to the conditional features. Moreover,\nin order to better extract degradation prior, we propose a task contrastive\nloss to decrease the inner-task distance and increase the cross-task distance\nbetween task-level features. Without predefining degradation maps, our blind\nframework can conduct one single parameter update to yield considerable SR\nresults. Extensive experiments demonstrate the effectiveness of CMDSR over\nvarious blind, even non-blind methods. The flexible BaseNet structure also\nreveals that CMDSR can be a general framework for large series of SISR models.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 17:15:25 GMT"}, {"version": "v2", "created": "Fri, 9 Apr 2021 13:16:57 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Yin", "Guanghao", ""], ["Wang", "Wei", ""], ["Yuan", "Zehuan", ""], ["Yu", "Dongdong", ""], ["Sun", "Shouqian", ""], ["Wang", "Changhu", ""]]}, {"id": "2104.03927", "submitter": "Jorge F. Lazo", "authors": "Jorge F. Lazo, Sara Moccia, Aldo Marzullo, Michele Catellani, Ottavio\n  De Cobelli, Benoit Rosa, Michel de Mathelin, Elena De Momi", "title": "A transfer-learning approach for lesion detection in endoscopic images\n  from the urinary tract", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Ureteroscopy and cystoscopy are the gold standard methods to identify and\ntreat tumors along the urinary tract. It has been reported that during a normal\nprocedure a rate of 10-20 % of the lesions could be missed. In this work we\nstudy the implementation of 3 different Convolutional Neural Networks (CNNs),\nusing a 2-steps training strategy, to classify images from the urinary tract\nwith and without lesions. A total of 6,101 images from ureteroscopy and\ncystoscopy procedures were collected. The CNNs were trained and tested using\ntransfer learning in a two-steps fashion on 3 datasets. The datasets used were:\n1) only ureteroscopy images, 2) only cystoscopy images and 3) the combination\nof both of them. For cystoscopy data, VGG performed better obtaining an Area\nUnder the ROC Curve (AUC) value of 0.846. In the cases of ureteroscopy and the\ncombination of both datasets, ResNet50 achieved the best results with AUC\nvalues of 0.987 and 0.940. The use of a training dataset that comprehends both\ndomains results in general better performances, but performing a second stage\nof transfer learning achieves comparable ones. There is no single model which\nperforms better in all scenarios, but ResNet50 is the network that achieves the\nbest performances in most of them. The obtained results open the opportunity\nfor further investigation with a view for improving lesion detection in\nendoscopic images of the urinary system.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 17:16:12 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Lazo", "Jorge F.", ""], ["Moccia", "Sara", ""], ["Marzullo", "Aldo", ""], ["Catellani", "Michele", ""], ["De Cobelli", "Ottavio", ""], ["Rosa", "Benoit", ""], ["de Mathelin", "Michel", ""], ["De Momi", "Elena", ""]]}, {"id": "2104.03952", "submitter": "Niv Cohen", "authors": "Niv Cohen and Yedid Hoshen", "title": "The Single-Noun Prior for Image Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-supervised clustering methods have achieved increasing accuracy in\nrecent years but do not yet perform as well as supervised classification\nmethods. This contrasts with the situation for feature learning, where\nself-supervised features have recently surpassed the performance of supervised\nfeatures on several important tasks. We hypothesize that the performance gap is\ndue to the difficulty of specifying, without supervision, which features\ncorrespond to class differences that are semantic to humans. To reduce the\nperformance gap, we introduce the \"single-noun\" prior - which states that\nsemantic clusters tend to correspond to concepts that humans label by a\nsingle-noun. By utilizing a pre-trained network that maps images and sentences\ninto a common space, we impose this prior obtaining a constrained optimization\ntask. We show that our formulation is a special case of the facility location\nproblem, and introduce a simple-yet-effective approach for solving this\noptimization task at scale. We test our approach on several commonly reported\nimage clustering datasets and obtain significant accuracy gains over the best\nexisting approaches.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 17:54:37 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Cohen", "Niv", ""], ["Hoshen", "Yedid", ""]]}, {"id": "2104.03953", "submitter": "Xu Chen", "authors": "Xu Chen, Yufeng Zheng, Michael J. Black, Otmar Hilliges, Andreas\n  Geiger", "title": "SNARF: Differentiable Forward Skinning for Animating Non-Rigid Neural\n  Implicit Shapes", "comments": "video: https://youtu.be/rCEpFTKjFHE ; supplementary materials:\n  https://bit.ly/3t1Tk6F", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural implicit surface representations have emerged as a promising paradigm\nto capture 3D shapes in a continuous and resolution-independent manner.\nHowever, adapting them to articulated shapes is non-trivial. Existing\napproaches learn a backward warp field that maps deformed to canonical points.\nHowever, this is problematic since the backward warp field is pose dependent\nand thus requires large amounts of data to learn. To address this, we introduce\nSNARF, which combines the advantages of linear blend skinning (LBS) for\npolygonal meshes with those of neural implicit surfaces by learning a forward\ndeformation field without direct supervision. This deformation field is defined\nin canonical, pose-independent space, allowing for generalization to unseen\nposes. Learning the deformation field from posed meshes alone is challenging\nsince the correspondences of deformed points are defined implicitly and may not\nbe unique under changes of topology. We propose a forward skinning model that\nfinds all canonical correspondences of any deformed point using iterative root\nfinding. We derive analytical gradients via implicit differentiation, enabling\nend-to-end training from 3D meshes with bone transformations. Compared to\nstate-of-the-art neural implicit representations, our approach generalizes\nbetter to unseen poses while preserving accuracy. We demonstrate our method in\nchallenging scenarios on (clothed) 3D humans in diverse and unseen poses.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 17:54:59 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Chen", "Xu", ""], ["Zheng", "Yufeng", ""], ["Black", "Michael J.", ""], ["Hilliges", "Otmar", ""], ["Geiger", "Andreas", ""]]}, {"id": "2104.03954", "submitter": "Shangzhe Wu", "authors": "Shangzhe Wu and Ameesh Makadia and Jiajun Wu and Noah Snavely and\n  Richard Tucker and Angjoo Kanazawa", "title": "De-rendering the World's Revolutionary Artefacts", "comments": "CVPR 2021. Project page: https://sorderender.github.io/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent works have shown exciting results in unsupervised image de-rendering\n-- learning to decompose 3D shape, appearance, and lighting from single-image\ncollections without explicit supervision. However, many of these assume\nsimplistic material and lighting models. We propose a method, termed RADAR,\nthat can recover environment illumination and surface materials from real\nsingle-image collections, relying neither on explicit 3D supervision, nor on\nmulti-view or multi-light images. Specifically, we focus on rotationally\nsymmetric artefacts that exhibit challenging surface properties including\nspecular reflections, such as vases. We introduce a novel self-supervised\nalbedo discriminator, which allows the model to recover plausible albedo\nwithout requiring any ground-truth during training. In conjunction with a shape\nreconstruction module exploiting rotational symmetry, we present an end-to-end\nlearning framework that is able to de-render the world's revolutionary\nartefacts. We conduct experiments on a real vase dataset and demonstrate\ncompelling decomposition results, allowing for applications including\nfree-viewpoint rendering and relighting.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 17:56:16 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Wu", "Shangzhe", ""], ["Makadia", "Ameesh", ""], ["Wu", "Jiajun", ""], ["Snavely", "Noah", ""], ["Tucker", "Richard", ""], ["Kanazawa", "Angjoo", ""]]}, {"id": "2104.03956", "submitter": "Sean Segal", "authors": "Sean Segal, Nishanth Kumar, Sergio Casas, Wenyuan Zeng, Mengye Ren,\n  Jingkang Wang, Raquel Urtasun", "title": "Just Label What You Need: Fine-Grained Active Selection for Perception\n  and Prediction through Partially Labeled Scenes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-driving vehicles must perceive and predict the future positions of\nnearby actors in order to avoid collisions and drive safely. A learned deep\nlearning module is often responsible for this task, requiring large-scale,\nhigh-quality training datasets. As data collection is often significantly\ncheaper than labeling in this domain, the decision of which subset of examples\nto label can have a profound impact on model performance. Active learning\ntechniques, which leverage the state of the current model to iteratively select\nexamples for labeling, offer a promising solution to this problem. However,\ndespite the appeal of this approach, there has been little scientific analysis\nof active learning approaches for the perception and prediction (P&P) problem.\nIn this work, we study active learning techniques for P&P and find that the\ntraditional active learning formulation is ill-suited for the P&P setting. We\nthus introduce generalizations that ensure that our approach is both cost-aware\nand allows for fine-grained selection of examples through partially labeled\nscenes. Our experiments on a real-world, large-scale self-driving dataset\nsuggest that fine-grained selection can improve the performance across\nperception, prediction, and downstream planning tasks.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 17:57:41 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Segal", "Sean", ""], ["Kumar", "Nishanth", ""], ["Casas", "Sergio", ""], ["Zeng", "Wenyuan", ""], ["Ren", "Mengye", ""], ["Wang", "Jingkang", ""], ["Urtasun", "Raquel", ""]]}, {"id": "2104.03960", "submitter": "Ishit Mehta", "authors": "Ishit Mehta, Micha\\\"el Gharbi, Connelly Barnes, Eli Shechtman, Ravi\n  Ramamoorthi, Manmohan Chandraker", "title": "Modulated Periodic Activations for Generalizable Local Functional\n  Representations", "comments": "Project Page at https://ishit.github.io/modsine/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multi-Layer Perceptrons (MLPs) make powerful functional representations for\nsampling and reconstruction problems involving low-dimensional signals like\nimages,shapes and light fields. Recent works have significantly improved their\nability to represent high-frequency content by using periodic activations or\npositional encodings. This often came at the expense of generalization: modern\nmethods are typically optimized for a single signal. We present a new\nrepresentation that generalizes to multiple instances and achieves\nstate-of-the-art fidelity. We use a dual-MLP architecture to encode the\nsignals. A synthesis network creates a functional mapping from a\nlow-dimensional input (e.g. pixel-position) to the output domain (e.g. RGB\ncolor). A modulation network maps a latent code corresponding to the target\nsignal to parameters that modulate the periodic activations of the synthesis\nnetwork. We also propose a local-functional representation which enables\ngeneralization. The signal's domain is partitioned into a regular grid,with\neach tile represented by a latent code. At test time, the signal is encoded\nwith high-fidelity by inferring (or directly optimizing) the latent code-book.\nOur approach produces generalizable functional representations of images,\nvideos and shapes, and achieves higher reconstruction quality than prior works\nthat are optimized for a single signal.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 17:59:04 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Mehta", "Ishit", ""], ["Gharbi", "Micha\u00ebl", ""], ["Barnes", "Connelly", ""], ["Shechtman", "Eli", ""], ["Ramamoorthi", "Ravi", ""], ["Chandraker", "Manmohan", ""]]}, {"id": "2104.03962", "submitter": "Colin Graber", "authors": "Colin Graber and Grace Tsai and Michael Firman and Gabriel Brostow and\n  Alexander Schwing", "title": "Panoptic Segmentation Forecasting", "comments": "CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Our goal is to forecast the near future given a set of recent observations.\nWe think this ability to forecast, i.e., to anticipate, is integral for the\nsuccess of autonomous agents which need not only passively analyze an\nobservation but also must react to it in real-time. Importantly, accurate\nforecasting hinges upon the chosen scene decomposition. We think that superior\nforecasting can be achieved by decomposing a dynamic scene into individual\n'things' and background 'stuff'. Background 'stuff' largely moves because of\ncamera motion, while foreground 'things' move because of both camera and\nindividual object motion. Following this decomposition, we introduce panoptic\nsegmentation forecasting. Panoptic segmentation forecasting opens up a\nmiddle-ground between existing extremes, which either forecast instance\ntrajectories or predict the appearance of future image frames. To address this\ntask we develop a two-component model: one component learns the dynamics of the\nbackground stuff by anticipating odometry, the other one anticipates the\ndynamics of detected things. We establish a leaderboard for this novel task,\nand validate a state-of-the-art model that outperforms available baselines.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 17:59:16 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Graber", "Colin", ""], ["Tsai", "Grace", ""], ["Firman", "Michael", ""], ["Brostow", "Gabriel", ""], ["Schwing", "Alexander", ""]]}, {"id": "2104.03963", "submitter": "Hsin-Ying Lee", "authors": "Chieh Hubert Lin, Hsin-Ying Lee, Yen-Chi Cheng, Sergey Tulyakov,\n  Ming-Hsuan Yang", "title": "InfinityGAN: Towards Infinite-Resolution Image Synthesis", "comments": "Project page: https://hubert0527.github.io/infinityGAN/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present InfinityGAN, a method to generate arbitrary-resolution images. The\nproblem is associated with several key challenges. First, scaling existing\nmodels to a high resolution is resource-constrained, both in terms of\ncomputation and availability of high-resolution training data. Infinity-GAN\ntrains and infers patch-by-patch seamlessly with low computational resources.\nSecond, large images should be locally and globally consistent, avoid\nrepetitive patterns, and look realistic. To address these, InfinityGAN takes\nglobal appearance, local structure and texture into account.With this\nformulation, we can generate images with resolution and level of detail not\nattainable before. Experimental evaluation supports that InfinityGAN generates\nimageswith superior global structure compared to baselines at the same time\nfeaturing parallelizable inference. Finally, we how several applications\nunlocked by our approach, such as fusing styles spatially, multi-modal\noutpainting and image inbetweening at arbitrary input and output resolutions\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 17:59:30 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Lin", "Chieh Hubert", ""], ["Lee", "Hsin-Ying", ""], ["Cheng", "Yen-Chi", ""], ["Tulyakov", "Sergey", ""], ["Yang", "Ming-Hsuan", ""]]}, {"id": "2104.03964", "submitter": "Ankan Kumar Bhunia", "authors": "Ankan Kumar Bhunia, Salman Khan, Hisham Cholakkal, Rao Muhammad Anwer,\n  Fahad Shahbaz Khan, Mubarak Shah", "title": "Handwriting Transformers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a novel transformer-based styled handwritten text image generation\napproach, HWT, that strives to learn both style-content entanglement as well as\nglobal and local writing style patterns. The proposed HWT captures the long and\nshort range relationships within the style examples through a self-attention\nmechanism, thereby encoding both global and local style patterns. Further, the\nproposed transformer-based HWT comprises an encoder-decoder attention that\nenables style-content entanglement by gathering the style representation of\neach query character. To the best of our knowledge, we are the first to\nintroduce a transformer-based generative network for styled handwritten text\ngeneration. Our proposed HWT generates realistic styled handwritten text images\nand significantly outperforms the state-of-the-art demonstrated through\nextensive qualitative, quantitative and human-based evaluations. The proposed\nHWT can handle arbitrary length of text and any desired writing style in a\nfew-shot setting. Further, our HWT generalizes well to the challenging scenario\nwhere both words and writing style are unseen during training, generating\nrealistic styled handwritten text images.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 17:59:43 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Bhunia", "Ankan Kumar", ""], ["Khan", "Salman", ""], ["Cholakkal", "Hisham", ""], ["Anwer", "Rao Muhammad", ""], ["Khan", "Fahad Shahbaz", ""], ["Shah", "Mubarak", ""]]}, {"id": "2104.03965", "submitter": "Matteo Poggi", "authors": "Filippo Aleotti, Matteo Poggi, Stefano Mattoccia", "title": "Learning optical flow from still images", "comments": "CVPR 2021. Project page with supplementary and code:\n  https://mattpoggi.github.io/projects/cvpr2021aleotti/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with the scarcity of data for training optical flow\nnetworks, highlighting the limitations of existing sources such as labeled\nsynthetic datasets or unlabeled real videos. Specifically, we introduce a\nframework to generate accurate ground-truth optical flow annotations quickly\nand in large amounts from any readily available single real picture. Given an\nimage, we use an off-the-shelf monocular depth estimation network to build a\nplausible point cloud for the observed scene. Then, we virtually move the\ncamera in the reconstructed environment with known motion vectors and rotation\nangles, allowing us to synthesize both a novel view and the corresponding\noptical flow field connecting each pixel in the input image to the one in the\nnew frame. When trained with our data, state-of-the-art optical flow networks\nachieve superior generalization to unseen real data compared to the same models\ntrained either on annotated synthetic datasets or unlabeled videos, and better\nspecialization if combined with synthetic images.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 17:59:58 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Aleotti", "Filippo", ""], ["Poggi", "Matteo", ""], ["Mattoccia", "Stefano", ""]]}, {"id": "2104.03978", "submitter": "Andrei Burov", "authors": "Andrei Burov and Matthias Nie{\\ss}ner and Justus Thies", "title": "Dynamic Surface Function Networks for Clothed Human Bodies", "comments": "Video: https://youtu.be/4wbSi9Sqdm4 | Project page:\n  https://github.com/andreiburov/DSFN", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel method for temporal coherent reconstruction and tracking\nof clothed humans. Given a monocular RGB-D sequence, we learn a person-specific\nbody model which is based on a dynamic surface function network. To this end,\nwe explicitly model the surface of the person using a multi-layer perceptron\n(MLP) which is embedded into the canonical space of the SMPL body model. With\nclassical forward rendering, the represented surface can be rasterized using\nthe topology of a template mesh. For each surface point of the template mesh,\nthe MLP is evaluated to predict the actual surface location. To handle\npose-dependent deformations, the MLP is conditioned on the SMPL pose\nparameters. We show that this surface representation as well as the pose\nparameters can be learned in a self-supervised fashion using the principle of\nanalysis-by-synthesis and differentiable rasterization. As a result, we are\nable to reconstruct a temporally coherent mesh sequence from the input data.\nThe underlying surface representation can be used to synthesize new animations\nof the reconstructed person including pose-dependent deformations.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 18:00:03 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Burov", "Andrei", ""], ["Nie\u00dfner", "Matthias", ""], ["Thies", "Justus", ""]]}, {"id": "2104.04006", "submitter": "Michail Mamalakis Mr", "authors": "Michail Mamalakis, Andrew J. Swift, Bart Vorselaars, Surajit Ray,\n  Simonne Weeks, Weiping Ding, Richard H. Clayton, Louise S. Mackenzie, Abhirup\n  Banerjee", "title": "DenResCov-19: A deep transfer learning network for robust automatic\n  classification of COVID-19, pneumonia, and tuberculosis from X-rays", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The global pandemic of COVID-19 is continuing to have a significant effect on\nthe well-being of global population, increasing the demand for rapid testing,\ndiagnosis, and treatment. Along with COVID-19, other etiologies of pneumonia\nand tuberculosis constitute additional challenges to the medical system. In\nthis regard, the objective of this work is to develop a new deep transfer\nlearning pipeline to diagnose patients with COVID-19, pneumonia, and\ntuberculosis, based on chest x-ray images. We observed in some instances\nDenseNet and Resnet have orthogonal performances. In our proposed model, we\nhave created an extra layer with convolutional neural network blocks to combine\nthese two models to establish superior performance over either model. The same\nstrategy can be useful in other applications where two competing networks with\ncomplementary performance are observed. We have tested the performance of our\nproposed network on two-class (pneumonia vs healthy), three-class (including\nCOVID-19), and four-class (including tuberculosis) classification problems. The\nproposed network has been able to successfully classify these lung diseases in\nall four datasets and has provided significant improvement over the benchmark\nnetworks of DenseNet, ResNet, and Inception-V3. These novel findings can\ndeliver a state-of-the-art pre-screening fast-track decision network to detect\nCOVID-19 and other lung pathologies.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 18:49:22 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Mamalakis", "Michail", ""], ["Swift", "Andrew J.", ""], ["Vorselaars", "Bart", ""], ["Ray", "Surajit", ""], ["Weeks", "Simonne", ""], ["Ding", "Weiping", ""], ["Clayton", "Richard H.", ""], ["Mackenzie", "Louise S.", ""], ["Banerjee", "Abhirup", ""]]}, {"id": "2104.04013", "submitter": "Mohamed Ibrahim", "authors": "Mohamed R. Ibrahim, James Haworth, Nicola Christie", "title": "Re-designing cities with conditional adversarial networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This paper introduces a conditional generative adversarial network to\nredesign a street-level image of urban scenes by generating 1) an urban\nintervention policy, 2) an attention map that localises where intervention is\nneeded, 3) a high-resolution street-level image (1024 X 1024 or 1536 X1536)\nafter implementing the intervention. We also introduce a new dataset that\ncomprises aligned street-level images of before and after urban interventions\nfrom real-life scenarios that make this research possible. The introduced\nmethod has been trained on different ranges of urban interventions applied to\nrealistic images. The trained model shows strong performance in re-modelling\ncities, outperforming existing methods that apply image-to-image translation in\nother domains that is computed in a single GPU. This research opens the door\nfor machine intelligence to play a role in re-thinking and re-designing the\ndifferent attributes of cities based on adversarial learning, going beyond the\nmainstream of facial landmarks manipulation or image synthesis from semantic\nsegmentation.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 19:03:34 GMT"}, {"version": "v2", "created": "Wed, 14 Apr 2021 09:43:26 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Ibrahim", "Mohamed R.", ""], ["Haworth", "James", ""], ["Christie", "Nicola", ""]]}, {"id": "2104.04015", "submitter": "Chun-Liang Li", "authors": "Chun-Liang Li, Kihyuk Sohn, Jinsung Yoon, Tomas Pfister", "title": "CutPaste: Self-Supervised Learning for Anomaly Detection and\n  Localization", "comments": "Published at CVPR 2021. The first two authors contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We aim at constructing a high performance model for defect detection that\ndetects unknown anomalous patterns of an image without anomalous data. To this\nend, we propose a two-stage framework for building anomaly detectors using\nnormal training data only. We first learn self-supervised deep representations\nand then build a generative one-class classifier on learned representations. We\nlearn representations by classifying normal data from the CutPaste, a simple\ndata augmentation strategy that cuts an image patch and pastes at a random\nlocation of a large image. Our empirical study on MVTec anomaly detection\ndataset demonstrates the proposed algorithm is general to be able to detect\nvarious types of real-world defects. We bring the improvement upon previous\narts by 3.1 AUCs when learning representations from scratch. By transfer\nlearning on pretrained representations on ImageNet, we achieve a new\nstate-of-theart 96.6 AUC. Lastly, we extend the framework to learn and extract\nrepresentations from patches to allow localizing defective areas without\nannotations during training.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 19:04:55 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Li", "Chun-Liang", ""], ["Sohn", "Kihyuk", ""], ["Yoon", "Jinsung", ""], ["Pfister", "Tomas", ""]]}, {"id": "2104.04029", "submitter": "Vida Adeli", "authors": "Vida Adeli, Mahsa Ehsanpour, Ian Reid, Juan Carlos Niebles, Silvio\n  Savarese, Ehsan Adeli, Hamid Rezatofighi", "title": "TRiPOD: Human Trajectory and Pose Dynamics Forecasting in the Wild", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Joint forecasting of human trajectory and pose dynamics is a fundamental\nbuilding block of various applications ranging from robotics and autonomous\ndriving to surveillance systems. Predicting body dynamics requires capturing\nsubtle information embedded in the humans' interactions with each other and\nwith the objects present in the scene. In this paper, we propose a novel\nTRajectory and POse Dynamics (nicknamed TRiPOD) method based on graph\nattentional networks to model the human-human and human-object interactions\nboth in the input space and the output space (decoded future output). The model\nis supplemented by a message passing interface over the graphs to fuse these\ndifferent levels of interactions efficiently. Furthermore, to incorporate a\nreal-world challenge, we propound to learn an indicator representing whether an\nestimated body joint is visible/invisible at each frame, e.g. due to occlusion\nor being outside the sensor field of view. Finally, we introduce a new\nbenchmark for this joint task based on two challenging datasets (PoseTrack and\n3DPW) and propose evaluation metrics to measure the effectiveness of\npredictions in the global space, even when there are invisible cases of joints.\nOur evaluation shows that TRiPOD outperforms all prior work and\nstate-of-the-art specifically designed for each of the trajectory and pose\nforecasting tasks.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 20:01:00 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Adeli", "Vida", ""], ["Ehsanpour", "Mahsa", ""], ["Reid", "Ian", ""], ["Niebles", "Juan Carlos", ""], ["Savarese", "Silvio", ""], ["Adeli", "Ehsan", ""], ["Rezatofighi", "Hamid", ""]]}, {"id": "2104.04055", "submitter": "Gaurav Bharaj", "authors": "David Ferman, Gaurav Bharaj", "title": "Generative Landmarks", "comments": "2", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a general purpose approach to detect landmarks with improved\ntemporal consistency, and personalization. Most sparse landmark detection\nmethods rely on laborious, manually labelled landmarks, where inconsistency in\nannotations over a temporal volume leads to sub-optimal landmark learning.\nFurther, high-quality landmarks with personalization is often hard to achieve.\nWe pose landmark detection as an image translation problem. We capture two sets\nof unpaired marked (with paint) and unmarked videos. We then use a generative\nadversarial network and cyclic consistency to predict deformations of landmark\ntemplates that simulate markers on unmarked images until these images are\nindistinguishable from ground-truth marked images. Our novel method does not\nrely on manually labelled priors, is temporally consistent, and image class\nagnostic -- face, and hand landmarks detection examples are shown.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 20:59:21 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Ferman", "David", ""], ["Bharaj", "Gaurav", ""]]}, {"id": "2104.04073", "submitter": "Shuai Chen", "authors": "Shuai Chen, Zirui Wang, Victor Prisacariu", "title": "Direct-PoseNet: Absolute Pose Regression with Photometric Consistency", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a relocalization pipeline, which combines an absolute pose\nregression (APR) network with a novel view synthesis based direct matching\nmodule, offering superior accuracy while maintaining low inference time. Our\ncontribution is twofold: i) we design a direct matching module that supplies a\nphotometric supervision signal to refine the pose regression network via\ndifferentiable rendering; ii) we modify the rotation representation from the\nclassical quaternion to SO(3) in pose regression, removing the need for\nbalancing rotation and translation loss terms. As a result, our network\nDirect-PoseNet achieves state-of-the-art performance among all other\nsingle-image APR methods on the 7-Scenes benchmark and the LLFF dataset.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 21:10:18 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Chen", "Shuai", ""], ["Wang", "Zirui", ""], ["Prisacariu", "Victor", ""]]}, {"id": "2104.04104", "submitter": "Zhiling Huang", "authors": "Zhiling Huang and Junwen Bu and Jie Chen", "title": "Image-based Virtual Fitting Room", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Virtual fitting room is a challenging task yet useful feature for e-commerce\nplatforms and fashion designers. Existing works can only detect very few types\nof fashion items. Besides they did poorly in changing the texture and style of\nthe selected fashion items. In this project, we propose a novel approach to\naddress this problem. We firstly used Mask R-CNN to find the regions of\ndifferent fashion items, and secondly used Neural Style Transfer to change the\nstyle of the selected fashion items. The dataset we used is composed of images\nfrom PaperDoll dataset and annotations provided by eBay's ModaNet. We trained 8\nmodels and our best model massively outperformed baseline models both\nquantitatively and qualitatively, with 68.72% mAP, 0.2% ASDR.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 22:53:08 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Huang", "Zhiling", ""], ["Bu", "Junwen", ""], ["Chen", "Jie", ""]]}, {"id": "2104.04107", "submitter": "Liang Tong", "authors": "Liang Tong, Zhengzhang Chen, Jingchao Ni, Wei Cheng, Dongjin Song,\n  Haifeng Chen, Yevgeniy Vorobeychik", "title": "FACESEC: A Fine-grained Robustness Evaluation Framework for Face\n  Recognition Systems", "comments": "Accepted by CVPR'21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present FACESEC, a framework for fine-grained robustness evaluation of\nface recognition systems. FACESEC evaluation is performed along four dimensions\nof adversarial modeling: the nature of perturbation (e.g., pixel-level or face\naccessories), the attacker's system knowledge (about training data and learning\narchitecture), goals (dodging or impersonation), and capability (tailored to\nindividual inputs or across sets of these). We use FACESEC to study five face\nrecognition systems in both closed-set and open-set settings, and to evaluate\nthe state-of-the-art approach for defending against physically realizable\nattacks on these. We find that accurate knowledge of neural architecture is\nsignificantly more important than knowledge of the training data in black-box\nattacks. Moreover, we observe that open-set face recognition systems are more\nvulnerable than closed-set systems under different types of attacks. The\nefficacy of attacks for other threat model variations, however, appears highly\ndependent on both the nature of perturbation and the neural network\narchitecture. For example, attacks that involve adversarial face masks are\nusually more potent, even against adversarially trained models, and the ArcFace\narchitecture tends to be more robust than the others.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 23:00:25 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Tong", "Liang", ""], ["Chen", "Zhengzhang", ""], ["Ni", "Jingchao", ""], ["Cheng", "Wei", ""], ["Song", "Dongjin", ""], ["Chen", "Haifeng", ""], ["Vorobeychik", "Yevgeniy", ""]]}, {"id": "2104.04112", "submitter": "Joel Ye", "authors": "Joel Ye, Dhruv Batra, Abhishek Das, and Erik Wijmans", "title": "Auxiliary Tasks and Exploration Enable ObjectNav", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  ObjectGoal Navigation (ObjectNav) is an embodied task wherein agents are to\nnavigate to an object instance in an unseen environment. Prior works have shown\nthat end-to-end ObjectNav agents that use vanilla visual and recurrent modules,\ne.g. a CNN+RNN, perform poorly due to overfitting and sample inefficiency. This\nhas motivated current state-of-the-art methods to mix analytic and learned\ncomponents and operate on explicit spatial maps of the environment. We instead\nre-enable a generic learned agent by adding auxiliary learning tasks and an\nexploration reward. Our agents achieve 24.5% success and 8.1% SPL, a 37% and 8%\nrelative improvement over prior state-of-the-art, respectively, on the Habitat\nObjectNav Challenge. From our analysis, we propose that agents will act to\nsimplify their visual inputs so as to smooth their RNN dynamics, and that\nauxiliary tasks reduce overfitting by minimizing effective RNN dimensionality;\ni.e. a performant ObjectNav agent that must maintain coherent plans over long\nhorizons does so by learning smooth, low-dimensional recurrent dynamics. Site:\nhttps://joel99.github.io/objectnav/\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 23:03:21 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Ye", "Joel", ""], ["Batra", "Dhruv", ""], ["Das", "Abhishek", ""], ["Wijmans", "Erik", ""]]}, {"id": "2104.04120", "submitter": "YeongHyeon Park", "authors": "YeongHyeon Park, JoonSung Lee, Wonseok Park", "title": "Self-Weighted Ensemble Method to Adjust the Influence of Individual\n  Models based on Reliability", "comments": "3 pages, 2 figures, 4 tables A preliminary version of the paper was\n  presented at the 33rd Workshop on Image Processing and Image Understanding", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image classification technology and performance based on Deep Learning have\nalready achieved high standards. Nevertheless, many efforts have conducted to\nimprove the stability of classification via ensembling. However, the existing\nensemble method has a limitation in that it requires extra effort including\ntime consumption to find the weight for each model output. In this paper, we\npropose a simple but improved ensemble method, naming with Self-Weighted\nEnsemble (SWE), that places the weight of each model via its verification\nreliability. The proposed ensemble method, SWE, reduces overall efforts for\nconstructing a classification system with varied classifiers. The performance\nusing SWE is 0.033% higher than the conventional ensemble method. Also, the\npercent of performance superiority to the previous model is up to 73.333%\n(ratio of 8:22).\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 00:20:01 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Park", "YeongHyeon", ""], ["Lee", "JoonSung", ""], ["Park", "Wonseok", ""]]}, {"id": "2104.04160", "submitter": "Yongjie Zhu", "authors": "Yongjie Zhu, Yinda Zhang, Si Li, Boxin Shi", "title": "Spatially-Varying Outdoor Lighting Estimation from Intrinsics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present SOLID-Net, a neural network for spatially-varying outdoor lighting\nestimation from a single outdoor image for any 2D pixel location. Previous work\nhas used a unified sky environment map to represent outdoor lighting. Instead,\nwe generate spatially-varying local lighting environment maps by combining\nglobal sky environment map with warped image information according to geometric\ninformation estimated from intrinsics. As no outdoor dataset with image and\nlocal lighting ground truth is readily available, we introduce the SOLID-Img\ndataset with physically-based rendered images and their corresponding intrinsic\nand lighting information. We train a deep neural network to regress intrinsic\ncues with physically-based constraints and use them to conduct global and local\nlightings estimation. Experiments on both synthetic and real datasets show that\nSOLID-Net significantly outperforms previous methods.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 02:28:54 GMT"}, {"version": "v2", "created": "Wed, 28 Apr 2021 05:22:29 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Zhu", "Yongjie", ""], ["Zhang", "Yinda", ""], ["Li", "Si", ""], ["Shi", "Boxin", ""]]}, {"id": "2104.04162", "submitter": "Ademola Okerinde", "authors": "Ademola Okerinde and Lior Shamir and William Hsu and Tom Theis and\n  Nasik Nafi", "title": "eGAN: Unsupervised approach to class imbalance using transfer learning", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Class imbalance is an inherent problem in many machine learning\nclassification tasks. This often leads to trained models that are unusable for\nany practical purpose. In this study we explore an unsupervised approach to\naddress these imbalances by leveraging transfer learning from pre-trained image\nclassification models to encoder-based Generative Adversarial Network (eGAN).\nTo the best of our knowledge, this is the first work to tackle this problem\nusing GAN without needing to augment with synthesized fake images.\n  In the proposed approach we use the discriminator network to output a\nnegative or positive score. We classify as minority, test samples with negative\nscores and as majority those with positive scores. Our approach eliminates\nepistemic uncertainty in model predictions, as the P(minority) + P(majority)\nneed not sum up to 1. The impact of transfer learning and combinations of\ndifferent pre-trained image classification models at the generator and\ndiscriminator is also explored. Best result of 0.69 F1-score was obtained on\nCIFAR-10 classification task with imbalance ratio of 1:2500.\n  Our approach also provides a mechanism of thresholding the specificity or\nsensitivity of our machine learning system. Keywords: Class imbalance, Transfer\nLearning, GAN, nash equilibrium\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 02:37:55 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Okerinde", "Ademola", ""], ["Shamir", "Lior", ""], ["Hsu", "William", ""], ["Theis", "Tom", ""], ["Nafi", "Nasik", ""]]}, {"id": "2104.04163", "submitter": "Hanjun Li", "authors": "Hanjun Li, Gaojie Wu, Wei-Shi Zheng", "title": "Combined Depth Space based Architecture Search For Person\n  Re-identification", "comments": "Accepted by CVPR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most works on person re-identification (ReID) take advantage of large\nbackbone networks such as ResNet, which are designed for image classification\ninstead of ReID, for feature extraction. However, these backbones may not be\ncomputationally efficient or the most suitable architectures for ReID. In this\nwork, we aim to design a lightweight and suitable network for ReID. We propose\na novel search space called Combined Depth Space (CDS), based on which we\nsearch for an efficient network architecture, which we call CDNet, via a\ndifferentiable architecture search algorithm. Through the use of the combined\nbasic building blocks in CDS, CDNet tends to focus on combined pattern\ninformation that is typically found in images of pedestrians. We then propose a\nlow-cost search strategy named the Top-k Sample Search strategy to make full\nuse of the search space and avoid trapping in local optimal result.\nFurthermore, an effective Fine-grained Balance Neck (FBLNeck), which is\nremovable at the inference time, is presented to balance the effects of triplet\nloss and softmax loss during the training process. Extensive experiments show\nthat our CDNet (~1.8M parameters) has comparable performance with\nstate-of-the-art lightweight networks.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 02:40:01 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Li", "Hanjun", ""], ["Wu", "Gaojie", ""], ["Zheng", "Wei-Shi", ""]]}, {"id": "2104.04167", "submitter": "Yuankai Qi", "authors": "Yuankai Qi, Zizheng Pan, Yicong Hong, Ming-Hsuan Yang, Anton van den\n  Hengel, Qi Wu", "title": "Know What and Know Where: An Object-and-Room Informed Sequential BERT\n  for Indoor Vision-Language Navigation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vision-and-Language Navigation (VLN) requires an agent to navigate to a\nremote location on the basis of natural-language instructions and a set of\nphoto-realistic panoramas. Most existing methods take words in instructions and\ndiscrete views of each panorama as the minimal unit of encoding. However, this\nrequires a model to match different textual landmarks in instructions (e.g.,\nTV, table) against the same view feature. In this work, we propose an\nobject-informed sequential BERT to encode visual perceptions and linguistic\ninstructions at the same fine-grained level, namely objects and words, to\nfacilitate the matching between visual and textual entities and hence \"know\nwhat\". Our sequential BERT enables the visual-textual clues to be interpreted\nin light of the temporal context, which is crucial to multi-round VLN tasks.\nAdditionally, we enable the model to identify the relative direction (e.g.,\nleft/right/front/back) of each navigable location and the room type (e.g.,\nbedroom, kitchen) of its current and final navigation goal, namely \"know\nwhere\", as such information is widely mentioned in instructions implying the\ndesired next and final locations. Extensive experiments demonstrate the\neffectiveness compared against several state-of-the-art methods on three indoor\nVLN tasks: REVERIE, NDH, and R2R.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 02:44:39 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Qi", "Yuankai", ""], ["Pan", "Zizheng", ""], ["Hong", "Yicong", ""], ["Yang", "Ming-Hsuan", ""], ["Hengel", "Anton van den", ""], ["Wu", "Qi", ""]]}, {"id": "2104.04170", "submitter": "Weihao Yuan", "authors": "Weihao Yuan, Yazhan Zhang, Bingkun Wu, Siyu Zhu, Ping Tan, Michael Yu\n  Wang, Qifeng Chen", "title": "Stereo Matching by Self-supervision of Multiscopic Vision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Self-supervised learning for depth estimation possesses several advantages\nover supervised learning. The benefits of no need for ground-truth depth,\nonline fine-tuning, and better generalization with unlimited data attract\nresearchers to seek self-supervised solutions. In this work, we propose a new\nself-supervised framework for stereo matching utilizing multiple images\ncaptured at aligned camera positions. A cross photometric loss, an\nuncertainty-aware mutual-supervision loss, and a new smoothness loss are\nintroduced to optimize the network in learning disparity maps end-to-end\nwithout ground-truth depth information. To train this framework, we build a new\nmultiscopic dataset consisting of synthetic images rendered by 3D engines and\nreal images captured by real cameras. After being trained with only the\nsynthetic images, our network can perform well in unseen outdoor scenes. Our\nexperiment shows that our model obtains better disparity maps than previous\nunsupervised methods on the KITTI dataset and is comparable to supervised\nmethods when generalized to unseen data. Our source code and dataset will be\nmade public, and more results are provided in the supplement.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 02:58:59 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Yuan", "Weihao", ""], ["Zhang", "Yazhan", ""], ["Wu", "Bingkun", ""], ["Zhu", "Siyu", ""], ["Tan", "Ping", ""], ["Wang", "Michael Yu", ""], ["Chen", "Qifeng", ""]]}, {"id": "2104.04179", "submitter": "Hisaichi Shibata", "authors": "Hisaichi Shibata, Shouhei Hanaoka, Yukihiro Nomura, Takahiro Nakao,\n  Tomomi Takenaga, Naoto Hayashi, Osamu Abe", "title": "X2CT-FLOW: Reconstruction of multiple volumetric chest computed\n  tomography images with different likelihoods from a uni- or biplanar chest\n  X-ray image using a flow-based generative model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose X2CT-FLOW for the reconstruction of volumetric chest computed\ntomography (CT) images from uni- or biplanar digitally reconstructed\nradiographs (DRRs) or chest X-ray (CXR) images on the basis of a flow-based\ndeep generative (FDG) model. With the adoption of X2CT-FLOW, all the\nreconstructed volumetric chest CT images satisfy the condition that each of\nthose projected onto each plane coincides with each input DRR or CXR image.\nMoreover, X2CT-FLOW can reconstruct multiple volumetric chest CT images with\ndifferent likelihoods. The volumetric chest CT images reconstructed from\nbiplanar DRRs showed good agreement with ground truth images in terms of the\nstructural similarity index (0.931 on average). Moreover, we show that\nX2CT-FLOW can actually reconstruct such multiple volumetric chest CT images\nfrom DRRs. Finally, we demonstrate that X2CT-FLOW can reconstruct multiple\nvolumetric chest CT images from a real uniplanar CXR image.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 03:30:27 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Shibata", "Hisaichi", ""], ["Hanaoka", "Shouhei", ""], ["Nomura", "Yukihiro", ""], ["Nakao", "Takahiro", ""], ["Takenaga", "Tomomi", ""], ["Hayashi", "Naoto", ""], ["Abe", "Osamu", ""]]}, {"id": "2104.04182", "submitter": "Santiago Castro", "authors": "Santiago Castro, Ruoyao Wang, Pingxuan Huang, Ian Stewart, Nan Liu,\n  Jonathan Stroud, Rada Mihalcea", "title": "Fill-in-the-blank as a Challenging Video Understanding Evaluation\n  Framework", "comments": "8 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Work to date on language-informed video understanding has primarily addressed\ntwo tasks: (1) video question answering using multiple-choice questions, where\nmodels perform relatively well because they exploit the fact that candidate\nanswers are readily available; and (2) video captioning, which relies on an\nopen-ended evaluation framework that is often inaccurate because system answers\nmay be perceived as incorrect if they differ in form from the ground truth. In\nthis paper, we propose fill-in-the-blanks as a video understanding evaluation\nframework that addresses these previous evaluation drawbacks, and more closely\nreflects real-life settings where no multiple choices are given. The task tests\na system understanding of a video by requiring the model to predict a masked\nnoun phrase in the caption of the video, given the video and the surrounding\ntext. We introduce a novel dataset consisting of 28,000 videos and\nfill-in-the-blank tests. We show that both a multimodal model and a strong\nlanguage model have a large gap with human performance, thus suggesting that\nthe task is more challenging than current video understanding benchmarks.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 04:00:10 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Castro", "Santiago", ""], ["Wang", "Ruoyao", ""], ["Huang", "Pingxuan", ""], ["Stewart", "Ian", ""], ["Liu", "Nan", ""], ["Stroud", "Jonathan", ""], ["Mihalcea", "Rada", ""]]}, {"id": "2104.04184", "submitter": "Firoj Alam", "authors": "Firoj Alam, Tanvirul Alam, Muhammad Imran, Ferda Ofli", "title": "Robust Training of Social Media Image Classification Models for Rapid\n  Disaster Response", "comments": "Social media images, Image Classification, Natural disasters, Crisis\n  Informatics, Deep learning. Extended version of arXiv:2011.08916. arXiv admin\n  note: substantial text overlap with arXiv:2011.08916", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CY cs.LG cs.SI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Images shared on social media help crisis managers gain situational awareness\nand assess incurred damages, among other response tasks. As the volume and\nvelocity of such content are typically high, real-time image classification has\nbecome an urgent need for a faster disaster response. Recent advances in\ncomputer vision and deep neural networks have enabled the development of models\nfor real-time image classification for a number of tasks, including detecting\ncrisis incidents, filtering irrelevant images, classifying images into specific\nhumanitarian categories, and assessing the severity of the damage. To develop\nrobust real-time models, it is necessary to understand the capability of the\npublicly available pre-trained models for these tasks, which remains to be\nunder-explored in the crisis informatics literature. In this study, we address\nsuch limitations by investigating ten different network architectures for four\ndifferent tasks using the largest publicly available datasets for these tasks.\nWe also explore various data augmentation strategies, semi-supervised\ntechniques, and a multitask learning setup. In our extensive experiments, we\nachieve promising results.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 04:30:04 GMT"}, {"version": "v2", "created": "Mon, 19 Jul 2021 12:56:08 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Alam", "Firoj", ""], ["Alam", "Tanvirul", ""], ["Imran", "Muhammad", ""], ["Ofli", "Ferda", ""]]}, {"id": "2104.04191", "submitter": "Jessica Yung", "authors": "Jessica Yung, Rob Romijnders, Alexander Kolesnikov, Lucas Beyer, Josip\n  Djolonga, Neil Houlsby, Sylvain Gelly, Mario Lucic, Xiaohua Zhai", "title": "SI-Score: An image dataset for fine-grained analysis of robustness to\n  object location, rotation and size", "comments": "4 pages (10 pages including references and appendix), 10 figures.\n  Accepted at the ICLR 2021 RobustML Workshop. arXiv admin note: text overlap\n  with arXiv:2007.08558", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Before deploying machine learning models it is critical to assess their\nrobustness. In the context of deep neural networks for image understanding,\nchanging the object location, rotation and size may affect the predictions in\nnon-trivial ways. In this work we perform a fine-grained analysis of robustness\nwith respect to these factors of variation using SI-Score, a synthetic dataset.\nIn particular, we investigate ResNets, Vision Transformers and CLIP, and\nidentify interesting qualitative differences between these.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 05:00:49 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Yung", "Jessica", ""], ["Romijnders", "Rob", ""], ["Kolesnikov", "Alexander", ""], ["Beyer", "Lucas", ""], ["Djolonga", "Josip", ""], ["Houlsby", "Neil", ""], ["Gelly", "Sylvain", ""], ["Lucic", "Mario", ""], ["Zhai", "Xiaohua", ""]]}, {"id": "2104.04192", "submitter": "Jie Hong", "authors": "Jie Hong, Pengfei Fang, Weihao Li, Tong Zhang, Christian Simon,\n  Mehrtash Harandi and Lars Petersson", "title": "Reinforced Attention for Few-Shot Learning and Beyond", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Few-shot learning aims to correctly recognize query samples from unseen\nclasses given a limited number of support samples, often by relying on global\nembeddings of images. In this paper, we propose to equip the backbone network\nwith an attention agent, which is trained by reinforcement learning. The policy\ngradient algorithm is employed to train the agent towards adaptively localizing\nthe representative regions on feature maps over time. We further design a\nreward function based on the prediction of the held-out data, thus helping the\nattention mechanism to generalize better across the unseen classes. The\nextensive experiments show, with the help of the reinforced attention, that our\nembedding network has the capability to progressively generate a more\ndiscriminative representation in few-shot learning. Moreover, experiments on\nthe task of image classification also show the effectiveness of the proposed\ndesign.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 05:01:15 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Hong", "Jie", ""], ["Fang", "Pengfei", ""], ["Li", "Weihao", ""], ["Zhang", "Tong", ""], ["Simon", "Christian", ""], ["Harandi", "Mehrtash", ""], ["Petersson", "Lars", ""]]}, {"id": "2104.04203", "submitter": "Kanji Tanaka", "authors": "Kyosuke Tashiro, Koji Takeda, Kanji Tanaka, Tomoe Hiroki", "title": "TaylorMade VDD: Domain-adaptive Visual Defect Detector for High-mix\n  Low-volume Production of Non-convex Cylindrical Metal Objects", "comments": "6 pages, 6 figures, technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Visual defect detection (VDD) for high-mix low-volume production of\nnon-convex metal objects, such as high-pressure cylindrical piping joint parts\n(VDD-HPPPs), is challenging because subtle difference in domain (e.g., metal\nobjects, imaging device, viewpoints, lighting) significantly affects the\nspecular reflection characteristics of individual metal object types. In this\npaper, we address this issue by introducing a tailor-made VDD framework that\ncan be automatically adapted to a new domain. Specifically, we formulate this\nadaptation task as the problem of network architecture search (NAS) on a deep\nobject-detection network, in which the network architecture is searched via\nreinforcement learning. We demonstrate the effectiveness of the proposed\nframework using the VDD-HPPPs task as a factory case study. Experimental\nresults show that the proposed method achieved higher burr detection accuracy\ncompared with the baseline method for data with different training/test domains\nfor the non-convex HPPPs, which are particularly affected by domain shifts.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 05:56:27 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Tashiro", "Kyosuke", ""], ["Takeda", "Koji", ""], ["Tanaka", "Kanji", ""], ["Hiroki", "Tomoe", ""]]}, {"id": "2104.04241", "submitter": "MaungMaung AprilPyone", "authors": "MaungMaung AprilPyone and Hitoshi Kiya", "title": "Piracy-Resistant DNN Watermarking by Block-Wise Image Transformation\n  with Secret Key", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel DNN watermarking method that utilizes a\nlearnable image transformation method with a secret key. The proposed method\nembeds a watermark pattern in a model by using learnable transformed images and\nallows us to remotely verify the ownership of the model. As a result, it is\npiracy-resistant, so the original watermark cannot be overwritten by a pirated\nwatermark, and adding a new watermark decreases the model accuracy unlike most\nof the existing DNN watermarking methods. In addition, it does not require a\nspecial pre-defined training set or trigger set. We empirically evaluated the\nproposed method on the CIFAR-10 dataset. The results show that it was resilient\nagainst fine-tuning and pruning attacks while maintaining a high\nwatermark-detection accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 08:21:53 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["AprilPyone", "MaungMaung", ""], ["Kiya", "Hitoshi", ""]]}, {"id": "2104.04255", "submitter": "Hichem Sahbi", "authors": "Hichem Sahbi", "title": "Skeleton-based Hand-Gesture Recognition with Lightweight Graph\n  Convolutional Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph convolutional networks (GCNs) aim at extending deep learning to\narbitrary irregular domains, namely graphs. Their success is highly dependent\non how the topology of input graphs is defined and most of the existing GCN\narchitectures rely on predefined or handcrafted graph structures. In this\npaper, we introduce a novel method that learns the topology (or connectivity)\nof input graphs as a part of GCN design. The main contribution of our method\nresides in building an orthogonal connectivity basis that optimally aggregates\nnodes, through their neighborhood, prior to achieve convolution. Our method\nalso considers a stochasticity criterion which acts as a regularizer that makes\nthe learned basis and the underlying GCNs lightweight while still being highly\neffective. Experiments conducted on the challenging task of skeleton-based\nhand-gesture recognition show the high effectiveness of the learned GCNs w.r.t.\nthe related work.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 09:06:53 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Sahbi", "Hichem", ""]]}, {"id": "2104.04268", "submitter": "Xiquan Guan", "authors": "Xiquan Guan, Huamin Feng, Weiming Zhang, Hang Zhou, Jie Zhang, and\n  Nenghai Yu", "title": "Reversible Watermarking in Deep Convolutional Neural Networks for\n  Integrity Authentication", "comments": "Accepted to ACM MM 2020", "journal-ref": null, "doi": "10.1145/3394171.3413729", "report-no": null, "categories": "cs.CR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks have made outstanding contributions in\nmany fields such as computer vision in the past few years and many researchers\npublished well-trained network for downloading. But recent studies have shown\nserious concerns about integrity due to model-reuse attacks and backdoor\nattacks. In order to protect these open-source networks, many algorithms have\nbeen proposed such as watermarking. However, these existing algorithms modify\nthe contents of the network permanently and are not suitable for integrity\nauthentication. In this paper, we propose a reversible watermarking algorithm\nfor integrity authentication. Specifically, we present the reversible\nwatermarking problem of deep convolutional neural networks and utilize the\npruning theory of model compression technology to construct a host sequence\nused for embedding watermarking information by histogram shift. As shown in the\nexperiments, the influence of embedding reversible watermarking on the\nclassification performance is less than 0.5% and the parameters of the model\ncan be fully recovered after extracting the watermarking. At the same time, the\nintegrity of the model can be verified by applying the reversible watermarking:\nif the model is modified illegally, the authentication information generated by\noriginal model will be absolutely different from the extracted watermarking\ninformation.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 09:32:21 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Guan", "Xiquan", ""], ["Feng", "Huamin", ""], ["Zhang", "Weiming", ""], ["Zhou", "Hang", ""], ["Zhang", "Jie", ""], ["Yu", "Nenghai", ""]]}, {"id": "2104.04275", "submitter": "Cheol-Hui Min", "authors": "Cheol-Hui Min, Jinseok Bae, Junho Lee and Young Min Kim", "title": "GATSBI: Generative Agent-centric Spatio-temporal Object Interaction", "comments": "accepted to CVPR'2021 as an oral presentation. Code and video will be\n  released soon", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present GATSBI, a generative model that can transform a sequence of raw\nobservations into a structured latent representation that fully captures the\nspatio-temporal context of the agent's actions. In vision-based decision-making\nscenarios, an agent faces complex high-dimensional observations where multiple\nentities interact with each other. The agent requires a good scene\nrepresentation of the visual observation that discerns essential components and\nconsistently propagates along the time horizon. Our method, GATSBI, utilizes\nunsupervised object-centric scene representation learning to separate an active\nagent, static background, and passive objects. GATSBI then models the\ninteractions reflecting the causal relationships among decomposed entities and\npredicts physically plausible future states. Our model generalizes to a variety\nof environments where different types of robots and objects dynamically\ninteract with each other. We show GATSBI achieves superior performance on scene\ndecomposition and video prediction compared to its state-of-the-art\ncounterparts.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 09:45:00 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Min", "Cheol-Hui", ""], ["Bae", "Jinseok", ""], ["Lee", "Junho", ""], ["Kim", "Young Min", ""]]}, {"id": "2104.04282", "submitter": "Naiyan Wang", "authors": "Aoming Liu, Zehao Huang, Zhiwu Huang, Naiyan Wang", "title": "Direct Differentiable Augmentation Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data augmentation has been an indispensable tool to improve the performance\nof deep neural networks, however the augmentation can hardly transfer among\ndifferent tasks and datasets. Consequently, a recent trend is to adopt AutoML\ntechnique to learn proper augmentation policy without extensive hand-crafted\ntuning. In this paper, we propose an efficient differentiable search algorithm\ncalled Direct Differentiable Augmentation Search (DDAS). It exploits\nmeta-learning with one-step gradient update and continuous relaxation to the\nexpected training loss for efficient search. Our DDAS can achieve efficient\naugmentation search without relying on approximations such as Gumbel Softmax or\nsecond order gradient approximation. To further reduce the adverse effect of\nimproper augmentations, we organize the search space into a two level\nhierarchy, in which we first decide whether to apply augmentation, and then\ndetermine the specific augmentation policy. On standard image classification\nbenchmarks, our DDAS achieves state-of-the-art performance and efficiency\ntradeoff while reducing the search cost dramatically, e.g. 0.15 GPU hours for\nCIFAR-10. In addition, we also use DDAS to search augmentation for object\ndetection task and achieve comparable performance with AutoAugment, while being\n1000x faster.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 10:02:24 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Liu", "Aoming", ""], ["Huang", "Zehao", ""], ["Huang", "Zhiwu", ""], ["Wang", "Naiyan", ""]]}, {"id": "2104.04289", "submitter": "Atsushi Hanamoto", "authors": "Ryuji Imamura, Kohei Azuma, Atsushi Hanamoto, and Atsunori Kanemura", "title": "MLF-SC: Incorporating multi-layer features to sparse coding for anomaly\n  detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anomalies in images occur in various scales from a small hole on a carpet to\na large stain. However, anomaly detection based on sparse coding, one of the\nwidely used anomaly detection methods, has an issue in dealing with anomalies\nthat are out of the patch size employed to sparsely represent images. A large\nanomaly can be considered normal if seen in a small scale, but it is not easy\nto determine a single scale (patch size) that works well for all images. Then,\nwe propose to incorporate multi-scale features to sparse coding and improve the\nperformance of anomaly detection. The proposed method, multi-layer feature\nsparse coding (MLF-SC), employs a neural network for feature extraction, and\nfeature maps from intermediate layers of the network are given to sparse\ncoding, whereas the standard sparse-coding-based anomaly detection method\ndirectly works on given images. We show that MLF-SC outperforms\nstate-of-the-art anomaly detection methods including those employing deep\nlearning. Our target data are the texture categories of the MVTec Anomaly\nDetection (MVTec AD) dataset, which is a modern benchmark dataset consisting of\nimages from the real world. Our idea can be a simple and practical option to\ndeal with practical data.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 10:20:34 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Imamura", "Ryuji", ""], ["Azuma", "Kohei", ""], ["Hanamoto", "Atsushi", ""], ["Kanemura", "Atsunori", ""]]}, {"id": "2104.04291", "submitter": "Heng Fang", "authors": "Heng Fang, Xi Yang, Taichi Kin, Takeo Igarashi", "title": "Brain Surface Reconstruction from MRI Images Based on Segmentation\n  Networks Applying Signed Distance Maps", "comments": "Accepted by IEEE ISBI 2021 (International Symposium on Biomedical\n  Imaging)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Whole-brain surface extraction is an essential topic in medical imaging\nsystems as it provides neurosurgeons with a broader view of surgical planning\nand abnormality detection. To solve the problem confronted in current deep\nlearning skull stripping methods lacking prior shape information, we propose a\nnew network architecture that incorporates knowledge of signed distance fields\nand introduce an additional Laplacian loss to ensure that the prediction\nresults retain shape information. We validated our newly proposed method by\nconducting experiments on our brain magnetic resonance imaging dataset (111\npatients). The evaluation results demonstrate that our approach achieves\ncomparable dice scores and also reduces the Hausdorff distance and average\nsymmetric surface distance, thus producing more stable and smooth brain\nisosurfaces.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 10:24:27 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Fang", "Heng", ""], ["Yang", "Xi", ""], ["Kin", "Taichi", ""], ["Igarashi", "Takeo", ""]]}, {"id": "2104.04310", "submitter": "Michael Tarasiou", "authors": "Michail Tarasiou, Riza Alp Guler, Stefanos Zafeiriou", "title": "Context-self contrastive pretraining for crop type semantic segmentation", "comments": "11 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper we propose a fully-supervised pretraining scheme based on\ncontrastive learning particularly tailored to dense classification tasks. The\nproposed Context-Self Contrastive Loss (CSCL) learns an embedding space that\nmakes semantic boundaries pop-up by use of a similarity metric between every\nlocation in an training sample and its local context. For crop type semantic\nsegmentation from satellite images we find performance at parcel boundaries to\nbe a critical bottleneck and explain how CSCL tackles the underlying cause of\nthat problem, improving the state-of-the-art performance in this task.\nAdditionally, using images from the Sentinel-2 (S2) satellite missions we\ncompile the largest, to our knowledge, dataset of satellite image timeseries\ndensely annotated by crop type and parcel identities, which we make publicly\navailable together with the data generation pipeline. Using that data we find\nCSCL, even with minimal pretraining, to improve all respective baselines and\npresent a process for semantic segmentation at super-resolution for obtaining\ncrop classes at a more granular level. The proposed method is further validated\non the task of semantic segmentation on 2D and 3D volumetric images showing\nconsistent performance improvements upon competitive baselines.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 11:29:44 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Tarasiou", "Michail", ""], ["Guler", "Riza Alp", ""], ["Zafeiriou", "Stefanos", ""]]}, {"id": "2104.04314", "submitter": "Zhelun Shen", "authors": "Zhelun Shen, Yuchao Dai, Zhibo Rao", "title": "CFNet: Cascade and Fused Cost Volume for Robust Stereo Matching", "comments": "accepted by CVPR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the ever-increasing capacity of large-scale annotated datasets has\nled to profound progress in stereo matching. However, most of these successes\nare limited to a specific dataset and cannot generalize well to other datasets.\nThe main difficulties lie in the large domain differences and unbalanced\ndisparity distribution across a variety of datasets, which greatly limit the\nreal-world applicability of current deep stereo matching models. In this paper,\nwe propose CFNet, a Cascade and Fused cost volume based network to improve the\nrobustness of the stereo matching network. First, we propose a fused cost\nvolume representation to deal with the large domain difference. By fusing\nmultiple low-resolution dense cost volumes to enlarge the receptive field, we\ncan extract robust structural representations for initial disparity estimation.\nSecond, we propose a cascade cost volume representation to alleviate the\nunbalanced disparity distribution. Specifically, we employ a variance-based\nuncertainty estimation to adaptively adjust the next stage disparity search\nspace, in this way driving the network progressively prune out the space of\nunlikely correspondences. By iteratively narrowing down the disparity search\nspace and improving the cost volume resolution, the disparity estimation is\ngradually refined in a coarse-to-fine manner. When trained on the same training\nimages and evaluated on KITTI, ETH3D, and Middlebury datasets with the fixed\nmodel parameters and hyperparameters, our proposed method achieves the\nstate-of-the-art overall performance and obtains the 1st place on the stereo\ntask of Robust Vision Challenge 2020. The code will be available at\nhttps://github.com/gallenszl/CFNet.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 11:38:59 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Shen", "Zhelun", ""], ["Dai", "Yuchao", ""], ["Rao", "Zhibo", ""]]}, {"id": "2104.04323", "submitter": "Johannes H\\\"ohne", "authors": "Jonas Dippel, Steffen Vogler, Johannes H\\\"ohne", "title": "Towards Fine-grained Visual Representations by Combining Contrastive\n  Learning with Image Reconstruction and Attention-weighted Pooling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents Contrastive Reconstruction, ConRec - a self-supervised\nlearning algorithm that obtains image representations by jointly optimizing a\ncontrastive and a self-reconstruction loss. We showcase that state-of-the-art\ncontrastive learning methods (e.g. SimCLR) have shortcomings to capture\nfine-grained visual features in their representations. ConRec extends the\nSimCLR framework by adding (1) a self-reconstruction task and (2) an attention\nmechanism within the contrastive learning task. This is accomplished by\napplying a simple encoder-decoder architecture with two heads. We show that\nboth extensions contribute towards an improved vector representation for images\nwith fine-grained visual features. Combining those concepts, ConRec outperforms\nSimCLR and SimCLR with Attention-Pooling on fine-grained classification\ndatasets.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 12:12:10 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Dippel", "Jonas", ""], ["Vogler", "Steffen", ""], ["H\u00f6hne", "Johannes", ""]]}, {"id": "2104.04329", "submitter": "Li Hu", "authors": "Li Hu, Peng Zhang, Bang Zhang, Pan Pan, Yinghui Xu, Rong Jin", "title": "Learning Position and Target Consistency for Memory-based Video Object\n  Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the problem of semi-supervised video object\nsegmentation(VOS). Multiple works have shown that memory-based approaches can\nbe effective for video object segmentation. They are mostly based on\npixel-level matching, both spatially and temporally. The main shortcoming of\nmemory-based approaches is that they do not take into account the sequential\norder among frames and do not exploit object-level knowledge from the target.\nTo address this limitation, we propose to Learn position and target Consistency\nframework for Memory-based video object segmentation, termed as LCM. It applies\nthe memory mechanism to retrieve pixels globally, and meanwhile learns position\nconsistency for more reliable segmentation. The learned location response\npromotes a better discrimination between target and distractors. Besides, LCM\nintroduces an object-level relationship from the target to maintain target\nconsistency, making LCM more robust to error drifting. Experiments show that\nour LCM achieves state-of-the-art performance on both DAVIS and Youtube-VOS\nbenchmark. And we rank the 1st in the DAVIS 2020 challenge semi-supervised VOS\ntask.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 12:22:37 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Hu", "Li", ""], ["Zhang", "Peng", ""], ["Zhang", "Bang", ""], ["Pan", "Pan", ""], ["Xu", "Yinghui", ""], ["Jin", "Rong", ""]]}, {"id": "2104.04359", "submitter": "David Noever", "authors": "David Noever, Samantha E. Miller Noever", "title": "Rock Hunting With Martian Machine Vision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The Mars Perseverance rover applies computer vision for navigation and hazard\navoidance. The challenge to do onboard object recognition highlights the need\nfor low-power, customized training, often including low-contrast backgrounds.\nWe investigate deep learning methods for the classification and detection of\nMartian rocks. We report greater than 97% accuracy for binary classifications\n(rock vs. rover). We fine-tune a detector to render geo-located bounding boxes\nwhile counting rocks. For these models to run on microcontrollers, we shrink\nand quantize the neural networks' weights and demonstrate a low-power rock\nhunter with faster frame rates (1 frame per second) but lower accuracy (37%).\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 13:44:27 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Noever", "David", ""], ["Noever", "Samantha E. Miller", ""]]}, {"id": "2104.04362", "submitter": "Xing Di", "authors": "Xing Di, Vishal M. Patel", "title": "Multimodal Face Synthesis from Visual Attributes", "comments": "IEEE Transactions on Biometrics, Behavior, and Identity Science\n  (T-BIOM) submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Synthesis of face images from visual attributes is an important problem in\ncomputer vision and biometrics due to its applications in law enforcement and\nentertainment. Recent advances in deep generative networks have made it\npossible to synthesize high-quality face images from visual attributes.\nHowever, existing methods are specifically designed for generating unimodal\nimages (i.e visible faces) from attributes. In this paper, we propose a novel\ngenerative adversarial network that simultaneously synthesizes identity\npreserving multimodal face images (i.e. visible, sketch, thermal, etc.) from\nvisual attributes without requiring paired data in different domains for\ntraining the network. We introduce a novel generator with multimodal\nstretch-out modules to simultaneously synthesize multimodal face images.\nAdditionally, multimodal stretch-in modules are introduced in the discriminator\nwhich discriminates between real and fake images. Extensive experiments and\ncomparisons with several state-of-the-art methods are performed to verify the\neffectiveness of the proposed attribute-based multimodal synthesis method.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 13:47:23 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Di", "Xing", ""], ["Patel", "Vishal M.", ""]]}, {"id": "2104.04369", "submitter": "Songyang Zhang", "authors": "Songyang Zhang, Linfeng Song, Lifeng Jin, Kun Xu, Dong Yu, Jiebo Luo", "title": "Video-aided Unsupervised Grammar Induction", "comments": "This paper is accepted by NAACL'21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate video-aided grammar induction, which learns a constituency\nparser from both unlabeled text and its corresponding video. Existing methods\nof multi-modal grammar induction focus on learning syntactic grammars from\ntext-image pairs, with promising results showing that the information from\nstatic images is useful in induction. However, videos provide even richer\ninformation, including not only static objects but also actions and state\nchanges useful for inducing verb phrases. In this paper, we explore rich\nfeatures (e.g. action, object, scene, audio, face, OCR and speech) from videos,\ntaking the recent Compound PCFG model as the baseline. We further propose a\nMulti-Modal Compound PCFG model (MMC-PCFG) to effectively aggregate these rich\nfeatures from different modalities. Our proposed MMC-PCFG is trained end-to-end\nand outperforms each individual modality and previous state-of-the-art systems\non three benchmarks, i.e. DiDeMo, YouCook2 and MSRVTT, confirming the\neffectiveness of leveraging video information for unsupervised grammar\ninduction.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 14:01:36 GMT"}, {"version": "v2", "created": "Tue, 4 May 2021 00:23:28 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Zhang", "Songyang", ""], ["Song", "Linfeng", ""], ["Jin", "Lifeng", ""], ["Xu", "Kun", ""], ["Yu", "Dong", ""], ["Luo", "Jiebo", ""]]}, {"id": "2104.04382", "submitter": "Le Yang", "authors": "Le Yang, Haojun Jiang, Ruojin Cai, Yulin Wang, Shiji Song, Gao Huang,\n  Qi Tian", "title": "CondenseNet V2: Sparse Feature Reactivation for Deep Networks", "comments": "CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reusing features in deep networks through dense connectivity is an effective\nway to achieve high computational efficiency. The recent proposed CondenseNet\nhas shown that this mechanism can be further improved if redundant features are\nremoved. In this paper, we propose an alternative approach named sparse feature\nreactivation (SFR), aiming at actively increasing the utility of features for\nreusing. In the proposed network, named CondenseNetV2, each layer can\nsimultaneously learn to 1) selectively reuse a set of most important features\nfrom preceding layers; and 2) actively update a set of preceding features to\nincrease their utility for later layers. Our experiments show that the proposed\nmodels achieve promising performance on image classification (ImageNet and\nCIFAR) and object detection (MS COCO) in terms of both theoretical efficiency\nand practical speed.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 14:12:43 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Yang", "Le", ""], ["Jiang", "Haojun", ""], ["Cai", "Ruojin", ""], ["Wang", "Yulin", ""], ["Song", "Shiji", ""], ["Huang", "Gao", ""], ["Tian", "Qi", ""]]}, {"id": "2104.04386", "submitter": "Binbin Huang", "authors": "Binbin Huang, Dongze Lian, Weixin Luo, Shenghua Gao", "title": "Look Before You Leap: Learning Landmark Features for One-Stage Visual\n  Grounding", "comments": "Code is available at https://github.com/svip-lab/LBYLNet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  An LBYL (`Look Before You Leap') Network is proposed for end-to-end trainable\none-stage visual grounding. The idea behind LBYL-Net is intuitive and\nstraightforward: we follow a language's description to localize the target\nobject based on its relative spatial relation to `Landmarks', which is\ncharacterized by some spatial positional words and some descriptive words about\nthe object. The core of our LBYL-Net is a landmark feature convolution module\nthat transmits the visual features with the guidance of linguistic description\nalong with different directions. Consequently, such a module encodes the\nrelative spatial positional relations between the current object and its\ncontext. Then we combine the contextual information from the landmark feature\nconvolution module with the target's visual features for grounding. To make\nthis landmark feature convolution light-weight, we introduce a dynamic\nprogramming algorithm (termed dynamic max pooling) with low complexity to\nextract the landmark feature. Thanks to the landmark feature convolution\nmodule, we mimic the human behavior of `Look Before You Leap' to design an\nLBYL-Net, which takes full consideration of contextual information. Extensive\nexperiments show our method's effectiveness in four grounding datasets.\nSpecifically, our LBYL-Net outperforms all state-of-the-art two-stage and\none-stage methods on ReferitGame. On RefCOCO and RefCOCO+, Our LBYL-Net also\nachieves comparable results or even better results than existing one-stage\nmethods.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 14:20:36 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Huang", "Binbin", ""], ["Lian", "Dongze", ""], ["Luo", "Weixin", ""], ["Gao", "Shenghua", ""]]}, {"id": "2104.04391", "submitter": "Mohsen Zand", "authors": "Mohsen Zand, Ali Etemad, and Michael Greenspan", "title": "Flow-based Autoregressive Structured Prediction of Human Motion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new method is proposed for human motion predition by learning temporal and\nspatial dependencies in an end-to-end deep neural network. The joint\nconnectivity is explicitly modeled using a novel autoregressive structured\nprediction representation based on flow-based generative models. We learn a\nlatent space of complex body poses in consecutive frames which is conditioned\non the high-dimensional structure input sequence. To construct each latent\nvariable, the general and local smoothness of the joint positions are\nconsidered in a generative process using conditional normalizing flows. As a\nresult, all frame-level and joint-level continuities in the sequence are\npreserved in the model. This enables us to parameterize the inter-frame and\nintra-frame relationships and joint connectivity for robust long-term\npredictions as well as short-term prediction. Our experiments on two\nchallenging benchmark datasets of Human3.6M and AMASS demonstrate that our\nproposed method is able to effectively model the sequence information for\nmotion prediction and outperform other techniques in 42 of the 48 total\nexperiment scenarios to set a new state-of-the-art.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 14:30:35 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Zand", "Mohsen", ""], ["Etemad", "Ali", ""], ["Greenspan", "Michael", ""]]}, {"id": "2104.04420", "submitter": "Senthil Yogamani", "authors": "Varun Ravi Kumar, Marvin Klingner, Senthil Yogamani, Markus Bach,\n  Stefan Milz, Tim Fingscheidt and Patrick M\\\"ader", "title": "SVDistNet: Self-Supervised Near-Field Distance Estimation on Surround\n  View Fisheye Cameras", "comments": "To be published at IEEE Transactions on Intelligent Transportation\n  Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A 360{\\deg} perception of scene geometry is essential for automated driving,\nnotably for parking and urban driving scenarios. Typically, it is achieved\nusing surround-view fisheye cameras, focusing on the near-field area around the\nvehicle. The majority of current depth estimation approaches focus on employing\njust a single camera, which cannot be straightforwardly generalized to multiple\ncameras. The depth estimation model must be tested on a variety of cameras\nequipped to millions of cars with varying camera geometries. Even within a\nsingle car, intrinsics vary due to manufacturing tolerances. Deep learning\nmodels are sensitive to these changes, and it is practically infeasible to\ntrain and test on each camera variant. As a result, we present novel\ncamera-geometry adaptive multi-scale convolutions which utilize the camera\nparameters as a conditional input, enabling the model to generalize to\npreviously unseen fisheye cameras. Additionally, we improve the distance\nestimation by pairwise and patchwise vector-based self-attention encoder\nnetworks. We evaluate our approach on the Fisheye WoodScape surround-view\ndataset, significantly improving over previous approaches. We also show a\ngeneralization of our approach across different camera viewing angles and\nperform extensive experiments to support our contributions. To enable\ncomparison with other approaches, we evaluate the front camera data on the\nKITTI dataset (pinhole camera images) and achieve state-of-the-art performance\namong self-supervised monocular methods. An overview video with qualitative\nresults is provided at https://youtu.be/bmX0UcU9wtA. Baseline code and dataset\nwill be made public.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 15:20:20 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Kumar", "Varun Ravi", ""], ["Klingner", "Marvin", ""], ["Yogamani", "Senthil", ""], ["Bach", "Markus", ""], ["Milz", "Stefan", ""], ["Fingscheidt", "Tim", ""], ["M\u00e4der", "Patrick", ""]]}, {"id": "2104.04430", "submitter": "Pascal Bohleber", "authors": "P. Bohleber, M. Roman, C. Barbante, S. Vascon, K. Siddiqi, M. Pelillo", "title": "Ice Core Science Meets Computer Vision: Challenges and Perspectives", "comments": "9 pages, 2 figures, submitted to Frontiers in Computer Science,\n  section Computer Vision", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV physics.geo-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Polar ice cores play a central role in studies of the earth's climate system\nthrough natural archives. A pressing issue is the analysis of the oldest,\nhighly thinned ice core sections, where the identification of paleoclimate\nsignals is particularly challenging. For this, state-of-the-art imaging by\nlaser-ablation inductively-coupled plasma mass spectrometry (LA-ICP-MS) has the\npotential to be revolutionary due to its combination of micron-scale 2D\nchemical information with visual features. However, the quantitative study of\nrecord preservation in chemical images raises new questions that call for the\nexpertise of the computer vision community. To illustrate this new\ninter-disciplinary frontier, we describe a selected set of key questions. One\ncritical task is to assess the paleoclimate significance of single line\nprofiles along the main core axis, which we show is a scale-dependent problem\nfor which advanced image analysis methods are critical. Another important issue\nis the evaluation of post-depositional layer changes, for which the chemical\nimages provide rich information. Accordingly, the time is ripe to begin an\nintensified exchange among the two scientific communities of computer vision\nand ice core science. The collaborative building of a new framework for\ninvestigating high-resolution chemical images with automated image analysis\ntechniques will also benefit the already wide-spread application of LA-ICP-MS\nchemical imaging in the geosciences.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 15:27:44 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Bohleber", "P.", ""], ["Roman", "M.", ""], ["Barbante", "C.", ""], ["Vascon", "S.", ""], ["Siddiqi", "K.", ""], ["Pelillo", "M.", ""]]}, {"id": "2104.04437", "submitter": "Minesh Mathew", "authors": "Minesh Mathew, Mohit Jain and CV Jawahar", "title": "Benchmarking Scene Text Recognition in Devanagari, Telugu and Malayalam", "comments": "This work was accepted at MOCR Workshop, ICDAR 2017 Uploading updated\n  draft which includes links to download datasets and rendering script", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by the success of Deep Learning based approaches to English scene\ntext recognition, we pose and benchmark scene text recognition for three Indic\nscripts - Devanagari, Telugu and Malayalam. Synthetic word images rendered from\nUnicode fonts are used for training the recognition system. And the performance\nis bench-marked on a new IIIT-ILST dataset comprising of hundreds of real scene\nimages containing text in the above mentioned scripts. We use a segmentation\nfree, hybrid but end-to-end trainable CNN-RNN deep neural network for\ntranscribing the word images to the corresponding texts. The cropped word\nimages need not be segmented into the sub-word units and the error is\ncalculated and backpropagated for the the given word image at once. The network\nis trained using CTC loss, which is proven quite effective for\nsequence-to-sequence transcription tasks. The CNN layers in the network learn\nto extract robust feature representations from word images. The sequence of\nfeatures learnt by the convolutional block is transcribed to a sequence of\nlabels by the RNN+CTC block. The transcription is not bound by word length or a\nlexicon and is ideal for Indian languages which are highly inflectional.\nIIIT-ILST dataset, synthetic word images dataset and the script used to render\nsynthetic images are available at\nhttp://cvit.iiit.ac.in/research/projects/cvit-projects/iiit-ilst\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 15:36:33 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Mathew", "Minesh", ""], ["Jain", "Mohit", ""], ["Jawahar", "CV", ""]]}, {"id": "2104.04443", "submitter": "Yujiang Wang", "authors": "Yingying Zhao, Mingzhi Dong, Yujiang Wang, Da Feng, Qin Lv, Robert P.\n  Dick, Dongsheng Li, Tun Lu, Ning Gu, Li Shang", "title": "A Reinforcement-Learning-Based Energy-Efficient Framework for Multi-Task\n  Video Analytics Pipeline", "comments": "IEEE Transactions on Multimedia", "journal-ref": null, "doi": "10.1109/TMM.2021.3076612", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep-learning-based video processing has yielded transformative results in\nrecent years. However, the video analytics pipeline is energy-intensive due to\nhigh data rates and reliance on complex inference algorithms, which limits its\nadoption in energy-constrained applications. Motivated by the observation of\nhigh and variable spatial redundancy and temporal dynamics in video data\nstreams, we design and evaluate an adaptive-resolution optimization framework\nto minimize the energy use of multi-task video analytics pipelines. Instead of\nheuristically tuning the input data resolution of individual tasks, our\nframework utilizes deep reinforcement learning to dynamically govern the input\nresolution and computation of the entire video analytics pipeline. By\nmonitoring the impact of varying resolution on the quality of high-dimensional\nvideo analytics features, hence the accuracy of video analytics results, the\nproposed end-to-end optimization framework learns the best non-myopic policy\nfor dynamically controlling the resolution of input video streams to globally\noptimize energy efficiency. Governed by reinforcement learning, optical flow is\nincorporated into the framework to minimize unnecessary spatio-temporal\nredundancy that leads to re-computation, while preserving accuracy. The\nproposed framework is applied to video instance segmentation which is one of\nthe most challenging computer vision tasks, and achieves better energy\nefficiency than all baseline methods of similar accuracy on the YouTube-VIS\ndataset.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 15:44:06 GMT"}, {"version": "v2", "created": "Sun, 2 May 2021 11:14:04 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Zhao", "Yingying", ""], ["Dong", "Mingzhi", ""], ["Wang", "Yujiang", ""], ["Feng", "Da", ""], ["Lv", "Qin", ""], ["Dick", "Robert P.", ""], ["Li", "Dongsheng", ""], ["Lu", "Tun", ""], ["Gu", "Ning", ""], ["Shang", "Li", ""]]}, {"id": "2104.04448", "submitter": "David Stutz", "authors": "David Stutz, Matthias Hein, Bernt Schiele", "title": "Relating Adversarially Robust Generalization to Flat Minima", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial training (AT) has become the de-facto standard to obtain models\nrobust against adversarial examples. However, AT exhibits severe robust\noverfitting: cross-entropy loss on adversarial examples, so-called robust loss,\ndecreases continuously on training examples, while eventually increasing on\ntest examples. In practice, this leads to poor robust generalization, i.e.,\nadversarial robustness does not generalize well to new examples. In this paper,\nwe study the relationship between robust generalization and flatness of the\nrobust loss landscape in weight space, i.e., whether robust loss changes\nsignificantly when perturbing weights. To this end, we propose average- and\nworst-case metrics to measure flatness in the robust loss landscape and show a\ncorrelation between good robust generalization and flatness. For example,\nthroughout training, flatness reduces significantly during overfitting such\nthat early stopping effectively finds flatter minima in the robust loss\nlandscape. Similarly, AT variants achieving higher adversarial robustness also\ncorrespond to flatter minima. This holds for many popular choices, e.g.,\nAT-AWP, TRADES, MART, AT with self-supervision or additional unlabeled\nexamples, as well as simple regularization techniques, e.g., AutoAugment,\nweight decay or label noise. For fair comparison across these approaches, our\nflatness measures are specifically designed to be scale-invariant and we\nconduct extensive experiments to validate our findings.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 15:55:01 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Stutz", "David", ""], ["Hein", "Matthias", ""], ["Schiele", "Bernt", ""]]}, {"id": "2104.04450", "submitter": "Kun Cao", "authors": "Shivam Khare, Kun Cao, James Rehg", "title": "Unsupervised Class-Incremental Learning Through Confusion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  While many works on Continual Learning have shown promising results for\nmitigating catastrophic forgetting, they have relied on supervised training. To\nsuccessfully learn in a label-agnostic incremental setting, a model must\ndistinguish between learned and novel classes to properly include samples for\ntraining. We introduce a novelty detection method that leverages network\nconfusion caused by training incoming data as a new class. We found that\nincorporating a class-imbalance during this detection method substantially\nenhances performance. The effectiveness of our approach is demonstrated across\na set of image classification benchmarks: MNIST, SVHN, CIFAR-10, CIFAR-100, and\nCRIB.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 15:58:43 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Khare", "Shivam", ""], ["Cao", "Kun", ""], ["Rehg", "James", ""]]}, {"id": "2104.04465", "submitter": "Shikun Liu", "authors": "Shikun Liu, Shuaifeng Zhi, Edward Johns, Andrew J. Davison", "title": "Bootstrapping Semantic Segmentation with Regional Contrast", "comments": "Project Page: https://shikun.io/projects/regional-contrast Code:\n  https://github.com/lorenmt/reco", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present ReCo, a contrastive learning framework designed at a regional\nlevel to assist learning in semantic segmentation. ReCo performs\nsemi-supervised or supervised pixel-level contrastive learning on a sparse set\nof hard negative pixels, with minimal additional memory footprint. ReCo is easy\nto implement, being built on top of off-the-shelf segmentation networks, and\nconsistently improves performance in both semi-supervised and supervised\nsemantic segmentation methods, achieving smoother segmentation boundaries and\nfaster convergence. The strongest effect is in semi-supervised learning with\nvery few labels. With ReCo, we achieve 50% mIoU in the CityScapes dataset,\nwhilst requiring only 20 labelled images, improving by 10% relative to the\nprevious state-of-the-art. Code is available at\nhttps://github.com/lorenmt/reco.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 16:26:29 GMT"}, {"version": "v2", "created": "Mon, 12 Apr 2021 00:53:22 GMT"}, {"version": "v3", "created": "Wed, 12 May 2021 18:01:39 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Liu", "Shikun", ""], ["Zhi", "Shuaifeng", ""], ["Johns", "Edward", ""], ["Davison", "Andrew J.", ""]]}, {"id": "2104.04480", "submitter": "Zekun Sun", "authors": "Zekun Sun and Yujie Han and Zeyu Hua and Na Ruan and Weijia Jia", "title": "Improving the Efficiency and Robustness of Deepfakes Detection through\n  Precise Geometric Features", "comments": "IEEE/CVF Conference on Computer Vision and Pattern Recognition 2021\n  (CVPR 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deepfakes is a branch of malicious techniques that transplant a target face\nto the original one in videos, resulting in serious problems such as\ninfringement of copyright, confusion of information, or even public panic.\nPrevious efforts for Deepfakes videos detection mainly focused on appearance\nfeatures, which have a risk of being bypassed by sophisticated manipulation,\nalso resulting in high model complexity and sensitiveness to noise. Besides,\nhow to mine the temporal features of manipulated videos and exploit them is\nstill an open question. We propose an efficient and robust framework named\nLRNet for detecting Deepfakes videos through temporal modeling on precise\ngeometric features. A novel calibration module is devised to enhance the\nprecision of geometric features, making it more discriminative, and a\ntwo-stream Recurrent Neural Network (RNN) is constructed for sufficient\nexploitation of temporal features. Compared to previous methods, our proposed\nmethod is lighter-weighted and easier to train. Moreover, our method has shown\nrobustness in detecting highly compressed or noise corrupted videos. Our model\nachieved 0.999 AUC on FaceForensics++ dataset. Meanwhile, it has a graceful\ndecline in performance (-0.042 AUC) when faced with highly compressed videos.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 16:57:55 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Sun", "Zekun", ""], ["Han", "Yujie", ""], ["Hua", "Zeyu", ""], ["Ruan", "Na", ""], ["Jia", "Weijia", ""]]}, {"id": "2104.04496", "submitter": "Eleni Charou Dr", "authors": "Dimitra Koumoutsou, Eleni Charou, Georgios Siolas, Giorgos Stamou", "title": "Class-Wise Principal Component Analysis for hyperspectral image feature\n  extraction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper introduces the Class-wise Principal Component Analysis, a\nsupervised feature extraction method for hyperspectral data. Hyperspectral\nImaging (HSI) has appeared in various fields in recent years, including Remote\nSensing. Realizing that information extraction tasks for hyperspectral images\nare burdened by data-specific issues, we identify and address two major\nproblems. Those are the Curse of Dimensionality which occurs due to the\nhigh-volume of the data cube and the class imbalance problem which is common in\nhyperspectral datasets. Dimensionality reduction is an essential preprocessing\nstep to complement a hyperspectral image classification task. Therefore, we\npropose a feature extraction algorithm for dimensionality reduction, based on\nPrincipal Component Analysis (PCA). Evaluations are carried out on the Indian\nPines dataset to demonstrate that significant improvements are achieved when\nusing the reduced data in a classification task.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 17:25:11 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Koumoutsou", "Dimitra", ""], ["Charou", "Eleni", ""], ["Siolas", "Georgios", ""], ["Stamou", "Giorgos", ""]]}, {"id": "2104.04518", "submitter": "Sivaramakrishnan Rajaraman", "authors": "Sivaramakrishnan Rajaraman, Ghada Zamzmi, Les Folio, Philip Alderson\n  and Sameer Antani", "title": "Chest X-Ray Bone Suppression for Improving Classification of\n  Tuberculosis-Consistent Findings", "comments": "22 pages, 14 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chest X-rays are the most commonly performed diagnostic examination to detect\ncardiopulmonary abnormalities. However, the presence of bony structures such as\nribs and clavicles can obscure subtle abnormalities, resulting in diagnostic\nerrors. This study aims to build a deep learning-based bone suppression model\nthat identifies and removes these occluding bony structures in frontal CXRs to\nassist in reducing errors in radiological interpretation, including DL\nworkflows, related to detecting manifestations consistent with tuberculosis\n(TB). Several bone suppression models with various deep architectures are\ntrained and optimized using the proposed combined loss function and their\nperformances are evaluated in a cross-institutional test setting. The\nbest-performing model is used to suppress bones in the publicly available\nShenzhen and Montgomery TB CXR collections. A VGG-16 model is pretrained on a\nlarge collection of publicly available CXRs. The CXR-pretrained model is then\nfine-tuned individually on the non-bone-suppressed and bone-suppressed CXRs of\nShenzhen and Montgomery TB CXR collections to classify them as showing normal\nlungs or TB manifestations. The performances of these models are compared using\nseveral performance metrics, analyzed for statistical significance, and their\npredictions are qualitatively interpreted through class-selective relevance\nmaps. It is observed that the models trained on bone-suppressed CXRs\nsignificantly outperformed (p<0.05) the models trained on the\nnon-bone-suppressed CXRs. Models trained on bone-suppressed CXRs improved\ndetection of TB-consistent findings and resulted in compact clustering of the\ndata points in the feature space signifying that bone suppression improved the\nmodel sensitivity toward TB classification.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 17:58:25 GMT"}, {"version": "v2", "created": "Fri, 7 May 2021 11:47:05 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Rajaraman", "Sivaramakrishnan", ""], ["Zamzmi", "Ghada", ""], ["Folio", "Les", ""], ["Alderson", "Philip", ""], ["Antani", "Sameer", ""]]}, {"id": "2104.04532", "submitter": "Dejan Azinovi\\'c", "authors": "Dejan Azinovi\\'c, Ricardo Martin-Brualla, Dan B Goldman, Matthias\n  Nie{\\ss}ner, Justus Thies", "title": "Neural RGB-D Surface Reconstruction", "comments": "Project page:\n  https://dazinovic.github.io/neural-rgbd-surface-reconstruction/ Video:\n  https://youtu.be/iWuSowPsC3g", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we explore how to leverage the success of implicit novel view\nsynthesis methods for surface reconstruction. Methods which learn a neural\nradiance field have shown amazing image synthesis results, but the underlying\ngeometry representation is only a coarse approximation of the real geometry. We\ndemonstrate how depth measurements can be incorporated into the radiance field\nformulation to produce more detailed and complete reconstruction results than\nusing methods based on either color or depth data alone. In contrast to a\ndensity field as the underlying geometry representation, we propose to learn a\ndeep neural network which stores a truncated signed distance field. Using this\nrepresentation, we show that one can still leverage differentiable volume\nrendering to estimate color values of the observed images during training to\ncompute a reconstruction loss. This is beneficial for learning the signed\ndistance field in regions with missing depth measurements. Furthermore, we\ncorrect misalignment errors of the camera, improving the overall reconstruction\nquality. In several experiments, we showcase our method and compare to existing\nworks on classical RGB-D fusion and learned representations.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 18:00:01 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Azinovi\u0107", "Dejan", ""], ["Martin-Brualla", "Ricardo", ""], ["Goldman", "Dan B", ""], ["Nie\u00dfner", "Matthias", ""], ["Thies", "Justus", ""]]}, {"id": "2104.04545", "submitter": "Neave O'Clery Dr", "authors": "Daniel Straulino, Juan C. Saldarriaga, Jairo A. G\\'omez, Juan C.\n  Duque, Neave O'Clery", "title": "Uncovering commercial activity in informal cities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.GN cs.CV physics.soc-ph q-fin.EC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge of the spatial organisation of economic activity within a city is\nkey to policy concerns. However, in developing cities with high levels of\ninformality, this information is often unavailable. Recent progress in machine\nlearning together with the availability of street imagery offers an affordable\nand easily automated solution. Here we propose an algorithm that can detect\nwhat we call 'visible firms' using street view imagery. Using Medell\\'in,\nColombia as a case study, we illustrate how this approach can be used to\nuncover previously unseen economic activity. Applying spatial analysis to our\ndataset we detect a polycentric structure with five distinct clusters located\nin both the established centre and peripheral areas. Comparing the density of\nvisible and registered firms, we find that informal activity concentrates in\npoor but densely populated areas. Our findings highlight the large gap between\nwhat is captured in official data and the reality on the ground.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 18:12:52 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Straulino", "Daniel", ""], ["Saldarriaga", "Juan C.", ""], ["G\u00f3mez", "Jairo A.", ""], ["Duque", "Juan C.", ""], ["O'Clery", "Neave", ""]]}, {"id": "2104.04598", "submitter": "Jayaprakash Akula", "authors": "Jatin Lamba, Abhishek, Jayaprakash Akula, Rishabh Dabral, Preethi\n  Jyothi, Ganesh Ramakrishnan", "title": "Cross-Modal learning for Audio-Visual Video Parsing", "comments": "Work accepted at Interspeech 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CV cs.LG eess.AS eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we present a novel approach to the audio-visual video parsing\n(AVVP) task that demarcates events from a video separately for audio and visual\nmodalities. The proposed parsing approach simultaneously detects the temporal\nboundaries in terms of start and end times of such events. We show how AVVP can\nbenefit from the following techniques geared towards effective cross-modal\nlearning: (i) adversarial training and skip connections (ii) global context\naware attention and, (iii) self-supervised pretraining using an audio-video\ngrounding objective to obtain cross-modal audio-video representations. We\npresent extensive experimental evaluations on the Look, Listen, and Parse (LLP)\ndataset and show that we outperform the state-of-the-art Hybrid Attention\nNetwork (HAN) on all five metrics proposed for AVVP. We also present several\nablations to validate the effect of pretraining, global attention and\nadversarial training.\n", "versions": [{"version": "v1", "created": "Sat, 3 Apr 2021 07:07:21 GMT"}, {"version": "v2", "created": "Mon, 21 Jun 2021 10:56:29 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Lamba", "Jatin", ""], ["Abhishek", "", ""], ["Akula", "Jayaprakash", ""], ["Dabral", "Rishabh", ""], ["Jyothi", "Preethi", ""], ["Ramakrishnan", "Ganesh", ""]]}, {"id": "2104.04606", "submitter": "Ali Mahdavi Amiri", "authors": "Jiongchao Jin, Arezou Fatemi, Wallace Lira, Fenggen Yu, Biao Leng, Rui\n  Ma, Ali Mahdavi-Amiri, Hao Zhang", "title": "RaidaR: A Rich Annotated Image Dataset of Rainy Street Scenes", "comments": "14 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce RaidaR, a rich annotated image dataset of rainy street scenes,\nto support autonomous driving research. The new dataset contains the largest\nnumber of rainy images (58,542) to date, 5,000 of which provide semantic\nsegmentations and 3,658 provide object instance segmentations. The RaidaR\nimages cover a wide range of realistic rain-induced artifacts, including fog,\ndroplets, and road reflections, which can effectively augment existing street\nscene datasets to improve data-driven machine perception during rainy weather.\nTo facilitate efficient annotation of a large volume of images, we develop a\nsemi-automatic scheme combining manual segmentation and an automated processing\nakin to cross validation, resulting in 10-20 fold reduction on annotation time.\nWe demonstrate the utility of our new dataset by showing how data augmentation\nwith RaidaR can elevate the accuracy of existing segmentation algorithms. We\nalso present a novel unpaired image-to-image translation algorithm for\nadding/removing rain artifacts, which directly benefits from RaidaR.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 21:15:34 GMT"}, {"version": "v2", "created": "Thu, 29 Jul 2021 00:56:45 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Jin", "Jiongchao", ""], ["Fatemi", "Arezou", ""], ["Lira", "Wallace", ""], ["Yu", "Fenggen", ""], ["Leng", "Biao", ""], ["Ma", "Rui", ""], ["Mahdavi-Amiri", "Ali", ""], ["Zhang", "Hao", ""]]}, {"id": "2104.04631", "submitter": "Yu-Wei Chao", "authors": "Yu-Wei Chao and Wei Yang and Yu Xiang and Pavlo Molchanov and Ankur\n  Handa and Jonathan Tremblay and Yashraj S. Narang and Karl Van Wyk and Umar\n  Iqbal and Stan Birchfield and Jan Kautz and Dieter Fox", "title": "DexYCB: A Benchmark for Capturing Hand Grasping of Objects", "comments": "Accepted to CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce DexYCB, a new dataset for capturing hand grasping of objects. We\nfirst compare DexYCB with a related one through cross-dataset evaluation. We\nthen present a thorough benchmark of state-of-the-art approaches on three\nrelevant tasks: 2D object and keypoint detection, 6D object pose estimation,\nand 3D hand pose estimation. Finally, we evaluate a new robotics-relevant task:\ngenerating safe robot grasps in human-to-robot object handover. Dataset and\ncode are available at https://dex-ycb.github.io.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 22:54:21 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Chao", "Yu-Wei", ""], ["Yang", "Wei", ""], ["Xiang", "Yu", ""], ["Molchanov", "Pavlo", ""], ["Handa", "Ankur", ""], ["Tremblay", "Jonathan", ""], ["Narang", "Yashraj S.", ""], ["Van Wyk", "Karl", ""], ["Iqbal", "Umar", ""], ["Birchfield", "Stan", ""], ["Kautz", "Jan", ""], ["Fox", "Dieter", ""]]}, {"id": "2104.04638", "submitter": "Shugao Ma", "authors": "Shugao Ma, Tomas Simon, Jason Saragih, Dawei Wang, Yuecheng Li,\n  Fernando De La Torre, Yaser Sheikh", "title": "Pixel Codec Avatars", "comments": "CVPR 2021 Oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Telecommunication with photorealistic avatars in virtual or augmented reality\nis a promising path for achieving authentic face-to-face communication in 3D\nover remote physical distances. In this work, we present the Pixel Codec\nAvatars (PiCA): a deep generative model of 3D human faces that achieves state\nof the art reconstruction performance while being computationally efficient and\nadaptive to the rendering conditions during execution. Our model combines two\ncore ideas: (1) a fully convolutional architecture for decoding spatially\nvarying features, and (2) a rendering-adaptive per-pixel decoder. Both\ntechniques are integrated via a dense surface representation that is learned in\na weakly-supervised manner from low-topology mesh tracking over training\nimages. We demonstrate that PiCA improves reconstruction over existing\ntechniques across testing expressions and views on persons of different gender\nand skin tone. Importantly, we show that the PiCA model is much smaller than\nthe state-of-art baseline model, and makes multi-person telecommunicaiton\npossible: on a single Oculus Quest 2 mobile VR headset, 5 avatars are rendered\nin realtime in the same scene.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 23:17:36 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Ma", "Shugao", ""], ["Simon", "Tomas", ""], ["Saragih", "Jason", ""], ["Wang", "Dawei", ""], ["Li", "Yuecheng", ""], ["De La Torre", "Fernando", ""], ["Sheikh", "Yaser", ""]]}, {"id": "2104.04641", "submitter": "Shiyu Tan", "authors": "Shiyu Tan, Yicheng Wu, Shoou-I Yu, Ashok Veeraraghavan", "title": "CodedStereo: Learned Phase Masks for Large Depth-of-field Stereo", "comments": "Accepted to CVPR 2021 as an oral presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV physics.optics", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional stereo suffers from a fundamental trade-off between imaging\nvolume and signal-to-noise ratio (SNR) -- due to the conflicting impact of\naperture size on both these variables. Inspired by the extended depth of field\ncameras, we propose a novel end-to-end learning-based technique to overcome\nthis limitation, by introducing a phase mask at the aperture plane of the\ncameras in a stereo imaging system. The phase mask creates a depth-dependent\npoint spread function, allowing us to recover sharp image texture and stereo\ncorrespondence over a significantly extended depth of field (EDOF) than\nconventional stereo. The phase mask pattern, the EDOF image reconstruction, and\nthe stereo disparity estimation are all trained together using an end-to-end\nlearned deep neural network. We perform theoretical analysis and\ncharacterization of the proposed approach and show a 6x increase in volume that\ncan be imaged in simulation. We also build an experimental prototype and\nvalidate the approach using real-world results acquired using this prototype\nsystem.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 23:44:52 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Tan", "Shiyu", ""], ["Wu", "Yicheng", ""], ["Yu", "Shoou-I", ""], ["Veeraraghavan", "Ashok", ""]]}, {"id": "2104.04650", "submitter": "Deval Mehta", "authors": "Deval Mehta, Umar Asif, Tian Hao, Erhan Bilal, Stefan Von Cavallar,\n  Stefan Harrer, Jeffrey Rogers", "title": "Towards Automated and Marker-less Parkinson Disease Assessment:\n  Predicting UPDRS Scores using Sit-stand videos", "comments": "Accepted by CVPR Workshops 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel deep learning enabled, video based analysis\nframework for assessing the Unified Parkinsons Disease Rating Scale (UPDRS)\nthat can be used in the clinic or at home. We report results from comparing the\nperformance of the framework to that of trained clinicians on a population of\n32 Parkinsons disease (PD) patients. In-person clinical assessments by trained\nneurologists are used as the ground truth for training our framework and for\ncomparing the performance. We find that the standard sit-to-stand activity can\nbe used to evaluate the UPDRS sub-scores of bradykinesia (BRADY) and posture\ninstability and gait disorders (PIGD). For BRADY we find F1-scores of 0.75\nusing our framework compared to 0.50 for the video based rater clinicians,\nwhile for PIGD we find 0.78 for the framework and 0.45 for the video based\nrater clinicians. We believe our proposed framework has potential to provide\nclinically acceptable end points of PD in greater granularity without imposing\nburdens on patients and clinicians, which empowers a variety of use cases such\nas passive tracking of PD progression in spaces such as nursing homes, in-home\nself-assessment, and enhanced tele-medicine.\n", "versions": [{"version": "v1", "created": "Sat, 10 Apr 2021 00:05:51 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Mehta", "Deval", ""], ["Asif", "Umar", ""], ["Hao", "Tian", ""], ["Bilal", "Erhan", ""], ["Von Cavallar", "Stefan", ""], ["Harrer", "Stefan", ""], ["Rogers", "Jeffrey", ""]]}, {"id": "2104.04662", "submitter": "Ping Zhang", "authors": "Ping Zhang, Zhenxiang Tao, Wenjie Yang, Minze Chen, Shan Ding,\n  Xiaodong Liu, Rui Yang, Hui Zhang", "title": "Unveiling personnel movement in a larger indoor area with a\n  non-overlapping multi-camera system", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Surveillance cameras are widely applied for indoor occupancy measurement and\nhuman movement perception, which benefit for building energy management and\nsocial security. To address the challenges of limited view angle of single\ncamera as well as lacking of inter-camera collaboration, this study presents a\nnon-overlapping multi-camera system to enlarge the surveillance area and\ndevotes to retrieve the same person appeared from different camera views. The\nsystem is deployed in an office building and four-day videos are collected. By\ntraining a deep convolutional neural network, the proposed system first\nextracts the appearance feature embeddings of each personal image, which\ndetected from different cameras, for similarity comparison. Then, a stochastic\ninter-camera transition matrix is associated with appearance feature for\nfurther improving the person re-identification ranking results. Finally, a\nnoise-suppression explanation is given for analyzing the matching improvements.\nThis paper expands the scope of indoor movement perception based on\nnon-overlapping multiple cameras and improves the accuracy of pedestrian\nre-identification without introducing additional types of sensors.\n", "versions": [{"version": "v1", "created": "Sat, 10 Apr 2021 01:44:26 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Zhang", "Ping", ""], ["Tao", "Zhenxiang", ""], ["Yang", "Wenjie", ""], ["Chen", "Minze", ""], ["Ding", "Shan", ""], ["Liu", "Xiaodong", ""], ["Yang", "Rui", ""], ["Zhang", "Hui", ""]]}, {"id": "2104.04665", "submitter": "Bin Deng", "authors": "Bin Deng, Yabin Zhang, Hui Tang, Changxing Ding, Kui Jia", "title": "On Universal Black-Box Domain Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study an arguably least restrictive setting of domain\nadaptation in a sense of practical deployment, where only the interface of\nsource model is available to the target domain, and where the label-space\nrelations between the two domains are allowed to be different and unknown. We\nterm such a setting as Universal Black-Box Domain Adaptation (UB$^2$DA). The\ngreat promise that UB$^2$DA makes, however, brings significant learning\nchallenges, since domain adaptation can only rely on the predictions of\nunlabeled target data in a partially overlapped label space, by accessing the\ninterface of source model. To tackle the challenges, we first note that the\nlearning task can be converted as two subtasks of in-class\\footnote{In this\npaper we use in-class (out-class) to describe the classes observed (not\nobserved) in the source black-box model.} discrimination and out-class\ndetection, which can be respectively learned by model distillation and entropy\nseparation. We propose to unify them into a self-training framework,\nregularized by consistency of predictions in local neighborhoods of target\nsamples. Our framework is simple, robust, and easy to be optimized. Experiments\non domain adaptation benchmarks show its efficacy. Notably, by accessing the\ninterface of source model only, our framework outperforms existing methods of\nuniversal domain adaptation that make use of source data and/or source models,\nwith a newly proposed (and arguably more reasonable) metric of H-score, and\nperforms on par with them with the metric of averaged class accuracy.\n", "versions": [{"version": "v1", "created": "Sat, 10 Apr 2021 02:21:09 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Deng", "Bin", ""], ["Zhang", "Yabin", ""], ["Tang", "Hui", ""], ["Ding", "Changxing", ""], ["Jia", "Kui", ""]]}, {"id": "2104.04672", "submitter": "Chen Liu", "authors": "Nanyan Zhu, Chen Liu, Xinyang Feng, Dipika Sikka, Sabrina\n  Gjerswold-Selleck, Scott A. Small, Jia Guo", "title": "Deep Learning Identifies Neuroimaging Signatures of Alzheimer's Disease\n  Using Structural and Synthesized Functional MRI Data", "comments": "Published in IEEE ISBI 2021. Available at\n  https://ieeexplore.ieee.org/document/9433808", "journal-ref": null, "doi": "10.1109/ISBI48211.2021.9433808", "report-no": null, "categories": "q-bio.QM cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Current neuroimaging techniques provide paths to investigate the structure\nand function of the brain in vivo and have made great advances in understanding\nAlzheimer's disease (AD). However, the group-level analyses prevalently used\nfor investigation and understanding of the disease are not applicable for\ndiagnosis of individuals. More recently, deep learning, which can efficiently\nanalyze large-scale complex patterns in 3D brain images, has helped pave the\nway for computer-aided individual diagnosis by providing accurate and automated\ndisease classification. Great progress has been made in classifying AD with\ndeep learning models developed upon increasingly available structural MRI data.\nThe lack of scale-matched functional neuroimaging data prevents such models\nfrom being further improved by observing functional changes in pathophysiology.\nHere we propose a potential solution by first learning a\nstructural-to-functional transformation in brain MRI, and further synthesizing\nspatially matched functional images from large-scale structural scans. We\nevaluated our approach by building computational models to discriminate\npatients with AD from healthy normal subjects and demonstrated a performance\nboost after combining the structural and synthesized functional brain images\ninto the same model. Furthermore, our regional analyses identified the temporal\nlobe to be the most predictive structural-region and the parieto-occipital lobe\nto be the most predictive functional-region of our model, which are both in\nconcordance with previous group-level neuroimaging findings. Together, we\ndemonstrate the potential of deep learning with large-scale structural and\nsynthesized functional MRI to impact AD classification and to identify AD's\nneuroimaging signatures.\n", "versions": [{"version": "v1", "created": "Sat, 10 Apr 2021 03:16:33 GMT"}, {"version": "v2", "created": "Fri, 28 May 2021 15:56:28 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Zhu", "Nanyan", ""], ["Liu", "Chen", ""], ["Feng", "Xinyang", ""], ["Sikka", "Dipika", ""], ["Gjerswold-Selleck", "Sabrina", ""], ["Small", "Scott A.", ""], ["Guo", "Jia", ""]]}, {"id": "2104.04687", "submitter": "Yueh-Cheng Liu", "authors": "Yueh-Cheng Liu, Yu-Kai Huang, Hung-Yueh Chiang, Hung-Ting Su, Zhe-Yu\n  Liu, Chin-Tang Chen, Ching-Yu Tseng, Winston H. Hsu", "title": "Learning from 2D: Contrastive Pixel-to-Point Knowledge Transfer for 3D\n  Pretraining", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Most 3D neural networks are trained from scratch owing to the lack of\nlarge-scale labeled datasets. In this paper, we present a novel 3D pretraining\nmethod by leveraging 2D networks learned from rich 2D datasets. We propose the\ncontrastive pixel-to-point knowledge transfer to effectively utilize the 2D\ninformation by mapping the pixel-level and point-level features into the same\nembedding space. Due to the heterogeneous nature between 2D and 3D networks, we\nintroduce the back-projection function to align the features between 2D and 3D\nto make the transfer possible. Additionally, we devise an upsampling feature\nprojection layer to increase the spatial resolution of high-level 2D feature\nmaps, which helps learning fine-grained 3D representations. With a pretrained\n2D network, the proposed pretraining process requires no additional 2D or 3D\nlabeled data, further alleviating the expansive 3D data annotation cost. To the\nbest of our knowledge, we are the first to exploit existing 2D trained weights\nto pretrain 3D deep neural networks. Our intensive experiments show that the 3D\nmodels pretrained with 2D knowledge boost the performances across various\nreal-world 3D downstream tasks.\n", "versions": [{"version": "v1", "created": "Sat, 10 Apr 2021 05:40:42 GMT"}, {"version": "v2", "created": "Mon, 26 Jul 2021 08:43:15 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Liu", "Yueh-Cheng", ""], ["Huang", "Yu-Kai", ""], ["Chiang", "Hung-Yueh", ""], ["Su", "Hung-Ting", ""], ["Liu", "Zhe-Yu", ""], ["Chen", "Chin-Tang", ""], ["Tseng", "Ching-Yu", ""], ["Hsu", "Winston H.", ""]]}, {"id": "2104.04691", "submitter": "Weiyao Wang", "authors": "Weiyao Wang, Matt Feiszli, Heng Wang, Du Tran", "title": "Unidentified Video Objects: A Benchmark for Dense, Open-World\n  Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current state-of-the-art object detection and segmentation methods work well\nunder the closed-world assumption. This closed-world setting assumes that the\nlist of object categories is available during training and deployment. However,\nmany real-world applications require detecting or segmenting novel objects,\ni.e., object categories never seen during training. In this paper, we present,\nUVO (Unidentified Video Objects), a new benchmark for open-world class-agnostic\nobject segmentation in videos. Besides shifting the problem focus to the\nopen-world setup, UVO is significantly larger, providing approximately 8 times\nmore videos compared with DAVIS, and 7 times more mask (instance) annotations\nper video compared with YouTube-VOS and YouTube-VIS. UVO is also more\nchallenging as it includes many videos with crowded scenes and complex\nbackground motions. We demonstrated that UVO can be used for other\napplications, such as object tracking and super-voxel segmentation, besides\nopen-world object segmentation. We believe that UVo is a versatile testbed for\nresearchers to develop novel approaches for open-world class-agnostic object\nsegmentation, and inspires new research directions towards a more comprehensive\nvideo understanding beyond classification and detection.\n", "versions": [{"version": "v1", "created": "Sat, 10 Apr 2021 06:16:25 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Wang", "Weiyao", ""], ["Feiszli", "Matt", ""], ["Wang", "Heng", ""], ["Tran", "Du", ""]]}, {"id": "2104.04715", "submitter": "Pascal Mettes", "authors": "Pascal Mettes, William Thong, Cees G. M. Snoek", "title": "Object Priors for Classifying and Localizing Unseen Actions", "comments": "Accepted to IJCV", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work strives for the classification and localization of human actions in\nvideos, without the need for any labeled video training examples. Where\nexisting work relies on transferring global attribute or object information\nfrom seen to unseen action videos, we seek to classify and spatio-temporally\nlocalize unseen actions in videos from image-based object information only. We\npropose three spatial object priors, which encode local person and object\ndetectors along with their spatial relations. On top we introduce three\nsemantic object priors, which extend semantic matching through word embeddings\nwith three simple functions that tackle semantic ambiguity, object\ndiscrimination, and object naming. A video embedding combines the spatial and\nsemantic object priors. It enables us to introduce a new video retrieval task\nthat retrieves action tubes in video collections based on user-specified\nobjects, spatial relations, and object size. Experimental evaluation on five\naction datasets shows the importance of spatial and semantic object priors for\nunseen actions. We find that persons and objects have preferred spatial\nrelations that benefit unseen action localization, while using multiple\nlanguages and simple object filtering directly improves semantic matching,\nleading to state-of-the-art results for both unseen action classification and\nlocalization.\n", "versions": [{"version": "v1", "created": "Sat, 10 Apr 2021 08:56:58 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Mettes", "Pascal", ""], ["Thong", "William", ""], ["Snoek", "Cees G. M.", ""]]}, {"id": "2104.04721", "submitter": "Mickael Cormier", "authors": "Mickael Cormier, Houraalsadat Mortazavi Moshkenan, Franz L\\\"orch,\n  J\\\"urgen Metzler, J\\\"urgen Beyerer", "title": "Do as we do: Multiple Person Video-To-Video Transfer", "comments": "Accepted to IEEE MIPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our goal is to transfer the motion of real people from a source video to a\ntarget video with realistic results. While recent advances significantly\nimproved image-to-image translations, only few works account for body motions\nand temporal consistency. However, those focus only on video re-targeting for a\nsingle actor/ for single actors. In this work, we propose a marker-less\napproach for multiple-person video-to-video transfer using pose as an\nintermediate representation. Given a source video with multiple persons dancing\nor working out, our method transfers the body motion of all actors to a new set\nof actors in a different video. Differently from recent \"do as I do\" methods,\nwe focus specifically on transferring multiple person at the same time and\ntackle the related identity switch problem. Our method is able to convincingly\ntransfer body motion to the target video, while preserving specific features of\nthe target video, such as feet touching the floor and relative position of the\nactors. The evaluation is performed with visual quality and appearance metrics\nusing publicly available videos with the permission of their owners.\n", "versions": [{"version": "v1", "created": "Sat, 10 Apr 2021 09:26:31 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Cormier", "Mickael", ""], ["Moshkenan", "Houraalsadat Mortazavi", ""], ["L\u00f6rch", "Franz", ""], ["Metzler", "J\u00fcrgen", ""], ["Beyerer", "J\u00fcrgen", ""]]}, {"id": "2104.04722", "submitter": "Petr Hurtik", "authors": "Petr Hurtik and Marek Vajgl", "title": "Coastline extraction from ALOS-2 satellite SAR images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The continuous monitoring of a shore plays an essential role in designing\nstrategies for shore protection against erosion. To avoid the effect of clouds\nand sunlight, satellite-based imagery with synthetic aperture radar is used to\nprovide the required data. We show how such data can be processed using\nstate-of-the-art methods, namely, by a deep-learning-based approach, to detect\nthe coastline location. We split the process into data reading, data\npreprocessing, model training, inference, ensembling, and postprocessing, and\ndescribe the best techniques for each of the parts. Finally, we present our own\nsolution that is able to precisely extract the coastline from an image even if\nit is not recognizable by a human. Our solution has been validated against the\nreal GPS location of the coastline during Signate's competition, where it was\nrunner-up among 109 teams across the whole world.\n", "versions": [{"version": "v1", "created": "Sat, 10 Apr 2021 09:29:58 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Hurtik", "Petr", ""], ["Vajgl", "Marek", ""]]}, {"id": "2104.04724", "submitter": "Bojun Ouyang", "authors": "Bojun Ouyang, Dan Raviv", "title": "Occlusion Guided Self-supervised Scene Flow Estimation on 3D Point\n  Clouds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the flow in 3D space of sparsely sampled points between two\nconsecutive time frames is the core stone of modern geometric-driven systems\nsuch as VR/AR, Robotics, and Autonomous driving. The lack of real,\nnon-simulated, labeled data for this task emphasizes the importance of self- or\nun-supervised deep architectures. This work presents a new self-supervised\ntraining method and an architecture for the 3D scene flow estimation under\nocclusions. Here we show that smart multi-layer fusion between flow prediction\nand occlusion detection outperforms traditional architectures by a large margin\nfor occluded and non-occluded scenarios. We report state-of-the-art results on\nFlyingthings3D and KITTI datasets for both the supervised and self-supervised\ntraining.\n", "versions": [{"version": "v1", "created": "Sat, 10 Apr 2021 09:55:19 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Ouyang", "Bojun", ""], ["Raviv", "Dan", ""]]}, {"id": "2104.04726", "submitter": "Mansi Sharma", "authors": "Mansi Sharma, Aditya Wadaskar", "title": "A Novel Unified Model for Multi-exposure Stereo Coding Based on Low Rank\n  Tucker-ALS and 3D-HEVC", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Display technology must offer high dynamic range (HDR) contrast-based depth\ninduction and 3D personalization simultaneously. Efficient algorithms to\ncompress HDR stereo data is critical. Direct capturing of HDR content is\ncomplicated due to the high expense and scarcity of HDR cameras. The HDR 3D\nimages could be generated in low-cost by fusing low-dynamic-range (LDR) images\nacquired using a stereo camera with various exposure settings. In this paper,\nan efficient scheme for coding multi-exposure stereo images is proposed based\non a tensor low-rank approximation scheme. The multi-exposure fusion can be\nrealized to generate HDR stereo output at the decoder for increased realism and\nexaggerated binocular 3D depth cues.\n  For exploiting spatial redundancy in LDR stereo images, the stack of\nmulti-exposure stereo images is decomposed into a set of projection matrices\nand a core tensor following an alternating least squares Tucker decomposition\nmodel. The compact, low-rank representation of the scene, thus, generated is\nfurther processed by 3D extension of High Efficiency Video Coding standard. The\nencoding with 3D-HEVC enhance the proposed scheme efficiency by exploiting\nintra-frame, inter-view and the inter-component redundancies in low-rank\napproximated representation. We consider constant luminance property of IPT and\nY'CbCr color space to precisely approximate intensity prediction and\nperceptually minimize the encoding distortion. Besides, the proposed scheme\ngives flexibility to adjust the bitrate of tensor latent components by changing\nthe rank of core tensor and its quantization. Extensive experiments on natural\nscenes demonstrate that the proposed scheme outperforms state-of-the-art\nJPEG-XT and 3D-HEVC range coding standards.\n", "versions": [{"version": "v1", "created": "Sat, 10 Apr 2021 10:10:14 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Sharma", "Mansi", ""], ["Wadaskar", "Aditya", ""]]}, {"id": "2104.04733", "submitter": "Nadeem Yousaf", "authors": "Nadeem Yousaf, Sarfaraz Hussein, Waqas Sultani", "title": "Estimation of BMI from Facial Images using Semantic Segmentation based\n  Region-Aware Pooling", "comments": "Accepted for publication in computers in biology and medicine", "journal-ref": "Computers in Biology and Medicine Volume 133, June 2021, Pages\n  104392", "doi": "10.1016/j.compbiomed.2021.104392", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Body-Mass-Index (BMI) conveys important information about one's life such as\nhealth and socio-economic conditions. Large-scale automatic estimation of BMIs\ncan help predict several societal behaviors such as health, job opportunities,\nfriendships, and popularity. The recent works have either employed hand-crafted\ngeometrical face features or face-level deep convolutional neural network\nfeatures for face to BMI prediction. The hand-crafted geometrical face feature\nlack generalizability and face-level deep features don't have detailed local\ninformation. Although useful, these methods missed the detailed local\ninformation which is essential for exact BMI prediction. In this paper, we\npropose to use deep features that are pooled from different face regions (eye,\nnose, eyebrow, lips, etc.,) and demonstrate that this explicit pooling from\nface regions can significantly boost the performance of BMI prediction. To\naddress the problem of accurate and pixel-level face regions localization, we\npropose to use face semantic segmentation in our framework. Extensive\nexperiments are performed using different Convolutional Neural Network (CNN)\nbackbones including FaceNet and VGG-face on three publicly available datasets:\nVisualBMI, Bollywood and VIP attributes. Experimental results demonstrate that,\nas compared to the recent works, the proposed Reg-GAP gives a percentage\nimprovement of 22.4\\% on VIP-attribute, 3.3\\% on VisualBMI, and 63.09\\% on the\nBollywood dataset.\n", "versions": [{"version": "v1", "created": "Sat, 10 Apr 2021 10:53:21 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Yousaf", "Nadeem", ""], ["Hussein", "Sarfaraz", ""], ["Sultani", "Waqas", ""]]}, {"id": "2104.04755", "submitter": "Akram Heidarizadeh", "authors": "Akram Heidarizadeh", "title": "Preprocessing Methods of Lane Detection and Tracking for Autonomous\n  Driving", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the past few years, researches on advanced driver assistance systems\n(ADASs) have been carried out and deployed in intelligent vehicles. Systems\nthat have been developed can perform different tasks, such as lane keeping\nassistance (LKA), lane departure warning (LDW), lane change warning (LCW) and\nadaptive cruise control (ACC). Real time lane detection and tracking (LDT) is\none of the most consequential parts to performing the above tasks. Images which\nare extracted from the video, contain noise and other unwanted factors such as\nvariation in lightening, shadow from nearby objects and etc. that requires\nrobust preprocessing methods for lane marking detection and tracking.\nPreprocessing is critical for the subsequent steps and real time performance\nbecause its main function is to remove the irrelevant image parts and enhance\nthe feature of interest. In this paper, we survey preprocessing methods for\ndetecting lane marking as well as tracking lane boundaries in real time\nfocusing on vision-based system.\n", "versions": [{"version": "v1", "created": "Sat, 10 Apr 2021 13:03:52 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Heidarizadeh", "Akram", ""]]}, {"id": "2104.04767", "submitter": "Sergei Belousov", "authors": "Sergei Belousov", "title": "MobileStyleGAN: A Lightweight Convolutional Neural Network for\n  High-Fidelity Image Synthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In recent years, the use of Generative Adversarial Networks (GANs) has become\nvery popular in generative image modeling. While style-based GAN architectures\nyield state-of-the-art results in high-fidelity image synthesis,\ncomputationally, they are highly complex. In our work, we focus on the\nperformance optimization of style-based generative models. We analyze the most\ncomputationally hard parts of StyleGAN2, and propose changes in the generator\nnetwork to make it possible to deploy style-based generative networks in the\nedge devices. We introduce MobileStyleGAN architecture, which has x3.5 fewer\nparameters and is x9.5 less computationally complex than StyleGAN2, while\nproviding comparable quality.\n", "versions": [{"version": "v1", "created": "Sat, 10 Apr 2021 13:46:49 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Belousov", "Sergei", ""]]}, {"id": "2104.04782", "submitter": "Tianfei Zhou", "authors": "Tianfei Zhou, Jianwu Li, Xueyi Li, Ling Shao", "title": "Target-Aware Object Discovery and Association for Unsupervised Video\n  Multi-Object Segmentation", "comments": "CVPR21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper addresses the task of unsupervised video multi-object\nsegmentation. Current approaches follow a two-stage paradigm: 1) detect object\nproposals using pre-trained Mask R-CNN, and 2) conduct generic feature matching\nfor temporal association using re-identification techniques. However, the\ngeneric features, widely used in both stages, are not reliable for\ncharacterizing unseen objects, leading to poor generalization. To address this,\nwe introduce a novel approach for more accurate and efficient spatio-temporal\nsegmentation. In particular, to address \\textbf{instance discrimination}, we\npropose to combine foreground region estimation and instance grouping together\nin one network, and additionally introduce temporal guidance for segmenting\neach frame, enabling more accurate object discovery. For \\textbf{temporal\nassociation}, we complement current video object segmentation architectures\nwith a discriminative appearance model, capable of capturing more fine-grained\ntarget-specific information. Given object proposals from the instance\ndiscrimination network, three essential strategies are adopted to achieve\naccurate segmentation: 1) target-specific tracking using a memory-augmented\nappearance model; 2) target-agnostic verification to trace possible tracklets\nfor the proposal; 3) adaptive memory updating using the verified segments. We\nevaluate the proposed approach on DAVIS$_{17}$ and YouTube-VIS, and the results\ndemonstrate that it outperforms state-of-the-art methods both in segmentation\naccuracy and inference speed.\n", "versions": [{"version": "v1", "created": "Sat, 10 Apr 2021 14:39:44 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Zhou", "Tianfei", ""], ["Li", "Jianwu", ""], ["Li", "Xueyi", ""], ["Shao", "Ling", ""]]}, {"id": "2104.04784", "submitter": "Javad Peymanfard", "authors": "Javad Peymanfard, Mohammad Reza Mohammadi, Hossein Zeinali and Nasser\n  Mozayani", "title": "Lip reading using external viseme decoding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lip-reading is the operation of recognizing speech from lip movements. This\nis a difficult task because the movements of the lips when pronouncing the\nwords are similar for some of them. Viseme is used to describe lip movements\nduring a conversation. This paper aims to show how to use external text data\n(for viseme-to-character mapping) by dividing video-to-character into two\nstages, namely converting video to viseme, and then converting viseme to\ncharacter by using separate models. Our proposed method improves word error\nrate by 4\\% compared to the normal sequence to sequence lip-reading model on\nthe BBC-Oxford Lip Reading Sentences 2 (LRS2) dataset.\n", "versions": [{"version": "v1", "created": "Sat, 10 Apr 2021 14:49:11 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Peymanfard", "Javad", ""], ["Mohammadi", "Mohammad Reza", ""], ["Zeinali", "Hossein", ""], ["Mozayani", "Nasser", ""]]}, {"id": "2104.04785", "submitter": "Bjorn Lutjens", "authors": "Bj\\\"orn L\\\"utjens, Brandon Leshchinskiy, Christian Requena-Mesa,\n  Farrukh Chishtie, Natalia D\\'iaz-Rodr\\'iguez, Oc\\'eane Boulais, Aruna\n  Sankaranarayanan, Aaron Pi\\~na, Yarin Gal, Chedy Ra\\\"issi, Alexander Lavin,\n  Dava Newman", "title": "Physically-Consistent Generative Adversarial Networks for Coastal Flood\n  Visualization", "comments": "arXiv admin note: text overlap with arXiv:2010.08103", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As climate change increases the intensity of natural disasters, society needs\nbetter tools for adaptation. Floods, for example, are the most frequent natural\ndisaster, and better tools for flood risk communication could increase the\nsupport for flood-resilient infrastructure development. Our work aims to enable\nmore visual communication of large-scale climate impacts via visualizing the\noutput of coastal flood models as satellite imagery. We propose the first deep\nlearning pipeline to ensure physical-consistency in synthetic visual satellite\nimagery. We advanced a state-of-the-art GAN called pix2pixHD, such that it\nproduces imagery that is physically-consistent with the output of an\nexpert-validated storm surge model (NOAA SLOSH). By evaluating the imagery\nrelative to physics-based flood maps, we find that our proposed framework\noutperforms baseline models in both physical-consistency and photorealism. We\nenvision our work to be the first step towards a global visualization of how\nclimate change shapes our landscape. Continuing on this path, we show that the\nproposed pipeline generalizes to visualize arctic sea ice melt. We also publish\na dataset of over 25k labelled image-pairs to study image-to-image translation\nin Earth observation.\n", "versions": [{"version": "v1", "created": "Sat, 10 Apr 2021 15:00:15 GMT"}, {"version": "v2", "created": "Wed, 5 May 2021 13:19:04 GMT"}, {"version": "v3", "created": "Wed, 19 May 2021 18:56:56 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["L\u00fctjens", "Bj\u00f6rn", ""], ["Leshchinskiy", "Brandon", ""], ["Requena-Mesa", "Christian", ""], ["Chishtie", "Farrukh", ""], ["D\u00edaz-Rodr\u00edguez", "Natalia", ""], ["Boulais", "Oc\u00e9ane", ""], ["Sankaranarayanan", "Aruna", ""], ["Pi\u00f1a", "Aaron", ""], ["Gal", "Yarin", ""], ["Ra\u00efssi", "Chedy", ""], ["Lavin", "Alexander", ""], ["Newman", "Dava", ""]]}, {"id": "2104.04794", "submitter": "Amin Jourabloo", "authors": "Amin Jourabloo, Fernando De la Torre, Jason Saragih, Shih-En Wei,\n  Te-Li Wang, Stephen Lombardi, Danielle Belko, Autumn Trimble, Hernan Badino", "title": "Robust Egocentric Photo-realistic Facial Expression Transfer for Virtual\n  Reality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Social presence, the feeling of being there with a real person, will fuel the\nnext generation of communication systems driven by digital humans in virtual\nreality (VR). The best 3D video-realistic VR avatars that minimize the uncanny\neffect rely on person-specific (PS) models. However, these PS models are\ntime-consuming to build and are typically trained with limited data\nvariability, which results in poor generalization and robustness. Major sources\nof variability that affects the accuracy of facial expression transfer\nalgorithms include using different VR headsets (e.g., camera configuration,\nslop of the headset), facial appearance changes over time (e.g., beard,\nmake-up), and environmental factors (e.g., lighting, backgrounds). This is a\nmajor drawback for the scalability of these models in VR. This paper makes\nprogress in overcoming these limitations by proposing an end-to-end\nmulti-identity architecture (MIA) trained with specialized augmentation\nstrategies. MIA drives the shape component of the avatar from three cameras in\nthe VR headset (two eyes, one mouth), in untrained subjects, using minimal\npersonalized information (i.e., neutral 3D mesh shape). Similarly, if the PS\ntexture decoder is available, MIA is able to drive the full avatar\n(shape+texture) robustly outperforming PS models in challenging scenarios. Our\nkey contribution to improve robustness and generalization, is that our method\nimplicitly decouples, in an unsupervised manner, the facial expression from\nnuisance factors (e.g., headset, environment, facial appearance). We\ndemonstrate the superior performance and robustness of the proposed method\nversus state-of-the-art PS approaches in a variety of experiments.\n", "versions": [{"version": "v1", "created": "Sat, 10 Apr 2021 15:48:53 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Jourabloo", "Amin", ""], ["De la Torre", "Fernando", ""], ["Saragih", "Jason", ""], ["Wei", "Shih-En", ""], ["Wang", "Te-Li", ""], ["Lombardi", "Stephen", ""], ["Belko", "Danielle", ""], ["Trimble", "Autumn", ""], ["Badino", "Hernan", ""]]}, {"id": "2104.04809", "submitter": "Tien Thanh Nguyen", "authors": "Truong Dang, Tien Thanh Nguyen, John McCall, Eyad Elyan, Carlos\n  Francisco Moreno-Garc\\'ia", "title": "Two layer Ensemble of Deep Learning Models for Medical Image\n  Segmentation", "comments": "8 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In recent years, deep learning has rapidly become a method of choice for the\nsegmentation of medical images. Deep Neural Network (DNN) architectures such as\nUNet have achieved state-of-the-art results on many medical datasets. To\nfurther improve the performance in the segmentation task, we develop an\nensemble system which combines various deep learning architectures. We propose\na two-layer ensemble of deep learning models for the segmentation of medical\nimages. The prediction for each training image pixel made by each model in the\nfirst layer is used as the augmented data of the training image for the second\nlayer of the ensemble. The prediction of the second layer is then combined by\nusing a weights-based scheme in which each model contributes differently to the\ncombined result. The weights are found by solving linear regression problems.\nExperiments conducted on two popular medical datasets namely CAMUS and\nKvasir-SEG show that the proposed method achieves better results concerning two\nperformance metrics (Dice Coefficient and Hausdorff distance) compared to some\nwell-known benchmark algorithms.\n", "versions": [{"version": "v1", "created": "Sat, 10 Apr 2021 16:52:34 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Dang", "Truong", ""], ["Nguyen", "Tien Thanh", ""], ["McCall", "John", ""], ["Elyan", "Eyad", ""], ["Moreno-Garc\u00eda", "Carlos Francisco", ""]]}, {"id": "2104.04829", "submitter": "Sally Ghanem", "authors": "Sally Ghanem, Siddharth Roheda, and Hamid Krim", "title": "Latent Code-Based Fusion: A Volterra Neural Network Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a deep structure encoder using the recently introduced Volterra\nNeural Networks (VNNs) to seek a latent representation of multi-modal data\nwhose features are jointly captured by a union of subspaces. The so-called\nself-representation embedding of the latent codes leads to a simplified fusion\nwhich is driven by a similarly constructed decoding. The Volterra Filter\narchitecture achieved reduction in parameter complexity is primarily due to\ncontrolled non-linearities being introduced by the higher-order convolutions in\ncontrast to generalized activation functions. Experimental results on two\ndifferent datasets have shown a significant improvement in the clustering\nperformance for VNNs auto-encoder over conventional Convolutional Neural\nNetworks (CNNs) auto-encoder. In addition, we also show that the proposed\napproach demonstrates a much-improved sample complexity over CNN-based\nauto-encoder with a superb robust classification performance.\n", "versions": [{"version": "v1", "created": "Sat, 10 Apr 2021 18:29:01 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Ghanem", "Sally", ""], ["Roheda", "Siddharth", ""], ["Krim", "Hamid", ""]]}, {"id": "2104.04832", "submitter": "Tien Thanh Nguyen", "authors": "Truong Dang, Thanh Nguyen, John McCall, Alan Wee-Chung Liew", "title": "Ensemble Learning based on Classifier Prediction Confidence and\n  Comprehensive Learning Particle Swarm Optimisation for polyp localisation", "comments": "10 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Colorectal cancer (CRC) is the first cause of death in many countries. CRC\noriginates from a small clump of cells on the lining of the colon called\npolyps, which over time might grow and become malignant. Early detection and\nremoval of polyps are therefore necessary for the prevention of colon cancer.\nIn this paper, we introduce an ensemble of medical polyp segmentation\nalgorithms. Based on an observation that different segmentation algorithms will\nperform well on different subsets of examples because of the nature and size of\ntraining sets they have been exposed to and because of method-intrinsic\nfactors, we propose to measure the confidence in the prediction of each\nalgorithm and then use an associate threshold to determine whether the\nconfidence is acceptable or not. An algorithm is selected for the ensemble if\nthe confidence is below its associate threshold. The optimal threshold for each\nsegmentation algorithm is found by using Comprehensive Learning Particle Swarm\nOptimization (CLPSO), a swarm intelligence algorithm. The Dice coefficient, a\npopular performance metric for image segmentation, is used as the fitness\ncriteria. Experimental results on two polyp segmentation datasets MICCAI2015\nand Kvasir-SEG confirm that our ensemble achieves better results compared to\nsome well-known segmentation algorithms.\n", "versions": [{"version": "v1", "created": "Sat, 10 Apr 2021 18:34:42 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Dang", "Truong", ""], ["Nguyen", "Thanh", ""], ["McCall", "John", ""], ["Liew", "Alan Wee-Chung", ""]]}, {"id": "2104.04843", "submitter": "Joseph Mundy", "authors": "Joseph L Mundy and Hank Theiss", "title": "Error Propagation in Satellite Multi-image Geometry", "comments": "15 pages, 27 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper describes an investigation of the source of geospatial error in\ndigital surface models (DSMs) constructed from multiple satellite images. In\nthis study the uncertainty in surface geometry is separated into two spatial\ncomponents; global error that affects the absolute position of the surface, and\nlocal error that varies from surface point to surface point. The global error\ncomponent is caused by inaccuracy in the satellite imaging process, mainly due\nto uncertainty in the satellite position and orientation (pose) during image\ncollection. A key result of the investigation is a new algorithm for\ndetermining the absolute geoposition of the DSM that takes into account the\npose covariance of each satellite during image collection. This covariance\ninformation is used to weigh the evidence from each image in the computation of\nthe global position of the DSM. The use of covariance information significantly\ndecreases the overall uncertainty in global position. The paper also describes\nan approach to the prediction of local error in the DSM surface. The observed\nvariance in surface position within a single stereo surface reconstruction\ndefines the local horizontal error. The variance in the fused set of elevations\nfrom multiple stereo pairs at a single DSM location defines the local vertical\nerror. These accuracy predictions are compared to ground truth provided by\nLiDAR scans of the same geographic region of interest.\n", "versions": [{"version": "v1", "created": "Sat, 10 Apr 2021 19:16:57 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Mundy", "Joseph L", ""], ["Theiss", "Hank", ""]]}, {"id": "2104.04866", "submitter": "Ruoyu Wang", "authors": "Ruoyu Wang, Xuchu Xu, Li Ding, Yang Huang, Chen Feng", "title": "Deep Weakly Supervised Positioning", "comments": "8 pages, 8 figures, submitted to IEEE Robotics and Automation Letters\n  (RA-L) and 2021 IEEE/RSJ International Conference on Intelligent Robots and\n  Systems (IROS 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  PoseNet can map a photo to the position where it is taken, which is appealing\nin robotics. However, training PoseNet requires full supervision, where ground\ntruth positions are non-trivial to obtain. Can we train PoseNet without knowing\nthe ground truth positions for each observation? We show that this is possible\nvia constraint-based weak-supervision, leading to the proposed framework:\nDeepGPS. Particularly, using wheel-encoder-estimated distances traveled by a\nrobot along random straight line segments as constraints between PoseNet\noutputs, DeepGPS can achieve a relative positioning error of less than 2%.\nMoreover, training DeepGPS can be done as auto-calibration with almost no human\nattendance, which is more attractive than its competing methods that typically\nrequire careful and expert-level manual calibration. We conduct various\nexperiments on simulated and real datasets to demonstrate the general\napplicability, effectiveness, and accuracy of DeepGPS, and perform a\ncomprehensive analysis of its robustness. Our code is available at\nhttps://ai4ce.github.io/DeepGPS/.\n", "versions": [{"version": "v1", "created": "Sat, 10 Apr 2021 21:19:08 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Wang", "Ruoyu", ""], ["Xu", "Xuchu", ""], ["Ding", "Li", ""], ["Huang", "Yang", ""], ["Feng", "Chen", ""]]}, {"id": "2104.04884", "submitter": "Abu Md Niamul Taufique", "authors": "Abu Md Niamul Taufique, David W. Messinger", "title": "Hyperspectral Pigment Analysis of Cultural Heritage Artifacts Using the\n  Opaque Form of Kubelka-Munk Theory", "comments": "11 pages, 9 figures", "journal-ref": "Proc. SPIE 10986, Algorithms, Technologies, and Applications for\n  Multispectral and Hyperspectral Imagery XXV, 1098611, 2019", "doi": "10.1117/12.2518451", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kubelka-Munk (K-M) theory has been successfully used to estimate pigment\nconcentrations in the pigment mixtures of modern paintings in spectral imagery.\nIn this study the single-constant K-M theory has been utilized for the\nclassification of green pigments in the Selden Map of China, a navigational map\nof the South China Sea likely created in the early seventeenth century.\nHyperspectral data of the map was collected at the Bodleian Library, University\nof Oxford, and can be used to estimate the pigment diversity, and spatial\ndistribution, within the map. This work seeks to assess the utility of\nanalyzing the data in the K/S space from Kubelka-Munk theory, as opposed to the\ntraditional reflectance domain. We estimate the dimensionality of the data and\nextract endmembers in the reflectance domain. Then we perform linear unmixing\nto estimate abundances in the K/S space, and following Bai, et al. (2017), we\nperform a classification in the abundance space. Finally, due to the lack of\nground truth labels, the classification accuracy was estimated by computing the\nmean spectrum of each class as the representative signature of that class, and\ncalculating the root mean squared error with all the pixels in that class to\ncreate a spatial representation of the error. This highlights both the\nmagnitude of, and any spatial pattern in, the errors, indicating if a\nparticular pigment is not well modeled in this approach.\n", "versions": [{"version": "v1", "created": "Sun, 11 Apr 2021 00:22:37 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Taufique", "Abu Md Niamul", ""], ["Messinger", "David W.", ""]]}, {"id": "2104.04891", "submitter": "Qingyong Hu", "authors": "Qingyong Hu, Bo Yang, Guangchi Fang, Yulan Guo, Ales Leonardis, Niki\n  Trigoni, Andrew Markham", "title": "SQN: Weakly-Supervised Semantic Segmentation of Large-Scale 3D Point\n  Clouds with 1000x Fewer Labels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.RO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We study the problem of labelling effort for semantic segmentation of\nlarge-scale 3D point clouds. Existing works usually rely on densely annotated\npoint-level semantic labels to provide supervision for network training.\nHowever, in real-world scenarios that contain billions of points, it is\nimpractical and extremely costly to manually annotate every single point. In\nthis paper, we first investigate whether dense 3D labels are truly required for\nlearning meaningful semantic representations. Interestingly, we find that the\nsegmentation performance of existing works only drops slightly given as few as\n1% of the annotations. However, beyond this point (e.g. 1 per thousand and\nbelow) existing techniques fail catastrophically. To this end, we propose a new\nweak supervision method to implicitly augment the total amount of available\nsupervision signals, by leveraging the semantic similarity between neighboring\npoints. Extensive experiments demonstrate that the proposed Semantic Query\nNetwork (SQN) achieves state-of-the-art performance on six large-scale open\ndatasets under weak supervision schemes, while requiring only 1000x fewer\nlabeled points for training. The code is available at\nhttps://github.com/QingyongHu/SQN.\n", "versions": [{"version": "v1", "created": "Sun, 11 Apr 2021 01:29:50 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Hu", "Qingyong", ""], ["Yang", "Bo", ""], ["Fang", "Guangchi", ""], ["Guo", "Yulan", ""], ["Leonardis", "Ales", ""], ["Trigoni", "Niki", ""], ["Markham", "Andrew", ""]]}, {"id": "2104.04899", "submitter": "Kaiwen Duan", "authors": "Kaiwen Duan, Lingxi Xie, Honggang Qi, Song Bai, Qingming Huang and Qi\n  Tian", "title": "Location-Sensitive Visual Recognition with Cross-IOU Loss", "comments": "13 pages, 7 figures and 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detection, instance segmentation, and pose estimation are popular\nvisual recognition tasks which require localizing the object by internal or\nboundary landmarks. This paper summarizes these tasks as location-sensitive\nvisual recognition and proposes a unified solution named location-sensitive\nnetwork (LSNet). Based on a deep neural network as the backbone, LSNet predicts\nan anchor point and a set of landmarks which together define the shape of the\ntarget object. The key to optimizing the LSNet lies in the ability of fitting\nvarious scales, for which we design a novel loss function named cross-IOU loss\nthat computes the cross-IOU of each anchor point-landmark pair to approximate\nthe global IOU between the prediction and ground-truth. The flexibly located\nand accurately predicted landmarks also enable LSNet to incorporate richer\ncontextual information for visual recognition. Evaluated on the MS-COCO\ndataset, LSNet set the new state-of-the-art accuracy for anchor-free object\ndetection (a 53.5% box AP) and instance segmentation (a 40.2% mask AP), and\nshows promising performance in detecting multi-scale human poses. Code is\navailable at https://github.com/Duankaiwen/LSNet\n", "versions": [{"version": "v1", "created": "Sun, 11 Apr 2021 02:17:14 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Duan", "Kaiwen", ""], ["Xie", "Lingxi", ""], ["Qi", "Honggang", ""], ["Bai", "Song", ""], ["Huang", "Qingming", ""], ["Tian", "Qi", ""]]}, {"id": "2104.04903", "submitter": "Chuang Yang", "authors": "Chuang Yang, Mulin Chen, Qi Wang, and Xuelong Li", "title": "RayNet: Real-time Scene Arbitrary-shape Text Detection with Multiple\n  Rays", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing object detection-based text detectors mainly concentrate on\ndetecting horizontal and multioriented text. However, they do not pay enough\nattention to complex-shape text (curved or other irregularly shaped text).\nRecently, segmentation-based text detection methods have been introduced to\ndeal with the complex-shape text; however, the pixel level processing increases\nthe computational cost significantly. To further improve the accuracy and\nefficiency, we propose a novel detection framework for arbitrary-shape text\ndetection, termed as RayNet. RayNet uses Center Point Set (CPS) and Ray\nDistance (RD) to fit text, where CPS is used to determine the text general\nposition and the RD is combined with CPS to compute Ray Points (RP) to localize\nthe text accurate shape. Since RP are disordered, we develop the Ray Points\nConnection (RPC) algorithm to reorder RP, which significantly improves the\ndetection performance of complex-shape text. RayNet achieves impressive\nperformance on existing curved text dataset (CTW1500) and quadrangle text\ndataset (ICDAR2015), which demonstrate its superiority against several\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sun, 11 Apr 2021 03:03:23 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Yang", "Chuang", ""], ["Chen", "Mulin", ""], ["Wang", "Qi", ""], ["Li", "Xuelong", ""]]}, {"id": "2104.04926", "submitter": "Dipti Mishra", "authors": "Dipti Mishra, Satish Kumar Singh, Rajat Kumar Singh, Krishna Preetham", "title": "Edge-Aware Image Compression using Deep Learning-based Super-resolution\n  Network", "comments": "13 pages, 9 figures, 16 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a learning-based compression scheme that envelopes a standard\ncodec between pre and post-processing deep CNNs. Specifically, we demonstrate\nimprovements over prior approaches utilizing a compression-decompression\nnetwork by introducing: (a) an edge-aware loss function to prevent blurring\nthat is commonly occurred in prior works & (b) a super-resolution convolutional\nneural network (CNN) for post-processing along with a corresponding\npre-processing network for improved rate-distortion performance in the low rate\nregime. The algorithm is assessed on a variety of datasets varying from low to\nhigh resolution namely Set 5, Set 7, Classic 5, Set 14, Live 1, Kodak, General\n100, CLIC 2019. When compared to JPEG, JPEG2000, BPG, and recent CNN approach,\nthe proposed algorithm contributes significant improvement in PSNR with an\napproximate gain of 20.75%, 8.47%, 3.22%, 3.23% and 24.59%, 14.46%, 10.14%,\n8.57% at low and high bit-rates respectively. Similarly, this improvement in\nMS-SSIM is approximately 71.43%, 50%, 36.36%, 23.08%, 64.70% and 64.47%,\n61.29%, 47.06%, 51.52%, 16.28% at low and high bit-rates respectively. With\nCLIC 2019 dataset, PSNR is found to be superior with approximately 16.67%,\n10.53%, 6.78%, and 24.62%, 17.39%, 14.08% at low and high bit-rates\nrespectively, over JPEG2000, BPG, and recent CNN approach. Similarly, the\nMS-SSIM is found to be superior with approximately 72%, 45.45%, 39.13%, 18.52%,\nand 71.43%, 50%, 41.18%, 17.07% at low and high bit-rates respectively,\ncompared to the same approaches. A similar type of improvement is achieved with\nother datasets also.\n", "versions": [{"version": "v1", "created": "Sun, 11 Apr 2021 05:50:31 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Mishra", "Dipti", ""], ["Singh", "Satish Kumar", ""], ["Singh", "Rajat Kumar", ""], ["Preetham", "Krishna", ""]]}, {"id": "2104.04945", "submitter": "Tomasz Szandala", "authors": "Tomasz Szandala", "title": "Enhancing Deep Neural Network Saliency Visualizations with Gradual\n  Extrapolation", "comments": "Published in IEEE Access:\n  https://ieeexplore.ieee.org/document/9468713", "journal-ref": "IEEE Access, 2021", "doi": "10.1109/ACCESS.2021.3093824", "report-no": null, "categories": "cs.CV cs.AI cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, an enhancement technique for the class activation mapping\nmethods such as gradient-weighted class activation maps or excitation\nbackpropagation is proposed to present the visual explanations of decisions\nfrom convolutional neural network-based models. The proposed idea, called\nGradual Extrapolation, can supplement any method that generates a heatmap\npicture by sharpening the output. Instead of producing a coarse localization\nmap that highlights the important predictive regions in the image, the proposed\nmethod outputs the specific shape that most contributes to the model output.\nThus, the proposed method improves the accuracy of saliency maps. The effect\nhas been achieved by the gradual propagation of the crude map obtained in the\ndeep layer through all preceding layers with respect to their activations. In\nvalidation tests conducted on a selected set of images, the faithfulness,\ninterpretability, and applicability of the method are evaluated. The proposed\ntechnique significantly improves the localization detection of the neural\nnetworks attention at low additional computational costs. Furthermore, the\nproposed method is applicable to a variety deep neural network models. The code\nfor the method can be found at\nhttps://github.com/szandala/gradual-extrapolation\n", "versions": [{"version": "v1", "created": "Sun, 11 Apr 2021 07:39:35 GMT"}, {"version": "v2", "created": "Sun, 27 Jun 2021 21:37:11 GMT"}, {"version": "v3", "created": "Wed, 7 Jul 2021 15:30:26 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Szandala", "Tomasz", ""]]}, {"id": "2104.04952", "submitter": "Junghyo Sohn", "authors": "Junghyo Sohn, Eunjin Jeon, Wonsik Jung, Eunsong Kang, Heung-Il Suk", "title": "Fine-Grained Attention for Weakly Supervised Object Localization", "comments": "16 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although recent advances in deep learning accelerated an improvement in a\nweakly supervised object localization (WSOL) task, there are still challenges\nto identify the entire body of an object, rather than only discriminative\nparts. In this paper, we propose a novel residual fine-grained attention (RFGA)\nmodule that autonomously excites the less activated regions of an object by\nutilizing information distributed over channels and locations within feature\nmaps in combination with a residual operation. To be specific, we devise a\nseries of mechanisms of triple-view attention representation, attention\nexpansion, and feature calibration. Unlike other attention-based WSOL methods\nthat learn a coarse attention map, having the same values across elements in\nfeature maps, our proposed RFGA learns fine-grained values in an attention map\nby assigning different attention values for each of the elements. We validated\nthe superiority of our proposed RFGA module by comparing it with the recent\nmethods in the literature over three datasets. Further, we analyzed the effect\nof each mechanism in our RFGA and visualized attention maps to get insights.\n", "versions": [{"version": "v1", "created": "Sun, 11 Apr 2021 08:14:05 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Sohn", "Junghyo", ""], ["Jeon", "Eunjin", ""], ["Jung", "Wonsik", ""], ["Kang", "Eunsong", ""], ["Suk", "Heung-Il", ""]]}, {"id": "2104.04953", "submitter": "Binyi Su", "authors": "Binyi Su, Zhong Zhou, Haiyong Chen, and Xiaochun Cao (Senior Member,\n  IEEE)", "title": "SIGAN: A Novel Image Generation Method for Solar Cell Defect\n  Segmentation and Augmentation", "comments": "11 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Solar cell electroluminescence (EL) defect segmentation is an interesting and\nchallenging topic. Many methods have been proposed for EL defect detection, but\nthese methods are still unsatisfactory due to the diversity of the defect and\nbackground. In this paper, we provide a new idea of using generative\nadversarial network (GAN) for defect segmentation. Firstly, the GAN-based\nmethod removes the defect region in the input defective image to get a\ndefect-free image, while keeping the background almost unchanged. Then, the\nsubtracted image is obtained by making difference between the defective input\nimage with the generated defect-free image. Finally, the defect region can be\nsegmented through thresholding the subtracted image. To keep the background\nunchanged before and after image generation, we propose a novel strong identity\nGAN (SIGAN), which adopts a novel strong identity loss to constraint the\nbackground consistency. The SIGAN can be used not only for defect segmentation,\nbut also small-samples defective dataset augmentation. Moreover, we release a\nnew solar cell EL image dataset named as EL-2019, which includes three types of\nimages: crack, finger interruption and defect-free. Experiments on EL-2019\ndataset show that the proposed method achieves 90.34% F-score, which\noutperforms many state-of-the-art methods in terms of solar cell defects\nsegmentation results.\n", "versions": [{"version": "v1", "created": "Sun, 11 Apr 2021 08:20:28 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Su", "Binyi", "", "Senior Member,\n  IEEE"], ["Zhou", "Zhong", "", "Senior Member,\n  IEEE"], ["Chen", "Haiyong", "", "Senior Member,\n  IEEE"], ["Cao", "Xiaochun", "", "Senior Member,\n  IEEE"]]}, {"id": "2104.04968", "submitter": "Yan Han", "authors": "Yan Han, Chongyan Chen, Ahmed Tewfik, Benjamin Glicksberg, Ying Ding,\n  Yifan Peng, Zhangyang Wang", "title": "Cross-Modal Contrastive Learning for Abnormality Classification and\n  Localization in Chest X-rays with Radiomics using a Feedback Loop", "comments": "Preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building a highly accurate predictive model for these tasks usually requires\na large number of manually annotated labels and pixel regions (bounding boxes)\nof abnormalities. However, it is expensive to acquire such annotations,\nespecially the bounding boxes. Recently, contrastive learning has shown strong\npromise in leveraging unlabeled natural images to produce highly generalizable\nand discriminative features. However, extending its power to the medical image\ndomain is under-explored and highly non-trivial, since medical images are much\nless amendable to data augmentations. In contrast, their domain knowledge, as\nwell as multi-modality information, is often crucial. To bridge this gap, we\npropose an end-to-end semi-supervised cross-modal contrastive learning\nframework, that simultaneously performs disease classification and localization\ntasks. The key knob of our framework is a unique positive sampling approach\ntailored for the medical images, by seamlessly integrating radiomic features as\nan auxiliary modality. Specifically, we first apply an image encoder to\nclassify the chest X-rays and to generate the image features. We next leverage\nGrad-CAM to highlight the crucial (abnormal) regions for chest X-rays (even\nwhen unannotated), from which we extract radiomic features. The radiomic\nfeatures are then passed through another dedicated encoder to act as the\npositive sample for the image features generated from the same chest X-ray. In\nthis way, our framework constitutes a feedback loop for image and radiomic\nmodality features to mutually reinforce each other. Their contrasting yields\ncross-modality representations that are both robust and interpretable.\nExtensive experiments on the NIH Chest X-ray dataset demonstrate that our\napproach outperforms existing baselines in both classification and localization\ntasks.\n", "versions": [{"version": "v1", "created": "Sun, 11 Apr 2021 09:16:29 GMT"}, {"version": "v2", "created": "Mon, 19 Apr 2021 17:42:38 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Han", "Yan", ""], ["Chen", "Chongyan", ""], ["Tewfik", "Ahmed", ""], ["Glicksberg", "Benjamin", ""], ["Ding", "Ying", ""], ["Peng", "Yifan", ""], ["Wang", "Zhangyang", ""]]}, {"id": "2104.04980", "submitter": "Ali Cheraghian", "authors": "Ali Cheraghian, Shafinn Rahman, Townim F. Chowdhury, Dylan Campbell,\n  Lars Petersson", "title": "Zero-Shot Learning on 3D Point Cloud Objects and Beyond", "comments": "arXiv admin note: substantial text overlap with arXiv:1912.07161", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zero-shot learning, the task of learning to recognize new classes not seen\nduring training, has received considerable attention in the case of 2D image\nclassification. However, despite the increasing ubiquity of 3D sensors, the\ncorresponding 3D point cloud classification problem has not been meaningfully\nexplored and introduces new challenges. In this paper, we identify some of the\nchallenges and apply 2D Zero-Shot Learning (ZSL) methods in the 3D domain to\nanalyze the performance of existing models. Then, we propose a novel approach\nto address the issues specific to 3D ZSL. We first present an inductive ZSL\nprocess and then extend it to the transductive ZSL and Generalized ZSL (GZSL)\nsettings for 3D point cloud classification. To this end, a novel loss function\nis developed that simultaneously aligns seen semantics with point cloud\nfeatures and takes advantage of unlabeled test data to address some known\nissues (e.g., the problems of domain adaptation, hubness, and data bias). While\ndesigned for the particularities of 3D point cloud classification, the method\nis shown to also be applicable to the more common use-case of 2D image\nclassification. An extensive set of experiments is carried out, establishing\nstate-of-the-art for ZSL and GZSL on synthetic (ModelNet40, ModelNet10, McGill)\nand real (ScanObjectNN) 3D point cloud datasets.\n", "versions": [{"version": "v1", "created": "Sun, 11 Apr 2021 10:04:06 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Cheraghian", "Ali", ""], ["Rahman", "Shafinn", ""], ["Chowdhury", "Townim F.", ""], ["Campbell", "Dylan", ""], ["Petersson", "Lars", ""]]}, {"id": "2104.04991", "submitter": "Wei Chen", "authors": "Wei Chen, Yu Liu, Erwin M. Bakker, Michael S. Lew", "title": "Integrating Information Theory and Adversarial Learning for Cross-modal\n  Retrieval", "comments": "Accepted by Pattern Recognition", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurately matching visual and textual data in cross-modal retrieval has been\nwidely studied in the multimedia community. To address these challenges posited\nby the heterogeneity gap and the semantic gap, we propose integrating Shannon\ninformation theory and adversarial learning. In terms of the heterogeneity gap,\nwe integrate modality classification and information entropy maximization\nadversarially. For this purpose, a modality classifier (as a discriminator) is\nbuilt to distinguish the text and image modalities according to their different\nstatistical properties. This discriminator uses its output probabilities to\ncompute Shannon information entropy, which measures the uncertainty of the\nmodality classification it performs. Moreover, feature encoders (as a\ngenerator) project uni-modal features into a commonly shared space and attempt\nto fool the discriminator by maximizing its output information entropy. Thus,\nmaximizing information entropy gradually reduces the distribution discrepancy\nof cross-modal features, thereby achieving a domain confusion state where the\ndiscriminator cannot classify two modalities confidently. To reduce the\nsemantic gap, Kullback-Leibler (KL) divergence and bi-directional triplet loss\nare used to associate the intra- and inter-modality similarity between features\nin the shared space. Furthermore, a regularization term based on KL-divergence\nwith temperature scaling is used to calibrate the biased label classifier\ncaused by the data imbalance issue. Extensive experiments with four deep models\non four benchmarks are conducted to demonstrate the effectiveness of the\nproposed approach.\n", "versions": [{"version": "v1", "created": "Sun, 11 Apr 2021 11:04:55 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Chen", "Wei", ""], ["Liu", "Yu", ""], ["Bakker", "Erwin M.", ""], ["Lew", "Michael S.", ""]]}, {"id": "2104.05014", "submitter": "Ziang Cheng", "authors": "Ziang Cheng, Hongdong Li, Richard Hartley, Yinqiang Zheng, Imari Sato", "title": "One Ring to Rule Them All: a simple solution to multi-view\n  3D-Reconstruction of shapes with unknown BRDF via a small Recurrent ResNet", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This paper proposes a simple method which solves an open problem of\nmulti-view 3D-Reconstruction for objects with unknown and generic surface\nmaterials, imaged by a freely moving camera and a freely moving point light\nsource. The object can have arbitrary (e.g. non-Lambertian), spatially-varying\n(or everywhere different) surface reflectances (svBRDF). Our solution consists\nof two smallsized neural networks (dubbed the 'Shape-Net' and 'BRDFNet'), each\nhaving about 1,000 neurons, used to parameterize the unknown shape and unknown\nsvBRDF, respectively. Key to our method is a special network design (namely, a\nResNet with a global feedback or 'ring' connection), which has a provable\nguarantee for finding a valid diffeomorphic shape parameterization. Despite the\nunderlying problem is highly non-convex hence impractical to solve by\ntraditional optimization techniques, our method converges reliably to high\nquality solutions, even without initialization. Extensive experiments\ndemonstrate the superiority of our method, and it naturally enables a wide\nrange of special-effect applications including novel-view-synthesis,\nrelighting, material retouching, and shape exchange without additional coding\neffort. We encourage the reader to view our demo video for better\nvisualizations.\n", "versions": [{"version": "v1", "created": "Sun, 11 Apr 2021 13:39:31 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Cheng", "Ziang", ""], ["Li", "Hongdong", ""], ["Hartley", "Richard", ""], ["Zheng", "Yinqiang", ""], ["Sato", "Imari", ""]]}, {"id": "2104.05015", "submitter": "Jin Zhang", "authors": "Jin Tang, Jin Zhang, Jianqin Yin", "title": "Temporal Consistency Two-Stream CNN for Human Motion Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fusion is critical for a two-stream network. In this paper, we propose a\nnovel temporal fusion (TF) module to fuse the two-stream joints' information to\npredict human motion, including a temporal concatenation and a reinforcement\ntrajectory spatial-temporal (TST) block, specifically designed to keep\nprediction temporal consistency. In particular, the temporal concatenation\nkeeps the temporal consistency of preliminary predictions from two streams.\nMeanwhile, the TST block improves the spatial-temporal feature coupling.\nHowever, the TF module can increase the temporal continuities between the first\npredicted pose and the given poses and between each predicted pose. The fusion\nis based on a two-stream network that consists of a dynamic velocity stream\n(V-Stream) and a static position stream (P-Stream) because we found that the\njoints' velocity information improves the short-term prediction, while the\njoints' position information is better at long-term prediction, and they are\ncomplementary in motion prediction. Finally, our approach achieves impressive\nresults on three benchmark datasets, including H3.6M, CMU-Mocap, and 3DPW in\nboth short-term and long-term predictions, confirming its effectiveness and\nefficiency.\n", "versions": [{"version": "v1", "created": "Sun, 11 Apr 2021 13:50:18 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Tang", "Jin", ""], ["Zhang", "Jin", ""], ["Yin", "Jianqin", ""]]}, {"id": "2104.05031", "submitter": "Naji Khosravan", "authors": "Rodney Lalonde, Naji Khosravan, Ulas Bagci", "title": "Deformable Capsules for Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Capsule networks promise significant benefits over convolutional networks by\nstoring stronger internal representations, and routing information based on the\nagreement between intermediate representations' projections. Despite this,\ntheir success has been mostly limited to small-scale classification datasets\ndue to their computationally expensive nature. Recent studies have partially\novercome this burden by locally-constraining the dynamic routing of features\nwith convolutional capsules. Though memory efficient, convolutional capsules\nimpose geometric constraints which fundamentally limit the ability of capsules\nto model the pose/deformation of objects. Further, they do not address the\nbigger memory concern of class-capsules scaling-up to bigger tasks such as\ndetection or large-scale classification. In this study, we introduce deformable\ncapsules (DeformCaps), a new capsule structure (SplitCaps), and a novel dynamic\nrouting algorithm (SE-Routing) to balance computational efficiency with the\nneed for modeling a large number of objects and classes. We demonstrate that\nthe proposed methods allow capsules to efficiently scale-up to large-scale\ncomputer vision tasks for the first time, and create the first-ever capsule\nnetwork for object detection in the literature. Our proposed architecture is a\none-stage detection framework and obtains results on MS COCO which are on-par\nwith state-of-the-art one-stage CNN-based methods, while producing fewer false\npositive detections.\n", "versions": [{"version": "v1", "created": "Sun, 11 Apr 2021 15:36:30 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Lalonde", "Rodney", ""], ["Khosravan", "Naji", ""], ["Bagci", "Ulas", ""]]}, {"id": "2104.05036", "submitter": "Sheng He", "authors": "Sheng He, Lambert Schomaker", "title": "GR-RNN: Global-Context Residual Recurrent Neural Networks for Writer\n  Identification", "comments": "To appear: Pattern Recognition", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This paper presents an end-to-end neural network system to identify writers\nthrough handwritten word images, which jointly integrates global-context\ninformation and a sequence of local fragment-based features. The global-context\ninformation is extracted from the tail of the neural network by a global\naverage pooling step. The sequence of local and fragment-based features is\nextracted from a low-level deep feature map which contains subtle information\nabout the handwriting style. The spatial relationship between the sequence of\nfragments is modeled by the recurrent neural network (RNN) to strengthen the\ndiscriminative ability of the local fragment features. We leverage the\ncomplementary information between the global-context and local fragments,\nresulting in the proposed global-context residual recurrent neural network\n(GR-RNN) method. The proposed method is evaluated on four public data sets and\nexperimental results demonstrate that it can provide state-of-the-art\nperformance. In addition, the neural networks trained on gray-scale images\nprovide better results than neural networks trained on binarized and contour\nimages, indicating that texture information plays an important role for writer\nidentification.\n  The source code will be available:\n  \\url{https://github.com/shengfly/writer-identification}.\n", "versions": [{"version": "v1", "created": "Sun, 11 Apr 2021 15:46:31 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["He", "Sheng", ""], ["Schomaker", "Lambert", ""]]}, {"id": "2104.05044", "submitter": "Maksym Ivashechkin", "authors": "Maksym Ivashechkin, Daniel Barath, Jiri Matas", "title": "USACv20: robust essential, fundamental and homography matrix estimation", "comments": "arXiv admin note: text overlap with arXiv:1912.05909", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We review the most recent RANSAC-like hypothesize-and-verify robust\nestimators. The best performing ones are combined to create a state-of-the-art\nversion of the Universal Sample Consensus (USAC) algorithm. A recent objective\nis to implement a modular and optimized framework, making future RANSAC modules\neasy to be included. The proposed method, USACv20, is tested on eight publicly\navailable real-world datasets, estimating homographies, fundamental and\nessential matrices. On average, USACv20 leads to the most geometrically\naccurate models and it is the fastest in comparison to the state-of-the-art\nrobust estimators. All reported properties improved performance of original\nUSAC algorithm significantly. The pipeline will be made available after\npublication.\n", "versions": [{"version": "v1", "created": "Sun, 11 Apr 2021 16:27:02 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Ivashechkin", "Maksym", ""], ["Barath", "Daniel", ""], ["Matas", "Jiri", ""]]}, {"id": "2104.05050", "submitter": "ZhongYang Liu Bo", "authors": "Yang Liu, Shengmao Zhang, Fei Wang, Wei Fan, Guohua Zou, Jing Bo", "title": "Research on Optimization Method of Multi-scale Fish Target Fast\n  Detection Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The fish target detection algorithm lacks a good quality data set, and the\nalgorithm achieves real-time detection with lower power consumption on embedded\ndevices, and it is difficult to balance the calculation speed and\nidentification ability. To this end, this paper collected and annotated a data\nset named \"Aquarium Fish\" of 84 fishes containing 10042 images, and based on\nthis data set, proposed a multi-scale input fast fish target detection network\n(BTP-yoloV3) and its optimization method. The experiment uses Depthwise\nconvolution to redesign the backbone of the yoloV4 network, which reduces the\namount of calculation by 94.1%, and the test accuracy is 92.34%. Then, the\ntraining model is enhanced with MixUp, CutMix, and mosaic to increase the test\naccuracy by 1.27%; Finally, use the mish, swish, and ELU activation functions\nto increase the test accuracy by 0.76%. As a result, the accuracy of testing\nthe network with 2000 fish images reached 94.37%, and the computational\ncomplexity of the network BFLOPS was only 5.47. Comparing the YoloV3~4,\nMobileNetV2-yoloV3, and YoloV3-tiny networks of migration learning on this data\nset. The results show that BTP-Yolov3 has smaller model parameters, faster\ncalculation speed, and lower energy consumption during operation while ensuring\nthe calculation accuracy. It provides a certain reference value for the\npractical application of neural network.\n", "versions": [{"version": "v1", "created": "Sun, 11 Apr 2021 16:53:34 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Liu", "Yang", ""], ["Zhang", "Shengmao", ""], ["Wang", "Fei", ""], ["Fan", "Wei", ""], ["Zou", "Guohua", ""], ["Bo", "Jing", ""]]}, {"id": "2104.05072", "submitter": "Furkan K{\\i}nl{\\i}", "authors": "Furkan K{\\i}nl{\\i}, Bar{\\i}\\c{s} \\\"Ozcan, Furkan K{\\i}ra\\c{c}", "title": "Instagram Filter Removal on Fashionable Images", "comments": "10 pages, 7 figures, Accepted to New Trends in Image Restoration and\n  Enhancement workshop and challenges on image and video processing in\n  conjunction with CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Social media images are generally transformed by filtering to obtain\naesthetically more pleasing appearances. However, CNNs generally fail to\ninterpret both the image and its filtered version as the same in the visual\nanalysis of social media images. We introduce Instagram Filter Removal Network\n(IFRNet) to mitigate the effects of image filters for social media analysis\napplications. To achieve this, we assume any filter applied to an image\nsubstantially injects a piece of additional style information to it, and we\nconsider this problem as a reverse style transfer problem. The visual effects\nof filtering can be directly removed by adaptively normalizing external style\ninformation in each level of the encoder. Experiments demonstrate that IFRNet\noutperforms all compared methods in quantitative and qualitative comparisons,\nand has the ability to remove the visual effects to a great extent.\nAdditionally, we present the filter classification performance of our proposed\nmodel, and analyze the dominant color estimation on the images unfiltered by\nall compared methods.\n", "versions": [{"version": "v1", "created": "Sun, 11 Apr 2021 18:44:43 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["K\u0131nl\u0131", "Furkan", ""], ["\u00d6zcan", "Bar\u0131\u015f", ""], ["K\u0131ra\u00e7", "Furkan", ""]]}, {"id": "2104.05077", "submitter": "Grigorios Chrysos", "authors": "Grigorios G Chrysos, Markos Georgopoulos, Yannis Panagakis", "title": "CoPE: Conditional image generation using Polynomial Expansions", "comments": "Under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative modeling has evolved to a notable field of machine learning. Deep\npolynomial neural networks (PNNs) have demonstrated impressive results in\nunsupervised image generation, where the task is to map an input vector (i.e.,\nnoise) to a synthesized image. However, the success of PNNs has not been\nreplicated in conditional generation tasks, such as super-resolution. Existing\nPNNs focus on single-variable polynomial expansions which do not fare well to\ntwo-variable inputs, i.e., the noise variable and the conditional variable. In\nthis work, we introduce a general framework, called CoPE, that enables a\npolynomial expansion of two input variables and captures their auto- and\ncross-correlations. We exhibit how CoPE can be trivially augmented to accept an\narbitrary number of input variables. CoPE is evaluated in five tasks\n(class-conditional generation, inverse problems, edges-to-image translation,\nimage-to-image translation, attribute-guided generation) involving eight\ndatasets. The thorough evaluation suggests that CoPE can be useful for tackling\ndiverse conditional generation tasks.\n", "versions": [{"version": "v1", "created": "Sun, 11 Apr 2021 19:02:37 GMT"}, {"version": "v2", "created": "Mon, 14 Jun 2021 12:52:20 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Chrysos", "Grigorios G", ""], ["Georgopoulos", "Markos", ""], ["Panagakis", "Yannis", ""]]}, {"id": "2104.05078", "submitter": "Oleg Shipitko", "authors": "Vera Soboleva, Oleg Shipitko", "title": "Raindrops on Windshield: Dataset and Lightweight Gradient-Based\n  Detection Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Autonomous vehicles use cameras as one of the primary sources of information\nabout the environment. Adverse weather conditions such as raindrops, snow, mud,\nand others, can lead to various image artifacts. Such artifacts significantly\ndegrade the quality and reliability of the obtained visual data and can lead to\naccidents if they are not detected in time. This paper presents ongoing work on\na new dataset for training and assessing vision algorithms' performance for\ndifferent tasks of image artifacts detection on either camera lens or\nwindshield. At the moment, we present a publicly available set of images\ncontaining $8190$ images, of which $3390$ contain raindrops. Images are\nannotated with the binary mask representing areas with raindrops. We\ndemonstrate the applicability of the dataset in the problems of raindrops\npresence detection and raindrop region segmentation. To augment the data, we\nalso propose an algorithm for data augmentation which allows the generation of\nsynthetic raindrops on images. Apart from the dataset, we present a novel\ngradient-based algorithm for raindrop presence detection in a video sequence.\nThe experimental evaluation proves that the algorithm reliably detects\nraindrops. Moreover, compared with the state-of-the-art cross-correlation-based\nalgorithm \\cite{Einecke2014}, the proposed algorithm showed a higher quality of\nraindrop presence detection and image processing speed, making it applicable\nfor the self-check procedure of real autonomous systems. The dataset is\navailable at\n\\href{https://github.com/EvoCargo/RaindropsOnWindshield}{$github.com/EvoCargo/RaindropsOnWindshield$}.\n", "versions": [{"version": "v1", "created": "Sun, 11 Apr 2021 19:04:59 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Soboleva", "Vera", ""], ["Shipitko", "Oleg", ""]]}, {"id": "2104.05107", "submitter": "Devis Tuia", "authors": "Devis Tuia, Ribana Roscher, Jan Dirk Wegner, Nathan Jacobs, Xiao Xiang\n  Zhu, Gustau Camps-Valls", "title": "Towards a Collective Agenda on AI for Earth Science Data Analysis", "comments": "In press at IEEE Geoscience and Remote Sensing Magazine", "journal-ref": null, "doi": "10.1109/MGRS.2020.3043504", "report-no": null, "categories": "cs.CV eess.SP", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In the last years we have witnessed the fields of geosciences and remote\nsensing and artificial intelligence to become closer. Thanks to both the\nmassive availability of observational data, improved simulations, and\nalgorithmic advances, these disciplines have found common objectives and\nchallenges to advance the modeling and understanding of the Earth system.\nDespite such great opportunities, we also observed a worrying tendency to\nremain in disciplinary comfort zones applying recent advances from artificial\nintelligence on well resolved remote sensing problems. Here we take a position\non research directions where we think the interface between these fields will\nhave the most impact and become potential game changers. In our declared agenda\nfor AI on Earth sciences, we aim to inspire researchers, especially the younger\ngenerations, to tackle these challenges for a real advance of remote sensing\nand the geosciences.\n", "versions": [{"version": "v1", "created": "Sun, 11 Apr 2021 20:54:44 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Tuia", "Devis", ""], ["Roscher", "Ribana", ""], ["Wegner", "Jan Dirk", ""], ["Jacobs", "Nathan", ""], ["Zhu", "Xiao Xiang", ""], ["Camps-Valls", "Gustau", ""]]}, {"id": "2104.05112", "submitter": "Zishen Wan", "authors": "Tian Gao, Zishen Wan, Yuyang Zhang, Bo Yu, Yanjun Zhang, Shaoshan Liu,\n  Arijit Raychowdhury", "title": "iELAS: An ELAS-Based Energy-Efficient Accelerator for Real-Time Stereo\n  Matching on FPGA Platform", "comments": "Equal contributions from first two authors. Accepted by IEEE\n  International Conference on Artificial Intelligence Circuits and Systems\n  (AICAS), June 6-9, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stereo matching is a critical task for robot navigation and autonomous\nvehicles, providing the depth estimation of surroundings. Among all stereo\nmatching algorithms, Efficient Large-scale Stereo (ELAS) offers one of the best\ntradeoffs between efficiency and accuracy. However, due to the inherent\niterative process and unpredictable memory access pattern, ELAS can only run at\n1.5-3 fps on high-end CPUs and difficult to achieve real-time performance on\nlow-power platforms. In this paper, we propose an energy-efficient architecture\nfor real-time ELAS-based stereo matching on FPGA platform. Moreover, the\noriginal computational-intensive and irregular triangulation module is reformed\nin a regular manner with points interpolation, which is much more\nhardware-friendly. Optimizations, including memory management, parallelism, and\npipelining, are further utilized to reduce memory footprint and improve\nthroughput. Compared with Intel i7 CPU and the state-of-the-art CPU+FPGA\nimplementation, our FPGA realization achieves up to 38.4x and 3.32x frame rate\nimprovement, and up to 27.1x and 1.13x energy efficiency improvement,\nrespectively.\n", "versions": [{"version": "v1", "created": "Sun, 11 Apr 2021 21:22:54 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Gao", "Tian", ""], ["Wan", "Zishen", ""], ["Zhang", "Yuyang", ""], ["Yu", "Bo", ""], ["Zhang", "Yanjun", ""], ["Liu", "Shaoshan", ""], ["Raychowdhury", "Arijit", ""]]}, {"id": "2104.05121", "submitter": "Sharath Chandra Guntuku", "authors": "Shubham Chaudhary, Sadbhawna, Vinit Jakhetiya, Badri N Subudhi, Ujjwal\n  Baid, Sharath Chandra Guntuku", "title": "Detecting COVID-19 and Community Acquired Pneumonia using Chest CT scan\n  images with Deep Learning", "comments": "Top Ranked Model Paper at the ICASSP 2021 COVID-19 Grand Challenge", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We propose a two-stage Convolutional Neural Network (CNN) based\nclassification framework for detecting COVID-19 and Community-Acquired\nPneumonia (CAP) using the chest Computed Tomography (CT) scan images. In the\nfirst stage, an infection - COVID-19 or CAP, is detected using a pre-trained\nDenseNet architecture. Then, in the second stage, a fine-grained three-way\nclassification is done using EfficientNet architecture. The proposed\nCOVID+CAP-CNN framework achieved a slice-level classification accuracy of over\n94% at identifying COVID-19 and CAP. Further, the proposed framework has the\npotential to be an initial screening tool for differential diagnosis of\nCOVID-19 and CAP, achieving a validation accuracy of over 89.3% at the finer\nthree-way COVID-19, CAP, and healthy classification. Within the IEEE ICASSP\n2021 Signal Processing Grand Challenge (SPGC) on COVID-19 Diagnosis, our\nproposed two-stage classification framework achieved an overall accuracy of 90%\nand sensitivity of .857, .9, and .942 at distinguishing COVID-19, CAP, and\nnormal individuals respectively, to rank first in the evaluation. Code and\nmodel weights are available at\nhttps://github.com/shubhamchaudhary2015/ct_covid19_cap_cnn\n", "versions": [{"version": "v1", "created": "Sun, 11 Apr 2021 22:05:19 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Chaudhary", "Shubham", ""], ["Sadbhawna", "", ""], ["Jakhetiya", "Vinit", ""], ["Subudhi", "Badri N", ""], ["Baid", "Ujjwal", ""], ["Guntuku", "Sharath Chandra", ""]]}, {"id": "2104.05124", "submitter": "Cuauhtemoc Daniel Suarez-Ramirez", "authors": "Cuauhtemoc Daniel Suarez-Ramirez, Miguel Gonzalez-Mendoza, Leonardo\n  Chang-Fernandez, Gilberto Ochoa-Ruiz, Mario Alberto Duran-Vega", "title": "A Bop and Beyond: A Second Order Optimizer for Binarized Neural Networks", "comments": "9 pages, 12 figures, Preprint accepted to the LatinX in CV Research\n  Workshop at CVPR'21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The optimization of Binary Neural Networks (BNNs) relies on approximating the\nreal-valued weights with their binarized representations. Current techniques\nfor weight-updating use the same approaches as traditional Neural Networks\n(NNs) with the extra requirement of using an approximation to the derivative of\nthe sign function - as it is the Dirac-Delta function - for back-propagation;\nthus, efforts are focused adapting full-precision techniques to work on BNNs.\nIn the literature, only one previous effort has tackled the problem of directly\ntraining the BNNs with bit-flips by using the first raw moment estimate of the\ngradients and comparing it against a threshold for deciding when to flip a\nweight (Bop). In this paper, we take an approach parallel to Adam which also\nuses the second raw moment estimate to normalize the first raw moment before\ndoing the comparison with the threshold, we call this method Bop2ndOrder. We\npresent two versions of the proposed optimizer: a biased one and a\nbias-corrected one, each with its own applications. Also, we present a complete\nablation study of the hyperparameters space, as well as the effect of using\nschedulers on each of them. For these studies, we tested the optimizer in\nCIFAR10 using the BinaryNet architecture. Also, we tested it in ImageNet 2012\nwith the XnorNet and BiRealNet architectures for accuracy. In both datasets our\napproach proved to converge faster, was robust to changes of the\nhyperparameters, and achieved better accuracy values.\n", "versions": [{"version": "v1", "created": "Sun, 11 Apr 2021 22:20:09 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Suarez-Ramirez", "Cuauhtemoc Daniel", ""], ["Gonzalez-Mendoza", "Miguel", ""], ["Chang-Fernandez", "Leonardo", ""], ["Ochoa-Ruiz", "Gilberto", ""], ["Duran-Vega", "Mario Alberto", ""]]}, {"id": "2104.05125", "submitter": "Evgeny Toropov", "authors": "Evgeny Toropov, Paola A. Buitrago, Jose M. F. Moura", "title": "Shuffler: A Large Scale Data Management Tool for ML in Computer Vision", "comments": null, "journal-ref": "PEARC 2019 Article No 23", "doi": "10.1145/3332186.3333046", "report-no": null, "categories": "cs.CV cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Datasets in the computer vision academic research community are primarily\nstatic. Once a dataset is accepted as a benchmark for a computer vision task,\nresearchers working on this task will not alter it in order to make their\nresults reproducible. At the same time, when exploring new tasks and new\napplications, datasets tend to be an ever changing entity. A practitioner may\ncombine existing public datasets, filter images or objects in them, change\nannotations or add new ones to fit a task at hand, visualize sample images, or\nperhaps output statistics in the form of text or plots. In fact, datasets\nchange as practitioners experiment with data as much as with algorithms, trying\nto make the most out of machine learning models. Given that ML and deep\nlearning call for large volumes of data to produce satisfactory results, it is\nno surprise that the resulting data and software management associated to\ndealing with live datasets can be quite complex. As far as we know, there is no\nflexible, publicly available instrument to facilitate manipulating image data\nand their annotations throughout a ML pipeline. In this work, we present\nShuffler, an open source tool that makes it easy to manage large computer\nvision datasets. It stores annotations in a relational, human-readable\ndatabase. Shuffler defines over 40 data handling operations with annotations\nthat are commonly useful in supervised learning applied to computer vision and\nsupports some of the most well-known computer vision datasets. Finally, it is\neasily extensible, making the addition of new operations and datasets a task\nthat is fast and easy to accomplish.\n", "versions": [{"version": "v1", "created": "Sun, 11 Apr 2021 22:27:28 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Toropov", "Evgeny", ""], ["Buitrago", "Paola A.", ""], ["Moura", "Jose M. F.", ""]]}, {"id": "2104.05145", "submitter": "Chaoxing Huang", "authors": "Chaoxing Huang", "title": "Event-based Timestamp Image Encoding Network for Human Action\n  Recognition and Anticipation", "comments": "This paper has been accepted by IJCNN 2021,the International Joint\n  Conference on Neural Networks. arXiv admin note: substantial text overlap\n  with arXiv:2009.13049", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Event camera is an asynchronous, high frequency vision sensor with low power\nconsumption, which is suitable for human action understanding task. It is vital\nto encode the spatial-temporal information of event data properly and use\nstandard computer vision tool to learn from the data. In this work, we propose\na timestamp image encoding 2D network, which takes the encoded spatial-temporal\nimages with polarity information of the event data as input and output the\naction label. In addition, we propose a future timestamp image generator to\ngenerate futureaction information to aid the model to anticipate the human\naction when the action is not completed. Experiment results show that our\nmethod can achieve the same level of performance as those RGB-based benchmarks\non real world action recognition,and also achieve the state of the art (SOTA)\nresult on gesture recognition. Our future timestamp image generating model can\neffectively improve the prediction accuracy when the action is not completed.\nWe also provide insight discussion on the importance of motion and appearance\ninformation in action recognition and anticipation.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 00:43:31 GMT"}, {"version": "v2", "created": "Tue, 13 Apr 2021 00:34:47 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Huang", "Chaoxing", ""]]}, {"id": "2104.05160", "submitter": "Delian Ruan", "authors": "Delian Ruan and Yan Yan and Shenqi Lai and Zhenhua Chai and Chunhua\n  Shen and Hanzi Wang", "title": "Feature Decomposition and Reconstruction Learning for Effective Facial\n  Expression Recognition", "comments": "accepted to CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel Feature Decomposition and Reconstruction\nLearning (FDRL) method for effective facial expression recognition. We view the\nexpression information as the combination of the shared information (expression\nsimilarities) across different expressions and the unique information\n(expression-specific variations) for each expression. More specifically, FDRL\nmainly consists of two crucial networks: a Feature Decomposition Network (FDN)\nand a Feature Reconstruction Network (FRN). In particular, FDN first decomposes\nthe basic features extracted from a backbone network into a set of facial\naction-aware latent features to model expression similarities. Then, FRN\ncaptures the intra-feature and inter-feature relationships for latent features\nto characterize expression-specific variations, and reconstructs the expression\nfeature. To this end, two modules including an intra-feature relation modeling\nmodule and an inter-feature relation modeling module are developed in FRN.\nExperimental results on both the in-the-lab databases (including CK+, MMI, and\nOulu-CASIA) and the in-the-wild databases (including RAF-DB and SFEW) show that\nthe proposed FDRL method consistently achieves higher recognition accuracy than\nseveral state-of-the-art methods. This clearly highlights the benefit of\nfeature decomposition and reconstruction for classifying expressions.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 02:22:45 GMT"}, {"version": "v2", "created": "Wed, 21 Apr 2021 01:19:47 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Ruan", "Delian", ""], ["Yan", "Yan", ""], ["Lai", "Shenqi", ""], ["Chai", "Zhenhua", ""], ["Shen", "Chunhua", ""], ["Wang", "Hanzi", ""]]}, {"id": "2104.05164", "submitter": "Shaolei Liu", "authors": "Xiaoyuan Luo, Shaolei Liu, Kexue Fu, Manning Wang, Zhijian Song", "title": "A Learnable Self-supervised Task for Unsupervised Domain Adaptation on\n  Point Clouds", "comments": "10 pages, 4 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have achieved promising performance in supervised point\ncloud applications, but manual annotation is extremely expensive and\ntime-consuming in supervised learning schemes. Unsupervised domain adaptation\n(UDA) addresses this problem by training a model with only labeled data in the\nsource domain but making the model generalize well in the target domain.\nExisting studies show that self-supervised learning using both source and\ntarget domain data can help improve the adaptability of trained models, but\nthey all rely on hand-crafted designs of the self-supervised tasks. In this\npaper, we propose a learnable self-supervised task and integrate it into a\nself-supervision-based point cloud UDA architecture. Specifically, we propose a\nlearnable nonlinear transformation that transforms a part of a point cloud to\ngenerate abundant and complicated point clouds while retaining the original\nsemantic information, and the proposed self-supervised task is to reconstruct\nthe original point cloud from the transformed ones. In the UDA architecture, an\nencoder is shared between the networks for the self-supervised task and the\nmain task of point cloud classification or segmentation, so that the encoder\ncan be trained to extract features suitable for both the source and the target\ndomain data. Experiments on PointDA-10 and PointSegDA datasets show that the\nproposed method achieves new state-of-the-art performance on both\nclassification and segmentation tasks of point cloud UDA. Code will be made\npublicly available.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 02:30:16 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Luo", "Xiaoyuan", ""], ["Liu", "Shaolei", ""], ["Fu", "Kexue", ""], ["Wang", "Manning", ""], ["Song", "Zhijian", ""]]}, {"id": "2104.05166", "submitter": "Long Dang", "authors": "Long Hoang Dang, Thao Minh Le, Vuong Le, Truyen Tran", "title": "Object-Centric Representation Learning for Video Question Answering", "comments": "Accepted by IJCNN 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video question answering (Video QA) presents a powerful testbed for\nhuman-like intelligent behaviors. The task demands new capabilities to\nintegrate video processing, language understanding, binding abstract linguistic\nconcepts to concrete visual artifacts, and deliberative reasoning over\nspacetime. Neural networks offer a promising approach to reach this potential\nthrough learning from examples rather than handcrafting features and rules.\nHowever, neural networks are predominantly feature-based - they map data to\nunstructured vectorial representation and thus can fall into the trap of\nexploiting shortcuts through surface statistics instead of true systematic\nreasoning seen in symbolic systems. To tackle this issue, we advocate for\nobject-centric representation as a basis for constructing spatio-temporal\nstructures from videos, essentially bridging the semantic gap between low-level\npattern recognition and high-level symbolic algebra. To this end, we propose a\nnew query-guided representation framework to turn a video into an evolving\nrelational graph of objects, whose features and interactions are dynamically\nand conditionally inferred. The object lives are then summarized into resumes,\nlending naturally for deliberative relational reasoning that produces an answer\nto the query. The framework is evaluated on major Video QA datasets,\ndemonstrating clear benefits of the object-centric approach to video reasoning.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 02:37:20 GMT"}, {"version": "v2", "created": "Tue, 13 Apr 2021 07:36:07 GMT"}, {"version": "v3", "created": "Fri, 9 Jul 2021 00:06:59 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Dang", "Long Hoang", ""], ["Le", "Thao Minh", ""], ["Le", "Vuong", ""], ["Tran", "Truyen", ""]]}, {"id": "2104.05167", "submitter": "Hao Jiang", "authors": "Hao Jiang, Vamsi Krishna Ithapu", "title": "Egocentric Pose Estimation from Human Vision Span", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating camera wearer's body pose from an egocentric view (egopose) is a\nvital task in augmented and virtual reality. Existing approaches either use a\nnarrow field of view front facing camera that barely captures the wearer, or an\nextruded head-mounted top-down camera for maximal wearer visibility. In this\npaper, we tackle the egopose estimation from a more natural human vision span,\nwhere camera wearer can be seen in the peripheral view and depending on the\nhead pose the wearer may become invisible or has a limited partial view. This\nis a realistic visual field for user-centric wearable devices like glasses\nwhich have front facing wide angle cameras. Existing solutions are not\nappropriate for this setting, and so, we propose a novel deep learning system\ntaking advantage of both the dynamic features from camera SLAM and the body\nshape imagery. We compute 3D head pose, 3D body pose, the figure/ground\nseparation, all at the same time while explicitly enforcing a certain geometric\nconsistency across pose attributes. We further show that this system can be\ntrained robustly with lots of existing mocap data so we do not have to collect\nand annotate large new datasets. Lastly, our system estimates egopose in real\ntime and on the fly while maintaining high accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 02:38:22 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Jiang", "Hao", ""], ["Ithapu", "Vamsi Krishna", ""]]}, {"id": "2104.05170", "submitter": "Somi Jeong", "authors": "Somi Jeong, Youngjung Kim, Eungbean Lee, Kwanghoon Sohn", "title": "Memory-guided Unsupervised Image-to-image Translation", "comments": "CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a novel unsupervised framework for instance-level image-to-image\ntranslation. Although recent advances have been made by incorporating\nadditional object annotations, existing methods often fail to handle images\nwith multiple disparate objects. The main cause is that, during inference, they\napply a global style to the whole image and do not consider the large style\ndiscrepancy between instance and background, or within instances. To address\nthis problem, we propose a class-aware memory network that explicitly reasons\nabout local style variations. A key-values memory structure, with a set of\nread/update operations, is introduced to record class-wise style variations and\naccess them without requiring an object detector at the test time. The key\nstores a domain-agnostic content representation for allocating memory items,\nwhile the values encode domain-specific style representations. We also present\na feature contrastive loss to boost the discriminative power of memory items.\nWe show that by incorporating our memory, we can transfer class-aware and\naccurate style representations across domains. Experimental results demonstrate\nthat our model outperforms recent instance-level methods and achieves\nstate-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 03:02:51 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Jeong", "Somi", ""], ["Kim", "Youngjung", ""], ["Lee", "Eungbean", ""], ["Sohn", "Kwanghoon", ""]]}, {"id": "2104.05171", "submitter": "Zixia Zhou", "authors": "Zixia Zhou, Yuanyuan Wang, Boudewijn P.F. Lelieveldt, Qian Tao", "title": "Deep Recursive Embedding for High-Dimensional Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  t-distributed stochastic neighbor embedding (t-SNE) is a well-established\nvisualization method for complex high-dimensional data. However, the original\nt-SNE method is nonparametric, stochastic, and often cannot well prevserve the\nglobal structure of data as it emphasizes local neighborhood. With t-SNE as a\nreference, we propose to combine the deep neural network (DNN) with the\nmathematical-grounded embedding rules for high-dimensional data embedding. We\nfirst introduce a deep embedding network (DEN) framework, which can learn a\nparametric mapping from high-dimensional space to low-dimensional embedding.\nDEN has a flexible architecture that can accommodate different input data\n(vector, image, or tensor) and loss functions. To improve the embedding\nperformance, a recursive training strategy is proposed to make use of the\nlatent representations extracted by DEN. Finally, we propose a two-stage loss\nfunction combining the advantages of two popular embedding methods, namely,\nt-SNE and uniform manifold approximation and projection (UMAP), for optimal\nvisualization effect. We name the proposed method Deep Recursive Embedding\n(DRE), which optimizes DEN with a recursive training strategy and two-stage\nlosse. Our experiments demonstrated the excellent performance of the proposed\nDRE method on high-dimensional data embedding, across a variety of public\ndatabases. Remarkably, our comparative results suggested that our proposed DRE\ncould lead to improved global structure preservation.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 03:04:38 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Zhou", "Zixia", ""], ["Wang", "Yuanyuan", ""], ["Lelieveldt", "Boudewijn P. F.", ""], ["Tao", "Qian", ""]]}, {"id": "2104.05177", "submitter": "Cheng Chi", "authors": "Cheng Chi and Shuran Song", "title": "GarmentNets: Category-Level Pose Estimation for Garments via Canonical\n  Space Shape Completion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper tackles the task of category-level pose estimation for garments.\nWith a near infinite degree of freedom, a garment's full configuration (i.e.,\nposes) is often described by the per-vertex 3D locations of its entire 3D\nsurface. However, garments are also commonly subject to extreme cases of\nself-occlusion, especially when folded or crumpled, making it challenging to\nperceive their full 3D surface. To address these challenges, we propose\nGarmentNets, where the key idea is to formulate the deformable object pose\nestimation problem as a shape completion task in the canonical space. This\ncanonical space is defined across garments instances within a category,\ntherefore, specifies the shared category-level pose. By mapping the observed\npartial surface to the canonical space and completing it in this space, the\noutput representation describes the garment's full configuration using a\ncomplete 3D mesh with the per-vertex canonical coordinate label. To properly\nhandle the thin 3D structure presented on garments, we proposed a novel 3D\nshape representation using the generalized winding number field. Experiments\ndemonstrate that GarmentNets is able to generalize to unseen garment instances\nand achieve significantly better performance compared to alternative\napproaches.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 03:18:00 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Chi", "Cheng", ""], ["Song", "Shuran", ""]]}, {"id": "2104.05215", "submitter": "Xiangde Luo", "authors": "Xiangde Luo, Tao Song, Guotai Wang, Jieneng Chen, Yinan Chen, Kang Li,\n  Dimitris N. Metaxas and Shaoting Zhang", "title": "SCPM-Net: An Anchor-free 3D Lung Nodule Detection Network using Sphere\n  Representation and Center Points Matching", "comments": "An extension of this paper\n  https://link.springer.com/chapter/10.1007/978-3-030-59725-2_53 (MICCAI2020\n  early accept), the first two authors contributed equally. Code:\n  https://github.com/HiLab-git/SCPM-Net", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Automatic and accurate lung nodule detection from 3D Computed Tomography\nscans plays a vital role in efficient lung cancer screening. Despite the\nstate-of-the-art performance obtained by recent anchor-based detectors using\nConvolutional Neural Networks, they require predetermined anchor parameters\nsuch as the size, number, and aspect ratio of anchors, and have limited\nrobustness when dealing with lung nodules with a massive variety of sizes. We\npropose a 3D sphere representation-based center-points matching detection\nnetwork (SCPM-Net) that is anchor-free and automatically predicts the position,\nradius, and offset of nodules without the manual design of nodule/anchor\nparameters. The SCPM-Net consists of two novel pillars: sphere representation\nand center points matching. To mimic the nodule annotation in clinical\npractice, we replace the conventional bounding box with the newly proposed\nbounding sphere. A compatible sphere-based intersection over-union loss\nfunction is introduced to train the lung nodule detection network stably and\nefficiently.We empower the network anchor-free by designing a positive\ncenter-points selection and matching (CPM) process, which naturally discards\npre-determined anchor boxes. An online hard example mining and re-focal loss\nsubsequently enable the CPM process more robust, resulting in more accurate\npoint assignment and the mitigation of class imbalance. In addition, to better\ncapture spatial information and 3D context for the detection, we propose to\nfuse multi-level spatial coordinate maps with the feature extractor and combine\nthem with 3D squeeze-and-excitation attention modules. Experimental results on\nthe LUNA16 dataset showed that our proposed SCPM-Net framework achieves\nsuperior performance compared with existing used anchor-based and anchor-free\nmethods for lung nodule detection.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 05:51:29 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Luo", "Xiangde", ""], ["Song", "Tao", ""], ["Wang", "Guotai", ""], ["Chen", "Jieneng", ""], ["Chen", "Yinan", ""], ["Li", "Kang", ""], ["Metaxas", "Dimitris N.", ""], ["Zhang", "Shaoting", ""]]}, {"id": "2104.05237", "submitter": "Zifan Shi", "authors": "Hao Ouyang, Zifan Shi, Chenyang Lei, Ka Lung Law and Qifeng Chen", "title": "Neural Camera Simulators", "comments": "Accepted to CVPR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a controllable camera simulator based on deep neural networks to\nsynthesize raw image data under different camera settings, including exposure\ntime, ISO, and aperture. The proposed simulator includes an exposure module\nthat utilizes the principle of modern lens designs for correcting the luminance\nlevel. It also contains a noise module using the noise level function and an\naperture module with adaptive attention to simulate the side effects on noise\nand defocus blur. To facilitate the learning of a simulator model, we collect a\ndataset of the 10,000 raw images of 450 scenes with different exposure\nsettings. Quantitative experiments and qualitative comparisons show that our\napproach outperforms relevant baselines in raw data synthesize on multiple\ncameras. Furthermore, the camera simulator enables various applications,\nincluding large-aperture enhancement, HDR, auto exposure, and data augmentation\nfor training local feature detectors. Our work represents the first attempt to\nsimulate a camera sensor's behavior leveraging both the advantage of\ntraditional raw sensor features and the power of data-driven deep learning.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 07:06:27 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Ouyang", "Hao", ""], ["Shi", "Zifan", ""], ["Lei", "Chenyang", ""], ["Law", "Ka Lung", ""], ["Chen", "Qifeng", ""]]}, {"id": "2104.05239", "submitter": "Chufeng Tang", "authors": "Chufeng Tang, Hang Chen, Xiao Li, Jianmin Li, Zhaoxiang Zhang, Xiaolin\n  Hu", "title": "Look Closer to Segment Better: Boundary Patch Refinement for Instance\n  Segmentation", "comments": "CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tremendous efforts have been made on instance segmentation but the mask\nquality is still not satisfactory. The boundaries of predicted instance masks\nare usually imprecise due to the low spatial resolution of feature maps and the\nimbalance problem caused by the extremely low proportion of boundary pixels. To\naddress these issues, we propose a conceptually simple yet effective\npost-processing refinement framework to improve the boundary quality based on\nthe results of any instance segmentation model, termed BPR. Following the idea\nof looking closer to segment boundaries better, we extract and refine a series\nof small boundary patches along the predicted instance boundaries. The\nrefinement is accomplished by a boundary patch refinement network at higher\nresolution. The proposed BPR framework yields significant improvements over the\nMask R-CNN baseline on Cityscapes benchmark, especially on the boundary-aware\nmetrics. Moreover, by applying the BPR framework to the PolyTransform + SegFix\nbaseline, we reached 1st place on the Cityscapes leaderboard.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 07:10:48 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Tang", "Chufeng", ""], ["Chen", "Hang", ""], ["Li", "Xiao", ""], ["Li", "Jianmin", ""], ["Zhang", "Zhaoxiang", ""], ["Hu", "Xiaolin", ""]]}, {"id": "2104.05248", "submitter": "Islam Nassar", "authors": "Islam Nassar, Samitha Herath, Ehsan Abbasnejad, Wray Buntine,\n  Gholamreza Haffari", "title": "All Labels Are Not Created Equal: Enhancing Semi-supervision via Label\n  Grouping and Co-training", "comments": "Accepted in CVPR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pseudo-labeling is a key component in semi-supervised learning (SSL). It\nrelies on iteratively using the model to generate artificial labels for the\nunlabeled data to train against. A common property among its various methods is\nthat they only rely on the model's prediction to make labeling decisions\nwithout considering any prior knowledge about the visual similarity among the\nclasses. In this paper, we demonstrate that this degrades the quality of\npseudo-labeling as it poorly represents visually similar classes in the pool of\npseudo-labeled data. We propose SemCo, a method which leverages label semantics\nand co-training to address this problem. We train two classifiers with two\ndifferent views of the class labels: one classifier uses the one-hot view of\nthe labels and disregards any potential similarity among the classes, while the\nother uses a distributed view of the labels and groups potentially similar\nclasses together. We then co-train the two classifiers to learn based on their\ndisagreements. We show that our method achieves state-of-the-art performance\nacross various SSL tasks including 5.6% accuracy improvement on Mini-ImageNet\ndataset with 1000 labeled examples. We also show that our method requires\nsmaller batch size and fewer training iterations to reach its best performance.\nWe make our code available at https://github.com/islam-nassar/semco.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 07:33:16 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Nassar", "Islam", ""], ["Herath", "Samitha", ""], ["Abbasnejad", "Ehsan", ""], ["Buntine", "Wray", ""], ["Haffari", "Gholamreza", ""]]}, {"id": "2104.05255", "submitter": "Marvin Klingner", "authors": "Marvin Klingner, Andreas B\\\"ar, Marcel Mross, Tim Fingscheidt", "title": "Improving Online Performance Prediction for Semantic Segmentation", "comments": "Accepted to CVPR SAIAD Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we address the task of observing the performance of a semantic\nsegmentation deep neural network (DNN) during online operation, i.e., during\ninference, which is of high importance in safety-critical applications such as\nautonomous driving. Here, many high-level decisions rely on such DNNs, which\nare usually evaluated offline, while their performance in online operation\nremains unknown. To solve this problem, we propose an improved online\nperformance prediction scheme, building on a recently proposed concept of\npredicting the primary semantic segmentation task's performance. This can be\nachieved by evaluating the auxiliary task of monocular depth estimation with a\nmeasurement supplied by a LiDAR sensor and a subsequent regression to the\nsemantic segmentation performance. In particular, we propose (i) sequential\ntraining methods for both tasks in a multi-task training setup, (ii) to share\nthe encoder as well as parts of the decoder between both task's networks for\nimproved efficiency, and (iii) a temporal statistics aggregation method, which\nsignificantly reduces the performance prediction error at the cost of a small\nalgorithmic latency. Evaluation on the KITTI dataset shows that all three\naspects improve the performance prediction compared to previous approaches.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 07:44:40 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Klingner", "Marvin", ""], ["B\u00e4r", "Andreas", ""], ["Mross", "Marcel", ""], ["Fingscheidt", "Tim", ""]]}, {"id": "2104.05261", "submitter": "Sebastian G\\\"undel", "authors": "Sebastian G\\\"undel, Arnaud A. A. Setio, Florin C. Ghesu, Sasa Grbic,\n  Bogdan Georgescu, Andreas Maier, Dorin Comaniciu", "title": "Robust Classification from Noisy Labels: Integrating Additional\n  Knowledge for Chest Radiography Abnormality Assessment", "comments": "Accepted in Medical Image Analysis (MedIA). arXiv admin note: text\n  overlap with arXiv:1905.06362", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Chest radiography is the most common radiographic examination performed in\ndaily clinical practice for the detection of various heart and lung\nabnormalities. The large amount of data to be read and reported, with more than\n100 studies per day for a single radiologist, poses a challenge in consistently\nmaintaining high interpretation accuracy. The introduction of large-scale\npublic datasets has led to a series of novel systems for automated abnormality\nclassification. However, the labels of these datasets were obtained using\nnatural language processed medical reports, yielding a large degree of label\nnoise that can impact the performance. In this study, we propose novel training\nstrategies that handle label noise from such suboptimal data. Prior label\nprobabilities were measured on a subset of training data re-read by 4\nboard-certified radiologists and were used during training to increase the\nrobustness of the training model to the label noise. Furthermore, we exploit\nthe high comorbidity of abnormalities observed in chest radiography and\nincorporate this information to further reduce the impact of label noise.\nAdditionally, anatomical knowledge is incorporated by training the system to\npredict lung and heart segmentation, as well as spatial knowledge labels. To\ndeal with multiple datasets and images derived from various scanners that apply\ndifferent post-processing techniques, we introduce a novel image normalization\nstrategy. Experiments were performed on an extensive collection of 297,541\nchest radiographs from 86,876 patients, leading to a state-of-the-art\nperformance level for 17 abnormalities from 2 datasets. With an average AUC\nscore of 0.880 across all abnormalities, our proposed training strategies can\nbe used to significantly improve performance scores.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 07:51:07 GMT"}, {"version": "v2", "created": "Wed, 14 Apr 2021 19:43:52 GMT"}, {"version": "v3", "created": "Wed, 21 Apr 2021 09:07:00 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["G\u00fcndel", "Sebastian", ""], ["Setio", "Arnaud A. A.", ""], ["Ghesu", "Florin C.", ""], ["Grbic", "Sasa", ""], ["Georgescu", "Bogdan", ""], ["Maier", "Andreas", ""], ["Comaniciu", "Dorin", ""]]}, {"id": "2104.05269", "submitter": "Changxing Ding", "authors": "Xubin Zhong, Xian Qu, Changxing Ding and Dacheng Tao", "title": "Glance and Gaze: Inferring Action-aware Points for One-Stage\n  Human-Object Interaction Detection", "comments": "Accepted to CVPR2021", "journal-ref": null, "doi": null, "report-no": "14 pages, 9 figures", "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern human-object interaction (HOI) detection approaches can be divided\ninto one-stage methods and twostage ones. One-stage models are more efficient\ndue to their straightforward architectures, but the two-stage models are still\nadvantageous in accuracy. Existing one-stage models usually begin by detecting\npredefined interaction areas or points, and then attend to these areas only for\ninteraction prediction; therefore, they lack reasoning steps that dynamically\nsearch for discriminative cues. In this paper, we propose a novel one-stage\nmethod, namely Glance and Gaze Network (GGNet), which adaptively models a set\nof actionaware points (ActPoints) via glance and gaze steps. The glance step\nquickly determines whether each pixel in the feature maps is an interaction\npoint. The gaze step leverages feature maps produced by the glance step to\nadaptively infer ActPoints around each pixel in a progressive manner. Features\nof the refined ActPoints are aggregated for interaction prediction. Moreover,\nwe design an actionaware approach that effectively matches each detected\ninteraction with its associated human-object pair, along with a novel hard\nnegative attentive loss to improve the optimization of GGNet. All the above\noperations are conducted simultaneously and efficiently for all pixels in the\nfeature maps. Finally, GGNet outperforms state-of-the-art methods by\nsignificant margins on both V-COCO and HICODET benchmarks. Code of GGNet is\navailable at https: //github.com/SherlockHolmes221/GGNet.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 08:01:04 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Zhong", "Xubin", ""], ["Qu", "Xian", ""], ["Ding", "Changxing", ""], ["Tao", "Dacheng", ""]]}, {"id": "2104.05279", "submitter": "Ahmet Iscen", "authors": "Ahmet Iscen, Andr\\'e Araujo, Boqing Gong, Cordelia Schmid", "title": "Class-Balanced Distillation for Long-Tailed Visual Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-world imagery is often characterized by a significant imbalance of the\nnumber of images per class, leading to long-tailed distributions. An effective\nand simple approach to long-tailed visual recognition is to learn feature\nrepresentations and a classifier separately, with instance and class-balanced\nsampling, respectively. In this work, we introduce a new framework, by making\nthe key observation that a feature representation learned with instance\nsampling is far from optimal in a long-tailed setting. Our main contribution is\na new training method, referred to as Class-Balanced Distillation (CBD), that\nleverages knowledge distillation to enhance feature representations. CBD allows\nthe feature representation to evolve in the second training stage, guided by\nthe teacher learned in the first stage. The second stage uses class-balanced\nsampling, in order to focus on under-represented classes. This framework can\nnaturally accommodate the usage of multiple teachers, unlocking the information\nfrom an ensemble of models to enhance recognition capabilities. Our experiments\nshow that the proposed technique consistently outperforms the state of the art\non long-tailed recognition benchmarks such as ImageNet-LT, iNaturalist17 and\niNaturalist18. The experiments also show that our method does not sacrifice the\naccuracy of head classes to improve the performance of tail classes, unlike\nmost existing work.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 08:21:03 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Iscen", "Ahmet", ""], ["Araujo", "Andr\u00e9", ""], ["Gong", "Boqing", ""], ["Schmid", "Cordelia", ""]]}, {"id": "2104.05282", "submitter": "David Reiser", "authors": "Jonas Straub, David Reiser and Hans W. Griepentrog", "title": "Approach for modeling single branches of meadow orchard trees with 3D\n  point clouds", "comments": null, "journal-ref": null, "doi": "10.3920/978-90-8686-916-9_88", "report-no": null, "categories": "cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The cultivation of orchard meadows provides an ecological benefit for\nbiodiversity, which is significantly higher than in intensively cultivated\norchards. The goal of this research is to create a tree model to automatically\ndetermine possible pruning points for stand-alone trees within meadows. The\nalgorithm which is presented here is capable of building a skeleton model based\non a pre-segmented photogrammetric 3D point cloud. Good results were achieved\nin assigning the points to their leading branches and building a virtual tree\nmodel, reaching an overall accuracy of 95.19 %. This model provided the\nnecessary information about the geometry of the tree for automated pruning.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 08:25:27 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Straub", "Jonas", ""], ["Reiser", "David", ""], ["Griepentrog", "Hans W.", ""]]}, {"id": "2104.05284", "submitter": "David Reiser", "authors": "Nils Lueling, David Reiser, Hans W. Griepentrog", "title": "Volume and leaf area calculation of cabbage with a neural network-based\n  instance segmentation", "comments": null, "journal-ref": null, "doi": "10.3920/978-90-8686-916-9_86", "report-no": null, "categories": "cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Fruit size and leaf area are important indicators for plant health and are of\ninterest for plant nutrient management, plant protection and harvest. In this\nresearch, an image-based method for measuring the fruit volume as well as the\nleaf area for cabbage is presented. For this purpose, a mask region-based\nconvolutional neural network (Mask R-CNN) was trained to segment the cabbage\nfruit from the leaves and assign it to the corresponding plant. The results\nindicated that even with a single camera, the developed method can provide a\ncalculation accuracy of fruit size of 92.6% and an accuracy of leaf area of\n89.8% on individual plant level.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 08:29:23 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Lueling", "Nils", ""], ["Reiser", "David", ""], ["Griepentrog", "Hans W.", ""]]}, {"id": "2104.05289", "submitter": "Yang Hong", "authors": "Yang Hong, Juyong Zhang, Boyi Jiang, Yudong Guo, Ligang Liu and Hujun\n  Bao", "title": "StereoPIFu: Depth Aware Clothed Human Digitization via Stereo Vision", "comments": "Accepted by CVPR2021. Project page:\n  http://crishy1995.github.io/StereoPIFuProject", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose StereoPIFu, which integrates the geometric\nconstraints of stereo vision with implicit function representation of PIFu, to\nrecover the 3D shape of the clothed human from a pair of low-cost rectified\nimages. First, we introduce the effective voxel-aligned features from a stereo\nvision-based network to enable depth-aware reconstruction. Moreover, the novel\nrelative z-offset is employed to associate predicted high-fidelity human depth\nand occupancy inference, which helps restore fine-level surface details.\nSecond, a network structure that fully utilizes the geometry information from\nthe stereo images is designed to improve the human body reconstruction quality.\nConsequently, our StereoPIFu can naturally infer the human body's spatial\nlocation in camera space and maintain the correct relative position of\ndifferent parts of the human body, which enables our method to capture human\nperformance. Compared with previous works, our StereoPIFu significantly\nimproves the robustness, completeness, and accuracy of the clothed human\nreconstruction, which is demonstrated by extensive experimental results.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 08:41:54 GMT"}, {"version": "v2", "created": "Tue, 13 Apr 2021 06:47:43 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Hong", "Yang", ""], ["Zhang", "Juyong", ""], ["Jiang", "Boyi", ""], ["Guo", "Yudong", ""], ["Liu", "Ligang", ""], ["Bao", "Hujun", ""]]}, {"id": "2104.05298", "submitter": "He Zhu", "authors": "He Zhu, Shan Yu", "title": "Intra-Class Uncertainty Loss Function for Classification", "comments": "Accepted to ICME 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most classification models can be considered as the process of matching\ntemplates. However, when intra-class uncertainty/variability is not considered,\nespecially for datasets containing unbalanced classes, this may lead to\nclassification errors. To address this issue, we propose a loss function with\nintra-class uncertainty following Gaussian distribution. Specifically, in our\nframework, the features extracted by deep networks of each class are\ncharacterized by independent Gaussian distribution. The parameters of\ndistribution are learned with a likelihood regularization along with other\nnetwork parameters. The means of the Gaussian play a similar role as the center\nanchor in existing methods, and the variance describes the uncertainty of\ndifferent classes. In addition, similar to the inter-class margin in\ntraditional loss functions, we introduce a margin to intra-class uncertainty to\nmake each cluster more compact and reduce the imbalance of feature distribution\nfrom different categories. Based on MNIST, CIFAR, ImageNet, and Long-tailed\nCIFAR analyses, the proposed approach shows improved classification\nperformance, through learning a better class representation.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 09:02:41 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Zhu", "He", ""], ["Yu", "Shan", ""]]}, {"id": "2104.05309", "submitter": "Kaicheng Yu", "authors": "Kaicheng Yu, Rene Ranftl, Mathieu Salzmann", "title": "Landmark Regularization: Ranking Guided Super-Net Training in Neural\n  Architecture Search", "comments": "Accepted to CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Weight sharing has become a de facto standard in neural architecture search\nbecause it enables the search to be done on commodity hardware. However, recent\nworks have empirically shown a ranking disorder between the performance of\nstand-alone architectures and that of the corresponding shared-weight networks.\nThis violates the main assumption of weight-sharing NAS algorithms, thus\nlimiting their effectiveness. We tackle this issue by proposing a\nregularization term that aims to maximize the correlation between the\nperformance rankings of the shared-weight network and that of the standalone\narchitectures using a small set of landmark architectures. We incorporate our\nregularization term into three different NAS algorithms and show that it\nconsistently improves performance across algorithms, search-spaces, and tasks.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 09:32:33 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Yu", "Kaicheng", ""], ["Ranftl", "Rene", ""], ["Salzmann", "Mathieu", ""]]}, {"id": "2104.05326", "submitter": "Vladyslav Andriiashen", "authors": "Vladyslav Andriiashen, Robert van Liere, Tristan van Leeuwen, Kees\n  Joost Batenburg", "title": "Unsupervised foreign object detection based on dual-energy\n  absorptiometry in the food industry", "comments": "This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  X-ray imaging is a widely used technique for non-destructive inspection of\nagricultural food products. One application of X-ray imaging is the autonomous,\nin-line detection of foreign objects in food samples. Examples of such\ninclusions are bone fragments in meat products, plastic and metal debris in\nfish, fruit infestations. This article presents a processing methodology for\nunsupervised foreign object detection based on dual-energy X-ray absorptiometry\n(DEXA). A foreign object is defined as a fragment of material with different\nX-ray attenuation properties than those belonging to the food product. A novel\nthickness correction model is introduced as a pre-processing technique for DEXA\ndata. The aim of the model is to homogenize regions in the image that belong to\nthe food product and enhance contrast where the foreign object is present. In\nthis way, the segmentation of the foreign object is more robust to noise and\nlack of contrast. The proposed methodology was applied to a dataset of 488\nsamples of meat products. The samples were acquired from a conveyor belt in a\nfood processing factory. Approximately 60\\% of the samples contain foreign\nobjects of different types and sizes, while the rest of the samples are void of\nforeign objects. The results show that samples without foreign objects are\ncorrectly identified in 97% of cases, the overall accuracy of foreign object\ndetection reaches 95%.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 10:15:15 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Andriiashen", "Vladyslav", ""], ["van Liere", "Robert", ""], ["van Leeuwen", "Tristan", ""], ["Batenburg", "Kees Joost", ""]]}, {"id": "2104.05327", "submitter": "Jacek Komorowski", "authors": "Jacek Komorowski, Monika Wysoczanska, Tomasz Trzcinski", "title": "MinkLoc++: Lidar and Monocular Image Fusion for Place Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce a discriminative multimodal descriptor based on a pair of sensor\nreadings: a point cloud from a LiDAR and an image from an RGB camera. Our\ndescriptor, named MinkLoc++, can be used for place recognition, re-localization\nand loop closure purposes in robotics or autonomous vehicles applications. We\nuse late fusion approach, where each modality is processed separately and fused\nin the final part of the processing pipeline. The proposed method achieves\nstate-of-the-art performance on standard place recognition benchmarks. We also\nidentify dominating modality problem when training a multimodal descriptor. The\nproblem manifests itself when the network focuses on a modality with a larger\noverfit to the training data. This drives the loss down during the training but\nleads to suboptimal performance on the evaluation set. In this work we describe\nhow to detect and mitigate such risk when using a deep metric learning approach\nto train a multimodal neural network. Our code is publicly available on the\nproject website: https://github.com/jac99/MinkLocMultimodal.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 10:16:08 GMT"}, {"version": "v2", "created": "Wed, 14 Apr 2021 10:02:05 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Komorowski", "Jacek", ""], ["Wysoczanska", "Monika", ""], ["Trzcinski", "Tomasz", ""]]}, {"id": "2104.05328", "submitter": "Sk Aziz Ali", "authors": "Sk Aziz Ali, Kerem Kahraman, Gerd Reis, Didier Stricker", "title": "RPSRNet: End-to-End Trainable Rigid Point Set Registration Network using\n  Barnes-Hut $2^D$-Tree Representation", "comments": "Computer Vision and Pattern Recognition (CVPR) 2021, (*Accepted)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose RPSRNet - a novel end-to-end trainable deep neural network for\nrigid point set registration. For this task, we use a novel $2^D$-tree\nrepresentation for the input point sets and a hierarchical deep feature\nembedding in the neural network. An iterative transformation refinement module\nin our network boosts the feature matching accuracy in the intermediate stages.\nWe achieve an inference speed of 12-15ms to register a pair of input point\nclouds as large as 250K. Extensive evaluation on (i) KITTI LiDAR odometry and\n(ii) ModelNet-40 datasets shows that our method outperforms prior\nstate-of-the-art methods - e.g., on the KITTI data set, DCP-v2 by1.3 and 1.5\ntimes, and PointNetLK by 1.8 and 1.9 times better rotational and translational\naccuracy respectively. Evaluation on ModelNet40 shows that RPSRNet is more\nrobust than other benchmark methods when the samples contain a significant\namount of noise and other disturbances. RPSRNet accurately registers point\nclouds with non-uniform sampling densities, e.g., LiDAR data, which cannot be\nprocessed by many existing deep-learning-based registration methods.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 10:16:09 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Ali", "Sk Aziz", ""], ["Kahraman", "Kerem", ""], ["Reis", "Gerd", ""], ["Stricker", "Didier", ""]]}, {"id": "2104.05344", "submitter": "Mateusz Ochal", "authors": "Mateusz Ochal, Massimiliano Patacchiola, Amos Storkey, Jose Vazquez,\n  Sen Wang", "title": "How Sensitive are Meta-Learners to Dataset Imbalance?", "comments": "Published as a workshop paper at the Learning to Learn workshop at\n  ICLR 2021. arXiv admin note: text overlap with arXiv:2101.02523", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Meta-Learning (ML) has proven to be a useful tool for training Few-Shot\nLearning (FSL) algorithms by exposure to batches of tasks sampled from a\nmeta-dataset. However, the standard training procedure overlooks the dynamic\nnature of the real-world where object classes are likely to occur at different\nfrequencies. While it is generally understood that imbalanced tasks harm the\nperformance of supervised methods, there is no significant research examining\nthe impact of imbalanced meta-datasets on the FSL evaluation task. This study\nexposes the magnitude and extent of this problem. Our results show that ML\nmethods are more robust against meta-dataset imbalance than imbalance at the\ntask-level with a similar imbalance ratio ($\\rho<20$), with the effect holding\neven in long-tail datasets under a larger imbalance ($\\rho=65$). Overall, these\nresults highlight an implicit strength of ML algorithms, capable of learning\ngeneralizable features under dataset imbalance and domain-shift. The code to\nreproduce the experiments is released under an open-source license.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 10:47:42 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Ochal", "Mateusz", ""], ["Patacchiola", "Massimiliano", ""], ["Storkey", "Amos", ""], ["Vazquez", "Jose", ""], ["Wang", "Sen", ""]]}, {"id": "2104.05345", "submitter": "Chunmei Feng", "authors": "Chun-Mei Feng, Zhanyuan Yang, Geng Chen, Yong Xu, Ling Shao", "title": "Dual-Octave Convolution for Accelerated Parallel MR Image Reconstruction", "comments": "Proceedings of the 35th AAAI Conference on Artificial Intelligence\n  (AAAI) 2021", "journal-ref": "Proceedings of the 35th AAAI Conference on Artificial Intelligence\n  (AAAI) 2021", "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Magnetic resonance (MR) image acquisition is an inherently prolonged process,\nwhose acceleration by obtaining multiple undersampled images simultaneously\nthrough parallel imaging has always been the subject of research. In this\npaper, we propose the Dual-Octave Convolution (Dual-OctConv), which is capable\nof learning multi-scale spatial-frequency features from both real and imaginary\ncomponents, for fast parallel MR image reconstruction. By reformulating the\ncomplex operations using octave convolutions, our model shows a strong ability\nto capture richer representations of MR images, while at the same time greatly\nreducing the spatial redundancy. More specifically, the input feature maps and\nconvolutional kernels are first split into two components (i.e., real and\nimaginary), which are then divided into four groups according to their spatial\nfrequencies. Then, our Dual-OctConv conducts intra-group information updating\nand inter-group information exchange to aggregate the contextual information\nacross different groups. Our framework provides two appealing benefits: (i) it\nencourages interactions between real and imaginary components at various\nspatial frequencies to achieve richer representational capacity, and (ii) it\nenlarges the receptive field by learning multiple spatial-frequency features of\nboth the real and imaginary components. We evaluate the performance of the\nproposed model on the acceleration of multi-coil MR image reconstruction.\nExtensive experiments are conducted on an {in vivo} knee dataset under\ndifferent undersampling patterns and acceleration factors. The experimental\nresults demonstrate the superiority of our model in accelerated parallel MR\nimage reconstruction. Our code is available at:\ngithub.com/chunmeifeng/Dual-OctConv.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 10:51:05 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Feng", "Chun-Mei", ""], ["Yang", "Zhanyuan", ""], ["Chen", "Geng", ""], ["Xu", "Yong", ""], ["Shao", "Ling", ""]]}, {"id": "2104.05358", "submitter": "Hiroshi Sasaki", "authors": "Hiroshi Sasaki, Chris G. Willcocks, Toby P. Breckon", "title": "UNIT-DDPM: UNpaired Image Translation with Denoising Diffusion\n  Probabilistic Models", "comments": "10 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel unpaired image-to-image translation method that uses\ndenoising diffusion probabilistic models without requiring adversarial\ntraining. Our method, UNpaired Image Translation with Denoising Diffusion\nProbabilistic Models (UNIT-DDPM), trains a generative model to infer the joint\ndistribution of images over both domains as a Markov chain by minimising a\ndenoising score matching objective conditioned on the other domain. In\nparticular, we update both domain translation models simultaneously, and we\ngenerate target domain images by a denoising Markov Chain Monte Carlo approach\nthat is conditioned on the input source domain images, based on Langevin\ndynamics. Our approach provides stable model training for image-to-image\ntranslation and generates high-quality image outputs. This enables\nstate-of-the-art Fr\\'echet Inception Distance (FID) performance on several\npublic datasets, including both colour and multispectral imagery, significantly\noutperforming the contemporary adversarial image-to-image translation methods.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 11:22:56 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Sasaki", "Hiroshi", ""], ["Willcocks", "Chris G.", ""], ["Breckon", "Toby P.", ""]]}, {"id": "2104.05367", "submitter": "Chuanxia Zheng", "authors": "Chuanxia Zheng, Duy-Son Dao, Guoxian Song, Tat-Jen Cham, Jianfei Cai", "title": "Visiting the Invisible: Layer-by-Layer Completed Scene Decomposition", "comments": "20 pages, 16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Existing scene understanding systems mainly focus on recognizing the visible\nparts of a scene, ignoring the intact appearance of physical objects in the\nreal-world. Concurrently, image completion has aimed to create plausible\nappearance for the invisible regions, but requires a manual mask as input. In\nthis work, we propose a higher-level scene understanding system to tackle both\nvisible and invisible parts of objects and backgrounds in a given scene.\nParticularly, we built a system to decompose a scene into individual objects,\ninfer their underlying occlusion relationships, and even automatically learn\nwhich parts of the objects are occluded that need to be completed. In order to\ndisentangle the occluded relationships of all objects in a complex scene, we\nuse the fact that the front object without being occluded is easy to be\nidentified, detected, and segmented. Our system interleaves the two tasks of\ninstance segmentation and scene completion through multiple iterations, solving\nfor objects layer-by-layer. We first provide a thorough experiment using a new\nrealistically rendered dataset with ground-truths for all invisible regions. To\nbridge the domain gap to real imagery where ground-truths are unavailable, we\nthen train another model with the pseudo-ground-truths generated from our\ntrained synthesis model. We demonstrate results on a wide variety of datasets\nand show significant improvement over the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 11:37:23 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Zheng", "Chuanxia", ""], ["Dao", "Duy-Son", ""], ["Song", "Guoxian", ""], ["Cham", "Tat-Jen", ""], ["Cai", "Jianfei", ""]]}, {"id": "2104.05374", "submitter": "Hongbin Xu", "authors": "Hongbin Xu, Zhipeng Zhou, Yu Qiao, Wenxiong Kang, Qiuxia Wu", "title": "Self-supervised Multi-view Stereo via Effective Co-Segmentation and\n  Data-Augmentation", "comments": "This paper is accepted by AAAI-21 with a Distinguished Paper Award", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies have witnessed that self-supervised methods based on view\nsynthesis obtain clear progress on multi-view stereo (MVS). However, existing\nmethods rely on the assumption that the corresponding points among different\nviews share the same color, which may not always be true in practice. This may\nlead to unreliable self-supervised signal and harm the final reconstruction\nperformance. To address the issue, we propose a framework integrated with more\nreliable supervision guided by semantic co-segmentation and data-augmentation.\nSpecially, we excavate mutual semantic from multi-view images to guide the\nsemantic consistency. And we devise effective data-augmentation mechanism which\nensures the transformation robustness by treating the prediction of regular\nsamples as pseudo ground truth to regularize the prediction of augmented\nsamples. Experimental results on DTU dataset show that our proposed methods\nachieve the state-of-the-art performance among unsupervised methods, and even\ncompete on par with supervised methods. Furthermore, extensive experiments on\nTanks&Temples dataset demonstrate the effective generalization ability of the\nproposed method.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 11:48:54 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Xu", "Hongbin", ""], ["Zhou", "Zhipeng", ""], ["Qiao", "Yu", ""], ["Kang", "Wenxiong", ""], ["Wu", "Qiuxia", ""]]}, {"id": "2104.05376", "submitter": "Tianwei Lin", "authors": "Tianwei Lin, Zhuoqi Ma, Fu Li, Dongliang He, Xin Li, Errui Ding,\n  Nannan Wang, Jie Li, Xinbo Gao", "title": "Drafting and Revision: Laplacian Pyramid Network for Fast High-Quality\n  Artistic Style Transfer", "comments": "Accepted by CVPR 2021. Codes will be released soon on\n  https://github.com/PaddlePaddle/PaddleGAN/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artistic style transfer aims at migrating the style from an example image to\na content image. Currently, optimization-based methods have achieved great\nstylization quality, but expensive time cost restricts their practical\napplications. Meanwhile, feed-forward methods still fail to synthesize complex\nstyle, especially when holistic global and local patterns exist. Inspired by\nthe common painting process of drawing a draft and revising the details, we\nintroduce a novel feed-forward method named Laplacian Pyramid Network\n(LapStyle). LapStyle first transfers global style patterns in low-resolution\nvia a Drafting Network. It then revises the local details in high-resolution\nvia a Revision Network, which hallucinates a residual image according to the\ndraft and the image textures extracted by Laplacian filtering. Higher\nresolution details can be easily generated by stacking Revision Networks with\nmultiple Laplacian pyramid levels. The final stylized image is obtained by\naggregating outputs of all pyramid levels. %We also introduce a patch\ndiscriminator to better learn local patterns adversarially. Experiments\ndemonstrate that our method can synthesize high quality stylized images in real\ntime, where holistic style patterns are properly transferred.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 11:53:53 GMT"}, {"version": "v2", "created": "Sun, 18 Apr 2021 01:09:09 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Lin", "Tianwei", ""], ["Ma", "Zhuoqi", ""], ["Li", "Fu", ""], ["He", "Dongliang", ""], ["Li", "Xin", ""], ["Ding", "Errui", ""], ["Wang", "Nannan", ""], ["Li", "Jie", ""], ["Gao", "Xinbo", ""]]}, {"id": "2104.05382", "submitter": "Haoran Zhao", "authors": "Haoran Zhao, Xin Sun, Junyu Dong, Hui Yu and Huiyu Zhou", "title": "Dual Discriminator Adversarial Distillation for Data-free Model\n  Compression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge distillation has been widely used to produce portable and efficient\nneural networks which can be well applied on edge devices for computer vision\ntasks. However, almost all top-performing knowledge distillation methods need\nto access the original training data, which usually has a huge size and is\noften unavailable. To tackle this problem, we propose a novel data-free\napproach in this paper, named Dual Discriminator Adversarial Distillation\n(DDAD) to distill a neural network without any training data or meta-data. To\nbe specific, we use a generator to create samples through dual discriminator\nadversarial distillation, which mimics the original training data. The\ngenerator not only uses the pre-trained teacher's intrinsic statistics in\nexisting batch normalization layers but also obtains the maximum discrepancy\nfrom the student model. Then the generated samples are used to train the\ncompact student network under the supervision of the teacher. The proposed\nmethod obtains an efficient student network which closely approximates its\nteacher network, despite using no original training data. Extensive experiments\nare conducted to to demonstrate the effectiveness of the proposed approach on\nCIFAR-10, CIFAR-100 and Caltech101 datasets for classification tasks. Moreover,\nwe extend our method to semantic segmentation tasks on several public datasets\nsuch as CamVid and NYUv2. All experiments show that our method outperforms all\nbaselines for data-free knowledge distillation.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 12:01:45 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Zhao", "Haoran", ""], ["Sun", "Xin", ""], ["Dong", "Junyu", ""], ["Yu", "Hui", ""], ["Zhou", "Huiyu", ""]]}, {"id": "2104.05418", "submitter": "Shuang Ma", "authors": "Shuang Ma, Zhaoyang Zeng, Daniel McDuff, Yale Song", "title": "Contrastive Learning of Global and Local Audio-Visual Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.SD eess.AS eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contrastive learning has delivered impressive results in many audio-visual\nrepresentation learning scenarios. However, existing approaches optimize for\nlearning either \\textit{global} representations useful for tasks such as\nclassification, or \\textit{local} representations useful for tasks such as\naudio-visual source localization and separation. While they produce\nsatisfactory results in their intended downstream scenarios, they often fail to\ngeneralize to tasks that they were not originally designed for. In this work,\nwe propose a versatile self-supervised approach to learn audio-visual\nrepresentations that generalize to both the tasks which require global semantic\ninformation (e.g., classification) and the tasks that require fine-grained\nspatio-temporal information (e.g. localization). We achieve this by optimizing\ntwo cross-modal contrastive objectives that together encourage our model to\nlearn discriminative global-local visual information given audio signals. To\nshow that our approach learns generalizable video representations, we evaluate\nit on various downstream scenarios including action/sound classification, lip\nreading, deepfake detection, and sound source localization.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 07:35:08 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Ma", "Shuang", ""], ["Zeng", "Zhaoyang", ""], ["McDuff", "Daniel", ""], ["Song", "Yale", ""]]}, {"id": "2104.05430", "submitter": "Sebastian Grans", "authors": "Sebastian Grans and Lars Tingelstad", "title": "Blazer: Laser Scanning Simulation using Physically Based Rendering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Line laser scanners are a sub-type of structured light 3D scanners that are\nrelatively common devices to find within the industrial setting, typically in\nthe context of assembly, process control, and welding. Despite its extensive\nuse, scanning of some materials remain a difficult or even impossible task\nwithout additional pre-processing. For instance, materials which are shiny, or\ntransparent. In this paper, we present a Blazer, a virtual line laser scanner\nthat, combined with physically based rendering, produces synthetic data with a\nrealistic light-matter interaction, and hence realistic appearance. This makes\nit eligible for the use as a tool in the development of novel algorithms, and\nin particular as a source of synthetic data for training of machine learning\nmodels. Similar systems exist for synthetic RGB-D data generation, but to our\nknowledge this the first publicly available implementation for synthetic line\nlaser data. We release this implementation under an open-source license to aid\nfurther research on line laser scanners.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 12:56:52 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Grans", "Sebastian", ""], ["Tingelstad", "Lars", ""]]}, {"id": "2104.05442", "submitter": "Sudipan Saha", "authors": "Jakob Gawlikowski, Sudipan Saha, Anna Kruspe, Xiao Xiang Zhu", "title": "Out-of-distribution detection in satellite image classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In satellite image analysis, distributional mismatch between the training and\ntest data may arise due to several reasons, including unseen classes in the\ntest data and differences in the geographic area. Deep learning based models\nmay behave in unexpected manner when subjected to test data that has such\ndistributional shifts from the training data, also called out-of-distribution\n(OOD) examples. Predictive uncertainly analysis is an emerging research topic\nwhich has not been explored much in context of satellite image analysis.\nTowards this, we adopt a Dirichlet Prior Network based model to quantify\ndistributional uncertainty of deep learning models for remote sensing. The\napproach seeks to maximize the representation gap between the in-domain and OOD\nexamples for a better identification of unknown examples at test time.\nExperimental results on three exemplary test scenarios show the efficacy of the\nmodel in satellite image analysis.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 11:11:52 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Gawlikowski", "Jakob", ""], ["Saha", "Sudipan", ""], ["Kruspe", "Anna", ""], ["Zhu", "Xiao Xiang", ""]]}, {"id": "2104.05443", "submitter": "Sudipan Saha", "authors": "Sudipan Saha, Biplab Banerjee, Xiao Xiang Zhu", "title": "Trusting small training dataset for supervised change detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning (DL) based supervised change detection (CD) models require\nlarge labeled training data. Due to the difficulty of collecting labeled\nmulti-temporal data, unsupervised methods are preferred in the CD literature.\nHowever, unsupervised methods cannot fully exploit the potentials of\ndata-driven deep learning and thus they are not absolute alternative to the\nsupervised methods. This motivates us to look deeper into the supervised DL\nmethods and investigate how they can be adopted intelligently for CD by\nminimizing the requirement of labeled training data. Towards this, in this work\nwe show that geographically diverse training dataset can yield significant\nimprovement over less diverse training datasets of the same size. We propose a\nsimple confidence indicator for verifying the trustworthiness/confidence of\nsupervised models trained with small labeled dataset. Moreover, we show that\nfor the test cases where supervised CD model is found to be less\nconfident/trustworthy, unsupervised methods often produce better result than\nthe supervised ones.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 10:57:03 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Saha", "Sudipan", ""], ["Banerjee", "Biplab", ""], ["Zhu", "Xiao Xiang", ""]]}, {"id": "2104.05450", "submitter": "Thibaud Brochet", "authors": "Thibaud Brochet, Jerome Lapuyade-Lahorgue, Sebastien Bougleux, Mathieu\n  Salaun, Su Ruan", "title": "Deep learning using Havrda-Charvat entropy for classification of\n  pulmonary endomicroscopy", "comments": "8 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pulmonary optical endomicroscopy (POE) is an imaging technology in real time.\nIt allows to examine pulmonary alveoli at a microscopic level. Acquired in\nclinical settings, a POE image sequence can have as much as 25% of the sequence\nbeing uninformative frames (i.e. pure-noise and motion artefacts). For future\ndata analysis, these uninformative frames must be first removed from the\nsequence. Therefore, the objective of our work is to develop an automatic\ndetection method of uninformative images in endomicroscopy images. We propose\nto take the detection problem as a classification one. Considering advantages\nof deep learning methods, a classifier based on CNN (Convolutional Neural\nNetwork) is designed with a new loss function based on Havrda-Charvat entropy\nwhich is a parametrical generalization of the Shannon entropy. We propose to\nuse this formula to get a better hold on all sorts of data since it provides a\nmodel more stable than the Shannon entropy. Our method is tested on one POE\ndataset including 2947 distinct images, is showing better results than using\nShannon entropy and behaves better with regard to the problem of overfitting.\n  Keywords: Deep Learning, CNN, Shannon entropy, Havrda-Charvat entropy,\nPulmonary optical endomicroscopy.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 13:16:04 GMT"}, {"version": "v2", "created": "Tue, 13 Apr 2021 14:50:52 GMT"}, {"version": "v3", "created": "Mon, 19 Apr 2021 09:53:20 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Brochet", "Thibaud", ""], ["Lapuyade-Lahorgue", "Jerome", ""], ["Bougleux", "Sebastien", ""], ["Salaun", "Mathieu", ""], ["Ruan", "Su", ""]]}, {"id": "2104.05458", "submitter": "Fei Qi", "authors": "Pengfei Wang, Chengquan Zhang, Fei Qi, Shanshan Liu, Xiaoqiang Zhang,\n  Pengyuan Lyu, Junyu Han, Jingtuo Liu, Errui Ding, Guangming Shi", "title": "PGNet: Real-time Arbitrarily-Shaped Text Spotting with Point Gathering\n  Network", "comments": "10 pages, 8 figures, AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The reading of arbitrarily-shaped text has received increasing research\nattention. However, existing text spotters are mostly built on two-stage\nframeworks or character-based methods, which suffer from either Non-Maximum\nSuppression (NMS), Region-of-Interest (RoI) operations, or character-level\nannotations. In this paper, to address the above problems, we propose a novel\nfully convolutional Point Gathering Network (PGNet) for reading\narbitrarily-shaped text in real-time. The PGNet is a single-shot text spotter,\nwhere the pixel-level character classification map is learned with proposed\nPG-CTC loss avoiding the usage of character-level annotations. With a PG-CTC\ndecoder, we gather high-level character classification vectors from\ntwo-dimensional space and decode them into text symbols without NMS and RoI\noperations involved, which guarantees high efficiency. Additionally, reasoning\nthe relations between each character and its neighbors, a graph refinement\nmodule (GRM) is proposed to optimize the coarse recognition and improve the\nend-to-end performance. Experiments prove that the proposed method achieves\ncompetitive accuracy, meanwhile significantly improving the running speed. In\nparticular, in Total-Text, it runs at 46.7 FPS, surpassing the previous\nspotters with a large margin.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 13:27:34 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Wang", "Pengfei", ""], ["Zhang", "Chengquan", ""], ["Qi", "Fei", ""], ["Liu", "Shanshan", ""], ["Zhang", "Xiaoqiang", ""], ["Lyu", "Pengyuan", ""], ["Han", "Junyu", ""], ["Liu", "Jingtuo", ""], ["Ding", "Errui", ""], ["Shi", "Guangming", ""]]}, {"id": "2104.05482", "submitter": "Hichem Sahbi", "authors": "Hichem Sahbi", "title": "Learning Chebyshev Basis in Graph Convolutional Networks for\n  Skeleton-based Action Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spectral graph convolutional networks (GCNs) are particular deep models which\naim at extending neural networks to arbitrary irregular domains. The principle\nof these networks consists in projecting graph signals using the\neigen-decomposition of their Laplacians, then achieving filtering in the\nspectral domain prior to back-project the resulting filtered signals onto the\ninput graph domain. However, the success of these operations is highly\ndependent on the relevance of the used Laplacians which are mostly handcrafted\nand this makes GCNs clearly sub-optimal. In this paper, we introduce a novel\nspectral GCN that learns not only the usual convolutional parameters but also\nthe Laplacian operators. The latter are designed \"end-to-end\" as a part of a\nrecursive Chebyshev decomposition with the particularity of conveying both the\ndifferential and the non-differential properties of the learned representations\n-- with increasing order and discrimination power -- without overparametrizing\nthe trained GCNs. Extensive experiments, conducted on the challenging task of\nskeleton-based action recognition, show the generalization ability and the\noutperformance of our proposed Laplacian design w.r.t. different baselines\n(built upon handcrafted and other learned Laplacians) as well as the related\nwork.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 14:08:58 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Sahbi", "Hichem", ""]]}, {"id": "2104.05485", "submitter": "Dongfang Yang", "authors": "Dongfang Yang, Haolin Zhang, Ekim Yurtsever, Keith Redmill, \\\"Umit\n  \\\"Ozg\\\"uner", "title": "Predicting Pedestrian Crossing Intention with Feature Fusion and\n  Spatio-Temporal Attention", "comments": "Submitted to 2021 IEEE/RSJ International Conference on Intelligent\n  Robots and Systems (IROS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting vulnerable road user behavior is an essential prerequisite for\ndeploying Automated Driving Systems (ADS) in the real-world. Pedestrian\ncrossing intention should be recognized in real-time, especially for urban\ndriving. Recent works have shown the potential of using vision-based deep\nneural network models for this task. However, these models are not robust and\ncertain issues still need to be resolved. First, the global spatio-temproal\ncontext that accounts for the interaction between the target pedestrian and the\nscene has not been properly utilized. Second, the optimum strategy for fusing\ndifferent sensor data has not been thoroughly investigated. This work addresses\nthe above limitations by introducing a novel neural network architecture to\nfuse inherently different spatio-temporal features for pedestrian crossing\nintention prediction. We fuse different phenomena such as sequences of RGB\nimagery, semantic segmentation masks, and ego-vehicle speed in an optimum way\nusing attention mechanisms and a stack of recurrent neural networks. The\noptimum architecture was obtained through exhaustive ablation and comparison\nstudies. Extensive comparative experiments on the JAAD pedestrian action\nprediction benchmark demonstrate the effectiveness of the proposed method,\nwhere state-of-the-art performance was achieved. Our code is open-source and\npublicly available.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 14:10:25 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Yang", "Dongfang", ""], ["Zhang", "Haolin", ""], ["Yurtsever", "Ekim", ""], ["Redmill", "Keith", ""], ["\u00d6zg\u00fcner", "\u00dcmit", ""]]}, {"id": "2104.05495", "submitter": "Mikael Brudfors", "authors": "Mikael Brudfors, Ya\\\"el Balbastre, John Ashburner, Geraint Rees,\n  Parashkev Nachev, S\\'ebastien Ourselin, M. Jorge Cardoso", "title": "An MRF-UNet Product of Experts for Image Segmentation", "comments": "Accepted at MIDL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  While convolutional neural networks (CNNs) trained by back-propagation have\nseen unprecedented success at semantic segmentation tasks, they are known to\nstruggle on out-of-distribution data. Markov random fields (MRFs) on the other\nhand, encode simpler distributions over labels that, although less flexible\nthan UNets, are less prone to over-fitting. In this paper, we propose to fuse\nboth strategies by computing the product of distributions of a UNet and an MRF.\nAs this product is intractable, we solve for an approximate distribution using\nan iterative mean-field approach. The resulting MRF-UNet is trained jointly by\nback-propagation. Compared to other works using conditional random fields\n(CRFs), the MRF has no dependency on the imaging data, which should allow for\nless over-fitting. We show on 3D neuroimaging data that this novel network\nimproves generalisation to out-of-distribution samples. Furthermore, it allows\nthe overall number of parameters to be reduced while preserving high accuracy.\nThese results suggest that a classic MRF smoothness prior can allow for less\nover-fitting when principally integrated into a CNN model. Our implementation\nis available at https://github.com/balbasty/nitorch.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 14:25:32 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Brudfors", "Mikael", ""], ["Balbastre", "Ya\u00ebl", ""], ["Ashburner", "John", ""], ["Rees", "Geraint", ""], ["Nachev", "Parashkev", ""], ["Ourselin", "S\u00e9bastien", ""], ["Cardoso", "M. Jorge", ""]]}, {"id": "2104.05518", "submitter": "Jeffrey Wen", "authors": "Jeffrey Wen, Fabian Benitez-Quiroz, Qianli Feng, Aleix Martinez", "title": "Diamond in the rough: Improving image realism by traversing the GAN\n  latent space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In just a few years, the photo-realism of images synthesized by Generative\nAdversarial Networks (GANs) has gone from somewhat reasonable to almost perfect\nlargely by increasing the complexity of the networks, e.g., adding layers,\nintermediate latent spaces, style-transfer parameters, etc. This trajectory has\nled many of the state-of-the-art GANs to be inaccessibly large, disengaging\nmany without large computational resources. Recognizing this, we explore a\nmethod for squeezing additional performance from existing, low-complexity GANs.\nFormally, we present an unsupervised method to find a direction in the latent\nspace that aligns with improved photo-realism. Our approach leaves the network\nunchanged while enhancing the fidelity of the generated image. We use a simple\ngenerator inversion to find the direction in the latent space that results in\nthe smallest change in the image space. Leveraging the learned structure of the\nlatent space, we find moving in this direction corrects many image artifacts\nand brings the image into greater realism. We verify our findings qualitatively\nand quantitatively, showing an improvement in Frechet Inception Distance (FID)\nexists along our trajectory which surpasses the original GAN and other\napproaches including a supervised method. We expand further and provide an\noptimization method to automatically select latent vectors along the path that\nbalance the variation and realism of samples. We apply our method to several\ndiverse datasets and three architectures of varying complexity to illustrate\nthe generalizability of our approach. By expanding the utility of\nlow-complexity and existing networks, we hope to encourage the democratization\nof GANs.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 14:45:29 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Wen", "Jeffrey", ""], ["Benitez-Quiroz", "Fabian", ""], ["Feng", "Qianli", ""], ["Martinez", "Aleix", ""]]}, {"id": "2104.05519", "submitter": "Bin Ren", "authors": "Bin Ren, Hao Tang, Fanyang Meng, Runwei Ding, Ling Shao, Philip H.S.\n  Torr, Nicu Sebe", "title": "Cloth Interactive Transformer for Virtual Try-On", "comments": "11 pages, 6 figures,", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  2D image-based virtual try-on has attracted increased attention from the\nmultimedia and computer vision communities. However, most of the existing\nimage-based virtual try-on methods directly put both person and the in-shop\nclothing representations together, without considering the mutual correlation\nbetween them. What is more, the long-range information, which is crucial for\ngenerating globally consistent results, is also hard to be established via the\nregular convolution operation. To alleviate these two problems, in this paper\nwe propose a novel two-stage Cloth Interactive Transformer (CIT) for virtual\ntry-on. In the first stage, we design a CIT matching block, aiming to perform a\nlearnable thin-plate spline transformation that can capture more reasonable\nlong-range relation. As a result, the warped in-shop clothing looks more\nnatural. In the second stage, we propose a novel CIT reasoning block for\nestablishing the global mutual interactive dependence. Based on this mutual\ndependence, the significant region within the input data can be highlighted,\nand consequently, the try-on results can become more realistic. Extensive\nexperiments on a public fashion dataset demonstrate that our CIT can achieve\nthe new state-of-the-art virtual try-on performance both qualitatively and\nquantitatively. The source code and trained models are available at\nhttps://github.com/Amazingren/CIT.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 14:45:32 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Ren", "Bin", ""], ["Tang", "Hao", ""], ["Meng", "Fanyang", ""], ["Ding", "Runwei", ""], ["Shao", "Ling", ""], ["Torr", "Philip H. S.", ""], ["Sebe", "Nicu", ""]]}, {"id": "2104.05527", "submitter": "An Zhang", "authors": "An Zhang, Xiang Wang, Chengfang Fang, Jie Shi, Tat-seng Chua, Zehua\n  Chen", "title": "A-FMI: Learning Attributions from Deep Networks via Feature Map\n  Importance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Gradient-based attribution methods can aid in the understanding of\nconvolutional neural networks (CNNs). However, the redundancy of attribution\nfeatures and the gradient saturation problem, which weaken the ability to\nidentify significant features and cause an explanation focus shift, are\nchallenges that attribution methods still face. In this work, we propose: 1) an\nessential characteristic, Strong Relevance, when selecting attribution\nfeatures; 2) a new concept, feature map importance (FMI), to refine the\ncontribution of each feature map, which is faithful to the CNN model; and 3) a\nnovel attribution method via FMI, termed A-FMI, to address the gradient\nsaturation problem, which couples the target image with a reference image, and\nassigns the FMI to the difference-from-reference at the granularity of feature\nmap. Through visual inspections and qualitative evaluations on the ImageNet\ndataset, we show the compelling advantages of A-FMI on its faithfulness,\ninsensitivity to the choice of reference, class discriminability, and superior\nexplanation performance compared with popular attribution methods across\nvarying CNN architectures.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 14:54:44 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Zhang", "An", ""], ["Wang", "Xiang", ""], ["Fang", "Chengfang", ""], ["Shi", "Jie", ""], ["Chua", "Tat-seng", ""], ["Chen", "Zehua", ""]]}, {"id": "2104.05533", "submitter": "Maria A. Zuluaga", "authors": "Francesco Galati and Maria A. Zuluaga", "title": "Efficient Model Monitoring for Quality Control in Cardiac Image\n  Segmentation", "comments": "Accepted to the 11th Biennial Meeting on Functional Imaging and\n  Modeling of the Heart (FIMH-2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning methods have reached state-of-the-art performance in cardiac\nimage segmentation. Currently, the main bottleneck towards their effective\ntranslation into clinics requires assuring continuous high model performance\nand segmentation results. In this work, we present a novel learning framework\nto monitor the performance of heart segmentation models in the absence of\nground truth. Formulated as an anomaly detection problem, the monitoring\nframework allows deriving surrogate quality measures for a segmentation and\nallows flagging suspicious results. We propose two different types of quality\nmeasures, a global score and a pixel-wise map. We demonstrate their use by\nreproducing the final rankings of a cardiac segmentation challenge in the\nabsence of ground truth. Results show that our framework is accurate, fast, and\nscalable, confirming it is a viable option for quality control monitoring in\nclinical practice and large population studies.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 14:58:58 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Galati", "Francesco", ""], ["Zuluaga", "Maria A.", ""]]}, {"id": "2104.05570", "submitter": "Bowen Li", "authors": "Bowen Li, Xinping Ren, Ke Yan, Le Lu, Guotong Xie, Jing Xiao, Dar-In\n  Tai, Adam P. Harrison", "title": "Learning from Subjective Ratings Using Auto-Decoded Deep Latent\n  Embeddings", "comments": "Main body includes 10 pages and 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Depending on the application, radiological diagnoses can be associated with\nhigh inter- and intra-rater variabilities. Most computer-aided diagnosis (CAD)\nsolutions treat such data as incontrovertible, exposing learning algorithms to\nconsiderable and possibly contradictory label noise and biases. Thus, managing\nsubjectivity in labels is a fundamental problem in medical imaging analysis. To\naddress this challenge, we introduce auto-decoded deep latent embeddings\n(ADDLE), which explicitly models the tendencies of each rater using an\nauto-decoder framework. After a simple linear transformation, the latent\nvariables can be injected into any backbone at any and multiple points,\nallowing the model to account for rater-specific effects on the diagnosis.\nImportantly, ADDLE does not expect multiple raters per image in training,\nmeaning it can readily learn from data mined from hospital archives. Moreover,\nthe complexity of training ADDLE does not increase as more raters are added.\nDuring inference each rater can be simulated and a 'mean' or 'greedy' virtual\nrating can be produced. We test ADDLE on the problem of liver steatosis\ndiagnosis from 2D ultrasound (US) by collecting 46 084 studies along with\nclinical US diagnoses originating from 65 different raters. We evaluated\ndiagnostic performance using a separate dataset with gold-standard biopsy\ndiagnoses. ADDLE can improve the partial areas under the curve (AUCs) for\ndiagnosing severe steatosis by 10.5% over standard classifiers while\noutperforming other annotator-noise approaches, including those requiring 65\ntimes the parameters.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 15:40:42 GMT"}, {"version": "v2", "created": "Fri, 16 Apr 2021 17:53:38 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Li", "Bowen", ""], ["Ren", "Xinping", ""], ["Yan", "Ke", ""], ["Lu", "Le", ""], ["Xie", "Guotong", ""], ["Xiao", "Jing", ""], ["Tai", "Dar-In", ""], ["Harrison", "Adam P.", ""]]}, {"id": "2104.05575", "submitter": "Rufin VanRullen", "authors": "Rufin VanRullen and Andrea Alamia", "title": "GAttANet: Global attention agreement for convolutional neural networks", "comments": "Paper accepted to ICANN 2021 - The 30th International Conference on\n  Artificial Neural Networks", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transformer attention architectures, similar to those developed for natural\nlanguage processing, have recently proved efficient also in vision, either in\nconjunction with or as a replacement for convolutional layers. Typically,\nvisual attention is inserted in the network architecture as a (series of)\nfeedforward self-attention module(s), with mutual key-query agreement as the\nmain selection and routing operation. However efficient, this strategy is only\nvaguely compatible with the way that attention is implemented in biological\nbrains: as a separate and unified network of attentional selection regions,\nreceiving inputs from and exerting modulatory influence on the entire hierarchy\nof visual regions. Here, we report experiments with a simple such attention\nsystem that can improve the performance of standard convolutional networks,\nwith relatively few additional parameters. Each spatial position in each layer\nof the network produces a key-query vector pair; all queries are then pooled\ninto a global attention query. On the next iteration, the match between each\nkey and the global attention query modulates the network's activations --\nemphasizing or silencing the locations that agree or disagree (respectively)\nwith the global attention system. We demonstrate the usefulness of this\nbrain-inspired Global Attention Agreement network (GAttANet) for various\nconvolutional backbones (from a simple 5-layer toy model to a standard ResNet50\narchitecture) and datasets (CIFAR10, CIFAR100, Imagenet-1k). Each time, our\nglobal attention system improves accuracy over the corresponding baseline.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 15:45:10 GMT"}, {"version": "v2", "created": "Wed, 30 Jun 2021 08:16:07 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["VanRullen", "Rufin", ""], ["Alamia", "Andrea", ""]]}, {"id": "2104.05600", "submitter": "Seong Jae Hwang", "authors": "Anthony Sicilia, Xingchen Zhao, Anastasia Sosnovskikh, Seong Jae Hwang", "title": "PAC Bayesian Performance Guarantees for Deep (Stochastic) Networks in\n  Medical Imaging", "comments": "MICCAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Application of deep neural networks to medical imaging tasks has in some\nsense become commonplace. Still, a \"thorn in the side\" of the deep learning\nmovement is the argument that deep networks are prone to overfitting and are\nthus unable to generalize well when datasets are small (as is common in medical\nimaging tasks). One way to bolster confidence is to provide mathematical\nguarantees, or bounds, on network performance after training which explicitly\nquantify the possibility of overfitting. In this work, we explore recent\nadvances using the PAC-Bayesian framework to provide bounds on generalization\nerror for large (stochastic) networks. While previous efforts focus on\nclassification in larger natural image datasets (e.g., MNIST and CIFAR-10), we\napply these techniques to both classification and segmentation in a smaller\nmedical imagining dataset: the ISIC 2018 challenge set. We observe the\nresultant bounds are competitive compared to a simpler baseline, while also\nbeing more explainable and alleviating the need for holdout sets.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 16:21:07 GMT"}, {"version": "v2", "created": "Thu, 8 Jul 2021 21:26:29 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Sicilia", "Anthony", ""], ["Zhao", "Xingchen", ""], ["Sosnovskikh", "Anastasia", ""], ["Hwang", "Seong Jae", ""]]}, {"id": "2104.05606", "submitter": "Minghan Li", "authors": "Minghan Li, Shuai Li, Lida Li and Lei Zhang", "title": "Spatial Feature Calibration and Temporal Fusion for Effective One-stage\n  Video Instance Segmentation", "comments": null, "journal-ref": "CVPR2021", "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Modern one-stage video instance segmentation networks suffer from two\nlimitations. First, convolutional features are neither aligned with anchor\nboxes nor with ground-truth bounding boxes, reducing the mask sensitivity to\nspatial location. Second, a video is directly divided into individual frames\nfor frame-level instance segmentation, ignoring the temporal correlation\nbetween adjacent frames. To address these issues, we propose a simple yet\neffective one-stage video instance segmentation framework by spatial\ncalibration and temporal fusion, namely STMask. To ensure spatial feature\ncalibration with ground-truth bounding boxes, we first predict regressed\nbounding boxes around ground-truth bounding boxes, and extract features from\nthem for frame-level instance segmentation. To further explore temporal\ncorrelation among video frames, we aggregate a temporal fusion module to infer\ninstance masks from each frame to its adjacent frames, which helps our\nframework to handle challenging videos such as motion blur, partial occlusion\nand unusual object-to-camera poses. Experiments on the YouTube-VIS valid set\nshow that the proposed STMask with ResNet-50/-101 backbone obtains 33.5 % /\n36.8 % mask AP, while achieving 28.6 / 23.4 FPS on video instance segmentation.\nThe code is released online https://github.com/MinghanLi/STMask.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 09:26:58 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Li", "Minghan", ""], ["Li", "Shuai", ""], ["Li", "Lida", ""], ["Zhang", "Lei", ""]]}, {"id": "2104.05622", "submitter": "Xinqi Zhu", "authors": "Xinqi Zhu, Chang Xu, Dacheng Tao", "title": "Where and What? Examining Interpretable Disentangled Representations", "comments": "CVPR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Capturing interpretable variations has long been one of the goals in\ndisentanglement learning. However, unlike the independence assumption,\ninterpretability has rarely been exploited to encourage disentanglement in the\nunsupervised setting. In this paper, we examine the interpretability of\ndisentangled representations by investigating two questions: where to be\ninterpreted and what to be interpreted? A latent code is easily to be\ninterpreted if it would consistently impact a certain subarea of the resulting\ngenerated image. We thus propose to learn a spatial mask to localize the effect\nof each individual latent dimension. On the other hand, interpretability\nusually comes from latent dimensions that capture simple and basic variations\nin data. We thus impose a perturbation on a certain dimension of the latent\ncode, and expect to identify the perturbation along this dimension from the\ngenerated images so that the encoding of simple variations can be enforced.\nAdditionally, we develop an unsupervised model selection method, which\naccumulates perceptual distance scores along axes in the latent space. On\nvarious datasets, our models can learn high-quality disentangled\nrepresentations without supervision, showing the proposed modeling of\ninterpretability is an effective proxy for achieving unsupervised\ndisentanglement.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 11:22:02 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Zhu", "Xinqi", ""], ["Xu", "Chang", ""], ["Tao", "Dacheng", ""]]}, {"id": "2104.05623", "submitter": "Pei Wang", "authors": "Pei Wang, Yijun Li, Nuno Vasconcelos", "title": "Rethinking and Improving the Robustness of Image Style Transfer", "comments": "Published in CVPR2021 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extensive research in neural style transfer methods has shown that the\ncorrelation between features extracted by a pre-trained VGG network has a\nremarkable ability to capture the visual style of an image. Surprisingly,\nhowever, this stylization quality is not robust and often degrades\nsignificantly when applied to features from more advanced and lightweight\nnetworks, such as those in the ResNet family. By performing extensive\nexperiments with different network architectures, we find that residual\nconnections, which represent the main architectural difference between VGG and\nResNet, produce feature maps of small entropy, which are not suitable for style\ntransfer. To improve the robustness of the ResNet architecture, we then propose\na simple yet effective solution based on a softmax transformation of the\nfeature activations that enhances their entropy. Experimental results\ndemonstrate that this small magic can greatly improve the quality of\nstylization results, even for networks with random weights. This suggests that\nthe architecture used for feature extraction is more important than the use of\nlearned weights for the task of style transfer.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 03:24:45 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Wang", "Pei", ""], ["Li", "Yijun", ""], ["Vasconcelos", "Nuno", ""]]}, {"id": "2104.05642", "submitter": "Annika Reinke", "authors": "Annika Reinke, Matthias Eisenmann, Minu D. Tizabi, Carole H. Sudre,\n  Tim R\\\"adsch, Michela Antonelli, Tal Arbel, Spyridon Bakas, M. Jorge Cardoso,\n  Veronika Cheplygina, Keyvan Farahani, Ben Glocker, Doreen Heckmann-N\\\"otzel,\n  Fabian Isensee, Pierre Jannin, Charles E. Kahn, Jens Kleesiek, Tahsin Kurc,\n  Michal Kozubek, Bennett A. Landman, Geert Litjens, Klaus Maier-Hein, Bjoern\n  Menze, Henning M\\\"uller, Jens Petersen, Mauricio Reyes, Nicola Rieke, Bram\n  Stieltjes, Ronald M. Summers, Sotirios A. Tsaftaris, Bram van Ginneken,\n  Annette Kopp-Schneider, Paul J\\\"ager, Lena Maier-Hein", "title": "Common Limitations of Image Processing Metrics: A Picture Story", "comments": "This is a dynamic paper on limitations of commonly used metrics. The\n  current version discusses segmentation metrics only, while future versions\n  will also include metrics for classification and detection. For missing use\n  cases, comments or questions, please contact a.reinke@dkfz.de or\n  l.maier-hein@dkfz.de. Substantial contributions to this document will be\n  acknowledged with a co-authorship", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While the importance of automatic image analysis is increasing at an enormous\npace, recent meta-research revealed major flaws with respect to algorithm\nvalidation. Specifically, performance metrics are key for objective,\ntransparent and comparative performance assessment, but relatively little\nattention has been given to the practical pitfalls when using specific metrics\nfor a given image analysis task. A common mission of several international\ninitiatives is therefore to provide researchers with guidelines and tools to\nchoose the performance metrics in a problem-aware manner. This dynamically\nupdated document has the purpose to illustrate important limitations of\nperformance metrics commonly applied in the field of image analysis. The\ncurrent version is based on a Delphi process on metrics conducted by an\ninternational consortium of image analysis experts.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 17:03:42 GMT"}, {"version": "v2", "created": "Tue, 13 Apr 2021 14:09:57 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Reinke", "Annika", ""], ["Eisenmann", "Matthias", ""], ["Tizabi", "Minu D.", ""], ["Sudre", "Carole H.", ""], ["R\u00e4dsch", "Tim", ""], ["Antonelli", "Michela", ""], ["Arbel", "Tal", ""], ["Bakas", "Spyridon", ""], ["Cardoso", "M. Jorge", ""], ["Cheplygina", "Veronika", ""], ["Farahani", "Keyvan", ""], ["Glocker", "Ben", ""], ["Heckmann-N\u00f6tzel", "Doreen", ""], ["Isensee", "Fabian", ""], ["Jannin", "Pierre", ""], ["Kahn", "Charles E.", ""], ["Kleesiek", "Jens", ""], ["Kurc", "Tahsin", ""], ["Kozubek", "Michal", ""], ["Landman", "Bennett A.", ""], ["Litjens", "Geert", ""], ["Maier-Hein", "Klaus", ""], ["Menze", "Bjoern", ""], ["M\u00fcller", "Henning", ""], ["Petersen", "Jens", ""], ["Reyes", "Mauricio", ""], ["Rieke", "Nicola", ""], ["Stieltjes", "Bram", ""], ["Summers", "Ronald M.", ""], ["Tsaftaris", "Sotirios A.", ""], ["van Ginneken", "Bram", ""], ["Kopp-Schneider", "Annette", ""], ["J\u00e4ger", "Paul", ""], ["Maier-Hein", "Lena", ""]]}, {"id": "2104.05647", "submitter": "Jordan J. Bird", "authors": "Jordan J. Bird, Chloe M. Barnes, Luis J. Manso, Anik\\'o Ek\\'art, Diego\n  R. Faria", "title": "Fruit Quality and Defect Image Classification with Conditional GAN Data\n  Augmentation", "comments": "16 pages, 12 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contemporary Artificial Intelligence technologies allow for the employment of\nComputer Vision to discern good crops from bad, providing a step in the\npipeline of selecting healthy fruit from undesirable fruit, such as those which\nare mouldy or gangrenous. State-of-the-art works in the field report high\naccuracy results on small datasets (<1000 images), which are not representative\nof the population regarding real-world usage. The goals of this study are to\nfurther enable real-world usage by improving generalisation with data\naugmentation as well as to reduce overfitting and energy usage through model\npruning. In this work, we suggest a machine learning pipeline that combines the\nideas of fine-tuning, transfer learning, and generative model-based training\ndata augmentation towards improving fruit quality image classification. A\nlinear network topology search is performed to tune a VGG16 lemon quality\nclassification model using a publicly-available dataset of 2690 images. We find\nthat appending a 4096 neuron fully connected layer to the convolutional layers\nleads to an image classification accuracy of 83.77%. We then train a\nConditional Generative Adversarial Network on the training data for 2000\nepochs, and it learns to generate relatively realistic images. Grad-CAM\nanalysis of the model trained on real photographs shows that the synthetic\nimages can exhibit classifiable characteristics such as shape, mould, and\ngangrene. A higher image classification accuracy of 88.75% is then attained by\naugmenting the training with synthetic images, arguing that Conditional\nGenerative Adversarial Networks have the ability to produce new data to\nalleviate issues of data scarcity. Finally, model pruning is performed via\npolynomial decay, where we find that the Conditional GAN-augmented\nclassification network can retain 81.16% classification accuracy when\ncompressed to 50% of its original size.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 17:13:05 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Bird", "Jordan J.", ""], ["Barnes", "Chloe M.", ""], ["Manso", "Luis J.", ""], ["Ek\u00e1rt", "Anik\u00f3", ""], ["Faria", "Diego R.", ""]]}, {"id": "2104.05652", "submitter": "Fenggen Yu", "authors": "Fenggen Yu, Zhiqin Chen, Manyi Li, Aditya Sanghi, Hooman Shayani, Ali\n  Mahdavi-Amiri and Hao Zhang", "title": "CAPRI-Net: Learning Compact CAD Shapes with Adaptive Primitive Assembly", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce CAPRI-Net, a neural network for learning compact and\ninterpretable implicit representations of 3D computer-aided design (CAD)\nmodels, in the form of adaptive primitive assemblies. Our network takes an\ninput 3D shape that can be provided as a point cloud or voxel grids, and\nreconstructs it by a compact assembly of quadric surface primitives via\nconstructive solid geometry (CSG) operations. The network is self-supervised\nwith a reconstruction loss, leading to faithful 3D reconstructions with sharp\nedges and plausible CSG trees, without any ground-truth shape assemblies. While\nthe parametric nature of CAD models does make them more predictable locally, at\nthe shape level, there is a great deal of structural and topological\nvariations, which present a significant generalizability challenge to\nstate-of-the-art neural models for 3D shapes. Our network addresses this\nchallenge by adaptive training with respect to each test shape, with which we\nfine-tune the network that was pre-trained on a model collection. We evaluate\nour learning framework on both ShapeNet and ABC, the largest and most diverse\nCAD dataset to date, in terms of reconstruction quality, shape edges,\ncompactness, and interpretability, to demonstrate superiority over current\nalternatives suitable for neural CAD reconstruction.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 17:21:19 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Yu", "Fenggen", ""], ["Chen", "Zhiqin", ""], ["Li", "Manyi", ""], ["Sanghi", "Aditya", ""], ["Shayani", "Hooman", ""], ["Mahdavi-Amiri", "Ali", ""], ["Zhang", "Hao", ""]]}, {"id": "2104.05666", "submitter": "Xuancheng Zhang", "authors": "Xuancheng Zhang, Yutong Feng, Siqi Li, Changqing Zou, Hai Wan, Xibin\n  Zhao, Yandong Guo, Yue Gao", "title": "View-Guided Point Cloud Completion", "comments": "10 pages, 8 figures, CVPR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a view-guided solution for the task of point cloud\ncompletion. Unlike most existing methods directly inferring the missing points\nusing shape priors, we address this task by introducing ViPC (view-guided point\ncloud completion) that takes the missing crucial global structure information\nfrom an extra single-view image. By leveraging a framework that sequentially\nperforms effective cross-modality and cross-level fusions, our method achieves\nsignificantly superior results over typical existing solutions on a new\nlarge-scale dataset we collect for the view-guided point cloud completion task.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 17:35:45 GMT"}, {"version": "v2", "created": "Tue, 13 Apr 2021 04:43:04 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Zhang", "Xuancheng", ""], ["Feng", "Yutong", ""], ["Li", "Siqi", ""], ["Zou", "Changqing", ""], ["Wan", "Hai", ""], ["Zhao", "Xibin", ""], ["Guo", "Yandong", ""], ["Gao", "Yue", ""]]}, {"id": "2104.05668", "submitter": "Jingcai Guo", "authors": "Jingcai Guo", "title": "Learning Robust Visual-semantic Mapping for Zero-shot Learning", "comments": "thesis, Dec 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zero-shot learning (ZSL) aims at recognizing unseen class examples (e.g.,\nimages) with knowledge transferred from seen classes. This is typically\nachieved by exploiting a semantic feature space shared by both seen and unseen\nclasses, e.g., attributes or word vectors, as the bridge. In ZSL, the common\npractice is to train a mapping function between the visual and semantic feature\nspaces with labeled seen class examples. When inferring, given unseen class\nexamples, the learned mapping function is reused to them and recognizes the\nclass labels on some metrics among their semantic relations. However, the\nvisual and semantic feature spaces are generally independent and exist in\nentirely different manifolds. Under such a paradigm, the ZSL models may easily\nsuffer from the domain shift problem when constructing and reusing the mapping\nfunction, which becomes the major challenge in ZSL. In this thesis, we explore\neffective ways to mitigate the domain shift problem and learn a robust mapping\nfunction between the visual and semantic feature spaces. We focus on fully\nempowering the semantic feature space, which is one of the key building blocks\nof ZSL. In summary, this thesis targets fully empowering the semantic feature\nspace and design effective solutions to mitigate the domain shift problem and\nhence obtain a more robust visual-semantic mapping function for ZSL. Extensive\nexperiments on various datasets demonstrate the effectiveness of our proposed\nmethods.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 17:39:38 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Guo", "Jingcai", ""]]}, {"id": "2104.05670", "submitter": "Mathis Petrovich", "authors": "Mathis Petrovich, Michael J. Black, G\\\"ul Varol", "title": "Action-Conditioned 3D Human Motion Synthesis with Transformer VAE", "comments": "14 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We tackle the problem of action-conditioned generation of realistic and\ndiverse human motion sequences. In contrast to methods that complete, or\nextend, motion sequences, this task does not require an initial pose or\nsequence. Here we learn an action-aware latent representation for human motions\nby training a generative variational autoencoder (VAE). By sampling from this\nlatent space and querying a certain duration through a series of positional\nencodings, we synthesize variable-length motion sequences conditioned on a\ncategorical action. Specifically, we design a Transformer-based architecture,\nACTOR, for encoding and decoding a sequence of parametric SMPL human body\nmodels estimated from action recognition datasets. We evaluate our approach on\nthe NTU RGB+D, HumanAct12 and UESTC datasets and show improvements over the\nstate of the art. Furthermore, we present two use cases: improving action\nrecognition through adding our synthesized data to training, and motion\ndenoising. Our code and models will be made available.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 17:40:27 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Petrovich", "Mathis", ""], ["Black", "Michael J.", ""], ["Varol", "G\u00fcl", ""]]}, {"id": "2104.05677", "submitter": "Iuliia Kotseruba", "authors": "Iuliia Kotseruba and John K. Tsotsos", "title": "Behavioral Research and Practical Models of Drivers' Attention", "comments": "78 pages, 21 figures, 9 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Driving is a routine activity for many, but it is far from simple. Drivers\ndeal with multiple concurrent tasks, such as keeping the vehicle in the lane,\nobserving and anticipating the actions of other road users, reacting to\nhazards, and dealing with distractions inside and outside the vehicle. Failure\nto notice and respond to the surrounding objects and events can cause\naccidents.\n  The ongoing improvements of the road infrastructure and vehicle mechanical\ndesign have made driving safer overall. Nevertheless, the problem of driver\ninattention has remained one of the primary causes of accidents. Therefore,\nunderstanding where the drivers look and why they do so can help eliminate\nsources of distractions and identify unsafe attention patterns. Research on\ndriver attention has implications for many practical applications such as\npolicy-making, improving driver education, enhancing road infrastructure and\nin-vehicle infotainment systems, as well as designing systems for driver\nmonitoring, driver assistance, and automated driving.\n  This report covers the literature on changes in drivers' visual attention\ndistribution due to factors, internal and external to the driver. Aspects of\nattention during driving have been explored across multiple disciplines,\nincluding psychology, human factors, human-computer interaction, intelligent\ntransportation, and computer vision, each offering different perspectives,\ngoals, and explanations for the observed phenomena. We link cross-disciplinary\ntheoretical and behavioral research on driver's attention to practical\nsolutions. Furthermore, limitations and directions for future research are\ndiscussed. This report is based on over 175 behavioral studies, nearly 100\npractical papers, 20 datasets, and over 70 surveys published since 2010. A\ncurated list of papers used for this report is available at\nhttps://github.com/ykotseruba/attention_and_driving.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 17:42:04 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Kotseruba", "Iuliia", ""], ["Tsotsos", "John K.", ""]]}, {"id": "2104.05693", "submitter": "Lakshmanan Nataraj", "authors": "Lakshmanan Nataraj, Michael Goebel, Tajuddin Manhar Mohammed,\n  Shivkumar Chandrasekaran, B. S. Manjunath", "title": "Holistic Image Manipulation Detection using Pixel Co-occurrence Matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Digital image forensics aims to detect images that have been digitally\nmanipulated. Realistic image forgeries involve a combination of splicing,\nresampling, region removal, smoothing and other manipulation methods. While\nmost detection methods in literature focus on detecting a particular type of\nmanipulation, it is challenging to identify doctored images that involve a host\nof manipulations. In this paper, we propose a novel approach to holistically\ndetect tampered images using a combination of pixel co-occurrence matrices and\ndeep learning. We extract horizontal and vertical co-occurrence matrices on\nthree color channels in the pixel domain and train a model using a deep\nconvolutional neural network (CNN) framework. Our method is agnostic to the\ntype of manipulation and classifies an image as tampered or untampered. We\ntrain and validate our model on a dataset of more than 86,000 images.\nExperimental results show that our approach is promising and achieves more than\n0.99 area under the curve (AUC) evaluation metric on the training and\nvalidation subsets. Further, our approach also generalizes well and achieves\naround 0.81 AUC on an unseen test dataset comprising more than 19,740 images\nreleased as part of the Media Forensics Challenge (MFC) 2020. Our score was\nhighest among all other teams that participated in the challenge, at the time\nof announcement of the challenge results.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 17:54:42 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Nataraj", "Lakshmanan", ""], ["Goebel", "Michael", ""], ["Mohammed", "Tajuddin Manhar", ""], ["Chandrasekaran", "Shivkumar", ""], ["Manjunath", "B. S.", ""]]}, {"id": "2104.05702", "submitter": "Zhiding Yu", "authors": "Nadine Chang, Zhiding Yu, Yu-Xiong Wang, Anima Anandkumar, Sanja\n  Fidler, Jose M. Alvarez", "title": "Image-Level or Object-Level? A Tale of Two Resampling Strategies for\n  Long-Tailed Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training on datasets with long-tailed distributions has been challenging for\nmajor recognition tasks such as classification and detection. To deal with this\nchallenge, image resampling is typically introduced as a simple but effective\napproach. However, we observe that long-tailed detection differs from\nclassification since multiple classes may be present in one image. As a result,\nimage resampling alone is not enough to yield a sufficiently balanced\ndistribution at the object level. We address object-level resampling by\nintroducing an object-centric memory replay strategy based on dynamic, episodic\nmemory banks. Our proposed strategy has two benefits: 1) convenient\nobject-level resampling without significant extra computation, and 2) implicit\nfeature-level augmentation from model updates. We show that image-level and\nobject-level resamplings are both important, and thus unify them with a joint\nresampling strategy (RIO). Our method outperforms state-of-the-art long-tailed\ndetection and segmentation methods on LVIS v0.5 across various backbones.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 17:58:30 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Chang", "Nadine", ""], ["Yu", "Zhiding", ""], ["Wang", "Yu-Xiong", ""], ["Anandkumar", "Anima", ""], ["Fidler", "Sanja", ""], ["Alvarez", "Jose M.", ""]]}, {"id": "2104.05703", "submitter": "Xiaoyu Xiang", "authors": "Xiaoyu Xiang, Ding Liu, Xiao Yang, Yiheng Zhu, Xiaohui Shen, Jan P.\n  Allebach", "title": "Adversarial Open Domain Adaption for Sketch-to-Photo Synthesis", "comments": "19 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we explore the open-domain sketch-to-photo translation, which\naims to synthesize a realistic photo from a freehand sketch with its class\nlabel, even if the sketches of that class are missing in the training data. It\nis challenging due to the lack of training supervision and the large geometry\ndistortion between the freehand sketch and photo domains. To synthesize the\nabsent freehand sketches from photos, we propose a framework that jointly\nlearns sketch-to-photo and photo-to-sketch generation. However, the generator\ntrained from fake sketches might lead to unsatisfying results when dealing with\nsketches of missing classes, due to the domain gap between synthesized sketches\nand real ones. To alleviate this issue, we further propose a simple yet\neffective open-domain sampling and optimization strategy to \"fool\" the\ngenerator into treating fake sketches as real ones. Our method takes advantage\nof the learned sketch-to-photo and photo-to-sketch mapping of in-domain data\nand generalizes them to the open-domain classes. We validate our method on the\nScribble and SketchyCOCO datasets. Compared with the recent competing methods,\nour approach shows impressive results in synthesizing realistic color, texture,\nand maintaining the geometric composition for various categories of open-domain\nsketches.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 17:58:46 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Xiang", "Xiaoyu", ""], ["Liu", "Ding", ""], ["Yang", "Xiao", ""], ["Zhu", "Yiheng", ""], ["Shen", "Xiaohui", ""], ["Allebach", "Jan P.", ""]]}, {"id": "2104.05704", "submitter": "Humphrey Shi", "authors": "Ali Hassani, Steven Walton, Nikhil Shah, Abulikemu Abuduweili, Jiachen\n  Li, Humphrey Shi", "title": "Escaping the Big Data Paradigm with Compact Transformers", "comments": "Added experiments on ImageNet and NLP tasks", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rise of Transformers as the standard for language processing, and\ntheir advancements in computer vision, along with their unprecedented size and\namounts of training data, many have come to believe that they are not suitable\nfor small sets of data. This trend leads to great concerns, including but not\nlimited to: limited availability of data in certain scientific domains and the\nexclusion of those with limited resource from research in the field. In this\npaper, we dispel the myth that transformers are \"data hungry\" and therefore can\nonly be applied to large sets of data. We show for the first time that with the\nright size and tokenization, transformers can perform head-to-head with\nstate-of-the-art CNNs on small datasets. Our model eliminates the requirement\nfor class token and positional embeddings through a novel sequence pooling\nstrategy and the use of convolutions. We show that compared to CNNs, our\ncompact transformers have fewer parameters and MACs, while obtaining similar\naccuracies. Our method is flexible in terms of model size, and can have as\nlittle as 0.28M parameters and achieve reasonable results. It can reach an\naccuracy of 95.29 % when training from scratch on CIFAR-10, which is comparable\nwith modern CNN based approaches, and a significant improvement over previous\nTransformer based models. Our simple and compact design democratizes\ntransformers by making them accessible to those equipped with basic computing\nresources and/or dealing with important small datasets. Our method works on\nlarger datasets, such as ImageNet (80.28% accuracy with 29% parameters of ViT),\nand NLP tasks as well. Our code and pre-trained models are publicly available\nat https://github.com/SHI-Labs/Compact-Transformers.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 17:58:56 GMT"}, {"version": "v2", "created": "Wed, 30 Jun 2021 17:03:41 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Hassani", "Ali", ""], ["Walton", "Steven", ""], ["Shah", "Nikhil", ""], ["Abuduweili", "Abulikemu", ""], ["Li", "Jiachen", ""], ["Shi", "Humphrey", ""]]}, {"id": "2104.05706", "submitter": "Yawei Li", "authors": "Yawei Li, He Chen, Zhaopeng Cui, Radu Timofte, Marc Pollefeys, Gregory\n  Chirikjian, Luc Van Gool", "title": "Towards Efficient Graph Convolutional Networks for Point Cloud Handling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we aim at improving the computational efficiency of graph\nconvolutional networks (GCNs) for learning on point clouds. The basic graph\nconvolution that is typically composed of a $K$-nearest neighbor (KNN) search\nand a multilayer perceptron (MLP) is examined. By mathematically analyzing the\noperations there, two findings to improve the efficiency of GCNs are obtained.\n(1) The local geometric structure information of 3D representations propagates\nsmoothly across the GCN that relies on KNN search to gather neighborhood\nfeatures. This motivates the simplification of multiple KNN searches in GCNs.\n(2) Shuffling the order of graph feature gathering and an MLP leads to\nequivalent or similar composite operations. Based on those findings, we\noptimize the computational procedure in GCNs. A series of experiments show that\nthe optimized networks have reduced computational complexity, decreased memory\nconsumption, and accelerated inference speed while maintaining comparable\naccuracy for learning on point clouds. Code will be available at\n\\url{https://github.com/ofsoundof/EfficientGCN.git}.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 17:59:16 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Li", "Yawei", ""], ["Chen", "He", ""], ["Cui", "Zhaopeng", ""], ["Timofte", "Radu", ""], ["Pollefeys", "Marc", ""], ["Chirikjian", "Gregory", ""], ["Van Gool", "Luc", ""]]}, {"id": "2104.05707", "submitter": "Yawei Li", "authors": "Yawei Li, Kai Zhang, Jiezhang Cao, Radu Timofte, Luc Van Gool", "title": "LocalViT: Bringing Locality to Vision Transformers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study how to introduce locality mechanisms into vision transformers. The\ntransformer network originates from machine translation and is particularly\ngood at modelling long-range dependencies within a long sequence. Although the\nglobal interaction between the token embeddings could be well modelled by the\nself-attention mechanism of transformers, what is lacking a locality mechanism\nfor information exchange within a local region. Yet, locality is essential for\nimages since it pertains to structures like lines, edges, shapes, and even\nobjects.\n  We add locality to vision transformers by introducing depth-wise convolution\ninto the feed-forward network. This seemingly simple solution is inspired by\nthe comparison between feed-forward networks and inverted residual blocks. The\nimportance of locality mechanisms is validated in two ways: 1) A wide range of\ndesign choices (activation function, layer placement, expansion ratio) are\navailable for incorporating locality mechanisms and all proper choices can lead\nto a performance gain over the baseline, and 2) The same locality mechanism is\nsuccessfully applied to 4 vision transformers, which shows the generalization\nof the locality concept. In particular, for ImageNet2012 classification, the\nlocality-enhanced transformers outperform the baselines DeiT-T and PVT-T by\n2.6\\% and 3.1\\% with a negligible increase in the number of parameters and\ncomputational effort. Code is available at\n\\url{https://github.com/ofsoundof/LocalViT}.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 17:59:22 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Li", "Yawei", ""], ["Zhang", "Kai", ""], ["Cao", "Jiezhang", ""], ["Timofte", "Radu", ""], ["Van Gool", "Luc", ""]]}, {"id": "2104.05742", "submitter": "Tarik A. Rashid", "authors": "Nitish Maharjan, Abeer Alsadoon, P.W.C. Prasad, Salma Abdullah, Tarik\n  A. Rashid", "title": "A Novel Visualization System of Using Augmented Reality in Knee\n  Replacement Surgery: Enhanced Bidirectional Maximum Correntropy Algorithm", "comments": "27 pages", "journal-ref": "The International Journal of Medical Robotics and Computer\n  Assisted Surgery, 2020", "doi": "10.1002/rcs.2154", "report-no": null, "categories": "cs.CV cs.AI cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Background and aim: Image registration and alignment are the main limitations\nof augmented reality-based knee replacement surgery. This research aims to\ndecrease the registration error, eliminate outcomes that are trapped in local\nminima to improve the alignment problems, handle the occlusion, and maximize\nthe overlapping parts. Methodology: markerless image registration method was\nused for Augmented reality-based knee replacement surgery to guide and\nvisualize the surgical operation. While weight least square algorithm was used\nto enhance stereo camera-based tracking by filling border occlusion in right to\nleft direction and non-border occlusion from left to right direction. Results:\nThis study has improved video precision to 0.57 mm~0.61 mm alignment error.\nFurthermore, with the use of bidirectional points, for example, forwards and\nbackwards directional cloud point, the iteration on image registration was\ndecreased. This has led to improve the processing time as well. The processing\ntime of video frames was improved to 7.4~11.74 fps. Conclusions: It seems clear\nthat this proposed system has focused on overcoming the misalignment difficulty\ncaused by movement of patient and enhancing the AR visualization during knee\nreplacement surgery. The proposed system was reliable and favorable which helps\nin eliminating alignment error by ascertaining the optimal rigid transformation\nbetween two cloud points and removing the outliers and non-Gaussian noise. The\nproposed augmented reality system helps in accurate visualization and\nnavigation of anatomy of knee such as femur, tibia, cartilage, blood vessels,\netc.\n", "versions": [{"version": "v1", "created": "Sat, 13 Mar 2021 19:18:16 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Maharjan", "Nitish", ""], ["Alsadoon", "Abeer", ""], ["Prasad", "P. W. C.", ""], ["Abdullah", "Salma", ""], ["Rashid", "Tarik A.", ""]]}, {"id": "2104.05744", "submitter": "Tarik A. Rashid", "authors": "Sagar Chhetri, Abeer Alsadoon, Thair Al Dala in, P. W. C. Prasad,\n  Tarik A. Rashid, Angelika Maag", "title": "Deep Learning for Vision-Based Fall Detection System: Enhanced Optical\n  Dynamic Flow", "comments": "16 pages", "journal-ref": "Computational Intelligence, 2020", "doi": "10.1111/coin.12428", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Accurate fall detection for the assistance of older people is crucial to\nreduce incidents of deaths or injuries due to falls. Meanwhile, a vision-based\nfall detection system has shown some significant results to detect falls.\nStill, numerous challenges need to be resolved. The impact of deep learning has\nchanged the landscape of the vision-based system, such as action recognition.\nThe deep learning technique has not been successfully implemented in\nvision-based fall detection systems due to the requirement of a large amount of\ncomputation power and the requirement of a large amount of sample training\ndata. This research aims to propose a vision-based fall detection system that\nimproves the accuracy of fall detection in some complex environments such as\nthe change of light condition in the room. Also, this research aims to increase\nthe performance of the pre-processing of video images. The proposed system\nconsists of the Enhanced Dynamic Optical Flow technique that encodes the\ntemporal data of optical flow videos by the method of rank pooling, which\nthereby improves the processing time of fall detection and improves the\nclassification accuracy in dynamic lighting conditions. The experimental\nresults showed that the classification accuracy of the fall detection improved\nby around 3% and the processing time by 40 to 50ms. The proposed system\nconcentrates on decreasing the processing time of fall detection and improving\nclassification accuracy. Meanwhile, it provides a mechanism for summarizing a\nvideo into a single image by using a dynamic optical flow technique, which\nhelps to increase the performance of image pre-processing steps.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 08:14:25 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Chhetri", "Sagar", ""], ["Alsadoon", "Abeer", ""], ["in", "Thair Al Dala", ""], ["Prasad", "P. W. C.", ""], ["Rashid", "Tarik A.", ""], ["Maag", "Angelika", ""]]}, {"id": "2104.05758", "submitter": "Miao Yin", "authors": "Miao Yin, Siyu Liao, Xiao-Yang Liu, Xiaodong Wang and Bo Yuan", "title": "Towards Extremely Compact RNNs for Video Recognition with Fully\n  Decomposed Hierarchical Tucker Structure", "comments": "This paper is a preprint that was accepted in CVPR'21. arXiv admin\n  note: substantial text overlap with arXiv:2005.04366", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Recurrent Neural Networks (RNNs) have been widely used in sequence analysis\nand modeling. However, when processing high-dimensional data, RNNs typically\nrequire very large model sizes, thereby bringing a series of deployment\nchallenges. Although various prior works have been proposed to reduce the RNN\nmodel sizes, executing RNN models in resource-restricted environments is still\na very challenging problem. In this paper, we propose to develop extremely\ncompact RNN models with fully decomposed hierarchical Tucker (FDHT) structure.\nThe HT decomposition does not only provide much higher storage cost reduction\nthan the other tensor decomposition approaches but also brings better accuracy\nperformance improvement for the compact RNN models. Meanwhile, unlike the\nexisting tensor decomposition-based methods that can only decompose the\ninput-to-hidden layer of RNNs, our proposed fully decomposition approach\nenables the comprehensive compression for the entire RNN models with\nmaintaining very high accuracy. Our experimental results on several popular\nvideo recognition datasets show that our proposed fully decomposed hierarchical\ntucker-based LSTM (FDHT-LSTM) is extremely compact and highly efficient. To the\nbest of our knowledge, FDHT-LSTM, for the first time, consistently achieves\nvery high accuracy with only few thousand parameters (3,132 to 8,808) on\ndifferent datasets. Compared with the state-of-the-art compressed RNN models,\nsuch as TT-LSTM, TR-LSTM and BT-LSTM, our FDHT-LSTM simultaneously enjoys both\norder-of-magnitude (3,985x to 10,711x) fewer parameters and significant\naccuracy improvement (0.6% to 12.7%).\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 18:40:44 GMT"}, {"version": "v2", "created": "Wed, 14 Apr 2021 23:51:47 GMT"}, {"version": "v3", "created": "Tue, 20 Apr 2021 17:54:22 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Yin", "Miao", ""], ["Liao", "Siyu", ""], ["Liu", "Xiao-Yang", ""], ["Wang", "Xiaodong", ""], ["Yuan", "Bo", ""]]}, {"id": "2104.05764", "submitter": "Fei Lu", "authors": "Fei Lu, Hyeonwoo Yu, Jean Oh", "title": "Domain Adaptive Monocular Depth Estimation With Semantic Information", "comments": "8 pages, 5 figures, code will be released soon", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The advent of deep learning has brought an impressive advance to monocular\ndepth estimation, e.g., supervised monocular depth estimation has been\nthoroughly investigated. However, the large amount of the RGB-to-depth dataset\nmay not be always available since collecting accurate depth ground truth\naccording to the RGB image is a time-consuming and expensive task. Although the\nnetwork can be trained on an alternative dataset to overcome the dataset scale\nproblem, the trained model is hard to generalize to the target domain due to\nthe domain discrepancy. Adversarial domain alignment has demonstrated its\nefficacy to mitigate the domain shift on simple image classification tasks in\nprevious works. However, traditional approaches hardly handle the conditional\nalignment as they solely consider the feature map of the network. In this\npaper, we propose an adversarial training model that leverages semantic\ninformation to narrow the domain gap. Based on the experiments conducted on the\ndatasets for the monocular depth estimation task including KITTI and\nCityscapes, the proposed compact model achieves state-of-the-art performance\ncomparable to complex latest models and shows favorable results on boundaries\nand objects at far distances.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 18:50:41 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Lu", "Fei", ""], ["Yu", "Hyeonwoo", ""], ["Oh", "Jean", ""]]}, {"id": "2104.05778", "submitter": "Saikat Dutta", "authors": "Saikat Dutta, Nisarg A. Shah, Anurag Mittal", "title": "Efficient Space-time Video Super Resolution using Low-Resolution Flow\n  and Mask Upsampling", "comments": "Accepted at NTIRE Workshop, CVPR 2021. Code and models:\n  https://github.com/saikatdutta/FMU_STSR", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores an efficient solution for Space-time Super-Resolution,\naiming to generate High-resolution Slow-motion videos from Low Resolution and\nLow Frame rate videos. A simplistic solution is the sequential running of Video\nSuper Resolution and Video Frame interpolation models. However, this type of\nsolutions are memory inefficient, have high inference time, and could not make\nthe proper use of space-time relation property. To this extent, we first\ninterpolate in LR space using quadratic modeling. Input LR frames are\nsuper-resolved using a state-of-the-art Video Super-Resolution method. Flowmaps\nand blending mask which are used to synthesize LR interpolated frame is reused\nin HR space using bilinear upsampling. This leads to a coarse estimate of HR\nintermediate frame which often contains artifacts along motion boundaries. We\nuse a refinement network to improve the quality of HR intermediate frame via\nresidual learning. Our model is lightweight and performs better than current\nstate-of-the-art models in REDS STSR Validation set.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 19:11:57 GMT"}, {"version": "v2", "created": "Mon, 3 May 2021 06:50:08 GMT"}, {"version": "v3", "created": "Tue, 8 Jun 2021 04:17:40 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Dutta", "Saikat", ""], ["Shah", "Nisarg A.", ""], ["Mittal", "Anurag", ""]]}, {"id": "2104.05779", "submitter": "Oranit Dror", "authors": "Idit Diamant, Oranit Dror, Hai Victor Habi, Arnon Netzer", "title": "Multi-View Image-to-Image Translation Supervised by 3D Pose", "comments": "*equal contribution", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the task of multi-view image-to-image translation for person image\ngeneration. The goal is to synthesize photo-realistic multi-view images with\npose-consistency across all views. Our proposed end-to-end framework is based\non a joint learning of multiple unpaired image-to-image translation models, one\nper camera viewpoint. The joint learning is imposed by constraints on the\nshared 3D human pose in order to encourage the 2D pose projections in all views\nto be consistent. Experimental results on the CMU-Panoptic dataset demonstrate\nthe effectiveness of the suggested framework in generating photo-realistic\nimages of persons with new poses that are more consistent across all views in\ncomparison to a standard Image-to-Image baseline. The code is available at:\nhttps://github.com/sony-si/MultiView-Img2Img\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 19:12:39 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Diamant", "Idit", ""], ["Dror", "Oranit", ""], ["Habi", "Hai Victor", ""], ["Netzer", "Arnon", ""]]}, {"id": "2104.05785", "submitter": "Kenji Kawaguchi", "authors": "Kenji Kawaguchi, Qingyun Sun", "title": "A Recipe for Global Convergence Guarantee in Deep Neural Networks", "comments": "Published in AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing global convergence guarantees of (stochastic) gradient descent do\nnot apply to practical deep networks in the practical regime of deep learning\nbeyond the neural tangent kernel (NTK) regime. This paper proposes an\nalgorithm, which is ensured to have global convergence guarantees in the\npractical regime beyond the NTK regime, under a verifiable condition called the\nexpressivity condition. The expressivity condition is defined to be both\ndata-dependent and architecture-dependent, which is the key property that makes\nour results applicable for practical settings beyond the NTK regime. On the one\nhand, the expressivity condition is theoretically proven to hold\ndata-independently for fully-connected deep neural networks with narrow hidden\nlayers and a single wide layer. On the other hand, the expressivity condition\nis numerically shown to hold data-dependently for deep (convolutional) ResNet\nwith batch normalization with various standard image datasets. We also show\nthat the proposed algorithm has generalization performances comparable with\nthose of the heuristic algorithm, with the same hyper-parameters and total\nnumber of iterations. Therefore, the proposed algorithm can be viewed as a step\ntowards providing theoretical guarantees for deep learning in the practical\nregime.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 19:25:30 GMT"}, {"version": "v2", "created": "Thu, 15 Apr 2021 20:35:03 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Kawaguchi", "Kenji", ""], ["Sun", "Qingyun", ""]]}, {"id": "2104.05786", "submitter": "Fei Xu", "authors": "Lu Cai, Fei Xu, Fidelma Dilemma, Daniel J. Murray, Cynthia A. Adkins,\n  Larry K Aagesen Jr, Min Xian, Luca Caprriot, Tiankai Yao", "title": "Understanding Fission Gas Bubble Distribution, Lanthanide\n  Transportation, and Thermal Conductivity Degradation in Neutron-irradiated\n  {\\alpha}-U Using Machine Learning", "comments": "19 pages, 12 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.mtrl-sci cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  UZr based metallic nuclear fuel is the leading candidate for next-generation\nsodium-cooled fast reactors in the United States. US research reactors have\nbeen using and testing this fuel type since the 1960s and accumulated\nconsiderable experience and knowledge about the fuel performance. However, most\nof knowledge remains empirical. The lack of mechanistic understanding of fuel\nperformance is preventing the qualification of UZr fuel for commercial use.\nThis paper proposes a data-driven approach, coupled with advanced post\nirradiation examination, powered by machine learning algorithms, to facilitate\nthe development of such understandings by providing unpreceded quantified new\ninsights into fission gas bubbles. Specifically, based on the advanced\npostirradiation examination data collected on a neutron-irradiated U-10Zr\nannular fuel, we developed a method to automatically detect, classify ~19,000\nfission gas bubbles into different categories, and quantitatively link the data\nto lanthanide transpiration along the radial temperature gradient. The approach\nis versatile and can be modified to study different coupled irradiation\neffects, such as secondary phase redistribution and degradation of thermal\nconductivity, in irradiated nuclear fuel.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 19:29:18 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Cai", "Lu", ""], ["Xu", "Fei", ""], ["Dilemma", "Fidelma", ""], ["Murray", "Daniel J.", ""], ["Adkins", "Cynthia A.", ""], ["Aagesen", "Larry K", "Jr"], ["Xian", "Min", ""], ["Caprriot", "Luca", ""], ["Yao", "Tiankai", ""]]}, {"id": "2104.05788", "submitter": "Ben Glocker", "authors": "Mobarakol Islam and Ben Glocker", "title": "Spatially Varying Label Smoothing: Capturing Uncertainty from Expert\n  Annotations", "comments": "Accepted at IPMI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of image segmentation is inherently noisy due to ambiguities\nregarding the exact location of boundaries between anatomical structures. We\nargue that this information can be extracted from the expert annotations at no\nextra cost, and when integrated into state-of-the-art neural networks, it can\nlead to improved calibration between soft probabilistic predictions and the\nunderlying uncertainty. We built upon label smoothing (LS) where a network is\ntrained on 'blurred' versions of the ground truth labels which has been shown\nto be effective for calibrating output predictions. However, LS is not taking\nthe local structure into account and results in overly smoothed predictions\nwith low confidence even for non-ambiguous regions. Here, we propose Spatially\nVarying Label Smoothing (SVLS), a soft labeling technique that captures the\nstructural uncertainty in semantic segmentation. SVLS also naturally lends\nitself to incorporate inter-rater uncertainty when multiple labelmaps are\navailable. The proposed approach is extensively validated on four clinical\nsegmentation tasks with different imaging modalities, number of classes and\nsingle and multi-rater expert annotations. The results demonstrate that SVLS,\ndespite its simplicity, obtains superior boundary prediction with improved\nuncertainty and model calibration.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 19:35:51 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Islam", "Mobarakol", ""], ["Glocker", "Ben", ""]]}, {"id": "2104.05813", "submitter": "Joao Paulo Silva Do Monte Lima", "authors": "Jo\\~ao Paulo Lima, Rafael Roberto, Lucas Figueiredo, Francisco\n  Sim\\~oes, Veronica Teichrieb", "title": "Generalizable Multi-Camera 3D Pedestrian Detection", "comments": "Accepted to CVPRW 2021, LatinX in Computer Vision (LXCV) Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a multi-camera 3D pedestrian detection method that does not need\nto train using data from the target scene. We estimate pedestrian location on\nthe ground plane using a novel heuristic based on human body poses and person's\nbounding boxes from an off-the-shelf monocular detector. We then project these\nlocations onto the world ground plane and fuse them with a new formulation of a\nclique cover problem. We also propose an optional step for exploiting\npedestrian appearance during fusion by using a domain-generalizable person\nre-identification model. We evaluated the proposed approach on the challenging\nWILDTRACK dataset. It obtained a MODA of 0.569 and an F-score of 0.78, superior\nto state-of-the-art generalizable detection techniques.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 20:58:25 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Lima", "Jo\u00e3o Paulo", ""], ["Roberto", "Rafael", ""], ["Figueiredo", "Lucas", ""], ["Sim\u00f5es", "Francisco", ""], ["Teichrieb", "Veronica", ""]]}, {"id": "2104.05823", "submitter": "Derek Gloudemans", "authors": "Derek Gloudemans, Daniel B. Work", "title": "Localization-Based Tracking", "comments": "10 pages, 5 figures, please email author for supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  End-to-end production of object tracklets from high resolution video in\nreal-time and with high accuracy remains a challenging problem due to the cost\nof object detection on each frame. In this work we present Localization-based\nTracking (LBT), an extension to any tracker that follows the tracking by\ndetection or joint detection and tracking paradigms. Localization-based\nTracking focuses only on regions likely to contain objects to boost detection\nspeed and avoid matching errors. We evaluate LBT as an extension to two example\ntrackers (KIOU and SORT) on the UA-DETRAC and MOT20 datasets. LBT-extended\ntrackers outperform all other reported algorithms in terms of PR-MOTA, PR-MOTP,\nand mostly tracked objects on the UA-DETRAC benchmark, establishing a new\nstate-of-the art. relative to tracking by detection with KIOU, LBT-extended\nKIOU achieves a 25% higher frame-rate and is 1.1% more accurate in terms of\nPR-MOTA on the UA-DETRAC dataset. LBT-extended SORT achieves a 62% speedup and\na 3.2% increase in PR-MOTA on the UA-DETRAC dataset. On MOT20, LBT-extended\nKIOU has a 50% higher frame-rate than tracking by detection and is 0.4% more\naccurate in terms of MOTA. As of submission time, our LBT-extended KIOU tracker\nplaces 10th overall on the MOT20 benchmark.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 21:13:55 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Gloudemans", "Derek", ""], ["Work", "Daniel B.", ""]]}, {"id": "2104.05833", "submitter": "Daiqing Li", "authors": "Daiqing Li, Junlin Yang, Karsten Kreis, Antonio Torralba, Sanja Fidler", "title": "Semantic Segmentation with Generative Models: Semi-Supervised Learning\n  and Strong Out-of-Domain Generalization", "comments": "CVPR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Training deep networks with limited labeled data while achieving a strong\ngeneralization ability is key in the quest to reduce human annotation efforts.\nThis is the goal of semi-supervised learning, which exploits more widely\navailable unlabeled data to complement small labeled data sets. In this paper,\nwe propose a novel framework for discriminative pixel-level tasks using a\ngenerative model of both images and labels. Concretely, we learn a generative\nadversarial network that captures the joint image-label distribution and is\ntrained efficiently using a large set of unlabeled images supplemented with\nonly few labeled ones. We build our architecture on top of StyleGAN2, augmented\nwith a label synthesis branch. Image labeling at test time is achieved by first\nembedding the target image into the joint latent space via an encoder network\nand test-time optimization, and then generating the label from the inferred\nembedding. We evaluate our approach in two important domains: medical image\nsegmentation and part-based face segmentation. We demonstrate strong in-domain\nperformance compared to several baselines, and are the first to showcase\nextreme out-of-domain generalization, such as transferring from CT to MRI in\nmedical imaging, and photographs of real faces to paintings, sculptures, and\neven cartoons and animal faces. Project Page:\n\\url{https://nv-tlabs.github.io/semanticGAN/}\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 21:41:25 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Li", "Daiqing", ""], ["Yang", "Junlin", ""], ["Kreis", "Karsten", ""], ["Torralba", "Antonio", ""], ["Fidler", "Sanja", ""]]}, {"id": "2104.05845", "submitter": "Yue Yang", "authors": "Yue Yang, Artemis Panagopoulou, Qing Lyu, Li Zhang, Mark Yatskar,\n  Chris Callison-Burch", "title": "Visual Goal-Step Inference using wikiHow", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Procedural events can often be thought of as a high level goal composed of a\nsequence of steps. Inferring the sub-sequence of steps of a goal can help\nartificial intelligence systems reason about human activities. Past work in NLP\nhas examined the task of goal-step inference for text. We introduce the visual\nanalogue. We propose the Visual Goal-Step Inference (VGSI) task where a model\nis given a textual goal and must choose a plausible step towards that goal from\namong four candidate images. Our task is challenging for state-of-the-art\nmuitimodal models. We introduce a novel dataset harvested from wikiHow that\nconsists of 772,294 images representing human actions. We show that the\nknowledge learned from our data can effectively transfer to other datasets like\nHowTo100M, increasing the multiple-choice accuracy by 15% to 20%. Our task will\nfacilitate multi-modal reasoning about procedural events.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 22:20:09 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Yang", "Yue", ""], ["Panagopoulou", "Artemis", ""], ["Lyu", "Qing", ""], ["Zhang", "Li", ""], ["Yatskar", "Mark", ""], ["Callison-Burch", "Chris", ""]]}, {"id": "2104.05858", "submitter": "Qing Lian", "authors": "Qing Lian, Botao Ye, Ruijia Xu, Weilong Yao, Tong Zhang", "title": "Geometry-aware data augmentation for monocular 3D object detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper focuses on monocular 3D object detection, one of the essential\nmodules in autonomous driving systems. A key challenge is that the depth\nrecovery problem is ill-posed in monocular data. In this work, we first conduct\na thorough analysis to reveal how existing methods fail to robustly estimate\ndepth when different geometry shifts occur. In particular, through a series of\nimage-based and instance-based manipulations for current detectors, we\nillustrate existing detectors are vulnerable in capturing the consistent\nrelationships between depth and both object apparent sizes and positions. To\nalleviate this issue and improve the robustness of detectors, we convert the\naforementioned manipulations into four corresponding 3D-aware data augmentation\ntechniques. At the image-level, we randomly manipulate the camera system,\nincluding its focal length, receptive field and location, to generate new\ntraining images with geometric shifts. At the instance level, we crop the\nforeground objects and randomly paste them to other scenes to generate new\ntraining instances. All the proposed augmentation techniques share the virtue\nthat geometry relationships in objects are preserved while their geometry is\nmanipulated. In light of the proposed data augmentation methods, not only the\ninstability of depth recovery is effectively alleviated, but also the final 3D\ndetection performance is significantly improved. This leads to superior\nimprovements on the KITTI and nuScenes monocular 3D detection benchmarks with\nstate-of-the-art results.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 23:12:48 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Lian", "Qing", ""], ["Ye", "Botao", ""], ["Xu", "Ruijia", ""], ["Yao", "Weilong", ""], ["Zhang", "Tong", ""]]}, {"id": "2104.05888", "submitter": "Xingjian Zhen", "authors": "Xingjian Zhen, Rudrasis Chakraborty, Vikas Singh", "title": "Simpler Certified Radius Maximization by Propagating Covariances", "comments": "This paper has been accepted by CVPR 2021 as an oral presentation. An\n  introduction video can be found: https://youtu.be/m1ya2oNf5iE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One strategy for adversarially training a robust model is to maximize its\ncertified radius -- the neighborhood around a given training sample for which\nthe model's prediction remains unchanged. The scheme typically involves\nanalyzing a \"smoothed\" classifier where one estimates the prediction\ncorresponding to Gaussian samples in the neighborhood of each sample in the\nmini-batch, accomplished in practice by Monte Carlo sampling. In this paper, we\ninvestigate the hypothesis that this sampling bottleneck can potentially be\nmitigated by identifying ways to directly propagate the covariance matrix of\nthe smoothed distribution through the network. To this end, we find that other\nthan certain adjustments to the network, propagating the covariances must also\nbe accompanied by additional accounting that keeps track of how the\ndistributional moments transform and interact at each stage in the network. We\nshow how satisfying these criteria yields an algorithm for maximizing the\ncertified radius on datasets including Cifar-10, ImageNet, and Places365 while\noffering runtime savings on networks with moderate depth, with a small\ncompromise in overall accuracy. We describe the details of the key\nmodifications that enable practical use. Via various experiments, we evaluate\nwhen our simplifications are sensible, and what the key benefits and\nlimitations are.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 01:38:36 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Zhen", "Xingjian", ""], ["Chakraborty", "Rudrasis", ""], ["Singh", "Vikas", ""]]}, {"id": "2104.05889", "submitter": "Shumit Saha", "authors": "Zabir Al Nazi, Fazla Rabbi Mashrur, Md Amirul Islam, Shumit Saha", "title": "Fibro-CoSANet: Pulmonary Fibrosis Prognosis Prediction using a\n  Convolutional Self Attention Network", "comments": "12 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Idiopathic pulmonary fibrosis (IPF) is a restrictive interstitial lung\ndisease that causes lung function decline by lung tissue scarring. Although\nlung function decline is assessed by the forced vital capacity (FVC),\ndetermining the accurate progression of IPF remains a challenge. To address\nthis challenge, we proposed Fibro-CoSANet, a novel end-to-end multi-modal\nlearning-based approach, to predict the FVC decline. Fibro-CoSANet utilized CT\nimages and demographic information in convolutional neural network frameworks\nwith a stacked attention layer. Extensive experiments on the OSIC Pulmonary\nFibrosis Progression Dataset demonstrated the superiority of our proposed\nFibro-CoSANet by achieving the new state-of-the-art modified Laplace\nLog-Likelihood score of -6.68. This network may benefit research areas\nconcerned with designing networks to improve the prognostic accuracy of IPF.\nThe source-code for Fibro-CoSANet is available at:\n\\url{https://github.com/zabir-nabil/Fibro-CoSANet}.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 01:44:08 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Nazi", "Zabir Al", ""], ["Mashrur", "Fazla Rabbi", ""], ["Islam", "Md Amirul", ""], ["Saha", "Shumit", ""]]}, {"id": "2104.05892", "submitter": "Jong Chul Ye", "authors": "Yujin Oh and Jong Chul Ye", "title": "Unifying domain adaptation and self-supervised learning for CXR\n  segmentation via AdaIN-based knowledge distillation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the segmentation labels are scarce, extensive researches have been\nconducted to train segmentation networks without labels or with only limited\nlabels. In particular, domain adaptation, self-supervised learning, and\nteacher-student architecture have been introduced to distill knowledge from\nvarious tasks to improve the segmentation performance. However, these\napproaches appear different from each other, so it is not clear how these\nseemingly different approaches can be combined for better performance. Inspired\nby the recent StarGANv2 for multi-domain image translation, here we propose a\nnovel segmentation framework via AdaIN-based knowledge distillation, where a\nsingle generator with AdaIN layers is trained along with the AdaIN code\ngenerator and style encoder so that the generator can perform both domain\nadaptation and segmentation. Specifically, our framework is designed to deal\nwith difficult situations in chest X-ray (CXR) segmentation tasks where\nsegmentation masks are only available for normal CXR data, but the trained\nmodel should be applied for both normal and abnormal CXR images. Since a single\ngenerator is used for abnormal to normal domain conversion and segmentation by\nsimply changing the AdaIN codes, the generator can synergistically learn the\ncommon features to improve segmentation performance. Experimental results using\nCXR data confirm that the trained network can achieve the state-of-the art\nsegmentation performance for both normal and abnormal CXR images.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 01:53:04 GMT"}, {"version": "v2", "created": "Fri, 23 Apr 2021 14:54:50 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Oh", "Yujin", ""], ["Ye", "Jong Chul", ""]]}, {"id": "2104.05893", "submitter": "Anna Rohrbach", "authors": "Grace Luo, Trevor Darrell, Anna Rohrbach", "title": "NewsCLIPpings: Automatic Generation of Out-of-Context Multimodal Media", "comments": "8 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The threat of online misinformation is hard to overestimate, with adversaries\nrelying on a range of tools, from cheap fakes to sophisticated deep fakes. We\nare motivated by a threat scenario where an image is being used out of context\nto support a certain narrative expressed in a caption. While some prior\ndatasets for detecting image-text inconsistency can be solved with blind models\ndue to linguistic cues introduced by text manipulation, we propose a dataset\nwhere both image and text are unmanipulated but mismatched. We introduce\nseveral strategies for automatic retrieval of suitable images for the given\ncaptions, capturing cases with related semantics but inconsistent entities as\nwell as matching entities but inconsistent semantic context. Our large-scale\nautomatically generated NewsCLIPpings Dataset requires models to jointly\nanalyze both modalities and to reason about entity mismatch as well as semantic\nmismatch between text and images in news media.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 01:53:26 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Luo", "Grace", ""], ["Darrell", "Trevor", ""], ["Rohrbach", "Anna", ""]]}, {"id": "2104.05895", "submitter": "Pei Wang", "authors": "Pei Wang, Yijun Li, Krishna Kumar Singh, Jingwan Lu, Nuno Vasconcelos", "title": "IMAGINE: Image Synthesis by Image-Guided Model Inversion", "comments": "Published in CVPR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce an inversion based method, denoted as IMAge-Guided model\nINvErsion (IMAGINE), to generate high-quality and diverse images from only a\nsingle training sample. We leverage the knowledge of image semantics from a\npre-trained classifier to achieve plausible generations via matching\nmulti-level feature representations in the classifier, associated with\nadversarial training with an external discriminator. IMAGINE enables the\nsynthesis procedure to simultaneously 1) enforce semantic specificity\nconstraints during the synthesis, 2) produce realistic images without generator\ntraining, and 3) give users intuitive control over the generation process. With\nextensive experimental results, we demonstrate qualitatively and quantitatively\nthat IMAGINE performs favorably against state-of-the-art GAN-based and\ninversion-based methods, across three different image domains (i.e., objects,\nscenes, and textures).\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 02:00:24 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Wang", "Pei", ""], ["Li", "Yijun", ""], ["Singh", "Krishna Kumar", ""], ["Lu", "Jingwan", ""], ["Vasconcelos", "Nuno", ""]]}, {"id": "2104.05901", "submitter": "Wenqi Huang", "authors": "Wenqi Huang, Sen Jia, Ziwen Ke, Zhuo-Xu Cui, Jing Cheng, Yanjie Zhu\n  and Dong Liang", "title": "SRR-Net: A Super-Resolution-Involved Reconstruction Method for High\n  Resolution MR Imaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Improving the image resolution and acquisition speed of magnetic resonance\nimaging (MRI) is a challenging problem. There are mainly two strategies dealing\nwith the speed-resolution trade-off: (1) $k$-space undersampling with\nhigh-resolution acquisition, and (2) a pipeline of lower resolution image\nreconstruction and image super-resolution. However, these approaches either\nhave limited performance at certain high acceleration factor or suffer from the\nerror accumulation of two-step structure. In this paper, we combine the idea of\nMR reconstruction and image super-resolution, and work on recovering HR images\nfrom low-resolution under-sampled $k$-space data directly. Particularly, the\nSR-involved reconstruction can be formulated as a variational problem, and a\nlearnable network unrolled from its solution algorithm is proposed. A\ndiscriminator was introduced to enhance the detail refining performance.\nExperiment results using in-vivo HR multi-coil brain data indicate that the\nproposed SRR-Net is capable of recovering high-resolution brain images with\nboth good visual quality and perceptual quality.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 02:19:12 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Huang", "Wenqi", ""], ["Jia", "Sen", ""], ["Ke", "Ziwen", ""], ["Cui", "Zhuo-Xu", ""], ["Cheng", "Jing", ""], ["Zhu", "Yanjie", ""], ["Liang", "Dong", ""]]}, {"id": "2104.05916", "submitter": "Bin Liao", "authors": "Bin Liao, Jinlong Hu", "title": "MESD: Exploring Optical Flow Assessment on Edge of Motion Objects with\n  Motion Edge Structure Difference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The optical flow estimation has been assessed in various applications. In\nthis paper, we propose a novel method named motion edge structure\ndifference(MESD) to assess estimation errors of optical flow fields on edge of\nmotion objects. We implement comparison experiments for MESD by evaluating five\nrepresentative optical flow algorithms on four popular benchmarks: MPI Sintel,\nMiddlebury, KITTI 2012 and KITTI 2015. Our experimental results demonstrate\nthat MESD can reasonably and discriminatively assess estimation errors of\noptical flow fields on motion edge. The results indicate that MESD could be a\nsupplementary metric to existing general assessment metrics for evaluating\noptical flow algorithms in related computer vision applications.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 03:27:41 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Liao", "Bin", ""], ["Hu", "Jinlong", ""]]}, {"id": "2104.05921", "submitter": "Xinyi Zhang", "authors": "Xinyi Zhang, Chengfang Fang, Jie Shi", "title": "Thief, Beware of What Get You There: Towards Understanding Model\n  Extraction Attack", "comments": "8 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model extraction increasingly attracts research attentions as keeping\ncommercial AI models private can retain a competitive advantage. In some\nscenarios, AI models are trained proprietarily, where neither pre-trained\nmodels nor sufficient in-distribution data is publicly available. Model\nextraction attacks against these models are typically more devastating.\nTherefore, in this paper, we empirically investigate the behaviors of model\nextraction under such scenarios. We find the effectiveness of existing\ntechniques significantly affected by the absence of pre-trained models. In\naddition, the impacts of the attacker's hyperparameters, e.g. model\narchitecture and optimizer, as well as the utilities of information retrieved\nfrom queries, are counterintuitive. We provide some insights on explaining the\npossible causes of these phenomena. With these observations, we formulate model\nextraction attacks into an adaptive framework that captures these factors with\ndeep reinforcement learning. Experiments show that the proposed framework can\nbe used to improve existing techniques, and show that model extraction is still\npossible in such strict scenarios. Our research can help system designers to\nconstruct better defense strategies based on their scenarios.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 03:46:59 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Zhang", "Xinyi", ""], ["Fang", "Chengfang", ""], ["Shi", "Jie", ""]]}, {"id": "2104.05932", "submitter": "Shubham Shrivastava", "authors": "Shubham Shrivastava", "title": "VR3Dense: Voxel Representation Learning for 3D Object Detection and\n  Monocular Dense Depth Reconstruction", "comments": "7 pages, 4 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D object detection and dense depth estimation are one of the most vital\ntasks in autonomous driving. Multiple sensor modalities can jointly attribute\ntowards better robot perception, and to that end, we introduce a method for\njointly training 3D object detection and monocular dense depth reconstruction\nneural networks. It takes as inputs, a LiDAR point-cloud, and a single RGB\nimage during inference and produces object pose predictions as well as a\ndensely reconstructed depth map. LiDAR point-cloud is converted into a set of\nvoxels, and its features are extracted using 3D convolution layers, from which\nwe regress object pose parameters. Corresponding RGB image features are\nextracted using another 2D convolutional neural network. We further use these\ncombined features to predict a dense depth map. While our object detection is\ntrained in a supervised manner, the depth prediction network is trained with\nboth self-supervised and supervised loss functions. We also introduce a loss\nfunction, edge-preserving smooth loss, and show that this results in better\ndepth estimation compared to the edge-aware smooth loss function, frequently\nused in depth prediction works.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 04:25:54 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Shrivastava", "Shubham", ""]]}, {"id": "2104.05940", "submitter": "Bin Wang", "authors": "Kaitai Zhang, Bin Wang, Hong-Shuo Chen, Ye Wang, Shiyu Mou, and C.-C.\n  Jay Kuo", "title": "Dynamic Texture Synthesis by Incorporating Long-range Spatial and\n  Temporal Correlations", "comments": "7 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main challenge of dynamic texture synthesis lies in how to maintain\nspatial and temporal consistency in synthesized videos. The major drawback of\nexisting dynamic texture synthesis models comes from poor treatment of the\nlong-range texture correlation and motion information. To address this problem,\nwe incorporate a new loss term, called the Shifted Gram loss, to capture the\nstructural and long-range correlation of the reference texture video.\nFurthermore, we introduce a frame sampling strategy to exploit long-period\nmotion across multiple frames. With these two new techniques, the application\nscope of existing texture synthesis models can be extended. That is, they can\nsynthesize not only homogeneous but also structured dynamic texture patterns.\nThorough experimental results are provided to demonstrate that our proposed\ndynamic texture synthesis model offers state-of-the-art visual performance.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 05:04:51 GMT"}, {"version": "v2", "created": "Wed, 14 Apr 2021 04:15:31 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Zhang", "Kaitai", ""], ["Wang", "Bin", ""], ["Chen", "Hong-Shuo", ""], ["Wang", "Ye", ""], ["Mou", "Shiyu", ""], ["Kuo", "C. -C. Jay", ""]]}, {"id": "2104.05965", "submitter": "Jae Won Cho", "authors": "Jae Won Cho, Dong-Jin Kim, Jinsoo Choi, Yunjae Jung, In So Kweon", "title": "Dealing with Missing Modalities in the Visual Question Answer-Difference\n  Prediction Task through Knowledge Distillation", "comments": "To appear in CVPR MULA Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we address the issues of missing modalities that have arisen\nfrom the Visual Question Answer-Difference prediction task and find a novel\nmethod to solve the task at hand. We address the missing modality-the ground\ntruth answers-that are not present at test time and use a privileged knowledge\ndistillation scheme to deal with the issue of the missing modality. In order to\nefficiently do so, we first introduce a model, the \"Big\" Teacher, that takes\nthe image/question/answer triplet as its input and outperforms the baseline,\nthen use a combination of models to distill knowledge to a target network\n(student) that only takes the image/question pair as its inputs. We experiment\nour models on the VizWiz and VQA-V2 Answer Difference datasets and show through\nextensive experimentation and ablation the performances of our method and a\ndiverse possibility for future research.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 06:41:11 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Cho", "Jae Won", ""], ["Kim", "Dong-Jin", ""], ["Choi", "Jinsoo", ""], ["Jung", "Yunjae", ""], ["Kweon", "In So", ""]]}, {"id": "2104.05969", "submitter": "Yongri Piao", "authors": "Yongri Piao, Yukun Zhang, Miao Zhang, Xinxin Ji", "title": "Dynamic Fusion Network For Light Field Depth Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Focus based methods have shown promising results for the task of depth\nestimation. However, most existing focus based depth estimation approaches\ndepend on maximal sharpness of the focal stack. Out of focus information in the\nfocal stack poses challenges for this task. In this paper, we propose a\ndynamically multi modal learning strategy which incorporates RGB data and the\nfocal stack in our framework. Our goal is to deeply excavate the spatial\ncorrelation in the focal stack by designing the spatial correlation perception\nmodule and dynamically fuse multi modal information between RGB data and the\nfocal stack in a adaptive way by designing the multi modal dynamic fusion\nmodule. The success of our method is demonstrated by achieving the state of the\nart performance on two datasets. Furthermore, we test our network on a set of\ndifferent focused images generated by a smart phone camera to prove that the\nproposed method not only broke the limitation of only using light field data,\nbut also open a path toward practical applications of depth estimation on\ncommon consumer level cameras data.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 06:45:11 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Piao", "Yongri", ""], ["Zhang", "Yukun", ""], ["Zhang", "Miao", ""], ["Ji", "Xinxin", ""]]}, {"id": "2104.05970", "submitter": "Yuxin Fang", "authors": "Shusheng Yang, Yuxin Fang, Xinggang Wang, Yu Li, Chen Fang, Ying Shan,\n  Bin Feng, Wenyu Liu", "title": "Crossover Learning for Fast Online Video Instance Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling temporal visual context across frames is critical for video instance\nsegmentation (VIS) and other video understanding tasks. In this paper, we\npropose a fast online VIS model named CrossVIS. For temporal information\nmodeling in VIS, we present a novel crossover learning scheme that uses the\ninstance feature in the current frame to pixel-wisely localize the same\ninstance in other frames. Different from previous schemes, crossover learning\ndoes not require any additional network parameters for feature enhancement. By\nintegrating with the instance segmentation loss, crossover learning enables\nefficient cross-frame instance-to-pixel relation learning and brings cost-free\nimprovement during inference. Besides, a global balanced instance embedding\nbranch is proposed for more accurate and more stable online instance\nassociation. We conduct extensive experiments on three challenging VIS\nbenchmarks, \\ie, YouTube-VIS-2019, OVIS, and YouTube-VIS-2021 to evaluate our\nmethods. To our knowledge, CrossVIS achieves state-of-the-art performance among\nall online VIS methods and shows a decent trade-off between latency and\naccuracy. Code will be available to facilitate future research.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 06:47:40 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Yang", "Shusheng", ""], ["Fang", "Yuxin", ""], ["Wang", "Xinggang", ""], ["Li", "Yu", ""], ["Fang", "Chen", ""], ["Shan", "Ying", ""], ["Feng", "Bin", ""], ["Liu", "Wenyu", ""]]}, {"id": "2104.05971", "submitter": "Yongri Piao", "authors": "Yongri Piao, Xinxin Ji, Miao Zhang, Yukun Zhang", "title": "Learning Multi-modal Information for Robust Light Field Depth Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Light field data has been demonstrated to facilitate the depth estimation\ntask. Most learning-based methods estimate the depth infor-mation from EPI or\nsub-aperture images, while less methods pay attention to the focal stack.\nExisting learning-based depth estimation methods from the focal stack lead to\nsuboptimal performance because of the defocus blur. In this paper, we propose a\nmulti-modal learning method for robust light field depth estimation. We first\nexcavate the internal spatial correlation by designing a context reasoning unit\nwhich separately extracts comprehensive contextual information from the focal\nstack and RGB images. Then we integrate the contextual information by\nexploiting a attention-guide cross-modal fusion module. Extensive experiments\ndemonstrate that our method achieves superior performance than existing\nrepresentative methods on two light field datasets. Moreover, visual results on\na mobile phone dataset show that our method can be widely used in daily life.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 06:51:27 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Piao", "Yongri", ""], ["Ji", "Xinxin", ""], ["Zhang", "Miao", ""], ["Zhang", "Yukun", ""]]}, {"id": "2104.05978", "submitter": "Djamila Aouada", "authors": "Mohamed Adel Musallam, Kassem Al Ismaeil, Oyebade Oyedotun, Marcos\n  Damian Perez, Michel Poucet, Djamila Aouada", "title": "SPARK: SPAcecraft Recognition leveraging Knowledge of Space Environment", "comments": "5 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper proposes the SPARK dataset as a new unique space object\nmulti-modal image dataset. Image-based object recognition is an important\ncomponent of Space Situational Awareness, especially for applications such as\non-orbit servicing, active debris removal, and satellite formation. However,\nthe lack of sufficient annotated space data has limited research efforts in\ndeveloping data-driven spacecraft recognition approaches. The SPARK dataset has\nbeen generated under a realistic space simulation environment, with a large\ndiversity in sensing conditions for different orbital scenarios. It provides\nabout 150k images per modality, RGB and depth, and 11 classes for spacecrafts\nand debris. This dataset offers an opportunity to benchmark and further develop\nobject recognition, classification and detection algorithms, as well as\nmulti-modal RGB-Depth approaches under space sensing conditions. Preliminary\nexperimental evaluation validates the relevance of the data, and highlights\ninteresting challenging scenarios specific to the space environment.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 07:16:55 GMT"}, {"version": "v2", "created": "Wed, 14 Apr 2021 01:58:18 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Musallam", "Mohamed Adel", ""], ["Ismaeil", "Kassem Al", ""], ["Oyedotun", "Oyebade", ""], ["Perez", "Marcos Damian", ""], ["Poucet", "Michel", ""], ["Aouada", "Djamila", ""]]}, {"id": "2104.05981", "submitter": "Shailaja Keyur Sampat", "authors": "Shailaja Keyur Sampat, Akshay Kumar, Yezhou Yang and Chitta Baral", "title": "CLEVR_HYP: A Challenge Dataset and Baselines for Visual Question\n  Answering with Hypothetical Actions over Images", "comments": "16 pages, 11 figures, Accepted as a Long Paper at NAACL-HLT 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Most existing research on visual question answering (VQA) is limited to\ninformation explicitly present in an image or a video. In this paper, we take\nvisual understanding to a higher level where systems are challenged to answer\nquestions that involve mentally simulating the hypothetical consequences of\nperforming specific actions in a given scenario. Towards that end, we formulate\na vision-language question answering task based on the CLEVR (Johnson et. al.,\n2017) dataset. We then modify the best existing VQA methods and propose\nbaseline solvers for this task. Finally, we motivate the development of better\nvision-language models by providing insights about the capability of diverse\narchitectures to perform joint reasoning over image-text modality. Our dataset\nsetup scripts and codes will be made publicly available at\nhttps://github.com/shailaja183/clevr_hyp.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 07:29:21 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Sampat", "Shailaja Keyur", ""], ["Kumar", "Akshay", ""], ["Yang", "Yezhou", ""], ["Baral", "Chitta", ""]]}, {"id": "2104.05988", "submitter": "Marcel B\\\"uhler", "authors": "Marcel C. B\\\"uhler (1), Abhimitra Meka (2), Gengyan Li (1 and 2),\n  Thabo Beeler (2), Otmar Hilliges (1) ((1) ETH Zurich, (2) Google)", "title": "VariTex: Variational Neural Face Textures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Deep generative models have recently demonstrated the ability to synthesize\nphotorealistic images of human faces with novel identities. A key challenge to\nthe wide applicability of such techniques is to provide independent control\nover semantically meaningful parameters: appearance, head pose, face shape, and\nfacial expressions. In this paper, we propose VariTex - to the best of our\nknowledge the first method that learns a variational latent feature space of\nneural face textures, which allows sampling of novel identities. We combine\nthis generative model with a parametric face model and gain explicit control\nover head pose and facial expressions. To generate images of complete human\nheads, we propose an additive decoder that generates plausible additional\ndetails such as hair. A novel training scheme enforces a pose independent\nlatent space and in consequence, allows learning of a one-to-many mapping\nbetween latent codes and pose-conditioned exterior regions. The resulting\nmethod can generate geometrically consistent images of novel identities\nallowing fine-grained control over head pose, face shape, and facial\nexpressions, facilitating a broad range of downstream tasks, like sampling\nnovel identities, re-posing, expression transfer, and more.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 07:47:53 GMT"}, {"version": "v2", "created": "Wed, 23 Jun 2021 17:24:03 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["B\u00fchler", "Marcel C.", "", "ETH Zurich"], ["Meka", "Abhimitra", "", "Google"], ["Li", "Gengyan", "", "1 and 2"], ["Beeler", "Thabo", "", "Google"], ["Hilliges", "Otmar", "", "ETH Zurich"]]}, {"id": "2104.06008", "submitter": "Zongshen Mu", "authors": "Zongshen Mu, Siliang Tang, Jie Tan, Qiang Yu, Yueting Zhuang", "title": "Disentangled Motif-aware Graph Learning for Phrase Grounding", "comments": "10 pages, 6 figures, AAAI 2021 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel graph learning framework for phrase\ngrounding in the image. Developing from the sequential to the dense graph\nmodel, existing works capture coarse-grained context but fail to distinguish\nthe diversity of context among phrases and image regions. In contrast, we pay\nspecial attention to different motifs implied in the context of the scene graph\nand devise the disentangled graph network to integrate the motif-aware\ncontextual information into representations. Besides, we adopt interventional\nstrategies at the feature and the structure levels to consolidate and\ngeneralize representations. Finally, the cross-modal attention network is\nutilized to fuse intra-modal features, where each phrase can be computed\nsimilarity with regions to select the best-grounded one. We validate the\nefficiency of disentangled and interventional graph network (DIGN) through a\nseries of ablation studies, and our model achieves state-of-the-art performance\non Flickr30K Entities and ReferIt Game benchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 08:20:07 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Mu", "Zongshen", ""], ["Tang", "Siliang", ""], ["Tan", "Jie", ""], ["Yu", "Qiang", ""], ["Zhuang", "Yueting", ""]]}, {"id": "2104.06031", "submitter": "Erik Franz", "authors": "Erik Franz, Barbara Solenthaler, Nils Thuerey", "title": "Global Transport for Fluid Reconstruction with Learned Self-Supervision", "comments": "CVPR 2021 oral, source code:\n  https://github.com/tum-pbs/Global-Flow-Transport", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV physics.flu-dyn", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel method to reconstruct volumetric flows from sparse views\nvia a global transport formulation. Instead of obtaining the space-time\nfunction of the observations, we reconstruct its motion based on a single\ninitial state. In addition we introduce a learned self-supervision that\nconstrains observations from unseen angles. These visual constraints are\ncoupled via the transport constraints and a differentiable rendering step to\narrive at a robust end-to-end reconstruction algorithm. This makes the\nreconstruction of highly realistic flow motions possible, even from only a\nsingle input view. We show with a variety of synthetic and real flows that the\nproposed global reconstruction of the transport process yields an improved\nreconstruction of the fluid motion.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 09:00:46 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Franz", "Erik", ""], ["Solenthaler", "Barbara", ""], ["Thuerey", "Nils", ""]]}, {"id": "2104.06041", "submitter": "Liang Peng", "authors": "Liang Peng, Fei Liu, Senbo Yan, Xiaofei He, Deng Cai", "title": "OCM3D: Object-Centric Monocular 3D Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image-only and pseudo-LiDAR representations are commonly used for monocular\n3D object detection. However, methods based on them have shortcomings of either\nnot well capturing the spatial relationships in neighbored image pixels or\nbeing hard to handle the noisy nature of the monocular pseudo-LiDAR point\ncloud. To overcome these issues, in this paper we propose a novel\nobject-centric voxel representation tailored for monocular 3D object detection.\nSpecifically, voxels are built on each object proposal, and their sizes are\nadaptively determined by the 3D spatial distribution of the points, allowing\nthe noisy point cloud to be organized effectively within a voxel grid. This\nrepresentation is proved to be able to locate the object in 3D space\naccurately. Furthermore, prior works would like to estimate the orientation via\ndeep features extracted from an entire image or a noisy point cloud. By\ncontrast, we argue that the local RoI information from the object image patch\nalone with a proper resizing scheme is a better input as it provides complete\nsemantic clues meanwhile excludes irrelevant interferences. Besides, we\ndecompose the confidence mechanism in monocular 3D object detection by\nconsidering the relationship between 3D objects and the associated 2D boxes.\nEvaluated on KITTI, our method outperforms state-of-the-art methods by a large\nmargin. The code will be made publicly available soon.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 09:15:40 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Peng", "Liang", ""], ["Liu", "Fei", ""], ["Yan", "Senbo", ""], ["He", "Xiaofei", ""], ["Cai", "Deng", ""]]}, {"id": "2104.06059", "submitter": "Zahra Gharaee", "authors": "Zahra Gharaee and Peter G\\\"ardenfors and Magnus Johnsson", "title": "First and Second Order Dynamics in a Hierarchical SOM system for Action\n  Recognition", "comments": null, "journal-ref": null, "doi": "10.1016/j.asoc.2017.06.007", "report-no": null, "categories": "cs.CV cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Human recognition of the actions of other humans is very efficient and is\nbased on patterns of movements. Our theoretical starting point is that the\ndynamics of the joint movements is important to action categorization. On the\nbasis of this theory, we present a novel action recognition system that employs\na hierarchy of Self-Organizing Maps together with a custom supervised neural\nnetwork that learns to categorize actions. The system preprocesses the input\nfrom a Kinect like 3D camera to exploit the information not only about joint\npositions, but also their first and second order dynamics. We evaluate our\nsystem in two experiments with publicly available data sets, and compare its\nperformance to the performance with less sophisticated preprocessing of the\ninput. The results show that including the dynamics of the actions improves the\nperformance. We also apply an attention mechanism that focuses on the parts of\nthe body that are the most involved in performing the actions.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 09:46:40 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Gharaee", "Zahra", ""], ["G\u00e4rdenfors", "Peter", ""], ["Johnsson", "Magnus", ""]]}, {"id": "2104.06064", "submitter": "Jakob Bo\\v{z}i\\v{c}", "authors": "Jakob Bo\\v{z}i\\v{c}, Domen Tabernik, Danijel Sko\\v{c}aj", "title": "Mixed supervision for surface-defect detection: from weakly to fully\n  supervised learning", "comments": "Accepted for publication in Computers in Industry", "journal-ref": null, "doi": "10.1016/j.compind.2021.103459", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep-learning methods have recently started being employed for addressing\nsurface-defect detection problems in industrial quality control. However, with\na large amount of data needed for learning, often requiring high-precision\nlabels, many industrial problems cannot be easily solved, or the cost of the\nsolutions would significantly increase due to the annotation requirements. In\nthis work, we relax heavy requirements of fully supervised learning methods and\nreduce the need for highly detailed annotations. By proposing a deep-learning\narchitecture, we explore the use of annotations of different details ranging\nfrom weak (image-level) labels through mixed supervision to full (pixel-level)\nannotations on the task of surface-defect detection. The proposed end-to-end\narchitecture is composed of two sub-networks yielding defect segmentation and\nclassification results. The proposed method is evaluated on several datasets\nfor industrial quality inspection: KolektorSDD, DAGM and Severstal Steel\nDefect. We also present a new dataset termed KolektorSDD2 with over 3000 images\ncontaining several types of defects, obtained while addressing a real-world\nindustrial problem. We demonstrate state-of-the-art results on all four\ndatasets. The proposed method outperforms all related approaches in fully\nsupervised settings and also outperforms weakly-supervised methods when only\nimage-level labels are available. We also show that mixed supervision with only\na handful of fully annotated samples added to weakly labelled training images\ncan result in performance comparable to the fully supervised model's\nperformance but at a significantly lower annotation cost.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 10:00:10 GMT"}, {"version": "v2", "created": "Mon, 19 Apr 2021 08:44:32 GMT"}, {"version": "v3", "created": "Tue, 20 Apr 2021 14:09:32 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Bo\u017ei\u010d", "Jakob", ""], ["Tabernik", "Domen", ""], ["Sko\u010daj", "Danijel", ""]]}, {"id": "2104.06070", "submitter": "Zahra Gharaee", "authors": "Zahra Gharaee and Peter G\\\"ardenfors and Magnus Johnsson", "title": "Online Recognition of Actions Involving Objects", "comments": null, "journal-ref": null, "doi": "10.1016/j.bica.2017.09.007", "report-no": null, "categories": "cs.RO cs.CV cs.HC cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present an online system for real time recognition of actions involving\nobjects working in online mode. The system merges two streams of information\nprocessing running in parallel. One is carried out by a hierarchical\nself-organizing map (SOM) system that recognizes the performed actions by\nanalysing the spatial trajectories of the agent's movements. It consists of two\nlayers of SOMs and a custom made supervised neural network. The activation\nsequences in the first layer SOM represent the sequences of significant\npostures of the agent during the performance of actions. These activation\nsequences are subsequently recoded and clustered in the second layer SOM, and\nthen labeled by the activity in the third layer custom made supervised neural\nnetwork. The second information processing stream is carried out by a second\nsystem that determines which object among several in the agent's vicinity the\naction is applied to. This is achieved by applying a proximity measure. The\npresented method combines the two information processing streams to determine\nwhat action the agent performed and on what object. The action recognition\nsystem has been tested with excellent performance.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 10:08:20 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Gharaee", "Zahra", ""], ["G\u00e4rdenfors", "Peter", ""], ["Johnsson", "Magnus", ""]]}, {"id": "2104.06083", "submitter": "Zhenhong Sun", "authors": "Zhenhong Sun, Zhiyu Tan, Xiuyu Sun, Fangyi Zhang, Dongyang Li, Yichen\n  Qian, Hao Li", "title": "Spatiotemporal Entropy Model is All You Need for Learned Video\n  Compression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The framework of dominant learned video compression methods is usually\ncomposed of motion prediction modules as well as motion vector and residual\nimage compression modules, suffering from its complex structure and error\npropagation problem. Approaches have been proposed to reduce the complexity by\nreplacing motion prediction modules with implicit flow networks. Error\npropagation aware training strategy is also proposed to alleviate incremental\nreconstruction errors from previously decoded frames. Although these methods\nhave brought some improvement, little attention has been paid to the framework\nitself. Inspired by the success of learned image compression through\nsimplifying the framework with a single deep neural network, it is natural to\nexpect a better performance in video compression via a simple yet appropriate\nframework. Therefore, we propose a framework to directly compress raw-pixel\nframes (rather than residual images), where no extra motion prediction module\nis required. Instead, an entropy model is used to estimate the spatiotemporal\nredundancy in a latent space rather than pixel level, which significantly\nreduces the complexity of the framework. Specifically, the whole framework is a\ncompression module, consisting of a unified auto-encoder which produces\nidentically distributed latents for all frames, and a spatiotemporal entropy\nestimation model to minimize the entropy of these latents. Experiments showed\nthat the proposed method outperforms state-of-the-art (SOTA) performance under\nthe metric of multiscale structural similarity (MS-SSIM) and achieves\ncompetitive results under the metric of PSNR.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 10:38:32 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Sun", "Zhenhong", ""], ["Tan", "Zhiyu", ""], ["Sun", "Xiuyu", ""], ["Zhang", "Fangyi", ""], ["Li", "Dongyang", ""], ["Qian", "Yichen", ""], ["Li", "Hao", ""]]}, {"id": "2104.06087", "submitter": "Dwarikanath Mahapatra", "authors": "Dwarikanath Mahapatra", "title": "Interpretability-Driven Sample Selection Using Self Supervised Learning\n  For Disease Classification And Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In supervised learning for medical image analysis, sample selection\nmethodologies are fundamental to attain optimum system performance promptly and\nwith minimal expert interactions (e.g. label querying in an active learning\nsetup). In this paper we propose a novel sample selection methodology based on\ndeep features leveraging information contained in interpretability saliency\nmaps. In the absence of ground truth labels for informative samples, we use a\nnovel self supervised learning based approach for training a classifier that\nlearns to identify the most informative sample in a given batch of images. We\ndemonstrate the benefits of the proposed approach, termed\nInterpretability-Driven Sample Selection (IDEAL), in an active learning setup\naimed at lung disease classification and histopathology image segmentation. We\nanalyze three different approaches to determine sample informativeness from\ninterpretability saliency maps: (i) an observational model stemming from\nfindings on previous uncertainty-based sample selection approaches, (ii) a\nradiomics-based model, and (iii) a novel data-driven self-supervised approach.\nWe compare IDEAL to other baselines using the publicly available NIH chest\nX-ray dataset for lung disease classification, and a public histopathology\nsegmentation dataset (GLaS), demonstrating the potential of using\ninterpretability information for sample selection in active learning systems.\nResults show our proposed self supervised approach outperforms other approaches\nin selecting informative samples leading to state of the art performance with\nfewer samples.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 10:46:33 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Mahapatra", "Dwarikanath", ""]]}, {"id": "2104.06094", "submitter": "Yan Zhao", "authors": "Yan Zhao, Weicong Chen, Xu Tan, Kai Huang, Jin Xu, Changhu Wang, and\n  Jihong Zhu", "title": "Improving Long-Tailed Classification from Instance Level", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data in the real world tends to exhibit a long-tailed label distribution,\nwhich poses great challenges for neural networks in classification. Existing\nmethods tackle this problem mainly from the coarse-grained class level,\nignoring the difference among instances, e.g., hard samples vs. easy samples.\nIn this paper, we revisit the long-tailed problem from the instance level and\npropose two instance-level components to improve long-tailed classification.\nThe first one is an Adaptive Logit Adjustment (ALA) loss, which applies an\nadaptive adjusting term to the logit. Different from the adjusting terms in\nexisting methods that are class-dependent and only focus on tail classes, we\ncarefully design an instance-specific term and add it on the class-dependent\nterm to make the network pay more attention to not only tailed class, but more\nimportantly hard samples. The second one is a Mixture-of-Experts (MoE) network,\nwhich contains a multi-expert module and an instance-aware routing module. The\nrouting module is designed to dynamically integrate the results of multiple\nexperts according to each input instance, and is trained jointly with the\nexperts network in an end-to-end manner.Extensive experiment results show that\nour method outperforms the state-of-the-art methods by 1% to 5% on common\nlong-tailed benchmarks including ImageNet-LT and iNaturalist.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 11:00:19 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Zhao", "Yan", ""], ["Chen", "Weicong", ""], ["Tan", "Xu", ""], ["Huang", "Kai", ""], ["Xu", "Jin", ""], ["Wang", "Changhu", ""], ["Zhu", "Jihong", ""]]}, {"id": "2104.06114", "submitter": "Bowen Cheng", "authors": "Bowen Cheng, Lu Sheng, Shaoshuai Shi, Ming Yang, Dong Xu", "title": "Back-tracing Representative Points for Voting-based 3D Object Detection\n  in Point Clouds", "comments": "CVPR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D object detection in point clouds is a challenging vision task that\nbenefits various applications for understanding the 3D visual world. Lots of\nrecent research focuses on how to exploit end-to-end trainable Hough voting for\ngenerating object proposals. However, the current voting strategy can only\nreceive partial votes from the surfaces of potential objects together with\nsevere outlier votes from the cluttered backgrounds, which hampers full\nutilization of the information from the input point clouds. Inspired by the\nback-tracing strategy in the conventional Hough voting methods, in this work,\nwe introduce a new 3D object detection method, named as Back-tracing\nRepresentative Points Network (BRNet), which generatively back-traces the\nrepresentative points from the vote centers and also revisits complementary\nseed points around these generated points, so as to better capture the fine\nlocal structural features surrounding the potential objects from the raw point\nclouds. Therefore, this bottom-up and then top-down strategy in our BRNet\nenforces mutual consistency between the predicted vote centers and the raw\nsurface points and thus achieves more reliable and flexible object localization\nand class prediction results. Our BRNet is simple but effective, which\nsignificantly outperforms the state-of-the-art methods on two large-scale point\ncloud datasets, ScanNet V2 (+7.5% in terms of mAP@0.50) and SUN RGB-D (+4.7% in\nterms of mAP@0.50), while it is still lightweight and efficient. Code will be\navailable at https://github.com/cheng052/BRNet.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 11:39:42 GMT"}, {"version": "v2", "created": "Wed, 14 Apr 2021 06:38:30 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Cheng", "Bowen", ""], ["Sheng", "Lu", ""], ["Shi", "Shaoshuai", ""], ["Yang", "Ming", ""], ["Xu", "Dong", ""]]}, {"id": "2104.06118", "submitter": "Haedong Jeong", "authors": "Ali Tousi, Haedong Jeong, Jiyeon Han, Hwanil Choi and Jaesik Choi", "title": "Automatic Correction of Internal Units in Generative Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Networks (GANs) have shown satisfactory performance in\nsynthetic image generation by devising complex network structure and\nadversarial training scheme. Even though GANs are able to synthesize realistic\nimages, there exists a number of generated images with defective visual\npatterns which are known as artifacts. While most of the recent work tries to\nfix artifact generations by perturbing latent code, few investigate internal\nunits of a generator to fix them. In this work, we devise a method that\nautomatically identifies the internal units generating various types of\nartifact images. We further propose the sequential correction algorithm which\nadjusts the generation flow by modifying the detected artifact units to improve\nthe quality of generation while preserving the original outline. Our method\noutperforms the baseline method in terms of FID-score and shows satisfactory\nresults with human evaluation.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 11:46:45 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Tousi", "Ali", ""], ["Jeong", "Haedong", ""], ["Han", "Jiyeon", ""], ["Choi", "Hwanil", ""], ["Choi", "Jaesik", ""]]}, {"id": "2104.06142", "submitter": "Pramod Chunduri", "authors": "Pramod Chunduri, Jaeho Bang, Yao Lu, Joy Arulraj", "title": "Zeus: Efficiently Localizing Actions in Videos using Reinforcement\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DB", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Detection and localization of actions in videos is an important problem in\npractice. A traffic analyst might be interested in studying the patterns in\nwhich vehicles move at a given intersection. State-of-the-art video analytics\nsystems are unable to efficiently and effectively answer such action queries.\nThe reasons are threefold. First, action detection and localization tasks\nrequire computationally expensive deep neural networks. Second, actions are\noften rare events. Third, actions are spread across a sequence of frames. It is\nimportant to take the entire sequence of frames into context for effectively\nanswering the query. It is critical to quickly skim through the irrelevant\nparts of the video to answer the action query efficiently.\n  In this paper, we present Zeus, a video analytics system tailored for\nanswering action queries. We propose a novel technique for efficiently\nanswering these queries using a deep reinforcement learning agent. Zeus trains\nan agent that learns to adaptively modify the input video segments to an action\nclassification network. The agent alters the input segments along three\ndimensions -- sampling rate, segment length, and resolution. Besides\nefficiency, Zeus is capable of answering the query at a user-specified target\naccuracy using a query optimizer that trains the agent based on an\naccuracy-aware reward function. Our evaluation of Zeus on a novel action\nlocalization dataset shows that it outperforms the state-of-the-art frame- and\nwindow-based techniques by up to 1.4x and 3x, respectively. Furthermore, unlike\nthe frame-based technique, it satisfies the user-specified target accuracy\nacross all the queries, at up to 2x higher accuracy, than frame-based methods.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 16:38:31 GMT"}, {"version": "v2", "created": "Mon, 19 Apr 2021 03:20:48 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Chunduri", "Pramod", ""], ["Bang", "Jaeho", ""], ["Lu", "Yao", ""], ["Arulraj", "Joy", ""]]}, {"id": "2104.06148", "submitter": "Jun Wan", "authors": "Ajian Liu, Chenxu Zhao, Zitong Yu, Jun Wan, Anyang Su, Xing Liu,\n  Zichang Tan, Sergio Escalera, Junliang Xing, Yanyan Liang, Guodong Guo, Zhen\n  Lei, Stan Z. Li and Du Zhang", "title": "Contrastive Context-Aware Learning for 3D High-Fidelity Mask Face\n  Presentation Attack Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face presentation attack detection (PAD) is essential to secure face\nrecognition systems primarily from high-fidelity mask attacks. Most existing 3D\nmask PAD benchmarks suffer from several drawbacks: 1) a limited number of mask\nidentities, types of sensors, and a total number of videos; 2) low-fidelity\nquality of facial masks. Basic deep models and remote photoplethysmography\n(rPPG) methods achieved acceptable performance on these benchmarks but still\nfar from the needs of practical scenarios. To bridge the gap to real-world\napplications, we introduce a largescale High-Fidelity Mask dataset, namely\nCASIA-SURF HiFiMask (briefly HiFiMask). Specifically, a total amount of 54,600\nvideos are recorded from 75 subjects with 225 realistic masks by 7 new kinds of\nsensors. Together with the dataset, we propose a novel Contrastive\nContext-aware Learning framework, namely CCL. CCL is a new training methodology\nfor supervised PAD tasks, which is able to learn by leveraging rich contexts\naccurately (e.g., subjects, mask material and lighting) among pairs of live\nfaces and high-fidelity mask attacks. Extensive experimental evaluations on\nHiFiMask and three additional 3D mask datasets demonstrate the effectiveness of\nour method.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 12:48:38 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Liu", "Ajian", ""], ["Zhao", "Chenxu", ""], ["Yu", "Zitong", ""], ["Wan", "Jun", ""], ["Su", "Anyang", ""], ["Liu", "Xing", ""], ["Tan", "Zichang", ""], ["Escalera", "Sergio", ""], ["Xing", "Junliang", ""], ["Liang", "Yanyan", ""], ["Guo", "Guodong", ""], ["Lei", "Zhen", ""], ["Li", "Stan Z.", ""], ["Zhang", "Du", ""]]}, {"id": "2104.06162", "submitter": "Xudong Xu", "authors": "Xudong Xu, Hang Zhou, Ziwei Liu, Bo Dai, Xiaogang Wang, Dahua Lin", "title": "Visually Informed Binaural Audio Generation without Binaural Audios", "comments": "Accepted by CVPR 2021. Code, models, and demo video are available on\n  our webpage: \\<https://sheldontsui.github.io/projects/PseudoBinaural>", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CV cs.MM eess.AS", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Stereophonic audio, especially binaural audio, plays an essential role in\nimmersive viewing environments. Recent research has explored generating\nvisually guided stereophonic audios supervised by multi-channel audio\ncollections. However, due to the requirement of professional recording devices,\nexisting datasets are limited in scale and variety, which impedes the\ngeneralization of supervised methods in real-world scenarios. In this work, we\npropose PseudoBinaural, an effective pipeline that is free of binaural\nrecordings. The key insight is to carefully build pseudo visual-stereo pairs\nwith mono data for training. Specifically, we leverage spherical harmonic\ndecomposition and head-related impulse response (HRIR) to identify the\nrelationship between spatial locations and received binaural audios. Then in\nthe visual modality, corresponding visual cues of the mono data are manually\nplaced at sound source positions to form the pairs. Compared to\nfully-supervised paradigms, our binaural-recording-free pipeline shows great\nstability in cross-dataset evaluation and achieves comparable performance under\nsubjective preference. Moreover, combined with binaural recordings, our method\nis able to further boost the performance of binaural audio generation under\nsupervised settings.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 13:07:33 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Xu", "Xudong", ""], ["Zhou", "Hang", ""], ["Liu", "Ziwei", ""], ["Dai", "Bo", ""], ["Wang", "Xiaogang", ""], ["Lin", "Dahua", ""]]}, {"id": "2104.06164", "submitter": "Jacopo Teneggi", "authors": "Jacopo Teneggi, Alexandre Luster, Jeremias Sulam", "title": "Fast Hierarchical Games for Image Explanations", "comments": "20 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As modern complex neural networks keep breaking records and solving harder\nproblems, their predictions also become less and less intelligible. The current\nlack of interpretability often undermines the deployment of accurate machine\nlearning tools in sensitive settings. In this work, we present a model-agnostic\nexplanation method for image classification based on a hierarchical extension\nof Shapley coefficients --Hierarchical Shap (h-Shap)-- that resolves some of\nthe limitations of current approaches. Unlike other Shapley-based explanation\nmethods, h-Shap is scalable and can be computed without the need of\napproximation. Under certain distributional assumptions, such as those common\nin multiple instance learning, h-Shap retrieves the exact Shapley coefficients\nwith an exponential improvement in computational complexity. We compare our\nhierarchical approach with popular Shapley-based and non-Shapley-based methods\non a synthetic dataset, a medical imaging scenario, and a general computer\nvision problem, showing that h-Shap outperforms the state of the art in both\naccuracy and runtime. Code and experiments are made publicly available.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 13:11:02 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Teneggi", "Jacopo", ""], ["Luster", "Alexandre", ""], ["Sulam", "Jeremias", ""]]}, {"id": "2104.06165", "submitter": "Shang Sun", "authors": "Shang Sun, Yunan Zheng, Xuelei Shi, Zhenyu Xu, Yiguang Liu", "title": "PHI-MVS: Plane Hypothesis Inference Multi-view Stereo for Large-Scale\n  Scene Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  PatchMatch based Multi-view Stereo (MVS) algorithms have achieved great\nsuccess in large-scale scene reconstruction tasks. However, reconstruction of\ntexture-less planes often fails as similarity measurement methods may become\nineffective on these regions. Thus, a new plane hypothesis inference strategy\nis proposed to handle the above issue. The procedure consists of two steps:\nFirst, multiple plane hypotheses are generated using filtered initial depth\nmaps on regions that are not successfully recovered; Second, depth hypotheses\nare selected using Markov Random Field (MRF). The strategy can significantly\nimprove the completeness of reconstruction results with only acceptable\ncomputing time increasing. Besides, a new acceleration scheme similar to\ndilated convolution can speed up the depth map estimating process with only a\nslight influence on the reconstruction. We integrated the above ideas into a\nnew MVS pipeline, Plane Hypothesis Inference Multi-view Stereo (PHI-MVS). The\nresult of PHI-MVS is validated on ETH3D public benchmarks, and it demonstrates\ncompeting performance against the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 13:16:00 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Sun", "Shang", ""], ["Zheng", "Yunan", ""], ["Shi", "Xuelei", ""], ["Xu", "Zhenyu", ""], ["Liu", "Yiguang", ""]]}, {"id": "2104.06174", "submitter": "Lingzhi He", "authors": "Lingzhi He, Hongguang Zhu, Feng Li, Huihui Bai, Runmin Cong, Chunjie\n  Zhang, Chunyu Lin, Meiqin Liu, Yao Zhao", "title": "Towards Fast and Accurate Real-World Depth Super-Resolution: Benchmark\n  Dataset and Baseline", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Depth maps obtained by commercial depth sensors are always in low-resolution,\nmaking it difficult to be used in various computer vision tasks. Thus, depth\nmap super-resolution (SR) is a practical and valuable task, which upscales the\ndepth map into high-resolution (HR) space. However, limited by the lack of\nreal-world paired low-resolution (LR) and HR depth maps, most existing methods\nuse downsampling to obtain paired training samples. To this end, we first\nconstruct a large-scale dataset named \"RGB-D-D\", which can greatly promote the\nstudy of depth map SR and even more depth-related real-world tasks. The \"D-D\"\nin our dataset represents the paired LR and HR depth maps captured from mobile\nphone and Lucid Helios respectively ranging from indoor scenes to challenging\noutdoor scenes. Besides, we provide a fast depth map super-resolution (FDSR)\nbaseline, in which the high-frequency component adaptively decomposed from RGB\nimage to guide the depth map SR. Extensive experiments on existing public\ndatasets demonstrate the effectiveness and efficiency of our network compared\nwith the state-of-the-art methods. Moreover, for the real-world LR depth maps,\nour algorithm can produce more accurate HR depth maps with clearer boundaries\nand to some extent correct the depth value errors.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 13:27:26 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["He", "Lingzhi", ""], ["Zhu", "Hongguang", ""], ["Li", "Feng", ""], ["Bai", "Huihui", ""], ["Cong", "Runmin", ""], ["Zhang", "Chunjie", ""], ["Lin", "Chunyu", ""], ["Liu", "Meiqin", ""], ["Zhao", "Yao", ""]]}, {"id": "2104.06176", "submitter": "Pedro Ricardo Ariel Salvador Bassi", "authors": "Pedro R. A. S. Bassi, Romis Attux", "title": "COVID-19 detection using chest X-rays: is lung segmentation important\n  for generalization?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We evaluated the generalization capability of deep neural networks (DNNs),\ntrained to classify chest X-rays as COVID-19, normal or pneumonia, using a\nrelatively small and mixed dataset.\n  We proposed a DNN architecture to perform lung segmentation and\nclassification. It stacks a segmentation module (U-Net), an original\nintermediate module and a classification module (DenseNet201). We compared it\nto a DenseNet201.\n  To evaluate generalization, we tested the DNNs with an external dataset (from\ndistinct localities) and used Bayesian inference to estimate the probability\ndistributions of performance metrics, like F1-Score.\n  Our proposed DNN achieved 0.917 AUC on the external test dataset, and the\nDenseNet, 0.906. Bayesian inference indicated mean accuracy of 76.1% and\n[0.695, 0.826] 95% HDI with segmentation and, without segmentation, 71.7% and\n[0.646, 0.786].\n  We proposed a novel DNN evaluation technique, using Layer-wise Relevance\nPropagation (LRP) and the Brixia score. LRP heatmaps indicated that areas where\nradiologists found strong COVID-19 symptoms and attributed high Brixia scores\nare the most important for the stacked DNN classification.\n  External validation showed smaller accuracies than internal validation,\nindicating dataset bias, which segmentation reduces. Performance in the\nexternal dataset and LRP analysis suggest that DNNs can be trained in small and\nmixed datasets and detect COVID-19.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 09:06:28 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Bassi", "Pedro R. A. S.", ""], ["Attux", "Romis", ""]]}, {"id": "2104.06191", "submitter": "Bruno Lecouat", "authors": "Bruno Lecouat, Jean Ponce, Julien Mairal", "title": "Aliasing is your Ally: End-to-End Super-Resolution from Raw Image Bursts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This presentation addresses the problem of reconstructing a high-resolution\nimage from multiple lower-resolution snapshots captured from slightly different\nviewpoints in space and time. Key challenges for solving this problem include\n(i) aligning the input pictures with sub-pixel accuracy, (ii) handling raw\n(noisy) images for maximal faithfulness to native camera data, and (iii)\ndesigning/learning an image prior (regularizer) well suited to the task. We\naddress these three challenges with a hybrid algorithm building on the insight\nfrom Wronski et al. that aliasing is an ally in this setting, with parameters\nthat can be learned end to end, while retaining the interpretability of\nclassical approaches to inverse problems. The effectiveness of our approach is\ndemonstrated on synthetic and real image bursts, setting a new state of the art\non several benchmarks and delivering excellent qualitative results on real raw\nbursts captured by smartphones and prosumer cameras.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 13:39:43 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Lecouat", "Bruno", ""], ["Ponce", "Jean", ""], ["Mairal", "Julien", ""]]}, {"id": "2104.06193", "submitter": "Konstantin Ushenin", "authors": "Garnik Vareldzhan, Kirill Yurkov, Konstantin Ushenin", "title": "Anomaly Detection in Image Datasets Using Convolutional Neural Networks,\n  Center Loss, and Mahalanobis Distance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  User activities generate a significant number of poor-quality or irrelevant\nimages and data vectors that cannot be processed in the main data processing\npipeline or included in the training dataset. Such samples can be found with\nmanual analysis by an expert or with anomalous detection algorithms. There are\nseveral formal definitions for the anomaly samples. For neural networks, the\nanomalous is usually defined as out-of-distribution samples. This work proposes\nmethods for supervised and semi-supervised detection of out-of-distribution\nsamples in image datasets. Our approach extends a typical neural network that\nsolves the image classification problem. Thus, one neural network after\nextension can solve image classification and anomalous detection problems\nsimultaneously. Proposed methods are based on the center loss and its effect on\na deep feature distribution in a last hidden layer of the neural network. This\npaper provides an analysis of the proposed methods for the LeNet and\nEfficientNet-B0 on the MNIST and ImageNet-30 datasets.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 13:44:03 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Vareldzhan", "Garnik", ""], ["Yurkov", "Kirill", ""], ["Ushenin", "Konstantin", ""]]}, {"id": "2104.06219", "submitter": "Hubert P. H. Shum", "authors": "Daniel Organisciak, Brian K. S. Isaac-Medina, Matthew Poyser, Shanfeng\n  Hu, Toby P. Breckon, Hubert P. H. Shum", "title": "UAV-ReID: A Benchmark on Unmanned Aerial Vehicle Re-identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As unmanned aerial vehicles (UAVs) become more accessible with a growing\nrange of applications, the potential risk of UAV disruption increases. Recent\ndevelopment in deep learning allows vision-based counter-UAV systems to detect\nand track UAVs with a single camera. However, the coverage of a single camera\nis limited, necessitating the need for multicamera configurations to match UAVs\nacross cameras - a problem known as re-identification (reID). While there has\nbeen extensive research on person and vehicle reID to match objects across time\nand viewpoints, to the best of our knowledge, there has been no research in UAV\nreID. UAVs are challenging to re-identify: they are much smaller than\npedestrians and vehicles and they are often detected in the air so appear at a\ngreater range of angles. Because no UAV data sets currently use multiple\ncameras, we propose the first new UAV re-identification data set, UAV-reID,\nthat facilitates the development of machine learning solutions in this emerging\narea. UAV-reID has two settings: Temporally-Near to evaluate performance across\nviews to assist tracking frameworks, and Big-to-Small to evaluate reID\nperformance across scale and to allow early reID when UAVs are detected from a\nlong distance. We conduct a benchmark study by extensively evaluating different\nreID backbones and loss functions. We demonstrate that with the right setup,\ndeep networks are powerful enough to learn good representations for UAVs,\nachieving 81.9% mAP on the Temporally-Near setting and 46.5% on the challenging\nBig-to-Small setting. Furthermore, we find that vision transformers are the\nmost robust to extreme variance of scale.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 14:13:09 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Organisciak", "Daniel", ""], ["Isaac-Medina", "Brian K. S.", ""], ["Poyser", "Matthew", ""], ["Hu", "Shanfeng", ""], ["Breckon", "Toby P.", ""], ["Shum", "Hubert P. H.", ""]]}, {"id": "2104.06231", "submitter": "Tongxue Zhou", "authors": "Tongxue Zhou, St\\'ephane Canu, Pierre Vera, Su Ruan", "title": "Latent Correlation Representation Learning for Brain Tumor Segmentation\n  with Missing MRI Modalities", "comments": "12 pages, 10 figures, accepted by IEEE Transactions on Image\n  Processing (8 April 2021). arXiv admin note: text overlap with\n  arXiv:2003.08870, arXiv:2102.03111", "journal-ref": "IEEE Transactions on Image Processing On page(s): 4263-4274 Print\n  ISSN: 1057-7149 Online ISSN: 1941-0042", "doi": "10.1109/TIP.2021.3070752", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Magnetic Resonance Imaging (MRI) is a widely used imaging technique to assess\nbrain tumor. Accurately segmenting brain tumor from MR images is the key to\nclinical diagnostics and treatment planning. In addition, multi-modal MR images\ncan provide complementary information for accurate brain tumor segmentation.\nHowever, it's common to miss some imaging modalities in clinical practice. In\nthis paper, we present a novel brain tumor segmentation algorithm with missing\nmodalities. Since it exists a strong correlation between multi-modalities, a\ncorrelation model is proposed to specially represent the latent multi-source\ncorrelation. Thanks to the obtained correlation representation, the\nsegmentation becomes more robust in the case of missing modality. First, the\nindividual representation produced by each encoder is used to estimate the\nmodality independent parameter. Then, the correlation model transforms all the\nindividual representations to the latent multi-source correlation\nrepresentations. Finally, the correlation representations across modalities are\nfused via attention mechanism into a shared representation to emphasize the\nmost important features for segmentation. We evaluate our model on BraTS 2018\nand BraTS 2019 dataset, it outperforms the current state-of-the-art methods and\nproduces robust results when one or more modalities are missing.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 14:21:09 GMT"}, {"version": "v2", "created": "Tue, 20 Apr 2021 13:51:09 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Zhou", "Tongxue", ""], ["Canu", "St\u00e9phane", ""], ["Vera", "Pierre", ""], ["Ruan", "Su", ""]]}, {"id": "2104.06237", "submitter": "Micha\\\"el Defferrard", "authors": "Jelena Banjac, Laur\\`ene Donati, Micha\\\"el Defferrard", "title": "Learning to recover orientations from projections in single-particle\n  cryo-EM", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV q-bio.QM stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A major challenge in single-particle cryo-electron microscopy (cryo-EM) is\nthat the orientations adopted by the 3D particles prior to imaging are unknown;\nyet, this knowledge is essential for high-resolution reconstruction. We present\na method to recover these orientations directly from the acquired set of 2D\nprojections. Our approach consists of two steps: (i) the estimation of\ndistances between pairs of projections, and (ii) the recovery of the\norientation of each projection from these distances. In step (i), pairwise\ndistances are estimated by a Siamese neural network trained on synthetic\ncryo-EM projections from resolved bio-structures. In step (ii), orientations\nare recovered by minimizing the difference between the distances estimated from\nthe projections and the distances induced by the recovered orientations. We\nevaluated the method on synthetic cryo-EM datasets. Current results demonstrate\nthat orientations can be accurately recovered from projections that are shifted\nand corrupted with a high level of noise. The accuracy of the recovery depends\non the accuracy of the distance estimator. While not yet deployed in a real\nexperimental setup, the proposed method offers a novel learning-based take on\norientation recovery in SPA. Our code is available at\nhttps://github.com/JelenaBanjac/protein-reconstruction\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 14:31:37 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Banjac", "Jelena", ""], ["Donati", "Laur\u00e8ne", ""], ["Defferrard", "Micha\u00ebl", ""]]}, {"id": "2104.06243", "submitter": "Xintong Li", "authors": "Chen Li, Xintong Li, Xiaoyan Li, Md Mamunur Rahaman, Xiaoqi Li, Jian\n  Wu, Yudong Yao, Marcin Grzegorzek", "title": "A State-of-the-art Survey of Artificial Neural Networks for Whole-slide\n  Image Analysis:from Popular Convolutional Neural Networks to Potential Visual\n  Transformers", "comments": "22 pages, 38 figures. arXiv admin note: substantial text overlap with\n  arXiv:2102.10553", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In recent years, with the advancement of computer-aided diagnosis (CAD)\ntechnology and whole slide image (WSI), histopathological WSI has gradually\nplayed a crucial aspect in the diagnosis and analysis of diseases. To increase\nthe objectivity and accuracy of pathologists' work, artificial neural network\n(ANN) methods have been generally needed in the segmentation, classification,\nand detection of histopathological WSI. In this paper, WSI analysis methods\nbased on ANN are reviewed. Firstly, the development status of WSI and ANN\nmethods is introduced. Secondly, we summarize the common ANN methods. Next, we\ndiscuss publicly available WSI datasets and evaluation metrics. These ANN\narchitectures for WSI processing are divided into classical neural networks and\ndeep neural networks (DNNs) and then analyzed. Finally, the application\nprospect of the analytical method in this field is discussed. The important\npotential method is Visual Transformers.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 14:39:33 GMT"}, {"version": "v2", "created": "Tue, 4 May 2021 06:09:58 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Li", "Chen", ""], ["Li", "Xintong", ""], ["Li", "Xiaoyan", ""], ["Rahaman", "Md Mamunur", ""], ["Li", "Xiaoqi", ""], ["Wu", "Jian", ""], ["Yao", "Yudong", ""], ["Grzegorzek", "Marcin", ""]]}, {"id": "2104.06279", "submitter": "Yihao Liu", "authors": "Yihao Liu, Jingwen He, Xiangyu Chen, Zhengwen Zhang, Hengyuan Zhao,\n  Chao Dong, Yu Qiao", "title": "Very Lightweight Photo Retouching Network with Conditional Sequential\n  Modulation", "comments": "Extended version of CSRNet (ECCV2020). arXiv admin note: substantial\n  text overlap with arXiv:2009.10390", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Photo retouching aims at improving the aesthetic visual quality of images\nthat suffer from photographic defects such as poor contrast, over/under\nexposure, and inharmonious saturation. In practice, photo retouching can be\naccomplished by a series of image processing operations. As most commonly-used\nretouching operations are pixel-independent, i.e., the manipulation on one\npixel is uncorrelated with its neighboring pixels, we can take advantage of\nthis property and design a specialized algorithm for efficient global photo\nretouching. We analyze these global operations and find that they can be\nmathematically formulated by a Multi-Layer Perceptron (MLP). Based on this\nobservation, we propose an extremely lightweight framework -- Conditional\nSequential Retouching Network (CSRNet). Benefiting from the utilization of\n$1\\times1$ convolution, CSRNet only contains less than 37K trainable\nparameters, which are orders of magnitude smaller than existing learning-based\nmethods. Experiments show that our method achieves state-of-the-art performance\non the benchmark MIT-Adobe FiveK dataset quantitively and qualitatively. In\naddition to achieve global photo retouching, the proposed framework can be\neasily extended to learn local enhancement effects. The extended model, namly\nCSRNet-L, also achieves competitive results in various local enhancement tasks.\nCodes will be available.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 15:11:02 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Liu", "Yihao", ""], ["He", "Jingwen", ""], ["Chen", "Xiangyu", ""], ["Zhang", "Zhengwen", ""], ["Zhao", "Hengyuan", ""], ["Dong", "Chao", ""], ["Qiao", "Yu", ""]]}, {"id": "2104.06316", "submitter": "Tarik A. Rashid", "authors": "Arjina Maharjan, Abeer Alsadoon, P.W.C. Prasad, Nada AlSallami, Tarik\n  A. Rashid, Ahmad Alrubaie, Sami Haddad", "title": "A Novel Solution of Using Mixed Reality in Bowel and Oral and\n  Maxillofacial Surgical Telepresence: 3D Mean Value Cloning algorithm", "comments": "27 pages", "journal-ref": "International Journal of Medical Robotics and Computer Assisted\n  Surgery,2020", "doi": "10.1002/rcs.2161", "report-no": null, "categories": "physics.med-ph cs.CV cs.GR cs.RO", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Background and aim: Most of the Mixed Reality models used in the surgical\ntelepresence are suffering from discrepancies in the boundary area and\nspatial-temporal inconsistency due to the illumination variation in the video\nframes. The aim behind this work is to propose a new solution that helps\nproduce the composite video by merging the augmented video of the surgery site\nand the virtual hand of the remote expertise surgeon. The purpose of the\nproposed solution is to decrease the processing time and enhance the accuracy\nof merged video by decreasing the overlay and visualization error and removing\nocclusion and artefacts. Methodology: The proposed system enhanced the mean\nvalue cloning algorithm that helps to maintain the spatial-temporal consistency\nof the final composite video. The enhanced algorithm includes the 3D mean value\ncoordinates and improvised mean value interpolant in the image cloning process,\nwhich helps to reduce the sawtooth, smudging and discolouration artefacts\naround the blending region. Results: As compared to the state of the art\nsolution, the accuracy in terms of overlay error of the proposed solution is\nimproved from 1.01mm to 0.80mm whereas the accuracy in terms of visualization\nerror is improved from 98.8% to 99.4%. The processing time is reduced to 0.173\nseconds from 0.211 seconds. Conclusion: Our solution helps make the object of\ninterest consistent with the light intensity of the target image by adding the\nspace distance that helps maintain the spatial consistency in the final merged\nvideo.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 10:01:06 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Maharjan", "Arjina", ""], ["Alsadoon", "Abeer", ""], ["Prasad", "P. W. C.", ""], ["AlSallami", "Nada", ""], ["Rashid", "Tarik A.", ""], ["Alrubaie", "Ahmad", ""], ["Haddad", "Sami", ""]]}, {"id": "2104.06365", "submitter": "Ian Berlot-Attwell", "authors": "Ian Berlot-Attwell", "title": "Neuro-Symbolic VQA: A review from the perspective of AGI desiderata", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  An ultimate goal of the AI and ML fields is artificial general intelligence\n(AGI); although such systems remain science fiction, various models exhibit\naspects of AGI. In this work, we look at neuro-symbolic (NS)approaches to\nvisual question answering (VQA) from the perspective of AGI desiderata. We see\nhow well these systems meet these desiderata, and how the desiderata often pull\nthe scientist in opposing directions. It is my hope that through this work we\ncan temper model evaluation on benchmarks with a discussion of the properties\nof these systems and their potential for future extension.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 17:23:19 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Berlot-Attwell", "Ian", ""]]}, {"id": "2104.06392", "submitter": "R. Kenny Jones", "authors": "R. Kenny Jones, David Charatan, Paul Guerrero, Niloy J. Mitra, Daniel\n  Ritchie", "title": "ShapeMOD: Macro Operation Discovery for 3D Shape Programs", "comments": "SIGGRAPH 2021. Project Page: https://rkjones4.github.io/shapeMOD.html", "journal-ref": null, "doi": "10.1145/3450626.3459821", "report-no": null, "categories": "cs.GR cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A popular way to create detailed yet easily controllable 3D shapes is via\nprocedural modeling, i.e. generating geometry using programs. Such programs\nconsist of a series of instructions along with their associated parameter\nvalues. To fully realize the benefits of this representation, a shape program\nshould be compact and only expose degrees of freedom that allow for meaningful\nmanipulation of output geometry. One way to achieve this goal is to design\nhigher-level macro operators that, when executed, expand into a series of\ncommands from the base shape modeling language. However, manually authoring\nsuch macros, much like shape programs themselves, is difficult and largely\nrestricted to domain experts. In this paper, we present ShapeMOD, an algorithm\nfor automatically discovering macros that are useful across large datasets of\n3D shape programs. ShapeMOD operates on shape programs expressed in an\nimperative, statement-based language. It is designed to discover macros that\nmake programs more compact by minimizing the number of function calls and free\nparameters required to represent an input shape collection. We run ShapeMOD on\nmultiple collections of programs expressed in a domain-specific language for 3D\nshape structures. We show that it automatically discovers a concise set of\nmacros that abstract out common structural and parametric patterns that\ngeneralize over large shape collections. We also demonstrate that the macros\nfound by ShapeMOD improve performance on downstream tasks including shape\ngenerative modeling and inferring programs from point clouds. Finally, we\nconduct a user study that indicates that ShapeMOD's discovered macros make\ninteractive shape editing more efficient.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 17:54:03 GMT"}, {"version": "v2", "created": "Tue, 22 Jun 2021 20:56:17 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Jones", "R. Kenny", ""], ["Charatan", "David", ""], ["Guerrero", "Paul", ""], ["Mitra", "Niloy J.", ""], ["Ritchie", "Daniel", ""]]}, {"id": "2104.06394", "submitter": "Gyungin Shin", "authors": "Gyungin Shin, Weidi Xie, Samuel Albanie", "title": "All you need are a few pixels: semantic segmentation with PixelPick", "comments": "14 pages, 8 figures; references added", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A central challenge for the task of semantic segmentation is the prohibitive\ncost of obtaining dense pixel-level annotations to supervise model training. In\nthis work, we show that in order to achieve a good level of segmentation\nperformance, all you need are a few well-chosen pixel labels. We make the\nfollowing contributions: (i) We investigate the novel semantic segmentation\nsetting in which labels are supplied only at sparse pixel locations, and show\nthat deep neural networks can use a handful of such labels to good effect; (ii)\nWe demonstrate how to exploit this phenomena within an active learning\nframework, termed PixelPick, to radically reduce labelling cost, and propose an\nefficient \"mouse-free\" annotation strategy to implement our approach; (iii) We\nconduct extensive experiments to study the influence of annotation diversity\nunder a fixed budget, model pretraining, model capacity and the sampling\nmechanism for picking pixels in this low annotation regime; (iv) We provide\ncomparisons to the existing state of the art in semantic segmentation with\nactive learning, and demonstrate comparable performance with up to two orders\nof magnitude fewer pixel annotations on the CamVid, Cityscapes and PASCAL VOC\n2012 benchmarks; (v) Finally, we evaluate the efficiency of our annotation\npipeline and its sensitivity to annotator error to demonstrate its\npracticality.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 17:55:33 GMT"}, {"version": "v2", "created": "Thu, 15 Apr 2021 17:04:20 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Shin", "Gyungin", ""], ["Xie", "Weidi", ""], ["Albanie", "Samuel", ""]]}, {"id": "2104.06397", "submitter": "Soumyadip Sengupta", "authors": "Daniel Lichy, Jiaye Wu, Soumyadip Sengupta, David W. Jacobs", "title": "Shape and Material Capture at Home", "comments": "Accepted to CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we present a technique for estimating the geometry and\nreflectance of objects using only a camera, flashlight, and optionally a\ntripod. We propose a simple data capture technique in which the user goes\naround the object, illuminating it with a flashlight and capturing only a few\nimages. Our main technical contribution is the introduction of a recursive\nneural architecture, which can predict geometry and reflectance at 2^{k}*2^{k}\nresolution given an input image at 2^{k}*2^{k} and estimated geometry and\nreflectance from the previous step at 2^{k-1}*2^{k-1}. This recursive\narchitecture, termed RecNet, is trained with 256x256 resolution but can easily\noperate on 1024x1024 images during inference. We show that our method produces\nmore accurate surface normal and albedo, especially in regions of specular\nhighlights and cast shadows, compared to previous approaches, given three or\nfewer input images. For the video and code, please visit the project website\nhttp://dlichy.github.io/ShapeAndMaterialAtHome/.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 17:57:34 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Lichy", "Daniel", ""], ["Wu", "Jiaye", ""], ["Sengupta", "Soumyadip", ""], ["Jacobs", "David W.", ""]]}, {"id": "2104.06399", "submitter": "Weijian Xu", "authors": "Weijian Xu, Yifan Xu, Tyler Chang, Zhuowen Tu", "title": "Co-Scale Conv-Attentional Image Transformers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present Co-scale conv-attentional image Transformers\n(CoaT), a Transformer-based image classifier equipped with co-scale and\nconv-attentional mechanisms. First, the co-scale mechanism maintains the\nintegrity of Transformers' encoder branches at individual scales, while\nallowing representations learned at different scales to effectively communicate\nwith each other; we design a series of serial and parallel blocks to realize\nthe co-scale attention mechanism. Second, we devise a conv-attentional\nmechanism by realizing a relative position embedding formulation in the\nfactorized attention module with an efficient convolution-like implementation.\nCoaT empowers image Transformers with enriched multi-scale and contextual\nmodeling capabilities. On ImageNet, relatively small CoaT models attain\nsuperior classification results compared with the similar-sized convolutional\nneural networks and image/vision Transformers. The effectiveness of CoaT's\nbackbone is also illustrated on object detection and instance segmentation,\ndemonstrating its applicability to the downstream computer vision tasks.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 17:58:29 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Xu", "Weijian", ""], ["Xu", "Yifan", ""], ["Chang", "Tyler", ""], ["Tu", "Zhuowen", ""]]}, {"id": "2104.06401", "submitter": "Triantafyllos Afouras", "authors": "Triantafyllos Afouras, Yuki M. Asano, Francois Fagan, Andrea Vedaldi,\n  Florian Metze", "title": "Self-supervised object detection from audio-visual correspondence", "comments": "Under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We tackle the problem of learning object detectors without supervision.\nDifferently from weakly-supervised object detection, we do not assume\nimage-level class labels. Instead, we extract a supervisory signal from\naudio-visual data, using the audio component to \"teach\" the object detector.\nWhile this problem is related to sound source localisation, it is considerably\nharder because the detector must classify the objects by type, enumerate each\ninstance of the object, and do so even when the object is silent. We tackle\nthis problem by first designing a self-supervised framework with a contrastive\nobjective that jointly learns to classify and localise objects. Then, without\nusing any supervision, we simply use these self-supervised labels and boxes to\ntrain an image-based object detector. With this, we outperform previous\nunsupervised and weakly-supervised detectors for the task of object detection\nand sound source localization. We also show that we can align this detector to\nground-truth classes with as little as one label per pseudo-class, and show how\nour method can learn to detect generic objects that go beyond instruments, such\nas airplanes and cats.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 17:59:03 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Afouras", "Triantafyllos", ""], ["Asano", "Yuki M.", ""], ["Fagan", "Francois", ""], ["Vedaldi", "Andrea", ""], ["Metze", "Florian", ""]]}, {"id": "2104.06402", "submitter": "Esther Robb", "authors": "Ting-I Hsieh, Esther Robb, Hwann-Tzong Chen, Jia-Bin Huang", "title": "DropLoss for Long-Tail Instance Segmentation", "comments": "Code at https://github.com/timy90022/DropLoss", "journal-ref": "AAAI 2021", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Long-tailed class distributions are prevalent among the practical\napplications of object detection and instance segmentation. Prior work in\nlong-tail instance segmentation addresses the imbalance of losses between rare\nand frequent categories by reducing the penalty for a model incorrectly\npredicting a rare class label. We demonstrate that the rare categories are\nheavily suppressed by correct background predictions, which reduces the\nprobability for all foreground categories with equal weight. Due to the\nrelative infrequency of rare categories, this leads to an imbalance that biases\ntowards predicting more frequent categories. Based on this insight, we develop\nDropLoss -- a novel adaptive loss to compensate for this imbalance without a\ntrade-off between rare and frequent categories. With this loss, we show\nstate-of-the-art mAP across rare, common, and frequent categories on the LVIS\ndataset.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 17:59:22 GMT"}, {"version": "v2", "created": "Sat, 17 Apr 2021 15:52:56 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Hsieh", "Ting-I", ""], ["Robb", "Esther", ""], ["Chen", "Hwann-Tzong", ""], ["Huang", "Jia-Bin", ""]]}, {"id": "2104.06403", "submitter": "Changqian Yu", "authors": "Changqian Yu, Bin Xiao, Changxin Gao, Lu Yuan, Lei Zhang, Nong Sang,\n  Jingdong Wang", "title": "Lite-HRNet: A Lightweight High-Resolution Network", "comments": "Accepted to CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present an efficient high-resolution network, Lite-HRNet, for human pose\nestimation. We start by simply applying the efficient shuffle block in\nShuffleNet to HRNet (high-resolution network), yielding stronger performance\nover popular lightweight networks, such as MobileNet, ShuffleNet, and Small\nHRNet.\n  We find that the heavily-used pointwise (1x1) convolutions in shuffle blocks\nbecome the computational bottleneck. We introduce a lightweight unit,\nconditional channel weighting, to replace costly pointwise (1x1) convolutions\nin shuffle blocks. The complexity of channel weighting is linear w.r.t the\nnumber of channels and lower than the quadratic time complexity for pointwise\nconvolutions. Our solution learns the weights from all the channels and over\nmultiple resolutions that are readily available in the parallel branches in\nHRNet. It uses the weights as the bridge to exchange information across\nchannels and resolutions, compensating the role played by the pointwise (1x1)\nconvolution. Lite-HRNet demonstrates superior results on human pose estimation\nover popular lightweight networks. Moreover, Lite-HRNet can be easily applied\nto semantic segmentation task in the same lightweight manner. The code and\nmodels have been publicly available at https://github.com/HRNet/Lite-HRNet.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 17:59:31 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Yu", "Changqian", ""], ["Xiao", "Bin", ""], ["Gao", "Changxin", ""], ["Yuan", "Lu", ""], ["Zhang", "Lei", ""], ["Sang", "Nong", ""], ["Wang", "Jingdong", ""]]}, {"id": "2104.06404", "submitter": "Bowen Cheng", "authors": "Bowen Cheng and Omkar Parkhi and Alexander Kirillov", "title": "Pointly-Supervised Instance Segmentation", "comments": "Project page: https://bowenc0221.github.io/point-sup", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose point-based instance-level annotation, a new form of weak\nsupervision for instance segmentation. It combines the standard bounding box\nannotation with labeled points that are uniformly sampled inside each bounding\nbox. We show that the existing instance segmentation models developed for full\nmask supervision, like Mask R-CNN, can be seamlessly trained with the\npoint-based annotation without any major modifications. In our experiments,\nMask R-CNN models trained on COCO, PASCAL VOC, Cityscapes, and LVIS with only\n10 annotated points per object achieve 94%--98% of their fully-supervised\nperformance. The new point-based annotation is approximately 5 times faster to\ncollect than object masks, making high-quality instance segmentation more\naccessible for new data.\n  Inspired by the new annotation form, we propose a modification to PointRend\ninstance segmentation module. For each object, the new architecture, called\nImplicit PointRend, generates parameters for a function that makes the final\npoint-level mask prediction. Implicit PointRend is more straightforward and\nuses a single point-level mask loss. Our experiments show that the new module\nis more suitable for the proposed point-based supervision.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 17:59:40 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Cheng", "Bowen", ""], ["Parkhi", "Omkar", ""], ["Kirillov", "Alexander", ""]]}, {"id": "2104.06405", "submitter": "Chen-Hsuan Lin", "authors": "Chen-Hsuan Lin, Wei-Chiu Ma, Antonio Torralba, Simon Lucey", "title": "BARF: Bundle-Adjusting Neural Radiance Fields", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Neural Radiance Fields (NeRF) have recently gained a surge of interest within\nthe computer vision community for its power to synthesize photorealistic novel\nviews of real-world scenes. One limitation of NeRF, however, is its requirement\nof accurate camera poses to learn the scene representations. In this paper, we\npropose Bundle-Adjusting Neural Radiance Fields (BARF) for training NeRF from\nimperfect (or even unknown) camera poses -- the joint problem of learning\nneural 3D representations and registering camera frames. We establish a\ntheoretical connection to classical image alignment and show that\ncoarse-to-fine registration is also applicable to NeRF. Furthermore, we show\nthat na\\\"ively applying positional encoding in NeRF has a negative impact on\nregistration with a synthesis-based objective. Experiments on synthetic and\nreal-world data show that BARF can effectively optimize the neural scene\nrepresentations and resolve large camera pose misalignment at the same time.\nThis enables view synthesis and localization of video sequences from unknown\ncamera poses, opening up new avenues for visual localization systems (e.g.\nSLAM) and potential applications for dense 3D mapping and reconstruction.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 17:59:51 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Lin", "Chen-Hsuan", ""], ["Ma", "Wei-Chiu", ""], ["Torralba", "Antonio", ""], ["Lucey", "Simon", ""]]}, {"id": "2104.06456", "submitter": "Alican Mertan", "authors": "Alican Mertan, Damien Jade Duff and Gozde Unal", "title": "Single Image Depth Estimation: An Overview", "comments": "This is a preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We review solutions to the problem of depth estimation, arguably the most\nimportant subtask in scene understanding. We focus on the single image depth\nestimation problem. Due to its properties, the single image depth estimation\nproblem is currently best tackled with machine learning methods, most\nsuccessfully with convolutional neural networks. We provide an overview of the\nfield by examining key works. We examine non-deep learning approaches that\nmostly predate deep learning and utilize hand-crafted features and assumptions,\nand more recent works that mostly use deep learning techniques. The single\nimage depth estimation problem is tackled first in a supervised fashion with\nabsolute or relative depth information acquired from human or sensor-labeled\ndata, or in an unsupervised way using unlabelled stereo images or video\ndatasets. We also study multitask approaches that combine the depth estimation\nproblem with related tasks such as semantic segmentation and surface normal\nestimation. Finally, we discuss investigations into the mechanisms, principles,\nand failure cases of contemporary solutions.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 18:58:37 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Mertan", "Alican", ""], ["Duff", "Damien Jade", ""], ["Unal", "Gozde", ""]]}, {"id": "2104.06459", "submitter": "Thomas Eboli", "authors": "Thomas Eboli, Jian Sun and Jean Ponce", "title": "Learning to Jointly Deblur, Demosaick and Denoise Raw Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We address the problem of non-blind deblurring and demosaicking of noisy raw\nimages. We adapt an existing learning-based approach to RGB image deblurring to\nhandle raw images by introducing a new interpretable module that jointly\ndemosaicks and deblurs them. We train this model on RGB images converted into\nraw ones following a realistic invertible camera pipeline. We demonstrate the\neffectiveness of this model over two-stage approaches stacking demosaicking and\ndeblurring modules on quantitive benchmarks. We also apply our approach to\nremove a camera's inherent blur (its color-dependent point-spread function)\nfrom real images, in essence deblurring sharp images.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 19:02:59 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Eboli", "Thomas", ""], ["Sun", "Jian", ""], ["Ponce", "Jean", ""]]}, {"id": "2104.06461", "submitter": "Anoop Cherian", "authors": "Anoop Cherian, Panagiotis Stanitsas, Jue Wang, Mehrtash Harandi,\n  Vassilios Morellas, Nikolaos Papanikolopoulos", "title": "Learning Log-Determinant Divergences for Positive Definite Matrices", "comments": "Accepted at Trans. PAMI (extended version of ICCV 2017 paper). arXiv\n  admin note: substantial text overlap with arXiv:1708.01741", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Representations in the form of Symmetric Positive Definite (SPD) matrices\nhave been popularized in a variety of visual learning applications due to their\ndemonstrated ability to capture rich second-order statistics of visual data.\nThere exist several similarity measures for comparing SPD matrices with\ndocumented benefits. However, selecting an appropriate measure for a given\nproblem remains a challenge and in most cases, is the result of a\ntrial-and-error process. In this paper, we propose to learn similarity measures\nin a data-driven manner. To this end, we capitalize on the \\alpha\\beta-log-det\ndivergence, which is a meta-divergence parametrized by scalars \\alpha and\n\\beta, subsuming a wide family of popular information divergences on SPD\nmatrices for distinct and discrete values of these parameters. Our key idea is\nto cast these parameters in a continuum and learn them from data. We\nsystematically extend this idea to learn vector-valued parameters, thereby\nincreasing the expressiveness of the underlying non-linear measure. We conjoin\nthe divergence learning problem with several standard tasks in machine\nlearning, including supervised discriminative dictionary learning and\nunsupervised SPD matrix clustering. We present Riemannian gradient descent\nschemes for optimizing our formulations efficiently, and show the usefulness of\nour method on eight standard computer vision tasks.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 19:09:43 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Cherian", "Anoop", ""], ["Stanitsas", "Panagiotis", ""], ["Wang", "Jue", ""], ["Harandi", "Mehrtash", ""], ["Morellas", "Vassilios", ""], ["Papanikolopoulos", "Nikolaos", ""]]}, {"id": "2104.06468", "submitter": "Junyu Chen", "authors": "Junyu Chen, Yufan He, Eric C. Frey, Ye Li, Yong Du", "title": "ViT-V-Net: Vision Transformer for Unsupervised Volumetric Medical Image\n  Registration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last decade, convolutional neural networks (ConvNets) have dominated\nand achieved state-of-the-art performances in a variety of medical imaging\napplications. However, the performances of ConvNets are still limited by\nlacking the understanding of long-range spatial relations in an image. The\nrecently proposed Vision Transformer (ViT) for image classification uses a\npurely self-attention-based model that learns long-range spatial relations to\nfocus on the relevant parts of an image. Nevertheless, ViT emphasizes the\nlow-resolution features because of the consecutive downsamplings, result in a\nlack of detailed localization information, making it unsuitable for image\nregistration. Recently, several ViT-based image segmentation methods have been\ncombined with ConvNets to improve the recovery of detailed localization\ninformation. Inspired by them, we present ViT-V-Net, which bridges ViT and\nConvNet to provide volumetric medical image registration. The experimental\nresults presented here demonstrate that the proposed architecture achieves\nsuperior performance to several top-performing registration methods.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 19:18:19 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Chen", "Junyu", ""], ["He", "Yufan", ""], ["Frey", "Eric C.", ""], ["Li", "Ye", ""], ["Du", "Yong", ""]]}, {"id": "2104.06476", "submitter": "Le Thanh Nguyen-Meidine", "authors": "Le Thanh Nguyen-Meidine, Madhu Kiran, Marco Pedersoli, Jose Dolz,\n  Louis-Antoine Blais-Morin, Eric Granger", "title": "Incremental Multi-Target Domain Adaptation for Object Detection with\n  Efficient Domain Transfer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Techniques for multi-target domain adaptation (MTDA) seek to adapt a\nrecognition model such that it can generalize well across multiple target\ndomains. While several successful techniques have been proposed for\nunsupervised single-target domain adaptation (STDA) in object detection,\nadapting a model to multiple target domains using unlabeled image data remains\na challenging and largely unexplored problem. Key challenges include the lack\nof bounding box annotations for target data, knowledge corruption, and the\ngrowing resource requirements needed to train accurate deep detection models.\nThe later requirements are augmented by the need to retraining a model with\nprevious-learned target data when adapting to each new target domain.\nCurrently, the only MTDA technique in literature for object detection relies on\ndistillation with a duplicated model to avoid knowledge corruption but does not\nleverage the source-target feature alignment after UDA. To address these\nchallenges, we propose a new Incremental MTDA technique for object detection\nthat can adapt a detector to multiple target domains, one at a time, without\nhaving to retain data of previously-learned target domains. Instead of\ndistillation, our technique efficiently transfers source images to a joint\ntarget domains' space, on the fly, thereby preserving knowledge during\nincremental MTDA. Using adversarial training, our Domain Transfer Module (DTM)\nis optimized to trick the domain classifiers into classifying source images as\nthough transferred into the target domain, thus allowing the DTM to generate\nsamples close to a joint distribution of target domains. Our proposed technique\nis validated on different MTDA detection benchmarks, and results show it\nimproving accuracy across multiple domains, despite the considerable reduction\nin complexity.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 19:35:54 GMT"}, {"version": "v2", "created": "Mon, 19 Apr 2021 14:41:20 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Nguyen-Meidine", "Le Thanh", ""], ["Kiran", "Madhu", ""], ["Pedersoli", "Marco", ""], ["Dolz", "Jose", ""], ["Blais-Morin", "Louis-Antoine", ""], ["Granger", "Eric", ""]]}, {"id": "2104.06485", "submitter": "Yi Wang", "authors": "Yi Wang, Stefano Zorzi, Ksenia Bittner", "title": "Machine-learned 3D Building Vectorization from Satellite Imagery", "comments": "Accepted to CVPR workshop (EarthVision 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a machine learning based approach for automatic 3D building\nreconstruction and vectorization. Taking a single-channel photogrammetric\ndigital surface model (DSM) and panchromatic (PAN) image as input, we first\nfilter out non-building objects and refine the building shapes of input DSM\nwith a conditional generative adversarial network (cGAN). The refined DSM and\nthe input PAN image are then used through a semantic segmentation network to\ndetect edges and corners of building roofs. Later, a set of vectorization\nalgorithms are proposed to build roof polygons. Finally, the height information\nfrom the refined DSM is added to the polygons to obtain a fully vectorized\nlevel of detail (LoD)-2 building model. We verify the effectiveness of our\nmethod on large-scale satellite images, where we obtain state-of-the-art\nperformance.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 19:57:30 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Wang", "Yi", ""], ["Zorzi", "Stefano", ""], ["Bittner", "Ksenia", ""]]}, {"id": "2104.06490", "submitter": "Yuxuan Zhang", "authors": "Yuxuan Zhang, Huan Ling, Jun Gao, Kangxue Yin, Jean-Francois Lafleche,\n  Adela Barriuso, Antonio Torralba, Sanja Fidler", "title": "DatasetGAN: Efficient Labeled Data Factory with Minimal Human Effort", "comments": "Accepted to CVPR 2021 as an Oral paper. Webpage:\n  https://nv-tlabs.github.io/datasetGAN/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce DatasetGAN: an automatic procedure to generate massive datasets\nof high-quality semantically segmented images requiring minimal human effort.\nCurrent deep networks are extremely data-hungry, benefiting from training on\nlarge-scale datasets, which are time consuming to annotate. Our method relies\non the power of recent GANs to generate realistic images. We show how the GAN\nlatent code can be decoded to produce a semantic segmentation of the image.\nTraining the decoder only needs a few labeled examples to generalize to the\nrest of the latent space, resulting in an infinite annotated dataset generator!\nThese generated datasets can then be used for training any computer vision\narchitecture just as real datasets are. As only a few images need to be\nmanually segmented, it becomes possible to annotate images in extreme detail\nand generate datasets with rich object and part segmentations. To showcase the\npower of our approach, we generated datasets for 7 image segmentation tasks\nwhich include pixel-level labels for 34 human face parts, and 32 car parts. Our\napproach outperforms all semi-supervised baselines significantly and is on par\nwith fully supervised methods, which in some cases require as much as 100x more\nannotated data as our method.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 20:08:29 GMT"}, {"version": "v2", "created": "Tue, 20 Apr 2021 00:27:10 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Zhang", "Yuxuan", ""], ["Ling", "Huan", ""], ["Gao", "Jun", ""], ["Yin", "Kangxue", ""], ["Lafleche", "Jean-Francois", ""], ["Barriuso", "Adela", ""], ["Torralba", "Antonio", ""], ["Fidler", "Sanja", ""]]}, {"id": "2104.06524", "submitter": "Madhu Kiran", "authors": "Madhu Kiran, R Gnana Praveen, Le Thanh Nguyen-Meidine, Soufiane\n  Belharbi, Louis-Antoine Blais-Morin, Eric Granger", "title": "Holistic Guidance for Occluded Person Re-Identification", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In real-world video surveillance applications, person re-identification\n(ReID) suffers from the effects of occlusions and detection errors. Despite\nrecent advances, occlusions continue to corrupt the features extracted by\nstate-of-art CNN backbones, and thereby deteriorate the accuracy of ReID\nsystems. To address this issue, methods in the literature use an additional\ncostly process such as pose estimation, where pose maps provide supervision to\nexclude occluded regions. In contrast, we introduce a novel Holistic Guidance\n(HG) method that relies only on person identity labels, and on the distribution\nof pairwise matching distances of datasets to alleviate the problem of\nocclusion, without requiring additional supervision. Hence, our proposed\nstudent-teacher framework is trained to address the occlusion problem by\nmatching the distributions of between- and within-class distances (DCDs) of\noccluded samples with that of holistic (non-occluded) samples, thereby using\nthe latter as a soft labeled reference to learn well separated DCDs. This\napproach is supported by our empirical study where the distribution of between-\nand within-class distances between images have more overlap in occluded than\nholistic datasets. In particular, features extracted from both datasets are\njointly learned using the student model to produce an attention map that allows\nseparating visible regions from occluded ones. In addition to this, a joint\ngenerative-discriminative backbone is trained with a denoising autoencoder,\nallowing the system to self-recover from occlusions. Extensive experiments on\nseveral challenging public datasets indicate that the proposed approach can\noutperform state-of-the-art methods on both occluded and holistic datasets\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 21:50:29 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Kiran", "Madhu", ""], ["Praveen", "R Gnana", ""], ["Nguyen-Meidine", "Le Thanh", ""], ["Belharbi", "Soufiane", ""], ["Blais-Morin", "Louis-Antoine", ""], ["Granger", "Eric", ""]]}, {"id": "2104.06534", "submitter": "Rakhil Immidisetti", "authors": "Rakhil Immidisetti, Shuowen Hu, Vishal M. Patel", "title": "Simultaneous Face Hallucination and Translation for Thermal to Visible\n  Face Verification using Axial-GAN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Existing thermal-to-visible face verification approaches expect the thermal\nand visible face images to be of similar resolution. This is unlikely in\nreal-world long-range surveillance systems, since humans are distant from the\ncameras. To address this issue, we introduce the task of thermal-to-visible\nface verification from low-resolution thermal images. Furthermore, we propose\nAxial-Generative Adversarial Network (Axial-GAN) to synthesize high-resolution\nvisible images for matching. In the proposed approach we augment the GAN\nframework with axial-attention layers which leverage the recent advances in\ntransformers for modelling long-range dependencies. We demonstrate the\neffectiveness of the proposed method by evaluating on two different\nthermal-visible face datasets. When compared to related state-of-the-art works,\nour results show significant improvements in both image quality and face\nverification performance, and are also much more efficient.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 22:34:28 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Immidisetti", "Rakhil", ""], ["Hu", "Shuowen", ""], ["Patel", "Vishal M.", ""]]}, {"id": "2104.06593", "submitter": "Yanni Ren", "authors": "Yanni Ren and Hangyu Deng and Hao Jiang and Jinglu Hu", "title": "A Semi-Supervised Classification Method of Apicomplexan Parasites and\n  Host Cell Using Contrastive Learning Strategy", "comments": "6 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A common shortfall of supervised learning for medical imaging is the greedy\nneed for human annotations, which is often expensive and time-consuming to\nobtain. This paper proposes a semi-supervised classification method for three\nkinds of apicomplexan parasites and non-infected host cells microscopic images,\nwhich uses a small number of labeled data and a large number of unlabeled data\nfor training. There are two challenges in microscopic image recognition. The\nfirst is that salient structures of the microscopic images are more fuzzy and\nintricate than natural images' on a real-world scale. The second is that\ninsignificant textures, like background staining, lightness, and contrast\nlevel, vary a lot in samples from different clinical scenarios. To address\nthese challenges, we aim to learn a distinguishable and appearance-invariant\nrepresentation by contrastive learning strategy. On one hand, macroscopic\nimages, which share similar shape characteristics in morphology, are introduced\nto contrast for structure enhancement. On the other hand, different appearance\ntransformations, including color distortion and flittering, are utilized to\ncontrast for texture elimination. In the case where only 1% of microscopic\nimages are labeled, the proposed method reaches an accuracy of 94.90% in a\ngeneralized testing set.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 02:34:50 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Ren", "Yanni", ""], ["Deng", "Hangyu", ""], ["Jiang", "Hao", ""], ["Hu", "Jinglu", ""]]}, {"id": "2104.06595", "submitter": "Rui Shao", "authors": "Rui Shao, Pramuditha Perera, Pong C. Yuen, Vishal M. Patel", "title": "Federated Generalized Face Presentation Attack Detection", "comments": "arXiv admin note: substantial text overlap with arXiv:2005.14638", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face presentation attack detection plays a critical role in the modern face\nrecognition pipeline. A face presentation attack detection model with good\ngeneralization can be obtained when it is trained with face images from\ndifferent input distributions and different types of spoof attacks. In reality,\ntraining data (both real face images and spoof images) are not directly shared\nbetween data owners due to legal and privacy issues. In this paper, with the\nmotivation of circumventing this challenge, we propose a Federated Face\nPresentation Attack Detection (FedPAD) framework that simultaneously takes\nadvantage of rich fPAD information available at different data owners while\npreserving data privacy. In the proposed framework, each data center locally\ntrains its own fPAD model. A server learns a global fPAD model by iteratively\naggregating model updates from all data centers without accessing private data\nin each of them. To equip the aggregated fPAD model in the server with better\ngeneralization ability to unseen attacks from users, following the basic idea\nof FedPAD, we further propose a Federated Generalized Face Presentation Attack\nDetection (FedGPAD) framework. A federated domain disentanglement strategy is\nintroduced in FedGPAD, which treats each data center as one domain and\ndecomposes the fPAD model into domain-invariant and domain-specific parts in\neach data center. Two parts disentangle the domain-invariant and\ndomain-specific features from images in each local data center, respectively. A\nserver learns a global fPAD model by only aggregating domain-invariant parts of\nthe fPAD models from data centers and thus a more generalized fPAD model can be\naggregated in server. We introduce the experimental setting to evaluate the\nproposed FedPAD and FedGPAD frameworks and carry out extensive experiments to\nprovide various insights about federated learning for fPAD.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 02:44:53 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Shao", "Rui", ""], ["Perera", "Pramuditha", ""], ["Yuen", "Pong C.", ""], ["Patel", "Vishal M.", ""]]}, {"id": "2104.06601", "submitter": "Ye Zheng", "authors": "Ye Zheng, Jiahong Wu, Yongqiang Qin, Faen Zhang, Li Cui", "title": "Zero-Shot Instance Segmentation", "comments": "8 pages, 6 figures", "journal-ref": "CVPR2021", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has significantly improved the precision of instance\nsegmentation with abundant labeled data. However, in many areas like medical\nand manufacturing, collecting sufficient data is extremely hard and labeling\nthis data requires high professional skills. We follow this motivation and\npropose a new task set named zero-shot instance segmentation (ZSI). In the\ntraining phase of ZSI, the model is trained with seen data, while in the\ntesting phase, it is used to segment all seen and unseen instances. We first\nformulate the ZSI task and propose a method to tackle the challenge, which\nconsists of Zero-shot Detector, Semantic Mask Head, Background Aware RPN and\nSynchronized Background Strategy. We present a new benchmark for zero-shot\ninstance segmentation based on the MS-COCO dataset. The extensive empirical\nresults in this benchmark show that our method not only surpasses the\nstate-of-the-art results in zero-shot object detection task but also achieves\npromising performance on ZSI. Our approach will serve as a solid baseline and\nfacilitate future research in zero-shot instance segmentation.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 03:02:48 GMT"}, {"version": "v2", "created": "Tue, 1 Jun 2021 03:05:23 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Zheng", "Ye", ""], ["Wu", "Jiahong", ""], ["Qin", "Yongqiang", ""], ["Zhang", "Faen", ""], ["Cui", "Li", ""]]}, {"id": "2104.06609", "submitter": "Chengrui Wang", "authors": "Chengrui Wang, Weihong Deng", "title": "Representative Forgery Mining for Fake Face Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although vanilla Convolutional Neural Network (CNN) based detectors can\nachieve satisfactory performance on fake face detection, we observe that the\ndetectors tend to seek forgeries on a limited region of face, which reveals\nthat the detectors is short of understanding of forgery. Therefore, we propose\nan attention-based data augmentation framework to guide detector refine and\nenlarge its attention. Specifically, our method tracks and occludes the Top-N\nsensitive facial regions, encouraging the detector to mine deeper into the\nregions ignored before for more representative forgery. Especially, our method\nis simple-to-use and can be easily integrated with various CNN models.\nExtensive experiments show that the detector trained with our method is capable\nto separately point out the representative forgery of fake faces generated by\ndifferent manipulation techniques, and our method enables a vanilla CNN-based\ndetector to achieve state-of-the-art performance without structure\nmodification.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 03:24:19 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Wang", "Chengrui", ""], ["Deng", "Weihong", ""]]}, {"id": "2104.06612", "submitter": "Seonho Park", "authors": "Seonho Park, Panos M. Pardalos", "title": "Deep Data Density Estimation through Donsker-Varadhan Representation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.IT math.IT math.PR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Estimating the data density is one of the challenging problems in deep\nlearning. In this paper, we present a simple yet effective method for\nestimating the data density using a deep neural network and the\nDonsker-Varadhan variational lower bound on the KL divergence. We show that the\noptimal critic function associated with the Donsker-Varadhan representation on\nthe KL divergence between the data and the uniform distribution can estimate\nthe data density. We also present the deep neural network-based modeling and\nits stochastic learning. The experimental results and possible applications of\nthe proposed method demonstrate that it is competitive with the previous\nmethods and has a lot of possibilities in applied to various applications.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 03:38:32 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Park", "Seonho", ""], ["Pardalos", "Panos M.", ""]]}, {"id": "2104.06615", "submitter": "Tao Ma", "authors": "Tao Ma, Zhizheng Liu, Yikang Li", "title": "Perception Entropy: A Metric for Multiple Sensors Configuration\n  Evaluation and Design", "comments": "7 pages, 5 figures, submitted to IROS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sensor configuration, including the sensor selections and their installation\nlocations, serves a crucial role in autonomous driving. A well-designed sensor\nconfiguration significantly improves the performance upper bound of the\nperception system. However, as leveraging multiple sensors is becoming the\nmainstream setting, existing methods mainly focusing on single-sensor\nconfiguration problems are hardly utilized in practice. To tackle these issues,\nwe propose a novel method based on conditional entropy in Bayesian theory to\nevaluate the sensor configurations containing both cameras and LiDARs.\nCorrespondingly, an evaluation metric, perception entropy, is introduced to\nmeasure the difference between two configurations, which considers both the\nperception algorithm performance and the selections of the sensors. To the best\nof our knowledge, this is the first method to tackle the multi-sensor\nconfiguration problem for autonomous vehicles. The simulation results,\nextensive comparisons, and analysis all demonstrate the superior performance of\nour proposed approach.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 03:52:57 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Ma", "Tao", ""], ["Liu", "Zhizheng", ""], ["Li", "Yikang", ""]]}, {"id": "2104.06637", "submitter": "Rui Liu", "authors": "Rui Liu, Hanming Deng, Yangyi Huang, Xiaoyu Shi, Lewei Lu, Wenxiu Sun,\n  Xiaogang Wang, Jifeng Dai, Hongsheng Li", "title": "Decoupled Spatial-Temporal Transformer for Video Inpainting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video inpainting aims to fill the given spatiotemporal holes with realistic\nappearance but is still a challenging task even with prosperous deep learning\napproaches. Recent works introduce the promising Transformer architecture into\ndeep video inpainting and achieve better performance. However, it still suffers\nfrom synthesizing blurry texture as well as huge computational cost. Towards\nthis end, we propose a novel Decoupled Spatial-Temporal Transformer (DSTT) for\nimproving video inpainting with exceptional efficiency. Our proposed DSTT\ndisentangles the task of learning spatial-temporal attention into 2 sub-tasks:\none is for attending temporal object movements on different frames at same\nspatial locations, which is achieved by temporally-decoupled Transformer block,\nand the other is for attending similar background textures on same frame of all\nspatial positions, which is achieved by spatially-decoupled Transformer block.\nThe interweaving stack of such two blocks makes our proposed model attend\nbackground textures and moving objects more precisely, and thus the attended\nplausible and temporally-coherent appearance can be propagated to fill the\nholes. In addition, a hierarchical encoder is adopted before the stack of\nTransformer blocks, for learning robust and hierarchical features that maintain\nmulti-level local spatial structure, resulting in the more representative token\nvectors. Seamless combination of these two novel designs forms a better\nspatial-temporal attention scheme and our proposed model achieves better\nperformance than state-of-the-art video inpainting approaches with significant\nboosted efficiency.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 05:47:46 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Liu", "Rui", ""], ["Deng", "Hanming", ""], ["Huang", "Yangyi", ""], ["Shi", "Xiaoyu", ""], ["Lu", "Lewei", ""], ["Sun", "Wenxiu", ""], ["Wang", "Xiaogang", ""], ["Dai", "Jifeng", ""], ["Li", "Hongsheng", ""]]}, {"id": "2104.06650", "submitter": "Zhengyao Lv", "authors": "Zhengyao Lv, Xiaoming Li, Xin Li, Fu Li, Tianwei Lin, Dongliang He and\n  Wangmeng Zuo", "title": "Learning Semantic Person Image Generation by Region-Adaptive\n  Normalization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human pose transfer has received great attention due to its wide\napplications, yet is still a challenging task that is not well solved. Recent\nworks have achieved great success to transfer the person image from the source\nto the target pose. However, most of them cannot well capture the semantic\nappearance, resulting in inconsistent and less realistic textures on the\nreconstructed results. To address this issue, we propose a new two-stage\nframework to handle the pose and appearance translation. In the first stage, we\npredict the target semantic parsing maps to eliminate the difficulties of pose\ntransfer and further benefit the latter translation of per-region appearance\nstyle. In the second one, with the predicted target semantic maps, we suggest a\nnew person image generation method by incorporating the region-adaptive\nnormalization, in which it takes the per-region styles to guide the target\nappearance generation. Extensive experiments show that our proposed SPGNet can\ngenerate more semantic, consistent, and photo-realistic results and perform\nfavorably against the state of the art methods in terms of quantitative and\nqualitative evaluation. The source code and model are available at\nhttps://github.com/cszy98/SPGNet.git.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 06:51:37 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Lv", "Zhengyao", ""], ["Li", "Xiaoming", ""], ["Li", "Xin", ""], ["Li", "Fu", ""], ["Lin", "Tianwei", ""], ["He", "Dongliang", ""], ["Zuo", "Wangmeng", ""]]}, {"id": "2104.06653", "submitter": "Halil \\.Ibrahim \\\"Ozt\\\"urk", "authors": "Halil \\.Ibrahim \\\"Ozt\\\"urk, Ahmet Burak Can", "title": "ADNet: Temporal Anomaly Detection in Surveillance Videos", "comments": "FGVRID workshop of ICPR conference, 15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Anomaly detection in surveillance videos is an important research problem in\ncomputer vision. In this paper, we propose ADNet, an anomaly detection network,\nwhich utilizes temporal convolutions to localize anomalies in videos. The model\nworks online by accepting consecutive windows consisting of fixed-number of\nvideo clips. Features extracted from video clips in a window are fed to ADNet,\nwhich allows to localize anomalies in videos effectively. We propose the AD\nLoss function to improve abnormal segment detection performance of ADNet.\nAdditionally, we propose to use F1@k metric for temporal anomaly detection.\nF1@k is a better evaluation metric than AUC in terms of not penalizing minor\nshifts in temporal segments and punishing short false positive temporal segment\npredictions. Furthermore, we extend UCF Crime dataset by adding two more\nanomaly classes and providing temporal anomaly annotations for all classes.\nFinally, we thoroughly evaluate our model on the extended UCF Crime dataset.\nADNet produces promising results with respect to F1@k metric. Dataset\nextensions and code will be publicly available upon publishing\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 07:00:10 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["\u00d6zt\u00fcrk", "Halil \u0130brahim", ""], ["Can", "Ahmet Burak", ""]]}, {"id": "2104.06689", "submitter": "Hui Lv", "authors": "Hui Lv, Chen Chen, Zhen Cui, Chunyan Xu, Yong Li, Jian Yang", "title": "Learning Normal Dynamics in Videos with Meta Prototype Network", "comments": "9 pages, 4 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Frame reconstruction (current or future frame) based on Auto-Encoder (AE) is\na popular method for video anomaly detection. With models trained on the normal\ndata, the reconstruction errors of anomalous scenes are usually much larger\nthan those of normal ones. Previous methods introduced the memory bank into AE,\nfor encoding diverse normal patterns across the training videos. However, they\nare memory-consuming and cannot cope with unseen new scenarios in the testing\ndata. In this work, we propose a dynamic prototype unit (DPU) to encode the\nnormal dynamics as prototypes in real time, free from extra memory cost. In\naddition, we introduce meta-learning to our DPU to form a novel few-shot\nnormalcy learner, namely Meta-Prototype Unit (MPU). It enables the fast\nadaption capability on new scenes by only consuming a few iterations of update.\nExtensive experiments are conducted on various benchmarks. The superior\nperformance over the state-of-the-art demonstrates the effectiveness of our\nmethod.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 08:25:53 GMT"}, {"version": "v2", "created": "Mon, 10 May 2021 10:53:26 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Lv", "Hui", ""], ["Chen", "Chen", ""], ["Cui", "Zhen", ""], ["Xu", "Chunyan", ""], ["Li", "Yong", ""], ["Yang", "Jian", ""]]}, {"id": "2104.06697", "submitter": "Wonkwang Lee", "authors": "Wonkwang Lee, Whie Jung, Han Zhang, Ting Chen, Jing Yu Koh, Thomas\n  Huang, Hyungsuk Yoon, Honglak Lee, Seunghoon Hong", "title": "Revisiting Hierarchical Approach for Persistent Long-Term Video\n  Prediction", "comments": "Accepted as a conference paper at ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning to predict the long-term future of video frames is notoriously\nchallenging due to inherent ambiguities in the distant future and dramatic\namplifications of prediction error through time. Despite the recent advances in\nthe literature, existing approaches are limited to moderately short-term\nprediction (less than a few seconds), while extrapolating it to a longer future\nquickly leads to destruction in structure and content. In this work, we revisit\nhierarchical models in video prediction. Our method predicts future frames by\nfirst estimating a sequence of semantic structures and subsequently translating\nthe structures to pixels by video-to-video translation. Despite the simplicity,\nwe show that modeling structures and their dynamics in the discrete semantic\nstructure space with a stochastic recurrent estimator leads to surprisingly\nsuccessful long-term prediction. We evaluate our method on three challenging\ndatasets involving car driving and human dancing, and demonstrate that it can\ngenerate complicated scene structures and motions over a very long time horizon\n(i.e., thousands frames), setting a new standard of video prediction with\norders of magnitude longer prediction time than existing approaches. Full\nvideos and codes are available at https://1konny.github.io/HVP/.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 08:39:38 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Lee", "Wonkwang", ""], ["Jung", "Whie", ""], ["Zhang", "Han", ""], ["Chen", "Ting", ""], ["Koh", "Jing Yu", ""], ["Huang", "Thomas", ""], ["Yoon", "Hyungsuk", ""], ["Lee", "Honglak", ""], ["Hong", "Seunghoon", ""]]}, {"id": "2104.06699", "submitter": "Feng Gao", "authors": "Xiaofan Qu, Feng Gao, Junyu Dong, Qian Du, Heng-Chao Li", "title": "Change Detection in Synthetic Aperture Radar Images Using a Dual-Domain\n  Network", "comments": "Accepted by IEEE Geoscience and Remote Sensing Letters, Code:\n  https://github.com/summitgao/SAR_CD_DDNet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Change detection from synthetic aperture radar (SAR) imagery is a critical\nyet challenging task. Existing methods mainly focus on feature extraction in\nspatial domain, and little attention has been paid to frequency domain.\nFurthermore, in patch-wise feature analysis, some noisy features in the\nmarginal region may be introduced. To tackle the above two challenges, we\npropose a Dual-Domain Network. Specifically, we take features from the discrete\ncosine transform domain into consideration and the reshaped DCT coefficients\nare integrated into the proposed model as the frequency domain branch. Feature\nrepresentations from both frequency and spatial domain are exploited to\nalleviate the speckle noise. In addition, we further propose a multi-region\nconvolution module, which emphasizes the central region of each patch. The\ncontextual information and central region features are modeled adaptively. The\nexperimental results on three SAR datasets demonstrate the effectiveness of the\nproposed model. Our codes are available at\nhttps://github.com/summitgao/SAR_CD_DDNet.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 08:41:48 GMT"}, {"version": "v2", "created": "Thu, 15 Apr 2021 00:57:26 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Qu", "Xiaofan", ""], ["Gao", "Feng", ""], ["Dong", "Junyu", ""], ["Du", "Qian", ""], ["Li", "Heng-Chao", ""]]}, {"id": "2104.06703", "submitter": "Dror Moran", "authors": "Dror Moran, Hodaya Koslowsky, Yoni Kasten, Haggai Maron, Meirav Galun,\n  Ronen Basri", "title": "Deep Permutation Equivariant Structure from Motion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing deep methods produce highly accurate 3D reconstructions in stereo\nand multiview stereo settings, i.e., when cameras are both internally and\nexternally calibrated. Nevertheless, the challenge of simultaneous recovery of\ncamera poses and 3D scene structure in multiview settings with deep networks is\nstill outstanding. Inspired by projective factorization for Structure from\nMotion (SFM) and by deep matrix completion techniques, we propose a neural\nnetwork architecture that, given a set of point tracks in multiple images of a\nstatic scene, recovers both the camera parameters and a (sparse) scene\nstructure by minimizing an unsupervised reprojection loss. Our network\narchitecture is designed to respect the structure of the problem: the sought\noutput is equivariant to permutations of both cameras and scene points.\nNotably, our method does not require initialization of camera parameters or 3D\npoint locations. We test our architecture in two setups: (1) single scene\nreconstruction and (2) learning from multiple scenes. Our experiments,\nconducted on a variety of datasets in both internally calibrated and\nuncalibrated settings, indicate that our method accurately recovers pose and\nstructure, on par with classical state of the art methods. Additionally, we\nshow that a pre-trained network can be used to reconstruct novel scenes using\ninexpensive fine-tuning with no loss of accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 08:50:06 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Moran", "Dror", ""], ["Koslowsky", "Hodaya", ""], ["Kasten", "Yoni", ""], ["Maron", "Haggai", ""], ["Galun", "Meirav", ""], ["Basri", "Ronen", ""]]}, {"id": "2104.06728", "submitter": "Ying Guo", "authors": "Ying Guo, Xingxing Wei, Guoqiu Wang, Bo Zhang", "title": "Meaningful Adversarial Stickers for Face Recognition in Physical World", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face recognition (FR) systems have been widely applied in safety-critical\nfields with the introduction of deep learning. However, the existence of\nadversarial examples brings potential security risks to FR systems. To identify\ntheir vulnerability and help improve their robustness, in this paper, we\npropose Meaningful Adversarial Stickers, a physically feasible and easily\nimplemented attack method by using meaningful real stickers existing in our\nlife, where the attackers manipulate the pasting parameters of stickers on the\nface, instead of designing perturbation patterns and then printing them like\nmost existing works. We conduct attacks in the black-box setting with limited\ninformation which is more challenging and practical. To effectively solve the\npasting position, rotation angle, and other parameters of the stickers, we\ndesign Region based Heuristic Differential Algorithm, which utilizes the\ninbreeding strategy based on regional aggregation of effective solutions and\nthe adaptive adjustment strategy of evaluation criteria. Extensive experiments\nare conducted on two public datasets including LFW and CelebA with respective\nto three representative FR models like FaceNet, SphereFace, and CosFace,\nachieving attack success rates of 81.78%, 72.93%, and 79.26% respectively with\nonly hundreds of queries. The results in the physical world confirm the\neffectiveness of our method in complex physical conditions. When continuously\nchanging the face posture of testers, the method can still perform successful\nattacks up to 98.46%, 91.30% and 86.96% in the time series.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 09:32:01 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Guo", "Ying", ""], ["Wei", "Xingxing", ""], ["Wang", "Guoqiu", ""], ["Zhang", "Bo", ""]]}, {"id": "2104.06730", "submitter": "Buyu Liu", "authors": "Buyu Liu, Bingbing Zhuang, Manmohan Chandraker", "title": "Weakly But Deeply Supervised Occlusion-Reasoned Parametric Layouts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an end-to-end network that takes a single perspective RGB image of\na complex road scene as input, to produce occlusion-reasoned layouts in\nperspective space as well as a top-view parametric space. In contrast to prior\nworks that require dense supervision such as semantic labels in perspective\nview, the only human annotations required by our method are for parametric\nattributes that are cheaper and less ambiguous to obtain. To solve this\nchallenging task, our design is comprised of modules that incorporate inductive\nbiases to learn occlusion-reasoning, geometric transformation and semantic\nabstraction, where each module may be supervised by appropriately transforming\nthe parametric annotations. We demonstrate how our design choices and proposed\ndeep supervision help achieve accurate predictions and meaningful\nrepresentations. We validate our approach on two public datasets, KITTI and\nNuScenes, to achieve state-of-the-art results with considerably lower human\nsupervision.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 09:32:29 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Liu", "Buyu", ""], ["Zhuang", "Bingbing", ""], ["Chandraker", "Manmohan", ""]]}, {"id": "2104.06757", "submitter": "Sharif Amit Kamran", "authors": "Sharif Amit Kamran, Khondker Fariha Hossain, Alireza Tavakkoli,\n  Stewart Lee Zuckerbrod, Salah A. Baker", "title": "VTGAN: Semi-supervised Retinal Image Synthesis and Disease Prediction\n  using Vision Transformers", "comments": "11 Pages, 4 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In Fluorescein Angiography (FA), an exogenous dye is injected in the\nbloodstream to image the vascular structure of the retina. The injected dye can\ncause adverse reactions such as nausea, vomiting, anaphylactic shock, and even\ndeath. In contrast, color fundus imaging is a non-invasive technique used for\nphotographing the retina but does not have sufficient fidelity for capturing\nits vascular structure. The only non-invasive method for capturing retinal\nvasculature is optical coherence tomography-angiography (OCTA). However, OCTA\nequipment is quite expensive, and stable imaging is limited to small areas on\nthe retina. In this paper, we propose a novel conditional generative\nadversarial network (GAN) capable of simultaneously synthesizing FA images from\nfundus photographs while predicting retinal degeneration. The proposed system\nhas the benefit of addressing the problem of imaging retinal vasculature in a\nnon-invasive manner as well as predicting the existence of retinal\nabnormalities. We use a semi-supervised approach to train our GAN using\nmultiple weighted losses on different modalities of data. Our experiments\nvalidate that the proposed architecture exceeds recent state-of-the-art\ngenerative networks for fundus-to-angiography synthesis. Moreover, our vision\ntransformer-based discriminators generalize quite well on out-of-distribution\ndata sets for retinal disease prediction.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 10:32:36 GMT"}, {"version": "v2", "created": "Tue, 6 Jul 2021 08:59:35 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Kamran", "Sharif Amit", ""], ["Hossain", "Khondker Fariha", ""], ["Tavakkoli", "Alireza", ""], ["Zuckerbrod", "Stewart Lee", ""], ["Baker", "Salah A.", ""]]}, {"id": "2104.06761", "submitter": "Hang Du", "authors": "Hang Du, Hailin Shi, Yinglu Liu, Dan Zeng, and Tao Mei", "title": "Towards NIR-VIS Masked Face Recognition", "comments": null, "journal-ref": null, "doi": "10.1109/LSP.2021.3071663", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Near-infrared to visible (NIR-VIS) face recognition is the most common case\nin heterogeneous face recognition, which aims to match a pair of face images\ncaptured from two different modalities. Existing deep learning based methods\nhave made remarkable progress in NIR-VIS face recognition, while it encounters\ncertain newly-emerged difficulties during the pandemic of COVID-19, since\npeople are supposed to wear facial masks to cut off the spread of the virus. We\ndefine this task as NIR-VIS masked face recognition, and find it problematic\nwith the masked face in the NIR probe image. First, the lack of masked face\ndata is a challenging issue for the network training. Second, most of the\nfacial parts (cheeks, mouth, nose etc.) are fully occluded by the mask, which\nleads to a large amount of loss of information. Third, the domain gap still\nexists in the remaining facial parts. In such scenario, the existing methods\nsuffer from significant performance degradation caused by the above issues. In\nthis paper, we aim to address the challenge of NIR-VIS masked face recognition\nfrom the perspectives of training data and training method. Specifically, we\npropose a novel heterogeneous training method to maximize the mutual\ninformation shared by the face representation of two domains with the help of\nsemi-siamese networks. In addition, a 3D face reconstruction based approach is\nemployed to synthesize masked face from the existing NIR image. Resorting to\nthese practices, our solution provides the domain-invariant face representation\nwhich is also robust to the mask occlusion. Extensive experiments on three\nNIR-VIS face datasets demonstrate the effectiveness and\ncross-dataset-generalization capacity of our method.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 10:40:09 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Du", "Hang", ""], ["Shi", "Hailin", ""], ["Liu", "Yinglu", ""], ["Zeng", "Dan", ""], ["Mei", "Tao", ""]]}, {"id": "2104.06770", "submitter": "Binh Nguyen Xuan", "authors": "Binh X. Nguyen, Binh D. Nguyen, Tuong Do, Erman Tjiputra, Quang D.\n  Tran, Anh Nguyen", "title": "Graph-based Person Signature for Person Re-Identifications", "comments": "Accepted in CVPR 2021 Workshops", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of person re-identification (ReID) is to match images of the same\nperson over multiple non-overlapping camera views. Due to the variations in\nvisual factors, previous works have investigated how the person identity, body\nparts, and attributes benefit the person ReID problem. However, the\ncorrelations between attributes, body parts, and within each attribute are not\nfully utilized. In this paper, we propose a new method to effectively aggregate\ndetailed person descriptions (attributes labels) and visual features (body\nparts and global features) into a graph, namely Graph-based Person Signature,\nand utilize Graph Convolutional Networks to learn the topological structure of\nthe visual signature of a person. The graph is integrated into a multi-branch\nmulti-task framework for person re-identification. The extensive experiments\nare conducted to demonstrate the effectiveness of our proposed approach on two\nlarge-scale datasets, including Market-1501 and DukeMTMC-ReID. Our approach\nachieves competitive results among the state of the art and outperforms other\nattribute-based or mask-guided methods.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 10:54:36 GMT"}, {"version": "v2", "created": "Sat, 17 Apr 2021 14:57:11 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Nguyen", "Binh X.", ""], ["Nguyen", "Binh D.", ""], ["Do", "Tuong", ""], ["Tjiputra", "Erman", ""], ["Tran", "Quang D.", ""], ["Nguyen", "Anh", ""]]}, {"id": "2104.06772", "submitter": "Anthony Ngo", "authors": "Anthony Ngo, Max Paul Bauer, Michael Resch", "title": "Deep Evaluation Metric: Learning to Evaluate Simulated Radar Point\n  Clouds for Virtual Testing of Autonomous Driving", "comments": "2021 IEEE Radar Conference (IEEE RadarConf 2021)", "journal-ref": null, "doi": "10.1109/RadarConf2147009.2021.9455235", "report-no": null, "categories": "cs.CV cs.AI eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The usage of environment sensor models for virtual testing is a promising\napproach to reduce the testing effort of autonomous driving. However, in order\nto deduce any statements regarding the performance of an autonomous driving\nfunction based on simulation, the sensor model has to be validated to determine\nthe discrepancy between the synthetic and real sensor data. Since a certain\ndegree of divergence can be assumed to exist, the sufficient level of fidelity\nmust be determined, which poses a major challenge. In particular, a method for\nquantifying the fidelity of a sensor model does not exist and the problem of\ndefining an appropriate metric remains. In this work, we train a neural network\nto distinguish real and simulated radar sensor data with the purpose of\nlearning the latent features of real radar point clouds. Furthermore, we\npropose the classifier's confidence score for the `real radar point cloud'\nclass as a metric to determine the degree of fidelity of synthetically\ngenerated radar data. The presented approach is evaluated and it can be\ndemonstrated that the proposed deep evaluation metric outperforms conventional\nmetrics in terms of its capability to identify characteristic differences\nbetween real and simulated radar data.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 11:04:50 GMT"}, {"version": "v2", "created": "Mon, 21 Jun 2021 08:30:09 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Ngo", "Anthony", ""], ["Bauer", "Max Paul", ""], ["Resch", "Michael", ""]]}, {"id": "2104.06773", "submitter": "Nermin Samet", "authors": "Nermin Samet, Samet Hicsonmez, Emre Akbas", "title": "HoughNet: Integrating near and long-range evidence for visual detection", "comments": "Under review at TPAMI. arXiv admin note: substantial text overlap\n  with arXiv:2007.02355", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents HoughNet, a one-stage, anchor-free, voting-based,\nbottom-up object detection method. Inspired by the Generalized Hough Transform,\nHoughNet determines the presence of an object at a certain location by the sum\nof the votes cast on that location. Votes are collected from both near and\nlong-distance locations based on a log-polar vote field. Thanks to this voting\nmechanism, HoughNet is able to integrate both near and long-range,\nclass-conditional evidence for visual recognition, thereby generalizing and\nenhancing current object detection methodology, which typically relies on only\nlocal evidence. On the COCO dataset, HoughNet's best model achieves $46.4$ $AP$\n(and $65.1$ $AP_{50}$), performing on par with the state-of-the-art in\nbottom-up object detection and outperforming most major one-stage and two-stage\nmethods. We further validate the effectiveness of our proposal in other visual\ndetection tasks, namely, video object detection, instance segmentation, 3D\nobject detection and keypoint detection for human pose estimation, and an\nadditional ``labels to photo`` image generation task, where the integration of\nour voting module consistently improves performance in all cases. Code is\navailable at \\url{https://github.com/nerminsamet/houghnet}.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 11:05:29 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Samet", "Nermin", ""], ["Hicsonmez", "Samet", ""], ["Akbas", "Emre", ""]]}, {"id": "2104.06779", "submitter": "Silvio Giancola", "authors": "Silvio Giancola, Bernard Ghanem", "title": "Temporally-Aware Feature Pooling for Action Spotting in Soccer\n  Broadcasts", "comments": "8 pages, Camera-Ready for CVSports 2021 (CVPRW)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Toward the goal of automatic production for sports broadcasts, a paramount\ntask consists in understanding the high-level semantic information of the game\nin play. For instance, recognizing and localizing the main actions of the game\nwould allow producers to adapt and automatize the broadcast production,\nfocusing on the important details of the game and maximizing the spectator\nengagement. In this paper, we focus our analysis on action spotting in soccer\nbroadcast, which consists in temporally localizing the main actions in a soccer\ngame. To that end, we propose a novel feature pooling method based on NetVLAD,\ndubbed NetVLAD++, that embeds temporally-aware knowledge. Different from\nprevious pooling methods that consider the temporal context as a single set to\npool from, we split the context before and after an action occurs. We argue\nthat considering the contextual information around the action spot as a single\nentity leads to a sub-optimal learning for the pooling module. With NetVLAD++,\nwe disentangle the context from the past and future frames and learn specific\nvocabularies of semantics for each subsets, avoiding to blend and blur such\nvocabulary in time. Injecting such prior knowledge creates more informative\npooling modules and more discriminative pooled features, leading into a better\nunderstanding of the actions. We train and evaluate our methodology on the\nrecent large-scale dataset SoccerNet-v2, reaching 53.4% Average-mAP for action\nspotting, a +12.7% improvement w.r.t the current state-of-the-art.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 11:09:03 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Giancola", "Silvio", ""], ["Ghanem", "Bernard", ""]]}, {"id": "2104.06780", "submitter": "Yong Man Ro", "authors": "Hak Gu Kim, Sangmin Lee, Seongyeop Kim, Heoun-taek Lim, Yong Man Ro", "title": "Towards a Better Understanding of VR Sickness: Physical Symptom\n  Prediction for VR Contents", "comments": "AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the black-box issue of VR sickness assessment (VRSA) by evaluating\nthe level of physical symptoms of VR sickness. For the VR contents inducing the\nsimilar VR sickness level, the physical symptoms can vary depending on the\ncharacteristics of the contents. Most of existing VRSA methods focused on\nassessing the overall VR sickness score. To make better understanding of VR\nsickness, it is required to predict and provide the level of major symptoms of\nVR sickness rather than overall degree of VR sickness. In this paper, we\npredict the degrees of main physical symptoms affecting the overall degree of\nVR sickness, which are disorientation, nausea, and oculomotor. In addition, we\nintroduce a new large-scale dataset for VRSA including 360 videos with various\nframe rates, physiological signals, and subjective scores. On VRSA benchmark\nand our newly collected dataset, our approach shows a potential to not only\nachieve the highest correlation with subjective scores, but also to better\nunderstand which symptoms are the main causes of VR sickness.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 11:09:03 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Kim", "Hak Gu", ""], ["Lee", "Sangmin", ""], ["Kim", "Seongyeop", ""], ["Lim", "Heoun-taek", ""], ["Ro", "Yong Man", ""]]}, {"id": "2104.06781", "submitter": "Ilker Bozcan", "authors": "Ilker Bozcan and Erdal Kayacan", "title": "Context-Dependent Anomaly Detection for Low Altitude Traffic\n  Surveillance", "comments": "7 pages, 4 figures, Accepted to IEEE International Conference on\n  Robotics and Automation 2021 (ICRA 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The detection of contextual anomalies is a challenging task for surveillance\nsince an observation can be considered anomalous or normal in a specific\nenvironmental context. An unmanned aerial vehicle (UAV) can utilize its aerial\nmonitoring capability and employ multiple sensors to gather contextual\ninformation about the environment and perform contextual anomaly detection. In\nthis work, we introduce a deep neural network-based method (CADNet) to find\npoint anomalies (i.e., single instance anomalous data) and contextual anomalies\n(i.e., context-specific abnormality) in an environment using a UAV. The method\nis based on a variational autoencoder (VAE) with a context sub-network. The\ncontext sub-network extracts contextual information regarding the environment\nusing GPS and time data, then feeds it to the VAE to predict anomalies\nconditioned on the context. To the best of our knowledge, our method is the\nfirst contextual anomaly detection method for UAV-assisted aerial surveillance.\nWe evaluate our method on the AU-AIR dataset in a traffic surveillance\nscenario. Quantitative comparisons against several baselines demonstrate the\nsuperiority of our approach in the anomaly detection tasks. The codes and data\nwill be available at https://bozcani.github.io/cadnet.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 11:12:04 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Bozcan", "Ilker", ""], ["Kayacan", "Erdal", ""]]}, {"id": "2104.06782", "submitter": "Yong Man Ro", "authors": "Hak Gu Kim, Minho Park, Sangmin Lee, Seongyeop Kim, Yong Man Ro", "title": "Visual Comfort Aware-Reinforcement Learning for Depth Adjustment of\n  Stereoscopic 3D Images", "comments": "AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Depth adjustment aims to enhance the visual experience of stereoscopic 3D\n(S3D) images, which accompanied with improving visual comfort and depth\nperception. For a human expert, the depth adjustment procedure is a sequence of\niterative decision making. The human expert iteratively adjusts the depth until\nhe is satisfied with the both levels of visual comfort and the perceived depth.\nIn this work, we present a novel deep reinforcement learning (DRL)-based\napproach for depth adjustment named VCA-RL (Visual Comfort Aware Reinforcement\nLearning) to explicitly model human sequential decision making in depth editing\noperations. We formulate the depth adjustment process as a Markov decision\nprocess where actions are defined as camera movement operations to control the\ndistance between the left and right cameras. Our agent is trained based on the\nguidance of an objective visual comfort assessment metric to learn the optimal\nsequence of camera movement actions in terms of perceptual aspects in\nstereoscopic viewing. With extensive experiments and user studies, we show the\neffectiveness of our VCA-RL model on three different S3D databases.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 11:15:08 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Kim", "Hak Gu", ""], ["Park", "Minho", ""], ["Lee", "Sangmin", ""], ["Kim", "Seongyeop", ""], ["Ro", "Yong Man", ""]]}, {"id": "2104.06789", "submitter": "Zhixiang Min", "authors": "Zhixiang Min, Yiding Yang, Enrique Dunn", "title": "VOLDOR: Visual Odometry from Log-logistic Dense Optical flow Residuals", "comments": "Paper was accepted to CVPR20. Proceedings of the IEEE/CVF Conference\n  on Computer Vision and Pattern Recognition. 2020. The arxiv version fixed a\n  few typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a dense indirect visual odometry method taking as input externally\nestimated optical flow fields instead of hand-crafted feature correspondences.\nWe define our problem as a probabilistic model and develop a generalized-EM\nformulation for the joint inference of camera motion, pixel depth, and\nmotion-track confidence. Contrary to traditional methods assuming\nGaussian-distributed observation errors, we supervise our inference framework\nunder an (empirically validated) adaptive log-logistic distribution model.\nMoreover, the log-logistic residual model generalizes well to different\nstate-of-the-art optical flow methods, making our approach modular and agnostic\nto the choice of optical flow estimators. Our method achieved top-ranking\nresults on both TUM RGB-D and KITTI odometry benchmarks. Our open-sourced\nimplementation is inherently GPU-friendly with only linear computational and\nstorage growth.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 11:39:19 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Min", "Zhixiang", ""], ["Yang", "Yiding", ""], ["Dunn", "Enrique", ""]]}, {"id": "2104.06797", "submitter": "Gaochang Wu", "authors": "Gaochang Wu, Yebin Liu, Lu Fang, Tianyou Chai", "title": "Revisiting Light Field Rendering with Deep Anti-Aliasing Neural Network", "comments": "15 pages, 12 figures. Accepted by IEEE TPAMI", "journal-ref": "IEEE TPAMI, 2021", "doi": "10.1109/TPAMI.2021.3073739", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The light field (LF) reconstruction is mainly confronted with two challenges,\nlarge disparity and the non-Lambertian effect. Typical approaches either\naddress the large disparity challenge using depth estimation followed by view\nsynthesis or eschew explicit depth information to enable non-Lambertian\nrendering, but rarely solve both challenges in a unified framework. In this\npaper, we revisit the classic LF rendering framework to address both challenges\nby incorporating it with advanced deep learning techniques. First, we\nanalytically show that the essential issue behind the large disparity and\nnon-Lambertian challenges is the aliasing problem. Classic LF rendering\napproaches typically mitigate the aliasing with a reconstruction filter in the\nFourier domain, which is, however, intractable to implement within a deep\nlearning pipeline. Instead, we introduce an alternative framework to perform\nanti-aliasing reconstruction in the image domain and analytically show\ncomparable efficacy on the aliasing issue. To explore the full potential, we\nthen embed the anti-aliasing framework into a deep neural network through the\ndesign of an integrated architecture and trainable parameters. The network is\ntrained through end-to-end optimization using a peculiar training set,\nincluding regular LFs and unstructured LFs. The proposed deep learning pipeline\nshows a substantial superiority in solving both the large disparity and the\nnon-Lambertian challenges compared with other state-of-the-art approaches. In\naddition to the view interpolation for an LF, we also show that the proposed\npipeline also benefits light field view extrapolation.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 12:03:25 GMT"}, {"version": "v2", "created": "Wed, 28 Apr 2021 02:38:30 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Wu", "Gaochang", ""], ["Liu", "Yebin", ""], ["Fang", "Lu", ""], ["Chai", "Tianyou", ""]]}, {"id": "2104.06800", "submitter": "Zhixiang Min", "authors": "Zhixiang Min, Enrique Dunn", "title": "VOLDOR-SLAM: For the Times When Feature-Based or Direct Methods Are Not\n  Good Enough", "comments": "Paper was accepted to ICRA21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a dense-indirect SLAM system using external dense optical flows as\ninput. We extend the recent probabilistic visual odometry model VOLDOR [Min et\nal. CVPR'20], by incorporating the use of geometric priors to 1) robustly\nbootstrap estimation from monocular capture, while 2) seamlessly supporting\nstereo and/or RGB-D input imagery. Our customized back-end tightly couples our\nintermediate geometric estimates with an adaptive priority scheme managing the\nconnectivity of an incremental pose graph. We leverage recent advances in dense\noptical flow methods to achieve accurate and robust camera pose estimates,\nwhile constructing fine-grain globally-consistent dense environmental maps. Our\nopen source implementation [https://github.com/htkseason/VOLDOR] operates\nonline at around 15 FPS on a single GTX1080Ti GPU.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 12:04:32 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Min", "Zhixiang", ""], ["Dunn", "Enrique", ""]]}, {"id": "2104.06806", "submitter": "Kai Wang", "authors": "Kai Wang, Luis Herranz, Joost van de Weijer", "title": "Continual learning in cross-modal retrieval", "comments": "2nd CLVISION workshop in CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multimodal representations and continual learning are two areas closely\nrelated to human intelligence. The former considers the learning of shared\nrepresentation spaces where information from different modalities can be\ncompared and integrated (we focus on cross-modal retrieval between language and\nvisual representations). The latter studies how to prevent forgetting a\npreviously learned task when learning a new one. While humans excel in these\ntwo aspects, deep neural networks are still quite limited. In this paper, we\npropose a combination of both problems into a continual cross-modal retrieval\nsetting, where we study how the catastrophic interference caused by new tasks\nimpacts the embedding spaces and their cross-modal alignment required for\neffective retrieval. We propose a general framework that decouples the\ntraining, indexing and querying stages. We also identify and study different\nfactors that may lead to forgetting, and propose tools to alleviate it. We\nfound that the indexing stage pays an important role and that simply avoiding\nreindexing the database with updated embedding networks can lead to significant\ngains. We evaluated our methods in two image-text retrieval datasets, obtaining\nsignificant gains with respect to the fine tuning baseline.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 12:13:39 GMT"}, {"version": "v2", "created": "Mon, 19 Apr 2021 14:20:57 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Wang", "Kai", ""], ["Herranz", "Luis", ""], ["van de Weijer", "Joost", ""]]}, {"id": "2104.06813", "submitter": "Hui Lv", "authors": "Hui Lv, Chunyan Xu, Zhen Cui", "title": "Global Information Guided Video Anomaly Detection", "comments": null, "journal-ref": null, "doi": "10.1145/3394171.3416277", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video anomaly detection (VAD) is currently a challenging task due to the\ncomplexity of anomaly as well as the lack of labor-intensive temporal\nannotations. In this paper, we propose an end-to-end Global Information Guided\n(GIG) anomaly detection framework for anomaly detection using the video-level\nannotations (i.e., weak labels). We propose to first mine the global pattern\ncues by leveraging the weak labels in a GIG module. Then we build a spatial\nreasoning module to measure the relevance between vectors in spatial domain\nwith the global cue vectors, and select the most related feature vectors for\ntemporal anomaly detection. The experimental results on the CityScene challenge\ndemonstrate the effectiveness of our model.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 12:32:13 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Lv", "Hui", ""], ["Xu", "Chunyan", ""], ["Cui", "Zhen", ""]]}, {"id": "2104.06815", "submitter": "Guo-Wang Xie", "authors": "Guo-Wang Xie, Fei Yin, Xu-Yao Zhang, and Cheng-Lin Liu", "title": "Dewarping Document Image By Displacement Flow Estimation with Fully\n  Convolutional Network", "comments": null, "journal-ref": "International Workshop on Document Analysis Systems. Springer,\n  Cham, 2020: 131-144", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As camera-based documents are increasingly used, the rectification of\ndistorted document images becomes a need to improve the recognition\nperformance. In this paper, we propose a novel framework for both rectifying\ndistorted document image and removing background finely, by estimating\npixel-wise displacements using a fully convolutional network (FCN). The\ndocument image is rectified by transformation according to the displacements of\npixels. The FCN is trained by regressing displacements of synthesized distorted\ndocuments, and to control the smoothness of displacements, we propose a Local\nSmooth Constraint (LSC) in regularization. Our approach is easy to implement\nand consumes moderate computing resource. Experiments proved that our approach\ncan dewarp document images effectively under various geometric distortions, and\nhas achieved the state-of-the-art performance in terms of local details and\noverall effect.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 12:32:36 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Xie", "Guo-Wang", ""], ["Yin", "Fei", ""], ["Zhang", "Xu-Yao", ""], ["Liu", "Cheng-Lin", ""]]}, {"id": "2104.06820", "submitter": "Utkarsh Ojha", "authors": "Utkarsh Ojha, Yijun Li, Jingwan Lu, Alexei A. Efros, Yong Jae Lee, Eli\n  Shechtman, Richard Zhang", "title": "Few-shot Image Generation via Cross-domain Correspondence", "comments": "CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training generative models, such as GANs, on a target domain containing\nlimited examples (e.g., 10) can easily result in overfitting. In this work, we\nseek to utilize a large source domain for pretraining and transfer the\ndiversity information from source to target. We propose to preserve the\nrelative similarities and differences between instances in the source via a\nnovel cross-domain distance consistency loss. To further reduce overfitting, we\npresent an anchor-based strategy to encourage different levels of realism over\ndifferent regions in the latent space. With extensive results in both\nphotorealistic and non-photorealistic domains, we demonstrate qualitatively and\nquantitatively that our few-shot model automatically discovers correspondences\nbetween source and target domains and generates more diverse and realistic\nimages than previous methods.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 17:59:35 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Ojha", "Utkarsh", ""], ["Li", "Yijun", ""], ["Lu", "Jingwan", ""], ["Efros", "Alexei A.", ""], ["Lee", "Yong Jae", ""], ["Shechtman", "Eli", ""], ["Zhang", "Richard", ""]]}, {"id": "2104.06826", "submitter": "Daniel Rivas", "authors": "Daniel Rivas, Francesc Guim, Jord\\`a Polo, Josep Ll. Berral, Pubudu M.\n  Silva, David Carrera", "title": "Towards Unsupervised Fine-Tuning for Edge Video Analytics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Judging by popular and generic computer vision challenges, such as the\nImageNet or PASCAL VOC, neural networks have proven to be exceptionally\naccurate in recognition tasks. However, state-of-the-art accuracy often comes\nat a high computational price, requiring equally state-of-the-art and high-end\nhardware acceleration to achieve anything near real-time performance. At the\nsame time, use cases such as smart cities or autonomous vehicles require an\nautomated analysis of images from fixed cameras in real-time. Due to the huge\nand constant amount of network bandwidth these streams would generate, we\ncannot rely on offloading compute to the omnipresent and omnipotent cloud.\nTherefore, a distributed Edge Cloud must be in charge to process images\nlocally. However, the Edge Cloud is, by nature, resource-constrained, which\nputs a limit on the computational complexity of the models executed in the\nedge. Nonetheless, there is a need for a meeting point between the Edge Cloud\nand accurate real-time video analytics. In this paper, we propose a method for\nimproving accuracy of edge models without any extra compute cost by means of\nautomatic model specialization. First, we show how the sole assumption of\nstatic cameras allows us to make a series of considerations that greatly\nsimplify the scope of the problem. Then, we present Edge AutoTuner, a framework\nthat implements and brings these considerations together to automate the\nend-to-end fine-tuning of models. Finally, we show that complex neural networks\n- able to generalize better - can be effectively used as teachers to annotate\ndatasets for the fine-tuning of lightweight neural networks and tailor them to\nthe specific edge context, which boosts accuracy at constant computational\ncost, and do so without any human interaction. Results show that our method can\nautomatically improve accuracy of pre-trained models by an average of 21%.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 12:57:40 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Rivas", "Daniel", ""], ["Guim", "Francesc", ""], ["Polo", "Jord\u00e0", ""], ["Berral", "Josep Ll.", ""], ["Silva", "Pubudu M.", ""], ["Carrera", "David", ""]]}, {"id": "2104.06832", "submitter": "Chengbo Dong", "authors": "Xinru Chen, Chengbo Dong, Jiaqi Ji, Juan Cao, Xirong Li", "title": "Image Manipulation Detection by Multi-View Multi-Scale Supervision", "comments": "Accepted by ICCV 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The key challenge of image manipulation detection is how to learn\ngeneralizable features that are sensitive to manipulations in novel data,\nwhilst specific to prevent false alarms on authentic images. Current research\nemphasizes the sensitivity, with the specificity overlooked. In this paper we\naddress both aspects by multi-view feature learning and multi-scale\nsupervision. By exploiting noise distribution and boundary artifact surrounding\ntampered regions, the former aims to learn semantic-agnostic and thus more\ngeneralizable features. The latter allows us to learn from authentic images\nwhich are nontrivial to be taken into account by current semantic segmentation\nnetwork based methods. Our thoughts are realized by a new network which we term\nMVSS-Net. Extensive experiments on five benchmark sets justify the viability of\nMVSS-Net for both pixel-level and image-level manipulation detection.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 13:05:58 GMT"}, {"version": "v2", "created": "Sun, 25 Jul 2021 06:45:00 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Chen", "Xinru", ""], ["Dong", "Chengbo", ""], ["Ji", "Jiaqi", ""], ["Cao", "Juan", ""], ["Li", "Xirong", ""]]}, {"id": "2104.06849", "submitter": "Marko Mihajlovic", "authors": "Marko Mihajlovic, Yan Zhang, Michael J. Black, Siyu Tang", "title": "LEAP: Learning Articulated Occupancy of People", "comments": "In Proceedings of the IEEE/CVF Conference on Computer Vision and\n  Pattern Recognition. 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Substantial progress has been made on modeling rigid 3D objects using deep\nimplicit representations. Yet, extending these methods to learn neural models\nof human shape is still in its infancy. Human bodies are complex and the key\nchallenge is to learn a representation that generalizes such that it can\nexpress body shape deformations for unseen subjects in unseen,\nhighly-articulated, poses. To address this challenge, we introduce LEAP\n(LEarning Articulated occupancy of People), a novel neural occupancy\nrepresentation of the human body. Given a set of bone transformations (i.e.\njoint locations and rotations) and a query point in space, LEAP first maps the\nquery point to a canonical space via learned linear blend skinning (LBS)\nfunctions and then efficiently queries the occupancy value via an occupancy\nnetwork that models accurate identity- and pose-dependent deformations in the\ncanonical space. Experiments show that our canonicalized occupancy estimation\nwith the learned LBS functions greatly improves the generalization capability\nof the learned occupancy representation across various human shapes and poses,\noutperforming existing solutions in all settings.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 13:41:56 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Mihajlovic", "Marko", ""], ["Zhang", "Yan", ""], ["Black", "Michael J.", ""], ["Tang", "Siyu", ""]]}, {"id": "2104.06856", "submitter": "Vishal Mandal", "authors": "Armstrong Aboah, Maged Shoman, Vishal Mandal, Sayedomidreza Davami,\n  Yaw Adu-Gyamfi, Anuj Sharma", "title": "A Vision-based System for Traffic Anomaly Detection using Deep Learning\n  and Decision Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Any intelligent traffic monitoring system must be able to detect anomalies\nsuch as traffic accidents in real time. In this paper, we propose a\nDecision-Tree - enabled approach powered by Deep Learning for extracting\nanomalies from traffic cameras while accurately estimating the start and end\ntime of the anomalous event. Our approach included creating a detection model,\nfollowed by anomaly detection and analysis. YOLOv5 served as the foundation for\nour detection model. The anomaly detection and analysis step entail traffic\nscene background estimation, road mask extraction, and adaptive thresholding.\nCandidate anomalies were passed through a decision tree to detect and analyze\nfinal anomalies. The proposed approach yielded an F1 score of 0.8571, and an S4\nscore of 0.5686, per the experimental validation.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 13:48:13 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Aboah", "Armstrong", ""], ["Shoman", "Maged", ""], ["Mandal", "Vishal", ""], ["Davami", "Sayedomidreza", ""], ["Adu-Gyamfi", "Yaw", ""], ["Sharma", "Anuj", ""]]}, {"id": "2104.06885", "submitter": "Yutao Chen", "authors": "Yutao Chen, Yuxuan Zhang, Zhongrui Huang, Zhenyao Luo, Jinpeng Chen", "title": "CelebHair: A New Large-Scale Dataset for Hairstyle Recommendation based\n  on CelebA", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we present a new large-scale dataset for hairstyle\nrecommendation, CelebHair, based on the celebrity facial attributes dataset,\nCelebA. Our dataset inherited the majority of facial images along with some\nbeauty-related facial attributes from CelebA. Additionally, we employed facial\nlandmark detection techniques to extract extra features such as nose length and\npupillary distance, and deep convolutional neural networks for face shape and\nhairstyle classification. Empirical comparison has demonstrated the superiority\nof our dataset to other existing hairstyle-related datasets regarding variety,\nveracity, and volume. Analysis and experiments have been conducted on the\ndataset in order to evaluate its robustness and usability.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 14:26:37 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Chen", "Yutao", ""], ["Zhang", "Yuxuan", ""], ["Huang", "Zhongrui", ""], ["Luo", "Zhenyao", ""], ["Chen", "Jinpeng", ""]]}, {"id": "2104.06903", "submitter": "Dongkwon Jin", "authors": "Dongkwon Jin, Wonhui Park, Seong-Gyun Jeong and Chang-Su Kim", "title": "Harmonious Semantic Line Detection via Maximal Weight Clique Selection", "comments": "Accepted to CVPR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel algorithm to detect an optimal set of semantic lines is proposed in\nthis work. We develop two networks: selection network (S-Net) and harmonization\nnetwork (H-Net). First, S-Net computes the probabilities and offsets of line\ncandidates. Second, we filter out irrelevant lines through a\nselection-and-removal process. Third, we construct a complete graph, whose edge\nweights are computed by H-Net. Finally, we determine a maximal weight clique\nrepresenting an optimal set of semantic lines. Moreover, to assess the overall\nharmony of detected lines, we propose a novel metric, called HIoU. Experimental\nresults demonstrate that the proposed algorithm can detect harmonious semantic\nlines effectively and efficiently. Our codes are available at\nhttps://github.com/dongkwonjin/Semantic-Line-MWCS.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 14:54:27 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Jin", "Dongkwon", ""], ["Park", "Wonhui", ""], ["Jeong", "Seong-Gyun", ""], ["Kim", "Chang-Su", ""]]}, {"id": "2104.06935", "submitter": "Julian Chibane", "authors": "Julian Chibane, Aayush Bansal, Verica Lazova, Gerard Pons-Moll", "title": "Stereo Radiance Fields (SRF): Learning View Synthesis for Sparse Views\n  of Novel Scenes", "comments": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\n  2021", "journal-ref": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\n  2021", "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent neural view synthesis methods have achieved impressive quality and\nrealism, surpassing classical pipelines which rely on multi-view\nreconstruction. State-of-the-Art methods, such as NeRF, are designed to learn a\nsingle scene with a neural network and require dense multi-view inputs. Testing\non a new scene requires re-training from scratch, which takes 2-3 days. In this\nwork, we introduce Stereo Radiance Fields (SRF), a neural view synthesis\napproach that is trained end-to-end, generalizes to new scenes, and requires\nonly sparse views at test time. The core idea is a neural architecture inspired\nby classical multi-view stereo methods, which estimates surface points by\nfinding similar image regions in stereo images. In SRF, we predict color and\ndensity for each 3D point given an encoding of its stereo correspondence in the\ninput images. The encoding is implicitly learned by an ensemble of pair-wise\nsimilarities -- emulating classical stereo. Experiments show that SRF learns\nstructure instead of overfitting on a scene. We train on multiple scenes of the\nDTU dataset and generalize to new ones without re-training, requiring only 10\nsparse and spread-out views as input. We show that 10-15 minutes of fine-tuning\nfurther improve the results, achieving significantly sharper, more detailed\nresults than scene-specific models. The code, model, and videos are available\nat https://virtualhumans.mpi-inf.mpg.de/srf/.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 15:38:57 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Chibane", "Julian", ""], ["Bansal", "Aayush", ""], ["Lazova", "Verica", ""], ["Pons-Moll", "Gerard", ""]]}, {"id": "2104.06936", "submitter": "Yuchen Ma", "authors": "Yuchen Ma, Songtao Liu, Zeming Li, Jian Sun", "title": "IQDet: Instance-wise Quality Distribution Sampling for Object Detection", "comments": "Accepted by CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a dense object detector with an instance-wise sampling strategy,\nnamed IQDet. Instead of using human prior sampling strategies, we first extract\nthe regional feature of each ground-truth to estimate the instance-wise quality\ndistribution. According to a mixture model in spatial dimensions, the\ndistribution is more noise-robust and adapted to the semantic pattern of each\ninstance. Based on the distribution, we propose a quality sampling strategy,\nwhich automatically selects training samples in a probabilistic manner and\ntrains with more high-quality samples. Extensive experiments on MS COCO show\nthat our method steadily improves baseline by nearly 2.4 AP without bells and\nwhistles. Moreover, our best model achieves 51.6 AP, outperforming all existing\nstate-of-the-art one-stage detectors and it is completely cost-free in\ninference time.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 15:57:22 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Ma", "Yuchen", ""], ["Liu", "Songtao", ""], ["Li", "Zeming", ""], ["Sun", "Jian", ""]]}, {"id": "2104.06937", "submitter": "Wilma Bainbridge", "authors": "Wilma A. Bainbridge", "title": "Shared memories driven by the intrinsic memorability of items", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  When we experience an event, it feels like our previous experiences, our\ninterpretations of that event (e.g., aesthetics, emotions), and our current\nstate will determine how we will remember it. However, recent work has revealed\na strong sway of the visual world itself in influencing what we remember and\nforget. Certain items -- including certain faces, words, images, and movements\n-- are intrinsically memorable or forgettable across observers, regardless of\nindividual differences. Further, neuroimaging research has revealed that the\nbrain is sensitive to memorability both rapidly and automatically during late\nperception. These strong consistencies in memory across people may reflect the\nbroad organizational principles of our sensory environment, and may reveal how\nthe brain prioritizes information before encoding items into memory. In this\nchapter, I will discuss our current state-of-the-art understanding of\nmemorability for visual information, and what these findings imply about how we\nperceive and remember visual events.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 16:03:27 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Bainbridge", "Wilma A.", ""]]}, {"id": "2104.06945", "submitter": "Giulio Reina", "authors": "Annalisa Milella, Roberto Marani, Antonio Petitti, Giulio Reina", "title": "In-field high throughput grapevine phenotyping with a consumer-grade\n  depth camera", "comments": null, "journal-ref": "Computers and Electronics in Agriculture 156 2019 293306", "doi": "10.1016/j.compag.2018.11.026", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Plant phenotyping, that is, the quantitative assessment of plant traits\nincluding growth, morphology, physiology, and yield, is a critical aspect\ntowards efficient and effective crop management. Currently, plant phenotyping\nis a manually intensive and time consuming process, which involves human\noperators making measurements in the field, based on visual estimates or using\nhand-held devices. In this work, methods for automated grapevine phenotyping\nare developed, aiming to canopy volume estimation and bunch detection and\ncounting. It is demonstrated that both measurements can be effectively\nperformed in the field using a consumer-grade depth camera mounted onboard an\nagricultural vehicle.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 16:16:27 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Milella", "Annalisa", ""], ["Marani", "Roberto", ""], ["Petitti", "Antonio", ""], ["Reina", "Giulio", ""]]}, {"id": "2104.06950", "submitter": "Jan Bedna\\v{r}\\'ik", "authors": "Jan Bednarik, Vladimir G. Kim, Siddhartha Chaudhuri, Shaifali\n  Parashar, Mathieu Salzmann, Pascal Fua, Noam Aigerman", "title": "Temporally-Coherent Surface Reconstruction via Metric-Consistent Atlases", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method for the unsupervised reconstruction of a\ntemporally-coherent sequence of surfaces from a sequence of time-evolving point\nclouds, yielding dense, semantically meaningful correspondences between all\nkeyframes. We represent the reconstructed surface as an atlas, using a neural\nnetwork. Using canonical correspondences defined via the atlas, we encourage\nthe reconstruction to be as isometric as possible across frames, leading to\nsemantically-meaningful reconstruction. Through experiments and comparisons, we\nempirically show that our method achieves results that exceed that state of the\nart in the accuracy of unsupervised correspondences and accuracy of surface\nreconstruction.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 16:21:22 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Bednarik", "Jan", ""], ["Kim", "Vladimir G.", ""], ["Chaudhuri", "Siddhartha", ""], ["Parashar", "Shaifali", ""], ["Salzmann", "Mathieu", ""], ["Fua", "Pascal", ""], ["Aigerman", "Noam", ""]]}, {"id": "2104.06954", "submitter": "Ivan Skorokhodov", "authors": "Ivan Skorokhodov, Grigorii Sotnikov, Mohamed Elhoseiny", "title": "Aligning Latent and Image Spaces to Connect the Unconnectable", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work, we develop a method to generate infinite high-resolution images\nwith diverse and complex content. It is based on a perfectly equivariant\ngenerator with synchronous interpolations in the image and latent spaces.\nLatent codes, when sampled, are positioned on the coordinate grid, and each\npixel is computed from an interpolation of the nearby style codes. We modify\nthe AdaIN mechanism to work in such a setup and train the generator in an\nadversarial setting to produce images positioned between any two latent\nvectors. At test time, this allows for generating complex and diverse infinite\nimages and connecting any two unrelated scenes into a single arbitrarily large\npanorama. Apart from that, we introduce LHQ: a new dataset of \\lhqsize\nhigh-resolution nature landscapes. We test the approach on LHQ, LSUN Tower and\nLSUN Bridge and outperform the baselines by at least 4 times in terms of\nquality and diversity of the produced infinite images. The project page is\nlocated at https://universome.github.io/alis.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 16:29:20 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Skorokhodov", "Ivan", ""], ["Sotnikov", "Grigorii", ""], ["Elhoseiny", "Mohamed", ""]]}, {"id": "2104.06957", "submitter": "Martin Ferianc", "authors": "Martin Ferianc, Divyansh Manocha, Hongxiang Fan, Miguel Rodrigues", "title": "ComBiNet: Compact Convolutional Bayesian Neural Network for Image\n  Segmentation", "comments": "Accepted for publication at ICANN 2021. Code at:\n  https://github.com/martinferianc/ComBiNet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fully convolutional U-shaped neural networks have largely been the dominant\napproach for pixel-wise image segmentation. In this work, we tackle two defects\nthat hinder their deployment in real-world applications: 1) Predictions lack\nuncertainty quantification that may be crucial to many decision-making systems;\n2) Large memory storage and computational consumption demanding extensive\nhardware resources. To address these issues and improve their practicality we\ndemonstrate a few-parameter compact Bayesian convolutional architecture, that\nachieves a marginal improvement in accuracy in comparison to related work using\nsignificantly fewer parameters and compute operations. The architecture\ncombines parameter-efficient operations such as separable convolutions,\nbilinear interpolation, multi-scale feature propagation and Bayesian inference\nfor per-pixel uncertainty quantification through Monte Carlo Dropout. The best\nperforming configurations required fewer than 2.5 million parameters on diverse\nchallenging datasets with few observations.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 16:33:48 GMT"}, {"version": "v2", "created": "Sat, 26 Jun 2021 11:34:44 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Ferianc", "Martin", ""], ["Manocha", "Divyansh", ""], ["Fan", "Hongxiang", ""], ["Rodrigues", "Miguel", ""]]}, {"id": "2104.06976", "submitter": "Xiang Zhang", "authors": "Ke Li, Shijie Wang, Xiang Zhang, Yifan Xu, Weijian Xu, Zhuowen Tu", "title": "Pose Recognition with Cascade Transformers", "comments": "Accepted to CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a regression-based pose recognition method using\ncascade Transformers. One way to categorize the existing approaches in this\ndomain is to separate them into 1). heatmap-based and 2). regression-based. In\ngeneral, heatmap-based methods achieve higher accuracy but are subject to\nvarious heuristic designs (not end-to-end mostly), whereas regression-based\napproaches attain relatively lower accuracy but they have less intermediate\nnon-differentiable steps. Here we utilize the encoder-decoder structure in\nTransformers to perform regression-based person and keypoint detection that is\ngeneral-purpose and requires less heuristic design compared with the existing\napproaches. We demonstrate the keypoint hypothesis (query) refinement process\nacross different self-attention layers to reveal the recursive self-attention\nmechanism in Transformers. In the experiments, we report competitive results\nfor pose recognition when compared with the competing regression-based methods.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 17:00:22 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Li", "Ke", ""], ["Wang", "Shijie", ""], ["Zhang", "Xiang", ""], ["Xu", "Yifan", ""], ["Xu", "Weijian", ""], ["Tu", "Zhuowen", ""]]}, {"id": "2104.06977", "submitter": "Zixiang Zhao", "authors": "Zixiang Zhao, Jiangshe Zhang, Shuang Xu, Chunxia Zhang, Junmin Liu", "title": "Discrete Cosine Transform Network for Guided Depth Map Super-Resolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Guided depth super-resolution (GDSR) is a hot topic in multi-modal image\nprocessing. The goal is to use high-resolution (HR) RGB images to provide extra\ninformation on edges and object contours, so that low-resolution depth maps can\nbe upsampled to HR ones. To solve the issues of RGB texture over-transferred,\ncross-modal feature extraction difficulty and unclear working mechanism of\nmodules in existing methods, we propose an advanced Discrete Cosine Transform\nNetwork (DCTNet), which is composed of four components. Firstly, the paired\nRGB/depth images are input into the semi-coupled feature extraction module. The\nshared convolution kernels extract the cross-modal common features, and the\nprivate kernels extract their unique features, respectively. Then the RGB\nfeatures are input into the edge attention mechanism to highlight the edges\nuseful for upsampling. Subsequently, in the Discrete Cosine Transform (DCT)\nmodule, where DCT is employed to solve the optimization problem designed for\nimage domain GDSR. The solution is then extended to implement the multi-channel\nRGB/depth features upsampling, which increases the rationality of DCTNet, and\nis more flexible and effective than conventional methods. The final depth\nprediction is output by the reconstruction module. Numerous qualitative and\nquantitative experiments demonstrate the effectiveness of our method, which can\ngenerate accurate and HR depth maps, surpassing state-of-the-art methods.\nMeanwhile, the rationality of modules is also proved by ablation experiments.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 17:01:03 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Zhao", "Zixiang", ""], ["Zhang", "Jiangshe", ""], ["Xu", "Shuang", ""], ["Zhang", "Chunxia", ""], ["Liu", "Junmin", ""]]}, {"id": "2104.06984", "submitter": "Yiyuan Yang", "authors": "Yiyuan Yang, Kenneth Li, Fernanda Eliott, Maithilee Kunda", "title": "Do Time Constraints Re-Prioritize Attention to Shapes During Visual\n  Photo Inspection?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  People's visual experiences of the world are easy to carve up and examine\nalong natural language boundaries, e.g., by category labels, attribute labels,\netc. However, it is more difficult to elicit detailed visuospatial information\nabout what a person attends to, e.g., the specific shape of a tree. Paying\nattention to the shapes of things not only feeds into well defined tasks like\nvisual category learning, but it is also what enables us to differentiate\nsimilarly named objects and to take on creative visual pursuits, like\npoetically describing the shape of a thing, or finding shapes in the clouds or\nstars. We use a new data collection method that elicits people's prioritized\nattention to shapes during visual photo inspection by asking them to trace\nimportant parts of the image under varying time constraints. Using data\ncollected via crowdsourcing over a set of 187 photographs, we examine changes\nin patterns of visual attention across individuals, across image types, and\nacross time constraints.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 17:07:27 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Yang", "Yiyuan", ""], ["Li", "Kenneth", ""], ["Eliott", "Fernanda", ""], ["Kunda", "Maithilee", ""]]}, {"id": "2104.06991", "submitter": "Chun Yang", "authors": "Chun Yang, Franz Rottensteiner, Christian Heipke", "title": "A hierarchical deep learning framework for the consistent classification\n  of land use objects in geospatial databases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Land use as contained in geospatial databases constitutes an essential input\nfor different applica-tions such as urban management, regional planning and\nenvironmental monitoring. In this paper, a hierarchical deep learning framework\nis proposed to verify the land use information. For this purpose, a two-step\nstrategy is applied. First, given high-resolution aerial images, the land cover\ninformation is determined. To achieve this, an encoder-decoder based\nconvolutional neural net-work (CNN) is proposed. Second, the pixel-wise land\ncover information along with the aerial images serves as input for another CNN\nto classify land use. Because the object catalogue of geospatial databases is\nfrequently constructed in a hierarchical manner, we propose a new CNN-based\nmethod aiming to predict land use in multiple levels hierarchically and\nsimultaneously. A so called Joint Optimization (JO) is proposed where\npredictions are made by selecting the hier-archical tuple over all levels which\nhas the maximum joint class scores, providing consistent results across the\ndifferent levels. The conducted experiments show that the CNN relying on JO\noutperforms previous results, achieving an overall accuracy up to 92.5%. In\naddition to the individual experiments on two test sites, we investigate\nwhether data showing different characteristics can improve the results of land\ncover and land use classification, when processed together. To do so, we\ncombine the two datasets and undertake some additional experiments. The results\nshow that adding more data helps both land cover and land use classification,\nespecially the identification of underrepre-sented categories, despite their\ndifferent characteristics.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 17:16:35 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Yang", "Chun", ""], ["Rottensteiner", "Franz", ""], ["Heipke", "Christian", ""]]}, {"id": "2104.07004", "submitter": "Ioannis Kansizoglou", "authors": "Ioannis Kansizoglou, Loukas Bampis, and Antonios Gasteratos", "title": "Do Neural Network Weights account for Classes Centers?", "comments": "Index Terms: Discriminative feature learning, deep neural networks,\n  symmetrical layer, geometric algebra", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The exploitation of Deep Neural Networks (DNNs) as descriptors in feature\nlearning challenges enjoys apparent popularity over the past few years. The\nabove tendency focuses on the development of effective loss functions that\nensure both high feature discrimination among different classes, as well as low\ngeodesic distance between the feature vectors of a given class. The vast\nmajority of the contemporary works rely their formulation on an empirical\nassumption about the feature space of a network's last hidden layer, claiming\nthat the weight vector of a class accounts for its geometrical center in the\nstudied space. The paper at hand follows a theoretical approach and indicates\nthat the aforementioned hypothesis is not exclusively met. This fact raises\nstability issues regarding the training procedure of a DNN, as shown in our\nexperimental study. Consequently, a specific symmetry is proposed and studied\nboth analytically and empirically that satisfies the above assumption,\naddressing the established convergence issues.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 17:37:52 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Kansizoglou", "Ioannis", ""], ["Bampis", "Loukas", ""], ["Gasteratos", "Antonios", ""]]}, {"id": "2104.07021", "submitter": "Aiyu Cui", "authors": "Aiyu Cui, Daniel McKee, Svetlana Lazebnik", "title": "Dressing in Order: Recurrent Person Image Generation for Pose Transfer,\n  Virtual Try-on and Outfit Editing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a flexible person generation framework called Dressing in\nOrder (DiOr), which supports 2D pose transfer, virtual try-on, and several\nfashion editing tasks. Key to DiOr is a novel recurrent generation pipeline to\nsequentially put garments on a person, so that trying on the same garments in\ndifferent orders will result in different looks. Our system can produce\ndressing effects not achievable by existing work, including different\ninteractions of garments (e.g., wearing a top tucked into the bottom or over\nit), as well as layering of multiple garments of the same type (e.g., jacket\nover shirt over t-shirt). DiOr explicitly encodes the shape and texture of each\ngarment, enabling these elements to be edited separately. Joint training on\npose transfer and inpainting helps with detail preservation and coherence of\ngenerated garments. Extensive evaluations show that DiOr outperforms other\nrecent methods like ADGAN in terms of output quality, and handles a wide range\nof editing functions for which there is no direct supervision.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 17:58:54 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Cui", "Aiyu", ""], ["McKee", "Daniel", ""], ["Lazebnik", "Svetlana", ""]]}, {"id": "2104.07056", "submitter": "Bo Zhou", "authors": "Bo Zhou, Zachary Augenfeld, Julius Chapiro, S. Kevin Zhou, Chi Liu,\n  James S. Duncan", "title": "Anatomy-guided Multimodal Registration by Learning Segmentation without\n  Ground Truth: Application to Intraprocedural CBCT/MR Liver Segmentation and\n  Registration", "comments": "12 pages, 8 figures, published at Medical Image Analysis (MedIA),\n  code available at https://github.com/bbbbbbzhou/APA2Seg-Net", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multimodal image registration has many applications in diagnostic medical\nimaging and image-guided interventions, such as Transcatheter Arterial\nChemoembolization (TACE) of liver cancer guided by intraprocedural CBCT and\npre-operative MR. The ability to register peri-procedurally acquired diagnostic\nimages into the intraprocedural environment can potentially improve the\nintra-procedural tumor targeting, which will significantly improve therapeutic\noutcomes. However, the intra-procedural CBCT often suffers from suboptimal\nimage quality due to lack of signal calibration for Hounsfield unit, limited\nFOV, and motion/metal artifacts. These non-ideal conditions make standard\nintensity-based multimodal registration methods infeasible to generate correct\ntransformation across modalities. While registration based on anatomic\nstructures, such as segmentation or landmarks, provides an efficient\nalternative, such anatomic structure information is not always available. One\ncan train a deep learning-based anatomy extractor, but it requires large-scale\nmanual annotations on specific modalities, which are often extremely\ntime-consuming to obtain and require expert radiological readers. To tackle\nthese issues, we leverage annotated datasets already existing in a source\nmodality and propose an anatomy-preserving domain adaptation to segmentation\nnetwork (APA2Seg-Net) for learning segmentation without target modality ground\ntruth. The segmenters are then integrated into our anatomy-guided multimodal\nregistration based on the robust point matching machine. Our experimental\nresults on in-house TACE patient data demonstrated that our APA2Seg-Net can\ngenerate robust CBCT and MR liver segmentation, and the anatomy-guided\nregistration framework with these segmenters can provide high-quality\nmultimodal registrations. Our code is available at\nhttps://github.com/bbbbbbzhou/APA2Seg-Net.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 18:07:03 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Zhou", "Bo", ""], ["Augenfeld", "Zachary", ""], ["Chapiro", "Julius", ""], ["Zhou", "S. Kevin", ""], ["Liu", "Chi", ""], ["Duncan", "James S.", ""]]}, {"id": "2104.07070", "submitter": "Vladan Stojni\\'c", "authors": "Vladan Stojni\\'c (1), Vladimir Risojevi\\'c (1) ((1) Faculty of\n  Electrical Engineering, University of Banja Luka, Bosnia and Herzegovina)", "title": "Self-Supervised Learning of Remote Sensing Scene Representations Using\n  Contrastive Multiview Coding", "comments": "EarthVision 2021 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years self-supervised learning has emerged as a promising candidate\nfor unsupervised representation learning. In the visual domain its applications\nare mostly studied in the context of images of natural scenes. However, its\napplicability is especially interesting in specific areas, like remote sensing\nand medicine, where it is hard to obtain huge amounts of labeled data. In this\nwork, we conduct an extensive analysis of the applicability of self-supervised\nlearning in remote sensing image classification. We analyze the influence of\nthe number and domain of images used for self-supervised pre-training on the\nperformance on downstream tasks. We show that, for the downstream task of\nremote sensing image classification, using self-supervised pre-training on\nremote sensing images can give better results than using supervised\npre-training on images of natural scenes. Besides, we also show that\nself-supervised pre-training can be easily extended to multispectral images\nproducing even better results on our downstream tasks.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 18:25:43 GMT"}, {"version": "v2", "created": "Thu, 3 Jun 2021 17:59:23 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Stojni\u0107", "Vladan", ""], ["Risojevi\u0107", "Vladimir", ""]]}, {"id": "2104.07077", "submitter": "Hyeonwoo Yu", "authors": "Hyeonwoo Yu and Jean Oh", "title": "Self-supervised Learning of 3D Object Understanding by Data Association\n  and Landmark Estimation for Image Sequence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we propose a self-supervised learningmethod for multi-object\npose estimation. 3D object under-standing from 2D image is a challenging task\nthat infers ad-ditional dimension from reduced-dimensional information.In\nparticular, the estimation of the 3D localization or orien-tation of an object\nrequires precise reasoning, unlike othersimple clustering tasks such as object\nclassification. There-fore, the scale of the training dataset becomes more\ncru-cial. However, it is challenging to obtain large amount of3D dataset since\nachieving 3D annotation is expensive andtime-consuming. If the scale of the\ntraining dataset can beincreased by involving the image sequence obtained\nfromsimple navigation, it is possible to overcome the scale lim-itation of the\ndataset and to have efficient adaptation tothe new environment. However, when\nthe self annotation isconducted on single image by the network itself,\ntrainingperformance of the network is bounded to the self perfor-mance.\nTherefore, we propose a strategy to exploit multipleobservations of the object\nin the image sequence in orderto surpass the self-performance: first, the\nlandmarks for theglobal object map are estimated through network predic-tion\nand data association, and the corrected annotation fora single frame is\nobtained. Then, network fine-tuning is con-ducted including the dataset\nobtained by self-annotation,thereby exceeding the performance boundary of the\nnetworkitself. The proposed method was evaluated on the KITTIdriving scene\ndataset, and we demonstrate the performanceimprovement in the pose estimation\nof multi-object in 3D space.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 18:59:08 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Yu", "Hyeonwoo", ""], ["Oh", "Jean", ""]]}, {"id": "2104.07083", "submitter": "Yih Cherng Lee", "authors": "Yih-Cherng Lee, Ling Yeung", "title": "SVS-net: A Novel Semantic Segmentation Network in Optical Coherence\n  Tomography Angiography Images", "comments": "6 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated vascular segmentation on optical coherence tomography angiography\n(OCTA) is important for the quantitative analyses of retinal microvasculature\nin neuroretinal and systemic diseases. Despite recent improvements, artifacts\ncontinue to pose challenges in segmentation. Our study focused on removing the\nspeckle noise artifact from OCTA images when performing segmentation. Speckle\nnoise is common in OCTA and is particularly prominent over large non-perfusion\nareas. It may interfere with the proper assessment of retinal vasculature. In\nthis study, we proposed a novel Supervision Vessel Segmentation network\n(SVS-net) to detect vessels of different sizes. The SVS-net includes a new\nattention-based module to describe vessel positions and facilitate the\nunderstanding of the network learning process. The model is efficient and\nexplainable and could be utilized to reduce the need for manual labeling. Our\nSVS-net had better performance in accuracy, recall, F1 score, and Kappa score\nwhen compared to other well recognized models.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 19:19:17 GMT"}, {"version": "v2", "created": "Sun, 18 Apr 2021 13:59:50 GMT"}, {"version": "v3", "created": "Wed, 21 Apr 2021 09:08:56 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Lee", "Yih-Cherng", ""], ["Yeung", "Ling", ""]]}, {"id": "2104.07085", "submitter": "Hongyi Pan Mr.", "authors": "Hongyi Pan, Diaa Dabawi and Ahmet Enis Cetin", "title": "Fast Walsh-Hadamard Transform and Smooth-Thresholding Based Binary\n  Layers in Deep Neural Networks", "comments": "The paper has been accepted to CVPR 2021 BiVision Workshop. We notice\n  the final Conv2D is also a 1x1 convolution layer so we update the result with\n  changing the layer in v2. In v3, we update citation 37 because its authorship\n  changes", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel layer based on fast Walsh-Hadamard\ntransform (WHT) and smooth-thresholding to replace $1\\times 1$ convolution\nlayers in deep neural networks. In the WHT domain, we denoise the transform\ndomain coefficients using the new smooth-thresholding non-linearity, a smoothed\nversion of the well-known soft-thresholding operator. We also introduce a\nfamily of multiplication-free operators from the basic 2$\\times$2 Hadamard\ntransform to implement $3\\times 3$ depthwise separable convolution layers.\nUsing these two types of layers, we replace the bottleneck layers in\nMobileNet-V2 to reduce the network's number of parameters with a slight loss in\naccuracy. For example, by replacing the final third bottleneck layers, we\nreduce the number of parameters from 2.270M to 540K. This reduces the accuracy\nfrom 95.21\\% to 92.98\\% on the CIFAR-10 dataset. Our approach significantly\nimproves the speed of data processing. The fast Walsh-Hadamard transform has a\ncomputational complexity of $O(m\\log_2 m)$. As a result, it is computationally\nmore efficient than the $1\\times1$ convolution layer. The fast Walsh-Hadamard\nlayer processes a tensor in $\\mathbb{R}^{10\\times32\\times32\\times1024}$ about 2\ntimes faster than $1\\times1$ convolution layer on NVIDIA Jetson Nano computer\nboard.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 19:23:36 GMT"}, {"version": "v2", "created": "Sun, 6 Jun 2021 17:40:08 GMT"}, {"version": "v3", "created": "Thu, 1 Jul 2021 17:04:10 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Pan", "Hongyi", ""], ["Dabawi", "Diaa", ""], ["Cetin", "Ahmet Enis", ""]]}, {"id": "2104.07098", "submitter": "Moustafa Meshry", "authors": "Moustafa Meshry, Yixuan Ren, Larry S Davis, Abhinav Shrivastava", "title": "StEP: Style-based Encoder Pre-training for Multi-modal Image Synthesis", "comments": "IEEE/CVF Conference on Computer Vision and Pattern Recognition\n  (CVPR), 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel approach for multi-modal Image-to-image (I2I) translation.\nTo tackle the one-to-many relationship between input and output domains,\nprevious works use complex training objectives to learn a latent embedding,\njointly with the generator, that models the variability of the output domain.\nIn contrast, we directly model the style variability of images, independent of\nthe image synthesis task. Specifically, we pre-train a generic style encoder\nusing a novel proxy task to learn an embedding of images, from arbitrary\ndomains, into a low-dimensional style latent space. The learned latent space\nintroduces several advantages over previous traditional approaches to\nmulti-modal I2I translation. First, it is not dependent on the target dataset,\nand generalizes well across multiple domains. Second, it learns a more powerful\nand expressive latent space, which improves the fidelity of style capture and\ntransfer. The proposed style pre-training also simplifies the training\nobjective and speeds up the training significantly. Furthermore, we provide a\ndetailed study of the contribution of different loss terms to the task of\nmulti-modal I2I translation, and propose a simple alternative to VAEs to enable\nsampling from unconstrained latent spaces. Finally, we achieve state-of-the-art\nresults on six challenging benchmarks with a simple training objective that\nincludes only a GAN loss and a reconstruction loss.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 19:58:24 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Meshry", "Moustafa", ""], ["Ren", "Yixuan", ""], ["Davis", "Larry S", ""], ["Shrivastava", "Abhinav", ""]]}, {"id": "2104.07135", "submitter": "Juhana Kangaspunta", "authors": "Juhana Kangaspunta, AJ Piergiovanni, Rico Jonschkowski, Michael Ryoo,\n  Anelia Angelova", "title": "Adaptive Intermediate Representations for Video Understanding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A common strategy to video understanding is to incorporate spatial and motion\ninformation by fusing features derived from RGB frames and optical flow. In\nthis work, we introduce a new way to leverage semantic segmentation as an\nintermediate representation for video understanding and use it in a way that\nrequires no additional labeling.\n  Second, we propose a general framework which learns the intermediate\nrepresentations (optical flow and semantic segmentation) jointly with the final\nvideo understanding task and allows the adaptation of the representations to\nthe end goal. Despite the use of intermediate representations within the\nnetwork, during inference, no additional data beyond RGB sequences is needed,\nenabling efficient recognition with a single network.\n  Finally, we present a way to find the optimal learning configuration by\nsearching the best loss weighting via evolution. We obtain more powerful visual\nrepresentations for videos which lead to performance gains over the\nstate-of-the-art.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 21:37:23 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Kangaspunta", "Juhana", ""], ["Piergiovanni", "AJ", ""], ["Jonschkowski", "Rico", ""], ["Ryoo", "Michael", ""], ["Angelova", "Anelia", ""]]}, {"id": "2104.07158", "submitter": "Poojan Oza", "authors": "Poojan Oza, Vishal M. Patel", "title": "Federated Learning-based Active Authentication on Mobile Devices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  User active authentication on mobile devices aims to learn a model that can\ncorrectly recognize the enrolled user based on device sensor information. Due\nto lack of negative class data, it is often modeled as a one-class\nclassification problem. In practice, mobile devices are connected to a central\nserver, e.g, all android-based devices are connected to Google server through\ninternet. This device-server structure can be exploited by recently proposed\nFederated Learning (FL) and Split Learning (SL) frameworks to perform\ncollaborative learning over the data distributed among multiple devices. Using\nFL/SL frameworks, we can alleviate the lack of negative data problem by\ntraining a user authentication model over multiple user data distributed across\ndevices. To this end, we propose a novel user active authentication training,\ntermed as Federated Active Authentication (FAA), that utilizes the principles\nof FL/SL. We first show that existing FL/SL methods are suboptimal for FAA as\nthey rely on the data to be distributed homogeneously (i.e. IID) across\ndevices, which is not true in the case of FAA. Subsequently, we propose a novel\nmethod that is able to tackle heterogeneous/non-IID distribution of data in\nFAA. Specifically, we first extract feature statistics such as mean and\nvariance corresponding to data from each user which are later combined in a\ncentral server to learn a multi-class classifier and sent back to the\nindividual devices. We conduct extensive experiments using three active\nauthentication benchmark datasets (MOBIO, UMDAA-01, UMDAA-02) and show that\nsuch approach performs better than state-of-the-art one-class based FAA methods\nand is also able to outperform traditional FL/SL methods.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 22:59:08 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Oza", "Poojan", ""], ["Patel", "Vishal M.", ""]]}, {"id": "2104.07164", "submitter": "Jiangpeng He", "authors": "Jiangpeng He and Fengqing Zhu", "title": "Unsupervised Continual Learning Via Pseudo Labels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continual learning, a promising future learning strategy, aims to learn new\ntasks incrementally using less computation and memory resources instead of\nretraining the model from scratch whenever new task arrives. However, existing\napproaches are designed in supervised fashion assuming all data from new tasks\nhave been annotated, which are not practical for many real-life applications.\nIn this work, we introduce a new framework to make continual learning feasible\nin unsupervised mode by using pseudo label obtained from cluster assignments to\nupdate model. We focus on image classification task under class-incremental\nsetting and assume no class label is provided for training in each incremental\nlearning step. For illustration purpose, we apply k-means clustering, knowledge\ndistillation loss and exemplar set as our baseline solution, which achieves\ncompetitive results even compared with supervised approaches on both\nchallenging CIFAR-100 and ImageNet (ILSVRC) datasets. We also demonstrate that\nthe performance of our baseline solution can be further improved by\nincorporating recently developed supervised continual learning techniques,\nshowing great potential for our framework to minimize the gap between\nsupervised and unsupervised continual learning.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 23:46:17 GMT"}, {"version": "v2", "created": "Wed, 21 Apr 2021 22:53:32 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["He", "Jiangpeng", ""], ["Zhu", "Fengqing", ""]]}, {"id": "2104.07177", "submitter": "Chao Cai Chris", "authors": "Chao Cai, Ruinan Jin, Peng Wang, Liyuan Ye, Hongbo Jiang, Jun Luo", "title": "PURE: Passive mUlti-peRson idEntification via Deep Footstep Separation\n  and Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently, \\textit{passive behavioral biometrics} (e.g., gesture or footstep)\nhave become promising complements to conventional user identification methods\n(e.g., face or fingerprint) under special situations, yet existing sensing\ntechnologies require lengthy measurement traces and cannot identify multiple\nusers at the same time. To this end, we propose \\systemname\\ as a passive\nmulti-person identification system leveraging deep learning enabled footstep\nseparation and recognition. \\systemname\\ passively identifies a user by\ndeciphering the unique \"footprints\" in its footstep. Different from existing\ngait-enabled recognition systems incurring a long sensing delay to acquire many\nfootsteps, \\systemname\\ can recognize a person by as few as only one step,\nsubstantially cutting the identification latency. To make \\systemname\\ adaptive\nto walking pace variations, environmental dynamics, and even unseen targets, we\napply an adversarial learning technique to improve its domain generalisability\nand identification accuracy. Finally, \\systemname\\ can defend itself against\nreplay attack, enabled by the richness of footstep and spatial awareness. We\nimplement a \\systemname\\ prototype using commodity hardware and evaluate it in\ntypical indoor settings. Evaluation results demonstrate a cross-domain\nidentification accuracy of over 90\\%.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 00:28:05 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Cai", "Chao", ""], ["Jin", "Ruinan", ""], ["Wang", "Peng", ""], ["Ye", "Liyuan", ""], ["Jiang", "Hongbo", ""], ["Luo", "Jun", ""]]}, {"id": "2104.07182", "submitter": "Zhaoen Su", "authors": "Zhaoen Su, Chao Wang, David Bradley, Carlos Vallespi-Gonzalez, Carl\n  Wellington, Nemanja Djuric", "title": "Convolutions for Spatial Interaction Modeling", "comments": "Supplementary material included", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In many different fields interactions between objects play a critical role in\ndetermining their behavior. Graph neural networks (GNNs) have emerged as a\npowerful tool for modeling interactions, although often at the cost of adding\nconsiderable complexity and latency. In this paper, we consider the problem of\nspatial interaction modeling in the context of predicting the motion of actors\naround autonomous vehicles, and investigate alternative approaches to GNNs. We\nrevisit convolutions and show that they can demonstrate comparable performance\nto graph networks in modeling spatial interactions with lower latency, thus\nproviding an effective and efficient alternative in time-critical systems.\nMoreover, we propose a novel interaction loss to further improve the\ninteraction modeling of the considered methods.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 00:41:30 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Su", "Zhaoen", ""], ["Wang", "Chao", ""], ["Bradley", "David", ""], ["Vallespi-Gonzalez", "Carlos", ""], ["Wellington", "Carl", ""], ["Djuric", "Nemanja", ""]]}, {"id": "2104.07196", "submitter": "Muhamad Risqi U. Saputra", "authors": "Muhamad Risqi U. Saputra, Chris Xiaoxuan Lu, Pedro P. B. de Gusmao,\n  Bing Wang, Andrew Markham, Niki Trigoni", "title": "Graph-based Thermal-Inertial SLAM with Probabilistic Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Simultaneous Localization and Mapping (SLAM) system typically employ\nvision-based sensors to observe the surrounding environment. However, the\nperformance of such systems highly depends on the ambient illumination\nconditions. In scenarios with adverse visibility or in the presence of airborne\nparticulates (e.g. smoke, dust, etc.), alternative modalities such as those\nbased on thermal imaging and inertial sensors are more promising. In this\npaper, we propose the first complete thermal-inertial SLAM system which\ncombines neural abstraction in the SLAM front end with robust pose graph\noptimization in the SLAM back end. We model the sensor abstraction in the front\nend by employing probabilistic deep learning parameterized by Mixture Density\nNetworks (MDN). Our key strategies to successfully model this encoding from\nthermal imagery are the usage of normalized 14-bit radiometric data, the\nincorporation of hallucinated visual (RGB) features, and the inclusion of\nfeature selection to estimate the MDN parameters. To enable a full SLAM system,\nwe also design an efficient global image descriptor which is able to detect\nloop closures from thermal embedding vectors. We performed extensive\nexperiments and analysis using three datasets, namely self-collected ground\nrobot and handheld data taken in indoor environment, and one public dataset\n(SubT-tunnel) collected in underground tunnel. Finally, we demonstrate that an\naccurate thermal-inertial SLAM system can be realized in conditions of both\nbenign and adverse visibility.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 01:39:15 GMT"}, {"version": "v2", "created": "Sun, 18 Apr 2021 08:35:26 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Saputra", "Muhamad Risqi U.", ""], ["Lu", "Chris Xiaoxuan", ""], ["de Gusmao", "Pedro P. B.", ""], ["Wang", "Bing", ""], ["Markham", "Andrew", ""], ["Trigoni", "Niki", ""]]}, {"id": "2104.07216", "submitter": "Jiawei Liu", "authors": "Jiawei Liu, Jing Zhang, Yicong Hong, Nick Barnes", "title": "Learning structure-aware semantic segmentation with image-level\n  supervision", "comments": null, "journal-ref": "IJCNN2021", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compared with expensive pixel-wise annotations, image-level labels make it\npossible to learn semantic segmentation in a weakly-supervised manner. Within\nthis pipeline, the class activation map (CAM) is obtained and further processed\nto serve as a pseudo label to train the semantic segmentation model in a\nfully-supervised manner. In this paper, we argue that the lost structure\ninformation in CAM limits its application in downstream semantic segmentation,\nleading to deteriorated predictions. Furthermore, the inconsistent class\nactivation scores inside the same object contradicts the common sense that each\nregion of the same object should belong to the same semantic category. To\nproduce sharp prediction with structure information, we introduce an auxiliary\nsemantic boundary detection module, which penalizes the deteriorated\npredictions. Furthermore, we adopt smoothness loss to encourage prediction\ninside the object to be consistent. Experimental results on the PASCAL-VOC\ndataset illustrate the effectiveness of the proposed solution.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 03:33:20 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Liu", "Jiawei", ""], ["Zhang", "Jing", ""], ["Hong", "Yicong", ""], ["Barnes", "Nick", ""]]}, {"id": "2104.07234", "submitter": "Kamal Chandra Paul", "authors": "Kamal Chandra Paul, Semih Aslan", "title": "An Improved Real-Time Face Recognition System at Low Resolution Based on\n  Local Binary Pattern Histogram Algorithm and CLAHE", "comments": "Journal, Optics and Photonics Journal", "journal-ref": "Optics and Photonics Journal, 2021, 11, 63-78", "doi": "10.4236/opj.2021.114005", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This research presents an improved real-time face recognition system at a low\nresolution of 15 pixels with pose and emotion and resolution variations. We\nhave designed our datasets named LRD200 and LRD100, which have been used for\ntraining and classification. The face detection part uses the Viola-Jones\nalgorithm, and the face recognition part receives the face image from the face\ndetection part to process it using the Local Binary Pattern Histogram (LBPH)\nalgorithm with preprocessing using contrast limited adaptive histogram\nequalization (CLAHE) and face alignment. The face database in this system can\nbe updated via our custom-built standalone android app and automatic restarting\nof the training and recognition process with an updated database. Using our\nproposed algorithm, a real-time face recognition accuracy of 78.40% at 15 px\nand 98.05% at 45 px have been achieved using the LRD200 database containing 200\nimages per person. With 100 images per person in the database (LRD100) the\nachieved accuracies are 60.60% at 15 px and 95% at 45 px respectively. A facial\ndeflection of about 30 degrees on either side from the front face showed an\naverage face recognition precision of 72.25% - 81.85%. This face recognition\nsystem can be employed for law enforcement purposes, where the surveillance\ncamera captures a low-resolution image because of the distance of a person from\nthe camera. It can also be used as a surveillance system in airports, bus\nstations, etc., to reduce the risk of possible criminal threats.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 04:54:29 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Paul", "Kamal Chandra", ""], ["Aslan", "Semih", ""]]}, {"id": "2104.07235", "submitter": "Jong Chul Ye", "authors": "Sangjoon Park, Gwanghyun Kim, Yujin Oh, Joon Beom Seo, Sang Min Lee,\n  Jin Hwan Kim, Sungjun Moon, Jae-Kwang Lim, Jong Chul Ye", "title": "Vision Transformer using Low-level Chest X-ray Feature Corpus for\n  COVID-19 Diagnosis and Severity Quantification", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Developing a robust algorithm to diagnose and quantify the severity of\nCOVID-19 using Chest X-ray (CXR) requires a large number of well-curated\nCOVID-19 datasets, which is difficult to collect under the global COVID-19\npandemic. On the other hand, CXR data with other findings are abundant. This\nsituation is ideally suited for the Vision Transformer (ViT) architecture,\nwhere a lot of unlabeled data can be used through structural modeling by the\nself-attention mechanism. However, the use of existing ViT is not optimal,\nsince feature embedding through direct patch flattening or ResNet backbone in\nthe standard ViT is not intended for CXR. To address this problem, here we\npropose a novel Vision Transformer that utilizes low-level CXR feature corpus\nobtained from a backbone network that extracts common CXR findings.\nSpecifically, the backbone network is first trained with large public datasets\nto detect common abnormal findings such as consolidation, opacity, edema, etc.\nThen, the embedded features from the backbone network are used as corpora for a\nTransformer model for the diagnosis and the severity quantification of\nCOVID-19. We evaluate our model on various external test datasets from totally\ndifferent institutions to evaluate the generalization capability. The\nexperimental results confirm that our model can achieve the state-of-the-art\nperformance in both diagnosis and severity quantification tasks with superior\ngeneralization capability, which are sine qua non of widespread deployment.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 04:54:48 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Park", "Sangjoon", ""], ["Kim", "Gwanghyun", ""], ["Oh", "Yujin", ""], ["Seo", "Joon Beom", ""], ["Lee", "Sang Min", ""], ["Kim", "Jin Hwan", ""], ["Moon", "Sungjun", ""], ["Lim", "Jae-Kwang", ""], ["Ye", "Jong Chul", ""]]}, {"id": "2104.07240", "submitter": "Osman Tursun", "authors": "Osman Tursun, Simon Denman, Sridha Sridharan, Clinton Fookes", "title": "Learning Regional Attention over Multi-resolution Deep Convolutional\n  Features for Trademark Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale trademark retrieval is an important content-based image retrieval\ntask. A recent study shows that off-the-shelf deep features aggregated with\nRegional-Maximum Activation of Convolutions (R-MAC) achieve state-of-the-art\nresults. However, R-MAC suffers in the presence of background clutter/trivial\nregions and scale variance, and discards important spatial information. We\nintroduce three simple but effective modifications to R-MAC to overcome these\ndrawbacks. First, we propose the use of both sum and max pooling to minimise\nthe loss of spatial information. We also employ domain-specific unsupervised\nsoft-attention to eliminate background clutter and unimportant regions.\nFinally, we add multi-resolution inputs to enhance the scale-invariance of\nR-MAC. We evaluate these three modifications on the million-scale METU dataset.\nOur results show that all modifications bring non-trivial improvements, and\nsurpass previous state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 05:18:28 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Tursun", "Osman", ""], ["Denman", "Simon", ""], ["Sridharan", "Sridha", ""], ["Fookes", "Clinton", ""]]}, {"id": "2104.07255", "submitter": "S\\'ebastien Arnold", "authors": "S\\'ebastien M. R. Arnold and Fei Sha", "title": "Embedding Adaptation is Still Needed for Few-Shot Learning", "comments": "In submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Constructing new and more challenging tasksets is a fruitful methodology to\nanalyse and understand few-shot classification methods. Unfortunately, existing\napproaches to building those tasksets are somewhat unsatisfactory: they either\nassume train and test task distributions to be identical -- which leads to\noverly optimistic evaluations -- or take a \"worst-case\" philosophy -- which\ntypically requires additional human labor such as obtaining semantic class\nrelationships. We propose ATG, a principled clustering method to defining train\nand test tasksets without additional human knowledge. ATG models train and test\ntask distributions while requiring them to share a predefined amount of\ninformation. We empirically demonstrate the effectiveness of ATG in generating\ntasksets that are easier, in-between, or harder than existing benchmarks,\nincluding those that rely on semantic information. Finally, we leverage our\ngenerated tasksets to shed a new light on few-shot classification:\ngradient-based methods -- previously believed to underperform -- can outperform\nmetric-based ones when transfer is most challenging.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 06:00:04 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Arnold", "S\u00e9bastien M. R.", ""], ["Sha", "Fei", ""]]}, {"id": "2104.07256", "submitter": "Chunhua Shen", "authors": "Jianlong Yuan, Yifan Liu, Chunhua Shen, Zhibin Wang, Hao Li", "title": "A Simple Baseline for Semi-supervised Semantic Segmentation with Strong\n  Data Augmentation", "comments": "11 pages. Fixed typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Recently, significant progress has been made on semantic segmentation.\nHowever, the success of supervised semantic segmentation typically relies on a\nlarge amount of labelled data, which is time-consuming and costly to obtain.\nInspired by the success of semi-supervised learning methods in image\nclassification, here we propose a simple yet effective semi-supervised learning\nframework for semantic segmentation. We demonstrate that the devil is in the\ndetails: a set of simple design and training techniques can collectively\nimprove the performance of semi-supervised semantic segmentation significantly.\nPrevious works [3, 27] fail to employ strong augmentation in pseudo label\nlearning efficiently, as the large distribution change caused by strong\naugmentation harms the batch normalisation statistics. We design a new batch\nnormalisation, namely distribution-specific batch normalisation (DSBN) to\naddress this problem and demonstrate the importance of strong augmentation for\nsemantic segmentation. Moreover, we design a self correction loss which is\neffective in noise resistance. We conduct a series of ablation studies to show\nthe effectiveness of each component. Our method achieves state-of-the-art\nresults in the semi-supervised settings on the Cityscapes and Pascal VOC\ndatasets.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 06:01:39 GMT"}, {"version": "v2", "created": "Mon, 19 Apr 2021 08:11:16 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Yuan", "Jianlong", ""], ["Liu", "Yifan", ""], ["Shen", "Chunhua", ""], ["Wang", "Zhibin", ""], ["Li", "Hao", ""]]}, {"id": "2104.07267", "submitter": "Patrick Grady", "authors": "Patrick Grady, Chengcheng Tang, Christopher D. Twigg, Minh Vo, Samarth\n  Brahmbhatt, Charles C. Kemp", "title": "ContactOpt: Optimizing Contact to Improve Grasps", "comments": "Conference on Computer Vision and Pattern Recognition (CVPR) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Physical contact between hands and objects plays a critical role in human\ngrasps. We show that optimizing the pose of a hand to achieve expected contact\nwith an object can improve hand poses inferred via image-based methods. Given a\nhand mesh and an object mesh, a deep model trained on ground truth contact data\ninfers desirable contact across the surfaces of the meshes. Then, ContactOpt\nefficiently optimizes the pose of the hand to achieve desirable contact using a\ndifferentiable contact model. Notably, our contact model encourages mesh\ninterpenetration to approximate deformable soft tissue in the hand. In our\nevaluations, our methods result in grasps that better match ground truth\ncontact, have lower kinematic error, and are significantly preferred by human\nparticipants. Code and models are available online.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 06:40:51 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Grady", "Patrick", ""], ["Tang", "Chengcheng", ""], ["Twigg", "Christopher D.", ""], ["Vo", "Minh", ""], ["Brahmbhatt", "Samarth", ""], ["Kemp", "Charles C.", ""]]}, {"id": "2104.07268", "submitter": "Boyang Wan Mr", "authors": "Boyang Wan, Yuming Fang, Xue Xia and Jiajie Mei", "title": "Weakly Supervised Video Anomaly Detection via Center-guided\n  Discriminative Learning", "comments": "Accepted in ICME 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Anomaly detection in surveillance videos is a challenging task due to the\ndiversity of anomalous video content and duration. In this paper, we consider\nvideo anomaly detection as a regression problem with respect to anomaly scores\nof video clips under weak supervision. Hence, we propose an anomaly detection\nframework, called Anomaly Regression Net (AR-Net), which only requires\nvideo-level labels in training stage. Further, to learn discriminative features\nfor anomaly detection, we design a dynamic multiple-instance learning loss and\na center loss for the proposed AR-Net. The former is used to enlarge the\ninter-class distance between anomalous and normal instances, while the latter\nis proposed to reduce the intra-class distance of normal instances.\nComprehensive experiments are performed on a challenging benchmark:\nShanghaiTech. Our method yields a new state-of-the-art result for video anomaly\ndetection on ShanghaiTech dataset\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 06:41:23 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Wan", "Boyang", ""], ["Fang", "Yuming", ""], ["Xia", "Xue", ""], ["Mei", "Jiajie", ""]]}, {"id": "2104.07279", "submitter": "Mohammad Reza Feizi Derakhshi", "authors": "Mohammad Saber Iraji, Mohammad-Reza Feizi-Derakhshi, Jafar Tanha", "title": "COVID-19 detection using deep convolutional neural networks and\n  binary-differential-algorithm-based feature selection on X-ray images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The new Coronavirus is spreading rapidly, and it has taken the lives of many\npeople so far. The virus has destructive effects on the human lung, and early\ndetection is very important. Deep Convolution neural networks are such powerful\ntools in classifying images. Therefore, in this paper, a hybrid approach based\non a deep network is presented. Feature vectors were extracted by applying a\ndeep convolution neural network on the images, and useful features were\nselected by the binary differential meta-heuristic algorithm. These optimized\nfeatures were given to the SVM classifier. A database consisting of three\ncategories of images such as COVID-19, pneumonia, and healthy included in 1092\nX-ray samples was considered. The proposed method achieved an accuracy of\n99.43%, a sensitivity of 99.16%, and a specificity of 99.57%. Our results\ndemonstrate that the suggested approach is better than recent studies on\nCOVID-19 detection with X-ray images.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 07:12:58 GMT"}, {"version": "v2", "created": "Sun, 30 May 2021 16:17:19 GMT"}, {"version": "v3", "created": "Mon, 5 Jul 2021 09:45:44 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Iraji", "Mohammad Saber", ""], ["Feizi-Derakhshi", "Mohammad-Reza", ""], ["Tanha", "Jafar", ""]]}, {"id": "2104.07300", "submitter": "Hongsuk Choi", "authors": "Hongsuk Choi, Gyeongsik Moon, JoonKyu Park, Kyoung Mu Lee", "title": "3DCrowdNet: 2D Human Pose-Guided3D Crowd Human Pose and Shape Estimation\n  in the Wild", "comments": "also attached the supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recovering accurate 3D human pose and shape from in-the-wild crowd scenes is\nhighly challenging and barely studied, despite their common presence. In this\nregard, we present 3DCrowdNet, a 2D human pose-guided 3D crowd pose and shape\nestimation system for in-the-wild scenes. 2D human pose estimation methods\nprovide relatively robust outputs on crowd scenes than 3D human pose estimation\nmethods, as they can exploit in-the-wild multi-person 2D datasets that include\ncrowd scenes. On the other hand, the 3D methods leverage 3D datasets, of which\nimages mostly contain a single actor without a crowd. The train data difference\nimpedes the 3D methods' ability to focus on a target person in in-the-wild\ncrowd scenes. Thus, we design our system to leverage the robust 2D pose outputs\nfrom off-the-shelf 2D pose estimators, which guide a network to focus on a\ntarget person and provide essential human articulation information. We show\nthat our 3DCrowdNet outperforms previous methods on in-the-wild crowd scenes.\nWe will release the codes.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 08:21:28 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Choi", "Hongsuk", ""], ["Moon", "Gyeongsik", ""], ["Park", "JoonKyu", ""], ["Lee", "Kyoung Mu", ""]]}, {"id": "2104.07303", "submitter": "Kai Yang", "authors": "Kai Yang, Zhenyu He, Wenjie Pei, Zikun Zhou, Xin Li, Di Yuan and\n  Haijun Zhang", "title": "SiamCorners: Siamese Corner Networks for Visual Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The current Siamese network based on region proposal network (RPN) has\nattracted great attention in visual tracking due to its excellent accuracy and\nhigh efficiency. However, the design of the RPN involves the selection of the\nnumber, scale, and aspect ratios of anchor boxes, which will affect the\napplicability and convenience of the model. Furthermore, these anchor boxes\nrequire complicated calculations, such as calculating their\nintersection-over-union (IoU) with ground truth bounding boxes.Due to the\nproblems related to anchor boxes, we propose a simple yet effective anchor-free\ntracker (named Siamese corner networks, SiamCorners), which is end-to-end\ntrained offline on large-scale image pairs. Specifically, we introduce a\nmodified corner pooling layer to convert the bounding box estimate of the\ntarget into a pair of corner predictions (the bottom-right and the top-left\ncorners). By tracking a target as a pair of corners, we avoid the need to\ndesign the anchor boxes. This will make the entire tracking algorithm more\nflexible and simple than anchorbased trackers. In our network design, we\nfurther introduce a layer-wise feature aggregation strategy that enables the\ncorner pooling module to predict multiple corners for a tracking target in deep\nnetworks. We then introduce a new penalty term that is used to select an\noptimal tracking box in these candidate corners. Finally, SiamCorners achieves\nexperimental results that are comparable to the state-of-art tracker while\nmaintaining a high running speed. In particular, SiamCorners achieves a 53.7%\nAUC on NFS30 and a 61.4% AUC on UAV123, while still running at 42 frames per\nsecond (FPS).\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 08:23:30 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Yang", "Kai", ""], ["He", "Zhenyu", ""], ["Pei", "Wenjie", ""], ["Zhou", "Zikun", ""], ["Li", "Xin", ""], ["Yuan", "Di", ""], ["Zhang", "Haijun", ""]]}, {"id": "2104.07308", "submitter": "Chunyu Li", "authors": "Chunyu Li, Yusuke Monno, and Masatoshi Okutomi", "title": "Spectral MVIR: Joint Reconstruction of 3D Shape and Spectral Reflectance", "comments": "Accepted by ICCP 2021. Project homepage:\n  http://www.ok.sc.e.titech.ac.jp/res/MVIR/smvir.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reconstructing an object's high-quality 3D shape with inherent spectral\nreflectance property, beyond typical device-dependent RGB albedos, opens the\ndoor to applications requiring a high-fidelity 3D model in terms of both\ngeometry and photometry. In this paper, we propose a novel Multi-View Inverse\nRendering (MVIR) method called Spectral MVIR for jointly reconstructing the 3D\nshape and the spectral reflectance for each point of object surfaces from\nmulti-view images captured using a standard RGB camera and low-cost lighting\nequipment such as an LED bulb or an LED projector. Our main contributions are\ntwofold: (i) We present a rendering model that considers both geometric and\nphotometric principles in the image formation by explicitly considering camera\nspectral sensitivity, light's spectral power distribution, and light source\npositions. (ii) Based on the derived model, we build a cost-optimization MVIR\nframework for the joint reconstruction of the 3D shape and the per-vertex\nspectral reflectance while estimating the light source positions and the\nshadows. Different from most existing spectral-3D acquisition methods, our\nmethod does not require expensive special equipment and cumbersome geometric\ncalibration. Experimental results using both synthetic and real-world data\ndemonstrate that our Spectral MVIR can acquire a high-quality 3D model with\naccurate spectral reflectance property.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 08:36:23 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Li", "Chunyu", ""], ["Monno", "Yusuke", ""], ["Okutomi", "Masatoshi", ""]]}, {"id": "2104.07350", "submitter": "Byeong-Uk Lee", "authors": "Byeong-Uk Lee, Kyunghyun Lee, In So Kweon", "title": "Depth Completion using Plane-Residual Representation", "comments": "Accepted to CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The basic framework of depth completion is to predict a pixel-wise dense\ndepth map using very sparse input data. In this paper, we try to solve this\nproblem in a more effective way, by reformulating the regression-based depth\nestimation problem into a combination of depth plane classification and\nresidual regression. Our proposed approach is to initially densify sparse depth\ninformation by figuring out which plane a pixel should lie among a number of\ndiscretized depth planes, and then calculate the final depth value by\npredicting the distance from the specified plane. This will help the network to\nlessen the burden of directly regressing the absolute depth information from\nnone, and to effectively obtain more accurate depth prediction result with less\ncomputation power and inference time. To do so, we firstly introduce a novel\nway of interpreting depth information with the closest depth plane label $p$\nand a residual value $r$, as we call it, Plane-Residual (PR) representation. We\nalso propose a depth completion network utilizing PR representation consisting\nof a shared encoder and two decoders, where one classifies the pixel's depth\nplane label, while the other one regresses the normalized distance from the\nclassified depth plane. By interpreting depth information in PR representation\nand using our corresponding depth completion network, we were able to acquire\nimproved depth completion performance with faster computation, compared to\nprevious approaches.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 10:17:53 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Lee", "Byeong-Uk", ""], ["Lee", "Kyunghyun", ""], ["Kweon", "In So", ""]]}, {"id": "2104.07389", "submitter": "Pooja Prajod", "authors": "Pooja Prajod, Dominik Schiller, Tobias Huber, Elisabeth Andr\\'e", "title": "Do Deep Neural Networks Forget Facial Action Units? -- Exploring the\n  Effects of Transfer Learning in Health Related Facial Expression Recognition", "comments": "The 5th International Workshop on Health Intelligence (W3PHIAI-21)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we present a process to investigate the effects of transfer\nlearning for automatic facial expression recognition from emotions to pain. To\nthis end, we first train a VGG16 convolutional neural network to automatically\ndiscern between eight categorical emotions. We then fine-tune successively\nlarger parts of this network to learn suitable representations for the task of\nautomatic pain recognition. Subsequently, we apply those fine-tuned\nrepresentations again to the original task of emotion recognition to further\ninvestigate the differences in performance between the models. In the second\nstep, we use Layer-wise Relevance Propagation to analyze predictions of the\nmodel that have been predicted correctly previously but are now wrongly\nclassified. Based on this analysis, we rely on the visual inspection of a human\nobserver to generate hypotheses about what has been forgotten by the model.\nFinally, we test those hypotheses quantitatively utilizing concept embedding\nanalysis methods. Our results show that the network, which was fully fine-tuned\nfor pain recognition, indeed payed less attention to two action units that are\nrelevant for expression recognition but not for pain recognition.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 11:37:19 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Prajod", "Pooja", ""], ["Schiller", "Dominik", ""], ["Huber", "Tobias", ""], ["Andr\u00e9", "Elisabeth", ""]]}, {"id": "2104.07393", "submitter": "Josef Gugglberger", "authors": "Josef Gugglberger, David Peer, Antonio Rodriguez-Sanchez", "title": "Training Deep Capsule Networks with Residual Connections", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Capsule networks are a type of neural network that have recently gained\nincreased popularity. They consist of groups of neurons, called capsules, which\nencode properties of objects or object parts. The connections between capsules\nencrypt part-whole relationships between objects through routing algorithms\nwhich route the output of capsules from lower level layers to upper level\nlayers. Capsule networks can reach state-of-the-art results on many challenging\ncomputer vision tasks, such as MNIST, Fashion-MNIST, and Small-NORB. However,\nmost capsule network implementations use two to three capsule layers, which\nlimits their applicability as expressivity grows exponentially with depth. One\napproach to overcome such limitations would be to train deeper network\narchitectures, as it has been done for convolutional neural networks with much\nincreased success. In this paper, we propose a methodology to train deeper\ncapsule networks using residual connections, which is evaluated on four\ndatasets and three different routing algorithms. Our experimental results show\nthat in fact, performance increases when training deeper capsule networks. The\nsource code is available on https://github.com/moejoe95/res-capsnet.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 11:42:44 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Gugglberger", "Josef", ""], ["Peer", "David", ""], ["Rodriguez-Sanchez", "Antonio", ""]]}, {"id": "2104.07419", "submitter": "Zitong Yu", "authors": "Zitong Yu, Xiaobai Li, Pichao Wang, Guoying Zhao", "title": "TransRPPG: Remote Photoplethysmography Transformer for 3D Mask Face\n  Presentation Attack Detection", "comments": "Submitted to IEEE Signal Processing Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  3D mask face presentation attack detection (PAD) plays a vital role in\nsecuring face recognition systems from emergent 3D mask attacks. Recently,\nremote photoplethysmography (rPPG) has been developed as an intrinsic liveness\nclue for 3D mask PAD without relying on the mask appearance. However, the rPPG\nfeatures for 3D mask PAD are still needed expert knowledge to design manually,\nwhich limits its further progress in the deep learning and big data era. In\nthis letter, we propose a pure rPPG transformer (TransRPPG) framework for\nlearning intrinsic liveness representation efficiently. At first, rPPG-based\nmulti-scale spatial-temporal maps (MSTmap) are constructed from facial skin and\nbackground regions. Then the transformer fully mines the global relationship\nwithin MSTmaps for liveness representation, and gives a binary prediction for\n3D mask detection. Comprehensive experiments are conducted on two benchmark\ndatasets to demonstrate the efficacy of the TransRPPG on both intra- and\ncross-dataset testings. Our TransRPPG is lightweight and efficient (with only\n547K parameters and 763M FLOPs), which is promising for mobile-level\napplications.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 12:33:13 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Yu", "Zitong", ""], ["Li", "Xiaobai", ""], ["Wang", "Pichao", ""], ["Zhao", "Guoying", ""]]}, {"id": "2104.07434", "submitter": "Liangyu Chen", "authors": "Liangyu Chen, Tong Yang, Xiangyu Zhang, Wei Zhang and Jian Sun", "title": "Points as Queries: Weakly Semi-supervised Object Detection by Points", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel point annotated setting for the weakly semi-supervised\nobject detection task, in which the dataset comprises small fully annotated\nimages and large weakly annotated images by points. It achieves a balance\nbetween tremendous annotation burden and detection performance. Based on this\nsetting, we analyze existing detectors and find that these detectors have\ndifficulty in fully exploiting the power of the annotated points. To solve\nthis, we introduce a new detector, Point DETR, which extends DETR by adding a\npoint encoder. Extensive experiments conducted on MS-COCO dataset in various\ndata settings show the effectiveness of our method. In particular, when using\n20% fully labeled data from COCO, our detector achieves a promising\nperformance, 33.3 AP, which outperforms a strong baseline (FCOS) by 2.0 AP, and\nwe demonstrate the point annotations bring over 10 points in various AR\nmetrics.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 13:08:25 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Chen", "Liangyu", ""], ["Yang", "Tong", ""], ["Zhang", "Xiangyu", ""], ["Zhang", "Wei", ""], ["Sun", "Jian", ""]]}, {"id": "2104.07446", "submitter": "Matthias De Lange", "authors": "Eli Verwimp, Matthias De Lange, Tinne Tuytelaars", "title": "Rehearsal revealed: The limits and merits of revisiting samples in\n  continual learning", "comments": "Preprint, code publicly available", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning from non-stationary data streams and overcoming catastrophic\nforgetting still poses a serious challenge for machine learning research.\nRather than aiming to improve state-of-the-art, in this work we provide insight\ninto the limits and merits of rehearsal, one of continual learning's most\nestablished methods. We hypothesize that models trained sequentially with\nrehearsal tend to stay in the same low-loss region after a task has finished,\nbut are at risk of overfitting on its sample memory, hence harming\ngeneralization. We provide both conceptual and strong empirical evidence on\nthree benchmarks for both behaviors, bringing novel insights into the dynamics\nof rehearsal and continual learning in general. Finally, we interpret important\ncontinual learning works in the light of our findings, allowing for a deeper\nunderstanding of their successes.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 13:28:14 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Verwimp", "Eli", ""], ["De Lange", "Matthias", ""], ["Tuytelaars", "Tinne", ""]]}, {"id": "2104.07452", "submitter": "Xinya Ji", "authors": "Xinya Ji, Hang Zhou, Kaisiyuan Wang, Wayne Wu, Chen Change Loy, Xun\n  Cao, Feng Xu", "title": "Audio-Driven Emotional Video Portraits", "comments": "Accepted by CVPR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite previous success in generating audio-driven talking heads, most of\nthe previous studies focus on the correlation between speech content and the\nmouth shape. Facial emotion, which is one of the most important features on\nnatural human faces, is always neglected in their methods. In this work, we\npresent Emotional Video Portraits (EVP), a system for synthesizing high-quality\nvideo portraits with vivid emotional dynamics driven by audios. Specifically,\nwe propose the Cross-Reconstructed Emotion Disentanglement technique to\ndecompose speech into two decoupled spaces, i.e., a duration-independent\nemotion space and a duration dependent content space. With the disentangled\nfeatures, dynamic 2D emotional facial landmarks can be deduced. Then we propose\nthe Target-Adaptive Face Synthesis technique to generate the final high-quality\nvideo portraits, by bridging the gap between the deduced landmarks and the\nnatural head poses of target videos. Extensive experiments demonstrate the\neffectiveness of our method both qualitatively and quantitatively.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 13:37:13 GMT"}, {"version": "v2", "created": "Thu, 20 May 2021 02:48:26 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Ji", "Xinya", ""], ["Zhou", "Hang", ""], ["Wang", "Kaisiyuan", ""], ["Wu", "Wayne", ""], ["Loy", "Chen Change", ""], ["Cao", "Xun", ""], ["Xu", "Feng", ""]]}, {"id": "2104.07461", "submitter": "Min-Hung Chen", "authors": "Min-Hung Chen, Baopu Li, Yingze Bao, Ghassan AlRegib", "title": "Action Segmentation with Mixed Temporal Domain Adaptation", "comments": "Winter Conference on Applications of Computer Vision (WACV) 2020.\n  Website: https://minhungchen.netlify.app/publication/mtda", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main progress for action segmentation comes from densely-annotated data\nfor fully-supervised learning. Since manual annotation for frame-level actions\nis time-consuming and challenging, we propose to exploit auxiliary unlabeled\nvideos, which are much easier to obtain, by shaping this problem as a domain\nadaptation (DA) problem. Although various DA techniques have been proposed in\nrecent years, most of them have been developed only for the spatial direction.\nTherefore, we propose Mixed Temporal Domain Adaptation (MTDA) to jointly align\nframe- and video-level embedded feature spaces across domains, and further\nintegrate with the domain attention mechanism to focus on aligning the\nframe-level features with higher domain discrepancy, leading to more effective\ndomain adaptation. Finally, we evaluate our proposed methods on three\nchallenging datasets (GTEA, 50Salads, and Breakfast), and validate that MTDA\noutperforms the current state-of-the-art methods on all three datasets by large\nmargins (e.g. 6.4% gain on F1@50 and 6.8% gain on the edit score for GTEA).\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 13:48:14 GMT"}, {"version": "v2", "created": "Fri, 16 Apr 2021 00:46:15 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Chen", "Min-Hung", ""], ["Li", "Baopu", ""], ["Bao", "Yingze", ""], ["AlRegib", "Ghassan", ""]]}, {"id": "2104.07473", "submitter": "Xiaoyu Xiang", "authors": "Xiaoyu Xiang, Yapeng Tian, Yulun Zhang, Yun Fu, Jan P. Allebach,\n  Chenliang Xu", "title": "Zooming SlowMo: An Efficient One-Stage Framework for Space-Time Video\n  Super-Resolution", "comments": "Journal version of \"Zooming Slow-Mo: Fast and Accurate One-Stage\n  Space-Time Video Super-Resolution\"(CVPR-2020). 14 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.MM eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we address the space-time video super-resolution, which aims\nat generating a high-resolution (HR) slow-motion video from a low-resolution\n(LR) and low frame rate (LFR) video sequence. A na\\\"ive method is to decompose\nit into two sub-tasks: video frame interpolation (VFI) and video\nsuper-resolution (VSR). Nevertheless, temporal interpolation and spatial\nupscaling are intra-related in this problem. Two-stage approaches cannot fully\nmake use of this natural property. Besides, state-of-the-art VFI or VSR deep\nnetworks usually have a large frame reconstruction module in order to obtain\nhigh-quality photo-realistic video frames, which makes the two-stage approaches\nhave large models and thus be relatively time-consuming. To overcome the\nissues, we present a one-stage space-time video super-resolution framework,\nwhich can directly reconstruct an HR slow-motion video sequence from an input\nLR and LFR video. Instead of reconstructing missing LR intermediate frames as\nVFI models do, we temporally interpolate LR frame features of the missing LR\nframes capturing local temporal contexts by a feature temporal interpolation\nmodule. Extensive experiments on widely used benchmarks demonstrate that the\nproposed framework not only achieves better qualitative and quantitative\nperformance on both clean and noisy LR frames but also is several times faster\nthan recent state-of-the-art two-stage networks. The source code is released in\nhttps://github.com/Mukosame/Zooming-Slow-Mo-CVPR-2020 .\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 17:59:23 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Xiang", "Xiaoyu", ""], ["Tian", "Yapeng", ""], ["Zhang", "Yulun", ""], ["Fu", "Yun", ""], ["Allebach", "Jan P.", ""], ["Xu", "Chenliang", ""]]}, {"id": "2104.07511", "submitter": "Idan Schwartz", "authors": "Idan Schwartz", "title": "Ensemble of MRR and NDCG models for Visual Dialog", "comments": "Accepted to NAACL2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.CV cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Assessing an AI agent that can converse in human language and understand\nvisual content is challenging. Generation metrics, such as BLEU scores favor\ncorrect syntax over semantics. Hence a discriminative approach is often used,\nwhere an agent ranks a set of candidate options. The mean reciprocal rank (MRR)\nmetric evaluates the model performance by taking into account the rank of a\nsingle human-derived answer. This approach, however, raises a new challenge:\nthe ambiguity and synonymy of answers, for instance, semantic equivalence\n(e.g., `yeah' and `yes'). To address this, the normalized discounted cumulative\ngain (NDCG) metric has been used to capture the relevance of all the correct\nanswers via dense annotations. However, the NDCG metric favors the usually\napplicable uncertain answers such as `I don't know. Crafting a model that\nexcels on both MRR and NDCG metrics is challenging. Ideally, an AI agent should\nanswer a human-like reply and validate the correctness of any answer. To\naddress this issue, we describe a two-step non-parametric ranking approach that\ncan merge strong MRR and NDCG models. Using our approach, we manage to keep\nmost MRR state-of-the-art performance (70.41% vs. 71.24%) and the NDCG\nstate-of-the-art performance (72.16% vs. 75.35%). Moreover, our approach won\nthe recent Visual Dialog 2020 challenge. Source code is available at\nhttps://github.com/idansc/mrr-ndcg.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 15:09:32 GMT"}, {"version": "v2", "created": "Mon, 21 Jun 2021 16:52:11 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Schwartz", "Idan", ""]]}, {"id": "2104.07516", "submitter": "Chengtang Yao", "authors": "Chengtang Yao, Yunde Jia, Huijun Di, Pengxiang Li, Yuwei Wu", "title": "A Decomposition Model for Stereo Matching", "comments": "CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we present a decomposition model for stereo matching to solve\nthe problem of excessive growth in computational cost (time and memory cost) as\nthe resolution increases. In order to reduce the huge cost of stereo matching\nat the original resolution, our model only runs dense matching at a very low\nresolution and uses sparse matching at different higher resolutions to recover\nthe disparity of lost details scale-by-scale. After the decomposition of stereo\nmatching, our model iteratively fuses the sparse and dense disparity maps from\nadjacent scales with an occlusion-aware mask. A refinement network is also\napplied to improving the fusion result. Compared with high-performance methods\nlike PSMNet and GANet, our method achieves $10-100\\times$ speed increase while\nobtaining comparable disparity estimation results.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 15:16:23 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Yao", "Chengtang", ""], ["Jia", "Yunde", ""], ["Di", "Huijun", ""], ["Li", "Pengxiang", ""], ["Wu", "Yuwei", ""]]}, {"id": "2104.07528", "submitter": "Kilian Kleeberger", "authors": "Kilian Kleeberger, Markus V\\\"olk, Richard Bormann, Marco F. Huber", "title": "Investigations on Output Parameterizations of Neural Networks for Single\n  Shot 6D Object Pose Estimation", "comments": "Accepted at 2021 IEEE International Conference on Robotics and\n  Automation (ICRA 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single shot approaches have demonstrated tremendous success on various\ncomputer vision tasks. Finding good parameterizations for 6D object pose\nestimation remains an open challenge. In this work, we propose different novel\nparameterizations for the output of the neural network for single shot 6D\nobject pose estimation. Our learning-based approach achieves state-of-the-art\nperformance on two public benchmark datasets. Furthermore, we demonstrate that\nthe pose estimates can be used for real-world robotic grasping tasks without\nadditional ICP refinement.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 15:29:53 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Kleeberger", "Kilian", ""], ["V\u00f6lk", "Markus", ""], ["Bormann", "Richard", ""], ["Huber", "Marco F.", ""]]}, {"id": "2104.07538", "submitter": "Laura von Rueden", "authors": "Laura von Rueden, Tim Wirtz, Fabian Hueger, Jan David Schneider, Nico\n  Piatkowski, Christian Bauckhage", "title": "Street-Map Based Validation of Semantic Segmentation in Autonomous\n  Driving", "comments": "Final version accepted at the International Conference on Pattern\n  Recognition (ICPR). arXiv admin note: substantial text overlap with\n  arXiv:2011.08008", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial intelligence for autonomous driving must meet strict requirements\non safety and robustness, which motivates the thorough validation of learned\nmodels. However, current validation approaches mostly require ground truth data\nand are thus both cost-intensive and limited in their applicability. We propose\nto overcome these limitations by a model agnostic validation using a-priori\nknowledge from street maps. In particular, we show how to validate semantic\nsegmentation masks and demonstrate the potential of our approach using\nOpenStreetMap. We introduce validation metrics that indicate false positive or\nnegative road segments. Besides the validation approach, we present a method to\ncorrect the vehicle's GPS position so that a more accurate localization can be\nused for the street-map based validation. Lastly, we present quantitative\nresults on the Cityscapes dataset indicating that our validation approach can\nindeed uncover errors in semantic segmentation masks.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 15:48:11 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["von Rueden", "Laura", ""], ["Wirtz", "Tim", ""], ["Hueger", "Fabian", ""], ["Schneider", "Jan David", ""], ["Piatkowski", "Nico", ""], ["Bauckhage", "Christian", ""]]}, {"id": "2104.07566", "submitter": "Fanyi Wang", "authors": "Fanyi Wang, Haotian Hu, Cheng Shen", "title": "BAM: A Lightweight and Efficient Balanced Attention Mechanism for Single\n  Image Super Resolution", "comments": "13 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Attention mechanism has shown enormous potential for single image\nsuper-resolution (SISR). However, existing works only proposed some attention\nmechanism for a specific network. A universal attention mechanism for SISR,\nwhich could further improve the performance of networks without attention and\nprovide a baseline for networks with attention, is still lacking. To fit this\ngap, we propose a lightweight and efficient Balanced Attention Mechanism (BAM),\nwhich consists of Avgpool Channel Attention Module (ACAM) and Maxpool Spatial\nAttention Module (MSAM) in parallel. The information extraction mechanism of\nACAM and MSAM effectively filters redundant information, making the overall\nstructure of BAM very lightweight. Owing to the parallel structure, during the\ngradient backpropagation process of BAM, ACAM and MSAM not only conduct\nself-optimization, but also mutual optimization so as to generate more balanced\nattention information. To verify the effectiveness and robustness of BAM, we\napplied it to 12 state-ofthe-art SISR networks. The results on 4 benchmark\ndatasets demonstrate that BAM can efficiently improve the networks'\nperformance, and for those with attention, the substitution with BAM further\nreduces the amount of parameters and increase the inference speed. Moreover,\nablation experiments were conducted to prove the minimalism of BAM.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 16:22:16 GMT"}, {"version": "v2", "created": "Sat, 26 Jun 2021 08:00:21 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Wang", "Fanyi", ""], ["Hu", "Haotian", ""], ["Shen", "Cheng", ""]]}, {"id": "2104.07586", "submitter": "Hongxu Yin", "authors": "Hongxu Yin, Arun Mallya, Arash Vahdat, Jose M. Alvarez, Jan Kautz,\n  Pavlo Molchanov", "title": "See through Gradients: Image Batch Recovery via GradInversion", "comments": "CVPR 2021 accepted paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training deep neural networks requires gradient estimation from data batches\nto update parameters. Gradients per parameter are averaged over a set of data\nand this has been presumed to be safe for privacy-preserving training in joint,\ncollaborative, and federated learning applications. Prior work only showed the\npossibility of recovering input data given gradients under very restrictive\nconditions - a single input point, or a network with no non-linearities, or a\nsmall 32x32 px input batch. Therefore, averaging gradients over larger batches\nwas thought to be safe. In this work, we introduce GradInversion, using which\ninput images from a larger batch (8 - 48 images) can also be recovered for\nlarge networks such as ResNets (50 layers), on complex datasets such as\nImageNet (1000 classes, 224x224 px). We formulate an optimization task that\nconverts random noise into natural images, matching gradients while\nregularizing image fidelity. We also propose an algorithm for target class\nlabel recovery given gradients. We further propose a group consistency\nregularization framework, where multiple agents starting from different random\nseeds work together to find an enhanced reconstruction of original data batch.\nWe show that gradients encode a surprisingly large amount of information, such\nthat all the individual images can be recovered with high fidelity via\nGradInversion, even for complex datasets, deep networks, and large batch sizes.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 16:43:17 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Yin", "Hongxu", ""], ["Mallya", "Arun", ""], ["Vahdat", "Arash", ""], ["Alvarez", "Jose M.", ""], ["Kautz", "Jan", ""], ["Molchanov", "Pavlo", ""]]}, {"id": "2104.07608", "submitter": "Yu-Chuan Su", "authors": "Yu-Chuan Su, Raviteja Vemulapalli, Ben Weiss, Chun-Te Chu, Philip\n  Andrew Mansfield, Lior Shapira, Colvin Pitts", "title": "Camera View Adjustment Prediction for Improving Image Composition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Image composition plays an important role in the quality of a photo. However,\nnot every camera user possesses the knowledge and expertise required for\ncapturing well-composed photos. While post-capture cropping can improve the\ncomposition sometimes, it does not work in many common scenarios in which the\nphotographer needs to adjust the camera view to capture the best shot. To\naddress this issue, we propose a deep learning-based approach that provides\nsuggestions to the photographer on how to adjust the camera view before\ncapturing. By optimizing the composition before a photo is captured, our system\nhelps photographers to capture better photos. As there is no publicly-available\ndataset for this task, we create a view adjustment dataset by repurposing\nexisting image cropping datasets. Furthermore, we propose a two-stage\nsemi-supervised approach that utilizes both labeled and unlabeled images for\ntraining a view adjustment model. Experiment results show that the proposed\nsemi-supervised approach outperforms the corresponding supervised alternatives,\nand our user study results show that the suggested view adjustment improves\nimage composition 79% of the time.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 17:18:31 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Su", "Yu-Chuan", ""], ["Vemulapalli", "Raviteja", ""], ["Weiss", "Ben", ""], ["Chu", "Chun-Te", ""], ["Mansfield", "Philip Andrew", ""], ["Shapira", "Lior", ""], ["Pitts", "Colvin", ""]]}, {"id": "2104.07636", "submitter": "Chitwan Saharia", "authors": "Chitwan Saharia, Jonathan Ho, William Chan, Tim Salimans, David J.\n  Fleet, Mohammad Norouzi", "title": "Image Super-Resolution via Iterative Refinement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present SR3, an approach to image Super-Resolution via Repeated\nRefinement. SR3 adapts denoising diffusion probabilistic models to conditional\nimage generation and performs super-resolution through a stochastic denoising\nprocess. Inference starts with pure Gaussian noise and iteratively refines the\nnoisy output using a U-Net model trained on denoising at various noise levels.\nSR3 exhibits strong performance on super-resolution tasks at different\nmagnification factors, on faces and natural images. We conduct human evaluation\non a standard 8X face super-resolution task on CelebA-HQ, comparing with SOTA\nGAN methods. SR3 achieves a fool rate close to 50%, suggesting photo-realistic\noutputs, while GANs do not exceed a fool rate of 34%. We further show the\neffectiveness of SR3 in cascaded image generation, where generative models are\nchained with super-resolution models, yielding a competitive FID score of 11.3\non ImageNet.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 17:50:42 GMT"}, {"version": "v2", "created": "Wed, 30 Jun 2021 07:34:57 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Saharia", "Chitwan", ""], ["Ho", "Jonathan", ""], ["Chan", "William", ""], ["Salimans", "Tim", ""], ["Fleet", "David J.", ""], ["Norouzi", "Mohammad", ""]]}, {"id": "2104.07645", "submitter": "Jiteng Mu", "authors": "Jiteng Mu, Weichao Qiu, Adam Kortylewski, Alan Yuille, Nuno\n  Vasconcelos, Xiaolong Wang", "title": "A-SDF: Learning Disentangled Signed Distance Functions for Articulated\n  Shape Representation", "comments": "Our project page is available at: https://jitengmu.github.io/A-SDF/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work has made significant progress on using implicit functions, as a\ncontinuous representation for 3D rigid object shape reconstruction. However,\nmuch less effort has been devoted to modeling general articulated objects.\nCompared to rigid objects, articulated objects have higher degrees of freedom,\nwhich makes it hard to generalize to unseen shapes. To deal with the large\nshape variance, we introduce Articulated Signed Distance Functions (A-SDF) to\nrepresent articulated shapes with a disentangled latent space, where we have\nseparate codes for encoding shape and articulation. We assume no prior\nknowledge on part geometry, articulation status, joint type, joint axis, and\njoint location. With this disentangled continuous representation, we\ndemonstrate that we can control the articulation input and animate unseen\ninstances with unseen joint angles. Furthermore, we propose a Test-Time\nAdaptation inference algorithm to adjust our model during inference. We\ndemonstrate our model generalize well to out-of-distribution and unseen data,\ne.g., partial point clouds and real-world depth images.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 17:53:54 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Mu", "Jiteng", ""], ["Qiu", "Weichao", ""], ["Kortylewski", "Adam", ""], ["Yuille", "Alan", ""], ["Vasconcelos", "Nuno", ""], ["Wang", "Xiaolong", ""]]}, {"id": "2104.07652", "submitter": "Robin Rombach", "authors": "Robin Rombach and Patrick Esser and Bj\\\"orn Ommer", "title": "Geometry-Free View Synthesis: Transformers and no 3D Priors", "comments": "Code available at https://git.io/JOnwn", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Is a geometric model required to synthesize novel views from a single image?\nBeing bound to local convolutions, CNNs need explicit 3D biases to model\ngeometric transformations. In contrast, we demonstrate that a transformer-based\nmodel can synthesize entirely novel views without any hand-engineered 3D\nbiases. This is achieved by (i) a global attention mechanism for implicitly\nlearning long-range 3D correspondences between source and target views, and\n(ii) a probabilistic formulation necessary to capture the ambiguity inherent in\npredicting novel views from a single image, thereby overcoming the limitations\nof previous approaches that are restricted to relatively small viewpoint\nchanges. We evaluate various ways to integrate 3D priors into a transformer\narchitecture. However, our experiments show that no such geometric priors are\nrequired and that the transformer is capable of implicitly learning 3D\nrelationships between images. Furthermore, this approach outperforms the state\nof the art in terms of visual quality while covering the full distribution of\npossible realizations. Code is available at https://git.io/JOnwn\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 17:58:05 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Rombach", "Robin", ""], ["Esser", "Patrick", ""], ["Ommer", "Bj\u00f6rn", ""]]}, {"id": "2104.07654", "submitter": "Nicha Dvornek", "authors": "Nicha C. Dvornek, Xiaoxiao Li, Juntang Zhuang, Pamela Ventola, and\n  James S. Duncan", "title": "Demographic-Guided Attention in Recurrent Neural Networks for Modeling\n  Neuropathophysiological Heterogeneity", "comments": "MLMI 2020 (MICCAI Workshop)", "journal-ref": null, "doi": "10.1007/978-3-030-59861-7_37", "report-no": null, "categories": "cs.LG cs.CV eess.IV q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Heterogeneous presentation of a neurological disorder suggests potential\ndifferences in the underlying pathophysiological changes that occur in the\nbrain. We propose to model heterogeneous patterns of functional network\ndifferences using a demographic-guided attention (DGA) mechanism for recurrent\nneural network models for prediction from functional magnetic resonance imaging\n(fMRI) time-series data. The context computed from the DGA head is used to help\nfocus on the appropriate functional networks based on individual demographic\ninformation. We demonstrate improved classification on 3 subsets of the ABIDE I\ndataset used in published studies that have previously produced\nstate-of-the-art results, evaluating performance under a leave-one-site-out\ncross-validation framework for better generalizeability to new data. Finally,\nwe provide examples of interpreting functional network differences based on\nindividual demographic variables.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 17:58:36 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Dvornek", "Nicha C.", ""], ["Li", "Xiaoxiao", ""], ["Zhuang", "Juntang", ""], ["Ventola", "Pamela", ""], ["Duncan", "James S.", ""]]}, {"id": "2104.07658", "submitter": "Weidi Xie", "authors": "Charig Yang, Hala Lamdouar, Erika Lu, Andrew Zisserman, Weidi Xie", "title": "Self-supervised Video Object Segmentation by Motion Grouping", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Animals have evolved highly functional visual systems to understand motion,\nassisting perception even under complex environments. In this paper, we work\ntowards developing a computer vision system able to segment objects by\nexploiting motion cues, i.e. motion segmentation. We make the following\ncontributions: First, we introduce a simple variant of the Transformer to\nsegment optical flow frames into primary objects and the background. Second, we\ntrain the architecture in a self-supervised manner, i.e. without using any\nmanual annotations. Third, we analyze several critical components of our method\nand conduct thorough ablation studies to validate their necessity. Fourth, we\nevaluate the proposed architecture on public benchmarks (DAVIS2016, SegTrackv2,\nand FBMS59). Despite using only optical flow as input, our approach achieves\nsuperior or comparable results to previous state-of-the-art self-supervised\nmethods, while being an order of magnitude faster. We additionally evaluate on\na challenging camouflage dataset (MoCA), significantly outperforming the other\nself-supervised approaches, and comparing favourably to the top supervised\napproach, highlighting the importance of motion cues, and the potential bias\ntowards visual appearance in existing video segmentation models.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 17:59:32 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Yang", "Charig", ""], ["Lamdouar", "Hala", ""], ["Lu", "Erika", ""], ["Zisserman", "Andrew", ""], ["Xie", "Weidi", ""]]}, {"id": "2104.07659", "submitter": "Arun Mallya", "authors": "Zekun Hao, Arun Mallya, Serge Belongie, Ming-Yu Liu", "title": "GANcraft: Unsupervised 3D Neural Rendering of Minecraft Worlds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present GANcraft, an unsupervised neural rendering framework for\ngenerating photorealistic images of large 3D block worlds such as those created\nin Minecraft. Our method takes a semantic block world as input, where each\nblock is assigned a semantic label such as dirt, grass, or water. We represent\nthe world as a continuous volumetric function and train our model to render\nview-consistent photorealistic images for a user-controlled camera. In the\nabsence of paired ground truth real images for the block world, we devise a\ntraining technique based on pseudo-ground truth and adversarial training. This\nstands in contrast to prior work on neural rendering for view synthesis, which\nrequires ground truth images to estimate scene geometry and view-dependent\nappearance. In addition to camera trajectory, GANcraft allows user control over\nboth scene semantics and output style. Experimental results with comparison to\nstrong baselines show the effectiveness of GANcraft on this novel task of\nphotorealistic 3D block world synthesis. The project website is available at\nhttps://nvlabs.github.io/GANcraft/ .\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 17:59:38 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Hao", "Zekun", ""], ["Mallya", "Arun", ""], ["Belongie", "Serge", ""], ["Liu", "Ming-Yu", ""]]}, {"id": "2104.07660", "submitter": "Qianli Ma", "authors": "Qianli Ma, Shunsuke Saito, Jinlong Yang, Siyu Tang, Michael J. Black", "title": "SCALE: Modeling Clothed Humans with a Surface Codec of Articulated Local\n  Elements", "comments": "In CVPR 2021. Project page: https://qianlim.github.io/SCALE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning to model and reconstruct humans in clothing is challenging due to\narticulation, non-rigid deformation, and varying clothing types and topologies.\nTo enable learning, the choice of representation is the key. Recent work uses\nneural networks to parameterize local surface elements. This approach captures\nlocally coherent geometry and non-planar details, can deal with varying\ntopology, and does not require registered training data. However, naively using\nsuch methods to model 3D clothed humans fails to capture fine-grained local\ndeformations and generalizes poorly. To address this, we present three key\ninnovations: First, we deform surface elements based on a human body model such\nthat large-scale deformations caused by articulation are explicitly separated\nfrom topological changes and local clothing deformations. Second, we address\nthe limitations of existing neural surface elements by regressing local\ngeometry from local features, significantly improving the expressiveness.\nThird, we learn a pose embedding on a 2D parameterization space that encodes\nposed body geometry, improving generalization to unseen poses by reducing\nnon-local spurious correlations. We demonstrate the efficacy of our surface\nrepresentation by learning models of complex clothing from point clouds. The\nclothing can change topology and deviate from the topology of the body. Once\nlearned, we can animate previously unseen motions, producing high-quality point\nclouds, from which we generate realistic images with neural rendering. We\nassess the importance of each technical contribution and show that our approach\noutperforms the state-of-the-art methods in terms of reconstruction accuracy\nand inference time. The code is available for research purposes at\nhttps://qianlim.github.io/SCALE .\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 17:59:39 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Ma", "Qianli", ""], ["Saito", "Shunsuke", ""], ["Yang", "Jinlong", ""], ["Tang", "Siyu", ""], ["Black", "Michael J.", ""]]}, {"id": "2104.07661", "submitter": "Dongdong Chen", "authors": "Tianyi Wei and Dongdong Chen and Wenbo Zhou and Jing Liao and Weiming\n  Zhang and Lu Yuan and Gang Hua and Nenghai Yu", "title": "A Simple Baseline for StyleGAN Inversion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the problem of StyleGAN inversion, which plays an\nessential role in enabling the pretrained StyleGAN to be used for real facial\nimage editing tasks. This problem has the high demand for quality and\nefficiency. Existing optimization-based methods can produce high quality\nresults, but the optimization often takes a long time. On the contrary,\nforward-based methods are usually faster but the quality of their results is\ninferior. In this paper, we present a new feed-forward network for StyleGAN\ninversion, with significant improvement in terms of efficiency and quality. In\nour inversion network, we introduce: 1) a shallower backbone with multiple\nefficient heads across scales; 2) multi-layer identity loss and multi-layer\nface parsing loss to the loss function; and 3) multi-stage refinement.\nCombining these designs together forms a simple and efficient baseline method\nwhich exploits all benefits of optimization-based and forward-based methods.\nQuantitative and qualitative results show that our method performs better than\nexisting forward-based methods and comparably to state-of-the-art\noptimization-based methods, while maintaining the high efficiency as well as\nforward-based methods. Moreover, a number of real image editing applications\ndemonstrate the efficacy of our method. Our project page is\n~\\url{https://wty-ustc.github.io/inversion}.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 17:59:49 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Wei", "Tianyi", ""], ["Chen", "Dongdong", ""], ["Zhou", "Wenbo", ""], ["Liao", "Jing", ""], ["Zhang", "Weiming", ""], ["Yuan", "Lu", ""], ["Hua", "Gang", ""], ["Yu", "Nenghai", ""]]}, {"id": "2104.07662", "submitter": "Deepak Pathak", "authors": "Yuqing Du, Olivia Watkins, Trevor Darrell, Pieter Abbeel, Deepak\n  Pathak", "title": "Auto-Tuned Sim-to-Real Transfer", "comments": "ICRA 2021. First two authors contributed equally. Website at\n  https://yuqingd.github.io/autotuned-sim2real/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Policies trained in simulation often fail when transferred to the real world\ndue to the `reality gap' where the simulator is unable to accurately capture\nthe dynamics and visual properties of the real world. Current approaches to\ntackle this problem, such as domain randomization, require prior knowledge and\nengineering to determine how much to randomize system parameters in order to\nlearn a policy that is robust to sim-to-real transfer while also not being too\nconservative. We propose a method for automatically tuning simulator system\nparameters to match the real world using only raw RGB images of the real world\nwithout the need to define rewards or estimate state. Our key insight is to\nreframe the auto-tuning of parameters as a search problem where we iteratively\nshift the simulation system parameters to approach the real-world system\nparameters. We propose a Search Param Model (SPM) that, given a sequence of\nobservations and actions and a set of system parameters, predicts whether the\ngiven parameters are higher or lower than the true parameters used to generate\nthe observations. We evaluate our method on multiple robotic control tasks in\nboth sim-to-sim and sim-to-real transfer, demonstrating significant improvement\nover naive domain randomization. Project videos and code at\nhttps://yuqingd.github.io/autotuned-sim2real/\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 17:59:55 GMT"}, {"version": "v2", "created": "Thu, 20 May 2021 17:58:26 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Du", "Yuqing", ""], ["Watkins", "Olivia", ""], ["Darrell", "Trevor", ""], ["Abbeel", "Pieter", ""], ["Pathak", "Deepak", ""]]}, {"id": "2104.07667", "submitter": "Meng Zhou", "authors": "Meng Zhou, Shanglin Mo", "title": "Shoulder Implant X-Ray Manufacturer Classification: Exploring with\n  Vision Transformer", "comments": "11 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shoulder replacement surgery, also called total shoulder replacement, is a\ncommon and complex surgery in Orthopedics discipline. It involves replacing a\ndead shoulder joint with an artificial implant. In the market, there are many\nartificial implant manufacturers and each of them may produce different\nimplants with different structures compares to other providers. The problem\narises in the following situation: a patient has some problems with the\nshoulder implant accessory and the manufacturer of that implant maybe unknown\nto either the patient or the doctor, therefore, correctly identification of the\nmanufacturer is the key prior to the treatment. In this paper, we will\ndemonstrate different methods for classifying the manufacturer of a shoulder\nimplant. We will use Vision Transformer approach to this task for the first\ntime ever\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 09:13:47 GMT"}, {"version": "v2", "created": "Wed, 21 Apr 2021 15:46:20 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Zhou", "Meng", ""], ["Mo", "Shanglin", ""]]}, {"id": "2104.07689", "submitter": "Junlin Han", "authors": "Junlin Han, Mehrdad Shoeiby, Lars Petersson, Mohammad Ali Armin", "title": "Dual Contrastive Learning for Unsupervised Image-to-Image Translation", "comments": "Accepted to NTIRE, CVPRW 2021. Code is available at\n  https://github.com/JunlinHan/DCLGAN", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised image-to-image translation tasks aim to find a mapping between a\nsource domain X and a target domain Y from unpaired training data. Contrastive\nlearning for Unpaired image-to-image Translation (CUT) yields state-of-the-art\nresults in modeling unsupervised image-to-image translation by maximizing\nmutual information between input and output patches using only one encoder for\nboth domains. In this paper, we propose a novel method based on contrastive\nlearning and a dual learning setting (exploiting two encoders) to infer an\nefficient mapping between unpaired data. Additionally, while CUT suffers from\nmode collapse, a variant of our method efficiently addresses this issue. We\nfurther demonstrate the advantage of our approach through extensive ablation\nstudies demonstrating superior performance comparing to recent approaches in\nmultiple challenging image translation tasks. Lastly, we demonstrate that the\ngap between unsupervised methods and supervised methods can be efficiently\nclosed.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 18:00:22 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Han", "Junlin", ""], ["Shoeiby", "Mehrdad", ""], ["Petersson", "Lars", ""], ["Armin", "Mohammad Ali", ""]]}, {"id": "2104.07713", "submitter": "Xiao Wang", "authors": "Xiao Wang, Guo-Jun Qi", "title": "Contrastive Learning with Stronger Augmentations", "comments": "10 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Representation learning has significantly been developed with the advance of\ncontrastive learning methods. Most of those methods have benefited from various\ndata augmentations that are carefully designated to maintain their identities\nso that the images transformed from the same instance can still be retrieved.\nHowever, those carefully designed transformations limited us to further explore\nthe novel patterns exposed by other transformations. Meanwhile, as found in our\nexperiments, the strong augmentations distorted the images' structures,\nresulting in difficult retrieval. Thus, we propose a general framework called\nContrastive Learning with Stronger Augmentations~(CLSA) to complement current\ncontrastive learning approaches. Here, the distribution divergence between the\nweakly and strongly augmented images over the representation bank is adopted to\nsupervise the retrieval of strongly augmented queries from a pool of instances.\nExperiments on the ImageNet dataset and downstream datasets showed the\ninformation from the strongly augmented images can significantly boost the\nperformance. For example, CLSA achieves top-1 accuracy of 76.2% on ImageNet\nwith a standard ResNet-50 architecture with a single-layer classifier\nfine-tuned, which is almost the same level as 76.5% of supervised results. The\ncode and pre-trained models are available in\nhttps://github.com/maple-research-lab/CLSA.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 18:40:04 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Wang", "Xiao", ""], ["Qi", "Guo-Jun", ""]]}, {"id": "2104.07719", "submitter": "Guangxing Han", "authors": "Guangxing Han, Shiyuan Huang, Jiawei Ma, Yicheng He, Shih-Fu Chang", "title": "Meta Faster R-CNN: Towards Accurate Few-Shot Object Detection with\n  Attentive Feature Alignment", "comments": "14 pages; Typos corrected and references added", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Few-shot object detection (FSOD) aims to detect objects using only few\nexamples. It's critically needed for many practical applications but so far\nremains challenging. We propose a meta-learning based few-shot object detection\nmethod by transferring meta-knowledge learned from data-abundant base classes\nto data-scarce novel classes. Our method incorporates a coarse-to-fine approach\ninto the proposal based object detection framework and integrates prototype\nbased classifiers into both the proposal generation and classification stages.\nTo improve proposal generation for few-shot novel classes, we propose to learn\na lightweight matching network to measure the similarity between each spatial\nposition in the query image feature map and spatially-pooled class features,\ninstead of the traditional object/nonobject classifier, thus generating\ncategory-specific proposals and improving proposal recall for novel classes. To\naddress the spatial misalignment between generated proposals and few-shot class\nexamples, we propose a novel attentive feature alignment method, thus improving\nthe performance of few-shot object detection. Meanwhile we jointly learn a\nFaster R-CNN detection head for base classes. Extensive experiments conducted\non multiple FSOD benchmarks show our proposed approach achieves state of the\nart results under (incremental) few-shot learning settings.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 19:01:27 GMT"}, {"version": "v2", "created": "Mon, 21 Jun 2021 17:30:08 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Han", "Guangxing", ""], ["Huang", "Shiyuan", ""], ["Ma", "Jiawei", ""], ["He", "Yicheng", ""], ["Chang", "Shih-Fu", ""]]}, {"id": "2104.07767", "submitter": "Menglin Jia", "authors": "Menglin Jia, Zuxuan Wu, Austin Reiter, Claire Cardie, Serge Belongie,\n  Ser-Nam Lim", "title": "Exploring Visual Engagement Signals for Representation Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual engagement in social media platforms comprises interactions with photo\nposts including comments, shares, and likes. In this paper, we leverage such\nvisual engagement clues as supervisory signals for representation learning.\nHowever, learning from engagement signals is non-trivial as it is not clear how\nto bridge the gap between low-level visual information and high-level social\ninteractions. We present VisE, a weakly supervised learning approach, which\nmaps social images to pseudo labels derived by clustered engagement signals. We\nthen study how models trained in this way benefit subjective downstream\ncomputer vision tasks such as emotion recognition or political bias detection.\nThrough extensive studies, we empirically demonstrate the effectiveness of VisE\nacross a diverse set of classification tasks beyond the scope of conventional\nrecognition.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 20:50:40 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Jia", "Menglin", ""], ["Wu", "Zuxuan", ""], ["Reiter", "Austin", ""], ["Cardie", "Claire", ""], ["Belongie", "Serge", ""], ["Lim", "Ser-Nam", ""]]}, {"id": "2104.07770", "submitter": "Haojin Yang", "authors": "Haojin Yang, Zhen Shen, Yucheng Zhao", "title": "AsymmNet: Towards ultralight convolution neural networks using\n  asymmetrical bottlenecks", "comments": "MAI@CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep convolutional neural networks (CNN) have achieved astonishing results in\na large variety of applications. However, using these models on mobile or\nembedded devices is difficult due to the limited memory and computation\nresources. Recently, the inverted residual block becomes the dominating\nsolution for the architecture design of compact CNNs. In this work, we\ncomprehensively investigated the existing design concepts, rethink the\nfunctional characteristics of two pointwise convolutions in the inverted\nresiduals. We propose a novel design, called asymmetrical bottlenecks.\nPrecisely, we adjust the first pointwise convolution dimension, enrich the\ninformation flow by feature reuse, and migrate saved computations to the second\npointwise convolution. By doing so we can further improve the accuracy without\nincreasing the computation overhead. The asymmetrical bottlenecks can be\nadopted as a drop-in replacement for the existing CNN blocks. We can thus\ncreate AsymmNet by easily stack those blocks according to proper depth and\nwidth conditions. Extensive experiments demonstrate that our proposed block\ndesign is more beneficial than the original inverted residual bottlenecks for\nmobile networks, especially useful for those ultralight CNNs within the regime\nof <220M MAdds. Code is available at https://github.com/Spark001/AsymmNet\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 20:58:39 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Yang", "Haojin", ""], ["Shen", "Zhen", ""], ["Zhao", "Yucheng", ""]]}, {"id": "2104.07778", "submitter": "Devis Tuia", "authors": "Devis Tuia, Claudio Persello, Lorenzo Bruzzone", "title": "Recent Advances in Domain Adaptation for the Classification of Remote\n  Sensing Data", "comments": null, "journal-ref": "IEEE Geoscience and Remote Sensing Magazine, 4(2): 41-57, 2016", "doi": "10.1109/MGRS.2016.2548504", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The success of supervised classification of remotely sensed images acquired\nover large geographical areas or at short time intervals strongly depends on\nthe representativity of the samples used to train the classification algorithm\nand to define the model. When training samples are collected from an image (or\na spatial region) different from the one used for mapping, spectral shifts\nbetween the two distributions are likely to make the model fail. Such shifts\nare generally due to differences in acquisition and atmospheric conditions or\nto changes in the nature of the object observed. In order to design\nclassification methods that are robust to data-set shifts, recent remote\nsensing literature has considered solutions based on domain adaptation (DA)\napproaches. Inspired by machine learning literature, several DA methods have\nbeen proposed to solve specific problems in remote sensing data classification.\nThis paper provides a critical review of the recent advances in DA for remote\nsensing and presents an overview of methods divided into four categories: i)\ninvariant feature selection; ii) representation matching; iii) adaptation of\nclassifiers and iv) selective sampling. We provide an overview of recent\nmethodologies, as well as examples of application of the considered techniques\nto real remote sensing images characterized by very high spatial and spectral\nresolution. Finally, we propose guidelines to the selection of the method to\nuse in real application scenarios.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 21:15:48 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Tuia", "Devis", ""], ["Persello", "Claudio", ""], ["Bruzzone", "Lorenzo", ""]]}, {"id": "2104.07784", "submitter": "Devis Tuia", "authors": "Devis Tuia, Michele Volpi, Loris Copa, Mikhail Kanevski, Jordi\n  Munoz-Mari", "title": "A survey of active learning algorithms for supervised remote sensing\n  image classification", "comments": null, "journal-ref": "IEEE Journal of Selected Topics in Signal Processing, 5(3): 606 -\n  617, 2011", "doi": "10.1109/JSTSP.2011.2139193", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Defining an efficient training set is one of the most delicate phases for the\nsuccess of remote sensing image classification routines. The complexity of the\nproblem, the limited temporal and financial resources, as well as the high\nintraclass variance can make an algorithm fail if it is trained with a\nsuboptimal dataset. Active learning aims at building efficient training sets by\niteratively improving the model performance through sampling. A user-defined\nheuristic ranks the unlabeled pixels according to a function of the uncertainty\nof their class membership and then the user is asked to provide labels for the\nmost uncertain pixels. This paper reviews and tests the main families of active\nlearning algorithms: committee, large margin and posterior probability-based.\nFor each of them, the most recent advances in the remote sensing community are\ndiscussed and some heuristics are detailed and tested. Several challenging\nremote sensing scenarios are considered, including very high spatial resolution\nand hyperspectral image classification. Finally, guidelines for choosing the\ngood architecture are provided for new and/or unexperienced user.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 21:36:59 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Tuia", "Devis", ""], ["Volpi", "Michele", ""], ["Copa", "Loris", ""], ["Kanevski", "Mikhail", ""], ["Munoz-Mari", "Jordi", ""]]}, {"id": "2104.07785", "submitter": "A. Ben Hamza", "authors": "Ibrahim Salim and A. Ben Hamza", "title": "Ridge Regression Neural Network for Pediatric Bone Age Assessment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Bone age is an important measure for assessing the skeletal and biological\nmaturity of children. Delayed or increased bone age is a serious concern for\npediatricians, and needs to be accurately assessed in a bid to determine\nwhether bone maturity is occurring at a rate consistent with chronological age.\nIn this paper, we introduce a unified deep learning framework for bone age\nassessment using instance segmentation and ridge regression. The proposed\napproach consists of two integrated stages. In the first stage, we employ an\nimage annotation and segmentation model to annotate and segment the hand from\nthe radiographic image, followed by background removal. In the second stage, we\ndesign a regression neural network architecture composed of a pre-trained\nconvolutional neural network for learning salient features from the segmented\npediatric hand radiographs and a ridge regression output layer for predicting\nthe bone age. Experimental evaluation on a dataset of hand radiographs\ndemonstrates the competitive performance of our approach in comparison with\nexisting deep learning based methods for bone age assessment.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 21:38:22 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Salim", "Ibrahim", ""], ["Hamza", "A. Ben", ""]]}, {"id": "2104.07787", "submitter": "Daniel Hernandez Diaz", "authors": "Daniel Hernandez Diaz, Siyang Qin, Reeve Ingle, Yasuhisa Fujii,\n  Alessandro Bissacco", "title": "Rethinking Text Line Recognition Models", "comments": "11 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we study the problem of text line recognition. Unlike most\napproaches targeting specific domains such as scene-text or handwritten\ndocuments, we investigate the general problem of developing a universal\narchitecture that can extract text from any image, regardless of source or\ninput modality. We consider two decoder families (Connectionist Temporal\nClassification and Transformer) and three encoder modules (Bidirectional LSTMs,\nSelf-Attention, and GRCLs), and conduct extensive experiments to compare their\naccuracy and performance on widely used public datasets of scene and\nhandwritten text. We find that a combination that so far has received little\nattention in the literature, namely a Self-Attention encoder coupled with the\nCTC decoder, when compounded with an external language model and trained on\nboth public and internal data, outperforms all the others in accuracy and\ncomputational complexity. Unlike the more common Transformer-based models, this\narchitecture can handle inputs of arbitrary length, a requirement for universal\nline recognition. Using an internal dataset collected from multiple sources, we\nalso expose the limitations of current public datasets in evaluating the\naccuracy of line recognizers, as the relatively narrow image width and sequence\nlength distributions do not allow to observe the quality degradation of the\nTransformer approach when applied to the transcription of long lines.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 21:43:13 GMT"}, {"version": "v2", "created": "Wed, 21 Apr 2021 21:44:58 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Diaz", "Daniel Hernandez", ""], ["Qin", "Siyang", ""], ["Ingle", "Reeve", ""], ["Fujii", "Yasuhisa", ""], ["Bissacco", "Alessandro", ""]]}, {"id": "2104.07791", "submitter": "Devis Tuia", "authors": "Devis Tuia, Jordi Munoz-Mari", "title": "Learning User's confidence for active learning", "comments": null, "journal-ref": "IEEE Transactions on Geoscience and Remote Sensing, 51(2): 872 -\n  880, 2013", "doi": "10.1109/TGRS.2012.2203605", "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In this paper, we study the applicability of active learning in operative\nscenarios: more particularly, we consider the well-known contradiction between\nthe active learning heuristics, which rank the pixels according to their\nuncertainty, and the user's confidence in labeling, which is related to both\nthe homogeneity of the pixel context and user's knowledge of the scene. We\npropose a filtering scheme based on a classifier that learns the confidence of\nthe user in labeling, thus minimizing the queries where the user would not be\nable to provide a class for the pixel. The capacity of a model to learn the\nuser's confidence is studied in detail, also showing the effect of resolution\nis such a learning task. Experiments on two QuickBird images of different\nresolutions (with and without pansharpening) and considering committees of\nusers prove the efficiency of the filtering scheme proposed, which maximizes\nthe number of useful queries with respect to traditional active learning.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 21:54:27 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Tuia", "Devis", ""], ["Munoz-Mari", "Jordi", ""]]}, {"id": "2104.07803", "submitter": "Devis Tuia", "authors": "Devis Tuia, Michele Volpi, Maxime Trolliet, Gustau Camps-Valls", "title": "Semisupervised Manifold Alignment of Multimodal Remote Sensing Images", "comments": null, "journal-ref": "IEEE Transactions on Geoscience and Remote Sensing, 52(12): 7708 -\n  7720, 2014", "doi": "10.1109/TGRS.2014.2317499", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We introduce a method for manifold alignment of different modalities (or\ndomains) of remote sensing images. The problem is recurrent when a set of\nmultitemporal, multisource, multisensor and multiangular images is available.\nIn these situations, images should ideally be spatially coregistred, corrected\nand compensated for differences in the image domains. Such procedures require\nthe interaction of the user, involve tuning of many parameters and heuristics,\nand are usually applied separately. Changes of sensors and acquisition\nconditions translate into shifts, twists, warps and foldings of the image\ndistributions (or manifolds). The proposed semisupervised manifold alignment\n(SS-MA) method aligns the images working directly on their manifolds, and is\nthus not restricted to images of the same resolutions, either spectral or\nspatial. SS-MA pulls close together samples of the same class while pushing\nthose of different classes apart. At the same time, it preserves the geometry\nof each manifold along the transformation. The method builds a linear\ninvertible transformation to a latent space where all images are alike, and\nreduces to solving a generalized eigenproblem of moderate size. We study the\nperformance of SS-MA in toy examples and in real multiangular, multitemporal,\nand multisource image classification problems. The method performs well for\nstrong deformations and leads to accurate classification for all domains.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 22:20:31 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Tuia", "Devis", ""], ["Volpi", "Michele", ""], ["Trolliet", "Maxime", ""], ["Camps-Valls", "Gustau", ""]]}, {"id": "2104.07819", "submitter": "Mohammadreza Mohseni", "authors": "Mohammadreza Mohseni, Jordan Yap, William Yolland, Majid Razmara, M\n  Stella Atkins", "title": "Out-of-Distribution Detection for Dermoscopic Image Classification", "comments": "10 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Medical image diagnosis can be achieved by deep neural networks, provided\nthere is enough varied training data for each disease class. However, a\nhitherto unknown disease class not encountered during training will inevitably\nbe misclassified, even if predicted with low probability. This problem is\nespecially important for medical image diagnosis, when an image of a hitherto\nunknown disease is presented for diagnosis, especially when the images come\nfrom the same image domain, such as dermoscopic skin images.\n  Current out-of-distribution detection algorithms act unfairly when the\nin-distribution classes are imbalanced, by favouring the most numerous disease\nin the training sets. This could lead to false diagnoses for rare cases which\nare often medically important. We developed a novel yet simple method to train\nneural networks, which enables them to classify in-distribution dermoscopic\nskin disease images and also detect novel diseases from dermoscopic images at\ntest time. We show that our BinaryHeads model not only does not hurt\nclassification balanced accuracy when the data is imbalanced, but also\nconsistently improves the balanced accuracy. We also introduce an important\nmethod to investigate the effectiveness of out-of-distribution detection\nmethods based on presence of varying amounts of out-of-distribution data, which\nmay arise in real-world settings.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 23:34:53 GMT"}, {"version": "v2", "created": "Mon, 19 Apr 2021 05:47:57 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Mohseni", "Mohammadreza", ""], ["Yap", "Jordan", ""], ["Yolland", "William", ""], ["Razmara", "Majid", ""], ["Atkins", "M Stella", ""]]}, {"id": "2104.07841", "submitter": "Zhengyu Chen", "authors": "Zhengyu Chen, Jixie Ge, Heshen Zhan, Siteng Huang, Donglin Wang", "title": "Pareto Self-Supervised Training for Few-Shot Learning", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While few-shot learning (FSL) aims for rapid generalization to new concepts\nwith little supervision, self-supervised learning (SSL) constructs supervisory\nsignals directly computed from unlabeled data. Exploiting the complementarity\nof these two manners, few-shot auxiliary learning has recently drawn much\nattention to deal with few labeled data. Previous works benefit from sharing\ninductive bias between the main task (FSL) and auxiliary tasks (SSL), where the\nshared parameters of tasks are optimized by minimizing a linear combination of\ntask losses. However, it is challenging to select a proper weight to balance\ntasks and reduce task conflict. To handle the problem as a whole, we propose a\nnovel approach named as Pareto self-supervised training (PSST) for FSL. PSST\nexplicitly decomposes the few-shot auxiliary problem into multiple constrained\nmulti-objective subproblems with different trade-off preferences, and here a\npreference region in which the main task achieves the best performance is\nidentified. Then, an effective preferred Pareto exploration is proposed to find\na set of optimal solutions in such a preference region. Extensive experiments\non several public benchmark datasets validate the effectiveness of our approach\nby achieving state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 01:26:25 GMT"}, {"version": "v2", "created": "Mon, 19 Apr 2021 02:55:31 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Chen", "Zhengyu", ""], ["Ge", "Jixie", ""], ["Zhan", "Heshen", ""], ["Huang", "Siteng", ""], ["Wang", "Donglin", ""]]}, {"id": "2104.07861", "submitter": "Mingmei Cheng", "authors": "Mingmei Cheng, Le Hui, Jin Xie, Jian Yang", "title": "SSPC-Net: Semi-supervised Semantic 3D Point Cloud Segmentation Network", "comments": "Accepted by AAAI 2021; Project page:\n  \\<https://github.com/MMCheng/SSPC-Net>", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point cloud semantic segmentation is a crucial task in 3D scene\nunderstanding. Existing methods mainly focus on employing a large number of\nannotated labels for supervised semantic segmentation. Nonetheless, manually\nlabeling such large point clouds for the supervised segmentation task is\ntime-consuming. In order to reduce the number of annotated labels, we propose a\nsemi-supervised semantic point cloud segmentation network, named SSPC-Net,\nwhere we train the semantic segmentation network by inferring the labels of\nunlabeled points from the few annotated 3D points. In our method, we first\npartition the whole point cloud into superpoints and build superpoint graphs to\nmine the long-range dependencies in point clouds. Based on the constructed\nsuperpoint graph, we then develop a dynamic label propagation method to\ngenerate the pseudo labels for the unsupervised superpoints. Particularly, we\nadopt a superpoint dropout strategy to dynamically select the generated pseudo\nlabels. In order to fully exploit the generated pseudo labels of the\nunsupervised superpoints, we furthermore propose a coupled attention mechanism\nfor superpoint feature embedding. Finally, we employ the cross-entropy loss to\ntrain the semantic segmentation network with the labels of the supervised\nsuperpoints and the pseudo labels of the unsupervised superpoints. Experiments\non various datasets demonstrate that our semi-supervised segmentation method\ncan achieve better performance than the current semi-supervised segmentation\nmethod with fewer annotated 3D points. Our code is available at\nhttps://github.com/MMCheng/SSPC-Net.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 02:37:27 GMT"}, {"version": "v2", "created": "Mon, 19 Apr 2021 03:18:00 GMT"}, {"version": "v3", "created": "Mon, 24 May 2021 14:04:18 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Cheng", "Mingmei", ""], ["Hui", "Le", ""], ["Xie", "Jin", ""], ["Yang", "Jian", ""]]}, {"id": "2104.07876", "submitter": "Xingxuan Zhang", "authors": "Xingxuan Zhang, Peng Cui, Renzhe Xu, Linjun Zhou, Yue He, Zheyan Shen", "title": "Deep Stable Learning for Out-Of-Distribution Generalization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approaches based on deep neural networks have achieved striking performance\nwhen testing data and training data share similar distribution, but can\nsignificantly fail otherwise. Therefore, eliminating the impact of distribution\nshifts between training and testing data is crucial for building\nperformance-promising deep models. Conventional methods assume either the known\nheterogeneity of training data (e.g. domain labels) or the approximately equal\ncapacities of different domains. In this paper, we consider a more challenging\ncase where neither of the above assumptions holds. We propose to address this\nproblem by removing the dependencies between features via learning weights for\ntraining samples, which helps deep models get rid of spurious correlations and,\nin turn, concentrate more on the true connection between discriminative\nfeatures and labels. Extensive experiments clearly demonstrate the\neffectiveness of our method on multiple distribution generalization benchmarks\ncompared with state-of-the-art counterparts. Through extensive experiments on\ndistribution generalization benchmarks including PACS, VLCS, MNIST-M, and NICO,\nwe show the effectiveness of our method compared with state-of-the-art\ncounterparts.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 03:54:21 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Zhang", "Xingxuan", ""], ["Cui", "Peng", ""], ["Xu", "Renzhe", ""], ["Zhou", "Linjun", ""], ["He", "Yue", ""], ["Shen", "Zheyan", ""]]}, {"id": "2104.07877", "submitter": "Fanyi Wang", "authors": "Fanyi Wang, Yihui Zhang", "title": "A De-raining semantic segmentation network for real-time foreground\n  segmentation", "comments": "12 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Few researches have been proposed specifically for real-time semantic\nsegmentation in rainy environments. However, the demand in this area is huge\nand it is challenging for lightweight networks. Therefore, this paper proposes\na lightweight network which is specially designed for the foreground\nsegmentation in rainy environments, named De-raining Semantic Segmentation\nNetwork (DRSNet). By analyzing the characteristics of raindrops, the\nMultiScaleSE Block is targetedly designed to encode the input image, it uses\nmulti-scale dilated convolutions to increase the receptive field, and SE\nattention mechanism to learn the weights of each channels. In order to combine\nsemantic information between different encoder and decoder layers, it is\nproposed to use Asymmetric Skip, that is, the higher semantic layer of encoder\nemploys bilinear interpolation and the output passes through pointwise\nconvolution, then added element-wise to the lower semantic layer of decoder.\nAccording to the control experiments, the performances of MultiScaleSE Block\nand Asymmetric Skip compared with SEResNet18 and Symmetric Skip respectively\nare improved to a certain degree on the Foreground Accuracy index. The\nparameters and the floating point of operations (FLOPs) of DRSNet is only 0.54M\nand 0.20GFLOPs separately. The state-of-the-art results and real-time\nperformances are achieved on both the UESTC all-day Scenery add rain\n(UAS-add-rain) and the Baidu People Segmentation add rain (BPS-add-rain)\nbenchmarks with the input sizes of 192*128, 384*256 and 768*512. The speed of\nDRSNet exceeds all the networks within 1GFLOPs, and Foreground Accuracy index\nis also the best among the similar magnitude networks on both benchmarks.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 04:09:13 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Wang", "Fanyi", ""], ["Zhang", "Yihui", ""]]}, {"id": "2104.07878", "submitter": "Yushan Zheng", "authors": "Yushan Zheng, Zhiguo Jiang, Haopeng Zhang, Fengying Xie, Jun Shi,\n  Chenghai Xue", "title": "Histopathology WSI Encoding based on GCNs for Scalable and Efficient\n  Retrieval of Diagnostically Relevant Regions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Content-based histopathological image retrieval (CBHIR) has become popular in\nrecent years in the domain of histopathological image analysis. CBHIR systems\nprovide auxiliary diagnosis information for pathologists by searching for and\nreturning regions that are contently similar to the region of interest (ROI)\nfrom a pre-established database. While, it is challenging and yet significant\nin clinical applications to retrieve diagnostically relevant regions from a\ndatabase that consists of histopathological whole slide images (WSIs) for a\nquery ROI. In this paper, we propose a novel framework for regions retrieval\nfrom WSI-database based on hierarchical graph convolutional networks (GCNs) and\nHash technique. Compared to the present CBHIR framework, the structural\ninformation of WSI is preserved through graph embedding of GCNs, which makes\nthe retrieval framework more sensitive to regions that are similar in tissue\ndistribution. Moreover, benefited from the hierarchical GCN structures, the\nproposed framework has good scalability for both the size and shape variation\nof ROIs. It allows the pathologist defining query regions using free curves\naccording to the appearance of tissue. Thirdly, the retrieval is achieved based\non Hash technique, which ensures the framework is efficient and thereby\nadequate for practical large-scale WSI-database. The proposed method was\nvalidated on two public datasets for histopathological WSI analysis and\ncompared to the state-of-the-art methods. The proposed method achieved mean\naverage precision above 0.857 on the ACDC-LungHP dataset and above 0.864 on the\nCamelyon16 dataset in the irregular region retrieval tasks, which are superior\nto the state-of-the-art methods. The average retrieval time from a database\nwithin 120 WSIs is 0.802 ms.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 04:12:33 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Zheng", "Yushan", ""], ["Jiang", "Zhiguo", ""], ["Zhang", "Haopeng", ""], ["Xie", "Fengying", ""], ["Shi", "Jun", ""], ["Xue", "Chenghai", ""]]}, {"id": "2104.07905", "submitter": "Yanghao Li", "authors": "Yanghao Li, Tushar Nagarajan, Bo Xiong, Kristen Grauman", "title": "Ego-Exo: Transferring Visual Representations from Third-person to\n  First-person Videos", "comments": "Accepted by CVPR-2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce an approach for pre-training egocentric video models using\nlarge-scale third-person video datasets. Learning from purely egocentric data\nis limited by low dataset scale and diversity, while using purely exocentric\n(third-person) data introduces a large domain mismatch. Our idea is to discover\nlatent signals in third-person video that are predictive of key\negocentric-specific properties. Incorporating these signals as knowledge\ndistillation losses during pre-training results in models that benefit from\nboth the scale and diversity of third-person video data, as well as\nrepresentations that capture salient egocentric properties. Our experiments\nshow that our Ego-Exo framework can be seamlessly integrated into standard\nvideo models; it outperforms all baselines when fine-tuned for egocentric\nactivity recognition, achieving state-of-the-art results on Charades-Ego and\nEPIC-Kitchens-100.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 06:10:10 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Li", "Yanghao", ""], ["Nagarajan", "Tushar", ""], ["Xiong", "Bo", ""], ["Grauman", "Kristen", ""]]}, {"id": "2104.07911", "submitter": "Shiva Azimi", "authors": "Shiva Azimi, Rohan Wadhawan, and Tapan K. Gandhi", "title": "Identifying Water Stress in Chickpea Plant by Analyzing Progressive\n  Changes in Shoot Images using Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  To meet the needs of a growing world population, we need to increase the\nglobal agricultural yields by employing modern, precision, and automated\nfarming methods. In the recent decade, high-throughput plant phenotyping\ntechniques, which combine non-invasive image analysis and machine learning,\nhave been successfully applied to identify and quantify plant health and\ndiseases. However, these image-based machine learning usually do not consider\nplant stress's progressive or temporal nature. This time-invariant approach\nalso requires images showing severe signs of stress to ensure high confidence\ndetections, thereby reducing this approach's feasibility for early detection\nand recovery of plants under stress. In order to overcome the problem mentioned\nabove, we propose a temporal analysis of the visual changes induced in the\nplant due to stress and apply it for the specific case of water stress\nidentification in Chickpea plant shoot images. For this, we have considered an\nimage dataset of two chickpea varieties JG-62 and Pusa-372, under three water\nstress conditions; control, young seedling, and before flowering, captured over\nfive months. We then develop an LSTM-CNN architecture to learn visual-temporal\npatterns from this dataset and predict the water stress category with high\nconfidence. To establish a baseline context, we also conduct a comparative\nanalysis of the CNN architecture used in the proposed model with the other CNN\ntechniques used for the time-invariant classification of water stress. The\nresults reveal that our proposed LSTM-CNN model has resulted in the ceiling\nlevel classification performance of \\textbf{98.52\\%} on JG-62 and\n\\textbf{97.78\\%} on Pusa-372 and the chickpea plant data. Lastly, we perform an\nablation study to determine the LSTM-CNN model's performance on decreasing the\namount of temporal session data used for training.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 06:23:19 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Azimi", "Shiva", ""], ["Wadhawan", "Rohan", ""], ["Gandhi", "Tapan K.", ""]]}, {"id": "2104.07916", "submitter": "Grigorios Chrysos", "authors": "Grigorios G Chrysos, Markos Georgopoulos, Jiankang Deng, Yannis\n  Panagakis", "title": "Polynomial Networks in Deep Classifiers", "comments": "Under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have been the driving force behind the success in\nclassification tasks, e.g., object and audio recognition. Impressive results\nand generalization have been achieved by a variety of recently proposed\narchitectures, the majority of which are seemingly disconnected. In this work,\nwe cast the study of deep classifiers under a unifying framework. In\nparticular, we express state-of-the-art architectures (e.g., residual and\nnon-local networks) in the form of different degree polynomials of the input.\nOur framework provides insights on the inductive biases of each model and\nenables natural extensions building upon their polynomial nature. The efficacy\nof the proposed models is evaluated on standard image and audio classification\nbenchmarks. The expressivity of the proposed models is highlighted both in\nterms of increased model performance as well as model compression. Lastly, the\nextensions allowed by this taxonomy showcase benefits in the presence of\nlimited data and long-tailed data distributions. We expect this taxonomy to\nprovide links between existing domain-specific architectures.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 06:41:20 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Chrysos", "Grigorios G", ""], ["Georgopoulos", "Markos", ""], ["Deng", "Jiankang", ""], ["Panagakis", "Yannis", ""]]}, {"id": "2104.07918", "submitter": "Dingwen Zhang", "authors": "Dingwen Zhang, Junwei Han, Gong Cheng, and Ming-Hsuan Yang", "title": "Weakly Supervised Object Localization and Detection: A Survey", "comments": "IEEE Transactions on Pattern Analysis and Machine Intelligence,\n  Accepted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As an emerging and challenging problem in the computer vision community,\nweakly supervised object localization and detection plays an important role for\ndeveloping new generation computer vision systems and has received significant\nattention in the past decade. As methods have been proposed, a comprehensive\nsurvey of these topics is of great importance. In this work, we review (1)\nclassic models, (2) approaches with feature representations from off-the-shelf\ndeep networks, (3) approaches solely based on deep learning, and (4) publicly\navailable datasets and standard evaluation metrics that are widely used in this\nfield. We also discuss the key challenges in this field, development history of\nthis field, advantages/disadvantages of the methods in each category, the\nrelationships between methods in different categories, applications of the\nweakly supervised object localization and detection methods, and potential\nfuture directions to further promote the development of this research field.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 06:44:50 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Zhang", "Dingwen", ""], ["Han", "Junwei", ""], ["Cheng", "Gong", ""], ["Yang", "Ming-Hsuan", ""]]}, {"id": "2104.07921", "submitter": "Hung Le", "authors": "Hung Le, Nancy F. Chen, Steven C.H. Hoi", "title": "VGNMN: Video-grounded Neural Module Network to Video-Grounded Language\n  Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Neural module networks (NMN) have achieved success in image-grounded tasks\nsuch as Visual Question Answering (VQA) on synthetic images. However, very\nlimited work on NMN has been studied in the video-grounded language tasks.\nThese tasks extend the complexity of traditional visual tasks with the\nadditional visual temporal variance. Motivated by recent NMN approaches on\nimage-grounded tasks, we introduce Video-grounded Neural Module Network (VGNMN)\nto model the information retrieval process in video-grounded language tasks as\na pipeline of neural modules. VGNMN first decomposes all language components to\nexplicitly resolve any entity references and detect corresponding action-based\ninputs from the question. The detected entities and actions are used as\nparameters to instantiate neural module networks and extract visual cues from\nthe video. Our experiments show that VGNMN can achieve promising performance on\ntwo video-grounded language tasks: video QA and video-grounded dialogues.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 06:47:41 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Le", "Hung", ""], ["Chen", "Nancy F.", ""], ["Hoi", "Steven C. H.", ""]]}, {"id": "2104.07925", "submitter": "Tu Vo", "authors": "Tu Vo", "title": "Attention! Stay Focus!", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We develop a deep convolutional neural networks(CNNs) to deal with the blurry\nartifacts caused by the defocus of the camera using dual-pixel images.\nSpecifically, we develop a double attention network which consists of\nattentional encoders, triple locals and global local modules to effectively\nextract useful information from each image in the dual-pixels and select the\nuseful information from each image and synthesize the final output image. We\ndemonstrate the effectiveness of the proposed deblurring algorithm in terms of\nboth qualitative and quantitative aspects by evaluating on the test set in the\nNTIRE 2021 Defocus Deblurring using Dual-pixel Images Challenge. The code, and\ntrained models are available at https://github.com/tuvovan/ATTSF.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 07:04:51 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Vo", "Tu", ""]]}, {"id": "2104.07954", "submitter": "Kaili Wang", "authors": "Kaili Wang, Jose Oramas, Tinne Tuytelaars", "title": "Towards Human-Understandable Visual Explanations:Imperceptible\n  High-frequency Cues Can Better Be Removed", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Explainable AI (XAI) methods focus on explaining what a neural network has\nlearned - in other words, identifying the features that are the most\ninfluential to the prediction. In this paper, we call them \"distinguishing\nfeatures\". However, whether a human can make sense of the generated explanation\nalso depends on the perceptibility of these features to humans. To make sure an\nexplanation is human-understandable, we argue that the capabilities of humans,\nconstrained by the Human Visual System (HVS) and psychophysics, need to be\ntaken into account. We propose the {\\em human perceptibility principle for\nXAI}, stating that, to generate human-understandable explanations, neural\nnetworks should be steered towards focusing on human-understandable cues during\ntraining. We conduct a case study regarding the classification of real vs. fake\nface images, where many of the distinguishing features picked up by standard\nneural networks turn out not to be perceptible to humans. By applying the\nproposed principle, a neural network with human-understandable explanations is\ntrained which, in a user study, is shown to better align with human intuition.\nThis is likely to make the AI more trustworthy and opens the door to humans\nlearning from machines. In the case study, we specifically investigate and\nanalyze the behaviour of the human-imperceptible high spatial frequency\nfeatures in neural networks and XAI methods.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 08:11:30 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Wang", "Kaili", ""], ["Oramas", "Jose", ""], ["Tuytelaars", "Tinne", ""]]}, {"id": "2104.07955", "submitter": "Weiqi Shu", "authors": "Weiqi Shu, Ling Wang, Bolong Liu, and Jie Liu", "title": "LAI Estimation of Cucumber Crop Based on Improved Fully Convolutional\n  Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  LAI (Leaf Area Index) is of great importance for crop yield estimation in\nagronomy. It is directly related to plant growth status, net assimilation rate,\nplant photosynthesis, and carbon dioxide in the environment. How to measure LAI\naccurately and efficiently is the key to the crop yield estimation problem.\nManual measurement consumes a lot of human resources and material resources.\nRemote sensing technology is not suitable for near-Earth LAI measurement.\nBesides, methods based on traditional digital image processing are greatly\naffected by environmental noise and image exposure. Nowadays, deep learning is\nwidely used in many fields. The improved FCN (Fully Convolutional Network) is\nproposed in our study for LAI measure task. Eighty-two cucumber images\ncollected from our greenhouse are labeled to fine-tuning the pre-trained model.\nThe result shows that the improved FCN model performs well on our dataset. Our\nmethod's mean IoU can reach 0.908, which is 11% better than conventional\nmethods and 4.7% better than the basic FCN model.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 08:12:06 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Shu", "Weiqi", ""], ["Wang", "Ling", ""], ["Liu", "Bolong", ""], ["Liu", "Jie", ""]]}, {"id": "2104.07960", "submitter": "Roman Seidel", "authors": "Roman Seidel, Andr\\'e Apitzsch, Gangolf Hirtz", "title": "OmniFlow: Human Omnidirectional Optical Flow", "comments": "CVPRW 2021: The Second OmniCV Workshop: Omnidirectional Computer\n  Vision in Research and Industry", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Optical flow is the motion of a pixel between at least two consecutive video\nframes and can be estimated through an end-to-end trainable convolutional\nneural network. To this end, large training datasets are required to improve\nthe accuracy of optical flow estimation. Our paper presents OmniFlow: a new\nsynthetic omnidirectional human optical flow dataset. Based on a rendering\nengine we create a naturalistic 3D indoor environment with textured rooms,\ncharacters, actions, objects, illumination and motion blur where all components\nof the environment are shuffled during the data capturing process. The\nsimulation has as output rendered images of household activities and the\ncorresponding forward and backward optical flow. To verify the data for\ntraining volumetric correspondence networks for optical flow estimation we\ntrain different subsets of the data and test on OmniFlow with and without\nTest-Time-Augmentation. As a result we have generated 23,653 image pairs and\ncorresponding forward and backward optical flow. Our dataset can be downloaded\nfrom: https://mytuc.org/byfs\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 08:25:20 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Seidel", "Roman", ""], ["Apitzsch", "Andr\u00e9", ""], ["Hirtz", "Gangolf", ""]]}, {"id": "2104.07961", "submitter": "Mingxing Li", "authors": "Mingxing Li, Chang Chen, Xiaoyu Liu, Wei Huang, Yueyi Zhang, Zhiwei\n  Xiong", "title": "Advanced Deep Networks for 3D Mitochondria Instance Segmentation", "comments": "4 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Mitochondria instance segmentation from electron microscopy (EM) images has\nseen notable progress since the introduction of deep learning methods. In this\npaper, we propose two advanced deep networks, named Res-UNet-R and Res-UNet-H,\nfor 3D mitochondria instance segmentation from Rat and Human samples.\nSpecifically, we design a simple yet effective anisotropic convolution block\nand deploy a multi-scale training strategy, which together boost the\nsegmentation performance. Moreover, we enhance the generalizability of the\ntrained models on the test set by adding a denoising operation as\npre-processing. In the Large-scale 3D Mitochondria Instance Segmentation\nChallenge, our team ranks the 1st on the leaderboard at the end of the testing\nphase. Code is available at\nhttps://github.com/Limingxing00/MitoEM2021-Challenge.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 08:27:44 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Li", "Mingxing", ""], ["Chen", "Chang", ""], ["Liu", "Xiaoyu", ""], ["Huang", "Wei", ""], ["Zhang", "Yueyi", ""], ["Xiong", "Zhiwei", ""]]}, {"id": "2104.07977", "submitter": "Rongtai Cai", "authors": "Rongtai Caiand Peng Zhu", "title": "Occlusion-aware Visual Tracker using Spatial Structural Information and\n  Dominant Features", "comments": "8 pages, 5 figures, Journal", "journal-ref": "The International Arab Journal of Information Technology (2021)", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To overcome the problem of occlusion in visual tracking, this paper proposes\nan occlusion-aware tracking algorithm. The proposed algorithm divides the\nobject into discrete image patches according to the pixel distribution of the\nobject by means of clustering. To avoid the drifting of the tracker to false\ntargets, the proposed algorithm extracts the dominant features, such as color\nhistogram or histogram of oriented gradient orientation, from these image\npatches, and uses them as cues for tracking. To enhance the robustness of the\ntracker, the proposed algorithm employs an implicit spatial structure between\nthese patches as another cue for tracking; Afterwards, the proposed algorithm\nincorporates these components into the particle filter framework, which results\nin a robust and precise tracker. Experimental results on color image sequences\nwith different resolutions show that the proposed tracker outperforms the\ncomparison algorithms on handling occlusion in visual tracking.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 09:01:45 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Zhu", "Rongtai Caiand Peng", ""]]}, {"id": "2104.07986", "submitter": "Jia Zheng", "authors": "Cheng Yang and Jia Zheng and Xili Dai and Rui Tang and Yi Ma and\n  Xiaojun Yuan", "title": "Learning to Reconstruct 3D Non-Cuboid Room Layout from a Single RGB\n  Image", "comments": "Code is available at https://github.com/Cyang0515/NonCuboidRoom", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single-image room layout reconstruction aims to reconstruct the enclosed 3D\nstructure of a room from a single image. Most previous work relies on the\ncuboid-shape prior. This paper considers a more general indoor assumption,\ni.e., the room layout consists of a single ceiling, a single floor, and several\nvertical walls. To this end, we first employ Convolutional Neural Networks to\ndetect planes and vertical lines between adjacent walls. Meanwhile, estimating\nthe 3D parameters for each plane. Then, a simple yet effective geometric\nreasoning method is adopted to achieve room layout reconstruction. Furthermore,\nwe optimize the 3D plane parameters to reconstruct a geometrically consistent\nroom layout between planes and lines. The experimental results on public\ndatasets validate the effectiveness and efficiency of our method.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 09:24:08 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Yang", "Cheng", ""], ["Zheng", "Jia", ""], ["Dai", "Xili", ""], ["Tang", "Rui", ""], ["Ma", "Yi", ""], ["Yuan", "Xiaojun", ""]]}, {"id": "2104.07993", "submitter": "Xiangteng He", "authors": "Xiangteng He, Yulin Pan, Mingqian Tang and Yiliang Lv", "title": "Self-supervised Video Retrieval Transformer Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Content-based video retrieval aims to find videos from a large video database\nthat are similar to or even near-duplicate of a given query video. Video\nrepresentation and similarity search algorithms are crucial to any video\nretrieval system. To derive effective video representation, most video\nretrieval systems require a large amount of manually annotated data for\ntraining, making it costly inefficient. In addition, most retrieval systems are\nbased on frame-level features for video similarity searching, making it\nexpensive both storage wise and search wise. We propose a novel video retrieval\nsystem, termed SVRTN, that effectively addresses the above shortcomings. It\nfirst applies self-supervised training to effectively learn video\nrepresentation from unlabeled data to avoid the expensive cost of manual\nannotation. Then, it exploits transformer structure to aggregate frame-level\nfeatures into clip-level to reduce both storage space and search complexity. It\ncan learn the complementary and discriminative information from the\ninteractions among clip frames, as well as acquire the frame permutation and\nmissing invariant ability to support more flexible retrieval manners.\nComprehensive experiments on two challenging video retrieval datasets, namely\nFIVR-200K and SVD, verify the effectiveness of our proposed SVRTN method, which\nachieves the best performance of video retrieval on accuracy and efficiency.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 09:43:45 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["He", "Xiangteng", ""], ["Pan", "Yulin", ""], ["Tang", "Mingqian", ""], ["Lv", "Yiliang", ""]]}, {"id": "2104.07995", "submitter": "Suzhen Wang", "authors": "Lincheng Li, Suzhen Wang, Zhimeng Zhang, Yu Ding, Yixing Zheng, Xin\n  Yu, Changjie Fan", "title": "Write-a-speaker: Text-based Emotional and Rhythmic Talking-head\n  Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In this paper, we propose a novel text-based talking-head video generation\nframework that synthesizes high-fidelity facial expressions and head motions in\naccordance with contextual sentiments as well as speech rhythm and pauses. To\nbe specific, our framework consists of a speaker-independent stage and a\nspeaker-specific stage. In the speaker-independent stage, we design three\nparallel networks to generate animation parameters of the mouth, upper face,\nand head from texts, separately. In the speaker-specific stage, we present a 3D\nface model guided attention network to synthesize videos tailored for different\nindividuals. It takes the animation parameters as input and exploits an\nattention mask to manipulate facial expression changes for the input\nindividuals. Furthermore, to better establish authentic correspondences between\nvisual motions (i.e., facial expression changes and head movements) and audios,\nwe leverage a high-accuracy motion capture dataset instead of relying on long\nvideos of specific individuals. After attaining the visual and audio\ncorrespondences, we can effectively train our network in an end-to-end fashion.\nExtensive experiments on qualitative and quantitative results demonstrate that\nour algorithm achieves high-quality photo-realistic talking-head videos\nincluding various facial expressions and head motions according to speech\nrhythms and outperforms the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 09:44:12 GMT"}, {"version": "v2", "created": "Fri, 7 May 2021 07:55:08 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Li", "Lincheng", ""], ["Wang", "Suzhen", ""], ["Zhang", "Zhimeng", ""], ["Ding", "Yu", ""], ["Zheng", "Yixing", ""], ["Yu", "Xin", ""], ["Fan", "Changjie", ""]]}, {"id": "2104.08009", "submitter": "Andreas Kurth", "authors": "Andreas Kurth, Fabian Schuiki, Luca Benini", "title": "Implementing CNN Layers on the Manticore Cluster-Based Many-Core\n  Architecture", "comments": "Technical report. 18 pages, 4 figures, 5 algorithms", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This document presents implementations of fundamental convolutional neural\nnetwork (CNN) layers on the Manticore cluster-based many-core architecture and\ndiscusses their characteristics and trade-offs.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 10:07:28 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Kurth", "Andreas", ""], ["Schuiki", "Fabian", ""], ["Benini", "Luca", ""]]}, {"id": "2104.08013", "submitter": "Pierre Zins", "authors": "Pierre Zins, Yuanlu Xu, Edmond Boyer, Stefanie Wuhrer, Tony Tung", "title": "Learning Implicit 3D Representations of Dressed Humans from Sparse Views", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently, data-driven single-view reconstruction methods have shown great\nprogress in modeling 3D dressed humans. However, such methods suffer heavily\nfrom depth ambiguities and occlusions inherent to single view inputs. In this\npaper, we address such issues by lifting the single-view input with additional\nviews and investigate the best strategy to suitably exploit information from\nmultiple views. We propose an end-to-end approach that learns an implicit 3D\nrepresentation of dressed humans from sparse camera views. Specifically, we\nintroduce two key components: first an attention-based fusion layer that learns\nto aggregate visual information from several viewpoints; second a mechanism\nthat encodes local 3D patterns under the multi-view context. In the\nexperiments, we show the proposed approach outperforms the state of the art on\nstandard data both quantitatively and qualitatively. Additionally, we apply our\nmethod on real data acquired with a multi-camera platform and demonstrate our\napproach can obtain results comparable to multi-view stereo with dramatically\nless views.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 10:20:26 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Zins", "Pierre", ""], ["Xu", "Yuanlu", ""], ["Boyer", "Edmond", ""], ["Wuhrer", "Stefanie", ""], ["Tung", "Tony", ""]]}, {"id": "2104.08029", "submitter": "Helena Russello", "authors": "Helena Russello, Rik van der Tol, Gert Kootstra", "title": "T-LEAP: occlusion-robust pose estimation of walking cows using temporal\n  information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As herd size on dairy farms continue to increase, automatic health monitoring\nof cows has gained in interest. Lameness, a prevalent health disorder in dairy\ncows, is commonly detected by analyzing the gait of cows. A cow's gait can be\ntracked in videos using pose estimation models because models learn to\nautomatically localize anatomical landmarks in images and videos. Most animal\npose estimation models are static, that is, videos are processed frame by frame\nand do not use any temporal information. In this work, a static deep-learning\nmodel for animal-pose-estimation was extended to a temporal model that includes\ninformation from past frames. We compared the performance of the static and\ntemporal pose estimation models. The data consisted of 1059 samples of 4\nconsecutive frames extracted from videos (30 fps) of 30 different dairy cows\nwalking through an outdoor passageway. As farm environments are prone to\nocclusions, we tested the robustness of the static and temporal models by\nadding artificial occlusions to the videos. The experiments showed that, on\nnon-occluded data, both static and temporal approaches achieved a Percentage of\nCorrect Keypoints (PCKh@0.2) of 99%. On occluded data, our temporal approach\noutperformed the static one by up to 32.9%, suggesting that using temporal data\nis beneficial for pose estimation in environments prone to occlusions, such as\ndairy farms. The generalization capabilities of the temporal model was\nevaluated by testing it on data containing unknown cows (cows not present in\nthe training set). The results showed that the average detection rate\n(PCKh@0.2) was of 93.8% on known cows and 87.6% on unknown cows, indicating\nthat the model is capable of generalizing well to new cows and that they could\nbe easily fine-tuned to new herds. Finally, we showed that with harder tasks,\nsuch as occlusions and unknown cows, a deeper architecture was more beneficial.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 10:50:56 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Russello", "Helena", ""], ["van der Tol", "Rik", ""], ["Kootstra", "Gert", ""]]}, {"id": "2104.08030", "submitter": "Young Hwi Kim", "authors": "Young Hwi Kim, Seonghyeon Nam, and Seon Joo Kim", "title": "Temporally smooth online action detection using cycle-consistent future\n  anticipation", "comments": "Accepted by Pattern Recognition", "journal-ref": "Pattern Recognition, Volume 116, August 2021, 107954", "doi": "10.1016/j.patcog.2021.107954", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Many video understanding tasks work in the offline setting by assuming that\nthe input video is given from the start to the end. However, many real-world\nproblems require the online setting, making a decision immediately using only\nthe current and the past frames of videos such as in autonomous driving and\nsurveillance systems. In this paper, we present a novel solution for online\naction detection by using a simple yet effective RNN-based networks called the\nFuture Anticipation and Temporally Smoothing network (FATSnet). The proposed\nnetwork consists of a module for anticipating the future that can be trained in\nan unsupervised manner with the cycle-consistency loss, and another component\nfor aggregating the past and the future for temporally smooth frame-by-frame\npredictions. We also propose a solution to relieve the performance loss when\nrunning RNN-based models on very long sequences. Evaluations on TVSeries,\nTHUMOS14, and BBDB show that our method achieve the state-of-the-art\nperformances compared to the previous works on online action detection.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 11:00:19 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Kim", "Young Hwi", ""], ["Nam", "Seonghyeon", ""], ["Kim", "Seon Joo", ""]]}, {"id": "2104.08038", "submitter": "Iuri Frosio", "authors": "Ekta Prashnani, Orazio Gallo, Joohwan Kim, Josef Spjut, Pradeep Sen,\n  Iuri Frosio", "title": "Noise-Aware Saliency Prediction for Videos with Incomplete Gaze Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep-learning-based algorithms have led to impressive results in\nvisual-saliency prediction, but the impact of noise in training gaze data has\nbeen largely overlooked. This issue is especially relevant for videos, where\nthe gaze data tends to be incomplete, and thus noisier, compared to images.\nTherefore, we propose a noise-aware training (NAT) paradigm for visual-saliency\nprediction that quantifies the uncertainty arising from gaze data\nincompleteness and inaccuracy, and accounts for it in training. We demonstrate\nthe advantage of NAT independently of the adopted model architecture, loss\nfunction, or training dataset. Given its robustness to the noise in incomplete\ntraining datasets, NAT ushers in the possibility of designing gaze datasets\nwith fewer human subjects. We also introduce the first dataset that offers a\nvideo-game context for video-saliency research, with rich temporal semantics,\nand multiple gaze attractors per frame.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 11:32:46 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Prashnani", "Ekta", ""], ["Gallo", "Orazio", ""], ["Kim", "Joohwan", ""], ["Spjut", "Josef", ""], ["Sen", "Pradeep", ""], ["Frosio", "Iuri", ""]]}, {"id": "2104.08045", "submitter": "Manoj Goyal", "authors": "Rachit S Munjal, Manoj Goyal, Rutika Moharir, Sukumar Moharana", "title": "TeLCoS: OnDevice Text Localization with Clustering of Script", "comments": "Accepted for publication in IJCNN 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Recent research in the field of text localization in a resource constrained\nenvironment has made extensive use of deep neural networks. Scene text\nlocalization and recognition on low-memory mobile devices have a wide range of\napplications including content extraction, image categorization and keyword\nbased image search. For text recognition of multi-lingual localized text, the\nOCR systems require prior knowledge of the script of each text instance. This\nleads to word script identification being an essential step for text\nrecognition. Most existing methods treat text localization, script\nidentification and text recognition as three separate tasks. This makes script\nidentification an overhead in the recognition pipeline. To reduce this\noverhead, we propose TeLCoS: OnDevice Text Localization with Clustering of\nScript, a multi-task dual branch lightweight CNN network that performs\nreal-time on device Text Localization and High-level Script Clustering\nsimultaneously. The network drastically reduces the number of calls to a\nseparate script identification module, by grouping and identifying some majorly\nused scripts through a single feed-forward pass over the localization network.\nWe also introduce a novel structural similarity based channel pruning mechanism\nto build an efficient network with only 1.15M parameters. Experiments on\nbenchmark datasets suggest that our method achieves state-of-the-art\nperformance, with execution latency of 60 ms for the entire pipeline on the\nExynos 990 chipset device.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 11:45:20 GMT"}, {"version": "v2", "created": "Wed, 21 Apr 2021 12:32:57 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Munjal", "Rachit S", ""], ["Goyal", "Manoj", ""], ["Moharir", "Rutika", ""], ["Moharana", "Sukumar", ""]]}, {"id": "2104.08052", "submitter": "Manoj Goyal", "authors": "Manoj Goyal, Rachit S Munjal, Sukumar Moharana, Deepak Garg, Debi\n  Prasanna Mohanty, Siva Prasad Thota", "title": "ScreenSeg: On-Device Screenshot Layout Analysis", "comments": "Accepted for publication in IJCNN 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We propose a novel end-to-end solution that performs a Hierarchical Layout\nAnalysis of screenshots and document images on resource constrained devices\nlike mobilephones. Our approach segments entities like Grid, Image, Text and\nIcon blocks occurring in a screenshot. We provide an option for smart editing\nby auto highlighting these entities for saving or sharing. Further this\nmulti-level layout analysis of screenshots has many use cases including content\nextraction, keyword-based image search, style transfer, etc. We have addressed\nthe limitations of known baseline approaches, supported a wide variety of\nsemantically complex screenshots, and developed an approach which is highly\noptimized for on-device deployment. In addition, we present a novel weighted\nNMS technique for filtering object proposals. We achieve an average precision\nof about 0.95 with a latency of around 200ms on Samsung Galaxy S10 Device for a\nscreenshot of 1080p resolution. The solution pipeline is already commercialized\nin Samsung Device applications i.e. Samsung Capture, Smart Crop, My Filter in\nCamera Application, Bixby Touch.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 11:59:13 GMT"}, {"version": "v2", "created": "Wed, 21 Apr 2021 12:28:00 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Goyal", "Manoj", ""], ["Munjal", "Rachit S", ""], ["Moharana", "Sukumar", ""], ["Garg", "Deepak", ""], ["Mohanty", "Debi Prasanna", ""], ["Thota", "Siva Prasad", ""]]}, {"id": "2104.08057", "submitter": "Pierre-Alain Fayolle", "authors": "Pierre-Alain Fayolle", "title": "Signed Distance Function Computation from an Implicit Surface", "comments": "Fix typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe in this short note a technique to convert an implicit surface\ninto a Signed Distance Function (SDF) while exactly preserving the zero\nlevel-set of the implicit. The proposed approach relies on embedding the input\nimplicit in the final layer of a neural network, which is trained to minimize a\nloss function characterizing the SDF.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 12:06:53 GMT"}, {"version": "v2", "created": "Fri, 4 Jun 2021 00:00:58 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Fayolle", "Pierre-Alain", ""]]}, {"id": "2104.08096", "submitter": "Tianping Li", "authors": "Tianping Li, Zhifeng Liu, Jianping Qiao", "title": "Multiple feature fusion-based video face tracking for IoT big data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the advancement of IoT and artificial intelligence technologies, and the\nneed for rapid application growth in fields such as security entrance control\nand financial business trade, facial information processing has become an\nimportant means for achieving identity authentication and information security.\nIn this paper, we propose a multi-feature fusion algorithm based on integral\nhistograms and a real-time update tracking particle filtering module. First,\nedge and colour features are extracted, weighting methods are used to weight\nthe colour histogram and edge features to describe facial features, and fusion\nof colour and edge features is made adaptive by using fusion coefficients to\nimprove face tracking reliability. Then, the integral histogram is integrated\ninto the particle filtering algorithm to simplify the calculation steps of\ncomplex particles. Finally, the tracking window size is adjusted in real time\naccording to the change in the average distance from the particle centre to the\nedge of the current model and the initial model to reduce the drift problem and\nachieve stable tracking with significant changes in the target dimension. The\nresults show that the algorithm improves video tracking accuracy, simplifies\nparticle operation complexity, improves the speed, and has good\nanti-interference ability and robustness.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 07:10:13 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Li", "Tianping", ""], ["Liu", "Zhifeng", ""], ["Qiao", "Jianping", ""]]}, {"id": "2104.08108", "submitter": "Shir Gur", "authors": "Shir Gur, Natalia Neverova, Chris Stauffer, Ser-Nam Lim, Douwe Kiela,\n  Austin Reiter", "title": "Cross-Modal Retrieval Augmentation for Multi-Modal Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in using retrieval components over external knowledge sources\nhave shown impressive results for a variety of downstream tasks in natural\nlanguage processing. Here, we explore the use of unstructured external\nknowledge sources of images and their corresponding captions for improving\nvisual question answering (VQA). First, we train a novel alignment model for\nembedding images and captions in the same space, which achieves substantial\nimprovement in performance on image-caption retrieval w.r.t. similar methods.\nSecond, we show that retrieval-augmented multi-modal transformers using the\ntrained alignment model improve results on VQA over strong baselines. We\nfurther conduct extensive experiments to establish the promise of this\napproach, and examine novel applications for inference time such as\nhot-swapping indices.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 13:27:45 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Gur", "Shir", ""], ["Neverova", "Natalia", ""], ["Stauffer", "Chris", ""], ["Lim", "Ser-Nam", ""], ["Kiela", "Douwe", ""], ["Reiter", "Austin", ""]]}, {"id": "2104.08112", "submitter": "Navya Nagananda", "authors": "Navya Nagananda, Breton Minnehan, Andreas Savakis", "title": "Grassmann Iterative Linear Discriminant Analysis with Proxy Matrix\n  Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Linear Discriminant Analysis (LDA) is commonly used for dimensionality\nreduction in pattern recognition and statistics. It is a supervised method that\naims to find the most discriminant space of reduced dimension that can be\nfurther used for classification. In this work, we present a Grassmann Iterative\nLDA method (GILDA) that is based on Proxy Matrix Optimization (PMO). PMO makes\nuse of automatic differentiation and stochastic gradient descent (SGD) on the\nGrassmann manifold to arrive at the optimal projection matrix. Our results show\nthat GILDAoutperforms the prevailing manifold optimization method.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 13:39:53 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Nagananda", "Navya", ""], ["Minnehan", "Breton", ""], ["Savakis", "Andreas", ""]]}, {"id": "2104.08120", "submitter": "Subham Nagar", "authors": "Subham Nagar and Ahlad Kumar", "title": "Orthogonal Features Based EEG Signals Denoising Using Fractional and\n  Compressed One-Dimensional CNN AutoEncoder", "comments": "13 pages, 9 figures, 37 references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a fractional one-dimensional convolutional neural network\n(CNN) autoencoder for denoising the Electroencephalogram (EEG) signals which\noften get contaminated with noise during the recording process, mostly due to\nmuscle artifacts (MA), introduced by the movement of muscles. The existing EEG\ndenoising methods make use of decomposition, thresholding and filtering\ntechniques. In the proposed approach, EEG signals are first transformed to\northogonal domain using Tchebichef moments before feeding to the proposed\narchitecture. A new hyper-parameter ($\\alpha$) is introduced which refers to\nthe fractional order with respect to which gradients are calculated during\nback-propagation. It is observed that by tuning $\\alpha$, the quality of the\nrestored signal improves significantly. Motivated by the high usage of portable\nlow energy devices which make use of compressed deep learning architectures,\nthe trainable parameters of the proposed architecture are compressed using\nrandomized singular value decomposition (RSVD) algorithm. The experiments are\nperformed on the standard EEG datasets, namely, Mendeley and Bonn. The study\nshows that the proposed fractional and compressed architecture performs better\nthan existing state-of-the-art signal denoising methods.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 13:58:05 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Nagar", "Subham", ""], ["Kumar", "Ahlad", ""]]}, {"id": "2104.08126", "submitter": "Dac Tung Vu", "authors": "Dac Tung Vu, Juan Luis Gonzalez, Munchurl Kim", "title": "Exploiting Global and Local Attentions for Heavy Rain Removal on Single\n  Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Heavy rain removal from a single image is the task of simultaneously\neliminating rain streaks and fog, which can dramatically degrade the quality of\ncaptured images. Most existing rain removal methods do not generalize well for\nthe heavy rain case. In this work, we propose a novel network architecture\nconsisting of three sub-networks to remove heavy rain from a single image\nwithout estimating rain streaks and fog separately. The first sub-net, a\nU-net-based architecture that incorporates our Spatial Channel Attention (SCA)\nblocks, extracts global features that provide sufficient contextual information\nneeded to remove atmospheric distortions caused by rain and fog. The second\nsub-net learns the additive residues information, which is useful in removing\nrain streak artifacts via our proposed Residual Inception Modules (RIM). The\nthird sub-net, the multiplicative sub-net, adopts our Channel-attentive\nInception Modules (CIM) and learns the essential brighter local features which\nare not effectively extracted in the SCA and additive sub-nets by modulating\nthe local pixel intensities in the derained images. Our three clean image\nresults are then combined via an attentive blending block to generate the final\nclean image. Our method with SCA, RIM, and CIM significantly outperforms the\nprevious state-of-the-art single-image deraining methods on the synthetic\ndatasets, shows considerably cleaner and sharper derained estimates on the real\nimage datasets. We present extensive experiments and ablation studies\nsupporting each of our method's contributions on both synthetic and real image\ndatasets.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 14:08:27 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Vu", "Dac Tung", ""], ["Gonzalez", "Juan Luis", ""], ["Kim", "Munchurl", ""]]}, {"id": "2104.08131", "submitter": "Simona Bottani", "authors": "Simona Bottani, Ninon Burgos, Aur\\'elien Maire, Adam Wild, Sebastian\n  Str\\\"oer, Didier Dormont, Olivier Colliot", "title": "Automatic quality control of brain T1-weighted magnetic resonance images\n  for a clinical data warehouse", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many studies on machine learning (ML) for computer-aided diagnosis have so\nfar been mostly restricted to high-quality research data. Clinical data\nwarehouses, gathering routine examinations from hospitals, offer great promises\nfor training and validation of ML models in a realistic setting. However, the\nuse of such clinical data warehouses requires quality control (QC) tools.\nVisual QC by experts is time-consuming and does not scale to large datasets. In\nthis paper, we propose a convolutional neural network (CNN) for the automatic\nQC of 3D T1-weighted brain MRI for a large heterogeneous clinical data\nwarehouse. To that purpose, we used the data warehouse of the hospitals of the\nGreater Paris area (Assistance Publique-H\\^opitaux de Paris [AP-HP]).\nSpecifically, the objectives were: 1) to identify images which are not proper\nT1-weighted brain MRIs; 2) to identify acquisitions for which gadolinium was\ninjected; 3) to rate the overall image quality. We used 5000 images for\ntraining and validation and a separate set of 500 images for testing. In order\nto train/validate the CNN, the data were annotated by two trained raters\naccording to a visual QC protocol that we specifically designed for application\nin the setting of a data warehouse. For objectives 1 and 2, our approach\nachieved excellent accuracy (balanced accuracy and F1-score \\textgreater 90\\%),\nsimilar to the human raters. For objective 3, the performance was good but\nsubstantially lower than that of human raters. Nevertheless, the automatic\napproach accurately identified (balanced accuracy and F1-score \\textgreater\n80\\%) low quality images, which would typically need to be excluded. Overall,\nour approach shall be useful for exploiting hospital data warehouses in medical\nimage computing.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 14:27:43 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Bottani", "Simona", ""], ["Burgos", "Ninon", ""], ["Maire", "Aur\u00e9lien", ""], ["Wild", "Adam", ""], ["Str\u00f6er", "Sebastian", ""], ["Dormont", "Didier", ""], ["Colliot", "Olivier", ""]]}, {"id": "2104.08147", "submitter": "Natasa Tagasovska", "authors": "Radhakrishna Achanta, Natasa Tagasovska", "title": "Uncertainty Surrogates for Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper we introduce a novel way of estimating prediction uncertainty\nin deep networks through the use of uncertainty surrogates. These surrogates\nare features of the penultimate layer of a deep network that are forced to\nmatch predefined patterns. The patterns themselves can be, among other\npossibilities, a known visual symbol. We show how our approach can be used for\nestimating uncertainty in prediction and out-of-distribution detection.\nAdditionally, the surrogates allow for interpretability of the ability of the\ndeep network to learn and at the same time lend robustness against adversarial\nattacks. Despite its simplicity, our approach is superior to the\nstate-of-the-art approaches on standard metrics as well as computational\nefficiency and ease of implementation. A wide range of experiments are\nperformed on standard datasets to prove the efficacy of our approach.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 14:50:28 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Achanta", "Radhakrishna", ""], ["Tagasovska", "Natasa", ""]]}, {"id": "2104.08160", "submitter": "Shaofei Wang", "authors": "Shaofei Wang, Andreas Geiger, Siyu Tang", "title": "Locally Aware Piecewise Transformation Fields for 3D Human Mesh\n  Registration", "comments": "CVPR camera ready. Project page:\n  https://taconite.github.io/PTF/website/PTF.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Registering point clouds of dressed humans to parametric human models is a\nchallenging task in computer vision. Traditional approaches often rely on\nheavily engineered pipelines that require accurate manual initialization of\nhuman poses and tedious post-processing. More recently, learning-based methods\nare proposed in hope to automate this process. We observe that pose\ninitialization is key to accurate registration but existing methods often fail\nto provide accurate pose initialization. One major obstacle is that, regressing\njoint rotations from point clouds or images of humans is still very\nchallenging. To this end, we propose novel piecewise transformation fields\n(PTF), a set of functions that learn 3D translation vectors to map any query\npoint in posed space to its correspond position in rest-pose space. We combine\nPTF with multi-class occupancy networks, obtaining a novel learning-based\nframework that learns to simultaneously predict shape and per-point\ncorrespondences between the posed space and the canonical space for clothed\nhuman. Our key insight is that the translation vector for each query point can\nbe effectively estimated using the point-aligned local features; consequently,\nrigid per bone transformations and joint rotations can be obtained efficiently\nvia a least-square fitting given the estimated point correspondences,\ncircumventing the challenging task of directly regressing joint rotations from\nneural networks. Furthermore, the proposed PTF facilitate canonicalized\noccupancy estimation, which greatly improves generalization capability and\nresults in more accurate surface reconstruction with only half of the\nparameters compared with the state-of-the-art. Both qualitative and\nquantitative studies show that fitting parametric models with poses initialized\nby our network results in much better registration quality, especially for\nextreme poses.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 15:16:09 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Wang", "Shaofei", ""], ["Geiger", "Andreas", ""], ["Tang", "Siyu", ""]]}, {"id": "2104.08183", "submitter": "Matthew Vowels", "authors": "Matthew J. Vowels, Necati Cihan Camgoz and Richard Bowden", "title": "Shadow-Mapping for Unsupervised Neural Causal Discovery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important goal across most scientific fields is the discovery of causal\nstructures underling a set of observations. Unfortunately, causal discovery\nmethods which are based on correlation or mutual information can often fail to\nidentify causal links in systems which exhibit dynamic relationships. Such\ndynamic systems (including the famous coupled logistic map) exhibit `mirage'\ncorrelations which appear and disappear depending on the observation window.\nThis means not only that correlation is not causation but, perhaps\ncounter-intuitively, that causation may occur without correlation. In this\npaper we describe Neural Shadow-Mapping, a neural network based method which\nembeds high-dimensional video data into a low-dimensional shadow\nrepresentation, for subsequent estimation of causal links. We demonstrate its\nperformance at discovering causal links from video-representations of dynamic\nsystems.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 15:50:03 GMT"}, {"version": "v2", "created": "Wed, 28 Apr 2021 12:58:44 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Vowels", "Matthew J.", ""], ["Camgoz", "Necati Cihan", ""], ["Bowden", "Richard", ""]]}, {"id": "2104.08188", "submitter": "Matias Valdenegro-Toro", "authors": "Matias Valdenegro-Toro", "title": "I Find Your Lack of Uncertainty in Computer Vision Disturbing", "comments": "LatinX in CV Workshop @ CVPR 2021, full paper track, camera ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks are used for many real world applications, but often they\nhave problems estimating their own confidence. This is particularly problematic\nfor computer vision applications aimed at making high stakes decisions with\nhumans and their lives. In this paper we make a meta-analysis of the\nliterature, showing that most if not all computer vision applications do not\nuse proper epistemic uncertainty quantification, which means that these models\nignore their own limitations. We describe the consequences of using models\nwithout proper uncertainty quantification, and motivate the community to adopt\nversions of the models they use that have proper calibrated epistemic\nuncertainty, in order to enable out of distribution detection. We close the\npaper with a summary of challenges on estimating uncertainty for computer\nvision applications and recommendations.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 15:58:27 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Valdenegro-Toro", "Matias", ""]]}, {"id": "2104.08194", "submitter": "Salman Khan", "authors": "Salman Khan and Fabio Cuzzolin", "title": "Spatiotemporal Deformable Models for Long-Term Complex Activity\n  Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Long-term complex activity recognition and localisation can be crucial for\nthe decision-making process of several autonomous systems, such as smart cars\nand surgical robots. Nonetheless, most current methods are designed to merely\nlocalise short-term action/activities or combinations of atomic actions that\nonly last for a few frames or seconds. In this paper, we address the problem of\nlong-term complex activity detection via a novel deformable, spatiotemporal\nparts-based model. Our framework consists of three main building blocks: (i)\naction tube detection, (ii) the modelling of the deformable geometry of parts,\nand (iii) a sparsity mechanism. Firstly, action tubes are detected in a series\nof snippets using an action tube detector. Next, a new 3D deformable RoI\npooling layer is designed for learning the flexible, deformable geometry of the\nconstellation of parts. Finally, a sparsity strategy differentiates between\nactivated and deactivate features. We also provide temporal complex activity\nannotation for the recently released ROAD autonomous driving dataset and the\nSARAS-ESAD surgical action dataset, to validate our method and show the\nadaptability of our framework to different domains. As they both contain long\nvideos portraying long-term activities they can be used as benchmarks for\nfuture work in this area.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 16:05:34 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Khan", "Salman", ""], ["Cuzzolin", "Fabio", ""]]}, {"id": "2104.08201", "submitter": "Yanan Sun", "authors": "Yanan Sun, Chi-Keung Tang, Yu-Wing Tai", "title": "Semantic Image Matting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural image matting separates the foreground from background in fractional\noccupancy which can be caused by highly transparent objects, complex foreground\n(e.g., net or tree), and/or objects containing very fine details (e.g., hairs).\nAlthough conventional matting formulation can be applied to all of the above\ncases, no previous work has attempted to reason the underlying causes of\nmatting due to various foreground semantics.\n  We show how to obtain better alpha mattes by incorporating into our framework\nsemantic classification of matting regions. Specifically, we consider and learn\n20 classes of matting patterns, and propose to extend the conventional trimap\nto semantic trimap. The proposed semantic trimap can be obtained automatically\nthrough patch structure analysis within trimap regions. Meanwhile, we learn a\nmulti-class discriminator to regularize the alpha prediction at semantic level,\nand content-sensitive weights to balance different regularization losses.\nExperiments on multiple benchmarks show that our method outperforms other\nmethods and has achieved the most competitive state-of-the-art performance.\nFinally, we contribute a large-scale Semantic Image Matting Dataset with\ncareful consideration of data balancing across different semantic classes.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 16:21:02 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Sun", "Yanan", ""], ["Tang", "Chi-Keung", ""], ["Tai", "Yu-Wing", ""]]}, {"id": "2104.08215", "submitter": "Tianlong Chen", "authors": "Tianlong Chen, Zhenyu Zhang, Xu Ouyang, Zechun Liu, Zhiqiang Shen,\n  Zhangyang Wang", "title": "\"BNN - BN = ?\": Training Binary Neural Networks without Batch\n  Normalization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Batch normalization (BN) is a key facilitator and considered essential for\nstate-of-the-art binary neural networks (BNN). However, the BN layer is costly\nto calculate and is typically implemented with non-binary parameters, leaving a\nhurdle for the efficient implementation of BNN training. It also introduces\nundesirable dependence between samples within each batch. Inspired by the\nlatest advance on Batch Normalization Free (BN-Free) training, we extend their\nframework to training BNNs, and for the first time demonstrate that BNs can be\ncompleted removed from BNN training and inference regimes. By plugging in and\ncustomizing techniques including adaptive gradient clipping, scale weight\nstandardization, and specialized bottleneck block, a BN-free BNN is capable of\nmaintaining competitive accuracy compared to its BN-based counterpart.\nExtensive experiments validate the effectiveness of our proposal across diverse\nBNN backbones and datasets. For example, after removing BNs from the\nstate-of-the-art ReActNets, it can still be trained with our proposed\nmethodology to achieve 92.08%, 68.34%, and 68.0% accuracy on CIFAR-10,\nCIFAR-100, and ImageNet respectively, with marginal performance drop\n(0.23%~0.44% on CIFAR and 1.40% on ImageNet). Codes and pre-trained models are\navailable at: https://github.com/VITA-Group/BNN_NoBN.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 16:46:57 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Chen", "Tianlong", ""], ["Zhang", "Zhenyu", ""], ["Ouyang", "Xu", ""], ["Liu", "Zechun", ""], ["Shen", "Zhiqiang", ""], ["Wang", "Zhangyang", ""]]}, {"id": "2104.08223", "submitter": "Alexander Richard", "authors": "Alexander Richard, Michael Zollhoefer, Yandong Wen, Fernando de la\n  Torre, Yaser Sheikh", "title": "MeshTalk: 3D Face Animation from Speech using Cross-Modality\n  Disentanglement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a generic method for generating full facial 3D animation\nfrom speech. Existing approaches to audio-driven facial animation exhibit\nuncanny or static upper face animation, fail to produce accurate and plausible\nco-articulation or rely on person-specific models that limit their scalability.\nTo improve upon existing models, we propose a generic audio-driven facial\nanimation approach that achieves highly realistic motion synthesis results for\nthe entire face. At the core of our approach is a categorical latent space for\nfacial animation that disentangles audio-correlated and audio-uncorrelated\ninformation based on a novel cross-modality loss. Our approach ensures highly\naccurate lip motion, while also synthesizing plausible animation of the parts\nof the face that are uncorrelated to the audio signal, such as eye blinks and\neye brow motion. We demonstrate that our approach outperforms several baselines\nand obtains state-of-the-art quality both qualitatively and quantitatively. A\nperceptual user study demonstrates that our approach is deemed more realistic\nthan the current state-of-the-art in over 75% of cases. We recommend watching\nthe supplemental video before reading the paper:\nhttps://research.fb.com/wp-content/uploads/2021/04/mesh_talk.mp4\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 17:05:40 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Richard", "Alexander", ""], ["Zollhoefer", "Michael", ""], ["Wen", "Yandong", ""], ["de la Torre", "Fernando", ""], ["Sheikh", "Yaser", ""]]}, {"id": "2104.08230", "submitter": "Kirill Mazur", "authors": "Ilya Zakharkin, Kirill Mazur, Artur Grigorev, Victor Lempitsky", "title": "Point-Based Modeling of Human Clothing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We propose a new approach to human clothing modeling based on point clouds.\nWithin this approach, we learn a deep model that can predict point clouds of\nvarious outfits, for various human poses and for various human body shapes.\nNotably, outfits of various types and topologies can be handled by the same\nmodel. Using the learned model, we can infer geometry of new outfits from as\nlittle as a singe image, and perform outfit retargeting to new bodies in new\nposes. We complement our geometric model with appearance modeling that uses the\npoint cloud geometry as a geometric scaffolding, and employs neural point-based\ngraphics to capture outfit appearance from videos and to re-render the captured\noutfits. We validate both geometric modeling and appearance modeling aspects of\nthe proposed approach against recently proposed methods, and establish the\nviability of point-based clothing modeling.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 17:12:33 GMT"}, {"version": "v2", "created": "Thu, 22 Apr 2021 16:53:30 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Zakharkin", "Ilya", ""], ["Mazur", "Kirill", ""], ["Grigorev", "Artur", ""], ["Lempitsky", "Victor", ""]]}, {"id": "2104.08241", "submitter": "Jiawei Liu", "authors": "Jiawei Liu, Zheng-Jun Zha, Wei Wu, Kecheng Zheng, Qibin Sun", "title": "Spatial-Temporal Correlation and Topology Learning for Person\n  Re-Identification in Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Video-based person re-identification aims to match pedestrians from video\nsequences across non-overlapping camera views. The key factor for video person\nre-identification is to effectively exploit both spatial and temporal clues\nfrom video sequences. In this work, we propose a novel Spatial-Temporal\nCorrelation and Topology Learning framework (CTL) to pursue discriminative and\nrobust representation by modeling cross-scale spatial-temporal correlation.\nSpecifically, CTL utilizes a CNN backbone and a key-points estimator to extract\nsemantic local features from human body at multiple granularities as graph\nnodes. It explores a context-reinforced topology to construct multi-scale\ngraphs by considering both global contextual information and physical\nconnections of human body. Moreover, a 3D graph convolution and a cross-scale\ngraph convolution are designed, which facilitate direct cross-spacetime and\ncross-scale information propagation for capturing hierarchical spatial-temporal\ndependencies and structural information. By jointly performing the two\nconvolutions, CTL effectively mines comprehensive clues that are complementary\nwith appearance information to enhance representational capacity. Extensive\nexperiments on two video benchmarks have demonstrated the effectiveness of the\nproposed method and the state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 14:32:12 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Liu", "Jiawei", ""], ["Zha", "Zheng-Jun", ""], ["Wu", "Wei", ""], ["Zheng", "Kecheng", ""], ["Sun", "Qibin", ""]]}, {"id": "2104.08244", "submitter": "Ikram Chairi", "authors": "Abdelkrim Alahyane and Mohamed El Fakir and Saad Benjelloun and Ikram\n  Chairi", "title": "Open data for Moroccan license plates for OCR applications : data\n  collection, labeling, and model construction", "comments": null, "journal-ref": null, "doi": null, "report-no": "MSDA reports 01", "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Significant number of researches have been developed recently around\nintelligent system for traffic management, especially, OCR based license plate\nrecognition, as it is considered as a main step for any automatic traffic\nmanagement system. Good quality data sets are increasingly needed and produced\nby the research community to improve the performance of those algorithms.\nFurthermore, a special need of data is noted for countries having special\ncharacters on their licence plates, like Morocco, where Arabic Alphabet is\nused. In this work, we present a labeled open data set of circulation plates\ntaken in Morocco, for different type of vehicles, namely cars, trucks and\nmotorcycles. This data was collected manually and consists of 705 unique and\ndifferent images. Furthermore this data was labeled for plate segmentation and\nfor matriculation number OCR. Also, As we show in this paper, the data can be\nenriched using data augmentation techniques to create training sets with few\nthousands of images for different machine leaning and AI applications. We\npresent and compare a set of models built on this data. Also, we publish this\ndata as an open access data to encourage innovation and applications in the\nfield of OCR and image processing for traffic control and other applications\nfor transportation and heterogeneous vehicle management.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 17:26:46 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Alahyane", "Abdelkrim", ""], ["Fakir", "Mohamed El", ""], ["Benjelloun", "Saad", ""], ["Chairi", "Ikram", ""]]}, {"id": "2104.08271", "submitter": "Simion-Vlad Bogolin", "authors": "Ioana Croitoru, Simion-Vlad Bogolin, Yang Liu, Samuel Albanie, Marius\n  Leordeanu, Hailin Jin, Andrew Zisserman", "title": "TEACHTEXT: CrossModal Generalized Distillation for Text-Video Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In recent years, considerable progress on the task of text-video retrieval\nhas been achieved by leveraging large-scale pretraining on visual and audio\ndatasets to construct powerful video encoders. By contrast, despite the natural\nsymmetry, the design of effective algorithms for exploiting large-scale\nlanguage pretraining remains under-explored. In this work, we are the first to\ninvestigate the design of such algorithms and propose a novel generalized\ndistillation method, TeachText, which leverages complementary cues from\nmultiple text encoders to provide an enhanced supervisory signal to the\nretrieval model. Moreover, we extend our method to video side modalities and\nshow that we can effectively reduce the number of used modalities at test time\nwithout compromising performance. Our approach advances the state of the art on\nseveral video retrieval benchmarks by a significant margin and adds no\ncomputational overhead at test time. Last but not least, we show an effective\napplication of our method for eliminating noise from retrieval datasets. Code\nand data can be found at https://www.robots.ox.ac.uk/~vgg/research/teachtext/.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 17:55:28 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Croitoru", "Ioana", ""], ["Bogolin", "Simion-Vlad", ""], ["Liu", "Yang", ""], ["Albanie", "Samuel", ""], ["Leordeanu", "Marius", ""], ["Jin", "Hailin", ""], ["Zisserman", "Andrew", ""]]}, {"id": "2104.08277", "submitter": "Sriram Narayanan", "authors": "Sriram Narayanan, Ramin Moslemi, Francesco Pittaluga, Buyu Liu,\n  Manmohan Chandraker", "title": "Divide-and-Conquer for Lane-Aware Diverse Trajectory Prediction", "comments": "CVPR 21 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Trajectory prediction is a safety-critical tool for autonomous vehicles to\nplan and execute actions. Our work addresses two key challenges in trajectory\nprediction, learning multimodal outputs, and better predictions by imposing\nconstraints using driving knowledge. Recent methods have achieved strong\nperformances using Multi-Choice Learning objectives like winner-takes-all (WTA)\nor best-of-many. But the impact of those methods in learning diverse hypotheses\nis under-studied as such objectives highly depend on their initialization for\ndiversity. As our first contribution, we propose a novel Divide-And-Conquer\n(DAC) approach that acts as a better initialization technique to WTA objective,\nresulting in diverse outputs without any spurious modes. Our second\ncontribution is a novel trajectory prediction framework called ALAN that uses\nexisting lane centerlines as anchors to provide trajectories constrained to the\ninput lanes. Our framework provides multi-agent trajectory outputs in a forward\npass by capturing interactions through hypercolumn descriptors and\nincorporating scene information in the form of rasterized images and per-agent\nlane anchors. Experiments on synthetic and real data show that the proposed DAC\ncaptures the data distribution better compare to other WTA family of\nobjectives. Further, we show that our ALAN approach provides on par or better\nperformance with SOTA methods evaluated on Nuscenes urban driving benchmark.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 17:58:56 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Narayanan", "Sriram", ""], ["Moslemi", "Ramin", ""], ["Pittaluga", "Francesco", ""], ["Liu", "Buyu", ""], ["Chandraker", "Manmohan", ""]]}, {"id": "2104.08278", "submitter": "Bingbing Zhuang", "authors": "Bingbing Zhuang, Manmohan Chandraker", "title": "Fusing the Old with the New: Learning Relative Camera Pose with\n  Geometry-Guided Uncertainty", "comments": "CVPR 2021, Oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Learning methods for relative camera pose estimation have been developed\nlargely in isolation from classical geometric approaches. The question of how\nto integrate predictions from deep neural networks (DNNs) and solutions from\ngeometric solvers, such as the 5-point algorithm, has as yet remained\nunder-explored. In this paper, we present a novel framework that involves\nprobabilistic fusion between the two families of predictions during network\ntraining, with a view to leveraging their complementary benefits in a learnable\nway. The fusion is achieved by learning the DNN uncertainty under explicit\nguidance by the geometric uncertainty, thereby learning to take into account\nthe geometric solution in relation to the DNN prediction. Our network features\na self-attention graph neural network, which drives the learning by enforcing\nstrong interactions between different correspondences and potentially modeling\ncomplex relationships between points. We propose motion parmeterizations\nsuitable for learning and show that our method achieves state-of-the-art\nperformance on the challenging DeMoN and ScanNet datasets. While we focus on\nrelative pose, we envision that our pipeline is broadly applicable for fusing\nclassical geometry and deep learning.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 17:59:06 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Zhuang", "Bingbing", ""], ["Chandraker", "Manmohan", ""]]}, {"id": "2104.08313", "submitter": "Benjamin Devillers", "authors": "Benjamin Devillers, Bhavin Choksi, Romain Bielawski and Rufin\n  VanRullen", "title": "Does language help generalization in vision models?", "comments": "Paper accepted for presentation at the ViGIL 2021 workshop @NAACL.\n  This version: added models to the comparison (ICMLM, TSM); added tests of\n  adversarial robustness; mistake identified and corrected in the normalization\n  of image features; results and conclusions updated accordingly", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vision models trained on multimodal datasets can benefit from the wide\navailability of large image-caption datasets. A recent model (CLIP) was found\nto generalize well in zero-shot and transfer learning settings. This could\nimply that linguistic or \"semantic grounding\" confers additional generalization\nabilities to the visual feature space. Here, we systematically evaluate various\nmultimodal architectures and vision-only models in terms of unsupervised\nclustering, few-shot learning, transfer learning and adversarial robustness. In\neach setting, multimodal training produced no additional generalization\ncapability compared to standard supervised visual training. We conclude that\nwork is still required for semantic grounding to help improve vision models.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 18:54:14 GMT"}, {"version": "v2", "created": "Sat, 15 May 2021 17:23:52 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Devillers", "Benjamin", ""], ["Choksi", "Bhavin", ""], ["Bielawski", "Romain", ""], ["VanRullen", "Rufin", ""]]}, {"id": "2104.08314", "submitter": "Hossam Amer", "authors": "Hossam Amer, Ahmed H. Salamah, Ahmad Sajedi, En-hui Yang", "title": "High Performance Convolution Using Sparsity and Patterns for Inference\n  in Deep Convolutional Neural Networks", "comments": "34 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deploying deep Convolutional Neural Networks (CNNs) is impacted by their\nmemory footprint and speed requirements, which mainly come from convolution.\nWidely-used convolution algorithms, im2col and MEC, produce a lowered matrix\nfrom an activation map by redundantly storing the map's elements included at\nhorizontal and/or vertical kernel overlappings without considering the sparsity\nof the map. Using the sparsity of the map, this paper proposes two new\nconvolution algorithms dubbed Compressed Pattern Overlap (CPO) and Compressed\nPattern Sets (CPS) that simultaneously decrease the memory footprint and\nincrease the inference speed while preserving the accuracy. CPO recognizes\nnon-zero elements (NZEs) at horizontal and vertical overlappings in the\nactivation maps. CPS further improves the memory savings of CPO by compressing\nthe index positions of neighboring NZEs. In both algorithms, channels/regions\nof the activation maps with all zeros are skipped. Then, CPO/CPS performs\nconvolution via Sparse Matrix-Vector Multiplication (SpMv) done on their sparse\nrepresentations. Experimental results conducted on CPUs show that average\nper-layer time savings reach up to 63% and Compression Ratio (CR) up to 26x\nwith respect to im2col. In some layers, our average per layer CPO/CPS time\nsavings are better by 28% and CR is better by 9.2x than the parallel\nimplementation of MEC. For a given CNN's inference, we offline select for each\nconvolution layer the best convolutional algorithm in terms of time between\neither CPO or CPS and im2col. Our algorithms were selected up to 56% of the\nnon-pointwise convolutional layers. Our offline selections yield CNN inference\ntime savings up to 9% and CR up to 10x.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 18:55:32 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Amer", "Hossam", ""], ["Salamah", "Ahmed H.", ""], ["Sajedi", "Ahmad", ""], ["Yang", "En-hui", ""]]}, {"id": "2104.08319", "submitter": "Charles Bonnineau", "authors": "Charles Bonnineau and Wassim Hamidouche and Jean-Francois Travers and\n  Naty Sidaty and Olivier Deforges", "title": "Multitask Learning for VVC Quality Enhancement and Super-Resolution", "comments": "accepted as a conference paper to Picture Coding Symposium (PCS) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The latest video coding standard, called versatile video coding (VVC),\nincludes several novel and refined coding tools at different levels of the\ncoding chain. These tools bring significant coding gains with respect to the\nprevious standard, high efficiency video coding (HEVC). However, the encoder\nmay still introduce visible coding artifacts, mainly caused by coding decisions\napplied to adjust the bitrate to the available bandwidth. Hence, pre and\npost-processing techniques are generally added to the coding pipeline to\nimprove the quality of the decoded video. These methods have recently shown\noutstanding results compared to traditional approaches, thanks to the recent\nadvances in deep learning. Generally, multiple neural networks are trained\nindependently to perform different tasks, thus omitting to benefit from the\nredundancy that exists between the models. In this paper, we investigate a\nlearning-based solution as a post-processing step to enhance the decoded VVC\nvideo quality. Our method relies on multitask learning to perform both quality\nenhancement and super-resolution using a single shared network optimized for\nmultiple degradation levels. The proposed solution enables a good performance\nin both mitigating coding artifacts and super-resolution with fewer network\nparameters compared to traditional specialized architectures.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 19:05:26 GMT"}, {"version": "v2", "created": "Tue, 20 Apr 2021 13:36:28 GMT"}, {"version": "v3", "created": "Mon, 3 May 2021 18:56:29 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Bonnineau", "Charles", ""], ["Hamidouche", "Wassim", ""], ["Travers", "Jean-Francois", ""], ["Sidaty", "Naty", ""], ["Deforges", "Olivier", ""]]}, {"id": "2104.08323", "submitter": "David Stutz", "authors": "David Stutz, Nandhini Chandramoorthy, Matthias Hein, Bernt Schiele", "title": "Random and Adversarial Bit Error Robustness: Energy-Efficient and Secure\n  DNN Accelerators", "comments": "arXiv admin note: substantial text overlap with arXiv:2006.13977", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AR cs.CR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural network (DNN) accelerators received considerable attention in\nrecent years due to the potential to save energy compared to mainstream\nhardware. Low-voltage operation of DNN accelerators allows to further reduce\nenergy consumption significantly, however, causes bit-level failures in the\nmemory storing the quantized DNN weights. Furthermore, DNN accelerators have\nbeen shown to be vulnerable to adversarial attacks on voltage controllers or\nindividual bits. In this paper, we show that a combination of robust\nfixed-point quantization, weight clipping, as well as random bit error training\n(RandBET) or adversarial bit error training (AdvBET) improves robustness\nagainst random or adversarial bit errors in quantized DNN weights\nsignificantly. This leads not only to high energy savings for low-voltage\noperation as well as low-precision quantization, but also improves security of\nDNN accelerators. Our approach generalizes across operating voltages and\naccelerators, as demonstrated on bit errors from profiled SRAM arrays, and\nachieves robustness against both targeted and untargeted bit-level attacks.\nWithout losing more than 0.8%/2% in test accuracy, we can reduce energy\nconsumption on CIFAR10 by 20%/30% for 8/4-bit quantization using RandBET.\nAllowing up to 320 adversarial bit errors, AdvBET reduces test error from above\n90% (chance level) to 26.22% on CIFAR10.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 19:11:14 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Stutz", "David", ""], ["Chandramoorthy", "Nandhini", ""], ["Hein", "Matthias", ""], ["Schiele", "Bernt", ""]]}, {"id": "2104.08353", "submitter": "Pablo Barros", "authors": "Pablo Barros, Alessandra Sciutti", "title": "I Only Have Eyes for You: The Impact of Masks On Convolutional-Based\n  Facial Expression Recognition", "comments": "Accepted at the LXCV Workshop @ CVPR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The current COVID-19 pandemic has shown us that we are still facing\nunpredictable challenges in our society. The necessary constrain on social\ninteractions affected heavily how we envision and prepare the future of social\nrobots and artificial agents in general. Adapting current affective perception\nmodels towards constrained perception based on the hard separation between\nfacial perception and affective understanding would help us to provide robust\nsystems. In this paper, we perform an in-depth analysis of how recognizing\naffect from persons with masks differs from general facial expression\nperception. We evaluate how the recently proposed FaceChannel adapts towards\nrecognizing facial expressions from persons with masks. In Our analysis, we\nevaluate different training and fine-tuning schemes to understand better the\nimpact of masked facial expressions. We also perform specific feature-level\nvisualization to demonstrate how the inherent capabilities of the FaceChannel\nto learn and combine facial features change when in a constrained social\ninteraction scenario.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 20:03:30 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Barros", "Pablo", ""], ["Sciutti", "Alessandra", ""]]}, {"id": "2104.08363", "submitter": "Anastasia Ianina", "authors": "Artur Grigorev, Karim Iskakov, Anastasia Ianina, Renat Bashirov, Ilya\n  Zakharkin, Alexander Vakhitov, Victor Lempitsky", "title": "StylePeople: A Generative Model of Fullbody Human Avatars", "comments": "CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We propose a new type of full-body human avatars, which combines parametric\nmesh-based body model with a neural texture. We show that with the help of\nneural textures, such avatars can successfully model clothing and hair, which\nusually poses a problem for mesh-based approaches. We also show how these\navatars can be created from multiple frames of a video using backpropagation.\nWe then propose a generative model for such avatars that can be trained from\ndatasets of images and videos of people. The generative model allows us to\nsample random avatars as well as to create dressed avatars of people from one\nor few images. The code for the project is available at\nsaic-violet.github.io/style-people.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 20:43:11 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Grigorev", "Artur", ""], ["Iskakov", "Karim", ""], ["Ianina", "Anastasia", ""], ["Bashirov", "Renat", ""], ["Zakharkin", "Ilya", ""], ["Vakhitov", "Alexander", ""], ["Lempitsky", "Victor", ""]]}, {"id": "2104.08368", "submitter": "Ameni Trabelsi", "authors": "Ameni Trabelsi, Ross J. Beveridge and Nathaniel Blanchard", "title": "Drowned out by the noise: Evidence for Tracking-free Motion Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous driving consists of a multitude of interacting modules, where each\nmodule must contend with errors from the others. Typically, the motion\nprediction module depends on a robust tracking system to capture each agent's\npast movement. In this work, we systematically explore the importance of the\ntracking module for the motion prediction task and ultimately conclude that the\ntracking module is detrimental to overall motion prediction performance when\nthe module is imperfect (with as low as 1% error). We explicitly compare models\nthat use tracking information to models that do not across multiple scenarios\nand conditions. We find that the tracking information only improves performance\nin noise-free conditions. A noise-free tracker is unlikely to remain noise-free\nin real-world scenarios, and the inevitable noise will subsequently negatively\naffect performance. We thus argue future work should be mindful of noise when\ndeveloping and testing motion/tracking modules, or that they should do away\nwith the tracking component entirely.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 21:03:55 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Trabelsi", "Ameni", ""], ["Beveridge", "Ross J.", ""], ["Blanchard", "Nathaniel", ""]]}, {"id": "2104.08373", "submitter": "Jun-Teng Yang", "authors": "Jun-Teng Yang, Guei-Ming Liu, Scott C.-H Huang", "title": "Multimodal Deception Detection in Videos via Analyzing Emotional\n  State-based Feature", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deception detection is an important task that has been a hot research topic\ndue to its potential applications. It can be applied to many areas from\nnational security (e.g, airport security, jurisprudence, and law enforcement)\nto real-life applications (e.g., business and computer vision). However, some\ncritical problems still exist and worth more investigation. One of the major\nchallenges is the data scarcity problem. Until now, only one multimodal\nbenchmark dataset on deception detection has been published, which contains 121\nvideo clips for deception detection (61 for deceptive class and 60 for truthful\nclass). This amount of data is hard to drive deep neural network-based methods.\nHence, they often suffered from the overfitting problem and the bad\ngeneralization ability. Also, the ground truth data contains some unusable\nframes for many factors including the face is too small to be recognized the\nfacial expression, face is covered by text, file corruption, etc.\n  However, most of the literature did not consider these problems. In this\npaper, we design a series of data preprocessing methods to deal with the\nproblem first. Then, we propose a multimodal deception detection framework to\nconstruct our novel emotional state-based feature and used open toolkit\nopenSMILE to extract the features from audio modality. A voting scheme is also\ndesigned to combine the emotional state information obtained from both visual\nmodality and audio modality. Finally, the novel emotion state transformation\n(EST) feature is determined by our algorithm. The critical analysis and\ncomparison of the proposed methods with the state-of-the-art multimodal method\nare showed that the overall performance has a great improvement of accuracy\nfrom 84.16% to 91.67% and ROC-AUC from 0.9211 to 0.9244.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 21:20:32 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Yang", "Jun-Teng", ""], ["Liu", "Guei-Ming", ""], ["Huang", "Scott C. -H", ""]]}, {"id": "2104.08381", "submitter": "Xin Wang", "authors": "Xin Wang, Thomas E. Huang, Benlin Liu, Fisher Yu, Xiaolong Wang,\n  Joseph E. Gonzalez, Trevor Darrell", "title": "Robust Object Detection via Instance-Level Temporal Cycle Confusion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Building reliable object detectors that are robust to domain shifts, such as\nvarious changes in context, viewpoint, and object appearances, is critical for\nreal-world applications. In this work, we study the effectiveness of auxiliary\nself-supervised tasks to improve the out-of-distribution generalization of\nobject detectors. Inspired by the principle of maximum entropy, we introduce a\nnovel self-supervised task, instance-level temporal cycle confusion (CycConf),\nwhich operates on the region features of the object detectors. For each object,\nthe task is to find the most different object proposals in the adjacent frame\nin a video and then cycle back to itself for self-supervision. CycConf\nencourages the object detector to explore invariant structures across instances\nunder various motions, which leads to improved model robustness in unseen\ndomains at test time. We observe consistent out-of-domain performance\nimprovements when training object detectors in tandem with self-supervised\ntasks on large-scale video datasets (BDD100K and Waymo open data). The joint\ntraining framework also establishes a new state-of-the-art on standard\nunsupervised domain adaptative detection benchmarks (Cityscapes, Foggy\nCityscapes, and Sim10K). The project page is available at\nhttps://xinw.ai/cyc-conf.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 21:35:08 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Wang", "Xin", ""], ["Huang", "Thomas E.", ""], ["Liu", "Benlin", ""], ["Yu", "Fisher", ""], ["Wang", "Xiaolong", ""], ["Gonzalez", "Joseph E.", ""], ["Darrell", "Trevor", ""]]}, {"id": "2104.08383", "submitter": "Jingnan Shi", "authors": "Jingnan Shi, Heng Yang, Luca Carlone", "title": "Optimal Pose and Shape Estimation for Category-level 3D Object\n  Perception", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a category-level perception problem, where one is given 3D sensor\ndata picturing an object of a given category (e.g. a car), and has to\nreconstruct the pose and shape of the object despite intra-class variability\n(i.e. different car models have different shapes). We consider an active shape\nmodel, where -- for an object category -- we are given a library of potential\nCAD models describing objects in that category, and we adopt a standard\nformulation where pose and shape estimation are formulated as a non-convex\noptimization. Our first contribution is to provide the first certifiably\noptimal solver for pose and shape estimation. In particular, we show that\nrotation estimation can be decoupled from the estimation of the object\ntranslation and shape, and we demonstrate that (i) the optimal object rotation\ncan be computed via a tight (small-size) semidefinite relaxation, and (ii) the\ntranslation and shape parameters can be computed in closed-form given the\nrotation. Our second contribution is to add an outlier rejection layer to our\nsolver, hence making it robust to a large number of misdetections. Towards this\ngoal, we wrap our optimal solver in a robust estimation scheme based on\ngraduated non-convexity. To further enhance robustness to outliers, we also\ndevelop the first graph-theoretic formulation to prune outliers in\ncategory-level perception, which removes outliers via convex hull and maximum\nclique computations; the resulting approach is robust to 70%-90% outliers. Our\nthird contribution is an extensive experimental evaluation. Besides providing\nan ablation study on a simulated dataset and on the PASCAL3D+ dataset, we\ncombine our solver with a deep-learned keypoint detector, and show that the\nresulting approach improves over the state of the art in vehicle pose\nestimation in the ApolloScape datasets.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 21:41:29 GMT"}, {"version": "v2", "created": "Wed, 12 May 2021 18:20:08 GMT"}, {"version": "v3", "created": "Thu, 24 Jun 2021 18:55:34 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Shi", "Jingnan", ""], ["Yang", "Heng", ""], ["Carlone", "Luca", ""]]}, {"id": "2104.08384", "submitter": "Mohammad Sadegh Rasooli", "authors": "Mohammad Sadegh Rasooli, Chris Callison-Burch, Derry Tanti Wijaya", "title": "\"Wikily\" Neural Machine Translation Tailored to Cross-Lingual Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a simple but effective approach for leveraging Wikipedia for\nneural machine translation as well as cross-lingual tasks of image captioning\nand dependency parsing without using any direct supervision from external\nparallel data or supervised models in the target language. We show that first\nsentences and titles of linked Wikipedia pages, as well as cross-lingual image\ncaptions, are strong signals for a seed parallel data to extract bilingual\ndictionaries and cross-lingual word embeddings for mining parallel text from\nWikipedia. Our final model achieves high BLEU scores that are close to or\nsometimes higher than strong supervised baselines in low-resource languages;\ne.g. supervised BLEU of 4.0 versus 12.1 from our model in English-to-Kazakh.\nMoreover, we tailor our wikily translation models to unsupervised image\ncaptioning and cross-lingual dependency parser transfer. In image captioning,\nwe train a multi-tasking machine translation and image captioning pipeline for\nArabic and English from which the Arabic training data is a wikily translation\nof the English captioning data. Our captioning results in Arabic are slightly\nbetter than that of its supervised model. In dependency parsing, we translate a\nlarge amount of monolingual text, and use it as an artificial training data in\nan annotation projection framework. We show that our model outperforms recent\nwork on cross-lingual transfer of dependency parsers.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 21:49:12 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Rasooli", "Mohammad Sadegh", ""], ["Callison-Burch", "Chris", ""], ["Wijaya", "Derry Tanti", ""]]}, {"id": "2104.08390", "submitter": "Chenqiu Zhao", "authors": "Chenqiu Zhao, Kangkang Hu and Anup Basu", "title": "Arithmetic Distribution Neural Network for Background Subtraction", "comments": "This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new Arithmetic Distribution Neural Network (ADNN) for learning\nthe distributions of temporal pixels during background subtraction. In our\nADNN, the arithmetic distribution operations are utilized to propose the\narithmetic distribution layers, including the product distribution layer and\nthe sum distribution layer. Furthermore, in order to improve the accuracy of\nthe proposed approach, an improved Bayesian refinement model based on\nneighboring information, with a GPU implementation, is introduced. In the\nforward pass and backpropagation of the proposed arithmetic distribution\nlayers, histograms are considered as probability density functions rather than\nmatrices. Thus, the proposed approach is able to utilize the probability\ninformation of the histogram and achieve promising results with a very simple\narchitecture compared to traditional convolutional neural networks. Evaluations\nusing standard benchmarks demonstrate the superiority of the proposed approach\ncompared to state-of-the-art traditional and deep learning methods. To the best\nof our knowledge, this is the first method to propose network layers based on\narithmetic distribution operations for learning distributions during background\nsubtraction.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 22:44:58 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Zhao", "Chenqiu", ""], ["Hu", "Kangkang", ""], ["Basu", "Anup", ""]]}, {"id": "2104.08391", "submitter": "Viresh Ranjan", "authors": "Viresh Ranjan, Udbhav Sharma, Thu Nguyen, Minh Hoai", "title": "Learning To Count Everything", "comments": "CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing works on visual counting primarily focus on one specific category at\na time, such as people, animals, and cells. In this paper, we are interested in\ncounting everything, that is to count objects from any category given only a\nfew annotated instances from that category. To this end, we pose counting as a\nfew-shot regression task. To tackle this task, we present a novel method that\ntakes a query image together with a few exemplar objects from the query image\nand predicts a density map for the presence of all objects of interest in the\nquery image. We also present a novel adaptation strategy to adapt our network\nto any novel visual category at test time, using only a few exemplar objects\nfrom the novel category. We also introduce a dataset of 147 object categories\ncontaining over 6000 images that are suitable for the few-shot counting task.\nThe images are annotated with two types of annotation, dots and bounding boxes,\nand they can be used for developing few-shot counting models. Experiments on\nthis dataset shows that our method outperforms several state-of-the-art object\ndetectors and few-shot counting approaches. Our code and dataset can be found\nat https://github.com/cvlab-stonybrook/LearningToCountEverything.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 22:45:58 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Ranjan", "Viresh", ""], ["Sharma", "Udbhav", ""], ["Nguyen", "Thu", ""], ["Hoai", "Minh", ""]]}, {"id": "2104.08403", "submitter": "Cho-Ying Wu", "authors": "Cho-Ying Wu, Qiangeng Xu, Ulrich Neumann", "title": "Accurate 3D Facial Geometry Prediction by Multi-Task, Multi-Modal, and\n  Multi-Representation Landmark Refinement Network", "comments": "Project page: https://choyingw.github.io/works/M3-LRN/index.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work focuses on complete 3D facial geometry prediction, including 3D\nfacial alignment via 3D face modeling and face orientation estimation using the\nproposed multi-task, multi-modal, and multi-representation landmark refinement\nnetwork (M$^3$-LRN). Our focus is on the important facial attributes, 3D\nlandmarks, and we fully utilize their embedded information to guide 3D facial\ngeometry learning. We first propose a multi-modal and multi-representation\nfeature aggregation for landmark refinement. Next, we are the first to study\n3DMM regression from sparse 3D landmarks and utilize multi-representation\nadvantage to attain better geometry prediction. We attain the state of the art\nfrom extensive experiments on all tasks of learning 3D facial geometry. We\nclosely validate contributions of each modality and representation. Our results\nare robust across cropped faces, underwater scenarios, and extreme poses.\nSpecially we adopt only simple and widely used network operations in M$^3$-LRN\nand attain a near 20\\% improvement on face orientation estimation over the\ncurrent best performance. See our project page here.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 23:22:41 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Wu", "Cho-Ying", ""], ["Xu", "Qiangeng", ""], ["Neumann", "Ulrich", ""]]}, {"id": "2104.08405", "submitter": "Te-Lin Wu", "authors": "Te-Lin Wu, Cheng Li, Mingyang Zhang, Tao Chen, Spurthi Amba Hombaiah,\n  Michael Bendersky", "title": "LAMPRET: Layout-Aware Multimodal PreTraining for Document Understanding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Document layout comprises both structural and visual (eg. font-sizes)\ninformation that is vital but often ignored by machine learning models. The few\nexisting models which do use layout information only consider textual contents,\nand overlook the existence of contents in other modalities such as images.\nAdditionally, spatial interactions of presented contents in a layout were never\nreally fully exploited. To bridge this gap, we parse a document into content\nblocks (eg. text, table, image) and propose a novel layout-aware multimodal\nhierarchical framework, LAMPreT, to model the blocks and the whole document.\nOur LAMPreT encodes each block with a multimodal transformer in the lower-level\nand aggregates the block-level representations and connections utilizing a\nspecifically designed transformer at the higher-level. We design hierarchical\npretraining objectives where the lower-level model is trained similarly to\nmultimodal grounding models, and the higher-level model is trained with our\nproposed novel layout-aware objectives. We evaluate the proposed model on two\nlayout-aware tasks -- text block filling and image suggestion and show the\neffectiveness of our proposed hierarchical architecture as well as pretraining\ntechniques.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 23:27:39 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Wu", "Te-Lin", ""], ["Li", "Cheng", ""], ["Zhang", "Mingyang", ""], ["Chen", "Tao", ""], ["Hombaiah", "Spurthi Amba", ""], ["Bendersky", "Michael", ""]]}, {"id": "2104.08418", "submitter": "Christopher Xie", "authors": "Christopher Xie, Keunhong Park, Ricardo Martin-Brualla, Matthew Brown", "title": "FiG-NeRF: Figure-Ground Neural Radiance Fields for 3D Object Category\n  Modelling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We investigate the use of Neural Radiance Fields (NeRF) to learn high quality\n3D object category models from collections of input images. In contrast to\nprevious work, we are able to do this whilst simultaneously separating\nforeground objects from their varying backgrounds. We achieve this via a\n2-component NeRF model, FiG-NeRF, that prefers explanation of the scene as a\ngeometrically constant background and a deformable foreground that represents\nthe object category. We show that this method can learn accurate 3D object\ncategory models using only photometric supervision and casually captured images\nof the objects. Additionally, our 2-part decomposition allows the model to\nperform accurate and crisp amodal segmentation. We quantitatively evaluate our\nmethod with view synthesis and image fidelity metrics, using synthetic,\nlab-captured, and in-the-wild data. Our results demonstrate convincing 3D\nobject category modelling that exceed the performance of existing methods.\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 01:38:54 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Xie", "Christopher", ""], ["Park", "Keunhong", ""], ["Martin-Brualla", "Ricardo", ""], ["Brown", "Matthew", ""]]}, {"id": "2104.08422", "submitter": "Trung-Nghia Le", "authors": "Marc Treu, Trung-Nghia Le, Huy H. Nguyen, Junichi Yamagishi, Isao\n  Echizen", "title": "Fashion-Guided Adversarial Attack on Person Segmentation", "comments": "Accepted to Workshop on Media Forensics, CVPR 2021. Project page:\n  https://github.com/nii-yamagishilab/fashion_adv", "journal-ref": "CVPR Workshops 2021", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents the first adversarial example based method for attacking\nhuman instance segmentation networks, namely person segmentation networks in\nshort, which are harder to fool than classification networks. We propose a\nnovel Fashion-Guided Adversarial Attack (FashionAdv) framework to automatically\nidentify attackable regions in the target image to minimize the effect on image\nquality. It generates adversarial textures learned from fashion style images\nand then overlays them on the clothing regions in the original image to make\nall persons in the image invisible to person segmentation networks. The\nsynthesized adversarial textures are inconspicuous and appear natural to the\nhuman eye. The effectiveness of the proposed method is enhanced by robustness\ntraining and by jointly attacking multiple components of the target network.\nExtensive experiments demonstrated the effectiveness of FashionAdv in terms of\nrobustness to image manipulations and storage in cyberspace as well as\nappearing natural to the human eye. The code and data are publicly released on\nour project page https://github.com/nii-yamagishilab/fashion_adv\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 02:17:33 GMT"}, {"version": "v2", "created": "Tue, 20 Apr 2021 02:16:48 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Treu", "Marc", ""], ["Le", "Trung-Nghia", ""], ["Nguyen", "Huy H.", ""], ["Yamagishi", "Junichi", ""], ["Echizen", "Isao", ""]]}, {"id": "2104.08446", "submitter": "Ramon Izquierdo Cordova", "authors": "Ramon Izquierdo-Cordova, Walterio Mayol-Cuevas", "title": "Towards Efficient Convolutional Network Models with Filter Distribution\n  Templates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Increasing number of filters in deeper layers when feature maps are decreased\nis a widely adopted pattern in convolutional network design. It can be found in\nclassical CNN architectures and in automatic discovered models. Even CNS\nmethods commonly explore a selection of multipliers derived from this pyramidal\npattern. We defy this practice by introducing a small set of templates\nconsisting of easy to implement, intuitive and aggressive variations of the\noriginal pyramidal distribution of filters in VGG and ResNet architectures.\nExperiments on CIFAR, CINIC10 and TinyImagenet datasets show that models\nproduced by our templates, are more efficient in terms of fewer parameters and\nmemory needs.\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 04:51:29 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Izquierdo-Cordova", "Ramon", ""], ["Mayol-Cuevas", "Walterio", ""]]}, {"id": "2104.08447", "submitter": "Nicole Han", "authors": "Nicole X. Han, William Yang Wang, Miguel P. Eckstein", "title": "Gaze Perception in Humans and CNN-Based Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Making accurate inferences about other individuals' locus of attention is\nessential for human social interactions and will be important for AI to\neffectively interact with humans. In this study, we compare how a CNN\n(convolutional neural network) based model of gaze and humans infer the locus\nof attention in images of real-world scenes with a number of individuals\nlooking at a common location. We show that compared to the model, humans'\nestimates of the locus of attention are more influenced by the context of the\nscene, such as the presence of the attended target and the number of\nindividuals in the image.\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 04:52:46 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Han", "Nicole X.", ""], ["Wang", "William Yang", ""], ["Eckstein", "Miguel P.", ""]]}, {"id": "2104.08466", "submitter": "Yiming Zhao", "authors": "Yiming Zhao, Lin Bai, Ziming Zhang and Xinming Huang", "title": "A Surface Geometry Model for LiDAR Depth Completion", "comments": "IEEE Robotics and Automation Letters (2021). Code link:\n  https://github.com/placeforyiming/RAL_Non-Learning_DepthCompletion", "journal-ref": null, "doi": "10.1109/LRA.2021.3068885", "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  LiDAR depth completion is a task that predicts depth values for every pixel\non the corresponding camera frame, although only sparse LiDAR points are\navailable. Most of the existing state-of-the-art solutions are based on deep\nneural networks, which need a large amount of data and heavy computations for\ntraining the models. In this letter, a novel non-learning depth completion\nmethod is proposed by exploiting the local surface geometry that is enhanced by\nan outlier removal algorithm. The proposed surface geometry model is inspired\nby the observation that most pixels with unknown depth have a nearby LiDAR\npoint. Therefore, it is assumed those pixels share the same surface with the\nnearest LiDAR point, and their respective depth can be estimated as the nearest\nLiDAR depth value plus a residual error. The residual error is calculated by\nusing a derived equation with several physical parameters as input, including\nthe known camera intrinsic parameters, estimated normal vector, and offset\ndistance on the image plane. The proposed method is further enhanced by an\noutlier removal algorithm that is designed to remove incorrectly mapped LiDAR\npoints from occluded regions. On KITTI dataset, the proposed solution achieves\nthe best error performance among all existing non-learning methods and is\ncomparable to the best self-supervised learning method and some supervised\nlearning methods. Moreover, since outlier points from occluded regions is a\ncommonly existing problem, the proposed outlier removal algorithm is a general\npreprocessing step that is applicable to many robotic systems with both camera\nand LiDAR sensors.\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 06:48:01 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Zhao", "Yiming", ""], ["Bai", "Lin", ""], ["Zhang", "Ziming", ""], ["Huang", "Xinming", ""]]}, {"id": "2104.08489", "submitter": "Yang Yang", "authors": "Yang Yang, Zhao-Yang Fu, De-Chuan Zhan, Zhi-Bin Liu, and Yuan Jiang", "title": "Semi-Supervised Multi-Modal Multi-Instance Multi-Label Deep Network with\n  Optimal Transport", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complex objects are usually with multiple labels, and can be represented by\nmultiple modal representations, e.g., the complex articles contain text and\nimage information as well as multiple annotations. Previous methods assume that\nthe homogeneous multi-modal data are consistent, while in real applications,\nthe raw data are disordered, e.g., the article constitutes with variable number\nof inconsistent text and image instances. Therefore, Multi-modal Multi-instance\nMulti-label (M3) learning provides a framework for handling such task and has\nexhibited excellent performance. However, M3 learning is facing two main\nchallenges: 1) how to effectively utilize label correlation; 2) how to take\nadvantage of multi-modal learning to process unlabeled instances. To solve\nthese problems, we first propose a novel Multi-modal Multi-instance Multi-label\nDeep Network (M3DN), which considers M3 learning in an end-to-end multi-modal\ndeep network and utilizes consistency principle among different modal bag-level\npredictions. Based on the M3DN, we learn the latent ground label metric with\nthe optimal transport. Moreover, we introduce the extrinsic unlabeled\nmulti-modal multi-instance data, and propose the M3DNS, which considers the\ninstance-level auto-encoder for single modality and modified bag-level optimal\ntransport to strengthen the consistency among modalities. Thereby M3DNS can\nbetter predict label and exploit label correlation simultaneously. Experiments\non benchmark datasets and real world WKG Game-Hub dataset validate the\neffectiveness of the proposed methods.\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 09:18:28 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Yang", "Yang", ""], ["Fu", "Zhao-Yang", ""], ["Zhan", "De-Chuan", ""], ["Liu", "Zhi-Bin", ""], ["Jiang", "Yuan", ""]]}, {"id": "2104.08500", "submitter": "Mingjian Zhu", "authors": "Mingjian Zhu, Kai Han, Yehui Tang, Yunhe Wang", "title": "Visual Transformer Pruning", "comments": "Accepted by the KDD 2021 Workshop on Model Mining", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vision transformer has achieved competitive performance on a variety of\ncomputer vision applications. However, their storage, run-time memory, and\ncomputational demands are hindering the deployment to mobile devices. Here we\npresent a vision transformer pruning approach, which identifies the impacts of\ndimensions in each layer of transformer and then executes pruning accordingly.\nBy encouraging dimension-wise sparsity in the transformer, important dimensions\nautomatically emerge. A great number of dimensions with small importance scores\ncan be discarded to achieve a high pruning ratio without significantly\ncompromising accuracy. The pipeline for vision transformer pruning is as\nfollows: 1) training with sparsity regularization; 2) pruning dimensions of\nlinear projections; 3) fine-tuning. The reduced parameters and FLOPs ratios of\nthe proposed algorithm are well evaluated and analyzed on ImageNet dataset to\ndemonstrate the effectiveness of our proposed method.\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 09:49:24 GMT"}, {"version": "v2", "created": "Tue, 20 Apr 2021 04:50:49 GMT"}, {"version": "v3", "created": "Wed, 14 Jul 2021 06:36:01 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Zhu", "Mingjian", ""], ["Han", "Kai", ""], ["Tang", "Yehui", ""], ["Wang", "Yunhe", ""]]}, {"id": "2104.08506", "submitter": "Lingyu Zhu", "authors": "Lingyu Zhu and Esa Rahtu", "title": "Visually Guided Sound Source Separation and Localization using\n  Self-Supervised Motion Representations", "comments": "18 pages. main paper: 8 pages; reference: 2 pages; supplementary\n  material: 8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The objective of this paper is to perform audio-visual sound source\nseparation, i.e.~to separate component audios from a mixture based on the\nvideos of sound sources. Moreover, we aim to pinpoint the source location in\nthe input video sequence. Recent works have shown impressive audio-visual\nseparation results when using prior knowledge of the source type (e.g. human\nplaying instrument) and pre-trained motion detectors (e.g. keypoints or optical\nflows). However, at the same time, the models are limited to a certain\napplication domain. In this paper, we address these limitations and make the\nfollowing contributions: i) we propose a two-stage architecture, called\nAppearance and Motion network (AMnet), where the stages specialise to\nappearance and motion cues, respectively. The entire system is trained in a\nself-supervised manner; ii) we introduce an Audio-Motion Embedding (AME)\nframework to explicitly represent the motions that related to sound; iii) we\npropose an audio-motion transformer architecture for audio and motion feature\nfusion; iv) we demonstrate state-of-the-art performance on two challenging\ndatasets (MUSIC-21 and AVE) despite the fact that we do not use any pre-trained\nkeypoint detectors or optical flow estimators. Project page:\nhttps://ly-zhu.github.io/self-supervised-motion-representations\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 10:09:15 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Zhu", "Lingyu", ""], ["Rahtu", "Esa", ""]]}, {"id": "2104.08510", "submitter": "Meng Liu", "authors": "Meng Liu, Longbiao Wang, Kong Aik Lee, Hanyi Zhang, Chang Zeng, Jianwu\n  Dang", "title": "Exploring Deep Learning for Joint Audio-Visual Lip Biometrics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Audio-visual (AV) lip biometrics is a promising authentication technique that\nleverages the benefits of both the audio and visual modalities in speech\ncommunication. Previous works have demonstrated the usefulness of AV lip\nbiometrics. However, the lack of a sizeable AV database hinders the exploration\nof deep-learning-based audio-visual lip biometrics. To address this problem, we\ncompile a moderate-size database using existing public databases. Meanwhile, we\nestablish the DeepLip AV lip biometrics system realized with a convolutional\nneural network (CNN) based video module, a time-delay neural network (TDNN)\nbased audio module, and a multimodal fusion module. Our experiments show that\nDeepLip outperforms traditional speaker recognition models in context modeling\nand achieves over 50% relative improvements compared with our best single\nmodality baseline, with an equal error rate of 0.75% and 1.11% on the test\ndatasets, respectively.\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 10:51:55 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Liu", "Meng", ""], ["Wang", "Longbiao", ""], ["Lee", "Kong Aik", ""], ["Zhang", "Hanyi", ""], ["Zeng", "Chang", ""], ["Dang", "Jianwu", ""]]}, {"id": "2104.08519", "submitter": "Shanmukh Reddy Manne", "authors": "Shanmukh Reddy Manne, Kiran Kumar Vupparaboina, Gowtham Chowdary\n  Gudapati, Ram Anudeep Peddoju, Chandra Prakash Konkimalla, Abhilash Goud,\n  Sarforaz Bin Bashar, Jay Chhablani, Soumya Jana", "title": "Efficient Screening of Diseased Eyes based on Fundus Autofluorescence\n  Images using Support Vector Machine", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  A variety of vision ailments are associated with geographic atrophy (GA) in\nthe foveal region of the eye. In current clinical practice, the ophthalmologist\nmanually detects potential presence of such GA based on fundus autofluorescence\n(FAF) images, and hence diagnoses the disease, when relevant. However, in view\nof the general scarcity of ophthalmologists relative to the large number of\nsubjects seeking eyecare, especially in remote regions, it becomes imperative\nto develop methods to direct expert time and effort to medically significant\ncases. Further, subjects from either disadvantaged background or remote\nlocalities, who face considerable economic/physical barrier in consulting\ntrained ophthalmologists, tend to seek medical attention only after being\nreasonably certain that an adverse condition exists. To serve the interest of\nboth the ophthalmologist and the potential patient, we plan a screening step,\nwhere healthy and diseased eyes are algorithmically differentiated with limited\ninput from only optometrists who are relatively more abundant in number.\nSpecifically, an early treatment diabetic retinopathy study (ETDRS) grid is\nplaced by an optometrist on each FAF image, based on which sectoral statistics\nare automatically collected. Using such statistics as features, healthy and\ndiseased eyes are proposed to be classified by training an algorithm using\navailable medical records. In this connection, we demonstrate the efficacy of\nsupport vector machines (SVM). Specifically, we consider SVM with linear as\nwell as radial basis function (RBF) kernel, and observe satisfactory\nperformance of both variants. Among those, we recommend the latter in view of\nits slight superiority in terms of classification accuracy (90.55% at a\nstandard training-to-test ratio of 80:20), and practical class-conditional\ncosts.\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 11:54:34 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Manne", "Shanmukh Reddy", ""], ["Vupparaboina", "Kiran Kumar", ""], ["Gudapati", "Gowtham Chowdary", ""], ["Peddoju", "Ram Anudeep", ""], ["Konkimalla", "Chandra Prakash", ""], ["Goud", "Abhilash", ""], ["Bashar", "Sarforaz Bin", ""], ["Chhablani", "Jay", ""], ["Jana", "Soumya", ""]]}, {"id": "2104.08527", "submitter": "Chun-Hao Paul Huang", "authors": "Muhammed Kocabas, Chun-Hao P. Huang, Otmar Hilliges, Michael J. Black", "title": "PARE: Part Attention Regressor for 3D Human Body Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite significant progress, we show that state of the art 3D human pose and\nshape estimation methods remain sensitive to partial occlusion and can produce\ndramatically wrong predictions although much of the body is observable. To\naddress this, we introduce a soft attention mechanism, called the Part\nAttention REgressor (PARE), that learns to predict body-part-guided attention\nmasks. We observe that state-of-the-art methods rely on global feature\nrepresentations, making them sensitive to even small occlusions. In contrast,\nPARE's part-guided attention mechanism overcomes these issues by exploiting\ninformation about the visibility of individual body parts while leveraging\ninformation from neighboring body-parts to predict occluded parts. We show\nqualitatively that PARE learns sensible attention masks, and quantitative\nevaluation confirms that PARE achieves more accurate and robust reconstruction\nresults than existing approaches on both occlusion-specific and standard\nbenchmarks. Code will be available for research purposes at\nhttps://pare.is.tue.mpg.de/.\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 12:42:56 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Kocabas", "Muhammed", ""], ["Huang", "Chun-Hao P.", ""], ["Hilliges", "Otmar", ""], ["Black", "Michael J.", ""]]}, {"id": "2104.08538", "submitter": "Jong Chul Ye", "authors": "Taesung Kwon, Jong Chul Ye", "title": "Cycle-free CycleGAN using Invertible Generator for Unsupervised Low-Dose\n  CT Denoising", "comments": "12 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently, CycleGAN was shown to provide high-performance, ultra-fast\ndenoising for low-dose X-ray computed tomography (CT) without the need for a\npaired training dataset. Although this was possible thanks to cycle\nconsistency, CycleGAN requires two generators and two discriminators to enforce\ncycle consistency, demanding significant GPU resources and technical skills for\ntraining. A recent proposal of tunable CycleGAN with Adaptive Instance\nNormalization (AdaIN) alleviates the problem in part by using a single\ngenerator. However, two discriminators and an additional AdaIN code generator\nare still required for training. To solve this problem, here we present a novel\ncycle-free Cycle-GAN architecture, which consists of a single generator and a\ndiscriminator but still guarantees cycle consistency. The main innovation comes\nfrom the observation that the use of an invertible generator automatically\nfulfills the cycle consistency condition and eliminates the additional\ndiscriminator in the CycleGAN formulation. To make the invertible generator\nmore effective, our network is implemented in the wavelet residual domain.\nExtensive experiments using various levels of low-dose CT images confirm that\nour method can significantly improve denoising performance using only 10% of\nlearnable parameters and faster training time compared to the conventional\nCycleGAN.\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 13:23:36 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Kwon", "Taesung", ""], ["Ye", "Jong Chul", ""]]}, {"id": "2104.08541", "submitter": "Jiajun Deng", "authors": "Jiajun Deng, Zhengyuan Yang, Tianlang Chen, Wengang Zhou, and Houqiang\n  Li", "title": "TransVG: End-to-End Visual Grounding with Transformers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we present a neat yet effective transformer-based framework\nfor visual grounding, namely TransVG, to address the task of grounding a\nlanguage query to the corresponding region onto an image. The state-of-the-art\nmethods, including two-stage or one-stage ones, rely on a complex module with\nmanually-designed mechanisms to perform the query reasoning and multi-modal\nfusion. However, the involvement of certain mechanisms in fusion module design,\nsuch as query decomposition and image scene graph, makes the models easily\noverfit to datasets with specific scenarios, and limits the plenitudinous\ninteraction between the visual-linguistic context. To avoid this caveat, we\npropose to establish the multi-modal correspondence by leveraging transformers,\nand empirically show that the complex fusion modules (e.g., modular attention\nnetwork, dynamic graph, and multi-modal tree) can be replaced by a simple stack\nof transformer encoder layers with higher performance. Moreover, we\nre-formulate the visual grounding as a direct coordinates regression problem\nand avoid making predictions out of a set of candidates (i.e., region proposals\nor anchor boxes). Extensive experiments are conducted on five widely used\ndatasets, and a series of state-of-the-art records are set by our TransVG. We\nbuild the benchmark of transformer-based visual grounding framework and will\nmake our code available to the public.\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 13:35:24 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Deng", "Jiajun", ""], ["Yang", "Zhengyuan", ""], ["Chen", "Tianlang", ""], ["Zhou", "Wengang", ""], ["Li", "Houqiang", ""]]}, {"id": "2104.08554", "submitter": "Suraj Mishra", "authors": "Suraj Mishra, Danny Z. Chen, X. Sharon Hu", "title": "Objective-Dependent Uncertainty Driven Retinal Vessel Segmentation", "comments": "ISBI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  From diagnosing neovascular diseases to detecting white matter lesions,\naccurate tiny vessel segmentation in fundus images is critical. Promising\nresults for accurate vessel segmentation have been known. However, their\neffectiveness in segmenting tiny vessels is still limited. In this paper, we\nstudy retinal vessel segmentation by incorporating tiny vessel segmentation\ninto our framework for the overall accurate vessel segmentation. To achieve\nthis, we propose a new deep convolutional neural network (CNN) which divides\nvessel segmentation into two separate objectives. Specifically, we consider the\noverall accurate vessel segmentation and tiny vessel segmentation as two\nindividual objectives. Then, by exploiting the objective-dependent\n(homoscedastic) uncertainty, we enable the network to learn both objectives\nsimultaneously. Further, to improve the individual objectives, we propose: (a)\na vessel weight map based auxiliary loss for enhancing tiny vessel connectivity\n(i.e., improving tiny vessel segmentation), and (b) an enhanced encoder-decoder\narchitecture for improved localization (i.e., for accurate vessel\nsegmentation). Using 3 public retinal vessel segmentation datasets (CHASE_DB1,\nDRIVE, and STARE), we verify the superiority of our proposed framework in\nsegmenting tiny vessels (8.3% average improvement in sensitivity) while\nachieving better area under the receiver operating characteristic curve (AUC)\ncompared to state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 14:17:09 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Mishra", "Suraj", ""], ["Chen", "Danny Z.", ""], ["Hu", "X. Sharon", ""]]}, {"id": "2104.08560", "submitter": "Andrea Burns", "authors": "Andrea Burns, Deniz Arsan, Sanjna Agrawal, Ranjitha Kumar, Kate\n  Saenko, Bryan A. Plummer", "title": "Mobile App Tasks with Iterative Feedback (MoTIF): Addressing Task\n  Feasibility in Interactive Visual Environments", "comments": "Accepted at the workshop on Visually Grounded Interaction and\n  Language (ViGIL) at NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In recent years, vision-language research has shifted to study tasks which\nrequire more complex reasoning, such as interactive question answering, visual\ncommon sense reasoning, and question-answer plausibility prediction. However,\nthe datasets used for these problems fail to capture the complexity of real\ninputs and multimodal environments, such as ambiguous natural language requests\nand diverse digital domains. We introduce Mobile app Tasks with Iterative\nFeedback (MoTIF), a dataset with natural language commands for the greatest\nnumber of interactive environments to date. MoTIF is the first to contain\nnatural language requests for interactive environments that are not\nsatisfiable, and we obtain follow-up questions on this subset to enable\nresearch on task uncertainty resolution. We perform initial feasibility\nclassification experiments and only reach an F1 score of 37.3, verifying the\nneed for richer vision-language representations and improved architectures to\nreason about task feasibility.\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 14:48:02 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Burns", "Andrea", ""], ["Arsan", "Deniz", ""], ["Agrawal", "Sanjna", ""], ["Kumar", "Ranjitha", ""], ["Saenko", "Kate", ""], ["Plummer", "Bryan A.", ""]]}, {"id": "2104.08568", "submitter": "Yan Xu", "authors": "Yan Xu, Yu-Jhe Li, Xinshuo Weng, Kris Kitani", "title": "Wide-Baseline Multi-Camera Calibration using Person Re-Identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of estimating the 3D pose of a network of cameras for\nlarge-environment wide-baseline scenarios, e.g., cameras for construction\nsites, sports stadiums, and public spaces. This task is challenging since\ndetecting and matching the same 3D keypoint observed from two very different\ncamera views is difficult, making standard structure-from-motion (SfM)\npipelines inapplicable. In such circumstances, treating people in the scene as\n\"keypoints\" and associating them across different camera views can be an\nalternative method for obtaining correspondences. Based on this intuition, we\npropose a method that uses ideas from person re-identification (re-ID) for\nwide-baseline camera calibration. Our method first employs a re-ID method to\nassociate human bounding boxes across cameras, then converts bounding box\ncorrespondences to point correspondences, and finally solves for camera pose\nusing multi-view geometry and bundle adjustment. Since our method does not\nrequire specialized calibration targets except for visible people, it applies\nto situations where frequent calibration updates are required. We perform\nextensive experiments on datasets captured from scenes of different sizes,\ncamera settings (indoor and outdoor), and human activities (walking, playing\nbasketball, construction). Experiment results show that our method achieves\nsimilar performance to standard SfM methods relying on manually labeled point\ncorrespondences.\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 15:09:18 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Xu", "Yan", ""], ["Li", "Yu-Jhe", ""], ["Weng", "Xinshuo", ""], ["Kitani", "Kris", ""]]}, {"id": "2104.08569", "submitter": "Gang Zhang", "authors": "Gang Zhang, Xin Lu, Jingru Tan, Jianmin Li, Zhaoxiang Zhang, Quanquan\n  Li, Xiaolin Hu", "title": "RefineMask: Towards High-Quality Instance Segmentation with Fine-Grained\n  Features", "comments": "Accepted by CVPR 2021. Code is available at\n  https://github.com/zhanggang001/RefineMask", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The two-stage methods for instance segmentation, e.g. Mask R-CNN, have\nachieved excellent performance recently. However, the segmented masks are still\nvery coarse due to the downsampling operations in both the feature pyramid and\nthe instance-wise pooling process, especially for large objects. In this work,\nwe propose a new method called RefineMask for high-quality instance\nsegmentation of objects and scenes, which incorporates fine-grained features\nduring the instance-wise segmenting process in a multi-stage manner. Through\nfusing more detailed information stage by stage, RefineMask is able to refine\nhigh-quality masks consistently. RefineMask succeeds in segmenting hard cases\nsuch as bent parts of objects that are over-smoothed by most previous methods\nand outputs accurate boundaries. Without bells and whistles, RefineMask yields\nsignificant gains of 2.6, 3.4, 3.8 AP over Mask R-CNN on COCO, LVIS, and\nCityscapes benchmarks respectively at a small amount of additional\ncomputational cost. Furthermore, our single-model result outperforms the winner\nof the LVIS Challenge 2020 by 1.3 points on the LVIS test-dev set and\nestablishes a new state-of-the-art. Code will be available at\nhttps://github.com/zhanggang001/RefineMask.\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 15:09:20 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Zhang", "Gang", ""], ["Lu", "Xin", ""], ["Tan", "Jingru", ""], ["Li", "Jianmin", ""], ["Zhang", "Zhaoxiang", ""], ["Li", "Quanquan", ""], ["Hu", "Xiaolin", ""]]}, {"id": "2104.08572", "submitter": "Christian Simon", "authors": "Christian Simon, Piotr Koniusz, Mehrtash Harandi", "title": "On Learning the Geodesic Path for Incremental Learning", "comments": "Accepted to CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks notoriously suffer from the problem of catastrophic\nforgetting, the phenomenon of forgetting the past knowledge when acquiring new\nknowledge. Overcoming catastrophic forgetting is of significant importance to\nemulate the process of \"incremental learning\", where the model is capable of\nlearning from sequential experience in an efficient and robust way.\nState-of-the-art techniques for incremental learning make use of knowledge\ndistillation towards preventing catastrophic forgetting. Therein, one updates\nthe network while ensuring that the network's responses to previously seen\nconcepts remain stable throughout updates. This in practice is done by\nminimizing the dissimilarity between current and previous responses of the\nnetwork one way or another. Our work contributes a novel method to the arsenal\nof distillation techniques. In contrast to the previous state of the art, we\npropose to firstly construct low-dimensional manifolds for previous and current\nresponses and minimize the dissimilarity between the responses along the\ngeodesic connecting the manifolds. This induces a more formidable knowledge\ndistillation with smooth properties which preserves the past knowledge more\nefficiently as observed by our comprehensive empirical study.\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 15:26:34 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Simon", "Christian", ""], ["Koniusz", "Piotr", ""], ["Harandi", "Mehrtash", ""]]}, {"id": "2104.08575", "submitter": "Hangqi Zhou", "authors": "Hangqi Zhou, Chao Huang, Shangqi Gao, Xiahai Zhuang", "title": "VSpSR: Explorable Super-Resolution via Variational Sparse Representation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Super-resolution (SR) is an ill-posed problem, which means that infinitely\nmany high-resolution (HR) images can be degraded to the same low-resolution\n(LR) image. To study the one-to-many stochastic SR mapping, we implicitly\nrepresent the non-local self-similarity of natural images and develop a\nVariational Sparse framework for Super-Resolution (VSpSR) via neural networks.\nSince every small patch of a HR image can be well approximated by the sparse\nrepresentation of atoms in an over-complete dictionary, we design a two-branch\nmodule, i.e., VSpM, to explore the SR space. Concretely, one branch of VSpM\nextracts patch-level basis from the LR input, and the other branch infers\npixel-wise variational distributions with respect to the sparse coefficients.\nBy repeatedly sampling coefficients, we could obtain infinite sparse\nrepresentations, and thus generate diverse HR images. According to the\npreliminary results of NTIRE 2021 challenge on learning SR space, our team\n(FudanZmic21) ranks 7-th in terms of released scores. The implementation of\nVSpSR is released at https://zmiclab.github.io/.\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 15:36:24 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Zhou", "Hangqi", ""], ["Huang", "Chao", ""], ["Gao", "Shangqi", ""], ["Zhuang", "Xiahai", ""]]}, {"id": "2104.08581", "submitter": "Ujjal Kr Dutta", "authors": "Ujjal Kr Dutta, Sandeep Repakula, Maulik Parmar, Abhinav Ravi", "title": "Color Variants Identification in Fashion e-commerce via Contrastive\n  Self-Supervised Representation Learning", "comments": "Accepted In IJCAI-21 Weakly Supervised Representation Learning (WSRL)\n  workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In this paper, we utilize deep visual Representation Learning to address an\nimportant problem in fashion e-commerce: color variants identification, i.e.,\nidentifying fashion products that match exactly in their design (or style), but\nonly to differ in their color. At first we attempt to tackle the problem by\nobtaining manual annotations (depicting whether two products are color\nvariants), and train a supervised triplet loss based neural network model to\nlearn representations of fashion products. However, for large scale real-world\nindustrial datasets such as addressed in our paper, it is infeasible to obtain\nannotations for the entire dataset, while capturing all the difficult corner\ncases. Interestingly, we observed that color variants are essentially\nmanifestations of color jitter based augmentations. Thus, we instead explore\nSelf-Supervised Learning (SSL) to solve this problem. We observed that existing\nstate-of-the-art SSL methods perform poor, for our problem. To address this, we\npropose a novel SSL based color variants model that simultaneously focuses on\ndifferent parts of an apparel. Quantitative and qualitative evaluation shows\nthat our method outperforms existing SSL methods, and at times, the supervised\nmodel.\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 15:51:56 GMT"}, {"version": "v2", "created": "Wed, 30 Jun 2021 22:07:44 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Dutta", "Ujjal Kr", ""], ["Repakula", "Sandeep", ""], ["Parmar", "Maulik", ""], ["Ravi", "Abhinav", ""]]}, {"id": "2104.08585", "submitter": "Dipesh Gyawali", "authors": "Dipesh Gyawali, Prashanga Pokharel, Ashutosh Chauhan, Subodh Chandra\n  Shakya", "title": "Age Range Estimation using MTCNN and VGG-Face Model", "comments": "6 pages, 10 figures", "journal-ref": "11th IEEE International Conference on Computing, Communication and\n  Networking Technologies (ICCCNT), 2020", "doi": "10.1109/ICCCNT49239.2020.9225443", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Convolutional Neural Network has amazed us with its usage on several\napplications. Age range estimation using CNN is emerging due to its application\nin myriad of areas which makes it a state-of-the-art area for research and\nimprove the estimation accuracy. A deep CNN model is used for identification of\npeople's age range in our proposed work. At first, we extracted only face\nimages from image dataset using MTCNN to remove unnecessary features other than\nface from the image. Secondly, we used random crop technique for data\naugmentation to improve the model performance. We have used the concept of\ntransfer learning in our research. A pretrained face recognition model i.e\nVGG-Face is used to build our model for identification of age range whose\nperformance is evaluated on Adience Benchmark for confirming the efficacy of\nour work. The performance in test set outperformed existing state-of-the-art by\nsubstantial margins.\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 15:54:14 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Gyawali", "Dipesh", ""], ["Pokharel", "Prashanga", ""], ["Chauhan", "Ashutosh", ""], ["Shakya", "Subodh Chandra", ""]]}, {"id": "2104.08623", "submitter": "Junyu Chen", "authors": "Junyu Chen, Ye Li, Licia P. Luna, Hyun Woo Chung, Steven P. Rowe, Yong\n  Du, Lilja B. Solnes, Eric C. Frey", "title": "Learning Fuzzy Clustering for SPECT/CT Segmentation via Convolutional\n  Neural Networks", "comments": "This manuscript has been published by Medical Physics (2021)", "journal-ref": null, "doi": "10.1002/mp.14903", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantitative bone single-photon emission computed tomography (QBSPECT) has\nthe potential to provide a better quantitative assessment of bone metastasis\nthan planar bone scintigraphy due to its ability to better quantify activity in\noverlapping structures. An important element of assessing response of bone\nmetastasis is accurate image segmentation. However, limited by the properties\nof QBSPECT images, the segmentation of anatomical regions-of-interests (ROIs)\nstill relies heavily on the manual delineation by experts. This work proposes a\nfast and robust automated segmentation method for partitioning a QBSPECT image\ninto lesion, bone, and background. We present a new unsupervised segmentation\nloss function and its semi- and supervised variants for training a\nconvolutional neural network (ConvNet). The loss functions were developed based\non the objective function of the classical Fuzzy C-means (FCM) algorithm. We\nconducted a comprehensive study to compare our proposed methods with ConvNets\ntrained using supervised loss functions and conventional clustering methods.\nThe Dice similarity coefficient (DSC) and several other metrics were used as\nfigures of merit as applied to the task of delineating lesion and bone in both\nsimulated and clinical SPECT/CT images. We experimentally demonstrated that the\nproposed methods yielded good segmentation results on a clinical dataset even\nthough the training was done using realistic simulated images. A ConvNet-based\nimage segmentation method that uses novel loss functions was developed and\nevaluated. The method can operate in unsupervised, semi-supervised, or\nfully-supervised modes depending on the availability of annotated training\ndata. The results demonstrated that the proposed method provides fast and\nrobust lesion and bone segmentation for QBSPECT/CT. The method can potentially\nbe applied to other medical image segmentation applications.\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 19:03:52 GMT"}, {"version": "v2", "created": "Thu, 22 Apr 2021 03:36:41 GMT"}, {"version": "v3", "created": "Fri, 28 May 2021 14:43:16 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Chen", "Junyu", ""], ["Li", "Ye", ""], ["Luna", "Licia P.", ""], ["Chung", "Hyun Woo", ""], ["Rowe", "Steven P.", ""], ["Du", "Yong", ""], ["Solnes", "Lilja B.", ""], ["Frey", "Eric C.", ""]]}, {"id": "2104.08633", "submitter": "Caroline Pacheco Do Espirito Silva", "authors": "Caroline Pacheco do Esp\\'irito Silva, Jos\\'e A. M. Felippe De Souza,\n  Antoine Vacavant, Thierry Bouwmans, Andrews Cordolino Sobral", "title": "Automated Mathematical Equation Structure Discovery for Visual Analysis", "comments": "25 pages, 8 figures, submitted to JMLR", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Finding the best mathematical equation to deal with the different challenges\nfound in complex scenarios requires a thorough understanding of the scenario\nand a trial and error process carried out by experts. In recent years, most\nstate-of-the-art equation discovery methods have been widely applied in\nmodeling and identification systems. However, equation discovery approaches can\nbe very useful in computer vision, particularly in the field of feature\nextraction. In this paper, we focus on recent AI advances to present a novel\nframework for automatically discovering equations from scratch with little\nhuman intervention to deal with the different challenges encountered in\nreal-world scenarios. In addition, our proposal can reduce human bias by\nproposing a search space design through generative network instead of\nhand-designed. As a proof of concept, the equations discovered by our framework\nare used to distinguish moving objects from the background in video sequences.\nExperimental results show the potential of the proposed approach and its\neffectiveness in discovering the best equation in video sequences. The code and\ndata are available at:\nhttps://github.com/carolinepacheco/equation-discovery-scene-analysis\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 19:42:06 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Silva", "Caroline Pacheco do Esp\u00edrito", ""], ["De Souza", "Jos\u00e9 A. M. Felippe", ""], ["Vacavant", "Antoine", ""], ["Bouwmans", "Thierry", ""], ["Sobral", "Andrews Cordolino", ""]]}, {"id": "2104.08657", "submitter": "Jiang Yu Zheng Dr.", "authors": "Jiang Yu Zheng", "title": "IUPUI Driving Videos and Images in All Weather and Illumination\n  Conditions", "comments": "10 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This document describes an image and video dataset of driving views captured\nin all weather and illumination conditions. The data set has been submitted to\nCDVL.\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 22:59:15 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Zheng", "Jiang Yu", ""]]}, {"id": "2104.08665", "submitter": "Tsung-Ming Tai", "authors": "Tsung-Ming Tai, Giuseppe Fiameni, Cheng-Kuang Lee, Oswald Lanz", "title": "Higher Order Recurrent Space-Time Transformer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Endowing visual agents with predictive capability is a key step towards video\nintelligence at scale. The predominant modeling paradigm for this is sequence\nlearning, mostly implemented through LSTMs. Feed-forward Transformer\narchitectures have replaced recurrent model designs in ML applications of\nlanguage processing and also partly in computer vision. In this paper we\ninvestigate on the competitiveness of Transformer-style architectures for video\npredictive tasks. To do so we propose HORST, a novel higher order recurrent\nlayer design whose core element is a spatial-temporal decomposition of\nself-attention for video. HORST achieves state of the art competitive\nperformance on Something-Something-V2 early action recognition and\nEPIC-Kitchens-55 action anticipation, without exploiting a task specific\ndesign. We believe this is promising evidence of causal predictive capability\nthat we attribute to our recurrent higher order design of self-attention.\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 23:51:05 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Tai", "Tsung-Ming", ""], ["Fiameni", "Giuseppe", ""], ["Lee", "Cheng-Kuang", ""], ["Lanz", "Oswald", ""]]}, {"id": "2104.08683", "submitter": "Chenxu Luo", "authors": "Chenxu Luo, Xiaodong Yang, Alan Yuille", "title": "Self-Supervised Pillar Motion Learning for Autonomous Driving", "comments": "cvpr2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Autonomous driving can benefit from motion behavior comprehension when\ninteracting with diverse traffic participants in highly dynamic environments.\nRecently, there has been a growing interest in estimating class-agnostic motion\ndirectly from point clouds. Current motion estimation methods usually require\nvast amount of annotated training data from self-driving scenes. However,\nmanually labeling point clouds is notoriously difficult, error-prone and\ntime-consuming. In this paper, we seek to answer the research question of\nwhether the abundant unlabeled data collections can be utilized for accurate\nand efficient motion learning. To this end, we propose a learning framework\nthat leverages free supervisory signals from point clouds and paired camera\nimages to estimate motion purely via self-supervision. Our model involves a\npoint cloud based structural consistency augmented with probabilistic motion\nmasking as well as a cross-sensor motion regularization to realize the desired\nself-supervision. Experiments reveal that our approach performs competitively\nto supervised methods, and achieves the state-of-the-art result when combining\nour self-supervised model with supervised fine-tuning.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 02:32:08 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Luo", "Chenxu", ""], ["Yang", "Xiaodong", ""], ["Yuille", "Alan", ""]]}, {"id": "2104.08688", "submitter": "Shixiang Zhu", "authors": "Shixiang Zhu, Sven Voigt, Henry Yuchi, Jordan Key, Yao Xie, Josh\n  Kacher, Surya R. Kalidindi", "title": "Harvesting data revolution for transmission electron microscopy (TEM)\n  using signal processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  TEM (Transmission Electron Microscopy) is a powerful tool for imaging\nmaterial structure and characterizing material chemistry. Recent advances in\ndata collection technology for TEM have enabled high-volume and high-resolution\ndata collection at a microsecond frame rate. This challenge requires the\ndevelopment of new data processing tools, including image analysis, feature\nextraction, and streaming data processing techniques. In this paper, we\nhighlight a few areas that have benefited from combining signal processing and\nstatistical analysis with data collection capabilities in TEM and present a\nfuture outlook in opportunities of integrating signal processing with automated\nTEM data analysis.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 02:56:04 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Zhu", "Shixiang", ""], ["Voigt", "Sven", ""], ["Yuchi", "Henry", ""], ["Key", "Jordan", ""], ["Xie", "Yao", ""], ["Kacher", "Josh", ""], ["Kalidindi", "Surya R.", ""]]}, {"id": "2104.08689", "submitter": "Kai Li", "authors": "Kai Li, Curtis Wigington, Chris Tensmeyer, Vlad I. Morariu, Handong\n  Zhao, Varun Manjunatha, Nikolaos Barmpalios, Yun Fu", "title": "RPCL: A Framework for Improving Cross-Domain Detection with Auxiliary\n  Tasks", "comments": "10 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Cross-Domain Detection (XDD) aims to train an object detector using labeled\nimage from a source domain but have good performance in the target domain with\nonly unlabeled images. Existing approaches achieve this either by aligning the\nfeature maps or the region proposals from the two domains, or by transferring\nthe style of source images to that of target image. Contrasted with prior work,\nthis paper provides a complementary solution to align domains by learning the\nsame auxiliary tasks in both domains simultaneously. These auxiliary tasks push\nimage from both domains towards shared spaces, which bridges the domain gap.\nSpecifically, this paper proposes Rotation Prediction and Consistency Learning\n(PRCL), a framework complementing existing XDD methods for domain alignment by\nleveraging the two auxiliary tasks. The first one encourages the model to\nextract region proposals from foreground regions by rotating an image and\npredicting the rotation angle from the extracted region proposals. The second\ntask encourages the model to be robust to changes in the image space by\noptimizing the model to make consistent class predictions for region proposals\nregardless of image perturbations. Experiments show the detection performance\ncan be consistently and significantly enhanced by applying the two proposed\ntasks to existing XDD methods.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 02:56:19 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Li", "Kai", ""], ["Wigington", "Curtis", ""], ["Tensmeyer", "Chris", ""], ["Morariu", "Vlad I.", ""], ["Zhao", "Handong", ""], ["Manjunatha", "Varun", ""], ["Barmpalios", "Nikolaos", ""], ["Fu", "Yun", ""]]}, {"id": "2104.08697", "submitter": "Dongchen Lu", "authors": "Dongchen Lu", "title": "OSKDet: Towards Orientation-sensitive Keypoint Localization for Rotated\n  Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rotated object detection is a challenging issue of computer vision field.\nLoss of spatial information and confusion of parametric order have been the\nbottleneck for rotated detection accuracy. In this paper, we propose an\norientation-sensitive keypoint based rotated detector OSKDet. We adopt a set of\nkeypoints to characterize the target and predict the keypoint heatmap on ROI to\nform a rotated target. By proposing the orientation-sensitive heatmap, OSKDet\ncould learn the shape and direction of rotated target implicitly and has\nstronger modeling capabilities for target representation, which improves the\nlocalization accuracy and acquires high quality detection results. To extract\nhighly effective features at border areas, we design a rotation-aware\ndeformable convolution module. Furthermore, we explore a new keypoint reorder\nalgorithm and feature fusion module based on the angle distribution to\neliminate the confusion of keypoint order. Experimental results on several\npublic benchmarks show the state-of-the-art performance of OSKDet.\nSpecifically, we achieve an AP of 77.81% on DOTA, 89.91% on HRSC2016, and\n97.18% on UCAS-AOD, respectively.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 03:40:52 GMT"}, {"version": "v2", "created": "Mon, 28 Jun 2021 14:57:11 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Lu", "Dongchen", ""]]}, {"id": "2104.08700", "submitter": "Yuxin Zhang", "authors": "Yuxin Zhang, Mingbao Lin, Fei Chao, Yan Wang, Yongjian Wu, Feiyue\n  Huang, Mingliang Xu, Yonghong Tian, Rongrong Ji", "title": "Lottery Jackpots Exist in Pre-trained Models", "comments": "10 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Network pruning is an effective approach to reduce network complexity without\nperformance compromise. Existing studies achieve the sparsity of neural\nnetworks via time-consuming weight tuning or complex search on networks with\nexpanded width, which greatly limits the applications of network pruning. In\nthis paper, we show that high-performing and sparse sub-networks without the\ninvolvement of weight tuning, termed \"lottery jackpots\", exist in pre-trained\nmodels with unexpanded width. For example, we obtain a lottery jackpot that has\nonly 10% parameters and still reaches the performance of the original dense\nVGGNet-19 without any modifications on the pre-trained weights. Furthermore, we\nobserve that the sparse masks derived from many existing pruning criteria have\na high overlap with the searched mask of our lottery jackpot, among which, the\nmagnitude-based pruning results in the most similar mask with ours. Based on\nthis insight, we initialize our sparse mask using the magnitude pruning,\nresulting in at least 3x cost reduction on the lottery jackpot search while\nachieves comparable or even better performance. Specifically, our\nmagnitude-based lottery jackpot removes 90% weights in the ResNet-50, while\neasily obtains more than 70% top-1 accuracy using only 10 searching epochs on\nImageNet.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 03:50:28 GMT"}, {"version": "v2", "created": "Wed, 2 Jun 2021 06:21:53 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Zhang", "Yuxin", ""], ["Lin", "Mingbao", ""], ["Chao", "Fei", ""], ["Wang", "Yan", ""], ["Wu", "Yongjian", ""], ["Huang", "Feiyue", ""], ["Xu", "Mingliang", ""], ["Tian", "Yonghong", ""], ["Ji", "Rongrong", ""]]}, {"id": "2104.08702", "submitter": "Mahdi S. Hosseini Dr.", "authors": "Andre Fu and Mahdi S. Hosseini and Konstantinos N. Plataniotis", "title": "Reconsidering CO2 emissions from Computer Vision", "comments": "Accepted for publication in CVPR 2021 Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Climate change is a pressing issue that is currently affecting and will\naffect every part of our lives. It's becoming incredibly vital we, as a\nsociety, address the climate crisis as a universal effort, including those in\nthe Computer Vision (CV) community. In this work, we analyze the total cost of\nCO2 emissions by breaking it into (1) the architecture creation cost and (2)\nthe life-time evaluation cost. We show that over time, these costs are\nnon-negligible and are having a direct impact on our future. Importantly, we\nconduct an ethical analysis of how the CV-community is unintentionally\noverlooking its own ethical AI principles by emitting this level of CO2. To\naddress these concerns, we propose adding \"enforcement\" as a pillar of ethical\nAI and provide some recommendations for how architecture designers and broader\nCV community can curb the climate crisis.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 04:01:40 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Fu", "Andre", ""], ["Hosseini", "Mahdi S.", ""], ["Plataniotis", "Konstantinos N.", ""]]}, {"id": "2104.08717", "submitter": "Bingyuan Liu", "authors": "Bingyuan Liu, Jose Dolz, Adrian Galdran, Riadh Kobbi, Ismail Ben Ayed", "title": "The hidden label-marginal biases of segmentation losses", "comments": "Code available at https://github.com/by-liu/SegLossBias", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most segmentation losses are arguably variants of the Cross-Entropy (CE) or\nDice loss. In the literature, there is no clear consensus as to which of these\nlosses is a better choice, with varying performances for each across different\nbenchmarks and applications. We develop a theoretical analysis that links these\ntwo types of losses, exposing their advantages and weaknesses. First, we\nexplicitly demonstrate that CE and Dice share a much deeper connection than\npreviously thought: CE is an upper bound on both logarithmic and linear Dice\nlosses. Furthermore, we provide an information-theoretic analysis, which\nhighlights hidden label-marginal biases : Dice has an intrinsic bias towards\nimbalanced solutions, whereas CE implicitly encourages the ground-truth region\nproportions. Our theoretical results explain the wide experimental evidence in\nthe medical-imaging literature, whereby Dice losses bring improvements for\nimbalanced segmentation. It also explains why CE dominates natural-image\nproblems with diverse class proportions, in which case Dice might have\ndifficulty adapting to different label-marginal distributions. Based on our\ntheoretical analysis, we propose a principled and simple solution, which\nenables to control explicitly the label-marginal bias. Our loss integrates CE\nwith explicit ${\\cal L}_1$ regularization, which encourages label marginals to\nmatch target class proportions, thereby mitigating class imbalance but without\nlosing generality. Comprehensive experiments and ablation studies over\ndifferent losses and applications validate our theoretical analysis, as well as\nthe effectiveness of our explicit label-marginal regularizers.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 04:59:39 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Liu", "Bingyuan", ""], ["Dolz", "Jose", ""], ["Galdran", "Adrian", ""], ["Kobbi", "Riadh", ""], ["Ayed", "Ismail Ben", ""]]}, {"id": "2104.08718", "submitter": "Jack Hessel", "authors": "Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, Yejin Choi", "title": "CLIPScore: A Reference-free Evaluation Metric for Image Captioning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image captioning has conventionally relied on reference-based automatic\nevaluations, where machine captions are compared against captions written by\nhumans. This is in stark contrast to the reference-free manner in which humans\nassess caption quality.\n  In this paper, we report the surprising empirical finding that CLIP (Radford\net al., 2021), a cross-modal model pretrained on 400M image+caption pairs from\nthe web, can be used for robust automatic evaluation of image captioning\nwithout the need for references. Experiments spanning several corpora\ndemonstrate that our new reference-free metric, CLIPScore, achieves the highest\ncorrelation with human judgements, outperforming existing reference-based\nmetrics like CIDEr and SPICE. Information gain experiments demonstrate that\nCLIPScore, with its tight focus on image-text compatibility, is complementary\nto existing reference-based metrics that emphasize text-text similarities.\nThus, we also present a reference-augmented version, RefCLIPScore, which\nachieves even higher correlation. Beyond literal description tasks, several\ncase studies reveal domains where CLIPScore performs well (clip-art images,\nalt-text rating), but also where it is relatively weaker vs reference-based\nmetrics, e.g., news captions that require richer contextual knowledge.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 05:00:29 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Hessel", "Jack", ""], ["Holtzman", "Ari", ""], ["Forbes", "Maxwell", ""], ["Bras", "Ronan Le", ""], ["Choi", "Yejin", ""]]}, {"id": "2104.08732", "submitter": "Burhan Rashid Hussein", "authors": "Burhan Rashid Hussein, Owais Ahmed Malik, Wee-Hong Ong, Johan Willem\n  Frederik Slik", "title": "Application of Computer Vision and Machine Learning for Digitized\n  Herbarium Specimens: A Systematic Literature Review", "comments": "42 pages, 9 figures, journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Herbarium contains treasures of millions of specimens which have been\npreserved for several years for scientific studies. To speed up more scientific\ndiscoveries, a digitization of these specimens is currently on going to\nfacilitate easy access and sharing of its data to a wider scientific community.\nOnline digital repositories such as IDigBio and GBIF have already accumulated\nmillions of specimen images yet to be explored. This presents a perfect time to\nautomate and speed up more novel discoveries using machine learning and\ncomputer vision. In this study, a thorough analysis and comparison of more than\n50 peer-reviewed studies which focus on application of computer vision and\nmachine learning techniques to digitized herbarium specimen have been examined.\nThe study categorizes different techniques and applications which have been\ncommonly used and it also highlights existing challenges together with their\npossible solutions. It is our hope that the outcome of this study will serve as\na strong foundation for beginners of the relevant field and will also shed more\nlight for both computer science and ecology experts.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 06:08:51 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Hussein", "Burhan Rashid", ""], ["Malik", "Owais Ahmed", ""], ["Ong", "Wee-Hong", ""], ["Slik", "Johan Willem Frederik", ""]]}, {"id": "2104.08736", "submitter": "Qi Qi", "authors": "Qi Qi, Youzhi Luo, Zhao Xu, Shuiwang Ji, Tianbao Yang", "title": "Stochastic Optimization of Areas Under Precision-Recall Curves with\n  Provable Convergence", "comments": "24 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Areas under ROC (AUROC) and precision-recall curves (AUPRC) are common\nmetrics for evaluating classification performance for imbalanced problems.\nCompared with AUROC, AUPRC is a more appropriate metric for highly imbalanced\ndatasets. While stochastic optimization of AUROC has been studied extensively,\nprincipled stochastic optimization of AUPRC has been rarely explored. In this\nwork, we propose a principled technical method to optimize AUPRC for deep\nlearning. Our approach is based on maximizing the averaged precision (AP),\nwhich is an unbiased point estimator of AUPRC. We cast the objective into a sum\nof {\\it dependent compositional functions} with inner functions dependent on\nrandom variables of the outer level. We propose efficient adaptive and\nnon-adaptive stochastic algorithms with {\\it provable convergence guarantee\nunder mild conditions} by leveraging recent advances in stochastic\ncompositional optimization. Extensive experimental results on image and graph\ndatasets demonstrate that our proposed method outperforms prior methods on\nimbalanced problems in terms of AUPRC. To the best of our knowledge, our work\nrepresents the first attempt to optimize AUPRC with provable convergence.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 06:22:21 GMT"}, {"version": "v2", "created": "Wed, 2 Jun 2021 05:06:40 GMT"}, {"version": "v3", "created": "Sun, 6 Jun 2021 15:20:25 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Qi", "Qi", ""], ["Luo", "Youzhi", ""], ["Xu", "Zhao", ""], ["Ji", "Shuiwang", ""], ["Yang", "Tianbao", ""]]}, {"id": "2104.08739", "submitter": "Shen Li", "authors": "Shen Li, Bingpeng Ma, Hong Chang, Shiguang Shan, Xilin Chen", "title": "Continuity-Discrimination Convolutional Neural Network for Visual Object\n  Tracking", "comments": "Accepted to ICME2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This paper proposes a novel model, named Continuity-Discrimination\nConvolutional Neural Network (CD-CNN), for visual object tracking. Existing\nstate-of-the-art tracking methods do not deal with temporal relationship in\nvideo sequences, which leads to imperfect feature representations. To address\nthis problem, CD-CNN models temporal appearance continuity based on the idea of\ntemporal slowness. Mathematically, we prove that, by introducing temporal\nappearance continuity into tracking, the upper bound of target appearance\nrepresentation error can be sufficiently small with high probability. Further,\nin order to alleviate inaccurate target localization and drifting, we propose a\nnovel notion, object-centroid, to characterize not only objectness but also the\nrelative position of the target within a given patch. Both temporal appearance\ncontinuity and object-centroid are jointly learned during offline training and\nthen transferred for online tracking. We evaluate our tracker through extensive\nexperiments on two challenging benchmarks and show its competitive tracking\nperformance compared with state-of-the-art trackers.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 06:35:03 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Li", "Shen", ""], ["Ma", "Bingpeng", ""], ["Chang", "Hong", ""], ["Shan", "Shiguang", ""], ["Chen", "Xilin", ""]]}, {"id": "2104.08760", "submitter": "Guangrun Wang", "authors": "Guangrun Wang, Keze Wang, Guangcong Wang, Philip H.S. Torr, Liang Lin", "title": "Towards Solving Inefficiency of Self-supervised Representation Learning", "comments": "12 pages, 5 figures, 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-supervised learning (especially contrastive learning) has attracted\ngreat interest due to its tremendous potentials in learning discriminative\nrepresentations in an unsupervised manner. Despite the acknowledged successes,\nexisting contrastive learning methods suffer from very low learning efficiency,\ne.g., taking about ten times more training epochs than supervised learning for\ncomparable recognition accuracy. In this paper, we discover two contradictory\nphenomena in contrastive learning that we call under-clustering and\nover-clustering problems, which are major obstacles to learning efficiency.\nUnder-clustering means that the model cannot efficiently learn to discover the\ndissimilarity between inter-class samples when the negative sample pairs for\ncontrastive learning are insufficient to differentiate all the actual object\ncategories. Over-clustering implies that the model cannot efficiently learn the\nfeature representation from excessive negative sample pairs, which enforces the\nmodel to over-cluster samples of the same actual categories into different\nclusters. To simultaneously overcome these two problems, we propose a novel\nself-supervised learning framework using a median triplet loss. Precisely, we\nemploy a triplet loss tending to maximize the relative distance between the\npositive pair and negative pairs to address the under-clustering problem; and\nwe construct the negative pair by selecting the negative sample of a median\nsimilarity score from all negative samples to avoid the over-clustering\nproblem, guaranteed by the Bernoulli Distribution model. We extensively\nevaluate our proposed framework in several large-scale benchmarks (e.g.,\nImageNet, SYSU-30k, and COCO). The results demonstrate the superior performance\n(e.g., the learning efficiency) of our model over the latest state-of-the-art\nmethods by a clear margin. Codes available at:\nhttps://github.com/wanggrun/triplet.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 07:47:10 GMT"}, {"version": "v2", "created": "Tue, 22 Jun 2021 10:30:49 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Wang", "Guangrun", ""], ["Wang", "Keze", ""], ["Wang", "Guangcong", ""], ["Torr", "Philip H. S.", ""], ["Lin", "Liang", ""]]}, {"id": "2104.08773", "submitter": "Swaroop Mishra", "authors": "Swaroop Mishra, Daniel Khashabi, Chitta Baral, Hannaneh Hajishirzi", "title": "Natural Instructions: Benchmarking Generalization to New Tasks from\n  Natural Language Instructions", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Can we enable NLP models to appropriately respond to instructional prompts\nand consequently generalize to new tasks? To study this question, we leverage\nthe existing NLP datasets and the instructions that were used to crowdsource\nthem to create NATURAL INSTRUCTIONS, a dataset of instructions and\ntask-specific input/output data. This dataset consists of 61 distinct language\ninstructions and about 600k task instances, and is used to evaluate existing\nstate-of-the-art language-models (LMs) in addressing new tasks by few-shot\nprompting of GPT3 and fine-tuning BART. Our analysis indicates that: (a) the\nexisting models indeed benefit from instructions and hence, show improved\ngeneralization to new tasks; (b) while models like GPT-3 generally benefit from\ninstructions, the extent of their gains varies across different fields of\ninstructions and also depends on the task being solved; (c) generalization to\nunseen tasks in NATURAL INSTRUCTIONS remains far from perfect for the\nstate-of-the-art, indicating significant room for more progress in this\ndirection.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 08:44:56 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Mishra", "Swaroop", ""], ["Khashabi", "Daniel", ""], ["Baral", "Chitta", ""], ["Hajishirzi", "Hannaneh", ""]]}, {"id": "2104.08777", "submitter": "Nidhi Gupta", "authors": "Nidhi Gupta, Wenju Liu", "title": "Line Segmentation from Unconstrained Handwritten Text Images using\n  Adaptive Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Line segmentation from handwritten text images is one of the challenging task\ndue to diversity and unknown variations as undefined spaces, styles,\norientations, stroke heights, overlapping, and alignments. Though abundant\nresearches, there is a need of improvement to achieve robustness and higher\nsegmentation rates. In the present work, an adaptive approach is used for the\nline segmentation from handwritten text images merging the alignment of\nconnected component coordinates and text height. The mathematical justification\nis provided for measuring the text height respective to the image size. The\nnovelty of the work lies in the text height calculation dynamically. The\nexperiments are tested on the dataset provided by the Chinese company for the\nproject. The proposed scheme is tested on two different type of datasets;\ndocument pages having base lines and plain pages. Dataset is highly complex and\nconsists of abundant and uncommon variations in handwriting patterns. The\nperformance of the proposed method is tested on our datasets as well as\nbenchmark datasets, namely IAM and ICDAR09 to achieve 98.01% detection rate on\naverage. The performance is examined on the above said datasets to observe\n91.99% and 96% detection rates, respectively.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 08:52:52 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Gupta", "Nidhi", ""], ["Liu", "Wenju", ""]]}, {"id": "2104.08783", "submitter": "Xin Sun", "authors": "Xin Sun, Changrui Chen, Xiaorui Wang, Junyu Dong, Huiyu Zhou, Sheng\n  Chen", "title": "Gaussian Dynamic Convolution for Efficient Single-Image Segmentation", "comments": null, "journal-ref": "IEEE Transactions on Circuits and Systems for Video Technology,\n  2021", "doi": "10.1109/TCSVT.2021.3096814", "report-no": "TCSVT-06243-2021", "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interactive single-image segmentation is ubiquitous in the scientific and\ncommercial imaging software. In this work, we focus on the single-image\nsegmentation problem only with some seeds such as scribbles. Inspired by the\ndynamic receptive field in the human being's visual system, we propose the\nGaussian dynamic convolution (GDC) to fast and efficiently aggregate the\ncontextual information for neural networks. The core idea is randomly selecting\nthe spatial sampling area according to the Gaussian distribution offsets. Our\nGDC can be easily used as a module to build lightweight or complex segmentation\nnetworks. We adopt the proposed GDC to address the typical single-image\nsegmentation tasks. Furthermore, we also build a Gaussian dynamic pyramid\nPooling to show its potential and generality in common semantic segmentation.\nExperiments demonstrate that the GDC outperforms other existing convolutions on\nthree benchmark segmentation datasets including Pascal-Context, Pascal-VOC\n2012, and Cityscapes. Additional experiments are also conducted to illustrate\nthat the GDC can produce richer and more vivid features compared with other\nconvolutions. In general, our GDC is conducive to the convolutional neural\nnetworks to form an overall impression of the image.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 09:20:55 GMT"}, {"version": "v2", "created": "Sun, 23 May 2021 11:28:04 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Sun", "Xin", ""], ["Chen", "Changrui", ""], ["Wang", "Xiaorui", ""], ["Dong", "Junyu", ""], ["Zhou", "Huiyu", ""], ["Chen", "Sheng", ""]]}, {"id": "2104.08789", "submitter": "Xavier Rafael-Palou", "authors": "Xavier Rafael-Palou, Anton Aubanell, Mario Ceresa, Vicent Ribas, Gemma\n  Piella, Miguel A. Gonz\\'alez Ballester", "title": "An Uncertainty-aware Hierarchical Probabilistic Network for Early\n  Prediction, Quantification and Segmentation of Pulmonary Tumour Growth", "comments": "24 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Early detection and quantification of tumour growth would help clinicians to\nprescribe more accurate treatments and provide better surgical planning.\nHowever, the multifactorial and heterogeneous nature of lung tumour progression\nhampers identification of growth patterns. In this study, we present a novel\nmethod based on a deep hierarchical generative and probabilistic framework\nthat, according to radiological guidelines, predicts tumour growth, quantifies\nits size and provides a semantic appearance of the future nodule. Unlike\nprevious deterministic solutions, the generative characteristic of our approach\nalso allows us to estimate the uncertainty in the predictions, especially\nimportant for complex and doubtful cases. Results of evaluating this method on\nan independent test set reported a tumour growth balanced accuracy of 74%, a\ntumour growth size MAE of 1.77 mm and a tumour segmentation Dice score of 78%.\nThese surpassed the performances of equivalent deterministic and alternative\ngenerative solutions (i.e. probabilistic U-Net, Bayesian test dropout and\nPix2Pix GAN) confirming the suitability of our approach.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 09:48:58 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Rafael-Palou", "Xavier", ""], ["Aubanell", "Anton", ""], ["Ceresa", "Mario", ""], ["Ribas", "Vicent", ""], ["Piella", "Gemma", ""], ["Ballester", "Miguel A. Gonz\u00e1lez", ""]]}, {"id": "2104.08797", "submitter": "Zengyi Qin", "authors": "Zengyi Qin, Jinglu Wang, Yan Lu", "title": "MonoGRNet: A General Framework for Monocular 3D Object Detection", "comments": "The IEEE Transactions on Pattern Analysis and Machine Intelligence\n  (TPAMI)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting and localizing objects in the real 3D space, which plays a crucial\nrole in scene understanding, is particularly challenging given only a monocular\nimage due to the geometric information loss during imagery projection. We\npropose MonoGRNet for the amodal 3D object detection from a monocular image via\ngeometric reasoning in both the observed 2D projection and the unobserved depth\ndimension. MonoGRNet decomposes the monocular 3D object detection task into\nfour sub-tasks including 2D object detection, instance-level depth estimation,\nprojected 3D center estimation and local corner regression. The task\ndecomposition significantly facilitates the monocular 3D object detection,\nallowing the target 3D bounding boxes to be efficiently predicted in a single\nforward pass, without using object proposals, post-processing or the\ncomputationally expensive pixel-level depth estimation utilized by previous\nmethods. In addition, MonoGRNet flexibly adapts to both fully and weakly\nsupervised learning, which improves the feasibility of our framework in diverse\nsettings. Experiments are conducted on KITTI, Cityscapes and MS COCO datasets.\nResults demonstrate the promising performance of our framework in various\nscenarios.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 10:07:52 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Qin", "Zengyi", ""], ["Wang", "Jinglu", ""], ["Lu", "Yan", ""]]}, {"id": "2104.08838", "submitter": "Yuanzhi Wang", "authors": "Yuanzhi Wang and Tao Lu and Yanduo Zhang and Yuntao Wu", "title": "Multi-scale Self-calibrated Network for Image Light Source Transfer", "comments": "8 pages,4 figures", "journal-ref": "CVPR 2021", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Image light source transfer (LLST), as the most challenging task in the\ndomain of image relighting, has attracted extensive attention in recent years.\nIn the latest research, LLST is decomposed three sub-tasks: scene reconversion,\nshadow estimation, and image re-rendering, which provides a new paradigm for\nimage relighting. However, many problems for scene reconversion and shadow\nestimation tasks, including uncalibrated feature information and poor semantic\ninformation, are still unresolved, thereby resulting in insufficient feature\nrepresentation. In this paper, we propose novel down-sampling feature\nself-calibrated block (DFSB) and up-sampling feature self-calibrated block\n(UFSB) as the basic blocks of feature encoder and decoder to calibrate feature\nrepresentation iteratively because the LLST is similar to the recalibration of\nimage light source. In addition, we fuse the multi-scale features of the\ndecoder in scene reconversion task to further explore and exploit more semantic\ninformation, thereby providing more accurate primary scene structure for image\nre-rendering. Experimental results in the VIDIT dataset show that the proposed\napproach significantly improves the performance for LLST.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 12:23:01 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Wang", "Yuanzhi", ""], ["Lu", "Tao", ""], ["Zhang", "Yanduo", ""], ["Wu", "Yuntao", ""]]}, {"id": "2104.08845", "submitter": "Kecheng Chen", "authors": "Kecheng Chen, Kun Long, Yazhou Ren, Jiayu Sun and Xiaorong Pu", "title": "Lesion-Inspired Denoising Network: Connecting Medical Image Denoising\n  and Lesion Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has achieved notable performance in the denoising task of\nlow-quality medical images and the detection task of lesions, respectively.\nHowever, existing low-quality medical image denoising approaches are\ndisconnected from the detection task of lesions. Intuitively, the quality of\ndenoised images will influence the lesion detection accuracy that in turn can\nbe used to affect the denoising performance. To this end, we propose a\nplay-and-plug medical image denoising framework, namely Lesion-Inspired\nDenoising Network (LIDnet), to collaboratively improve both denoising\nperformance and detection accuracy of denoised medical images. Specifically, we\npropose to insert the feedback of downstream detection task into existing\ndenoising framework by jointly learning a multi-loss objective. Instead of\nusing perceptual loss calculated on the entire feature map, a novel\nregion-of-interest (ROI) perceptual loss induced by the lesion detection task\nis proposed to further connect these two tasks. To achieve better optimization\nfor overall framework, we propose a customized collaborative training strategy\nfor LIDnet. On consideration of clinical usability and imaging characteristics,\nthree low-dose CT images datasets are used to evaluate the effectiveness of the\nproposed LIDnet. Experiments show that, by equipping with LIDnet, both of the\ndenoising and lesion detection performance of baseline methods can be\nsignificantly improved.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 12:53:36 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Chen", "Kecheng", ""], ["Long", "Kun", ""], ["Ren", "Yazhou", ""], ["Sun", "Jiayu", ""], ["Pu", "Xiaorong", ""]]}, {"id": "2104.08850", "submitter": "Kemal Tugrul Yesilbek", "authors": "Kemal Tugrul Yesilbek, T. Metin Sezgin", "title": "On Training Sketch Recognizers for New Domains", "comments": "Accepted for The 1st Workshop on Sketch-Oriented Deep Learning\n  (SketchDL) @ CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sketch recognition algorithms are engineered and evaluated using publicly\navailable datasets contributed by the sketch recognition community over the\nyears. While existing datasets contain sketches of a limited set of generic\nobjects, each new domain inevitably requires collecting new data for training\ndomain specific recognizers. This gives rise to two fundamental concerns:\nFirst, will the data collection protocol yield ecologically valid data? Second,\nwill the amount of collected data suffice to train sufficiently accurate\nclassifiers? In this paper, we draw attention to these two concerns. We show\nthat the ecological validity of the data collection protocol and the ability to\naccommodate small datasets are significant factors impacting recognizer\naccuracy in realistic scenarios. More specifically, using sketch-based gaming\nas a use case, we show that deep learning methods, as well as more traditional\nmethods, suffer significantly from dataset shift. Furthermore, we demonstrate\nthat in realistic scenarios where data is scarce and expensive, standard\nmeasures taken for adapting deep learners to small datasets fall short of\ncomparing favorably with alternatives. Although transfer learning, and\nextensive data augmentation help deep learners, they still perform\nsignificantly worse compared to standard setups (e.g., SVMs and GBMs with\nstandard feature representations). We pose learning from small datasets as a\nkey problem for the deep sketch recognition field, one which has been ignored\nin the bulk of the existing literature.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 13:24:49 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Yesilbek", "Kemal Tugrul", ""], ["Sezgin", "T. Metin", ""]]}, {"id": "2104.08852", "submitter": "Xiaoyu Li", "authors": "Xiaoyu Li, Bo Zhang, Jing Liao, Pedro V. Sander", "title": "Let's See Clearly: Contaminant Artifact Removal for Moving Cameras", "comments": "10 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contaminants such as dust, dirt and moisture adhering to the camera lens can\ngreatly affect the quality and clarity of the resulting image or video. In this\npaper, we propose a video restoration method to automatically remove these\ncontaminants and produce a clean video. Our approach first seeks to detect\nattention maps that indicate the regions that need to be restored. In order to\nleverage the corresponding clean pixels from adjacent frames, we propose a flow\ncompletion module to hallucinate the flow of the background scene to the\nattention regions degraded by the contaminants. Guided by the attention maps\nand completed flows, we propose a recurrent technique to restore the input\nframe by fetching clean pixels from adjacent frames. Finally, a multi-frame\nprocessing stage is used to further process the entire video sequence in order\nto enforce temporal consistency. The entire network is trained on a synthetic\ndataset that approximates the physical lighting properties of contaminant\nartifacts. This new dataset and our novel framework lead to our method that is\nable to address different contaminants and outperforms competitive restoration\napproaches both qualitatively and quantitatively.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 13:37:34 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Li", "Xiaoyu", ""], ["Zhang", "Bo", ""], ["Liao", "Jing", ""], ["Sander", "Pedro V.", ""]]}, {"id": "2104.08854", "submitter": "Jia Wang", "authors": "Jia Wang, Ping Wang, Biao Li, Ruigang Fu, Junzheng Wu", "title": "An Improved Discriminative Optimization for 3D Rigid Point Cloud\n  Registration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Discriminative Optimization (DO) algorithm has been proved much\nsuccessful in 3D point cloud registration. In the original DO, the feature\n(descriptor) of two point cloud was defined as a histogram, and the element of\nhistogram indicates the weights of scene points in \"front\" or \"back\" side of a\nmodel point. In this paper, we extended the histogram which indicate the sides\nfrom \"front-back\" to \"front-back\", \"up-down\", and \"clockwise-anticlockwise\". In\naddition, we reweighted the extended histogram according to the model points'\ndistribution. We evaluated the proposed Improved DO on the Stanford Bunny and\nOxford SensatUrban dataset, and compared it with six classical State-Of-The-Art\npoint cloud registration algorithms. The experimental result demonstrates our\nalgorithm achieves comparable performance in point registration accuracy and\nroot-mean-sqart-error.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 13:39:52 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Wang", "Jia", ""], ["Wang", "Ping", ""], ["Li", "Biao", ""], ["Fu", "Ruigang", ""], ["Wu", "Junzheng", ""]]}, {"id": "2104.08859", "submitter": "Fagner Cunha", "authors": "Fagner Cunha, Eulanda M. dos Santos, Raimundo Barreto, Juan G. Colonna", "title": "Filtering Empty Camera Trap Images in Embedded Systems", "comments": "Accepted to CVPR 2021 (Mobile AI workshop and challenges)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monitoring wildlife through camera traps produces a massive amount of images,\nwhose a significant portion does not contain animals, being later discarded.\nEmbedding deep learning models to identify animals and filter these images\ndirectly in those devices brings advantages such as savings in the storage and\ntransmission of data, usually resource-constrained in this type of equipment.\nIn this work, we present a comparative study on animal recognition models to\nanalyze the trade-off between precision and inference latency on edge devices.\nTo accomplish this objective, we investigate classifiers and object detectors\nof various input resolutions and optimize them using quantization and reducing\nthe number of model filters. The confidence threshold of each model was\nadjusted to obtain 96% recall for the nonempty class, since instances from the\nempty class are expected to be discarded. The experiments show that, when using\nthe same set of images for training, detectors achieve superior performance,\neliminating at least 10% more empty images than classifiers with comparable\nlatencies. Considering the high cost of generating labels for the detection\nproblem, when there is a massive number of images labeled for classification\n(about one million instances, ten times more than those available for\ndetection), classifiers are able to reach results comparable to detectors but\nwith half latency.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 13:56:22 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Cunha", "Fagner", ""], ["Santos", "Eulanda M. dos", ""], ["Barreto", "Raimundo", ""], ["Colonna", "Juan G.", ""]]}, {"id": "2104.08860", "submitter": "Huaishao Luo", "authors": "Huaishao Luo, Lei Ji, Ming Zhong, Yang Chen, Wen Lei, Nan Duan,\n  Tianrui Li", "title": "CLIP4Clip: An Empirical Study of CLIP for End to End Video Clip\n  Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Video-text retrieval plays an essential role in multi-modal research and has\nbeen widely used in many real-world web applications. The CLIP (Contrastive\nLanguage-Image Pre-training), an image-language pre-training model, has\ndemonstrated the power of visual concepts learning from web collected\nimage-text datasets. In this paper, we propose a CLIP4Clip model to transfer\nthe knowledge of the CLIP model to video-language retrieval in an end-to-end\nmanner. Several questions are investigated via empirical studies: 1) Whether\nimage feature is enough for video-text retrieval? 2) How a post-pretraining on\na large-scale video-text dataset based on the CLIP affect the performance? 3)\nWhat is the practical mechanism to model temporal dependency between video\nframes? And 4) The Hyper-parameters sensitivity of the model on video-text\nretrieval task. Extensive experimental results present that the CLIP4Clip model\ntransferred from the CLIP can achieve SOTA results on various video-text\nretrieval datasets, including MSR-VTT, MSVC, LSMDC, ActivityNet, and DiDeMo. We\nrelease our code at https://github.com/ArrowLuo/CLIP4Clip.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 13:59:50 GMT"}, {"version": "v2", "created": "Sat, 8 May 2021 08:25:57 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Luo", "Huaishao", ""], ["Ji", "Lei", ""], ["Zhong", "Ming", ""], ["Chen", "Yang", ""], ["Lei", "Wen", ""], ["Duan", "Nan", ""], ["Li", "Tianrui", ""]]}, {"id": "2104.08862", "submitter": "Hengli Wang", "authors": "Hengli Wang, Peide Cai, Rui Fan, Yuxiang Sun, Ming Liu", "title": "End-to-End Interactive Prediction and Planning with Optical Flow\n  Distillation for Autonomous Driving", "comments": "10 pages, 5 figures and 4 tables. This paper is accepted by CVPRW\n  2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the recent advancement of deep learning technology, data-driven\napproaches for autonomous car prediction and planning have achieved\nextraordinary performance. Nevertheless, most of these approaches follow a\nnon-interactive prediction and planning paradigm, hypothesizing that a\nvehicle's behaviors do not affect others. The approaches based on such a\nnon-interactive philosophy typically perform acceptably in sparse traffic\nscenarios but can easily fail in dense traffic scenarios. Therefore, we propose\nan end-to-end interactive neural motion planner (INMP) for autonomous driving\nin this paper. Given a set of past surrounding-view images and a high\ndefinition map, our INMP first generates a feature map in bird's-eye-view\nspace, which is then processed to detect other agents and perform interactive\nprediction and planning jointly. Also, we adopt an optical flow distillation\nparadigm, which can effectively improve the network performance while still\nmaintaining its real-time inference speed. Extensive experiments on the\nnuScenes dataset and in the closed-loop Carla simulation environment\ndemonstrate the effectiveness and efficiency of our INMP for the detection,\nprediction, and planning tasks. Our project page is at\nsites.google.com/view/inmp-ofd.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 14:05:18 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Wang", "Hengli", ""], ["Cai", "Peide", ""], ["Fan", "Rui", ""], ["Sun", "Yuxiang", ""], ["Liu", "Ming", ""]]}, {"id": "2104.08885", "submitter": "Patrick Zschech", "authors": "Christoph Sager, Christian Janiesch, Patrick Zschech", "title": "A survey of image labelling for computer vision applications", "comments": "Published online first in Journal of Business Analytics", "journal-ref": null, "doi": "10.1080/2573234X.2021.1908861", "report-no": null, "categories": "cs.CV cs.HC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Supervised machine learning methods for image analysis require large amounts\nof labelled training data to solve computer vision problems. The recent rise of\ndeep learning algorithms for recognising image content has led to the emergence\nof many ad-hoc labelling tools. With this survey, we capture and systematise\nthe commonalities as well as the distinctions between existing image labelling\nsoftware. We perform a structured literature review to compile the underlying\nconcepts and features of image labelling software such as annotation\nexpressiveness and degree of automation. We structure the manual labelling task\nby its organisation of work, user interface design options, and user support\ntechniques to derive a systematisation schema for this survey. Applying it to\navailable software and the body of literature, enabled us to uncover several\napplication archetypes and key domains such as image retrieval or instance\nidentification in healthcare or television.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 16:01:55 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Sager", "Christoph", ""], ["Janiesch", "Christian", ""], ["Zschech", "Patrick", ""]]}, {"id": "2104.08886", "submitter": "Szymon P{\\l}otka", "authors": "Szymon P{\\l}otka, Tomasz W{\\l}odarczyk, Ryszard Szczerba,\n  Przemys{\\l}aw Rokita, Patrycja Bartkowska, Oskar Komisarek, Artur\n  Matthews-Brzozowski, Tomasz Trzci\\'nski", "title": "Convolutional Neural Networks in Orthodontics: a review", "comments": "Preprint to Elsevier", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Convolutional neural networks (CNNs) are used in many areas of computer\nvision, such as object tracking and recognition, security, military, and\nbiomedical image analysis. This review presents the application of\nconvolutional neural networks in one of the fields of dentistry - orthodontics.\nAdvances in medical imaging technologies and methods allow CNNs to be used in\northodontics to shorten the planning time of orthodontic treatment, including\nan automatic search of landmarks on cephalometric X-ray images, tooth\nsegmentation on Cone-Beam Computed Tomography (CBCT) images or digital models,\nand classification of defects on X-Ray panoramic images. In this work, we\ndescribe the current methods, the architectures of deep convolutional neural\nnetworks used, and their implementations, together with a comparison of the\nresults achieved by them. The promising results and visualizations of the\ndescribed studies show that the use of methods based on convolutional neural\nnetworks allows for the improvement of computer-based orthodontic treatment\nplanning, both by reducing the examination time and, in many cases, by\nperforming the analysis much more accurately than a manual orthodontist does.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 16:02:30 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["P\u0142otka", "Szymon", ""], ["W\u0142odarczyk", "Tomasz", ""], ["Szczerba", "Ryszard", ""], ["Rokita", "Przemys\u0142aw", ""], ["Bartkowska", "Patrycja", ""], ["Komisarek", "Oskar", ""], ["Matthews-Brzozowski", "Artur", ""], ["Trzci\u0144ski", "Tomasz", ""]]}, {"id": "2104.08894", "submitter": "Ahmed Abdelkader", "authors": "Phillip Pope, Chen Zhu, Ahmed Abdelkader, Micah Goldblum, Tom\n  Goldstein", "title": "The Intrinsic Dimension of Images and Its Impact on Learning", "comments": "To appear at ICLR 2021 (spotlight), 17 pages with appendix, 15\n  figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  It is widely believed that natural image data exhibits low-dimensional\nstructure despite the high dimensionality of conventional pixel\nrepresentations. This idea underlies a common intuition for the remarkable\nsuccess of deep learning in computer vision. In this work, we apply dimension\nestimation tools to popular datasets and investigate the role of\nlow-dimensional structure in deep learning. We find that common natural image\ndatasets indeed have very low intrinsic dimension relative to the high number\nof pixels in the images. Additionally, we find that low dimensional datasets\nare easier for neural networks to learn, and models solving these tasks\ngeneralize better from training to test data. Along the way, we develop a\ntechnique for validating our dimension estimation tools on synthetic data\ngenerated by GANs allowing us to actively manipulate the intrinsic dimension by\ncontrolling the image generation process. Code for our experiments may be found\nhere https://github.com/ppope/dimensions.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 16:29:23 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Pope", "Phillip", ""], ["Zhu", "Chen", ""], ["Abdelkader", "Ahmed", ""], ["Goldblum", "Micah", ""], ["Goldstein", "Tom", ""]]}, {"id": "2104.08899", "submitter": "Decky Aspandi", "authors": "Decky Aspandi-Latif, Sally Goldin, Preesan Rakwatin, Kurt Rudahl", "title": "Texture Based Classification of High Resolution Remotely Sensed Imagery\n  using Weber Local Descriptor", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Traditional image classification techniques often produce unsatisfactory\nresults when applied to high spatial resolution data because classes in high\nresolution images are not spectrally homogeneous. Texture offers an alternative\nsource of information for classifying these images. This paper evaluates a\nrecently developed, computationally simple texture metric called Weber Local\nDescriptor (WLD) for use in classifying high resolution QuickBird panchromatic\ndata. We compared WLD with state-of-the art texture descriptors (TD) including\nLocal Binary Pattern (LBP) and its rotation-invariant version LBPRIU. We also\ninvestigated whether incorporating VAR, a TD that captures brightness\nvariation, would improve the accuracy of LBPRIU and WLD. We found that WLD\ngenerally produces more accurate classification results than the other TD we\nexamined, and is also more robust to varying parameters. We have implemented an\noptimised algorithm for calculating WLD which makes the technique practical in\nterms of computation time. Overall, our results indicate that WLD is a\npromising approach for classifying high resolution remote sensing data.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 16:37:34 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Aspandi-Latif", "Decky", ""], ["Goldin", "Sally", ""], ["Rakwatin", "Preesan", ""], ["Rudahl", "Kurt", ""]]}, {"id": "2104.08902", "submitter": "Huan Liu", "authors": "Yankun Yu, Huan Liu, Minghan Fu, Jun Chen, Xiyao Wang, Keyan Wang", "title": "A Two-branch Neural Network for Non-homogeneous Dehazing via Ensemble\n  Learning", "comments": "Accepted in CVPRW 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, there has been rapid and significant progress on image dehazing.\nMany deep learning based methods have shown their superb performance in\nhandling homogeneous dehazing problems. However, we observe that even if a\ncarefully designed convolutional neural network (CNN) can perform well on\nlarge-scaled dehazing benchmarks, the network usually fails on the\nnon-homogeneous dehazing datasets introduced by NTIRE challenges. The reasons\nare mainly in two folds. Firstly, due to its non-homogeneous nature, the\nnon-uniformly distributed haze is harder to be removed than the homogeneous\nhaze. Secondly, the research challenge only provides limited data (there are\nonly 25 training pairs in NH-Haze 2021 dataset). Thus, learning the mapping\nfrom the domain of hazy images to that of clear ones based on very limited data\nis extremely hard. To this end, we propose a simple but effective approach for\nnon-homogeneous dehazing via ensemble learning. To be specific, we introduce a\ntwo-branch neural network to separately deal with the aforementioned problems\nand then map their distinct features by a learnable fusion tail. We show\nextensive experimental results to illustrate the effectiveness of our proposed\nmethod.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 16:39:13 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Yu", "Yankun", ""], ["Liu", "Huan", ""], ["Fu", "Minghan", ""], ["Chen", "Jun", ""], ["Wang", "Xiyao", ""], ["Wang", "Keyan", ""]]}, {"id": "2104.08910", "submitter": "Weihao Xia", "authors": "Weihao Xia, Yujiu Yang, Jing-Hao Xue, Baoyuan Wu", "title": "Towards Open-World Text-Guided Face Image Generation and Manipulation", "comments": "arXiv admin note: substantial text overlap with arXiv:2012.03308", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The existing text-guided image synthesis methods can only produce limited\nquality results with at most \\mbox{$\\text{256}^2$} resolution and the textual\ninstructions are constrained in a small Corpus. In this work, we propose a\nunified framework for both face image generation and manipulation that produces\ndiverse and high-quality images with an unprecedented resolution at 1024 from\nmultimodal inputs. More importantly, our method supports open-world scenarios,\nincluding both image and text, without any re-training, fine-tuning, or\npost-processing. To be specific, we propose a brand new paradigm of text-guided\nimage generation and manipulation based on the superior characteristics of a\npretrained GAN model. Our proposed paradigm includes two novel strategies. The\nfirst strategy is to train a text encoder to obtain latent codes that align\nwith the hierarchically semantic of the aforementioned pretrained GAN model.\nThe second strategy is to directly optimize the latent codes in the latent\nspace of the pretrained GAN model with guidance from a pretrained language\nmodel. The latent codes can be randomly sampled from a prior distribution or\ninverted from a given image, which provides inherent supports for both image\ngeneration and manipulation from multi-modal inputs, such as sketches or\nsemantic labels, with textual guidance. To facilitate text-guided multi-modal\nsynthesis, we propose the Multi-Modal CelebA-HQ, a large-scale dataset\nconsisting of real face images and corresponding semantic segmentation map,\nsketch, and textual descriptions. Extensive experiments on the introduced\ndataset demonstrate the superior performance of our proposed method. Code and\ndata are available at https://github.com/weihaox/TediGAN.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 16:56:07 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Xia", "Weihao", ""], ["Yang", "Yujiu", ""], ["Xue", "Jing-Hao", ""], ["Wu", "Baoyuan", ""]]}, {"id": "2104.08918", "submitter": "Julian True", "authors": "Julian True and Naimul Khan", "title": "Motion Vector Extrapolation for Video Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Despite the continued successes of computationally efficient deep neural\nnetwork architectures for video object detection, performance continually\narrives at the great trilemma of speed versus accuracy versus computational\nresources (pick two). Current attempts to exploit temporal information in video\ndata to overcome this trilemma are bottlenecked by the state-of-the-art in\nobject detection models. We present, a technique which performs video object\ndetection through the use of off-the-shelf object detectors alongside existing\noptical flow based motion estimation techniques in parallel. Through a set of\nexperiments on the benchmark MOT20 dataset, we demonstrate that our approach\nsignificantly reduces the baseline latency of any given object detector without\nsacrificing any accuracy. Further latency reduction, up to 25x lower than the\noriginal latency, can be achieved with minimal accuracy loss. MOVEX enables low\nlatency video object detection on common CPU based systems, thus allowing for\nhigh performance video object detection beyond the domain of GPU computing. The\ncode is available at https://github.com/juliantrue/movex.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 17:26:37 GMT"}, {"version": "v2", "created": "Sun, 13 Jun 2021 16:49:54 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["True", "Julian", ""], ["Khan", "Naimul", ""]]}, {"id": "2104.08945", "submitter": "Ruizhe Cheng", "authors": "Ruizhe Cheng, Bichen Wu, Peizhao Zhang, Peter Vajda, Joseph E.\n  Gonzalez", "title": "Data-Efficient Language-Supervised Zero-Shot Learning with\n  Self-Distillation", "comments": "4 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Traditional computer vision models are trained to predict a fixed set of\npredefined categories. Recently, natural language has been shown to be a\nbroader and richer source of supervision that provides finer descriptions to\nvisual concepts than supervised \"gold\" labels. Previous works, such as CLIP,\nuse a simple pretraining task of predicting the pairings between images and\ntext captions. CLIP, however, is data hungry and requires more than 400M image\ntext pairs for training. We propose a data-efficient contrastive distillation\nmethod that uses soft labels to learn from noisy image-text pairs. Our model\ntransfers knowledge from pretrained image and sentence encoders and achieves\nstrong performance with only 3M image text pairs, 133x smaller than CLIP. Our\nmethod exceeds the previous SoTA of general zero-shot learning on ImageNet\n21k+1k by 73% relatively with a ResNet50 image encoder and DeCLUTR text\nencoder. We also beat CLIP by 10.5% relatively on zero-shot evaluation on\nGoogle Open Images (19,958 classes).\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 19:55:31 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Cheng", "Ruizhe", ""], ["Wu", "Bichen", ""], ["Zhang", "Peizhao", ""], ["Vajda", "Peter", ""], ["Gonzalez", "Joseph E.", ""]]}, {"id": "2104.08954", "submitter": "Roohallah Alizadehsani", "authors": "Fahime Khozeimeh, Danial Sharifrazi, Navid Hoseini Izadi, Javad\n  Hassannataj Joloudari, Afshin Shoeibi, Roohallah Alizadehsani, Juan M.\n  Gorriz, Sadiq Hussain, Zahra Alizadeh Sani, Hossein Moosaei, Abbas Khosravi,\n  Saeid Nahavandi, Sheikh Mohammed Shariful Islam", "title": "CNN AE: Convolution Neural Network combined with Autoencoder approach to\n  detect survival chance of COVID 19 patients", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel method named CNN-AE to predict survival\nchance of COVID-19 patients using a CNN trained on clinical information. To\nfurther increase the prediction accuracy, we use the CNN in combination with an\nautoencoder. Our method is one of the first that aims to predict survival\nchance of already infected patients. We rely on clinical data to carry out the\nprediction. The motivation is that the required resources to prepare CT images\nare expensive and limited compared to the resources required to collect\nclinical data such as blood pressure, liver disease, etc. We evaluate our\nmethod on a publicly available clinical dataset of deceased and recovered\npatients which we have collected. Careful analysis of the dataset properties is\nalso presented which consists of important features extraction and correlation\ncomputation between features. Since most of COVID-19 patients are usually\nrecovered, the number of deceased samples of our dataset is low leading to data\nimbalance. To remedy this issue, a data augmentation procedure based on\nautoencoders is proposed. To demonstrate the generality of our augmentation\nmethod, we train random forest and Na\\\"ive Bayes on our dataset with and\nwithout augmentation and compare their performance. We also evaluate our method\non another dataset for further generality verification. Experimental results\nreveal the superiority of CNN-AE method compared to the standard CNN as well as\nother methods such as random forest and Na\\\"ive Bayes. COVID-19 detection\naverage accuracy of CNN-AE is 96.05% which is higher than CNN average accuracy\nof 92.49%. To show that clinical data can be used as a reliable dataset for\nCOVID-19 survival chance prediction, CNN-AE is compared with a standard CNN\nwhich is trained on CT images.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 20:31:17 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Khozeimeh", "Fahime", ""], ["Sharifrazi", "Danial", ""], ["Izadi", "Navid Hoseini", ""], ["Joloudari", "Javad Hassannataj", ""], ["Shoeibi", "Afshin", ""], ["Alizadehsani", "Roohallah", ""], ["Gorriz", "Juan M.", ""], ["Hussain", "Sadiq", ""], ["Sani", "Zahra Alizadeh", ""], ["Moosaei", "Hossein", ""], ["Khosravi", "Abbas", ""], ["Nahavandi", "Saeid", ""], ["Islam", "Sheikh Mohammed Shariful", ""]]}, {"id": "2104.08984", "submitter": "Aritra Ghosh", "authors": "Aritra Ghosh and Andrew Lan", "title": "Contrastive Learning Improves Model Robustness Under Label Noise", "comments": "Learning from Limited or Imperfect Data (L^2ID) Workshop @ CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural network-based classifiers trained with the categorical\ncross-entropy (CCE) loss are sensitive to label noise in the training data. One\ncommon type of method that can mitigate the impact of label noise can be viewed\nas supervised robust methods; one can simply replace the CCE loss with a loss\nthat is robust to label noise, or re-weight training samples and down-weight\nthose with higher loss values. Recently, another type of method using\nsemi-supervised learning (SSL) has been proposed, which augments these\nsupervised robust methods to exploit (possibly) noisy samples more effectively.\nAlthough supervised robust methods perform well across different data types,\nthey have been shown to be inferior to the SSL methods on image classification\ntasks under label noise. Therefore, it remains to be seen that whether these\nsupervised robust methods can also perform well if they can utilize the\nunlabeled samples more effectively. In this paper, we show that by initializing\nsupervised robust methods using representations learned through contrastive\nlearning leads to significantly improved performance under label noise.\nSurprisingly, even the simplest method (training a classifier with the CCE\nloss) can outperform the state-of-the-art SSL method by more than 50\\% under\nhigh label noise when initialized with contrastive learning. Our implementation\nwill be publicly available at\n{\\url{https://github.com/arghosh/noisy_label_pretrain}}.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 00:27:58 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Ghosh", "Aritra", ""], ["Lan", "Andrew", ""]]}, {"id": "2104.08997", "submitter": "Bishwas Mandal", "authors": "Bishwas Mandal, Adaeze Okeukwu, Yihong Theis", "title": "Masked Face Recognition using ResNet-50", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Over the last twenty years, there have seen several outbreaks of different\ncoronavirus diseases across the world. These outbreaks often led to respiratory\ntract diseases and have proved to be fatal sometimes. Currently, we are facing\nan elusive health crisis with the emergence of COVID-19 disease of the\ncoronavirus family. One of the modes of transmission of COVID- 19 is airborne\ntransmission. This transmission occurs as humans breathe in the droplets\nreleased by an infected person through breathing, speaking, singing, coughing,\nor sneezing. Hence, public health officials have mandated the use of face masks\nwhich can reduce disease transmission by 65%. For face recognition programs,\ncommonly used for security verification purposes, the use of face mask presents\nan arduous challenge since these programs were typically trained with human\nfaces devoid of masks but now due to the onset of Covid-19 pandemic, they are\nforced to identify faces with masks. Hence, this paper investigates the same\nproblem by developing a deep learning based model capable of accurately\nidentifying people with face-masks. In this paper, the authors train a\nResNet-50 based architecture that performs well at recognizing masked faces.\nThe outcome of this study could be seamlessly integrated into existing face\nrecognition programs that are designed to detect faces for security\nverification purposes.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 01:09:47 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Mandal", "Bishwas", ""], ["Okeukwu", "Adaeze", ""], ["Theis", "Yihong", ""]]}, {"id": "2104.09008", "submitter": "Hu Wang", "authors": "Hu Wang, Congbo Ma, Chunhua Shen", "title": "Kernel Agnostic Real-world Image Super-resolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, deep neural network models have achieved impressive results in\nvarious research fields. Come with it, an increasing number of attentions have\nbeen attracted by deep super-resolution (SR) approaches. Many existing methods\nattempt to restore high-resolution images from directly down-sampled\nlow-resolution images or with the assumption of Gaussian degradation kernels\nwith additive noises for their simplicities. However, in real-world scenarios,\nhighly complex kernels and non-additive noises may be involved, even though the\ndistorted images are visually similar to the clear ones. Existing SR models are\nfacing difficulties to deal with real-world images under such circumstances. In\nthis paper, we introduce a new kernel agnostic SR framework to deal with\nreal-world image SR problem. The framework can be hanged seamlessly to multiple\nmainstream models. In the proposed framework, the degradation kernels and\nnoises are adaptively modeled rather than explicitly specified. Moreover, we\nalso propose an iterative supervision process and frequency-attended objective\nfrom orthogonal perspectives to further boost the performance. The experiments\nvalidate the effectiveness of the proposed framework on multiple real-world\ndatasets.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 01:51:21 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Wang", "Hu", ""], ["Ma", "Congbo", ""], ["Shen", "Chunhua", ""]]}, {"id": "2104.09021", "submitter": "Uehwan Kim", "authors": "Ue-Hwan Kim, Yewon Hwang, Sun-Kyung Lee, Jong-Hwan Kim", "title": "Writing in The Air: Unconstrained Text Recognition from Finger Movement\n  Using Spatio-Temporal Convolution", "comments": "10 pages, 6 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a new benchmark dataset for the challenging\nwriting in the air (WiTA) task -- an elaborate task bridging vision and NLP.\nWiTA implements an intuitive and natural writing method with finger movement\nfor human-computer interaction (HCI). Our WiTA dataset will facilitate the\ndevelopment of data-driven WiTA systems which thus far have displayed\nunsatisfactory performance -- due to lack of dataset as well as traditional\nstatistical models they have adopted. Our dataset consists of five sub-datasets\nin two languages (Korean and English) and amounts to 209,926 video instances\nfrom 122 participants. We capture finger movement for WiTA with RGB cameras to\nensure wide accessibility and cost-efficiency. Next, we propose spatio-temporal\nresidual network architectures inspired by 3D ResNet. These models perform\nunconstrained text recognition from finger movement, guarantee a real-time\noperation by processing 435 and 697 decoding frames-per-second for Korean and\nEnglish, respectively, and will serve as an evaluation standard. Our dataset\nand the source codes are available at https://github.com/Uehwan/WiTA.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 02:37:46 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Kim", "Ue-Hwan", ""], ["Hwang", "Yewon", ""], ["Lee", "Sun-Kyung", ""], ["Kim", "Jong-Hwan", ""]]}, {"id": "2104.09023", "submitter": "Hao Pan", "authors": "Lei Chu, Hao Pan, Wenping Wang", "title": "Unsupervised Shape Completion via Deep Prior in the Neural Tangent\n  Kernel Perspective", "comments": "This is the author's preprint; see the final publication at ACM TOG", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel approach for completing and reconstructing 3D shapes from\nincomplete scanned data by using deep neural networks. Rather than being\ntrained on supervised completion tasks and applied on a testing shape, the\nnetwork is optimized from scratch on the single testing shape, to fully adapt\nto the shape and complete the missing data using contextual guidance from the\nknown regions. The ability to complete missing data by an untrained neural\nnetwork is usually referred to as the deep prior. In this paper, we interpret\nthe deep prior from a neural tangent kernel (NTK) perspective and show that the\ncompleted shape patches by the trained CNN are naturally similar to existing\npatches, as they are proximate in the kernel feature space induced by NTK. The\ninterpretation allows us to design more efficient network structures and\nlearning mechanisms for the shape completion and reconstruction task. Being\nmore aware of structural regularities than both traditional and other\nunsupervised learning-based reconstruction methods, our approach completes\nlarge missing regions with plausible shapes and complements supervised\nlearning-based methods that use database priors by requiring no extra training\ndata set and showing flexible adaptation to a particular shape instance.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 02:41:15 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Chu", "Lei", ""], ["Pan", "Hao", ""], ["Wang", "Wenping", ""]]}, {"id": "2104.09035", "submitter": "Liang Peng", "authors": "Liang Peng, Fei Liu, Zhengxu Yu, Senbo Yan, Dan Deng, Deng Cai", "title": "Lidar Point Cloud Guided Monocular 3D Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monocular 3D object detection is drawing increasing attention from the\ncommunity as it enables cars to perceive the world in 3D with a single camera.\nHowever, monocular 3D detection currently struggles with extremely lower\ndetection rates compared to LiDAR-based methods, limiting its applications. The\npoor accuracy is mainly caused by the absence of accurate depth cues due to the\nill-posed nature of monocular imagery. LiDAR point clouds, which provide\naccurate depth measurement, can offer beneficial information for the training\nof monocular methods. Prior works only use LiDAR point clouds to train a depth\nestimator. This implicit way does not fully utilize LiDAR point clouds,\nconsequently leading to suboptimal performances. To effectively take advantage\nof LiDAR point clouds, in this paper we propose a general, simple yet effective\nframework for monocular methods. Specifically, we use LiDAR point clouds to\ndirectly guide the training of monocular 3D detectors, allowing them to learn\ndesired objectives meanwhile eliminating the extra annotation cost. Thanks to\nthe general design, our method can be plugged into any monocular 3D detection\nmethod, significantly boosting the performance. In conclusion, we take the\nfirst place on KITTI monocular 3D detection benchmark and increase the BEV/3D\nAP from 11.88/8.65 to 22.06/16.80 on the hard setting for the prior\nstate-of-the-art method. The code will be made publicly available soon.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 03:41:09 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Peng", "Liang", ""], ["Liu", "Fei", ""], ["Yu", "Zhengxu", ""], ["Yan", "Senbo", ""], ["Deng", "Dan", ""], ["Cai", "Deng", ""]]}, {"id": "2104.09044", "submitter": "Pengguang Chen", "authors": "Pengguang Chen, Shu Liu, Hengshuang Zhao, Jiaya Jia", "title": "Distilling Knowledge via Knowledge Review", "comments": "CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge distillation transfers knowledge from the teacher network to the\nstudent one, with the goal of greatly improving the performance of the student\nnetwork. Previous methods mostly focus on proposing feature transformation and\nloss functions between the same level's features to improve the effectiveness.\nWe differently study the factor of connection path cross levels between teacher\nand student networks, and reveal its great importance. For the first time in\nknowledge distillation, cross-stage connection paths are proposed. Our new\nreview mechanism is effective and structurally simple. Our finally designed\nnested and compact framework requires negligible computation overhead, and\noutperforms other methods on a variety of tasks. We apply our method to\nclassification, object detection, and instance segmentation tasks. All of them\nwitness significant student network performance improvement. Code is available\nat https://github.com/Jia-Research-Lab/ReviewKD\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 04:36:24 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Chen", "Pengguang", ""], ["Liu", "Shu", ""], ["Zhao", "Hengshuang", ""], ["Jia", "Jiaya", ""]]}, {"id": "2104.09045", "submitter": "Aritra Ghosh", "authors": "Aritra Ghosh, Andrew Lan", "title": "Do We Really Need Gold Samples for Sample Weighting Under Label Noise?", "comments": "WACV 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning with labels noise has gained significant traction recently due to\nthe sensitivity of deep neural networks under label noise under common loss\nfunctions. Losses that are theoretically robust to label noise, however, often\nmakes training difficult. Consequently, several recently proposed methods, such\nas Meta-Weight-Net (MW-Net), use a small number of unbiased, clean samples to\nlearn a weighting function that downweights samples that are likely to have\ncorrupted labels under the meta-learning framework. However, obtaining such a\nset of clean samples is not always feasible in practice. In this paper, we\nanalytically show that one can easily train MW-Net without access to clean\nsamples simply by using a loss function that is robust to label noise, such as\nmean absolute error, as the meta objective to train the weighting network. We\nexperimentally show that our method beats all existing methods that do not use\nclean samples and performs on-par with methods that use gold samples on\nbenchmark datasets across various noise types and noise rates.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 04:36:51 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Ghosh", "Aritra", ""], ["Lan", "Andrew", ""]]}, {"id": "2104.09048", "submitter": "Joon Young Ahn", "authors": "Joon Young Ahn and Nam Ik Cho", "title": "Neural Architecture Search for Image Super-Resolution Using Densely\n  Constructed Search Space: DeCoNAS", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The recent progress of deep convolutional neural networks has enabled great\nsuccess in single image super-resolution (SISR) and many other vision tasks.\nTheir performances are also being increased by deepening the networks and\ndeveloping more sophisticated network structures. However, finding an optimal\nstructure for the given problem is a difficult task, even for human experts.\nFor this reason, neural architecture search (NAS) methods have been introduced,\nwhich automate the procedure of constructing the structures. In this paper, we\nexpand the NAS to the super-resolution domain and find a lightweight densely\nconnected network named DeCoNASNet. We use a hierarchical search strategy to\nfind the best connection with local and global features. In this process, we\ndefine a complexity-based penalty for solving image super-resolution, which can\nbe considered a multi-objective problem. Experiments show that our DeCoNASNet\noutperforms the state-of-the-art lightweight super-resolution networks designed\nby handcraft methods and existing NAS-based design.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 04:51:16 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Ahn", "Joon Young", ""], ["Cho", "Nam Ik", ""]]}, {"id": "2104.09059", "submitter": "Fei Shen", "authors": "Fei Shen, Xin He, Mengwan Wei and Yi Xie", "title": "A Competitive Method to VIPriors Object Detection Challenge", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this report, we introduce the technical details of our submission to the\nVIPriors object detection challenge. Our solution is based on mmdetction of a\nstrong baseline open-source detection toolbox. Firstly, we introduce an\neffective data augmentation method to address the lack of data problem, which\ncontains bbox-jitter, grid-mask, and mix-up. Secondly, we present a robust\nregion of interest (ROI) extraction method to learn more significant ROI\nfeatures via embedding global context features. Thirdly, we propose a\nmulti-model integration strategy to refinement the prediction box, which\nweighted boxes fusion (WBF). Experimental results demonstrate that our approach\ncan significantly improve the average precision (AP) of object detection on the\nsubset of the COCO2017 dataset.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 05:33:39 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Shen", "Fei", ""], ["He", "Xin", ""], ["Wei", "Mengwan", ""], ["Xie", "Yi", ""]]}, {"id": "2104.09065", "submitter": "Minjun Li", "authors": "Minjun Li, Yanghua Jin, Huachun Zhu", "title": "Surrogate Gradient Field for Latent Space Manipulation", "comments": "19 pages, 18 figures, CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative adversarial networks (GANs) can generate high-quality images from\nsampled latent codes. Recent works attempt to edit an image by manipulating its\nunderlying latent code, but rarely go beyond the basic task of attribute\nadjustment. We propose the first method that enables manipulation with\nmultidimensional condition such as keypoints and captions. Specifically, we\ndesign an algorithm that searches for a new latent code that satisfies the\ntarget condition based on the Surrogate Gradient Field (SGF) induced by an\nauxiliary mapping network. For quantitative comparison, we propose a metric to\nevaluate the disentanglement of manipulation methods. Thorough experimental\nanalysis on the facial attribute adjustment task shows that our method\noutperforms state-of-the-art methods in disentanglement. We further apply our\nmethod to tasks of various condition modalities to demonstrate that our method\ncan alter complex image properties such as keypoints and captions.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 06:15:06 GMT"}, {"version": "v2", "created": "Tue, 20 Apr 2021 15:55:27 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Li", "Minjun", ""], ["Jin", "Yanghua", ""], ["Zhu", "Huachun", ""]]}, {"id": "2104.09068", "submitter": "Tengfei Wang", "authors": "Tengfei Wang, Hao Ouyang, Qifeng Chen", "title": "Image Inpainting with External-internal Learning and Monochromic\n  Bottleneck", "comments": "CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although recent inpainting approaches have demonstrated significant\nimprovements with deep neural networks, they still suffer from artifacts such\nas blunt structures and abrupt colors when filling in the missing regions. To\naddress these issues, we propose an external-internal inpainting scheme with a\nmonochromic bottleneck that helps image inpainting models remove these\nartifacts. In the external learning stage, we reconstruct missing structures\nand details in the monochromic space to reduce the learning dimension. In the\ninternal learning stage, we propose a novel internal color propagation method\nwith progressive learning strategies for consistent color restoration.\nExtensive experiments demonstrate that our proposed scheme helps image\ninpainting models produce more structure-preserved and visually compelling\nresults.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 06:22:10 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Wang", "Tengfei", ""], ["Ouyang", "Hao", ""], ["Chen", "Qifeng", ""]]}, {"id": "2104.09116", "submitter": "Dingkang Liang", "authors": "Dingkang Liang, Xiwu Chen, Wei Xu, Yu Zhou, Xiang Bai", "title": "TransCrowd: Weakly-Supervised Crowd Counting with Transformer", "comments": "Teach Report. Code is available at\n  https://github.com/dk-liang/TransCrowd", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The mainstream crowd counting methods usually utilize the convolution neural\nnetwork (CNN) to regress a density map, requiring point-level annotations.\nHowever, annotating each person with a point is an expensive and laborious\nprocess. During the testing phase, the point-level annotations are not\nconsidered to evaluate the counting accuracy, which means the point-level\nannotations are redundant. Hence, it is desirable to develop weakly-supervised\ncounting methods that just rely on count level annotations, a more economical\nway of labeling. Current weakly-supervised counting methods adopt the CNN to\nregress a total count of the crowd by an image-to-count paradigm. However,\nhaving limited receptive fields for context modeling is an intrinsic limitation\nof these weakly-supervised CNN-based methods. These methods thus can not\nachieve satisfactory performance, limited applications in the real-word. The\nTransformer is a popular sequence-to-sequence prediction model in NLP, which\ncontains a global receptive field. In this paper, we propose TransCrowd, which\nreformulates the weakly-supervised crowd counting problem from the perspective\nof sequence-to-count based on Transformer. We observe that the proposed\nTransCrowd can effectively extract the semantic crowd information by using the\nself-attention mechanism of Transformer. To the best of our knowledge, this is\nthe first work to adopt a pure Transformer for crowd counting research.\nExperiments on five benchmark datasets demonstrate that the proposed TransCrowd\nachieves superior performance compared with all the weakly-supervised CNN-based\ncounting methods and gains highly competitive counting performance compared\nwith some popular fully-supervised counting methods. Code is available at\nhttps://github.com/dk-liang/TransCrowd.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 08:12:50 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Liang", "Dingkang", ""], ["Chen", "Xiwu", ""], ["Xu", "Wei", ""], ["Zhou", "Yu", ""], ["Bai", "Xiang", ""]]}, {"id": "2104.09123", "submitter": "Laura D\\\"orr", "authors": "Laura D\\\"orr, Felix Brandt, Alexander Naumann, Martin Pouls", "title": "TetraPackNet: Four-Corner-Based Object Detection in Logistics Use-Cases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  While common image object detection tasks focus on bounding boxes or\nsegmentation masks as object representations, we consider the problem of\nfinding objects based on four arbitrary vertices. We propose a novel model,\nnamed TetraPackNet, to tackle this problem. TetraPackNet is based on CornerNet\nand uses similar algorithms and ideas. It is designated for applications\nrequiring high-accuracy detection of regularly shaped objects, which is the\ncase in the logistics use-case of packaging structure recognition. We evaluate\nour model on our specific real-world dataset for this use-case. Baselined\nagainst a previous solution, consisting of a Mask R-CNN model and suitable\npost-processing steps, TetraPackNet achieves superior results (9% higher in\naccuracy) in the sub-task of four-corner based transport unit side detection.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 08:22:14 GMT"}, {"version": "v2", "created": "Wed, 23 Jun 2021 08:52:49 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["D\u00f6rr", "Laura", ""], ["Brandt", "Felix", ""], ["Naumann", "Alexander", ""], ["Pouls", "Martin", ""]]}, {"id": "2104.09124", "submitter": "Jia-Xin Zhuang", "authors": "Yuting Gao, Jia-Xin Zhuang, Ke Li, Hao Cheng, Xiaowei Guo, Feiyue\n  Huang, Rongrong Ji, Xing Sun", "title": "DisCo: Remedy Self-supervised Learning on Lightweight Models with\n  Distilled Contrastive Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While self-supervised representation learning (SSL) has received widespread\nattention from the community, recent research argue that its performance will\nsuffer a cliff fall when the model size decreases. The current method mainly\nrelies on contrastive learning to train the network and in this work, we\npropose a simple yet effective Distilled Contrastive Learning (DisCo) to ease\nthe issue by a large margin. Specifically, we find the final embedding obtained\nby the mainstream SSL methods contains the most fruitful information, and\npropose to distill the final embedding to maximally transmit a teacher's\nknowledge to a lightweight model by constraining the last embedding of the\nstudent to be consistent with that of the teacher. In addition, in the\nexperiment, we find that there exists a phenomenon termed Distilling BottleNeck\nand present to enlarge the embedding dimension to alleviate this problem. Our\nmethod does not introduce any extra parameter to lightweight models during\ndeployment. Experimental results demonstrate that our method achieves the\nstate-of-the-art on all lightweight models. Particularly, when\nResNet-101/ResNet-50 is used as teacher to teach EfficientNet-B0, the linear\nresult of EfficientNet-B0 on ImageNet is very close to ResNet-101/ResNet-50,\nbut the number of parameters of EfficientNet-B0 is only 9.4%/16.3% of\nResNet-101/ResNet-50.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 08:22:52 GMT"}, {"version": "v2", "created": "Wed, 14 Jul 2021 11:29:35 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Gao", "Yuting", ""], ["Zhuang", "Jia-Xin", ""], ["Li", "Ke", ""], ["Cheng", "Hao", ""], ["Guo", "Xiaowei", ""], ["Huang", "Feiyue", ""], ["Ji", "Rongrong", ""], ["Sun", "Xing", ""]]}, {"id": "2104.09125", "submitter": "Amir Hertz", "authors": "Amir Hertz, Or Perel, Raja Giryes, Olga Sorkine-Hornung and Daniel\n  Cohen-Or", "title": "SAPE: Spatially-Adaptive Progressive Encoding for Neural Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.GR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Multilayer-perceptrons (MLP) are known to struggle with learning functions of\nhigh-frequencies, and in particular cases with wide frequency bands. We present\na spatially adaptive progressive encoding (SAPE) scheme for input signals of\nMLP networks, which enables them to better fit a wide range of frequencies\nwithout sacrificing training stability or requiring any domain specific\npreprocessing. SAPE gradually unmasks signal components with increasing\nfrequencies as a function of time and space. The progressive exposure of\nfrequencies is monitored by a feedback loop throughout the neural optimization\nprocess, allowing changes to propagate at different rates among local spatial\nportions of the signal space. We demonstrate the advantage of SAPE on a variety\nof domains and applications, including regression of low dimensional signals\nand images, representation learning of occupancy networks, and a geometric task\nof mesh transfer between 3D shapes.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 08:22:55 GMT"}, {"version": "v2", "created": "Fri, 28 May 2021 15:46:32 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Hertz", "Amir", ""], ["Perel", "Or", ""], ["Giryes", "Raja", ""], ["Sorkine-Hornung", "Olga", ""], ["Cohen-Or", "Daniel", ""]]}, {"id": "2104.09133", "submitter": "Lei Sun", "authors": "Lei Sun", "title": "RANSIC: Fast and Highly Robust Estimation for Rotation Search and Point\n  Cloud Registration using Invariant Compatibility", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Correspondence-based rotation search and point cloud registration are two\nfundamental problems in robotics and computer vision. However, the presence of\noutliers, sometimes even occupying the great majority of the putative\ncorrespondences, can make many existing algorithms either fail or have very\nhigh computational cost. In this paper, we present RANSIC (RANdom Sampling with\nInvariant Compatibility), a fast and highly robust method applicable to both\nproblems based on a new paradigm combining random sampling with invariance and\ncompatibility. Generally, RANSIC starts with randomly selecting small subsets\nfrom the correspondence set, then seeks potential inliers as graph vertices\nfrom the random subsets through the compatibility tests of invariants\nestablished in each problem, and eventually returns the eligible inliers when\nthere exists at least one K-degree vertex (K is automatically updated depending\non the problem) and the residual errors satisfy a certain termination condition\nat the same time. In multiple synthetic and real experiments, we demonstrate\nthat RANSIC is fast for use, robust against over 95% outliers, and also able to\nrecall approximately 100% inliers, outperforming other state-of-the-art solvers\nfor both the rotation search and the point cloud registration problems.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 08:29:34 GMT"}, {"version": "v2", "created": "Tue, 20 Apr 2021 08:21:43 GMT"}, {"version": "v3", "created": "Wed, 21 Apr 2021 07:46:20 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Sun", "Lei", ""]]}, {"id": "2104.09134", "submitter": "Dawit Mureja Argaw", "authors": "Dawit Mureja Argaw, Junsik Kim, Francois Rameau, Chaoning Zhang, In So\n  Kweon", "title": "Restoration of Video Frames from a Single Blurred Image with Motion\n  Understanding", "comments": "Accepted to CVPRW, NTIRE 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a novel framework to generate clean video frames from a single\nmotion-blurred image. While a broad range of literature focuses on recovering a\nsingle image from a blurred image, in this work, we tackle a more challenging\ntask i.e. video restoration from a blurred image. We formulate video\nrestoration from a single blurred image as an inverse problem by setting clean\nimage sequence and their respective motion as latent factors, and the blurred\nimage as an observation. Our framework is based on an encoder-decoder structure\nwith spatial transformer network modules to restore a video sequence and its\nunderlying motion in an end-to-end manner. We design a loss function and\nregularizers with complementary properties to stabilize the training and\nanalyze variant models of the proposed network. The effectiveness and\ntransferability of our network are highlighted through a large set of\nexperiments on two different types of datasets: camera rotation blurs generated\nfrom panorama scenes and dynamic motion blurs in high speed videos.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 08:32:57 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Argaw", "Dawit Mureja", ""], ["Kim", "Junsik", ""], ["Rameau", "Francois", ""], ["Zhang", "Chaoning", ""], ["Kweon", "In So", ""]]}, {"id": "2104.09136", "submitter": "Kai Li", "authors": "Kai Li, Chang Liu, Handong Zhao, Yulun Zhang, Yun Fu", "title": "Semi-Supervised Domain Adaptation with Prototypical Alignment and\n  Consistency Learning", "comments": "10 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Domain adaptation enhances generalizability of a model across domains with\ndomain shifts. Most research effort has been spent on Unsupervised Domain\nAdaption (UDA) which trains a model jointly with labeled source data and\nunlabeled target data. This paper studies how much it can help address domain\nshifts if we further have a few target samples (e.g., one sample per class)\nlabeled. This is the so-called semi-supervised domain adaptation (SSDA) problem\nand the few labeled target samples are termed as ``landmarks''. To explore the\nfull potential of landmarks, we incorporate a prototypical alignment (PA)\nmodule which calculates a target prototype for each class from the landmarks;\nsource samples are then aligned with the target prototype from the same class.\nTo further alleviate label scarcity, we propose a data augmentation based\nsolution. Specifically, we severely perturb the labeled images, making PA\nnon-trivial to achieve and thus promoting model generalizability. Moreover, we\napply consistency learning on unlabeled target images, by perturbing each image\nwith light transformations and strong transformations. Then, the strongly\nperturbed image can enjoy ``supervised-like'' training using the pseudo label\ninferred from the lightly perturbed one. Experiments show that the proposed\nmethod, though simple, reaches significant performance gains over\nstate-of-the-art methods, and enjoys the flexibility of being able to serve as\na plug-and-play component to various existing UDA methods and improve\nadaptation performance with landmarks provided. Our code is available at\n\\url{https://github.com/kailigo/pacl}.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 08:46:08 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Li", "Kai", ""], ["Liu", "Chang", ""], ["Zhao", "Handong", ""], ["Zhang", "Yulun", ""], ["Fu", "Yun", ""]]}, {"id": "2104.09145", "submitter": "Konstantinos Papadopoulos", "authors": "Konstantinos Papadopoulos, Anis Kacem, Abdelrahman Shabayek, Djamila\n  Aouada", "title": "Face-GCN: A Graph Convolutional Network for 3D Dynamic Face\n  Identification/Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Face identification/recognition has significantly advanced over the past\nyears. However, most of the proposed approaches rely on static RGB frames and\non neutral facial expressions. This has two disadvantages. First, important\nfacial shape cues are ignored. Second, facial deformations due to expressions\ncan have an impact on the performance of such a method. In this paper, we\npropose a novel framework for dynamic 3D face identification/recognition based\non facial keypoints. Each dynamic sequence of facial expressions is represented\nas a spatio-temporal graph, which is constructed using 3D facial landmarks.\nEach graph node contains local shape and texture features that are extracted\nfrom its neighborhood. For the classification/identification of faces, a\nSpatio-temporal Graph Convolutional Network (ST-GCN) is used. Finally, we\nevaluate our approach on a challenging dynamic 3D facial expression dataset.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 09:05:39 GMT"}, {"version": "v2", "created": "Tue, 20 Apr 2021 08:36:36 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Papadopoulos", "Konstantinos", ""], ["Kacem", "Anis", ""], ["Shabayek", "Abdelrahman", ""], ["Aouada", "Djamila", ""]]}, {"id": "2104.09152", "submitter": "Yulin Zhang", "authors": "Yulin Zhang, Bo Ma, Longyao Liu and Xin Yi", "title": "Self-Paced Uncertainty Estimation for One-shot Person Re-Identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The one-shot Person Re-ID scenario faces two kinds of uncertainties when\nconstructing the prediction model from $X$ to $Y$. The first is model\nuncertainty, which captures the noise of the parameters in DNNs due to a lack\nof training data. The second is data uncertainty, which can be divided into two\nsub-types: one is image noise, where severe occlusion and the complex\nbackground contain irrelevant information about the identity; the other is\nlabel noise, where mislabeled affects visual appearance learning. In this\npaper, to tackle these issues, we propose a novel Self-Paced Uncertainty\nEstimation Network (SPUE-Net) for one-shot Person Re-ID. By introducing a\nself-paced sampling strategy, our method can estimate the pseudo-labels of\nunlabeled samples iteratively to expand the labeled samples gradually and\nremove model uncertainty without extra supervision. We divide the pseudo-label\nsamples into two subsets to make the use of training samples more reasonable\nand effective. In addition, we apply a Co-operative learning method of local\nuncertainty estimation combined with determinacy estimation to achieve better\nhidden space feature mining and to improve the precision of selected\npseudo-labeled samples, which reduces data uncertainty. Extensive comparative\nevaluation experiments on video-based and image-based datasets show that\nSPUE-Net has significant advantages over the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 09:20:30 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Zhang", "Yulin", ""], ["Ma", "Bo", ""], ["Liu", "Longyao", ""], ["Yi", "Xin", ""]]}, {"id": "2104.09159", "submitter": "Lamberto Ballan", "authors": "Yunrui Guo, Guglielmo Camporese, Wenjing Yang, Alessandro Sperduti,\n  Lamberto Ballan", "title": "Conditional Variational Capsule Network for Open Set Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In open set recognition, a classifier has to detect unknown classes that are\nnot known at training time. In order to recognize new classes, the classifier\nhas to project the input samples of known classes in very compact and separated\nregions of the features space in order to discriminate outlier samples of\nunknown classes. Recently proposed Capsule Networks have shown to outperform\nalternatives in many fields, particularly in image recognition, however they\nhave not been fully applied yet to open-set recognition. In capsule networks,\nscalar neurons are replaced by capsule vectors or matrices, whose entries\nrepresent different properties of objects. In our proposal, during training,\ncapsules features of the same known class are encouraged to match a pre-defined\ngaussian, one for each class. To this end, we use the variational autoencoder\nframework, with a set of gaussian prior as the approximation for the posterior\ndistribution. In this way, we are able to control the compactness of the\nfeatures of the same class around the center of the gaussians, thus controlling\nthe ability of the classifier in detecting samples from unknown classes. We\nconducted several experiments and ablation of our model, obtaining state of the\nart results on different datasets in the open set recognition and unknown\ndetection tasks.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 09:39:30 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Guo", "Yunrui", ""], ["Camporese", "Guglielmo", ""], ["Yang", "Wenjing", ""], ["Sperduti", "Alessandro", ""], ["Ballan", "Lamberto", ""]]}, {"id": "2104.09169", "submitter": "Henry Howard-Jenkins", "authors": "Henry Howard-Jenkins, Jose-Raul Ruiz-Sarmiento, Victor Adrian\n  Prisacariu", "title": "LaLaLoc: Latent Layout Localisation in Dynamic, Unvisited Environments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present LaLaLoc to localise in environments without the need for prior\nvisitation, and in a manner that is robust to large changes in scene\nappearance, such as a full rearrangement of furniture. Specifically, LaLaLoc\nperforms localisation through latent representations of room layout. LaLaLoc\nlearns a rich embedding space shared between RGB panoramas and layouts inferred\nfrom a known floor plan that encodes the structural similarity between\nlocations. Further, LaLaLoc introduces direct, cross-modal pose optimisation in\nits latent space. Thus, LaLaLoc enables fine-grained pose estimation in a scene\nwithout the need for prior visitation, as well as being robust to dynamics,\nsuch as a change in furniture configuration. We show that in a domestic\nenvironment LaLaLoc is able to accurately localise a single RGB panorama image\nto within 8.3cm, given only a floor plan as a prior.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 09:49:13 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Howard-Jenkins", "Henry", ""], ["Ruiz-Sarmiento", "Jose-Raul", ""], ["Prisacariu", "Victor Adrian", ""]]}, {"id": "2104.09176", "submitter": "Stefan Zernetsch", "authors": "Stefan Zernetsch, Hannes Reichert, Viktor Kress, Konrad Doll, Bernhard\n  Sick", "title": "Cyclist Intention Detection: A Probabilistic Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This article presents a holistic approach for probabilistic cyclist intention\ndetection. A basic movement detection based on motion history images (MHI) and\na residual convolutional neural network (ResNet) are used to estimate\nprobabilities for the current cyclist motion state. These probabilities are\nused as weights in a probabilistic ensemble trajectory forecast. The ensemble\nconsists of specialized models, which produce individual forecasts in the form\nof Gaussian distributions under the assumption of a certain motion state of the\ncyclist (e.g. cyclist is starting or turning left). By weighting the\nspecialized models, we create forecasts in the from of Gaussian mixtures that\ndefine regions within which the cyclists will reside with a certain\nprobability. To evaluate our method, we rate the reliability, sharpness, and\npositional accuracy of our forecasted distributions. We compare our method to a\nsingle model approach which produces forecasts in the form of Gaussian\ndistributions and show that our method is able to produce more reliable and\nsharper outputs while retaining comparable positional accuracy. Both methods\nare evaluated using a dataset created at a public traffic intersection. Our\ncode and the dataset are made publicly available.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 09:59:04 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Zernetsch", "Stefan", ""], ["Reichert", "Hannes", ""], ["Kress", "Viktor", ""], ["Doll", "Konrad", ""], ["Sick", "Bernhard", ""]]}, {"id": "2104.09191", "submitter": "Waqar Ahmed", "authors": "Waqar Ahmed, Andrea Zunino, Pietro Morerio and Vittorio Murino", "title": "Compact CNN Structure Learning by Knowledge Distillation", "comments": "This paper has been accepted to ICPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The concept of compressing deep Convolutional Neural Networks (CNNs) is\nessential to use limited computation, power, and memory resources on embedded\ndevices. However, existing methods achieve this objective at the cost of a drop\nin inference accuracy in computer vision tasks. To address such a drawback, we\npropose a framework that leverages knowledge distillation along with\ncustomizable block-wise optimization to learn a lightweight CNN structure while\npreserving better control over the compression-performance tradeoff.\nConsidering specific resource constraints, e.g., floating-point operations per\ninference (FLOPs) or model-parameters, our method results in a state of the art\nnetwork compression while being capable of achieving better inference accuracy.\nIn a comprehensive evaluation, we demonstrate that our method is effective,\nrobust, and consistent with results over a variety of network architectures and\ndatasets, at negligible training overhead. In particular, for the already\ncompact network MobileNet_v2, our method offers up to 2x and 5.2x better model\ncompression in terms of FLOPs and model-parameters, respectively, while getting\n1.05% better model performance than the baseline network.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 10:34:22 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Ahmed", "Waqar", ""], ["Zunino", "Andrea", ""], ["Morerio", "Pietro", ""], ["Murino", "Vittorio", ""]]}, {"id": "2104.09216", "submitter": "Jiacheng Chen", "authors": "Jiacheng Chen, Bin-Bin Gao, Zongqing Lu, Jing-Hao Xue, Chengjie Wang,\n  Qingmin Liao", "title": "SCNet: Enhancing Few-Shot Semantic Segmentation by Self-Contrastive\n  Background Prototypes", "comments": "9 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Few-shot semantic segmentation aims to segment novel-class objects in a query\nimage with only a few annotated examples in support images. Most of advanced\nsolutions exploit a metric learning framework that performs segmentation\nthrough matching each pixel to a learned foreground prototype. However, this\nframework suffers from biased classification due to incomplete construction of\nsample pairs with the foreground prototype only. To address this issue, in this\npaper, we introduce a complementary self-contrastive task into few-shot\nsemantic segmentation. Our new model is able to associate the pixels in a\nregion with the prototype of this region, no matter they are in the foreground\nor background. To this end, we generate self-contrastive background prototypes\ndirectly from the query image, with which we enable the construction of\ncomplete sample pairs and thus a complementary and auxiliary segmentation task\nto achieve the training of a better segmentation model. Extensive experiments\non PASCAL-5$^i$ and COCO-20$^i$ demonstrate clearly the superiority of our\nproposal. At no expense of inference efficiency, our model achieves\nstate-of-the results in both 1-shot and 5-shot settings for few-shot semantic\nsegmentation.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 11:21:47 GMT"}, {"version": "v2", "created": "Sun, 25 Apr 2021 12:15:10 GMT"}, {"version": "v3", "created": "Wed, 28 Apr 2021 04:29:59 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Chen", "Jiacheng", ""], ["Gao", "Bin-Bin", ""], ["Lu", "Zongqing", ""], ["Xue", "Jing-Hao", ""], ["Wang", "Chengjie", ""], ["Liao", "Qingmin", ""]]}, {"id": "2104.09223", "submitter": "Jiahao Wang", "authors": "Jiahao Wang, Han Shu, Weihao Xia, Yujiu Yang, Yunhe Wang", "title": "Coarse-to-Fine Searching for Efficient Generative Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the neural architecture search (NAS) problem for\ndeveloping efficient generator networks. Compared with deep models for visual\nrecognition tasks, generative adversarial network (GAN) are usually designed to\nconduct various complex image generation. We first discover an intact search\nspace of generator networks including three dimensionalities, i.e., path,\noperator, channel for fully excavating the network performance. To reduce the\nhuge search cost, we explore a coarse-to-fine search strategy which divides the\noverall search process into three sub-optimization problems accordingly. In\naddition, a fair supernet training approach is utilized to ensure that all\nsub-networks can be updated fairly and stably. Experiments results on\nbenchmarks show that we can provide generator networks with better image\nquality and lower computational costs over the state-of-the-art methods. For\nexample, with our method, it takes only about 8 GPU hours on the entire\nedges-to-shoes dataset to get a 2.56 MB model with a 24.13 FID score and 10 GPU\nhours on the entire Urban100 dataset to get a 1.49 MB model with a 24.94 PSNR\nscore.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 11:46:20 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Wang", "Jiahao", ""], ["Shu", "Han", ""], ["Xia", "Weihao", ""], ["Yang", "Yujiu", ""], ["Wang", "Yunhe", ""]]}, {"id": "2104.09224", "submitter": "Aditya Prakash", "authors": "Aditya Prakash, Kashyap Chitta, Andreas Geiger", "title": "Multi-Modal Fusion Transformer for End-to-End Autonomous Driving", "comments": "CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How should representations from complementary sensors be integrated for\nautonomous driving? Geometry-based sensor fusion has shown great promise for\nperception tasks such as object detection and motion forecasting. However, for\nthe actual driving task, the global context of the 3D scene is key, e.g. a\nchange in traffic light state can affect the behavior of a vehicle\ngeometrically distant from that traffic light. Geometry alone may therefore be\ninsufficient for effectively fusing representations in end-to-end driving\nmodels. In this work, we demonstrate that imitation learning policies based on\nexisting sensor fusion methods under-perform in the presence of a high density\nof dynamic agents and complex scenarios, which require global contextual\nreasoning, such as handling traffic oncoming from multiple directions at\nuncontrolled intersections. Therefore, we propose TransFuser, a novel\nMulti-Modal Fusion Transformer, to integrate image and LiDAR representations\nusing attention. We experimentally validate the efficacy of our approach in\nurban settings involving complex scenarios using the CARLA urban driving\nsimulator. Our approach achieves state-of-the-art driving performance while\nreducing collisions by 76% compared to geometry-based fusion.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 11:48:13 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Prakash", "Aditya", ""], ["Chitta", "Kashyap", ""], ["Geiger", "Andreas", ""]]}, {"id": "2104.09248", "submitter": "Albert Garcia", "authors": "Albert Garcia, Mohamed Adel Musallam, Vincent Gaudilliere, Enjie\n  Ghorbel, Kassem Al Ismaeil, Marcos Perez, Djamila Aouada", "title": "LSPnet: A 2D Localization-oriented Spacecraft Pose Estimation Neural\n  Network", "comments": "9 pages, 5 figures, to be published at AI4Space 2021 IEEE/CVF\n  Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Being capable of estimating the pose of uncooperative objects in space has\nbeen proposed as a key asset for enabling safe close-proximity operations such\nas space rendezvous, in-orbit servicing and active debris removal. Usual\napproaches for pose estimation involve classical computer vision-based\nsolutions or the application of Deep Learning (DL) techniques. This work\nexplores a novel DL-based methodology, using Convolutional Neural Networks\n(CNNs), for estimating the pose of uncooperative spacecrafts. Contrary to other\napproaches, the proposed CNN directly regresses poses without needing any prior\n3D information. Moreover, bounding boxes of the spacecraft in the image are\npredicted in a simple, yet efficient manner. The performed experiments show how\nthis work competes with the state-of-the-art in uncooperative spacecraft pose\nestimation, including works which require 3D information as well as works which\npredict bounding boxes through sophisticated CNNs.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 12:46:05 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Garcia", "Albert", ""], ["Musallam", "Mohamed Adel", ""], ["Gaudilliere", "Vincent", ""], ["Ghorbel", "Enjie", ""], ["Ismaeil", "Kassem Al", ""], ["Perez", "Marcos", ""], ["Aouada", "Djamila", ""]]}, {"id": "2104.09254", "submitter": "Linara Adilova", "authors": "Linara Adilova, Elena Schulz, Maram Akila, Sebastian Houben, Jan David\n  Schneider, Fabian Hueger, Tim Wirtz", "title": "Plants Don't Walk on the Street: Common-Sense Reasoning for Reliable\n  Semantic Segmentation", "comments": "Published at SAIAD (Safe Artificial Intelligence for Automated\n  Driving) workshop at CVPR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data-driven sensor interpretation in autonomous driving can lead to highly\nimplausible predictions as can most of the time be verified with common-sense\nknowledge. However, learning common knowledge only from data is hard and\napproaches for knowledge integration are an active research area. We propose to\nuse a partly human-designed, partly learned set of rules to describe relations\nbetween objects of a traffic scene on a high level of abstraction. In doing so,\nwe improve and robustify existing deep neural networks consuming low-level\nsensor information. We present an initial study adapting the well-established\nProbabilistic Soft Logic (PSL) framework to validate and improve on the problem\nof semantic segmentation. We describe in detail how we integrate common\nknowledge into the segmentation pipeline using PSL and verify our approach in a\nset of experiments demonstrating the increase in robustness against several\nsevere image distortions applied to the A2D2 autonomous driving data set.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 12:51:06 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Adilova", "Linara", ""], ["Schulz", "Elena", ""], ["Akila", "Maram", ""], ["Houben", "Sebastian", ""], ["Schneider", "Jan David", ""], ["Hueger", "Fabian", ""], ["Wirtz", "Tim", ""]]}, {"id": "2104.09259", "submitter": "Akin Caliskan", "authors": "Akin Caliskan, Armin Mustafa, Adrian Hilton", "title": "Temporal Consistency Loss for High Resolution Textured and Clothed\n  3DHuman Reconstruction from Monocular Video", "comments": "To appear in Dynavis Workshop, CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel method to learn temporally consistent 3D reconstruction of\nclothed people from a monocular video. Recent methods for 3D human\nreconstruction from monocular video using volumetric, implicit or parametric\nhuman shape models, produce per frame reconstructions giving temporally\ninconsistent output and limited performance when applied to video. In this\npaper, we introduce an approach to learn temporally consistent features for\ntextured reconstruction of clothed 3D human sequences from monocular video by\nproposing two advances: a novel temporal consistency loss function; and hybrid\nrepresentation learning for implicit 3D reconstruction from 2D images and\ncoarse 3D geometry. The proposed advances improve the temporal consistency and\naccuracy of both the 3D reconstruction and texture prediction from a monocular\nvideo. Comprehensive comparative performance evaluation on images of people\ndemonstrates that the proposed method significantly outperforms the\nstate-of-the-art learning-based single image 3D human shape estimation\napproaches achieving significant improvement of reconstruction accuracy,\ncompleteness, quality and temporal consistency.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 13:04:29 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Caliskan", "Akin", ""], ["Mustafa", "Armin", ""], ["Hilton", "Adrian", ""]]}, {"id": "2104.09283", "submitter": "Akin Caliskan", "authors": "Armin Mustafa, Akin Caliskan, Lourdes Agapito, Adrian Hilton", "title": "Multi-person Implicit Reconstruction from a Single Image", "comments": "To appear in The IEEE Conference on Computer Vision and Pattern\n  Recognition (CVPR) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new end-to-end learning framework to obtain detailed and\nspatially coherent reconstructions of multiple people from a single image.\nExisting multi-person methods suffer from two main drawbacks: they are often\nmodel-based and therefore cannot capture accurate 3D models of people with\nloose clothing and hair; or they require manual intervention to resolve\nocclusions or interactions. Our method addresses both limitations by\nintroducing the first end-to-end learning approach to perform model-free\nimplicit reconstruction for realistic 3D capture of multiple clothed people in\narbitrary poses (with occlusions) from a single image. Our network\nsimultaneously estimates the 3D geometry of each person and their 6DOF spatial\nlocations, to obtain a coherent multi-human reconstruction. In addition, we\nintroduce a new synthetic dataset that depicts images with a varying number of\ninter-occluded humans and a variety of clothing and hair styles. We demonstrate\nrobust, high-resolution reconstructions on images of multiple humans with\ncomplex occlusions, loose clothing and a large variety of poses and scenes. Our\nquantitative evaluation on both synthetic and real-world datasets demonstrates\nstate-of-the-art performance with significant improvements in the accuracy and\ncompleteness of the reconstructions over competing approaches.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 13:21:55 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Mustafa", "Armin", ""], ["Caliskan", "Akin", ""], ["Agapito", "Lourdes", ""], ["Hilton", "Adrian", ""]]}, {"id": "2104.09284", "submitter": "Xitong Gao", "authors": "Yunrui Yu, Xitong Gao, Cheng-Zhong Xu", "title": "LAFEAT: Piercing Through Adversarial Defenses with Latent Features", "comments": "Accepted as an oral paper in Conference on Computer Vision and\n  Pattern Recognition (CVPR) 2021. 11 pages, 6 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Deep convolutional neural networks are susceptible to adversarial attacks.\nThey can be easily deceived to give an incorrect output by adding a tiny\nperturbation to the input. This presents a great challenge in making CNNs\nrobust against such attacks. An influx of new defense techniques have been\nproposed to this end. In this paper, we show that latent features in certain\n\"robust\" models are surprisingly susceptible to adversarial attacks. On top of\nthis, we introduce a unified $\\ell_\\infty$-norm white-box attack algorithm\nwhich harnesses latent features in its gradient descent steps, namely LAFEAT.\nWe show that not only is it computationally much more efficient for successful\nattacks, but it is also a stronger adversary than the current state-of-the-art\nacross a wide range of defense mechanisms. This suggests that model robustness\ncould be contingent on the effective use of the defender's hidden components,\nand it should no longer be viewed from a holistic perspective.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 13:22:20 GMT"}, {"version": "v2", "created": "Tue, 20 Apr 2021 07:35:16 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Yu", "Yunrui", ""], ["Gao", "Xitong", ""], ["Xu", "Cheng-Zhong", ""]]}, {"id": "2104.09301", "submitter": "William Beksi", "authors": "Pritam Karmokar, Kashish Dhal, William J. Beksi, Animesh Chakravarthy", "title": "Vision-Based Guidance for Tracking Dynamic Objects", "comments": "To be published in the 2021 International Conference on Unmanned\n  Aircraft Systems (ICUAS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we present a novel vision-based framework for tracking dynamic\nobjects using guidance laws based on a rendezvous cone approach. These guidance\nlaws enable an unmanned aircraft system equipped with a monocular camera to\ncontinuously follow a moving object within the sensor's field of view. We\nidentify and classify feature point estimators for managing the occurrence of\nocclusions during the tracking process in an exclusive manner. Furthermore, we\ndevelop an open-source simulation environment and perform a series of\nsimulations to show the efficacy of our methods.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 13:45:56 GMT"}, {"version": "v2", "created": "Sat, 15 May 2021 00:34:47 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Karmokar", "Pritam", ""], ["Dhal", "Kashish", ""], ["Beksi", "William J.", ""], ["Chakravarthy", "Animesh", ""]]}, {"id": "2104.09310", "submitter": "Philippe Weitz", "authors": "Philippe Weitz, Yinxi Wang, Kimmo Kartasalo, Lars Egevad, Johan\n  Lindberg, Henrik Gr\\\"onberg, Martin Eklund, Mattias Rantalainen", "title": "Transcriptome-wide prediction of prostate cancer gene expression from\n  histopathology images using co-expression based convolutional neural networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Molecular phenotyping by gene expression profiling is common in contemporary\ncancer research and in molecular diagnostics. However, molecular profiling\nremains costly and resource intense to implement, and is just starting to be\nintroduced into clinical diagnostics. Molecular changes, including genetic\nalterations and gene expression changes, occuring in tumors cause morphological\nchanges in tissue, which can be observed on the microscopic level. The\nrelationship between morphological patterns and some of the molecular\nphenotypes can be exploited to predict molecular phenotypes directly from\nroutine haematoxylin and eosin (H&E) stained whole slide images (WSIs) using\ndeep convolutional neural networks (CNNs). In this study, we propose a new,\ncomputationally efficient approach for disease specific modelling of\nrelationships between morphology and gene expression, and we conducted the\nfirst transcriptome-wide analysis in prostate cancer, using CNNs to predict\nbulk RNA-sequencing estimates from WSIs of H&E stained tissue. The work is\nbased on the TCGA PRAD study and includes both WSIs and RNA-seq data for 370\npatients. Out of 15586 protein coding and sufficiently frequently expressed\ntranscripts, 6618 had predicted expression significantly associated with\nRNA-seq estimates (FDR-adjusted p-value < 1*10-4) in a cross-validation. 5419\n(81.9%) of these were subsequently validated in a held-out test set. We also\ndemonstrate the ability to predict a prostate cancer specific cell cycle\nprogression score directly from WSIs. These findings suggest that contemporary\ncomputer vision models offer an inexpensive and scalable solution for\nprediction of gene expression phenotypes directly from WSIs, providing\nopportunity for cost-effective large-scale research studies and molecular\ndiagnostics.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 13:50:25 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Weitz", "Philippe", ""], ["Wang", "Yinxi", ""], ["Kartasalo", "Kimmo", ""], ["Egevad", "Lars", ""], ["Lindberg", "Johan", ""], ["Gr\u00f6nberg", "Henrik", ""], ["Eklund", "Martin", ""], ["Rantalainen", "Mattias", ""]]}, {"id": "2104.09313", "submitter": "Fabian Schrumpf", "authors": "Fabian Schrumpf, Patrick Frenzel, Christoph Aust, Georg Osterhoff,\n  Mirco Fuchs", "title": "Assessment of deep learning based blood pressure prediction from PPG and\n  rPPG signals", "comments": "(Accepted / In press) 2021 IEEE/CVF Conference on Computer Vision and\n  Pattern Recognition Workshop (CVPRW)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exploiting photoplethysmography signals (PPG) for non-invasive blood pressure\n(BP) measurement is interesting for various reasons. First, PPG can easily be\nmeasured using fingerclip sensors. Second, camera-based approaches allow to\nderive remote PPG (rPPG) signals similar to PPG and therefore provide the\nopportunity for non-invasive measurements of BP. Various methods relying on\nmachine learning techniques have recently been published. Performances are\noften reported as the mean average error (MAE) on the data which is\nproblematic. This work aims to analyze the PPG- and rPPG-based BP prediction\nerror with respect to the underlying data distribution. First, we train\nestablished neural network (NN) architectures and derive an appropriate\nparameterization of input segments drawn from continuous PPG signals. Second,\nwe apply this parameterization to a larger PPG dataset and train NNs to predict\nBP. The resulting prediction errors increase towards less frequent BP values.\nThird, we use transfer learning to train the NNs for rPPG based BP prediction.\nThe resulting performances are similar to the PPG-only case. Finally, we apply\na personalization technique and retrain our NNs with subject-specific data.\nThis slightly reduces the prediction errors.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 15:56:58 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Schrumpf", "Fabian", ""], ["Frenzel", "Patrick", ""], ["Aust", "Christoph", ""], ["Osterhoff", "Georg", ""], ["Fuchs", "Mirco", ""]]}, {"id": "2104.09315", "submitter": "Megh Shukla", "authors": "Megh Shukla, Shuaib Ahmed", "title": "A Mathematical Analysis of Learning Loss for Active Learning in\n  Regression", "comments": "Accepted: 2021 IEEE CVPR Workshop on Fair, Data Efficient and Trusted\n  Computer Vision", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Active learning continues to remain significant in the industry since it is\ndata efficient. Not only is it cost effective on a constrained budget,\ncontinuous refinement of the model allows for early detection and resolution of\nfailure scenarios during the model development stage. Identifying and fixing\nfailures with the model is crucial as industrial applications demand that the\nunderlying model performs accurately in all foreseeable use cases. One popular\nstate-of-the-art technique that specializes in continuously refining the model\nvia failure identification is Learning Loss. Although simple and elegant, this\napproach is empirically motivated. Our paper develops a foundation for Learning\nLoss which enables us to propose a novel modification we call LearningLoss++.\nWe show that gradients are crucial in interpreting how Learning Loss works,\nwith rigorous analysis and comparison of the gradients between Learning Loss\nand LearningLoss++. We also propose a convolutional architecture that combines\nfeatures at different scales to predict the loss. We validate LearningLoss++\nfor regression on the task of human pose estimation (using MPII and LSP\ndatasets), as done in Learning Loss. We show that LearningLoss++ outperforms in\nidentifying scenarios where the model is likely to perform poorly, which on\nmodel refinement translates into reliable performance in the open world.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 13:54:20 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Shukla", "Megh", ""], ["Ahmed", "Shuaib", ""]]}, {"id": "2104.09333", "submitter": "Adrien Deli\\`ege Mr", "authors": "Anthony Cioppa, Adrien Deli\\`ege, Floriane Magera, Silvio Giancola,\n  Olivier Barnich, Bernard Ghanem, Marc Van Droogenbroeck", "title": "Camera Calibration and Player Localization in SoccerNet-v2 and\n  Investigation of their Representations for Action Spotting", "comments": "Paper accepted at the CVsports workshop at CVPR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Soccer broadcast video understanding has been drawing a lot of attention in\nrecent years within data scientists and industrial companies. This is mainly\ndue to the lucrative potential unlocked by effective deep learning techniques\ndeveloped in the field of computer vision. In this work, we focus on the topic\nof camera calibration and on its current limitations for the scientific\ncommunity. More precisely, we tackle the absence of a large-scale calibration\ndataset and of a public calibration network trained on such a dataset.\nSpecifically, we distill a powerful commercial calibration tool in a recent\nneural network architecture on the large-scale SoccerNet dataset, composed of\nuntrimmed broadcast videos of 500 soccer games. We further release our\ndistilled network, and leverage it to provide 3 ways of representing the\ncalibration results along with player localization. Finally, we exploit those\nrepresentations within the current best architecture for the action spotting\ntask of SoccerNet-v2, and achieve new state-of-the-art performances.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 14:21:05 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Cioppa", "Anthony", ""], ["Deli\u00e8ge", "Adrien", ""], ["Magera", "Floriane", ""], ["Giancola", "Silvio", ""], ["Barnich", "Olivier", ""], ["Ghanem", "Bernard", ""], ["Van Droogenbroeck", "Marc", ""]]}, {"id": "2104.09335", "submitter": "Alexandru Kampmann", "authors": "Alexandru Kampmann, Michael Lamberti, Nikola Petrovic, Stefan\n  Kowalewski, Bassam Alrifaee", "title": "Infrared Beacons for Robust Localization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a localization system that uses infrared beacons and a\ncamera equipped with an optical band-pass filter. Our system can reliably\ndetect and identify individual beacons at 100m distance regardless of lighting\nconditions. We describe the camera and beacon design as well as the image\nprocessing pipeline in detail. In our experiments, we investigate and\ndemonstrate the ability of the system to recognize our beacons in both daytime\nand nighttime conditions. High precision localization is a key enabler for\nautomated vehicles but remains unsolved, despite strong recent improvements.\nOur low-cost, infrastructure-based approach helps solve the localization\nproblem. All datasets are made available.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 14:23:20 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Kampmann", "Alexandru", ""], ["Lamberti", "Michael", ""], ["Petrovic", "Nikola", ""], ["Kowalewski", "Stefan", ""], ["Alrifaee", "Bassam", ""]]}, {"id": "2104.09359", "submitter": "Yann Labb\\'e", "authors": "Yann Labb\\'e, Justin Carpentier, Mathieu Aubry, Josef Sivic", "title": "Single-view robot pose and joint angle estimation via render & compare", "comments": "Accepted at CVPR 2021 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce RoboPose, a method to estimate the joint angles and the 6D\ncamera-to-robot pose of a known articulated robot from a single RGB image. This\nis an important problem to grant mobile and itinerant autonomous systems the\nability to interact with other robots using only visual information in\nnon-instrumented environments, especially in the context of collaborative\nrobotics. It is also challenging because robots have many degrees of freedom\nand an infinite space of possible configurations that often result in\nself-occlusions and depth ambiguities when imaged by a single camera. The\ncontributions of this work are three-fold. First, we introduce a new render &\ncompare approach for estimating the 6D pose and joint angles of an articulated\nrobot that can be trained from synthetic data, generalizes to new unseen robot\nconfigurations at test time, and can be applied to a variety of robots. Second,\nwe experimentally demonstrate the importance of the robot parametrization for\nthe iterative pose updates and design a parametrization strategy that is\nindependent of the robot structure. Finally, we show experimental results on\nexisting benchmark datasets for four different robots and demonstrate that our\nmethod significantly outperforms the state of the art. Code and pre-trained\nmodels are available on the project webpage\nhttps://www.di.ens.fr/willow/research/robopose/.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 14:48:29 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Labb\u00e9", "Yann", ""], ["Carpentier", "Justin", ""], ["Aubry", "Mathieu", ""], ["Sivic", "Josef", ""]]}, {"id": "2104.09367", "submitter": "Haiyan Wu", "authors": "Haiyan Wu, Yanyun Qu, Shaohui Lin, Jian Zhou, Ruizhi Qiao, Zhizhong\n  Zhang, Yuan Xie, Lizhuang Ma", "title": "Contrastive Learning for Compact Single Image Dehazing", "comments": "CVPR 2021 accepted paper. Code: https://github.com/GlassyWu/AECR-Net", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Single image dehazing is a challenging ill-posed problem due to the severe\ninformation degeneration. However, existing deep learning based dehazing\nmethods only adopt clear images as positive samples to guide the training of\ndehazing network while negative information is unexploited. Moreover, most of\nthem focus on strengthening the dehazing network with an increase of depth and\nwidth, leading to a significant requirement of computation and memory. In this\npaper, we propose a novel contrastive regularization (CR) built upon\ncontrastive learning to exploit both the information of hazy images and clear\nimages as negative and positive samples, respectively. CR ensures that the\nrestored image is pulled to closer to the clear image and pushed to far away\nfrom the hazy image in the representation space. Furthermore, considering\ntrade-off between performance and memory storage, we develop a compact dehazing\nnetwork based on autoencoder-like (AE) framework. It involves an adaptive mixup\noperation and a dynamic feature enhancement module, which can benefit from\npreserving information flow adaptively and expanding the receptive field to\nimprove the network's transformation capability, respectively. We term our\ndehazing network with autoencoder and contrastive regularization as AECR-Net.\nThe extensive experiments on synthetic and real-world datasets demonstrate that\nour AECR-Net surpass the state-of-the-art approaches. The code is released in\nhttps://github.com/GlassyWu/AECR-Net.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 14:56:21 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Wu", "Haiyan", ""], ["Qu", "Yanyun", ""], ["Lin", "Shaohui", ""], ["Zhou", "Jian", ""], ["Qiao", "Ruizhi", ""], ["Zhang", "Zhizhong", ""], ["Xie", "Yuan", ""], ["Ma", "Lizhuang", ""]]}, {"id": "2104.09370", "submitter": "Sudeep Katakol", "authors": "Sudeep Katakol, Luis Herranz, Fei Yang and Marta Mrak", "title": "DANICE: Domain adaptation without forgetting in neural image compression", "comments": "Accepted to CLIC Workshop at CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural image compression (NIC) is a new coding paradigm where coding\ncapabilities are captured by deep models learned from data. This data-driven\nnature enables new potential functionalities. In this paper, we study the\nadaptability of codecs to custom domains of interest. We show that NIC codecs\nare transferable and that they can be adapted with relatively few target domain\nimages. However, naive adaptation interferes with the solution optimized for\nthe original source domain, resulting in forgetting the original coding\ncapabilities in that domain, and may even break the compatibility with\npreviously encoded bitstreams. Addressing these problems, we propose Codec\nAdaptation without Forgetting (CAwF), a framework that can avoid these problems\nby adding a small amount of custom parameters, where the source codec remains\nembedded and unchanged during the adaptation process. Experiments demonstrate\nits effectiveness and provide useful insights on the characteristics of\ncatastrophic interference in NIC.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 14:58:37 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Katakol", "Sudeep", ""], ["Herranz", "Luis", ""], ["Yang", "Fei", ""], ["Mrak", "Marta", ""]]}, {"id": "2104.09375", "submitter": "Burak Ekim", "authors": "Burak Ekim, Elif Sertel", "title": "A Multi-Task Deep Learning Framework for Building Footprint Segmentation", "comments": "International Geoscience and Remote Sensing Symposium (IGARSS), Jul\n  2021, Brussels, Belgium", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The task of building footprint segmentation has been well-studied in the\ncontext of remote sensing (RS) as it provides valuable information in many\naspects, however, difficulties brought by the nature of RS images such as\nvariations in the spatial arrangements and in-consistent constructional\npatterns require studying further, since it often causes poorly classified\nsegmentation maps. We address this need by designing a joint optimization\nscheme for the task of building footprint delineation and introducing two\nauxiliary tasks; image reconstruction and building footprint boundary\nsegmentation with the intent to reveal the common underlying structure to\nadvance the classification accuracy of a single task model under the favor of\nauxiliary tasks. In particular, we propose a deep multi-task learning (MTL)\nbased unified fully convolutional framework which operates in an end-to-end\nmanner by making use of joint loss function with learnable loss weights\nconsidering the homoscedastic uncertainty of each task loss. Experimental\nresults conducted on the SpaceNet6 dataset demonstrate the potential of the\nproposed MTL framework as it improves the classification accuracy considerably\ncompared to single-task and lesser compounded tasks.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 15:07:27 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Ekim", "Burak", ""], ["Sertel", "Elif", ""]]}, {"id": "2104.09378", "submitter": "Mansi Sharma", "authors": "Joshitha R and Mansi Sharma", "title": "A Hierarchical Coding Scheme for Glasses-free 3D Displays Based on\n  Scalable Hybrid Layered Representation of Real-World Light Fields", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel hierarchical coding scheme for light fields based\non transmittance patterns of low-rank multiplicative layers and Fourier\ndisparity layers. The proposed scheme learns stacked multiplicative layers from\nsubsets of light field views determined from different scanning orders. The\nmultiplicative layers are optimized using a fast data-driven convolutional\nneural network (CNN). The spatial correlation in layer patterns is exploited\nwith varying low ranks in factorization derived from singular value\ndecomposition on a Krylov subspace. Further, encoding with HEVC efficiently\nremoves intra-view and inter-view correlation in low-rank approximated layers.\nThe initial subset of approximated decoded views from multiplicative\nrepresentation is used to construct Fourier disparity layer (FDL)\nrepresentation. The FDL model synthesizes second subset of views which is\nidentified by a pre-defined hierarchical prediction order. The correlations\nbetween the prediction residue of synthesized views is further eliminated by\nencoding the residual signal. The set of views obtained from decoding the\nresidual is employed in order to refine the FDL model and predict the next\nsubset of views with improved accuracy. This hierarchical procedure is repeated\nuntil all light field views are encoded. The critical advantage of proposed\nhybrid layered representation and coding scheme is that it utilizes not just\nspatial and temporal redundancies, but efficiently exploits the strong\nintrinsic similarities among neighboring sub-aperture images in both horizontal\nand vertical directions as specified by different predication orders. Besides,\nthe scheme is flexible to realize a range of multiple bitrates at the decoder\nwithin a single integrated system. The compression performance analyzed with\nreal light field shows substantial bitrate savings, maintaining good\nreconstruction quality.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 15:09:21 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["R", "Joshitha", ""], ["Sharma", "Mansi", ""]]}, {"id": "2104.09379", "submitter": "Yihang Yin", "authors": "Yihang Yin, Siyu Huang, Xiang Zhang, Dejing Dou", "title": "BM-NAS: Bilevel Multimodal Neural Architecture Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) have shown superior performances on various\nmultimodal learning problems. However, it often requires huge efforts to adapt\nDNNs to individual multimodal tasks by manually engineering unimodal features\nand designing multimodal feature fusion strategies. This paper proposes Bilevel\nMultimodal Neural Architecture Search (BM-NAS) framework, which makes the\narchitecture of multimodal fusion models fully searchable via a bilevel\nsearching scheme. At the upper level, BM-NAS selects the inter/intra-modal\nfeature pairs from the pretrained unimodal backbones. At the lower level,\nBM-NAS learns the fusion strategy for each feature pair, which is a combination\nof predefined primitive operations. The primitive operations are elaborately\ndesigned and they can be flexibly combined to accommodate various effective\nfeature fusion modules such as multi-head attention (Transformer) and Attention\non Attention (AoA). Experimental results on three multimodal tasks demonstrate\nthe effectiveness and efficiency of the proposed BM-NAS framework. BM-NAS\nachieves competitive performances with much less search time and fewer model\nparameters in comparison with the existing generalized multimodal NAS methods.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 15:09:49 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Yin", "Yihang", ""], ["Huang", "Siyu", ""], ["Zhang", "Xiang", ""], ["Dou", "Dejing", ""]]}, {"id": "2104.09386", "submitter": "S M A Sharif", "authors": "SMA Sharif, Rizwan Ali Naqvi, Mithun Biswas, and Kim Sungjun", "title": "A Two-stage Deep Network for High Dynamic Range Image Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Mapping a single exposure low dynamic range (LDR) image into a high dynamic\nrange (HDR) is considered among the most strenuous image to image translation\ntasks due to exposure-related missing information. This study tackles the\nchallenges of single-shot LDR to HDR mapping by proposing a novel two-stage\ndeep network. Notably, our proposed method aims to reconstruct an HDR image\nwithout knowing hardware information, including camera response function (CRF)\nand exposure settings. Therefore, we aim to perform image enhancement task like\ndenoising, exposure correction, etc., in the first stage. Additionally, the\nsecond stage of our deep network learns tone mapping and bit-expansion from a\nconvex set of data samples. The qualitative and quantitative comparisons\ndemonstrate that the proposed method can outperform the existing LDR to HDR\nworks with a marginal difference. Apart from that, we collected an LDR image\ndataset incorporating different camera systems. The evaluation with our\ncollected real-world LDR images illustrates that the proposed method can\nreconstruct plausible HDR images without presenting any visual artefacts. Code\navailable: https://github. com/sharif-apu/twostageHDR_NTIRE21.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 15:19:17 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Sharif", "SMA", ""], ["Naqvi", "Rizwan Ali", ""], ["Biswas", "Mithun", ""], ["Sungjun", "Kim", ""]]}, {"id": "2104.09398", "submitter": "S M A Sharif", "authors": "SMA Sharif, and Rizwan Ali Naqvi, and Mithun Biswas", "title": "Beyond Joint Demosaicking and Denoising: An Image Processing Pipeline\n  for a Pixel-bin Image Sensor", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Pixel binning is considered one of the most prominent solutions to tackle the\nhardware limitation of smartphone cameras. Despite numerous advantages, such an\nimage sensor has to appropriate an artefact-prone non-Bayer colour filter array\n(CFA) to enable the binning capability. Contrarily, performing essential image\nsignal processing (ISP) tasks like demosaicking and denoising, explicitly with\nsuch CFA patterns, makes the reconstruction process notably complicated. In\nthis paper, we tackle the challenges of joint demosaicing and denoising (JDD)\non such an image sensor by introducing a novel learning-based method. The\nproposed method leverages the depth and spatial attention in a deep network.\nThe proposed network is guided by a multi-term objective function, including\ntwo novel perceptual losses to produce visually plausible images. On top of\nthat, we stretch the proposed image processing pipeline to comprehensively\nreconstruct and enhance the images captured with a smartphone camera, which\nuses pixel binning techniques. The experimental results illustrate that the\nproposed method can outperform the existing methods by a noticeable margin in\nqualitative and quantitative comparisons. Code available:\nhttps://github.com/sharif-apu/BJDD_CVPR21.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 15:41:28 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Sharif", "SMA", ""], ["Naqvi", "Rizwan Ali", ""], ["Biswas", "Mithun", ""]]}, {"id": "2104.09403", "submitter": "Ankur Mali", "authors": "Shivansh Rao and Vikas Kumar and Daniel Kifer and Lee Giles and Ankur\n  Mali", "title": "OmniLayout: Room Layout Reconstruction from Indoor Spherical Panoramas", "comments": "Accepted at CVPR, OmniCV Workshop. 10 Pages, 9 Figures, 6 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Given a single RGB panorama, the goal of 3D layout reconstruction is to\nestimate the room layout by predicting the corners, floor boundary, and ceiling\nboundary. A common approach has been to use standard convolutional networks to\npredict the corners and boundaries, followed by post-processing to generate the\n3D layout. However, the space-varying distortions in panoramic images are not\ncompatible with the translational equivariance property of standard\nconvolutions, thus degrading performance. Instead, we propose to use spherical\nconvolutions. The resulting network, which we call OmniLayout performs\nconvolutions directly on the sphere surface, sampling according to inverse\nequirectangular projection and hence invariant to equirectangular distortions.\nUsing a new evaluation metric, we show that our network reduces the error in\nthe heavily distorted regions (near the poles) by approx 25 % when compared to\nstandard convolutional networks. Experimental results show that OmniLayout\noutperforms the state-of-the-art by approx 4% on two different benchmark\ndatasets (PanoContext and Stanford 2D-3D). Code is available at\nhttps://github.com/rshivansh/OmniLayout.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 15:44:10 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Rao", "Shivansh", ""], ["Kumar", "Vikas", ""], ["Kifer", "Daniel", ""], ["Giles", "Lee", ""], ["Mali", "Ankur", ""]]}, {"id": "2104.09411", "submitter": "Yong Liu Stephen", "authors": "Chenyi Lei, Shixian Luo, Yong Liu, Wanggui He, Jiamang Wang, Guoxin\n  Wang, Haihong Tang, Chunyan Miao, Houqiang Li", "title": "Understanding Chinese Video and Language via Contrastive Multimodal\n  Pre-Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The pre-trained neural models have recently achieved impressive performances\nin understanding multimodal content. However, it is still very challenging to\npre-train neural models for video and language understanding, especially for\nChinese video-language data, due to the following reasons. Firstly, existing\nvideo-language pre-training algorithms mainly focus on the co-occurrence of\nwords and video frames, but ignore other valuable semantic and structure\ninformation of video-language content, e.g., sequential order and\nspatiotemporal relationships. Secondly, there exist conflicts between video\nsentence alignment and other proxy tasks. Thirdly, there is a lack of\nlarge-scale and high-quality Chinese video-language datasets (e.g., including\n10 million unique videos), which are the fundamental success conditions for\npre-training techniques.\n  In this work, we propose a novel video-language understanding framework named\nVICTOR, which stands for VIdeo-language understanding via Contrastive\nmulTimOdal pRe-training. Besides general proxy tasks such as masked language\nmodeling, VICTOR constructs several novel proxy tasks under the contrastive\nlearning paradigm, making the model be more robust and able to capture more\ncomplex multimodal semantic and structural relationships from different\nperspectives. VICTOR is trained on a large-scale Chinese video-language\ndataset, including over 10 million complete videos with corresponding\nhigh-quality textual descriptions. We apply the pre-trained VICTOR model to a\nseries of downstream applications and demonstrate its superior performances,\ncomparing against the state-of-the-art pre-training methods such as VideoBERT\nand UniVL. The codes and trained checkpoints will be publicly available to\nnourish further developments of the research community.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 15:58:45 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Lei", "Chenyi", ""], ["Luo", "Shixian", ""], ["Liu", "Yong", ""], ["He", "Wanggui", ""], ["Wang", "Jiamang", ""], ["Wang", "Guoxin", ""], ["Tang", "Haihong", ""], ["Miao", "Chunyan", ""], ["Li", "Houqiang", ""]]}, {"id": "2104.09415", "submitter": "Guanbin Li", "authors": "Jichang Li, Guanbin Li, Yemin Shi, Yizhou Yu", "title": "Cross-Domain Adaptive Clustering for Semi-Supervised Domain Adaptation", "comments": "To appear in CVPR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In semi-supervised domain adaptation, a few labeled samples per class in the\ntarget domain guide features of the remaining target samples to aggregate\naround them. However, the trained model cannot produce a highly discriminative\nfeature representation for the target domain because the training data is\ndominated by labeled samples from the source domain. This could lead to\ndisconnection between the labeled and unlabeled target samples as well as\nmisalignment between unlabeled target samples and the source domain. In this\npaper, we propose a novel approach called Cross-domain Adaptive Clustering to\naddress this problem. To achieve both inter-domain and intra-domain adaptation,\nwe first introduce an adversarial adaptive clustering loss to group features of\nunlabeled target data into clusters and perform cluster-wise feature alignment\nacross the source and target domains. We further apply pseudo labeling to\nunlabeled samples in the target domain and retain pseudo-labels with high\nconfidence. Pseudo labeling expands the number of ``labeled\" samples in each\nclass in the target domain, and thus produces a more robust and powerful\ncluster core for each class to facilitate adversarial learning. Extensive\nexperiments on benchmark datasets, including DomainNet, Office-Home and Office,\ndemonstrate that our proposed approach achieves the state-of-the-art\nperformance in semi-supervised domain adaptation.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 16:07:32 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Li", "Jichang", ""], ["Li", "Guanbin", ""], ["Shi", "Yemin", ""], ["Yu", "Yizhou", ""]]}, {"id": "2104.09425", "submitter": "Vikash Sehwag", "authors": "Vikash Sehwag, Saeed Mahloujifar, Tinashe Handina, Sihui Dai, Chong\n  Xiang, Mung Chiang, Prateek Mittal", "title": "Improving Adversarial Robustness Using Proxy Distributions", "comments": "24 pages, 5 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We focus on the use of proxy distributions, i.e., approximations of the\nunderlying distribution of the training dataset, in both understanding and\nimproving the adversarial robustness in image classification. While additional\ntraining data helps in adversarial training, curating a very large number of\nreal-world images is challenging. In contrast, proxy distributions enable us to\nsample a potentially unlimited number of images and improve adversarial\nrobustness using these samples. We first ask the question: when does\nadversarial robustness benefit from incorporating additional samples from the\nproxy distribution in the training stage? We prove that the difference between\nthe robustness of a classifier on the proxy and original training dataset\ndistribution is upper bounded by the conditional Wasserstein distance between\nthem. Our result confirms the intuition that samples from a proxy distribution\nthat closely approximates training dataset distribution should be able to boost\nadversarial robustness. Motivated by this finding, we leverage samples from\nstate-of-the-art generative models, which can closely approximate training data\ndistribution, to improve robustness. In particular, we improve robust accuracy\nby up to 6.1% and 5.7% in $l_{\\infty}$ and $l_2$ threat model, and certified\nrobust accuracy by 6.7% over baselines not using proxy distributions on the\nCIFAR-10 dataset. Since we can sample an unlimited number of images from a\nproxy distribution, it also allows us to investigate the effect of an\nincreasing number of training samples on adversarial robustness. Here we\nprovide the first large scale empirical investigation of accuracy vs robustness\ntrade-off and sample complexity of adversarial training by training deep neural\nnetworks on 2K to 10M images.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 16:17:12 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Sehwag", "Vikash", ""], ["Mahloujifar", "Saeed", ""], ["Handina", "Tinashe", ""], ["Dai", "Sihui", ""], ["Xiang", "Chong", ""], ["Chiang", "Mung", ""], ["Mittal", "Prateek", ""]]}, {"id": "2104.09435", "submitter": "Jong Chul Ye", "authors": "Hyoungjun Park, Myeongsu Na, Bumju Kim, Soohyun Park, Ki Hean Kim,\n  Sunghoe Chang, and Jong Chul Ye", "title": "Deep learning enables reference-free isotropic super-resolution for\n  volumetric fluorescence microscopy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Volumetric imaging by fluorescence microscopy is often limited by anisotropic\nspatial resolution from inferior axial resolution compared to the lateral\nresolution. To address this problem, here we present a deep-learning-enabled\nunsupervised super-resolution technique that enhances anisotropic images in\nvolumetric fluorescence microscopy. In contrast to the existing deep learning\napproaches that require matched high-resolution target volume images, our\nmethod greatly reduces the effort to put into practice as the training of a\nnetwork requires as little as a single 3D image stack, without a priori\nknowledge of the image formation process, registration of training data, or\nseparate acquisition of target data. This is achieved based on the optimal\ntransport driven cycle-consistent generative adversarial network that learns\nfrom an unpaired matching between high-resolution 2D images in lateral image\nplane and low-resolution 2D images in the other planes. Using fluorescence\nconfocal microscopy and light-sheet microscopy, we demonstrate that the trained\nnetwork not only enhances axial resolution, but also restores suppressed visual\ndetails between the imaging planes and removes imaging artifacts.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 16:31:12 GMT"}, {"version": "v2", "created": "Mon, 7 Jun 2021 02:45:47 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Park", "Hyoungjun", ""], ["Na", "Myeongsu", ""], ["Kim", "Bumju", ""], ["Park", "Soohyun", ""], ["Kim", "Ki Hean", ""], ["Chang", "Sunghoe", ""], ["Ye", "Jong Chul", ""]]}, {"id": "2104.09441", "submitter": "Chao Liang", "authors": "Chao Liang and Zhipeng Zhang and Xue Zhou and Bing Li and Yi Lu and\n  Weiming Hu", "title": "One More Check: Making \"Fake Background\" Be Tracked Again", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The one-shot multi-object tracking, which integrates object detection and ID\nembedding extraction into a unified network, has achieved groundbreaking\nresults in recent years. However, current one-shot trackers solely rely on\nsingle-frame detections to predict candidate bounding boxes, which may be\nunreliable when facing disastrous visual degradation, e.g., motion blur,\nocclusions. Once a target bounding box is mistakenly classified as background\nby the detector, the temporal consistency of its corresponding tracklet will be\nno longer maintained, as shown in Fig. 1. In this paper, we set out to restore\nthe misclassified bounding boxes, i.e., fake background, by proposing a\nre-check network. The re-check network propagates previous tracklets to the\ncurrent frame by exploring the relation between cross-frame temporal cues and\ncurrent candidates using the modified cross-correlation layer. The propagation\nresults help to reload the \"fake background\" and eventually repair the broken\ntracklets. By inserting the re-check network to a strong baseline tracker\nCSTrack (a variant of JDE), our model achieves favorable gains by $70.7\n\\rightarrow 76.7$, $70.6 \\rightarrow 76.3$ MOTA on MOT16 and MOT17,\nrespectively. Code is publicly available at https://github.com/JudasDie/SOTS.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 16:42:47 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Liang", "Chao", ""], ["Zhang", "Zhipeng", ""], ["Zhou", "Xue", ""], ["Li", "Bing", ""], ["Lu", "Yi", ""], ["Hu", "Weiming", ""]]}, {"id": "2104.09447", "submitter": "Guy Ben-Yosef", "authors": "Guy Ben-Yosef, Gabriel Kreiman, Shimon Ullman", "title": "What can human minimal videos tell us about dynamic recognition models?", "comments": "Published as a workshop paper at Bridging AI and Cognitive Science\n  (ICLR 2020). Extended paper was published at Cognition", "journal-ref": null, "doi": "10.1016/j.cognition.2020.104263", "report-no": null, "categories": "cs.CV cs.AI q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In human vision objects and their parts can be visually recognized from\npurely spatial or purely temporal information but the mechanisms integrating\nspace and time are poorly understood. Here we show that human visual\nrecognition of objects and actions can be achieved by efficiently combining\nspatial and motion cues in configurations where each source on its own is\ninsufficient for recognition. This analysis is obtained by identifying minimal\nvideos: these are short and tiny video clips in which objects, parts, and\nactions can be reliably recognized, but any reduction in either space or time\nmakes them unrecognizable. State-of-the-art deep networks for dynamic visual\nrecognition cannot replicate human behavior in these configurations. This gap\nbetween humans and machines points to critical mechanisms in human dynamic\nvision that are lacking in current models.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 16:53:25 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Ben-Yosef", "Guy", ""], ["Kreiman", "Gabriel", ""], ["Ullman", "Shimon", ""]]}, {"id": "2104.09453", "submitter": "Jing Liang", "authors": "Jing Liang, Li Niu, Liqing Zhang", "title": "Inharmonious Region Localization", "comments": "12 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The advance of image editing techniques allows users to create artistic\nworks, but the manipulated regions may be incompatible with the background.\nLocalizing the inharmonious region is an appealing yet challenging task.\nRealizing that this task requires effective aggregation of multi-scale\ncontextual information and suppression of redundant information, we design\nnovel Bi-directional Feature Integration (BFI) block and Global-context Guided\nDecoder (GGD) block to fuse multi-scale features in the encoder and decoder\nrespectively. We also employ Mask-guided Dual Attention (MDA) block between the\nencoder and decoder to suppress the redundant information. Experiments on the\nimage harmonization dataset demonstrate that our method achieves competitive\nperformance for inharmonious region localization. The source code is available\nat https://github.com/bcmi/DIRL.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 17:12:58 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Liang", "Jing", ""], ["Niu", "Li", ""], ["Zhang", "Liqing", ""]]}, {"id": "2104.09457", "submitter": "Zhen Wei", "authors": "Zhen Wei, Bingkun Liu, Weinong Wang, Yu-Wing Tai", "title": "Few-Shot Model Adaptation for Customized Facial Landmark Detection,\n  Segmentation, Stylization and Shadow Removal", "comments": "with supplementary file", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Despite excellent progress has been made, the performance of deep learning\nbased algorithms still heavily rely on specific datasets, which are difficult\nto extend due to labor-intensive labeling. Moreover, because of the advancement\nof new applications, initial definition of data annotations might not always\nmeet the requirements of new functionalities. Thus, there is always a great\ndemand in customized data annotations. To address the above issues, we propose\nthe Few-Shot Model Adaptation (FSMA) framework and demonstrate its potential on\nseveral important tasks on Faces. The FSMA first acquires robust facial image\nembeddings by training an adversarial auto-encoder using large-scale unlabeled\ndata. Then the model is equipped with feature adaptation and fusion layers, and\nadapts to the target task efficiently using a minimal amount of annotated\nimages. The FSMA framework is prominent in its versatility across a wide range\nof facial image applications. The FSMA achieves state-of-the-art few-shot\nlandmark detection performance and it offers satisfying solutions for few-shot\nface segmentation, stylization and facial shadow removal tasks for the first\ntime.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 17:15:56 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Wei", "Zhen", ""], ["Liu", "Bingkun", ""], ["Wang", "Weinong", ""], ["Tai", "Yu-Wing", ""]]}, {"id": "2104.09461", "submitter": "Xiang Xiang", "authors": "Xin Wei, Runqi Qiu, Houyu Yu, Yurun Yang, Haoyu Tian, Xiang Xiang", "title": "Entropy-based Optimization via A* Algorithm for Parking Space\n  Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This paper addresses the path planning problems for recommending parking\nspaces, given the difficulties of identifying the most optimal route to vacant\nparking spaces and the shortest time to leave the parking space. Our\noptimization approach is based on the entropy method and realized by the A*\nalgorithm. Experiments have shown that the combination of A* and the entropy\nvalue induces the optimal parking solution with the shortest route while being\nrobust to environmental factors.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 17:24:51 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Wei", "Xin", ""], ["Qiu", "Runqi", ""], ["Yu", "Houyu", ""], ["Yang", "Yurun", ""], ["Tian", "Haoyu", ""], ["Xiang", "Xiang", ""]]}, {"id": "2104.09467", "submitter": "Michalis Lazarou Mr", "authors": "Michalis Lazarou, Yannis Avrithis, Tania Stathaki", "title": "Few-shot learning via tensor hallucination", "comments": "Accepted as oral at ICLR2021 workshop: \"Synthetic Data Generation:\n  Quality, Privacy, Bias\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Few-shot classification addresses the challenge of classifying examples given\nonly limited labeled data. A powerful approach is to go beyond data\naugmentation, towards data synthesis. However, most of data\naugmentation/synthesis methods for few-shot classification are overly complex\nand sophisticated, e.g. training a wGAN with multiple regularizers or training\na network to transfer latent diversities from known to novel classes. We make\ntwo contributions, namely we show that: (1) using a simple loss function is\nmore than enough for training a feature generator in the few-shot setting; and\n(2) learning to generate tensor features instead of vector features is\nsuperior. Extensive experiments on miniImagenet, CUB and CIFAR-FS datasets show\nthat our method sets a new state of the art, outperforming more sophisticated\nfew-shot data augmentation methods.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 17:30:33 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Lazarou", "Michalis", ""], ["Avrithis", "Yannis", ""], ["Stathaki", "Tania", ""]]}, {"id": "2104.09493", "submitter": "Megh Shukla", "authors": "Megh Shukla", "title": "EGL++: Extending Expected Gradient Length to Active Learning for Human\n  Pose Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  State of the art human pose estimation models continue to rely on large\nquantities of labelled data for robust performance. Since labelling budget is\noften constrained, active learning algorithms are important in retaining the\noverall performance of the model at a lower cost. Although active learning has\nbeen well studied in literature, few techniques are reported for human pose\nestimation. In this paper, we theoretically derive expected gradient length for\nregression, and propose EGL++, a novel heuristic algorithm that extends\nexpected gradient length to tasks where discrete labels are not available. We\nachieve this by computing low dimensional representations of the original\nimages which are then used to form a neighborhood graph. We use this graph to:\n1) Obtain a set of neighbors for a given sample, with each neighbor iteratively\nassumed to represent the ground truth for gradient calculation 2) Quantify the\nprobability of each sample being a neighbor in the above set, facilitating the\nexpected gradient step. Such an approach allows us to provide an approximate\nsolution to the otherwise intractable task of integrating over the continuous\noutput domain. To validate EGL++, we use the same datasets (Leeds Sports Pose,\nMPII) and experimental design as suggested by previous literature, achieving\ncompetitive results in comparison to these methods.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 17:56:59 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Shukla", "Megh", ""]]}, {"id": "2104.09496", "submitter": "Chuhan Zhang", "authors": "Chuhan Zhang, Ankush Gupta, Andrew Zisserman", "title": "Temporal Query Networks for Fine-grained Video Understanding", "comments": "Accepted to CVPR 2021(Oral). Project page:\n  http://www.robots.ox.ac.uk/~vgg/research/tqn/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our objective in this work is fine-grained classification of actions in\nuntrimmed videos, where the actions may be temporally extended or may span only\na few frames of the video. We cast this into a query-response mechanism, where\neach query addresses a particular question, and has its own response label set.\nWe make the following four contributions: (I) We propose a new model - a\nTemporal Query Network - which enables the query-response functionality, and a\nstructural understanding of fine-grained actions. It attends to relevant\nsegments for each query with a temporal attention mechanism, and can be trained\nusing only the labels for each query. (ii) We propose a new way - stochastic\nfeature bank update - to train a network on videos of various lengths with the\ndense sampling required to respond to fine-grained queries. (iii) We compare\nthe TQN to other architectures and text supervision methods, and analyze their\npros and cons. Finally, (iv) we evaluate the method extensively on the FineGym\nand Diving48 benchmarks for fine-grained action classification and surpass the\nstate-of-the-art using only RGB features.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 17:58:48 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Zhang", "Chuhan", ""], ["Gupta", "Ankush", ""], ["Zisserman", "Andrew", ""]]}, {"id": "2104.09497", "submitter": "Haoyu Chen", "authors": "Haoyu Chen, Jinjin Gu, Zhi Zhang", "title": "Attention in Attention Network for Image Super-Resolution", "comments": "10 pages, 8 figures. Codes will be available at\n  $\\href{https://github.com/haoyuc/A2N}{\\text{this https URL}}$", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks have allowed remarkable advances in single\nimage super-resolution (SISR) over the last decade. Among recent advances in\nSISR, attention mechanisms are crucial for high performance SR models. However,\nfew works really discuss why attention works and how it works. In this work, we\nattempt to quantify and visualize the static attention mechanisms and show that\nnot all attention modules are equally beneficial. We then propose attention in\nattention network (A$^2$N) for highly accurate image SR. Specifically, our\nA$^2$N consists of a non-attention branch and a coupling attention branch.\nAttention dropout module is proposed to generate dynamic attention weights for\nthese two branches based on input features that can suppress unwanted attention\nadjustments. This allows attention modules to specialize to beneficial examples\nwithout otherwise penalties and thus greatly improve the capacity of the\nattention network with little parameter overhead. Experiments have demonstrated\nthat our model could achieve superior trade-off performances comparing with\nstate-of-the-art lightweight networks. Experiments on local attribution maps\nalso prove attention in attention (A$^2$) structure can extract features from a\nwider range.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 17:59:06 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Chen", "Haoyu", ""], ["Gu", "Jinjin", ""], ["Zhang", "Zhi", ""]]}, {"id": "2104.09498", "submitter": "Daniel Geng", "authors": "Daniel Geng, Andrew Owens", "title": "Comparing Correspondences: Video Prediction with Correspondence-wise\n  Losses", "comments": "Website at http://dangeng.github.io/CorrWiseLosses", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Today's image prediction methods struggle to change the locations of objects\nin a scene, producing blurry images that average over the many positions they\nmight occupy. In this paper, we propose a simple change to existing image\nsimilarity metrics that makes them more robust to positional errors: we match\nthe images using optical flow, then measure the visual similarity of\ncorresponding pixels. This change leads to crisper and more perceptually\naccurate predictions, and can be used with any image prediction network. We\napply our method to predicting future frames of a video, where it obtains\nstrong performance with simple, off-the-shelf architectures.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 17:59:29 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Geng", "Daniel", ""], ["Owens", "Andrew", ""]]}, {"id": "2104.09556", "submitter": "Ruicheng Feng", "authors": "Ruicheng Feng, Chongyi Li, Huaijin Chen, Shuai Li, Chen Change Loy,\n  Jinwei Gu", "title": "Removing Diffraction Image Artifacts in Under-Display Camera via Dynamic\n  Skip Connection Network", "comments": "CVPR 2021 camera-ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent development of Under-Display Camera (UDC) systems provides a true\nbezel-less and notch-free viewing experience on smartphones (and TV, laptops,\ntablets), while allowing images to be captured from the selfie camera embedded\nunderneath. In a typical UDC system, the microstructure of the semi-transparent\norganic light-emitting diode (OLED) pixel array attenuates and diffracts the\nincident light on the camera, resulting in significant image quality\ndegradation. Oftentimes, noise, flare, haze, and blur can be observed in UDC\nimages. In this work, we aim to analyze and tackle the aforementioned\ndegradation problems. We define a physics-based image formation model to better\nunderstand the degradation. In addition, we utilize one of the world's first\ncommodity UDC smartphone prototypes to measure the real-world Point Spread\nFunction (PSF) of the UDC system, and provide a model-based data synthesis\npipeline to generate realistically degraded images. We specially design a new\ndomain knowledge-enabled Dynamic Skip Connection Network (DISCNet) to restore\nthe UDC images. We demonstrate the effectiveness of our method through\nextensive experiments on both synthetic and real UDC data. Our physics-based\nimage formation model and proposed DISCNet can provide foundations for further\nexploration in UDC image restoration, and even for general diffraction artifact\nremoval in a broader sense.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 18:41:45 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Feng", "Ruicheng", ""], ["Li", "Chongyi", ""], ["Chen", "Huaijin", ""], ["Li", "Shuai", ""], ["Loy", "Chen Change", ""], ["Gu", "Jinwei", ""]]}, {"id": "2104.09563", "submitter": "Romain Dupuis", "authors": "Madalina Ciortan, Romain Dupuis, Thomas Peel", "title": "A Framework using Contrastive Learning for Classification with Noisy\n  Labels", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a framework using contrastive learning as a pre-training task to\nperform image classification in the presence of noisy labels. Recent strategies\nsuch as pseudo-labeling, sample selection with Gaussian Mixture models,\nweighted supervised contrastive learning have been combined into a fine-tuning\nphase following the pre-training. This paper provides an extensive empirical\nstudy showing that a preliminary contrastive learning step brings a significant\ngain in performance when using different loss functions: non-robust, robust,\nand early-learning regularized. Our experiments performed on standard\nbenchmarks and real-world datasets demonstrate that: i) the contrastive\npre-training increases the robustness of any loss function to noisy labels and\nii) the additional fine-tuning phase can further improve accuracy but at the\ncost of additional complexity.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 18:51:22 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Ciortan", "Madalina", ""], ["Dupuis", "Romain", ""], ["Peel", "Thomas", ""]]}, {"id": "2104.09568", "submitter": "Arindam Chaudhuri Arc", "authors": "Aashna Ahuja, Arindam Chaudhuri", "title": "Detecting Vehicle Type and License Plate Number of different Vehicles on\n  Images", "comments": "Present Research Work in Progress", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With ever increasing number of vehicles, vehicular tracking is one of the\nmajor challenges faced by urban areas. In this paper we try to develop a model\nthat can locate a particular vehicle that the user is looking for depending on\ntwo factors 1. the Type of vehicle and the 2. License plate number of the car.\nThe proposed system uses a unique mixture consisting of Mask R-CNN model for\nvehicle type detection, WpodNet and pytesseract for License Plate detection and\nPrediction of letters in it.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 19:51:17 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Ahuja", "Aashna", ""], ["Chaudhuri", "Arindam", ""]]}, {"id": "2104.09580", "submitter": "Jialu Li", "authors": "Jialu Li, Hao Tan, Mohit Bansal", "title": "Improving Cross-Modal Alignment in Vision Language Navigation via\n  Syntactic Information", "comments": "NAACL 2021 (10 pages)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vision language navigation is the task that requires an agent to navigate\nthrough a 3D environment based on natural language instructions. One key\nchallenge in this task is to ground instructions with the current visual\ninformation that the agent perceives. Most of the existing work employs soft\nattention over individual words to locate the instruction required for the next\naction. However, different words have different functions in a sentence (e.g.,\nmodifiers convey attributes, verbs convey actions). Syntax information like\ndependencies and phrase structures can aid the agent to locate important parts\nof the instruction. Hence, in this paper, we propose a navigation agent that\nutilizes syntax information derived from a dependency tree to enhance alignment\nbetween the instruction and the current visual scenes. Empirically, our agent\noutperforms the baseline model that does not use syntax information on the\nRoom-to-Room dataset, especially in the unseen environment. Besides, our agent\nachieves the new state-of-the-art on Room-Across-Room dataset, which contains\ninstructions in 3 languages (English, Hindi, and Telugu). We also show that our\nagent is better at aligning instructions with the current visual information\nvia qualitative visualizations. Code and models:\nhttps://github.com/jialuli-luka/SyntaxVLN\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 19:18:41 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Li", "Jialu", ""], ["Tan", "Hao", ""], ["Bansal", "Mohit", ""]]}, {"id": "2104.09587", "submitter": "Yan Xia", "authors": "Yaqi Xia, Yan Xia, Wei Li, Rui Song, Kailang Cao, Uwe Stilla", "title": "ASFM-Net: Asymmetrical Siamese Feature Matching Network for Point\n  Completion", "comments": "This work achieves the 1st place in the leaderboard of Completion3D", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We tackle the problem of object completion from point clouds and propose a\nnovel point cloud completion network employing an asymmetrical Siamese feature\nmatching strategy, termed as ASFM-Net. Specifically, the asymmetrical Siamese\nauto-encoder neural network is adopted to map the partial and complete input\npoint cloud into a shared latent space, which can capture detailed shape prior.\nThen we design an iterative refinement unit to generate complete shapes with\nfine-grained details by integrating prior information. Experiments are\nconducted on PCN dataset and Completion3D benchmark, demonstrating the\nstate-of-the-art performance of the proposed ASFM-Net. Our method achieves the\n1st place in the leaderboard of Completion3D and outperforms existing methods\nwith a large margin, about 12%. Codes will be open-sourced.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 19:42:42 GMT"}, {"version": "v2", "created": "Fri, 23 Apr 2021 08:13:54 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Xia", "Yaqi", ""], ["Xia", "Yan", ""], ["Li", "Wei", ""], ["Song", "Rui", ""], ["Cao", "Kailang", ""], ["Stilla", "Uwe", ""]]}, {"id": "2104.09621", "submitter": "Karl Willis", "authors": "Karl D.D. Willis, Pradeep Kumar Jayaraman, Joseph G. Lambourne, Hang\n  Chu, Yewen Pu", "title": "Engineering Sketch Generation for Computer-Aided Design", "comments": "The 1st Workshop on Sketch-Oriented Deep Learning @ CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Engineering sketches form the 2D basis of parametric Computer-Aided Design\n(CAD), the foremost modeling paradigm for manufactured objects. In this paper\nwe tackle the problem of learning based engineering sketch generation as a\nfirst step towards synthesis and composition of parametric CAD models. We\npropose two generative models, CurveGen and TurtleGen, for engineering sketch\ngeneration. Both models generate curve primitives without the need for a sketch\nconstraint solver and explicitly consider topology for downstream use with\nconstraints and 3D CAD modeling operations. We find in our perceptual\nevaluation using human subjects that both CurveGen and TurtleGen produce more\nrealistic engineering sketches when compared with the current state-of-the-art\nfor engineering sketch generation.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 20:38:36 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Willis", "Karl D. D.", ""], ["Jayaraman", "Pradeep Kumar", ""], ["Lambourne", "Joseph G.", ""], ["Chu", "Hang", ""], ["Pu", "Yewen", ""]]}, {"id": "2104.09630", "submitter": "Danilo Comminiello", "authors": "Eleonora Grassucci, Edoardo Cicero, Danilo Comminiello", "title": "Quaternion Generative Adversarial Networks", "comments": "Accepted as a Chapter for the SPRINGER book \"Generative Adversarial\n  Learning: Architectures and Applications\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latest Generative Adversarial Networks (GANs) are gathering outstanding\nresults through a large-scale training, thus employing models composed of\nmillions of parameters requiring extensive computational capabilities. Building\nsuch huge models undermines their replicability and increases the training\ninstability. Moreover, multi-channel data, such as images or audio, are usually\nprocessed by realvalued convolutional networks that flatten and concatenate the\ninput, often losing intra-channel spatial relations. To address these issues\nrelated to complexity and information loss, we propose a family of\nquaternion-valued generative adversarial networks (QGANs). QGANs exploit the\nproperties of quaternion algebra, e.g., the Hamilton product, that allows to\nprocess channels as a single entity and capture internal latent relations,\nwhile reducing by a factor of 4 the overall number of parameters. We show how\nto design QGANs and to extend the proposed approach even to advanced models.We\ncompare the proposed QGANs with real-valued counterparts on several image\ngeneration benchmarks. Results show that QGANs are able to obtain better FID\nscores than real-valued GANs and to generate visually pleasing images.\nFurthermore, QGANs save up to 75% of the training parameters. We believe these\nresults may pave the way to novel, more accessible, GANs capable of improving\nperformance and saving computational resources.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 20:46:18 GMT"}, {"version": "v2", "created": "Tue, 27 Jul 2021 15:30:42 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Grassucci", "Eleonora", ""], ["Cicero", "Edoardo", ""], ["Comminiello", "Danilo", ""]]}, {"id": "2104.09648", "submitter": "Joel Hestness", "authors": "Mihir Pendse, Vithursan Thangarasa, Vitaliy Chiley, Ryan Holmdahl,\n  Joel Hestness, Dennis DeCoste", "title": "Memory Efficient 3D U-Net with Reversible Mobile Inverted Bottlenecks\n  for Brain Tumor Segmentation", "comments": "11 pages, 5 figures, Published at MICCAI Brainles 2020", "journal-ref": "Brainlesion: Glioma, Multiple Sclerosis, Stroke and Traumatic\n  Brain Injuries (2021) 388-397", "doi": "10.1007/978-3-030-72087-2_34", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose combining memory saving techniques with traditional U-Net\narchitectures to increase the complexity of the models on the Brain Tumor\nSegmentation (BraTS) challenge. The BraTS challenge consists of a 3D\nsegmentation of a 240x240x155x4 input image into a set of tumor classes.\nBecause of the large volume and need for 3D convolutional layers, this task is\nvery memory intensive. To address this, prior approaches use smaller cropped\nimages while constraining the model's depth and width. Our 3D U-Net uses a\nreversible version of the mobile inverted bottleneck block defined in\nMobileNetV2, MnasNet and the more recent EfficientNet architectures to save\nactivation memory during training. Using reversible layers enables the model to\nrecompute input activations given the outputs of that layer, saving memory by\neliminating the need to store activations during the forward pass. The inverted\nresidual bottleneck block uses lightweight depthwise separable convolutions to\nreduce computation by decomposing convolutions into a pointwise convolution and\na depthwise convolution. Further, this block inverts traditional bottleneck\nblocks by placing an intermediate expansion layer between the input and output\nlinear 1x1 convolution, reducing the total number of channels. Given a fixed\nmemory budget, with these memory saving techniques, we are able to train image\nvolumes up to 3x larger, models with 25% more depth, or models with up to 2x\nthe number of channels than a corresponding non-reversible network.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 21:23:55 GMT"}, {"version": "v2", "created": "Wed, 21 Apr 2021 01:02:05 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Pendse", "Mihir", ""], ["Thangarasa", "Vithursan", ""], ["Chiley", "Vitaliy", ""], ["Holmdahl", "Ryan", ""], ["Hestness", "Joel", ""], ["DeCoste", "Dennis", ""]]}, {"id": "2104.09667", "submitter": "Ilia Shumailov", "authors": "Ilia Shumailov, Zakhar Shumaylov, Dmitry Kazhdan, Yiren Zhao, Nicolas\n  Papernot, Murat A. Erdogdu, Ross Anderson", "title": "Manipulating SGD with Data Ordering Attacks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CR cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Machine learning is vulnerable to a wide variety of attacks. It is now well\nunderstood that by changing the underlying data distribution, an adversary can\npoison the model trained with it or introduce backdoors. In this paper we\npresent a novel class of training-time attacks that require no changes to the\nunderlying dataset or model architecture, but instead only change the order in\nwhich data are supplied to the model. In particular, we find that the attacker\ncan either prevent the model from learning, or poison it to learn behaviours\nspecified by the attacker. Furthermore, we find that even a single\nadversarially-ordered epoch can be enough to slow down model learning, or even\nto reset all of the learning progress. Indeed, the attacks presented here are\nnot specific to the model or dataset, but rather target the stochastic nature\nof modern learning procedures. We extensively evaluate our attacks on computer\nvision and natural language benchmarks to find that the adversary can disrupt\nmodel training and even introduce backdoors.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 22:17:27 GMT"}, {"version": "v2", "created": "Sat, 5 Jun 2021 10:22:15 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Shumailov", "Ilia", ""], ["Shumaylov", "Zakhar", ""], ["Kazhdan", "Dmitry", ""], ["Zhao", "Yiren", ""], ["Papernot", "Nicolas", ""], ["Erdogdu", "Murat A.", ""], ["Anderson", "Ross", ""]]}, {"id": "2104.09699", "submitter": "Qiangguo Jin", "authors": "Qiangguo Jin and Hui Cui and Changming Sun and Zhaopeng Meng and Leyi\n  Wei and Ran Su", "title": "Domain adaptation based self-correction model for COVID-19 infection\n  segmentation in CT images", "comments": null, "journal-ref": null, "doi": "10.1016/j.eswa.2021.114848", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The capability of generalization to unseen domains is crucial for deep\nlearning models when considering real-world scenarios. However, current\navailable medical image datasets, such as those for COVID-19 CT images, have\nlarge variations of infections and domain shift problems. To address this\nissue, we propose a prior knowledge driven domain adaptation and a dual-domain\nenhanced self-correction learning scheme. Based on the novel learning schemes,\na domain adaptation based self-correction model (DASC-Net) is proposed for\nCOVID-19 infection segmentation on CT images. DASC-Net consists of a novel\nattention and feature domain enhanced domain adaptation model (AFD-DA) to solve\nthe domain shifts and a self-correction learning process to refine segmentation\nresults. The innovations in AFD-DA include an image-level activation feature\nextractor with attention to lung abnormalities and a multi-level discrimination\nmodule for hierarchical feature domain alignment. The proposed self-correction\nlearning process adaptively aggregates the learned model and corresponding\npseudo labels for the propagation of aligned source and target domain\ninformation to alleviate the overfitting to noises caused by pseudo labels.\nExtensive experiments over three publicly available COVID-19 CT datasets\ndemonstrate that DASC-Net consistently outperforms state-of-the-art\nsegmentation, domain shift, and coronavirus infection segmentation methods.\nAblation analysis further shows the effectiveness of the major components in\nour model. The DASC-Net enriches the theory of domain adaptation and\nself-correction learning in medical imaging and can be generalized to\nmulti-site COVID-19 infection segmentation on CT images for clinical\ndeployment.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 00:45:01 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Jin", "Qiangguo", ""], ["Cui", "Hui", ""], ["Sun", "Changming", ""], ["Meng", "Zhaopeng", ""], ["Wei", "Leyi", ""], ["Su", "Ran", ""]]}, {"id": "2104.09701", "submitter": "Qiangguo Jin", "authors": "Qiangguo Jin and Hui Cui and Changming Sun and Zhaopeng Meng and Ran\n  Su", "title": "Free-form tumor synthesis in computed tomography images via richer\n  generative adversarial network", "comments": null, "journal-ref": null, "doi": "10.1016/j.knosys.2021.106753", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The insufficiency of annotated medical imaging scans for cancer makes it\nchallenging to train and validate data-hungry deep learning models in precision\noncology. We propose a new richer generative adversarial network for free-form\n3D tumor/lesion synthesis in computed tomography (CT) images. The network is\ncomposed of a new richer convolutional feature enhanced dilated-gated generator\n(RicherDG) and a hybrid loss function. The RicherDG has dilated-gated\nconvolution layers to enable tumor-painting and to enlarge perceptive fields;\nand it has a novel richer convolutional feature association branch to recover\nmulti-scale convolutional features especially from uncertain boundaries between\ntumor and surrounding healthy tissues. The hybrid loss function, which consists\nof a diverse range of losses, is designed to aggregate complementary\ninformation to improve optimization.\n  We perform a comprehensive evaluation of the synthesis results on a wide\nrange of public CT image datasets covering the liver, kidney tumors, and lung\nnodules. The qualitative and quantitative evaluations and ablation study\ndemonstrated improved synthesizing results over advanced tumor synthesis\nmethods.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 00:49:35 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Jin", "Qiangguo", ""], ["Cui", "Hui", ""], ["Sun", "Changming", ""], ["Meng", "Zhaopeng", ""], ["Su", "Ran", ""]]}, {"id": "2104.09722", "submitter": "Qilong Zhang", "authors": "Lianli Gao, Qilong Zhang, Xiaosu Zhu, Jingkuan Song and Heng Tao Shen", "title": "Staircase Sign Method for Boosting Adversarial Attacks", "comments": "13 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crafting adversarial examples for the transfer-based attack is challenging\nand remains a research hot spot. Currently, such attack methods are based on\nthe hypothesis that the substitute model and the victim's model learn similar\ndecision boundaries, and they conventionally apply Sign Method (SM) to\nmanipulate the gradient as the resultant perturbation. Although SM is\nefficient, it only extracts the sign of gradient units but ignores their value\ndifference, which inevitably leads to a serious deviation. Therefore, we\npropose a novel Staircase Sign Method (S$^2$M) to alleviate this issue, thus\nboosting transfer-based attacks. Technically, our method heuristically divides\nthe gradient sign into several segments according to the values of the gradient\nunits, and then assigns each segment with a staircase weight for better\ncrafting adversarial perturbation. As a result, our adversarial examples\nperform better in both white-box and black-box manner without being more\nvisible. Since S$^2$M just manipulates the resultant gradient, our method can\nbe generally integrated into any transfer-based attacks, and the computational\noverhead is negligible. Extensive experiments on the ImageNet dataset\ndemonstrate the effectiveness of our proposed methods, which significantly\nimprove the transferability (i.e., on average, \\textbf{5.1\\%} for normally\ntrained models and \\textbf{11.2\\%} for adversarially trained defenses). Our\ncode is available at:\n\\url{https://github.com/qilong-zhang/Staircase-sign-method}.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 02:31:55 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Gao", "Lianli", ""], ["Zhang", "Qilong", ""], ["Zhu", "Xiaosu", ""], ["Song", "Jingkuan", ""], ["Shen", "Heng Tao", ""]]}, {"id": "2104.09752", "submitter": "Zijian Kuang", "authors": "Zijian Kuang and Xinran Tie", "title": "Flow-based Video Segmentation for Human Head and Shoulders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video segmentation for the human head and shoulders is essential in creating\nelegant media for videoconferencing and virtual reality applications. The main\nchallenge is to process high-quality background subtraction in a real-time\nmanner and address the segmentation issues under motion blurs, e.g., shaking\nthe head or waving hands during conference video. To overcome the motion blur\nproblem in video segmentation, we propose a novel flow-based encoder-decoder\nnetwork (FUNet) that combines both traditional Horn-Schunck optical-flow\nestimation technique and convolutional neural networks to perform robust\nreal-time video segmentation. We also introduce a video and image segmentation\ndataset: ConferenceVideoSegmentationDataset. Code and pre-trained models are\navailable on our GitHub repository:\n\\url{https://github.com/kuangzijian/Flow-Based-Video-Matting}.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 04:05:36 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Kuang", "Zijian", ""], ["Tie", "Xinran", ""]]}, {"id": "2104.09754", "submitter": "Nao Uehara", "authors": "Nao Uehara, Teruaki Hayashi, Yukio Ohsawa", "title": "Hierarchical entropy and domain interaction to understand the structure\n  in an image", "comments": "20pages,17figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this study, we devise a model that introduces two hierarchies into\ninformation entropy. The two hierarchies are the size of the region for which\nentropy is calculated and the size of the component that determines whether the\nstructures in the image are integrated or not. And this model uses two\nindicators, hierarchical entropy and domain interaction. Both indicators\nincrease or decrease due to the integration or fragmentation of the structure\nin the image. It aims to help people interpret and explain what the structure\nin an image looks like from two indicators that change with the size of the\nregion and the component. First, we conduct experiments using images and\nqualitatively evaluate how the two indicators change. Next, we explain the\nrelationship with the hidden structure of Vermeer's girl with a pearl earring\nusing the change of hierarchical entropy. Finally, we clarify the relationship\nbetween the change of domain interaction and the appropriate segment result of\nthe image by an experiment using a questionnaire.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 04:29:13 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Uehara", "Nao", ""], ["Hayashi", "Teruaki", ""], ["Ohsawa", "Yukio", ""]]}, {"id": "2104.09757", "submitter": "Kai Yi", "authors": "Mohamed Elhoseiny, Divyansh Jha, Kai Yi, Ivan Skorokhodov", "title": "Imaginative Walks: Generative Random Walk Deviation Loss for Improved\n  Unseen Learning Representation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a novel loss for generative models, dubbed as GRaWD (Generative\nRandom Walk Deviation), to improve learning representations of unexplored\nvisual spaces. Quality learning representation of unseen classes (or styles) is\ncrucial to facilitate novel image generation and better generative\nunderstanding of unseen visual classes (a.k.a. Zero-Shot Learning, ZSL). By\ngenerating representations of unseen classes from their semantic descriptions,\nsuch as attributes or text, Generative ZSL aims at identifying unseen\ncategories discriminatively from seen ones. We define GRaWD by constructing a\ndynamic graph, including the seen class/style centers and generated samples in\nthe current mini-batch. Our loss starts a random walk probability from each\ncenter through visual generations produced from hallucinated unseen classes. As\na deviation signal, we encourage the random walk to eventually land after t\nsteps in a feature representation that is hard to classify to any of the seen\nclasses. We show that our loss can improve unseen class representation quality\non four text-based ZSL benchmarks on CUB and NABirds datasets and three\nattribute-based ZSL benchmarks on AWA2, SUN, and aPY datasets. We also study\nour loss's ability to produce meaningful novel visual art generations on\nWikiArt dataset. Our experiments and human studies show that our loss can\nimprove StyleGAN1 and StyleGAN2 generation quality, creating novel art that is\nsignificantly more preferred. Code will be made available.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 04:34:28 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Elhoseiny", "Mohamed", ""], ["Jha", "Divyansh", ""], ["Yi", "Kai", ""], ["Skorokhodov", "Ivan", ""]]}, {"id": "2104.09758", "submitter": "Keval Doshi", "authors": "Keval Doshi, Yasin Yilmaz", "title": "An Efficient Approach for Anomaly Detection in Traffic Videos", "comments": "Accepted to CVPR 2021 - AI City Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Due to its relevance in intelligent transportation systems, anomaly detection\nin traffic videos has recently received much interest. It remains a difficult\nproblem due to a variety of factors influencing the video quality of a\nreal-time traffic feed, such as temperature, perspective, lighting conditions,\nand so on. Even though state-of-the-art methods perform well on the available\nbenchmark datasets, they need a large amount of external training data as well\nas substantial computational resources. In this paper, we propose an efficient\napproach for a video anomaly detection system which is capable of running at\nthe edge devices, e.g., on a roadside camera. The proposed approach comprises a\npre-processing module that detects changes in the scene and removes the\ncorrupted frames, a two-stage background modelling module and a two-stage\nobject detector. Finally, a backtracking anomaly detection algorithm computes a\nsimilarity statistic and decides on the onset time of the anomaly. We also\npropose a sequential change detection algorithm that can quickly adapt to a new\nscene and detect changes in the similarity statistic. Experimental results on\nthe Track 4 test set of the 2021 AI City Challenge show the efficacy of the\nproposed framework as we achieve an F1-score of 0.9157 along with 8.4027 root\nmean square error (RMSE) and are ranked fourth in the competition.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 04:43:18 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Doshi", "Keval", ""], ["Yilmaz", "Yasin", ""]]}, {"id": "2104.09760", "submitter": "Zejia Weng", "authors": "Zejia Weng, Zuxuan Wu, Hengduo Li, Yu-Gang Jiang", "title": "HMS: Hierarchical Modality Selection for Efficient Video Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Videos are multimodal in nature. Conventional video recognition pipelines\ntypically fuse multimodal features for improved performance. However, this is\nnot only computationally expensive but also neglects the fact that different\nvideos rely on different modalities for predictions. This paper introduces\nHierarchical Modality Selection (HMS), a simple yet efficient multimodal\nlearning framework for efficient video recognition. HMS operates on a low-cost\nmodality, i.e., audio clues, by default, and dynamically decides on-the-fly\nwhether to use computationally-expensive modalities, including appearance and\nmotion clues, on a per-input basis. This is achieved by the collaboration of\nthree LSTMs that are organized in a hierarchical manner. In particular, LSTMs\nthat operate on high-cost modalities contain a gating module, which takes as\ninputs lower-level features and historical information to adaptively determine\nwhether to activate its corresponding modality; otherwise it simply reuses\nhistorical information. We conduct extensive experiments on two large-scale\nvideo benchmarks, FCVID and ActivityNet, and the results demonstrate the\nproposed approach can effectively explore multimodal information for improved\nclassification performance while requiring much less computation.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 04:47:04 GMT"}, {"version": "v2", "created": "Wed, 21 Apr 2021 03:00:57 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Weng", "Zejia", ""], ["Wu", "Zuxuan", ""], ["Li", "Hengduo", ""], ["Jiang", "Yu-Gang", ""]]}, {"id": "2104.09762", "submitter": "Xinzhu Bei", "authors": "Xinzhu Bei, Yanchao Yang, Stefano Soatto", "title": "Learning Semantic-Aware Dynamics for Video Prediction", "comments": "Paper accepted at CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an architecture and training scheme to predict video frames by\nexplicitly modeling dis-occlusions and capturing the evolution of semantically\nconsistent regions in the video. The scene layout (semantic map) and motion\n(optical flow) are decomposed into layers, which are predicted and fused with\ntheir context to generate future layouts and motions. The appearance of the\nscene is warped from past frames using the predicted motion in co-visible\nregions; dis-occluded regions are synthesized with content-aware inpainting\nutilizing the predicted scene layout. The result is a predictive model that\nexplicitly represents objects and learns their class-specific motion, which we\nevaluate on video prediction benchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 05:00:24 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Bei", "Xinzhu", ""], ["Yang", "Yanchao", ""], ["Soatto", "Stefano", ""]]}, {"id": "2104.09770", "submitter": "Junke Wang", "authors": "Junke Wang, Zuxuan Wu, Jingjing Chen, and Yu-Gang Jiang", "title": "M2TR: Multi-modal Multi-scale Transformers for Deepfake Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The widespread dissemination of forged images generated by Deepfake\ntechniques has posed a serious threat to the trustworthiness of digital\ninformation. This demands effective approaches that can detect perceptually\nconvincing Deepfakes generated by advanced manipulation techniques. Most\nexisting approaches combat Deepfakes with deep neural networks by mapping the\ninput image to a binary prediction without capturing the consistency among\ndifferent pixels. In this paper, we aim to capture the subtle manipulation\nartifacts at different scales for Deepfake detection. We achieve this with\ntransformer models, which have recently demonstrated superior performance in\nmodeling dependencies between pixels for a variety of recognition tasks in\ncomputer vision. In particular, we introduce a Multi-modal Multi-scale\nTRansformer (M2TR), which uses a multi-scale transformer that operates on\npatches of different sizes to detect the local inconsistency at different\nspatial levels. To improve the detection results and enhance the robustness of\nour method to image compression, M2TR also takes frequency information, which\nis further combined with RGB features using a cross modality fusion module.\nDeveloping and evaluating Deepfake detection methods requires large-scale\ndatasets. However, we observe that samples in existing benchmarks contain\nsevere artifacts and lack diversity. This motivates us to introduce a\nhigh-quality Deepfake dataset, SR-DF, which consists of 4,000 DeepFake videos\ngenerated by state-of-the-art face swapping and facial reenactment methods. On\nthree Deepfake datasets, we conduct extensive experiments to verify the\neffectiveness of the proposed method, which outperforms state-of-the-art\nDeepfake detection methods.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 05:43:44 GMT"}, {"version": "v2", "created": "Wed, 21 Apr 2021 12:59:29 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Wang", "Junke", ""], ["Wu", "Zuxuan", ""], ["Chen", "Jingjing", ""], ["Jiang", "Yu-Gang", ""]]}, {"id": "2104.09789", "submitter": "Chaithanya Kumar Mummadi", "authors": "Chaithanya Kumar Mummadi, Ranjitha Subramaniam, Robin Hutmacher,\n  Julien Vitay, Volker Fischer, Jan Hendrik Metzen", "title": "Does enhanced shape bias improve neural network robustness to common\n  corruptions?", "comments": "20 pages, 9 figures, 12 tables, accepted at ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Convolutional neural networks (CNNs) learn to extract representations of\ncomplex features, such as object shapes and textures to solve image recognition\ntasks. Recent work indicates that CNNs trained on ImageNet are biased towards\nfeatures that encode textures and that these alone are sufficient to generalize\nto unseen test data from the same distribution as the training data but often\nfail to generalize to out-of-distribution data. It has been shown that\naugmenting the training data with different image styles decreases this texture\nbias in favor of increased shape bias while at the same time improving\nrobustness to common corruptions, such as noise and blur. Commonly, this is\ninterpreted as shape bias increasing corruption robustness. However, this\nrelationship is only hypothesized. We perform a systematic study of different\nways of composing inputs based on natural images, explicit edge information,\nand stylization. While stylization is essential for achieving high corruption\nrobustness, we do not find a clear correlation between shape bias and\nrobustness. We conclude that the data augmentation caused by style-variation\naccounts for the improved corruption robustness and increased shape bias is\nonly a byproduct.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 07:06:53 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Mummadi", "Chaithanya Kumar", ""], ["Subramaniam", "Ranjitha", ""], ["Hutmacher", "Robin", ""], ["Vitay", "Julien", ""], ["Fischer", "Volker", ""], ["Metzen", "Jan Hendrik", ""]]}, {"id": "2104.09793", "submitter": "JuneKyu Park", "authors": "JuneKyu Park, Jeong-Hyeon Moon, Namhyuk Ahn and Kyung-Ah Sohn", "title": "What is Wrong with One-Class Anomaly Detection?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  From a safety perspective, a machine learning method embedded in real-world\napplications is required to distinguish irregular situations. For this reason,\nthere has been a growing interest in the anomaly detection (AD) task. Since we\ncannot observe abnormal samples for most of the cases, recent AD methods\nattempt to formulate it as a task of classifying whether the sample is normal\nor not. However, they potentially fail when the given normal samples are\ninherited from diverse semantic labels. To tackle this problem, we introduce a\nlatent class-condition-based AD scenario. In addition, we propose a\nconfidence-based self-labeling AD framework tailored to our proposed scenario.\nSince our method leverages the hidden class information, it successfully avoids\ngenerating the undesirable loose decision region that one-class methods suffer.\nOur proposed framework outperforms the recent one-class AD methods in the\nlatent multi-class scenarios.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 07:10:00 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Park", "JuneKyu", ""], ["Moon", "Jeong-Hyeon", ""], ["Ahn", "Namhyuk", ""], ["Sohn", "Kyung-Ah", ""]]}, {"id": "2104.09804", "submitter": "Wu Zheng", "authors": "Wu Zheng, Weiliang Tang, Li Jiang, Chi-Wing Fu", "title": "SE-SSD: Self-Ensembling Single-Stage Object Detector From Point Cloud", "comments": "Accepted by CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Self-Ensembling Single-Stage object Detector (SE-SSD) for accurate\nand efficient 3D object detection in outdoor point clouds. Our key focus is on\nexploiting both soft and hard targets with our formulated constraints to\njointly optimize the model, without introducing extra computation in the\ninference. Specifically, SE-SSD contains a pair of teacher and student SSDs, in\nwhich we design an effective IoU-based matching strategy to filter soft targets\nfrom the teacher and formulate a consistency loss to align student predictions\nwith them. Also, to maximize the distilled knowledge for ensembling the\nteacher, we design a new augmentation scheme to produce shape-aware augmented\nsamples to train the student, aiming to encourage it to infer complete object\nshapes. Lastly, to better exploit hard targets, we design an ODIoU loss to\nsupervise the student with constraints on the predicted box centers and\norientations. Our SE-SSD attains top performance compared with all prior\npublished works. Also, it attains top precisions for car detection in the KITTI\nbenchmark (ranked 1st and 2nd on the BEV and 3D leaderboards, respectively)\nwith an ultra-high inference speed. The code is available at\nhttps://github.com/Vegeta2020/SE-SSD.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 07:33:03 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Zheng", "Wu", ""], ["Tang", "Weiliang", ""], ["Jiang", "Li", ""], ["Fu", "Chi-Wing", ""]]}, {"id": "2104.09805", "submitter": "Yanpeng Sun", "authors": "Zechao Li, Yanpeng Sun, and Jinhui Tang", "title": "CTNet: Context-based Tandem Network for Semantic Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contextual information has been shown to be powerful for semantic\nsegmentation. This work proposes a novel Context-based Tandem Network (CTNet)\nby interactively exploring the spatial contextual information and the channel\ncontextual information, which can discover the semantic context for semantic\nsegmentation. Specifically, the Spatial Contextual Module (SCM) is leveraged to\nuncover the spatial contextual dependency between pixels by exploring the\ncorrelation between pixels and categories. Meanwhile, the Channel Contextual\nModule (CCM) is introduced to learn the semantic features including the\nsemantic feature maps and class-specific features by modeling the long-term\nsemantic dependence between channels. The learned semantic features are\nutilized as the prior knowledge to guide the learning of SCM, which can make\nSCM obtain more accurate long-range spatial dependency. Finally, to further\nimprove the performance of the learned representations for semantic\nsegmentation, the results of the two context modules are adaptively integrated\nto achieve better results. Extensive experiments are conducted on three\nwidely-used datasets, i.e., PASCAL-Context, ADE20K and PASCAL VOC2012. The\nresults demonstrate the superior performance of the proposed CTNet by\ncomparison with several state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 07:33:11 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Li", "Zechao", ""], ["Sun", "Yanpeng", ""], ["Tang", "Jinhui", ""]]}, {"id": "2104.09807", "submitter": "Bar Mayo", "authors": "Bar Mayo, Tamir Hazan and Ayellet Tal", "title": "Visual Navigation with Spatial Attention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work focuses on object goal visual navigation, aiming at finding the\nlocation of an object from a given class, where in each step the agent is\nprovided with an egocentric RGB image of the scene. We propose to learn the\nagent's policy using a reinforcement learning algorithm. Our key contribution\nis a novel attention probability model for visual navigation tasks. This\nattention encodes semantic information about observed objects, as well as\nspatial information about their place. This combination of the \"what\" and the\n\"where\" allows the agent to navigate toward the sought-after object\neffectively. The attention model is shown to improve the agent's policy and to\nachieve state-of-the-art results on commonly-used datasets.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 07:39:52 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Mayo", "Bar", ""], ["Hazan", "Tamir", ""], ["Tal", "Ayellet", ""]]}, {"id": "2104.09808", "submitter": "Leon Varga", "authors": "Leon Amadeus Varga, Jan Makowski and Andreas Zell", "title": "Measuring the Ripeness of Fruit with Hyperspectral Imaging and Deep\n  Learning", "comments": "IJCNN 2021 (Accepted 10.04.21)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We present a system to measure the ripeness of fruit with a hyperspectral\ncamera and a suitable deep neural network architecture. This architecture did\noutperform competitive baseline models on the prediction of the ripeness state\nof fruit. For this, we recorded a data set of ripening avocados and kiwis,\nwhich we make public. We also describe the process of data collection in a\nmanner that the adaption for other fruit is easy. The trained network is\nvalidated empirically, and we investigate the trained features. Furthermore, a\ntechnique is introduced to visualize the ripening process.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 07:43:19 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Varga", "Leon Amadeus", ""], ["Makowski", "Jan", ""], ["Zell", "Andreas", ""]]}, {"id": "2104.09815", "submitter": "Tomasz Kryjak", "authors": "Artur Cyba and Hubert Szolc and Tomasz Kryjak", "title": "A simple vision-based navigation and control strategy for autonomous\n  drone racing", "comments": "Submitted to the MMAR 2021 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.SY eess.IV eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a control system that allows a drone to fly\nautonomously through a series of gates marked with ArUco tags. A simple and\nlow-cost DJI Tello EDU quad-rotor platform was used. Based on the API provided\nby the manufacturer, we have created a Python application that enables the\ncommunication with the drone over WiFi, realises drone positioning based on\nvisual feedback, and generates control. Two control strategies were proposed,\ncompared, and critically analysed. In addition, the accuracy of the positioning\nmethod used was measured. The application was evaluated on a laptop computer\n(about 40 fps) and a Nvidia Jetson TX2 embedded GPU platform (about 25 fps). We\nprovide the developed code on GitHub.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 08:02:02 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Cyba", "Artur", ""], ["Szolc", "Hubert", ""], ["Kryjak", "Tomasz", ""]]}, {"id": "2104.09829", "submitter": "Leonid Karlinsky", "authors": "Assaf Arbelle, Sivan Doveh, Amit Alfassy, Joseph Shtok, Guy Lev, Eli\n  Schwartz, Hilde Kuehne, Hila Barak Levi, Prasanna Sattigeri, Rameswar Panda,\n  Chun-Fu Chen, Alex Bronstein, Kate Saenko, Shimon Ullman, Raja Giryes,\n  Rogerio Feris, Leonid Karlinsky", "title": "Detector-Free Weakly Supervised Grounding by Separation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, there is an abundance of data involving images and surrounding\nfree-form text weakly corresponding to those images. Weakly Supervised\nphrase-Grounding (WSG) deals with the task of using this data to learn to\nlocalize (or to ground) arbitrary text phrases in images without any additional\nannotations. However, most recent SotA methods for WSG assume the existence of\na pre-trained object detector, relying on it to produce the ROIs for\nlocalization. In this work, we focus on the task of Detector-Free WSG (DF-WSG)\nto solve WSG without relying on a pre-trained detector. We directly learn\neverything from the images and associated free-form text pairs, thus\npotentially gaining an advantage on the categories unsupported by the detector.\nThe key idea behind our proposed Grounding by Separation (GbS) method is\nsynthesizing `text to image-regions' associations by random alpha-blending of\narbitrary image pairs and using the corresponding texts of the pair as\nconditions to recover the alpha map from the blended image via a segmentation\nnetwork. At test time, this allows using the query phrase as a condition for a\nnon-blended query image, thus interpreting the test image as a composition of a\nregion corresponding to the phrase and the complement region. Using this\napproach we demonstrate a significant accuracy improvement, of up to $8.5\\%$\nover previous DF-WSG SotA, for a range of benchmarks including Flickr30K,\nVisual Genome, and ReferIt, as well as a significant complementary improvement\n(above $7\\%$) over the detector-based approaches for WSG.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 08:27:31 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Arbelle", "Assaf", ""], ["Doveh", "Sivan", ""], ["Alfassy", "Amit", ""], ["Shtok", "Joseph", ""], ["Lev", "Guy", ""], ["Schwartz", "Eli", ""], ["Kuehne", "Hilde", ""], ["Levi", "Hila Barak", ""], ["Sattigeri", "Prasanna", ""], ["Panda", "Rameswar", ""], ["Chen", "Chun-Fu", ""], ["Bronstein", "Alex", ""], ["Saenko", "Kate", ""], ["Ullman", "Shimon", ""], ["Giryes", "Raja", ""], ["Feris", "Rogerio", ""], ["Karlinsky", "Leonid", ""]]}, {"id": "2104.09830", "submitter": "Gongzhe Li", "authors": "Gongzhe Li, Zhiwen Tan, Linpeng Pan", "title": "A novel three-stage training strategy for long-tailed classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The long-tailed distribution datasets poses great challenges for deep\nlearning based classification models on how to handle the class imbalance\nproblem. Existing solutions usually involve class-balacing strategies or\ntransfer learing from head- to tail-classes or use two-stages learning strategy\nto re-train the classifier. However, the existing methods are difficult to\nsolve the low quality problem when images are obtained by SAR. To address this\nproblem, we establish a novel three-stages training strategy, which has\nexcellent results for processing SAR image datasets with long-tailed\ndistribution. Specifically, we divide training procedure into three stages. The\nfirst stage is to use all kinds of images for rough-training, so as to get the\nrough-training model with rich content. The second stage is to make the rough\nmodel learn the feature expression by using the residual dataset with the class\n0 removed. The third stage is to fine tune the model using class-balanced\ndatasets with all 10 classes (including the overall model fine tuning and\nclassifier re-optimization). Through this new training strategy, we only use\nthe information of SAR image dataset and the network model with very small\nparameters to achieve the top 1 accuracy of 22.34 in development phase.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 08:29:27 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Li", "Gongzhe", ""], ["Tan", "Zhiwen", ""], ["Pan", "Linpeng", ""]]}, {"id": "2104.09841", "submitter": "Daehee Kim", "authors": "Daehee Kim, Seunghyun Park, Jinkyu Kim, and Jaekoo Lee", "title": "SelfReg: Self-supervised Contrastive Regularization for Domain\n  Generalization", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In general, an experimental environment for deep learning assumes that the\ntraining and the test dataset are sampled from the same distribution. However,\nin real-world situations, a difference in the distribution between two\ndatasets, domain shift, may occur, which becomes a major factor impeding the\ngeneralization performance of the model. The research field to solve this\nproblem is called domain generalization, and it alleviates the domain shift\nproblem by extracting domain-invariant features explicitly or implicitly. In\nrecent studies, contrastive learning-based domain generalization approaches\nhave been proposed and achieved high performance. These approaches require\nsampling of the negative data pair. However, the performance of contrastive\nlearning fundamentally depends on quality and quantity of negative data pairs.\nTo address this issue, we propose a new regularization method for domain\ngeneralization based on contrastive learning, self-supervised contrastive\nregularization (SelfReg). The proposed approach use only positive data pairs,\nthus it resolves various problems caused by negative pair sampling. Moreover,\nwe propose a class-specific domain perturbation layer (CDPL), which makes it\npossible to effectively apply mixup augmentation even when only positive data\npairs are used. The experimental results show that the techniques incorporated\nby SelfReg contributed to the performance in a compatible manner. In the recent\nbenchmark, DomainBed, the proposed method shows comparable performance to the\nconventional state-of-the-art alternatives. Codes are available at\nhttps://github.com/dnap512/SelfReg.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 09:08:29 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Kim", "Daehee", ""], ["Park", "Seunghyun", ""], ["Kim", "Jinkyu", ""], ["Lee", "Jaekoo", ""]]}, {"id": "2104.09866", "submitter": "Elahe Arani", "authors": "Prashant Bhat, Elahe Arani, and Bahram Zonooz", "title": "Distill on the Go: Online knowledge distillation in self-supervised\n  learning", "comments": "Spotlight @ Learning from Limited or Imperfect Data (L2ID) Workshop -\n  CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Self-supervised learning solves pretext prediction tasks that do not require\nannotations to learn feature representations. For vision tasks, pretext tasks\nsuch as predicting rotation, solving jigsaw are solely created from the input\ndata. Yet, predicting this known information helps in learning representations\nuseful for downstream tasks. However, recent works have shown that wider and\ndeeper models benefit more from self-supervised learning than smaller models.\nTo address the issue of self-supervised pre-training of smaller models, we\npropose Distill-on-the-Go (DoGo), a self-supervised learning paradigm using\nsingle-stage online knowledge distillation to improve the representation\nquality of the smaller models. We employ deep mutual learning strategy in which\ntwo models collaboratively learn from each other to improve one another.\nSpecifically, each model is trained using self-supervised learning along with\ndistillation that aligns each model's softmax probabilities of similarity\nscores with that of the peer model. We conduct extensive experiments on\nmultiple benchmark datasets, learning objectives, and architectures to\ndemonstrate the potential of our proposed method. Our results show significant\nperformance gain in the presence of noisy and limited labels and generalization\nto out-of-distribution data.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 09:59:23 GMT"}, {"version": "v2", "created": "Wed, 30 Jun 2021 13:03:14 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Bhat", "Prashant", ""], ["Arani", "Elahe", ""], ["Zonooz", "Bahram", ""]]}, {"id": "2104.09874", "submitter": "David Montero", "authors": "David Montero, Marcos Nieto, Peter Leskovsky and Naiara Aginako", "title": "Boosting Masked Face Recognition with Multi-Task ArcFace", "comments": "6 pages, 4 figures. The paper is under consideration at Pattern\n  Recognition Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In this paper, we address the problem of face recognition with masks. Given\nthe global health crisis caused by COVID-19, mouth and nose-covering masks have\nbecome an essential everyday-clothing-accessory. This sanitary measure has put\nthe state-of-the-art face recognition models on the ropes since they have not\nbeen designed to work with masked faces. In addition, the need has arisen for\napplications capable of detecting whether the subjects are wearing masks to\ncontrol the spread of the virus. To overcome these problems a full training\npipeline is presented based on the ArcFace work, with several modifications for\nthe backbone and the loss function. From the original face-recognition dataset,\na masked version is generated using data augmentation, and both datasets are\ncombined during the training process. The selected network, based on ResNet-50,\nis modified to also output the probability of mask usage without adding any\ncomputational cost. Furthermore, the ArcFace loss is combined with the\nmask-usage classification loss, resulting in a new function named Multi-Task\nArcFace (MTArcFace). Experimental results show that the proposed approach\nhighly boosts the original model accuracy when dealing with masked faces, while\npreserving almost the same accuracy on the original non-masked datasets.\nFurthermore, it achieves an average accuracy of 99.78% in mask-usage\nclassification.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 10:12:04 GMT"}, {"version": "v2", "created": "Wed, 21 Apr 2021 06:54:29 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Montero", "David", ""], ["Nieto", "Marcos", ""], ["Leskovsky", "Peter", ""], ["Aginako", "Naiara", ""]]}, {"id": "2104.09877", "submitter": "Dawa Derksen", "authors": "Dawa Derksen, Dario Izzo", "title": "Shadow Neural Radiance Fields for Multi-view Satellite Photogrammetry", "comments": "Accepted to CVPR2021 - EarthVision", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present a new generic method for shadow-aware multi-view satellite\nphotogrammetry of Earth Observation scenes. Our proposed method, the Shadow\nNeural Radiance Field (S-NeRF) follows recent advances in implicit volumetric\nrepresentation learning. For each scene, we train S-NeRF using very high\nspatial resolution optical images taken from known viewing angles. The learning\nrequires no labels or shape priors: it is self-supervised by an image\nreconstruction loss. To accommodate for changing light source conditions both\nfrom a directional light source (the Sun) and a diffuse light source (the sky),\nwe extend the NeRF approach in two ways. First, direct illumination from the\nSun is modeled via a local light source visibility field. Second, indirect\nillumination from a diffuse light source is learned as a non-local color field\nas a function of the position of the Sun. Quantitatively, the combination of\nthese factors reduces the altitude and color errors in shaded areas, compared\nto NeRF. The S-NeRF methodology not only performs novel view synthesis and full\n3D shape estimation, it also enables shadow detection, albedo synthesis, and\ntransient object filtering, without any explicit shape supervision.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 10:17:34 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Derksen", "Dawa", ""], ["Izzo", "Dario", ""]]}, {"id": "2104.09878", "submitter": "Roc\\'io", "authors": "Roc\\'io del Amor, La\\\"etitia Launet, Adri\\'an Colomer, Ana\\\"is\n  Moscard\\'o, Andr\\'es Mosquera-Zamudio, Carlos Monteagudo and Valery Naranjo", "title": "An Attention-based Weakly Supervised framework for Spitzoid Melanocytic\n  Lesion Diagnosis in WSI", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Melanoma is an aggressive neoplasm responsible for the majority of deaths\nfrom skin cancer. Specifically, spitzoid melanocytic tumors are one of the most\nchallenging melanocytic lesions due to their ambiguous morphological features.\nThe gold standard for its diagnosis and prognosis is the analysis of skin\nbiopsies. In this process, dermatopathologists visualize skin histology slides\nunder a microscope, in a high time-consuming and subjective task. In the last\nyears, computer-aided diagnosis (CAD) systems have emerged as a promising tool\nthat could support pathologists in daily clinical practice. Nevertheless, no\nautomatic CAD systems have yet been proposed for the analysis of spitzoid\nlesions. Regarding common melanoma, no proposed system allows both the\nselection of the tumoral region and the prediction of the diagnosis as benign\nor malignant. Motivated by this, we propose a novel end-to-end\nweakly-supervised deep learning model, based on inductive transfer learning\nwith an improved convolutional neural network (CNN) to refine the embedding\nfeatures of the latent space. The framework is composed of a source model in\ncharge of finding the tumor patch-level patterns, and a target model focuses on\nthe specific diagnosis of a biopsy. The latter retrains the backbone of the\nsource model through a multiple instance learning workflow to obtain the\nbiopsy-level scoring. To evaluate the performance of the proposed methods, we\nperform extensive experiments on a private skin database with spitzoid lesions.\nTest results reach an accuracy of 0.9231 and 0.80 for the source and the target\nmodels, respectively. Besides, the heat map findings are directly in line with\nthe clinicians' medical decision and even highlight, in some cases, patterns of\ninterest that were overlooked by the pathologist due to the huge workload.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 10:18:57 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["del Amor", "Roc\u00edo", ""], ["Launet", "La\u00ebtitia", ""], ["Colomer", "Adri\u00e1n", ""], ["Moscard\u00f3", "Ana\u00efs", ""], ["Mosquera-Zamudio", "Andr\u00e9s", ""], ["Monteagudo", "Carlos", ""], ["Naranjo", "Valery", ""]]}, {"id": "2104.09886", "submitter": "Junxuan Li", "authors": "Junxuan Li, Hongdong Li and Yasuyuki Matsushita", "title": "Lighting, Reflectance and Geometry Estimation from 360$^{\\circ}$\n  Panoramic Stereo", "comments": "Accepted to CVPR 2021. Codes in:\n  https://github.com/junxuan-li/LRG_360Panoramic", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a method for estimating high-definition spatially-varying\nlighting, reflectance, and geometry of a scene from 360$^{\\circ}$ stereo\nimages. Our model takes advantage of the 360$^{\\circ}$ input to observe the\nentire scene with geometric detail, then jointly estimates the scene's\nproperties with physical constraints. We first reconstruct a near-field\nenvironment light for predicting the lighting at any 3D location within the\nscene. Then we present a deep learning model that leverages the stereo\ninformation to infer the reflectance and surface normal. Lastly, we incorporate\nthe physical constraints between lighting and geometry to refine the\nreflectance of the scene. Both quantitative and qualitative experiments show\nthat our method, benefiting from the 360$^{\\circ}$ observation of the scene,\noutperforms prior state-of-the-art methods and enables more augmented reality\napplications such as mirror-objects insertion.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 10:41:50 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Li", "Junxuan", ""], ["Li", "Hongdong", ""], ["Matsushita", "Yasuyuki", ""]]}, {"id": "2104.09887", "submitter": "Jianhao Jiao", "authors": "Jianhao Jiao and Huaiyang Huang and Liang Li and Zhijian He and Yilong\n  Zhu and Ming Liu", "title": "Comparing Representations in Tracking for Event Camera-based SLAM", "comments": "9 pages, 7 figures, accepted by CVPR Workshop 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates two typical image-type representations for event\ncamera-based tracking: time surface (TS) and event map (EM). Based on the\noriginal TS-based tracker, we make use of these two representations'\ncomplementary strengths to develop an enhanced version. The proposed tracker\nconsists of a general strategy to evaluate the optimization problem's\ndegeneracy online and then switch proper representations. Both TS and EM are\nmotion- and scene-dependent, and thus it is important to figure out their\nlimitations in tracking. We develop six tracker variations and conduct a\nthorough comparison of them on sequences covering various scenarios and motion\ncomplexities. We release our implementations and detailed results to benefit\nthe research community on event cameras: https:\n//github.com/gogojjh/ESVO_extension.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 10:41:57 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Jiao", "Jianhao", ""], ["Huang", "Huaiyang", ""], ["Li", "Liang", ""], ["He", "Zhijian", ""], ["Zhu", "Yilong", ""], ["Liu", "Ming", ""]]}, {"id": "2104.09895", "submitter": "Roy Friedman", "authors": "Roy Friedman, Yair Weiss", "title": "Posterior Sampling for Image Restoration using Explicit Patch Priors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Almost all existing methods for image restoration are based on optimizing the\nmean squared error (MSE), even though it is known that the best estimate in\nterms of MSE may yield a highly atypical image due to the fact that there are\nmany plausible restorations for a given noisy image. In this paper, we show how\nto combine explicit priors on patches of natural images in order to sample from\nthe posterior probability of a full image given a degraded image. We prove that\nour algorithm generates correct samples from the distribution $p(x|y) \\propto\n\\exp(-E(x|y))$ where $E(x|y)$ is the cost function minimized in previous\npatch-based approaches that compute a single restoration. Unlike previous\napproaches that computed a single restoration using MAP or MMSE, our method\nmakes explicit the uncertainty in the restored images and guarantees that all\npatches in the restored images will be typical given the patch prior. Unlike\nprevious approaches that used implicit priors on fixed-size images, our\napproach can be used with images of any size. Our experimental results show\nthat posterior sampling using patch priors yields images of high perceptual\nquality and high PSNR on a range of challenging image restoration problems.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 11:11:33 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Friedman", "Roy", ""], ["Weiss", "Yair", ""]]}, {"id": "2104.09903", "submitter": "David Fernandez Llorca", "authors": "Antonio Hern\\'andez Mart\\'inez, Javier Lorenzo D\\'iaz, Iv\\'an Garc\\'ia\n  Daza, David Fern\\'andez Llorca", "title": "Data-driven vehicle speed detection from synthetic driving simulator\n  images", "comments": "Submitted to the IEEE Intelligent Transportation Systems Conference\n  2021 (ITSC2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite all the challenges and limitations, vision-based vehicle speed\ndetection is gaining research interest due to its great potential benefits such\nas cost reduction, and enhanced additional functions. As stated in a recent\nsurvey [1], the use of learning-based approaches to address this problem is\nstill in its infancy. One of the main difficulties is the need for a large\namount of data, which must contain the input sequences and, more importantly,\nthe output values corresponding to the actual speed of the vehicles. Data\ncollection in this context requires a complex and costly setup to capture the\nimages from the camera synchronized with a high precision speed sensor to\ngenerate the ground truth speed values. In this paper we explore, for the first\ntime, the use of synthetic images generated from a driving simulator (e.g.,\nCARLA) to address vehicle speed detection using a learning-based approach. We\nsimulate a virtual camera placed over a stretch of road, and generate thousands\nof images with variability corresponding to multiple speeds, different vehicle\ntypes and colors, and lighting and weather conditions. Two different approaches\nto map the sequence of images to an output speed (regression) are studied,\nincluding CNN-GRU and 3D-CNN. We present preliminary results that support the\nhigh potential of this approach to address vehicle speed detection.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 11:26:13 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Mart\u00ednez", "Antonio Hern\u00e1ndez", ""], ["D\u00edaz", "Javier Lorenzo", ""], ["Daza", "Iv\u00e1n Garc\u00eda", ""], ["Llorca", "David Fern\u00e1ndez", ""]]}, {"id": "2104.09907", "submitter": "Kaustubh Kulkarni", "authors": "Kaustubh Milind Kulkarni and Sucheth Shenoy", "title": "Table Tennis Stroke Recognition Using Two-Dimensional Human Pose\n  Estimation", "comments": "Accepted at CVPR Sports Workshop 2021 (7th International Workshop on\n  Computer Vision in Sports) (CVSports)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We introduce a novel method for collecting table tennis video data and\nperform stroke detection and classification. A diverse dataset containing video\ndata of 11 basic strokes obtained from 14 professional table tennis players,\nsumming up to a total of 22111 videos has been collected using the proposed\nsetup. The temporal convolutional neural network model developed using 2D pose\nestimation performs multiclass classification of these 11 table tennis strokes\nwith a validation accuracy of 99.37%. Moreover, the neural network generalizes\nwell over the data of a player excluded from the training and validation\ndataset, classifying the fresh strokes with an overall best accuracy of 98.72%.\nVarious model architectures using machine learning and deep learning based\napproaches have been trained for stroke recognition and their performances have\nbeen compared and benchmarked. Inferences such as performance monitoring and\nstroke comparison of the players using the model have been discussed.\nTherefore, we are contributing to the development of a computer vision based\nsports analytics system for the sport of table tennis that focuses on the\npreviously unexploited aspect of the sport i.e., a player's strokes, which is\nextremely insightful for performance improvement.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 11:32:43 GMT"}, {"version": "v2", "created": "Mon, 31 May 2021 18:59:57 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Kulkarni", "Kaustubh Milind", ""], ["Shenoy", "Sucheth", ""]]}, {"id": "2104.09917", "submitter": "ZengShun Zhao", "authors": "ZengShun Zhaoa (1), Yulong Wang (1), Ke Liu (1), Haoran Yang (1), Qian\n  Sun (1), Heng Qiao (2) ((1) Shandong University of Science and Technology,(2)\n  University of Florida)", "title": "Semantic Segmentation by Improved Generative Adversarial Networks", "comments": "arXiv admin note: text overlap with arXiv:1802.07934 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While most existing segmentation methods usually combined the powerful\nfeature extraction capabilities of CNNs with Conditional Random Fields (CRFs)\npost-processing, the result always limited by the fault of CRFs . Due to the\nnotoriously slow calculation speeds and poor efficiency of CRFs, in recent\nyears, CRFs post-processing has been gradually eliminated. In this paper, an\nimproved Generative Adversarial Networks (GANs) for image semantic segmentation\ntask (semantic segmentation by GANs, Seg-GAN) is proposed to facilitate further\nsegmentation research. In addition, we introduce Convolutional CRFs (ConvCRFs)\nas an effective improvement solution for the image semantic segmentation task.\nTowards the goal of differentiating the segmentation results from the ground\ntruth distribution and improving the details of the output images, the proposed\ndiscriminator network is specially designed in a full convolutional manner\ncombined with cascaded ConvCRFs. Besides, the adversarial loss aggressively\nencourages the output image to be close to the distribution of the ground\ntruth. Our method not only learns an end-to-end mapping from input image to\ncorresponding output image, but also learns a loss function to train this\nmapping. The experiments show that our method achieves better performance than\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 11:59:29 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Zhaoa", "ZengShun", ""], ["Wang", "Yulong", ""], ["Liu", "Ke", ""], ["Yang", "Haoran", ""], ["Sun", "Qian", ""], ["Qiao", "Heng", ""]]}, {"id": "2104.09918", "submitter": "Ushasi Chaudhuri", "authors": "Ushasi Chaudhuri, Biplab Banerjee, Avik Bhattacharya, Mihai Datcu", "title": "CrossATNet - A Novel Cross-Attention Based Framework for Sketch-Based\n  Image Retrieval", "comments": "Accepted in Journal of Image and Vision Computing", "journal-ref": null, "doi": "10.1016/j.imavis.2020.104003", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a novel framework for cross-modal zero-shot learning (ZSL) in the\ncontext of sketch-based image retrieval (SBIR). Conventionally, the SBIR schema\nmainly considers simultaneous mappings among the two image views and the\nsemantic side information. Therefore, it is desirable to consider fine-grained\nclasses mainly in the sketch domain using highly discriminative and\nsemantically rich feature space. However, the existing deep generative\nmodeling-based SBIR approaches majorly focus on bridging the gaps between the\nseen and unseen classes by generating pseudo-unseen-class samples. Besides,\nviolating the ZSL protocol by not utilizing any unseen-class information during\ntraining, such techniques do not pay explicit attention to modeling the\ndiscriminative nature of the shared space. Also, we note that learning a\nunified feature space for both the multi-view visual data is a tedious task\nconsidering the significant domain difference between sketches and color\nimages. In this respect, as a remedy, we introduce a novel framework for\nzero-shot SBIR. While we define a cross-modal triplet loss to ensure the\ndiscriminative nature of the shared space, an innovative cross-modal attention\nlearning strategy is also proposed to guide feature extraction from the image\ndomain exploiting information from the respective sketch counterpart. In order\nto preserve the semantic consistency of the shared space, we consider a graph\nCNN-based module that propagates the semantic class topology to the shared\nspace. To ensure an improved response time during inference, we further explore\nthe possibility of representing the shared space in terms of hash codes.\nExperimental results obtained on the benchmark TU-Berlin and the Sketchy\ndatasets confirm the superiority of CrossATNet in yielding state-of-the-art\nresults.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 12:11:12 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Chaudhuri", "Ushasi", ""], ["Banerjee", "Biplab", ""], ["Bhattacharya", "Avik", ""], ["Datcu", "Mihai", ""]]}, {"id": "2104.09949", "submitter": "Stylianos Venieris", "authors": "Mario Almeida, Stefanos Laskaridis, Stylianos I. Venieris, Ilias\n  Leontiadis, Nicholas D. Lane", "title": "DynO: Dynamic Onloading of Deep Neural Networks from Cloud to Device", "comments": "Under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, there has been an explosive growth of mobile and embedded\napplications using convolutional neural networks(CNNs). To alleviate their\nexcessive computational demands, developers have traditionally resorted to\ncloud offloading, inducing high infrastructure costs and a strong dependence on\nnetworking conditions. On the other end, the emergence of powerful SoCs is\ngradually enabling on-device execution. Nonetheless, low- and mid-tier\nplatforms still struggle to run state-of-the-art CNNs sufficiently. In this\npaper, we present DynO, a distributed inference framework that combines the\nbest of both worlds to address several challenges, such as device\nheterogeneity, varying bandwidth and multi-objective requirements. Key\ncomponents that enable this are its novel CNN-specific data packing method,\nwhich exploits the variability of precision needs in different parts of the CNN\nwhen onloading computation, and its novel scheduler that jointly tunes the\npartition point and transferred data precision at run time to adapt inference\nto its execution environment. Quantitative evaluation shows that DynO\noutperforms the current state-of-the-art, improving throughput by over an order\nof magnitude over device-only execution and up to 7.9x over competing CNN\noffloading systems, with up to 60x less data transferred.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 13:20:15 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Almeida", "Mario", ""], ["Laskaridis", "Stefanos", ""], ["Venieris", "Stylianos I.", ""], ["Leontiadis", "Ilias", ""], ["Lane", "Nicholas D.", ""]]}, {"id": "2104.09952", "submitter": "Yuan Zhi", "authors": "Yuan Zhi, Zhan Tong, Limin Wang, Gangshan Wu", "title": "MGSampler: An Explainable Sampling Strategy for Video Action Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Frame sampling is a fundamental problem in video action recognition due to\nthe essential redundancy in time and limited computation resources. The\nexisting sampling strategy often employs a fixed frame selection and lacks the\nflexibility to deal with complex variations in videos. In this paper, we\npresent an explainable, adaptive, and effective frame sampler, called\nMotion-guided Sampler (MGSampler). Our basic motivation is that motion is an\nimportant and universal signal that can drive us to select frames from videos\nadaptively. Accordingly, we propose two important properties in our MGSampler\ndesign: motion sensitive and motion uniform. First, we present two different\nmotion representations to enable us to efficiently distinguish the motion\nsalient frames from the background. Then, we devise a motion-uniform sampling\nstrategy based on the cumulative motion distribution to ensure the sampled\nframes evenly cover all the important frames with high motion saliency. Our\nMGSampler yields a new principled and holistic sample scheme, that could be\nincorporated into any existing video architecture. Experiments on five\nbenchmarks demonstrate the effectiveness of our MGSampler over previously fixed\nsampling strategies, and also its generalization power across different\nbackbones, video models, and datasets.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 13:24:01 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Zhi", "Yuan", ""], ["Tong", "Zhan", ""], ["Wang", "Limin", ""], ["Wu", "Gangshan", ""]]}, {"id": "2104.09957", "submitter": "Matt Groh", "authors": "Matthew Groh, Caleb Harris, Luis Soenksen, Felix Lau, Rachel Han,\n  Aerin Kim, Arash Koochek, Omar Badri", "title": "Evaluating Deep Neural Networks Trained on Clinical Images in\n  Dermatology with the Fitzpatrick 17k Dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  How does the accuracy of deep neural network models trained to classify\nclinical images of skin conditions vary across skin color? While recent studies\ndemonstrate computer vision models can serve as a useful decision support tool\nin healthcare and provide dermatologist-level classification on a number of\nspecific tasks, darker skin is underrepresented in the data. Most publicly\navailable data sets do not include Fitzpatrick skin type labels. We annotate\n16,577 clinical images sourced from two dermatology atlases with Fitzpatrick\nskin type labels and open-source these annotations. Based on these labels, we\nfind that there are significantly more images of light skin types than dark\nskin types in this dataset. We train a deep neural network model to classify\n114 skin conditions and find that the model is most accurate on skin types\nsimilar to those it was trained on. In addition, we evaluate how an algorithmic\napproach to identifying skin tones, individual typology angle, compares with\nFitzpatrick skin type labels annotated by a team of human labelers.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 13:37:30 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Groh", "Matthew", ""], ["Harris", "Caleb", ""], ["Soenksen", "Luis", ""], ["Lau", "Felix", ""], ["Han", "Rachel", ""], ["Kim", "Aerin", ""], ["Koochek", "Arash", ""], ["Badri", "Omar", ""]]}, {"id": "2104.09958", "submitter": "Martin Engelcke", "authors": "Martin Engelcke, Oiwi Parker Jones, Ingmar Posner", "title": "GENESIS-V2: Inferring Unordered Object Representations without Iterative\n  Refinement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advances in object-centric generative models (OCGMs) have culminated in the\ndevelopment of a broad range of methods for unsupervised object segmentation\nand interpretable object-centric scene generation. These methods, however, are\nlimited to simulated and real-world datasets with limited visual complexity.\nMoreover, object representations are often inferred using RNNs which do not\nscale well to large images or iterative refinement which avoids imposing an\nunnatural ordering on objects in an image but requires the a priori\ninitialisation of a fixed number of object representations. In contrast to\nestablished paradigms, this work proposes an embedding-based approach in which\nembeddings of pixels are clustered in a differentiable fashion using a\nstochastic, non-parametric stick-breaking process. Similar to iterative\nrefinement, this clustering procedure also leads to randomly ordered object\nrepresentations, but without the need of initialising a fixed number of\nclusters a priori. This is used to develop a new model, GENESIS-V2, which can\ninfer a variable number of object representations without using RNNs or\niterative refinement. We show that GENESIS-V2 outperforms previous methods for\nunsupervised image segmentation and object-centric scene generation on\nestablished synthetic datasets as well as more complex real-world datasets.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 14:59:27 GMT"}, {"version": "v2", "created": "Wed, 21 Apr 2021 14:52:11 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Engelcke", "Martin", ""], ["Jones", "Oiwi Parker", ""], ["Posner", "Ingmar", ""]]}, {"id": "2104.09993", "submitter": "Lo\\\"ic J\\'ez\\'equel", "authors": "Loic Jezequel, Ngoc-Son Vu, Jean Beaudet, Aymeric Histace", "title": "Fine-grained Anomaly Detection via Multi-task Self-Supervision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Detecting anomalies using deep learning has become a major challenge over the\nlast years, and is becoming increasingly promising in several fields. The\nintroduction of self-supervised learning has greatly helped many methods\nincluding anomaly detection where simple geometric transformation recognition\ntasks are used. However these methods do not perform well on fine-grained\nproblems since they lack finer features. By combining in a multi-task framework\nhigh-scale shape features oriented task with low-scale fine features oriented\ntask, our method greatly improves fine-grained anomaly detection. It\noutperforms state-of-the-art with up to 31% relative error reduction measured\nwith AUROC on various anomaly detection problems.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 14:19:08 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Jezequel", "Loic", ""], ["Vu", "Ngoc-Son", ""], ["Beaudet", "Jean", ""], ["Histace", "Aymeric", ""]]}, {"id": "2104.10003", "submitter": "Andrew Lauziere", "authors": "Andrew Lauziere, Ryan Christensen, Hari Shroff, Radu Balan", "title": "An Exact Hypergraph Matching Algorithm for Nuclear Identification in\n  Embryonic Caenorhabditis elegans", "comments": "20 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DM math.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Finding an optimal correspondence between point sets is a common task in\ncomputer vision. Existing techniques assume relatively simple relationships\namong points and do not guarantee an optimal match. We introduce an algorithm\ncapable of exactly solving point set matching by modeling the task as\nhypergraph matching. The algorithm extends the classical branch and bound\nparadigm to select and aggregate vertices under a proposed decomposition of the\nmultilinear objective function. The methodology is motivated by Caenorhabditis\nelegans, a model organism used frequently in developmental biology and\nneurobiology. The embryonic C. elegans contains seam cells that can act as\nfiducial markers allowing the identification of other nuclei during embryo\ndevelopment. The proposed algorithm identifies seam cells more accurately than\nestablished point-set matching methods, while providing a framework to approach\nother similarly complex point set matching tasks.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 14:34:30 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Lauziere", "Andrew", ""], ["Christensen", "Ryan", ""], ["Shroff", "Hari", ""], ["Balan", "Radu", ""]]}, {"id": "2104.10011", "submitter": "Daniel Koguciuk M.Sc.Eng.", "authors": "Daniel Koguciuk, Elahe Arani, Bahram Zonooz", "title": "Perceptual Loss for Robust Unsupervised Homography Estimation", "comments": "Accepted at Image Matching: Local Features & Beyond (CVPR 2021\n  Workshop)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Homography estimation is often an indispensable step in many computer vision\ntasks. The existing approaches, however, are not robust to illumination and/or\nlarger viewpoint changes. In this paper, we propose bidirectional implicit\nHomography Estimation (biHomE) loss for unsupervised homography estimation.\nbiHomE minimizes the distance in the feature space between the warped image\nfrom the source viewpoint and the corresponding image from the target\nviewpoint. Since we use a fixed pre-trained feature extractor and the only\nlearnable component of our framework is the homography network, we effectively\ndecouple the homography estimation from representation learning. We use an\nadditional photometric distortion step in the synthetic COCO dataset generation\nto better represent the illumination variation of the real-world scenarios. We\nshow that biHomE achieves state-of-the-art performance on synthetic COCO\ndataset, which is also comparable or better compared to supervised approaches.\nFurthermore, the empirical results demonstrate the robustness of our approach\nto illumination variation compared to existing methods.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 14:41:54 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Koguciuk", "Daniel", ""], ["Arani", "Elahe", ""], ["Zonooz", "Bahram", ""]]}, {"id": "2104.10029", "submitter": "Dongnan Liu", "authors": "Yang Ma, Chaoyi Zhang, Mariano Cabezas, Yang Song, Zihao Tang, Dongnan\n  Liu, Weidong Cai, Michael Barnett, Chenyu Wang", "title": "Multiple Sclerosis Lesion Analysis in Brain Magnetic Resonance Images:\n  Techniques and Clinical Applications", "comments": "12 pages, 5 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Multiple sclerosis (MS) is a chronic inflammatory and degenerative disease of\nthe central nervous system, characterized by the appearance of focal lesions in\nthe white and gray matter that topographically correlate with an individual\npatient's neurological symptoms and signs. Magnetic resonance imaging (MRI)\nprovides detailed in-vivo structural information, permitting the quantification\nand categorization of MS lesions that critically inform disease management.\nTraditionally, MS lesions have been manually annotated on 2D MRI slices, a\nprocess that is inefficient and prone to inter-/intra-observer errors.\nRecently, automated statistical imaging analysis techniques have been proposed\nto extract and segment MS lesions based on MRI voxel intensity. However, their\neffectiveness is limited by the heterogeneity of both MRI data acquisition\ntechniques and the appearance of MS lesions. By learning complex lesion\nrepresentations directly from images, deep learning techniques have achieved\nremarkable breakthroughs in the MS lesion segmentation task. Here, we provide a\ncomprehensive review of state-of-the-art automatic statistical and\ndeep-learning MS segmentation methods and discuss current and future clinical\napplications. Further, we review technical strategies, such as domain\nadaptation, to enhance MS lesion segmentation in real-world clinical settings.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 15:08:51 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Ma", "Yang", ""], ["Zhang", "Chaoyi", ""], ["Cabezas", "Mariano", ""], ["Song", "Yang", ""], ["Tang", "Zihao", ""], ["Liu", "Dongnan", ""], ["Cai", "Weidong", ""], ["Barnett", "Michael", ""], ["Wang", "Chenyu", ""]]}, {"id": "2104.10036", "submitter": "Pankaj Mishra", "authors": "Pankaj Mishra, Riccardo Verk, Daniele Fornasier, Claudio Piciarelli,\n  Gian Luca Foresti", "title": "VT-ADL: A Vision Transformer Network for Image Anomaly Detection and\n  Localization", "comments": "6 Pages, 4 images, conference published paper", "journal-ref": "IEEE ISIE 2021", "doi": null, "report-no": "KD-003638", "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a transformer-based image anomaly detection and localization\nnetwork. Our proposed model is a combination of a reconstruction-based approach\nand patch embedding. The use of transformer networks helps to preserve the\nspatial information of the embedded patches, which are later processed by a\nGaussian mixture density network to localize the anomalous areas. In addition,\nwe also publish BTAD, a real-world industrial anomaly dataset. Our results are\ncompared with other state-of-the-art algorithms using publicly available\ndatasets like MNIST and MVTec.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 15:12:30 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Mishra", "Pankaj", ""], ["Verk", "Riccardo", ""], ["Fornasier", "Daniele", ""], ["Piciarelli", "Claudio", ""], ["Foresti", "Gian Luca", ""]]}, {"id": "2104.10047", "submitter": "Ignacio Sarasua", "authors": "Ignacio Sarasua, Jonwong Lee, Christian Wachinger", "title": "Geometric Deep Learning on Anatomical Meshes for the Prediction of\n  Alzheimer's Disease", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Geometric deep learning can find representations that are optimal for a given\ntask and therefore improve the performance over pre-defined representations.\n  While current work has mainly focused on point representations, meshes also\ncontain connectivity information and are therefore a more comprehensive\ncharacterization of the underlying anatomical surface.\n  In this work, we evaluate four recent geometric deep learning approaches that\noperate on mesh representations.\n  These approaches can be grouped into template-free and template-based\napproaches, where the template-based methods need a more elaborate\npre-processing step with the definition of a common reference template and\ncorrespondences.\n  We compare the different networks for the prediction of Alzheimer's disease\nbased on the meshes of the hippocampus.\n  Our results show advantages for template-based methods in terms of accuracy,\nnumber of learnable parameters, and training speed.\n  While the template creation may be limiting for some applications,\nneuroimaging has a long history of building templates with automated tools\nreadily available.\n  Overall, working with meshes is more involved than working with simplistic\npoint clouds, but they also offer new avenues for designing geometric deep\nlearning architectures.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 15:17:13 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Sarasua", "Ignacio", ""], ["Lee", "Jonwong", ""], ["Wachinger", "Christian", ""]]}, {"id": "2104.10051", "submitter": "Steffen Czolbe", "authors": "Steffen Czolbe, Oswin Krause and Aasa Feragen", "title": "Semantic similarity metrics for learned image registration", "comments": "Published at MIDL 2021 (Oral). Reviews and discussion on Open Review:\n  https://openreview.net/forum?id=9M5cH--UdcC. arXiv admin note: text overlap\n  with arXiv:2011.05735", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a semantic similarity metric for image registration. Existing\nmetrics like Euclidean Distance or Normalized Cross-Correlation focus on\naligning intensity values, giving difficulties with low intensity contrast or\nnoise. Our approach learns dataset-specific features that drive the\noptimization of a learning-based registration model. We train both an\nunsupervised approach using an auto-encoder, and a semi-supervised approach\nusing supplemental segmentation data to extract semantic features for image\nregistration. Comparing to existing methods across multiple image modalities\nand applications, we achieve consistently high registration accuracy. A learned\ninvariance to noise gives smoother transformations on low-quality images.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 15:23:58 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Czolbe", "Steffen", ""], ["Krause", "Oswin", ""], ["Feragen", "Aasa", ""]]}, {"id": "2104.10054", "submitter": "Xiaohan Wang", "authors": "Xiaohan Wang, Linchao Zhu, Yi Yang", "title": "T2VLAD: Global-Local Sequence Alignment for Text-Video Retrieval", "comments": "Accepted to CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text-video retrieval is a challenging task that aims to search relevant video\ncontents based on natural language descriptions. The key to this problem is to\nmeasure text-video similarities in a joint embedding space. However, most\nexisting methods only consider the global cross-modal similarity and overlook\nthe local details. Some works incorporate the local comparisons through\ncross-modal local matching and reasoning. These complex operations introduce\ntremendous computation. In this paper, we design an efficient global-local\nalignment method. The multi-modal video sequences and text features are\nadaptively aggregated with a set of shared semantic centers. The local\ncross-modal similarities are computed between the video feature and text\nfeature within the same center. This design enables the meticulous local\ncomparison and reduces the computational cost of the interaction between each\ntext-video pair. Moreover, a global alignment method is proposed to provide a\nglobal cross-modal measurement that is complementary to the local perspective.\nThe global aggregated visual features also provide additional supervision,\nwhich is indispensable to the optimization of the learnable semantic centers.\nWe achieve consistent improvements on three standard text-video retrieval\nbenchmarks and outperform the state-of-the-art by a clear margin.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 15:26:24 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Wang", "Xiaohan", ""], ["Zhu", "Linchao", ""], ["Yang", "Yi", ""]]}, {"id": "2104.10064", "submitter": "Jiaxin Cheng", "authors": "Jiaxin Cheng, Ayush Jaiswal, Yue Wu, Pradeep Natarajan, Prem Natarajan", "title": "Style-Aware Normalized Loss for Improving Arbitrary Style Transfer", "comments": "Accepted as CVPR 2021 Oral Paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Neural Style Transfer (NST) has quickly evolved from single-style to\ninfinite-style models, also known as Arbitrary Style Transfer (AST). Although\nappealing results have been widely reported in literature, our empirical\nstudies on four well-known AST approaches (GoogleMagenta, AdaIN,\nLinearTransfer, and SANet) show that more than 50% of the time, AST stylized\nimages are not acceptable to human users, typically due to under- or\nover-stylization. We systematically study the cause of this imbalanced style\ntransferability (IST) and propose a simple yet effective solution to mitigate\nthis issue. Our studies show that the IST issue is related to the conventional\nAST style loss, and reveal that the root cause is the equal weightage of\ntraining samples irrespective of the properties of their corresponding style\nimages, which biases the model towards certain styles. Through investigation of\nthe theoretical bounds of the AST style loss, we propose a new loss that\nlargely overcomes IST. Theoretical analysis and experimental results validate\nthe effectiveness of our loss, with over 80% relative improvement in style\ndeception rate and 98% relatively higher preference in human evaluation.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 20:02:08 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Cheng", "Jiaxin", ""], ["Jaiswal", "Ayush", ""], ["Wu", "Yue", ""], ["Natarajan", "Pradeep", ""], ["Natarajan", "Prem", ""]]}, {"id": "2104.10065", "submitter": "Yingpeng Deng", "authors": "Yingpeng Deng and Lina J. Karam", "title": "Learning-based Compression for Material and Texture Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Learning-based image compression was shown to achieve a competitive\nperformance with state-of-the-art transform-based codecs. This motivated the\ndevelopment of new learning-based visual compression standards such as JPEG-AI.\nOf particular interest to these emerging standards is the development of\nlearning-based image compression systems targeting both humans and machines.\nThis paper is concerned with learning-based compression schemes whose\ncompressed-domain representations can be utilized to perform visual processing\nand computer vision tasks directly in the compressed domain. Such a\ncharacteristic has been incorporated as part of the scope and requirements of\nthe new emerging JPEG-AI standard. In our work, we adopt the learning-based\nJPEG-AI framework for performing material and texture recognition using the\ncompressed-domain latent representation at varing bit-rates. For comparison,\nperformance results are presented using compressed but fully decoded images in\nthe pixel domain as well as original uncompressed images. The obtained\nperformance results show that even though decoded images can degrade the\nclassification performance of the model trained with original images,\nretraining the model with decoded images will largely reduce the performance\ngap for the adopted texture dataset. It is also shown that the\ncompressed-domain classification can yield a competitive performance in terms\nof Top-1 and Top-5 accuracy while using a smaller reduced-complexity\nclassification model.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 23:16:26 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Deng", "Yingpeng", ""], ["Karam", "Lina J.", ""]]}, {"id": "2104.10068", "submitter": "Maria Koshkina", "authors": "Maria Koshkina, Hemanth Pidaparthy, James H. Elder", "title": "Contrastive Learning for Sports Video: Unsupervised Player\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We address the problem of unsupervised classification of players in a team\nsport according to their team affiliation, when jersey colours and design are\nnot known a priori. We adopt a contrastive learning approach in which an\nembedding network learns to maximize the distance between representations of\nplayers on different teams relative to players on the same team, in a purely\nunsupervised fashion, without any labelled data. We evaluate the approach using\na new hockey dataset and find that it outperforms prior unsupervised approaches\nby a substantial margin, particularly for real-time application when only a\nsmall number of frames are available for unsupervised learning before team\nassignments must be made. Remarkably, we show that our contrastive method\nachieves 94% accuracy after unsupervised training on only a single frame, with\naccuracy rising to 97% within 500 frames (17 seconds of game time). We further\ndemonstrate how accurate team classification allows accurate team-conditional\nheat maps of player positioning to be computed.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 20:24:02 GMT"}, {"version": "v2", "created": "Mon, 3 May 2021 18:30:09 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Koshkina", "Maria", ""], ["Pidaparthy", "Hemanth", ""], ["Elder", "James H.", ""]]}, {"id": "2104.10078", "submitter": "Michael Oechsle", "authors": "Michael Oechsle, Songyou Peng, Andreas Geiger", "title": "UNISURF: Unifying Neural Implicit Surfaces and Radiance Fields for\n  Multi-View Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Neural implicit 3D representations have emerged as a powerful paradigm for\nreconstructing surfaces from multi-view images and synthesizing novel views.\nUnfortunately, existing methods such as DVR or IDR require accurate per-pixel\nobject masks as supervision. At the same time, neural radiance fields have\nrevolutionized novel view synthesis. However, NeRF's estimated volume density\ndoes not admit accurate surface reconstruction. Our key insight is that\nimplicit surface models and radiance fields can be formulated in a unified way,\nenabling both surface and volume rendering using the same model. This unified\nperspective enables novel, more efficient sampling procedures and the ability\nto reconstruct accurate surfaces without input masks. We compare our method on\nthe DTU, BlendedMVS, and a synthetic indoor dataset. Our experiments\ndemonstrate that we outperform NeRF in terms of reconstruction quality while\nperforming on par with IDR without requiring masks.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 15:59:38 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Oechsle", "Michael", ""], ["Peng", "Songyou", ""], ["Geiger", "Andreas", ""]]}, {"id": "2104.10093", "submitter": "Gido van de Ven", "authors": "Gido M. van de Ven, Zhe Li, Andreas S. Tolias", "title": "Class-Incremental Learning with Generative Classifiers", "comments": "To appear in the IEEE Conference on Computer Vision and Pattern\n  Recognition Workshop (CVPR-W) on Continual Learning in Computer Vision\n  (CLVision) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Incrementally training deep neural networks to recognize new classes is a\nchallenging problem. Most existing class-incremental learning methods store\ndata or use generative replay, both of which have drawbacks, while\n'rehearsal-free' alternatives such as parameter regularization or\nbias-correction methods do not consistently achieve high performance. Here, we\nput forward a new strategy for class-incremental learning: generative\nclassification. Rather than directly learning the conditional distribution\np(y|x), our proposal is to learn the joint distribution p(x,y), factorized as\np(x|y)p(y), and to perform classification using Bayes' rule. As a\nproof-of-principle, here we implement this strategy by training a variational\nautoencoder for each class to be learned and by using importance sampling to\nestimate the likelihoods p(x|y). This simple approach performs very well on a\ndiverse set of continual learning benchmarks, outperforming generative replay\nand other existing baselines that do not store data.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 16:26:14 GMT"}, {"version": "v2", "created": "Wed, 28 Apr 2021 09:19:48 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["van de Ven", "Gido M.", ""], ["Li", "Zhe", ""], ["Tolias", "Andreas S.", ""]]}, {"id": "2104.10116", "submitter": "Joshua Ebenezer", "authors": "Joshua P. Ebenezer, Yongjun Wu, Hai Wei, Sriram Sethuraman, Zongyi Liu", "title": "Detection of Audio-Video Synchronization Errors Via Event Detection", "comments": "To be published in ICASSP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present a new method and a large-scale database to detect audio-video\nsynchronization(A/V sync) errors in tennis videos. A deep network is trained to\ndetect the visual signature of the tennis ball being hit by the racquet in the\nvideo stream. Another deep network is trained to detect the auditory signature\nof the same event in the audio stream. During evaluation, the audio stream is\nsearched by the audio network for the audio event of the ball being hit. If the\nevent is found in audio, the neighboring interval in video is searched for the\ncorresponding visual signature. If the event is not found in the video stream\nbut is found in the audio stream, A/V sync error is flagged. We developed a\nlarge-scaled database of 504,300 frames from 6 hours of videos of tennis\nevents, simulated A/V sync errors, and found our method achieves high accuracy\non the task.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 16:54:44 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Ebenezer", "Joshua P.", ""], ["Wu", "Yongjun", ""], ["Wei", "Hai", ""], ["Sethuraman", "Sriram", ""], ["Liu", "Zongyi", ""]]}, {"id": "2104.10122", "submitter": "Ali Abedi", "authors": "Ali Abedi and Shehroz S. Khan", "title": "Improving state-of-the-art in Detecting Student Engagement with Resnet\n  and TCN Hybrid Network", "comments": "7 pages, 3 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Automatic detection of students' engagement in online learning settings is a\nkey element to improve the quality of learning and to deliver personalized\nlearning materials to them. Varying levels of engagement exhibited by students\nin an online classroom is an affective behavior that takes place over space and\ntime. Therefore, we formulate detecting levels of students' engagement from\nvideos as a spatio-temporal classification problem. In this paper, we present a\nnovel end-to-end Residual Network (ResNet) and Temporal Convolutional Network\n(TCN) hybrid neural network architecture for students' engagement level\ndetection in videos. The 2D ResNet extracts spatial features from consecutive\nvideo frames, and the TCN analyzes the temporal changes in video frames to\ndetect the level of engagement. The spatial and temporal arms of the hybrid\nnetwork are jointly trained on raw video frames of a large publicly available\nstudents' engagement detection dataset, DAiSEE. We compared our method with\nseveral competing students' engagement detection methods on this dataset. The\nResNet+TCN architecture outperforms all other studied methods, improves the\nstate-of-the-art engagement level detection accuracy, and sets a new baseline\nfor future research.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 17:10:13 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Abedi", "Ali", ""], ["Khan", "Shehroz S.", ""]]}, {"id": "2104.10127", "submitter": "Yuchao Dai Dr.", "authors": "Yuxin Mao, Jing Zhang, Zhexiong Wan, Yuchao Dai, Aixuan Li, Yunqiu Lv,\n  Xinyu Tian, Deng-Ping Fan, and Nick Barnes", "title": "Transformer Transforms Salient Object Detection and Camouflaged Object\n  Detection", "comments": "Technical report, 18 pages, 22 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The transformer networks are particularly good at modeling long-range\ndependencies within a long sequence. In this paper, we conduct research on\napplying the transformer networks for salient object detection (SOD). We adopt\nthe dense transformer backbone for fully supervised RGB image based SOD, RGB-D\nimage pair based SOD, and weakly supervised SOD within a unified framework\nbased on the observation that the transformer backbone can provide accurate\nstructure modeling, which makes it powerful in learning from weak labels with\nless structure information. Further, we find that the vision transformer\narchitectures do not offer direct spatial supervision, instead encoding\nposition as a feature. Therefore, we investigate the contributions of two\nstrategies to provide stronger spatial supervision through the transformer\nlayers within our unified framework, namely deep supervision and\ndifficulty-aware learning. We find that deep supervision can get gradients back\ninto the higher level features, thus leads to uniform activation within the\nsame semantic object. Difficulty-aware learning on the other hand is capable of\nidentifying the hard pixels for effective hard negative mining. We also\nvisualize features of conventional backbone and transformer backbone before and\nafter fine-tuning them for SOD, and find that transformer backbone encodes more\naccurate object structure information and more distinct semantic information\nwithin the lower and higher level features respectively. We also apply our\nmodel to camouflaged object detection (COD) and achieve similar observations as\nthe above three SOD tasks. Extensive experimental results on various SOD and\nCOD tasks illustrate that transformer networks can transform SOD and COD,\nleading to new benchmarks for each related task. The source code and\nexperimental results are available via our project page:\nhttps://github.com/fupiao1998/TrasformerSOD.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 17:12:51 GMT"}, {"version": "v2", "created": "Sat, 26 Jun 2021 04:21:59 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Mao", "Yuxin", ""], ["Zhang", "Jing", ""], ["Wan", "Zhexiong", ""], ["Dai", "Yuchao", ""], ["Li", "Aixuan", ""], ["Lv", "Yunqiu", ""], ["Tian", "Xinyu", ""], ["Fan", "Deng-Ping", ""], ["Barnes", "Nick", ""]]}, {"id": "2104.10133", "submitter": "Scott Ettinger", "authors": "Scott Ettinger, Shuyang Cheng, Benjamin Caine, Chenxi Liu, Hang Zhao,\n  Sabeek Pradhan, Yuning Chai, Ben Sapp, Charles Qi, Yin Zhou, Zoey Yang,\n  Aurelien Chouard, Pei Sun, Jiquan Ngiam, Vijay Vasudevan, Alexander McCauley,\n  Jonathon Shlens, Dragomir Anguelov", "title": "Large Scale Interactive Motion Forecasting for Autonomous Driving : The\n  Waymo Open Motion Dataset", "comments": "15 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As autonomous driving systems mature, motion forecasting has received\nincreasing attention as a critical requirement for planning. Of particular\nimportance are interactive situations such as merges, unprotected turns, etc.,\nwhere predicting individual object motion is not sufficient. Joint predictions\nof multiple objects are required for effective route planning. There has been a\ncritical need for high-quality motion data that is rich in both interactions\nand annotation to develop motion planning models. In this work, we introduce\nthe most diverse interactive motion dataset to our knowledge, and provide\nspecific labels for interacting objects suitable for developing joint\nprediction models. With over 100,000 scenes, each 20 seconds long at 10 Hz, our\nnew dataset contains more than 570 hours of unique data over 1750 km of\nroadways. It was collected by mining for interesting interactions between\nvehicles, pedestrians, and cyclists across six cities within the United States.\nWe use a high-accuracy 3D auto-labeling system to generate high quality 3D\nbounding boxes for each road agent, and provide corresponding high definition\n3D maps for each scene. Furthermore, we introduce a new set of metrics that\nprovides a comprehensive evaluation of both single agent and joint agent\ninteraction motion forecasting models. Finally, we provide strong baseline\nmodels for individual-agent prediction and joint-prediction. We hope that this\nnew large-scale interactive motion dataset will provide new opportunities for\nadvancing motion forecasting models.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 17:19:05 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Ettinger", "Scott", ""], ["Cheng", "Shuyang", ""], ["Caine", "Benjamin", ""], ["Liu", "Chenxi", ""], ["Zhao", "Hang", ""], ["Pradhan", "Sabeek", ""], ["Chai", "Yuning", ""], ["Sapp", "Ben", ""], ["Qi", "Charles", ""], ["Zhou", "Yin", ""], ["Yang", "Zoey", ""], ["Chouard", "Aurelien", ""], ["Sun", "Pei", ""], ["Ngiam", "Jiquan", ""], ["Vasudevan", "Vijay", ""], ["McCauley", "Alexander", ""], ["Shlens", "Jonathon", ""], ["Anguelov", "Dragomir", ""]]}, {"id": "2104.10154", "submitter": "Liang Pan", "authors": "Liang Pan, Xinyi Chen, Zhongang Cai, Junzhe Zhang, Haiyu Zhao, Shuai\n  Yi, Ziwei Liu", "title": "Variational Relational Point Completion Network", "comments": "15 pages, 13 figures, accepted to CVPR 2021 (Oral), project webpage:\n  https://paul007pl.github.io/projects/VRCNet.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-scanned point clouds are often incomplete due to viewpoint, occlusion,\nand noise. Existing point cloud completion methods tend to generate global\nshape skeletons and hence lack fine local details. Furthermore, they mostly\nlearn a deterministic partial-to-complete mapping, but overlook structural\nrelations in man-made objects. To tackle these challenges, this paper proposes\na variational framework, Variational Relational point Completion network\n(VRCNet) with two appealing properties: 1) Probabilistic Modeling. In\nparticular, we propose a dual-path architecture to enable principled\nprobabilistic modeling across partial and complete clouds. One path consumes\ncomplete point clouds for reconstruction by learning a point VAE. The other\npath generates complete shapes for partial point clouds, whose embedded\ndistribution is guided by distribution obtained from the reconstruction path\nduring training. 2) Relational Enhancement. Specifically, we carefully design\npoint self-attention kernel and point selective kernel module to exploit\nrelational point features, which refines local shape details conditioned on the\ncoarse completion. In addition, we contribute a multi-view partial point cloud\ndataset (MVP dataset) containing over 100,000 high-quality scans, which renders\npartial 3D shapes from 26 uniformly distributed camera poses for each 3D CAD\nmodel. Extensive experiments demonstrate that VRCNet outperforms\nstate-of-theart methods on all standard point cloud completion benchmarks.\nNotably, VRCNet shows great generalizability and robustness on real-world point\ncloud scans.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 17:53:40 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Pan", "Liang", ""], ["Chen", "Xinyi", ""], ["Cai", "Zhongang", ""], ["Zhang", "Junzhe", ""], ["Zhao", "Haiyu", ""], ["Yi", "Shuai", ""], ["Liu", "Ziwei", ""]]}, {"id": "2104.10156", "submitter": "Yi-Wen Chen", "authors": "Yi-Wen Chen, Yi-Hsuan Tsai, Ming-Hsuan Yang", "title": "Understanding Synonymous Referring Expressions via Contrastive Features", "comments": "Codes and models will be available at\n  https://github.com/wenz116/RefContrast", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Referring expression comprehension aims to localize objects identified by\nnatural language descriptions. This is a challenging task as it requires\nunderstanding of both visual and language domains. One nature is that each\nobject can be described by synonymous sentences with paraphrases, and such\nvarieties in languages have critical impact on learning a comprehension model.\nWhile prior work usually treats each sentence and attends it to an object\nseparately, we focus on learning a referring expression comprehension model\nthat considers the property in synonymous sentences. To this end, we develop an\nend-to-end trainable framework to learn contrastive features on the image and\nobject instance levels, where features extracted from synonymous sentences to\ndescribe the same object should be closer to each other after mapping to the\nvisual domain. We conduct extensive experiments to evaluate the proposed\nalgorithm on several benchmark datasets, and demonstrate that our method\nperforms favorably against the state-of-the-art approaches. Furthermore, since\nthe varieties in expressions become larger across datasets when they describe\nobjects in different ways, we present the cross-dataset and transfer learning\nsettings to validate the ability of our learned transferable features.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 17:56:24 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Chen", "Yi-Wen", ""], ["Tsai", "Yi-Hsuan", ""], ["Yang", "Ming-Hsuan", ""]]}, {"id": "2104.10157", "submitter": "Wilson Yan", "authors": "Wilson Yan, Yunzhi Zhang, Pieter Abbeel, Aravind Srinivas", "title": "VideoGPT: Video Generation using VQ-VAE and Transformers", "comments": "Project website: https://wilson1yan.github.io/videogpt/index.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present VideoGPT: a conceptually simple architecture for scaling\nlikelihood based generative modeling to natural videos. VideoGPT uses VQ-VAE\nthat learns downsampled discrete latent representations of a raw video by\nemploying 3D convolutions and axial self-attention. A simple GPT-like\narchitecture is then used to autoregressively model the discrete latents using\nspatio-temporal position encodings. Despite the simplicity in formulation and\nease of training, our architecture is able to generate samples competitive with\nstate-of-the-art GAN models for video generation on the BAIR Robot dataset, and\ngenerate high fidelity natural images from UCF-101 and Tumbler GIF Dataset\n(TGIF). We hope our proposed architecture serves as a reproducible reference\nfor a minimalistic implementation of transformer based video generation models.\nSamples and code are available at\nhttps://wilson1yan.github.io/videogpt/index.html\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 17:58:03 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Yan", "Wilson", ""], ["Zhang", "Yunzhi", ""], ["Abbeel", "Pieter", ""], ["Srinivas", "Aravind", ""]]}, {"id": "2104.10195", "submitter": "Yingda Xia", "authors": "Yingda Xia, Dong Yang, Wenqi Li, Andriy Myronenko, Daguang Xu,\n  Hirofumi Obinata, Hitoshi Mori, Peng An, Stephanie Harmon, Evrim Turkbey,\n  Baris Turkbey, Bradford Wood, Francesca Patella, Elvira Stellato, Gianpaolo\n  Carrafiello, Anna Ierardi, Alan Yuille, Holger Roth", "title": "Auto-FedAvg: Learnable Federated Averaging for Multi-Institutional\n  Medical Image Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Federated learning (FL) enables collaborative model training while preserving\neach participant's privacy, which is particularly beneficial to the medical\nfield. FedAvg is a standard algorithm that uses fixed weights, often\noriginating from the dataset sizes at each client, to aggregate the distributed\nlearned models on a server during the FL process. However, non-identical data\ndistribution across clients, known as the non-i.i.d problem in FL, could make\nthis assumption for setting fixed aggregation weights sub-optimal. In this\nwork, we design a new data-driven approach, namely Auto-FedAvg, where\naggregation weights are dynamically adjusted, depending on data distributions\nacross data silos and the current training progress of the models. We\ndisentangle the parameter set into two parts, local model parameters and global\naggregation parameters, and update them iteratively with a\ncommunication-efficient algorithm. We first show the validity of our approach\nby outperforming state-of-the-art FL methods for image recognition on a\nheterogeneous data split of CIFAR-10. Furthermore, we demonstrate our\nalgorithm's effectiveness on two multi-institutional medical image analysis\ntasks, i.e., COVID-19 lesion segmentation in chest CT and pancreas segmentation\nin abdominal CT.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 18:29:44 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Xia", "Yingda", ""], ["Yang", "Dong", ""], ["Li", "Wenqi", ""], ["Myronenko", "Andriy", ""], ["Xu", "Daguang", ""], ["Obinata", "Hirofumi", ""], ["Mori", "Hitoshi", ""], ["An", "Peng", ""], ["Harmon", "Stephanie", ""], ["Turkbey", "Evrim", ""], ["Turkbey", "Baris", ""], ["Wood", "Bradford", ""], ["Patella", "Francesca", ""], ["Stellato", "Elvira", ""], ["Carrafiello", "Gianpaolo", ""], ["Ierardi", "Anna", ""], ["Yuille", "Alan", ""], ["Roth", "Holger", ""]]}, {"id": "2104.10223", "submitter": "Luis Oala", "authors": "Saul Calderon-Ramirez and Luis Oala", "title": "More Than Meets The Eye: Semi-supervised Learning Under Non-IID Data", "comments": "Presented as a RobustML workshop paper at ICLR 2021. Both authors\n  contributed equally. This article extends arXiv:2006.07767", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A common heuristic in semi-supervised deep learning (SSDL) is to select\nunlabelled data based on a notion of semantic similarity to the labelled data.\nFor example, labelled images of numbers should be paired with unlabelled images\nof numbers instead of, say, unlabelled images of cars. We refer to this\npractice as semantic data set matching. In this work, we demonstrate the limits\nof semantic data set matching. We show that it can sometimes even degrade the\nperformance for a state of the art SSDL algorithm. We present and make\navailable a comprehensive simulation sandbox, called non-IID-SSDL, for stress\ntesting an SSDL algorithm under different degrees of distribution mismatch\nbetween the labelled and unlabelled data sets. In addition, we demonstrate that\nsimple density based dissimilarity measures in the feature space of a generic\nclassifier offer a promising and more reliable quantitative matching criterion\nto select unlabelled data before SSDL training.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 19:51:10 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Calderon-Ramirez", "Saul", ""], ["Oala", "Luis", ""]]}, {"id": "2104.10249", "submitter": "Saba Dadsetan", "authors": "Saba Dadsetan, David Pichler, David Wilson, Naira Hovakimyan, Jennifer\n  Hobbs", "title": "Superpixels and Graph Convolutional Neural Networks for Efficient\n  Detection of Nutrient Deficiency Stress from Aerial Imagery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Advances in remote sensing technology have led to the capture of massive\namounts of data. Increased image resolution, more frequent revisit times, and\nadditional spectral channels have created an explosion in the amount of data\nthat is available to provide analyses and intelligence across domains,\nincluding agriculture. However, the processing of this data comes with a cost\nin terms of computation time and money, both of which must be considered when\nthe goal of an algorithm is to provide real-time intelligence to improve\nefficiencies. Specifically, we seek to identify nutrient deficient areas from\nremotely sensed data to alert farmers to regions that require attention;\ndetection of nutrient deficient areas is a key task in precision agriculture as\nfarmers must quickly respond to struggling areas to protect their harvests.\nPast methods have focused on pixel-level classification (i.e. semantic\nsegmentation) of the field to achieve these tasks, often using deep learning\nmodels with tens-of-millions of parameters. In contrast, we propose a much\nlighter graph-based method to perform node-based classification. We first use\nSimple Linear Iterative Cluster (SLIC) to produce superpixels across the field.\nThen, to perform segmentation across the non-Euclidean domain of superpixels,\nwe leverage a Graph Convolutional Neural Network (GCN). This model has\n4-orders-of-magnitude fewer parameters than a CNN model and trains in a matter\nof minutes.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 21:18:16 GMT"}, {"version": "v2", "created": "Thu, 22 Apr 2021 00:44:11 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Dadsetan", "Saba", ""], ["Pichler", "David", ""], ["Wilson", "David", ""], ["Hovakimyan", "Naira", ""], ["Hobbs", "Jennifer", ""]]}, {"id": "2104.10252", "submitter": "Marcella Cornia", "authors": "Samuele Poppi, Marcella Cornia, Lorenzo Baraldi, Rita Cucchiara", "title": "Revisiting The Evaluation of Class Activation Mapping for\n  Explainability: A Novel Metric and Experimental Analysis", "comments": "CVPR 2021 Workshop on Responsible Computer Vision", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the request for deep learning solutions increases, the need for\nexplainability is even more fundamental. In this setting, particular attention\nhas been given to visualization techniques, that try to attribute the right\nrelevance to each input pixel with respect to the output of the network. In\nthis paper, we focus on Class Activation Mapping (CAM) approaches, which\nprovide an effective visualization by taking weighted averages of the\nactivation maps. To enhance the evaluation and the reproducibility of such\napproaches, we propose a novel set of metrics to quantify explanation maps,\nwhich show better effectiveness and simplify comparisons between approaches. To\nevaluate the appropriateness of the proposal, we compare different CAM-based\nvisualization methods on the entire ImageNet validation set, fostering proper\ncomparisons and reproducibility.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 21:34:24 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Poppi", "Samuele", ""], ["Cornia", "Marcella", ""], ["Baraldi", "Lorenzo", ""], ["Cucchiara", "Rita", ""]]}, {"id": "2104.10268", "submitter": "Fayaz Ali Dharejo", "authors": "Fayaz Ali Dharejo, Farah Deeba, Yuanchun Zhou, Bhagwan Das, Munsif Ali\n  Jatoi, Muhammad Zawish, Yi Du, and Xuezhi Wang", "title": "TWIST-GAN: Towards Wavelet Transform and Transferred GAN for\n  Spatio-Temporal Single Image Super Resolution", "comments": "Accepted: ACM TIST (10-03-2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Single Image Super-resolution (SISR) produces high-resolution images with\nfine spatial resolutions from aremotely sensed image with low spatial\nresolution. Recently, deep learning and generative adversarial networks(GANs)\nhave made breakthroughs for the challenging task of single image\nsuper-resolution (SISR). However, thegenerated image still suffers from\nundesirable artifacts such as, the absence of texture-feature representationand\nhigh-frequency information. We propose a frequency domain-based spatio-temporal\nremote sensingsingle image super-resolution technique to reconstruct the HR\nimage combined with generative adversarialnetworks (GANs) on various frequency\nbands (TWIST-GAN). We have introduced a new method incorporatingWavelet\nTransform (WT) characteristics and transferred generative adversarial network.\nThe LR image hasbeen split into various frequency bands by using the WT,\nwhereas, the transfer generative adversarial networkpredicts high-frequency\ncomponents via a proposed architecture. Finally, the inverse transfer of\nwaveletsproduces a reconstructed image with super-resolution. The model is\nfirst trained on an external DIV2 Kdataset and validated with the UC Merceed\nLandsat remote sensing dataset and Set14 with each image sizeof 256x256.\nFollowing that, transferred GANs are used to process spatio-temporal remote\nsensing images inorder to minimize computation cost differences and improve\ntexture information. The findings are comparedqualitatively and qualitatively\nwith the current state-of-art approaches. In addition, we saved about 43% of\ntheGPU memory during training and accelerated the execution of our simplified\nversion by eliminating batchnormalization layers.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 22:12:38 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Dharejo", "Fayaz Ali", ""], ["Deeba", "Farah", ""], ["Zhou", "Yuanchun", ""], ["Das", "Bhagwan", ""], ["Jatoi", "Munsif Ali", ""], ["Zawish", "Muhammad", ""], ["Du", "Yi", ""], ["Wang", "Xuezhi", ""]]}, {"id": "2104.10273", "submitter": "Anis Kacem", "authors": "Anis Kacem, Kseniya Cherenkova, Djamila Aouada", "title": "Disentangled Face Identity Representations for joint 3D Face Recognition\n  and Expression Neutralisation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we propose a new deep learning-based approach for\ndisentangling face identity representations from expressive 3D faces. Given a\n3D face, our approach not only extracts a disentangled identity representation\nbut also generates a realistic 3D face with a neutral expression while\npredicting its identity. The proposed network consists of three components; (1)\na Graph Convolutional Autoencoder (GCA) to encode the 3D faces into latent\nrepresentations, (2) a Generative Adversarial Network (GAN) that translates the\nlatent representations of expressive faces into those of neutral faces, (3) and\nan identity recognition sub-network taking advantage of the neutralized latent\nrepresentations for 3D face recognition. The whole network is trained in an\nend-to-end manner. Experiments are conducted on three publicly available\ndatasets showing the effectiveness of the proposed approach.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 22:33:10 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Kacem", "Anis", ""], ["Cherenkova", "Kseniya", ""], ["Aouada", "Djamila", ""]]}, {"id": "2104.10278", "submitter": "Jose M. Saavedra PhD", "authors": "Pablo Torres and Jose M. Saavedra", "title": "Compact and Effective Representations for Sketch-based Image Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sketch-based image retrieval (SBIR) has undergone an increasing interest in\nthe community of computer vision bringing high impact in real applications. For\ninstance, SBIR brings an increased benefit to eCommerce search engines because\nit allows users to formulate a query just by drawing what they need to buy.\nHowever, current methods showing high precision in retrieval work in a high\ndimensional space, which negatively affects aspects like memory consumption and\ntime processing. Although some authors have also proposed compact\nrepresentations, these drastically degrade the performance in a low dimension.\nTherefore in this work, we present different results of evaluating methods for\nproducing compact embeddings in the context of sketch-based image retrieval.\nOur main interest is in strategies aiming to keep the local structure of the\noriginal space. The recent unsupervised local-topology preserving dimension\nreduction method UMAP fits our requirements and shows outstanding performance,\nimproving even the precision achieved by SOTA methods. We evaluate six methods\nin two different datasets. We use Flickr15K and eCommerce datasets; the latter\nis another contribution of this work. We show that UMAP allows us to have\nfeature vectors of 16 bytes improving precision by more than 35%.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 22:48:19 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Torres", "Pablo", ""], ["Saavedra", "Jose M.", ""]]}, {"id": "2104.10283", "submitter": "Weixin Liang", "authors": "Weixin Liang, Yanhao Jiang and Zixuan Liu", "title": "GraghVQA: Language-Guided Graph Neural Networks for Graph-based Visual\n  Question Answering", "comments": "NAACL 2021 MAI-Workshop. Code available at\n  https://github.com/codexxxl/GraphVQA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Images are more than a collection of objects or attributes -- they represent\na web of relationships among interconnected objects. Scene Graph has emerged as\na new modality for a structured graphical representation of images. Scene Graph\nencodes objects as nodes connected via pairwise relations as edges. To support\nquestion answering on scene graphs, we propose GraphVQA, a language-guided\ngraph neural network framework that translates and executes a natural language\nquestion as multiple iterations of message passing among graph nodes. We\nexplore the design space of GraphVQA framework, and discuss the trade-off of\ndifferent design choices. Our experiments on GQA dataset show that GraphVQA\noutperforms the state-of-the-art model by a large margin (88.43% vs. 94.78%).\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 23:54:41 GMT"}, {"version": "v2", "created": "Wed, 2 Jun 2021 05:29:00 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Liang", "Weixin", ""], ["Jiang", "Yanhao", ""], ["Liu", "Zixuan", ""]]}, {"id": "2104.10291", "submitter": "Alexander Mai", "authors": "Alexander Mai, Allen Yang, Dominique E. Meyer", "title": "Soft Expectation and Deep Maximization for Image Feature Detection", "comments": "9 pages, 3 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Central to the application of many multi-view geometry algorithms is the\nextraction of matching points between multiple viewpoints, enabling classical\ntasks such as camera pose estimation and 3D reconstruction. Over the decades,\nmany approaches that characterize these points have been proposed based on\nhand-tuned appearance models and more recently data-driven learning methods. We\npropose SEDM, an iterative semi-supervised learning process that flips the\nquestion and first looks for repeatable 3D points, then trains a detector to\nlocalize them in image space. Our technique poses the problem as one of\nexpectation maximization (EM), where the likelihood of the detector locating\nthe 3D points is the objective function to be maximized. We utilize the\ngeometry of the scene to refine the estimates of the location of these 3D\npoints and produce a new pseudo ground truth during the expectation step, then\ntrain a detector to predict this pseudo ground truth in the maximization step.\nWe apply our detector to standard benchmarks in visual localization, sparse 3D\nreconstruction, and mean matching accuracy. Our results show that this new\nmodel trained using SEDM is able to better localize the underlying 3D points in\na scene, improving mean SfM quality by $-0.15\\pm0.11$ mean reprojection error\nwhen compared to SuperPoint or $-0.38\\pm0.23$ when compared to R2D2.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 00:35:32 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Mai", "Alexander", ""], ["Yang", "Allen", ""], ["Meyer", "Dominique E.", ""]]}, {"id": "2104.10299", "submitter": "Cho-Ying Wu", "authors": "Cho-Ying Wu, Ke Xu, Chin-Cheng Hsu, Ulrich Neumann", "title": "Voice2Mesh: Cross-Modal 3D Face Model Generation from Voices", "comments": "Project page: https://choyingw.github.io/works/Voice2Mesh/index.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV cs.LG cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work focuses on the analysis that whether 3D face models can be learned\nfrom only the speech inputs of speakers. Previous works for cross-modal face\nsynthesis study image generation from voices. However, image synthesis includes\nvariations such as hairstyles, backgrounds, and facial textures, that are\narguably irrelevant to voice or without direct studies to show correlations. We\ninstead investigate the ability to reconstruct 3D faces to concentrate on only\ngeometry, which is more physiologically grounded. We propose both the\nsupervised learning and unsupervised learning frameworks. Especially we\ndemonstrate how unsupervised learning is possible in the absence of a direct\nvoice-to-3D-face dataset under limited availability of 3D face scans when the\nmodel is equipped with knowledge distillation. To evaluate the performance, we\nalso propose several metrics to measure the geometric fitness of two 3D faces\nbased on points, lines, and regions. We find that 3D face shapes can be\nreconstructed from voices. Experimental results suggest that 3D faces can be\nreconstructed from voices, and our method can improve the performance over the\nbaseline. The best performance gains (15% - 20%) on ear-to-ear distance ratio\nmetric (ER) coincides with the intuition that one can roughly envision whether\na speaker's face is overall wider or thinner only from a person's voice. See\nour project page for codes and data.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 01:14:50 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Wu", "Cho-Ying", ""], ["Xu", "Ke", ""], ["Hsu", "Chin-Cheng", ""], ["Neumann", "Ulrich", ""]]}, {"id": "2104.10315", "submitter": "Zhimeng Huang", "authors": "Zhimeng Huang, Chuanmin Jia, Shanshe Wang, Siwei Ma", "title": "Visual Analysis Motivated Rate-Distortion Model for Image Coding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimized for pixel fidelity metrics, images compressed by existing image\ncodec are facing systematic challenges when used for visual analysis tasks,\nespecially under low-bitrate coding. This paper proposes a visual\nanalysis-motivated rate-distortion model for Versatile Video Coding (VVC) intra\ncompression. The proposed model has two major contributions, a novel rate\nallocation strategy and a new distortion measurement model. We first propose\nthe region of interest for machine (ROIM) to evaluate the degree of importance\nfor each coding tree unit (CTU) in visual analysis. Then, a novel CTU-level bit\nallocation model is proposed based on ROIM and the local texture\ncharacteristics of each CTU. After an in-depth analysis of multiple distortion\nmodels, a visual analysis friendly distortion criteria is subsequently proposed\nby extracting deep feature of each coding unit (CU). To alleviate the problem\nof lacking spatial context information when calculating the distortion of each\nCU, we finally propose a multi-scale feature distortion (MSFD) metric using\ndifferent neighboring pixels by weighting the extracted deep features in each\nscale. Extensive experimental results show that the proposed scheme could\nachieve up to 28.17\\% bitrate saving under the same analysis performance among\nseveral typical visual analysis tasks such as image classification, object\ndetection, and semantic segmentation.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 02:27:34 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Huang", "Zhimeng", ""], ["Jia", "Chuanmin", ""], ["Wang", "Shanshe", ""], ["Ma", "Siwei", ""]]}, {"id": "2104.10325", "submitter": "Sanghyun Son", "authors": "Sanghyun Son and Kyoung Mu Lee", "title": "SRWarp: Generalized Image Super-Resolution under Arbitrary\n  Transformation", "comments": "Accepted to CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Deep CNNs have achieved significant successes in image processing and its\napplications, including single image super-resolution (SR). However,\nconventional methods still resort to some predetermined integer scaling\nfactors, e.g., x2 or x4. Thus, they are difficult to be applied when arbitrary\ntarget resolutions are required. Recent approaches extend the scope to\nreal-valued upsampling factors, even with varying aspect ratios to handle the\nlimitation. In this paper, we propose the SRWarp framework to further\ngeneralize the SR tasks toward an arbitrary image transformation. We interpret\nthe traditional image warping task, specifically when the input is enlarged, as\na spatially-varying SR problem. We also propose several novel formulations,\nincluding the adaptive warping layer and multiscale blending, to reconstruct\nvisually favorable results in the transformation process. Compared with\nprevious methods, we do not constrain the SR model on a regular grid but allow\nnumerous possible deformations for flexible and diverse image editing.\nExtensive experiments and ablation studies justify the necessity and\ndemonstrate the advantage of the proposed SRWarp method under various\ntransformations.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 02:50:41 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Son", "Sanghyun", ""], ["Lee", "Kyoung Mu", ""]]}, {"id": "2104.10326", "submitter": "Jingyu Liu", "authors": "Jie Lian and Jingyu Liu and Shu Zhang and Kai Gao and Xiaoqing Liu and\n  Dingwen Zhang and Yizhou Yu", "title": "A Structure-Aware Relation Network for Thoracic Diseases Detection and\n  Segmentation", "comments": "This paper has been accepted by IEEE Transactions on Medical Imaging", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Instance level detection and segmentation of thoracic diseases or\nabnormalities are crucial for automatic diagnosis in chest X-ray images.\nLeveraging on constant structure and disease relations extracted from domain\nknowledge, we propose a structure-aware relation network (SAR-Net) extending\nMask R-CNN. The SAR-Net consists of three relation modules: 1. the anatomical\nstructure relation module encoding spatial relations between diseases and\nanatomical parts. 2. the contextual relation module aggregating clues based on\nquery-key pair of disease RoI and lung fields. 3. the disease relation module\npropagating co-occurrence and causal relations into disease proposals. Towards\nmaking a practical system, we also provide ChestX-Det, a chest X-Ray dataset\nwith instance-level annotations (boxes and masks). ChestX-Det is a subset of\nthe public dataset NIH ChestX-ray14. It contains ~3500 images of 13 common\ndisease categories labeled by three board-certified radiologists. We evaluate\nour SAR-Net on it and another dataset DR-Private. Experimental results show\nthat it can enhance the strong baseline of Mask R-CNN with significant\nimprovements. The ChestX-Det is released at\nhttps://github.com/Deepwise-AILab/ChestX-Det-Dataset.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 02:57:02 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Lian", "Jie", ""], ["Liu", "Jingyu", ""], ["Zhang", "Shu", ""], ["Gao", "Kai", ""], ["Liu", "Xiaoqing", ""], ["Zhang", "Dingwen", ""], ["Yu", "Yizhou", ""]]}, {"id": "2104.10329", "submitter": "Wen Tang", "authors": "Wen Tang, Emilie Chouzenoux, Jean-Christophe Pesquet, and Hamid Krim", "title": "Deep Transform and Metric Learning Networks", "comments": "Accepted by ICASSP 2021. arXiv admin note: substantial text overlap\n  with arXiv:2002.07898", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Based on its great successes in inference and denosing tasks, Dictionary\nLearning (DL) and its related sparse optimization formulations have garnered a\nlot of research interest. While most solutions have focused on single layer\ndictionaries, the recently improved Deep DL methods have also fallen short on a\nnumber of issues. We hence propose a novel Deep DL approach where each DL layer\ncan be formulated and solved as a combination of one linear layer and a\nRecurrent Neural Network, where the RNN is flexibly regraded as a\nlayer-associated learned metric. Our proposed work unveils new insights between\nthe Neural Networks and Deep DL, and provides a novel, efficient and\ncompetitive approach to jointly learn the deep transforms and metrics.\nExtensive experiments are carried out to demonstrate that the proposed method\ncan not only outperform existing Deep DL, but also state-of-the-art generic\nConvolutional Neural Networks.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 03:10:15 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Tang", "Wen", ""], ["Chouzenoux", "Emilie", ""], ["Pesquet", "Jean-Christophe", ""], ["Krim", "Hamid", ""]]}, {"id": "2104.10330", "submitter": "Rui Qian", "authors": "Rui Qian, Xin Lai, Xirong Li", "title": "Boundary-Aware 3D Object Detection from Point Clouds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Currently, existing state-of-the-art 3D object detectors are in two-stage\nparadigm. These methods typically comprise two steps: 1) Utilize region\nproposal network to propose a fraction of high-quality proposals in a bottom-up\nfashion. 2) Resize and pool the semantic features from the proposed regions to\nsummarize RoI-wise representations for further refinement. Note that these\nRoI-wise representations in step 2) are considered individually as an\nuncorrelated entry when fed to following detection headers. Nevertheless, we\nobserve these proposals generated by step 1) offset from ground truth somehow,\nemerging in local neighborhood densely with an underlying probability.\nChallenges arise in the case where a proposal largely forsakes its boundary\ninformation due to coordinate offset while existing networks lack corresponding\ninformation compensation mechanism. In this paper, we propose BANet for 3D\nobject detection from point clouds. Specifically, instead of refining each\nproposal independently as previous works do, we represent each proposal as a\nnode for graph construction within a given cut-off threshold, associating\nproposals in the form of local neighborhood graph, with boundary correlations\nof an object being explicitly exploited. Besides, we devise a lightweight\nRegion Feature Aggregation Network to fully exploit voxel-wise, pixel-wise, and\npoint-wise feature with expanding receptive fields for more informative\nRoI-wise representations. As of Apr. 17th, 2021, our BANet achieves on par\nperformance on KITTI 3D detection leaderboard and ranks $1^{st}$ on $Moderate$\ndifficulty of $Car$ category on KITTI BEV detection leaderboard. The source\ncode will be released once the paper is accepted.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 03:10:33 GMT"}, {"version": "v2", "created": "Tue, 4 May 2021 13:01:39 GMT"}, {"version": "v3", "created": "Wed, 5 May 2021 01:51:06 GMT"}, {"version": "v4", "created": "Mon, 10 May 2021 08:26:19 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Qian", "Rui", ""], ["Lai", "Xin", ""], ["Li", "Xirong", ""]]}, {"id": "2104.10338", "submitter": "Yan Hong", "authors": "Yan Hong, Li Niu, Jianfu Zhang, Liqing Zhang", "title": "Shadow Generation for Composite Image in Real-world Scenes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image composition targets at inserting a foreground object on a background\nimage. Most previous image composition methods focus on adjusting the\nforeground to make it compatible with background while ignoring the shadow\neffect of foreground on the background. In this work, we focus on generating\nplausible shadow for the foreground object in the composite image. First, we\ncontribute a real-world shadow generation dataset DESOBA by generating\nsynthetic composite images based on paired real images and deshadowed images.\nThen, we propose a novel shadow generation network SGRNet, which consists of a\nshadow mask prediction stage and a shadow filling stage. In the shadow mask\nprediction stage, foreground and background information are thoroughly\ninteracted to generate foreground shadow mask. In the shadow filling stage,\nshadow parameters are predicted to fill the shadow area. Extensive experiments\non our DESOBA dataset and real composite images demonstrate the effectiveness\nof our proposed method.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 03:30:02 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Hong", "Yan", ""], ["Niu", "Li", ""], ["Zhang", "Jianfu", ""], ["Zhang", "Liqing", ""]]}, {"id": "2104.10345", "submitter": "Maur\\'icio Pamplona Segundo", "authors": "Mauricio Pamplona Segundo, Allan Pinto, Rodrigo Minetto, Ricardo da\n  Silva Torres, Sudeep Sarkar", "title": "Measuring economic activity from space: a case study using flying\n  airplanes and COVID-19", "comments": "11 pages, 11 figures", "journal-ref": null, "doi": "10.1109/JSTARS.2021.3094053", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work introduces a novel solution to measure economic activity through\nremote sensing for a wide range of spatial areas. We hypothesized that\ndisturbances in human behavior caused by major life-changing events leave\nsignatures in satellite imagery that allows devising relevant image-based\nindicators to estimate their impacts and support decision-makers. We present a\ncase study for the COVID-19 coronavirus outbreak, which imposed severe mobility\nrestrictions and caused worldwide disruptions, using flying airplane detection\naround the 30 busiest airports in Europe to quantify and analyze the lockdown's\neffects and post-lockdown recovery. Our solution won the Rapid Action\nCoronavirus Earth observation (RACE) upscaling challenge, sponsored by the\nEuropean Space Agency and the European Commission, and now integrates the RACE\ndashboard. This platform combines satellite data and artificial intelligence to\npromote a progressive and safe reopening of essential activities. Code and CNN\nmodels are available at https://github.com/maups/covid19-custom-script-contest\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 04:01:25 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Segundo", "Mauricio Pamplona", ""], ["Pinto", "Allan", ""], ["Minetto", "Rodrigo", ""], ["Torres", "Ricardo da Silva", ""], ["Sarkar", "Sudeep", ""]]}, {"id": "2104.10348", "submitter": "Pravin Nair", "authors": "Pravin Nair and Ruturaj G. Gavaskar and Kunal N. Chaudhury", "title": "Fixed-Point and Objective Convergence of Plug-and-Play Algorithms", "comments": "Published in IEEE Transactions on Computational Imaging", "journal-ref": "in IEEE Transactions on Computational Imaging, vol. 7, pp.\n  337-348, 2021", "doi": "10.1109/TCI.2021.3066053", "report-no": null, "categories": "math.OC cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A standard model for image reconstruction involves the minimization of a\ndata-fidelity term along with a regularizer, where the optimization is\nperformed using proximal algorithms such as ISTA and ADMM. In plug-and-play\n(PnP) regularization, the proximal operator (associated with the regularizer)\nin ISTA and ADMM is replaced by a powerful image denoiser. Although PnP\nregularization works surprisingly well in practice, its theoretical convergence\n-- whether convergence of the PnP iterates is guaranteed and if they minimize\nsome objective function -- is not completely understood even for simple linear\ndenoisers such as nonlocal means. In particular, while there are works where\neither iterate or objective convergence is established separately, a\nsimultaneous guarantee on iterate and objective convergence is not available\nfor any denoiser to our knowledge. In this paper, we establish both forms of\nconvergence for a special class of linear denoisers. Notably, unlike existing\nworks where the focus is on symmetric denoisers, our analysis covers\nnon-symmetric denoisers such as nonlocal means and almost any convex\ndata-fidelity. The novelty in this regard is that we make use of the\nconvergence theory of averaged operators and we work with a special inner\nproduct (and norm) derived from the linear denoiser; the latter requires us to\nappropriately define the gradient and proximal operators associated with the\ndata-fidelity term. We validate our convergence results using image\nreconstruction experiments.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 04:25:17 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Nair", "Pravin", ""], ["Gavaskar", "Ruturaj G.", ""], ["Chaudhury", "Kunal N.", ""]]}, {"id": "2104.10351", "submitter": "Feifei Shao", "authors": "Feifei Shao, Yawei Luo, Li Zhang, Lu Ye, Siliang Tang, Yi Yang, Jun\n  Xiao", "title": "Improving Weakly-supervised Object Localization via Causal Intervention", "comments": "11 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent emerged weakly supervised object localization (WSOL) methods can\nlearn to localize an object in the image only using image-level labels.\nPrevious works endeavor to perceive the interval objects from the small and\nsparse discriminative attention map, yet ignoring the co-occurrence confounder\n(e.g., bird and sky), which makes the model inspection (e.g., CAM) hard to\ndistinguish between the object and context. In this paper, we make an early\nattempt to tackle this challenge via causal intervention (CI). Our proposed\nmethod, dubbed CI-CAM, explores the causalities among images, contexts, and\ncategories to eliminate the biased co-occurrence in the class activation maps\nthus improving the accuracy of object localization. Extensive experiments on\nseveral benchmarks demonstrate the effectiveness of CI-CAM in learning the\nclear object boundaries from confounding contexts. Particularly, in\nCUB-200-2011 which severely suffers from the co-occurrence confounder, CI-CAM\nsignificantly outperforms the traditional CAM-based baseline (58.39% vs 52.4%\nin top-1 localization accuracy). While in more general scenarios such as\nImageNet, CI-CAM can also perform on par with the state of the arts.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 04:44:33 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Shao", "Feifei", ""], ["Luo", "Yawei", ""], ["Zhang", "Li", ""], ["Ye", "Lu", ""], ["Tang", "Siliang", ""], ["Yang", "Yi", ""], ["Xiao", "Jun", ""]]}, {"id": "2104.10355", "submitter": "Wei-Lun Chao", "authors": "Jihyung Kil, Wei-Lun Chao", "title": "Revisiting Document Representations for Large-Scale Zero-Shot Learning", "comments": "Accepted to NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Zero-shot learning aims to recognize unseen objects using their semantic\nrepresentations. Most existing works use visual attributes labeled by humans,\nnot suitable for large-scale applications. In this paper, we revisit the use of\ndocuments as semantic representations. We argue that documents like Wikipedia\npages contain rich visual information, which however can easily be buried by\nthe vast amount of non-visual sentences. To address this issue, we propose a\nsemi-automatic mechanism for visual sentence extraction that leverages the\ndocument section headers and the clustering structure of visual sentences. The\nextracted visual sentences, after a novel weighting scheme to distinguish\nsimilar classes, essentially form semantic representations like visual\nattributes but need much less human effort. On the ImageNet dataset with over\n10,000 unseen classes, our representations lead to a 64% relative improvement\nagainst the commonly used ones.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 05:17:55 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Kil", "Jihyung", ""], ["Chao", "Wei-Lun", ""]]}, {"id": "2104.10369", "submitter": "Jun Zhou", "authors": "Jun Zhou, Wei Jin, Mingjie Wang, Xiuping Liu, Zhiyang Li, Zhaobin Liu", "title": "Improvement of Normal Estimation for PointClouds via Simplifying Surface\n  Fitting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the burst development of neural networks in recent years, the task of\nnormal estimation has once again become a concern. By introducing the neural\nnetworks to classic methods based on problem-specific knowledge, the\nadaptability of the normal estimation algorithm to noise and scale has been\ngreatly improved. However, the compatibility between neural networks and the\ntraditional methods has not been considered. Similar to the principle of\nOccam's razor, that is, the simpler is better. We observe that a more\nsimplified process of surface fitting can significantly improve the accuracy of\nthe normal estimation. In this paper, two simple-yet-effective strategies are\nproposed to address the compatibility between the neural networks and surface\nfitting process to improve normal estimation. Firstly, a dynamic top-k\nselection strategy is introduced to better focus on the most critical points of\na given patch, and the points selected by our learning method tend to fit a\nsurface by way of a simple tangent plane, which can dramatically improve the\nnormal estimation results of patches with sharp corners or complex patterns.\nThen, we propose a point update strategy before local surface fitting, which\nsmooths the sharp boundary of the patch to simplify the surface fitting\nprocess, significantly reducing the fitting distortion and improving the\naccuracy of the predicted point normal. The experiments analyze the\neffectiveness of our proposed strategies and demonstrate that our method\nachieves SOTA results with the advantage of higher estimation accuracy over\nmost existed approaches.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 06:13:29 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Zhou", "Jun", ""], ["Jin", "Wei", ""], ["Wang", "Mingjie", ""], ["Liu", "Xiuping", ""], ["Li", "Zhiyang", ""], ["Liu", "Zhaobin", ""]]}, {"id": "2104.10376", "submitter": "Kekai Sheng", "authors": "Yifan Xu, Kekai Sheng, Weiming Dong, Baoyuan Wu, Changsheng Xu,\n  Bao-Gang Hu", "title": "Towards Corruption-Agnostic Robust Domain Adaptation", "comments": "The first literature to investigate the topic of corruption-agnostic\n  robust domain adaptation, a new practical and challenging domain adaptation\n  setting", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Big progress has been achieved in domain adaptation in decades. Existing\nworks are always based on an ideal assumption that testing target domain are\ni.i.d. with training target domains. However, due to unpredictable corruptions\n(e.g., noise and blur) in real data like web images, domain adaptation methods\nare increasingly required to be corruption robust on target domains. In this\npaper, we investigate a new task, Corruption-agnostic Robust Domain Adaptation\n(CRDA): to be accurate on original data and robust against\nunavailable-for-training corruptions on target domains. This task is\nnon-trivial due to large domain discrepancy and unsupervised target domains. We\nobserve that simple combinations of popular methods of domain adaptation and\ncorruption robustness have sub-optimal CRDA results. We propose a new approach\nbased on two technical insights into CRDA: 1) an easy-to-plug module called\nDomain Discrepancy Generator (DDG) that generates samples that enlarge domain\ndiscrepancy to mimic unpredictable corruptions; 2) a simple but effective\nteacher-student scheme with contrastive loss to enhance the constraints on\ntarget domains. Experiments verify that DDG keeps or even improves performance\non original data and achieves better corruption robustness that baselines.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 06:27:48 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Xu", "Yifan", ""], ["Sheng", "Kekai", ""], ["Dong", "Weiming", ""], ["Wu", "Baoyuan", ""], ["Xu", "Changsheng", ""], ["Hu", "Bao-Gang", ""]]}, {"id": "2104.10377", "submitter": "Yujing Jiang", "authors": "Yujing Jiang, Xingjun Ma, Sarah Monazam Erfani and James Bailey", "title": "Dual Head Adversarial Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) are known to be vulnerable to adversarial\nexamples/attacks, raising concerns about their reliability in safety-critical\napplications. A number of defense methods have been proposed to train robust\nDNNs resistant to adversarial attacks, among which adversarial training has so\nfar demonstrated the most promising results. However, recent studies have shown\nthat there exists an inherent tradeoff between accuracy and robustness in\nadversarially-trained DNNs. In this paper, we propose a novel technique Dual\nHead Adversarial Training (DH-AT) to further improve the robustness of existing\nadversarial training methods. Different from existing improved variants of\nadversarial training, DH-AT modifies both the architecture of the network and\nthe training strategy to seek more robustness. Specifically, DH-AT first\nattaches a second network head (or branch) to one intermediate layer of the\nnetwork, then uses a lightweight convolutional neural network (CNN) to\naggregate the outputs of the two heads. The training strategy is also adapted\nto reflect the relative importance of the two heads. We empirically show, on\nmultiple benchmark datasets, that DH-AT can bring notable robustness\nimprovements to existing adversarial training methods. Compared with TRADES,\none state-of-the-art adversarial training method, our DH-AT can improve the\nrobustness by 3.4% against PGD40 and 2.3% against AutoAttack, and also improve\nthe clean accuracy by 1.8%.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 06:31:33 GMT"}, {"version": "v2", "created": "Thu, 22 Apr 2021 06:01:25 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Jiang", "Yujing", ""], ["Ma", "Xingjun", ""], ["Erfani", "Sarah Monazam", ""], ["Bailey", "James", ""]]}, {"id": "2104.10386", "submitter": "Yuk Heo", "authors": "Yuk Heo, Yeong Jun Koh, Chang-Su Kim", "title": "Guided Interactive Video Object Segmentation Using Reliability-Based\n  Attention Maps", "comments": "accepted to CVPR2021 (oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel guided interactive segmentation (GIS) algorithm for video\nobjects to improve the segmentation accuracy and reduce the interaction time.\nFirst, we design the reliability-based attention module to analyze the\nreliability of multiple annotated frames. Second, we develop the\nintersection-aware propagation module to propagate segmentation results to\nneighboring frames. Third, we introduce the GIS mechanism for a user to select\nunsatisfactory frames quickly with less effort. Experimental results\ndemonstrate that the proposed algorithm provides more accurate segmentation\nresults at a faster speed than conventional algorithms. Codes are available at\nhttps://github.com/yuk6heo/GIS-RAmap.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 07:08:57 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Heo", "Yuk", ""], ["Koh", "Yeong Jun", ""], ["Kim", "Chang-Su", ""]]}, {"id": "2104.10401", "submitter": "Sang Hun Lee", "authors": "Sangrok Lee, Taekang Woo, Sang Hun Lee", "title": "Multi-Attention-Based Soft Partition Network for Vehicle\n  Re-Identification", "comments": "10 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Vehicle re-identification (Re-ID) distinguishes between the same vehicle and\nother vehicles in images. It is challenging due to significant intra-instance\ndifferences between identical vehicles from different views and subtle\ninter-instance differences of similar vehicles. Researchers have tried to\naddress this problem by extracting features robust to variations of viewpoints\nand environments. More recently, they tried to improve performance by using\nadditional metadata such as key points, orientation, and temporal information.\nAlthough these attempts have been relatively successful, they all require\nexpensive annotations. Therefore, this paper proposes a novel deep neural\nnetwork called a multi-attention-based soft partition (MUSP) network to solve\nthis problem. This network does not use metadata and only uses multiple soft\nattentions to identify a specific vehicle area. This function was performed by\nmetadata in previous studies. Experiments verified that MUSP achieved\nstate-of-the-art (SOTA) performance for the VehicleID dataset without any\nadditional annotations and was comparable to VeRi-776 and VERI-Wild.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 08:13:17 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Lee", "Sangrok", ""], ["Woo", "Taekang", ""], ["Lee", "Sang Hun", ""]]}, {"id": "2104.10406", "submitter": "Shiyang Yan", "authors": "Shiyang Yan, Li Yu, Yuan Xie", "title": "Discrete-continuous Action Space Policy Gradient-based Attention for\n  Image-Text Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Image-text matching is an important multi-modal task with massive\napplications. It tries to match the image and the text with similar semantic\ninformation. Existing approaches do not explicitly transform the different\nmodalities into a common space. Meanwhile, the attention mechanism which is\nwidely used in image-text matching models does not have supervision. We propose\na novel attention scheme which projects the image and text embedding into a\ncommon space and optimises the attention weights directly towards the\nevaluation metrics. The proposed attention scheme can be considered as a kind\nof supervised attention and requiring no additional annotations. It is trained\nvia a novel Discrete-continuous action space policy gradient algorithm, which\nis more effective in modelling complex action space than previous continuous\naction space policy gradient. We evaluate the proposed methods on two\nwidely-used benchmark datasets: Flickr30k and MS-COCO, outperforming the\nprevious approaches by a large margin.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 08:34:22 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Yan", "Shiyang", ""], ["Yu", "Li", ""], ["Xie", "Yuan", ""]]}, {"id": "2104.10412", "submitter": "Kanishk Jain", "authors": "Kanishk Jain, Vineet Gandhi", "title": "Comprehensive Multi-Modal Interactions for Referring Image Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We investigate Referring Image Segmentation (RIS), which outputs a\nsegmentation map corresponding to the given natural language description. To\nsolve RIS efficiently, we need to understand each word's relationship with\nother words, each region in the image to other regions, and cross-modal\nalignment between linguistic and visual domains. Recent methods model these\nthree types of interactions sequentially. We argue that such a modular approach\nlimits these methods' performance, and joint simultaneous reasoning can help\nresolve ambiguities. To this end, we propose a Joint Reasoning (JRM) module and\na novel Cross-Modal Multi-Level Fusion (CMMLF) module for tackling this task.\nJRM effectively models the referent's multi-modal context by jointly reasoning\nover visual and linguistic modalities (performing word-word, image\nregion-region, word-region interactions in a single module). CMMLF module\nfurther refines the segmentation masks by exchanging contextual information\nacross visual hierarchy through linguistic features acting as a bridge. We\npresent thorough ablation studies and validate our approach's performance on\nfour benchmark datasets, and show that the proposed method outperforms the\nexisting state-of-the-art methods on all four datasets by significant margins.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 08:45:09 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Jain", "Kanishk", ""], ["Gandhi", "Vineet", ""]]}, {"id": "2104.10414", "submitter": "Yao Gao", "authors": "Zhong-Qiu Zhao, Yao Gao, Yuchen Ge and Weidong Tian", "title": "Orderly Dual-Teacher Knowledge Distillation for Lightweight Human Pose\n  Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although deep convolution neural networks (DCNN) have achieved excellent\nperformance in human pose estimation, these networks often have a large number\nof parameters and computations, leading to the slow inference speed. For this\nissue, an effective solution is knowledge distillation, which transfers\nknowledge from a large pre-trained network (teacher) to a small network\n(student). However, there are some defects in the existing approaches: (I) Only\na single teacher is adopted, neglecting the potential that a student can learn\nfrom multiple teachers. (II) The human segmentation mask can be regarded as\nadditional prior information to restrict the location of keypoints, which is\nnever utilized. (III) A student with a small number of parameters cannot fully\nimitate heatmaps provided by datasets and teachers. (IV) There exists noise in\nheatmaps generated by teachers, which causes model degradation. To overcome\nthese defects, we propose an orderly dual-teacher knowledge distillation (ODKD)\nframework, which consists of two teachers with different capabilities.\nSpecifically, the weaker one (primary teacher, PT) is used to teach keypoints\ninformation, the stronger one (senior teacher, ST) is utilized to transfer\nsegmentation and keypoints information by adding the human segmentation mask.\nTaking dual-teacher together, an orderly learning strategy is proposed to\npromote knowledge absorbability. Moreover, we employ a binarization operation\nwhich further improves the learning ability of the student and reduces noise in\nheatmaps. Experimental results on COCO and OCHuman keypoints datasets show that\nour proposed ODKD can improve the performance of different lightweight models\nby a large margin, and HRNet-W16 equipped with ODKD achieves state-of-the-art\nperformance for lightweight human pose estimation.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 08:50:36 GMT"}, {"version": "v2", "created": "Mon, 14 Jun 2021 15:28:36 GMT"}, {"version": "v3", "created": "Thu, 17 Jun 2021 12:01:49 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Zhao", "Zhong-Qiu", ""], ["Gao", "Yao", ""], ["Ge", "Yuchen", ""], ["Tian", "Weidong", ""]]}, {"id": "2104.10419", "submitter": "Xin Huang", "authors": "Xin Huang, Xinxin Wang, Wenyu Lv, Xiaying Bai, Xiang Long, Kaipeng\n  Deng, Qingqing Dang, Shumin Han, Qiwen Liu, Xiaoguang Hu, Dianhai Yu, Yanjun\n  Ma, Osamu Yoshie", "title": "PP-YOLOv2: A Practical Object Detector", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Being effective and efficient is essential to an object detector for\npractical use. To meet these two concerns, we comprehensively evaluate a\ncollection of existing refinements to improve the performance of PP-YOLO while\nalmost keep the infer time unchanged. This paper will analyze a collection of\nrefinements and empirically evaluate their impact on the final model\nperformance through incremental ablation study. Things we tried that didn't\nwork will also be discussed. By combining multiple effective refinements, we\nboost PP-YOLO's performance from 45.9% mAP to 49.5% mAP on COCO2017 test-dev.\nSince a significant margin of performance has been made, we present PP-YOLOv2.\nIn terms of speed, PP-YOLOv2 runs in 68.9FPS at 640x640 input size. Paddle\ninference engine with TensorRT, FP16-precision, and batch size = 1 further\nimproves PP-YOLOv2's infer speed, which achieves 106.5 FPS. Such a performance\nsurpasses existing object detectors with roughly the same amount of parameters\n(i.e., YOLOv4-CSP, YOLOv5l). Besides, PP-YOLOv2 with ResNet101 achieves 50.3%\nmAP on COCO2017 test-dev. Source code is at\nhttps://github.com/PaddlePaddle/PaddleDetection.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 08:55:37 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Huang", "Xin", ""], ["Wang", "Xinxin", ""], ["Lv", "Wenyu", ""], ["Bai", "Xiaying", ""], ["Long", "Xiang", ""], ["Deng", "Kaipeng", ""], ["Dang", "Qingqing", ""], ["Han", "Shumin", ""], ["Liu", "Qiwen", ""], ["Hu", "Xiaoguang", ""], ["Yu", "Dianhai", ""], ["Ma", "Yanjun", ""], ["Yoshie", "Osamu", ""]]}, {"id": "2104.10420", "submitter": "Xiaotao Li", "authors": "Zeyu Chen, Xinhang Zhang, Juan Li, Jingxuan Ni, Gang Chen, Shaohua\n  Wang, Fangfang Fan, Changfeng Charles Wang, Xiaotao Li", "title": "Machine vision detection to daily facial fatigue with a nonlocal 3D\n  attention network", "comments": "25 pages, 6 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Fatigue detection is valued for people to keep mental health and prevent\nsafety accidents. However, detecting facial fatigue, especially mild fatigue in\nthe real world via machine vision is still a challenging issue due to lack of\nnon-lab dataset and well-defined algorithms. In order to improve the detection\ncapability on facial fatigue that can be used widely in daily life, this paper\nprovided an audiovisual dataset named DLFD (daily-life fatigue dataset) which\nreflected people's facial fatigue state in the wild. A framework using\n3D-ResNet along with non-local attention mechanism was training for extraction\nof local and long-range features in spatial and temporal dimensions. Then, a\ncompacted loss function combining mean squared error and cross-entropy was\ndesigned to predict both continuous and categorical fatigue degrees. Our\nproposed framework has reached an average accuracy of 90.8% on validation set\nand 72.5% on test set for binary classification, standing a good position\ncompared to other state-of-the-art methods. The analysis of feature map\nvisualization revealed that our framework captured facial dynamics and\nattempted to build a connection with fatigue state. Our experimental results in\nmultiple metrics proved that our framework captured some typical, micro and\ndynamic facial features along spatiotemporal dimensions, contributing to the\nmild fatigue detection in the wild.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 08:58:46 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Chen", "Zeyu", ""], ["Zhang", "Xinhang", ""], ["Li", "Juan", ""], ["Ni", "Jingxuan", ""], ["Chen", "Gang", ""], ["Wang", "Shaohua", ""], ["Fan", "Fangfang", ""], ["Wang", "Changfeng Charles", ""], ["Li", "Xiaotao", ""]]}, {"id": "2104.10425", "submitter": "Andreas Panteli", "authors": "Andreas Panteli, Jonas Teuwen, Hugo Horlings, Efstratios Gavves", "title": "Sparse-Shot Learning for Extremely Many Localisations", "comments": "14 pages, 7 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Object localisation is typically considered in the context of regular images,\nfor instance depicting objects like people or cars. In these images there is\ntypically a relatively small number of instances per image per class, which\nusually is manageable to annotate. However, outside the realm of regular images\nwe are often confronted with a different situation. In computational pathology\ndigitised tissue sections are extremely large images, whose dimensions quickly\nexceed 250'000x250'000 pixels, where relevant objects, such as tumour cells or\nlymphocytes can quickly number in the millions. Annotating them all is\npractically impossible and annotating sparsely a few, out of many more, is the\nonly possibility. Unfortunately, learning from sparse annotations, or\nsparse-shot learning, clashes with standard supervised learning because what is\nnot annotated is treated as a negative. However, assigning negative labels to\nwhat are true positives leads to confusion in the gradients and biased\nlearning. To this end, we present exclusive cross entropy, which slows down the\nbiased learning by examining the second-order loss derivatives in order to drop\nthe loss terms corresponding to likely biased terms. Experiments on nine\ndatasets and two different localisation tasks, detection with YOLLO and\nsegmentation with Unet, show that we obtain considerable improvements compared\nto cross entropy or focal loss, while often reaching the best possible\nperformance for the model with only 10-40 of annotations.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 09:09:54 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Panteli", "Andreas", ""], ["Teuwen", "Jonas", ""], ["Horlings", "Hugo", ""], ["Gavves", "Efstratios", ""]]}, {"id": "2104.10442", "submitter": "Jianyong Chen", "authors": "Yiqin Zhu, Jianyong Chen, Lingyu Liang, Zhanghui Kuang, Lianwen Jin\n  and Wayne Zhang", "title": "Fourier Contour Embedding for Arbitrary-Shaped Text Detection", "comments": "Accepted by CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the main challenges for arbitrary-shaped text detection is to design a\ngood text instance representation that allows networks to learn diverse text\ngeometry variances. Most of existing methods model text instances in image\nspatial domain via masks or contour point sequences in the Cartesian or the\npolar coordinate system. However, the mask representation might lead to\nexpensive post-processing, while the point sequence one may have limited\ncapability to model texts with highly-curved shapes. To tackle these problems,\nwe model text instances in the Fourier domain and propose one novel Fourier\nContour Embedding (FCE) method to represent arbitrary shaped text contours as\ncompact signatures. We further construct FCENet with a backbone, feature\npyramid networks (FPN) and a simple post-processing with the Inverse Fourier\nTransformation (IFT) and Non-Maximum Suppression (NMS). Different from previous\nmethods, FCENet first predicts compact Fourier signatures of text instances,\nand then reconstructs text contours via IFT and NMS during test. Extensive\nexperiments demonstrate that FCE is accurate and robust to fit contours of\nscene texts even with highly-curved shapes, and also validate the effectiveness\nand the good generalization of FCENet for arbitrary-shaped text detection.\nFurthermore, experimental results show that our FCENet is superior to the\nstate-of-the-art (SOTA) methods on CTW1500 and Total-Text, especially on\nchallenging highly-curved text subset.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 10:21:57 GMT"}, {"version": "v2", "created": "Thu, 22 Apr 2021 06:03:58 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Zhu", "Yiqin", ""], ["Chen", "Jianyong", ""], ["Liang", "Lingyu", ""], ["Kuang", "Zhanghui", ""], ["Jin", "Lianwen", ""], ["Zhang", "Wayne", ""]]}, {"id": "2104.10447", "submitter": "Gyeongmin Lee", "authors": "Heejung Park, Gyeong Min Lee, Soopil Kim, Ga Hyung Ryu, Areum Jeong,\n  Sang Hyun Park, Min Sagong", "title": "A Meta-Learning Approach for Medical Image Registration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-rigid registration is a necessary but challenging task in medical imaging\nstudies. Recently, unsupervised registration models have shown good\nperformance, but they often require a large-scale training dataset and long\ntraining times. Therefore, in real world application where only dozens to\nhundreds of image pairs are available, existing models cannot be practically\nused. To address these limitations, we propose a novel unsupervised\nregistration model which is integrated with a gradient-based meta learning\nframework. In particular, we train a meta learner which finds an initialization\npoint of parameters by utilizing a variety of existing registration datasets.\nTo quickly adapt to various tasks, the meta learner was updated to get close to\nthe center of parameters which are fine-tuned for each registration task.\nThereby, our model can adapt to unseen domain tasks via a short fine-tuning\nprocess and perform accurate registration. To verify the superiority of our\nmodel, we train the model for various 2D medical image registration tasks such\nas retinal choroid Optical Coherence Tomography Angiography (OCTA), CT organs,\nand brain MRI scans and test on registration of retinal OCTA Superficial\nCapillary Plexus (SCP). In our experiments, the proposed model obtained\nsignificantly improved performance in terms of accuracy and training time\ncompared to other registration models.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 10:27:05 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Park", "Heejung", ""], ["Lee", "Gyeong Min", ""], ["Kim", "Soopil", ""], ["Ryu", "Ga Hyung", ""], ["Jeong", "Areum", ""], ["Park", "Sang Hyun", ""], ["Sagong", "Min", ""]]}, {"id": "2104.10453", "submitter": "Kimberly Mai", "authors": "Kimberly T. Mai, Toby Davies, Lewis D. Griffin", "title": "Brittle Features May Help Anomaly Detection", "comments": "Accepted to Women in Computer Vision workshop at CVPR (2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  One-class anomaly detection is challenging. A representation that clearly\ndistinguishes anomalies from normal data is ideal, but arriving at this\nrepresentation is difficult since only normal data is available at training\ntime. We examine the performance of representations, transferred from auxiliary\ntasks, for anomaly detection. Our results suggest that the choice of\nrepresentation is more important than the anomaly detector used with these\nrepresentations, although knowledge distillation can work better than using the\nrepresentations directly. In addition, separability between anomalies and\nnormal data is important but not the sole factor for a good representation, as\nanomaly detection performance is also correlated with more adversarially\nbrittle features in the representation space. Finally, we show our\nconfiguration can detect 96.4% of anomalies in a genuine X-ray security\ndataset, outperforming previous results.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 10:46:58 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Mai", "Kimberly T.", ""], ["Davies", "Toby", ""], ["Griffin", "Lewis D.", ""]]}, {"id": "2104.10459", "submitter": "Kenneth Co", "authors": "Kenneth T. Co, David Martinez Rego, Emil C. Lupu", "title": "Jacobian Regularization for Mitigating Universal Adversarial\n  Perturbations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CR cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Universal Adversarial Perturbations (UAPs) are input perturbations that can\nfool a neural network on large sets of data. They are a class of attacks that\nrepresents a significant threat as they facilitate realistic, practical, and\nlow-cost attacks on neural networks. In this work, we derive upper bounds for\nthe effectiveness of UAPs based on norms of data-dependent Jacobians. We\nempirically verify that Jacobian regularization greatly increases model\nrobustness to UAPs by up to four times whilst maintaining clean performance.\nOur theoretical analysis also allows us to formulate a metric for the strength\nof shared adversarial perturbations between pairs of inputs. We apply this\nmetric to benchmark datasets and show that it is highly correlated with the\nactual observed robustness. This suggests that realistic and practical\nuniversal attacks can be reliably mitigated without sacrificing clean accuracy,\nwhich shows promise for the robustness of machine learning systems.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 11:00:21 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Co", "Kenneth T.", ""], ["Rego", "David Martinez", ""], ["Lupu", "Emil C.", ""]]}, {"id": "2104.10461", "submitter": "Arian Bakhtiarnia", "authors": "Arian Bakhtiarnia, Qi Zhang and Alexandros Iosifidis", "title": "Improving the Accuracy of Early Exits in Multi-Exit Architectures via\n  Curriculum Learning", "comments": "Accepted by the 2021 International Joint Conference on Neural\n  Networks (IJCNN 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deploying deep learning services for time-sensitive and resource-constrained\nsettings such as IoT using edge computing systems is a challenging task that\nrequires dynamic adjustment of inference time. Multi-exit architectures allow\ndeep neural networks to terminate their execution early in order to adhere to\ntight deadlines at the cost of accuracy. To mitigate this cost, in this paper\nwe introduce a novel method called Multi-Exit Curriculum Learning that utilizes\ncurriculum learning, a training strategy for neural networks that imitates\nhuman learning by sorting the training samples based on their difficulty and\ngradually introducing them to the network. Experiments on CIFAR-10 and\nCIFAR-100 datasets and various configurations of multi-exit architectures show\nthat our method consistently improves the accuracy of early exits compared to\nthe standard training approach.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 11:12:35 GMT"}, {"version": "v2", "created": "Thu, 22 Apr 2021 07:45:31 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Bakhtiarnia", "Arian", ""], ["Zhang", "Qi", ""], ["Iosifidis", "Alexandros", ""]]}, {"id": "2104.10475", "submitter": "Haiyang Mei", "authors": "Haiyang Mei, Ge-Peng Ji, Ziqi Wei, Xin Yang, Xiaopeng Wei, Deng-Ping\n  Fan", "title": "Camouflaged Object Segmentation with Distraction Mining", "comments": "CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Camouflaged object segmentation (COS) aims to identify objects that are\n\"perfectly\" assimilate into their surroundings, which has a wide range of\nvaluable applications. The key challenge of COS is that there exist high\nintrinsic similarities between the candidate objects and noise background. In\nthis paper, we strive to embrace challenges towards effective and efficient\nCOS. To this end, we develop a bio-inspired framework, termed Positioning and\nFocus Network (PFNet), which mimics the process of predation in nature.\nSpecifically, our PFNet contains two key modules, i.e., the positioning module\n(PM) and the focus module (FM). The PM is designed to mimic the detection\nprocess in predation for positioning the potential target objects from a global\nperspective and the FM is then used to perform the identification process in\npredation for progressively refining the coarse prediction via focusing on the\nambiguous regions. Notably, in the FM, we develop a novel distraction mining\nstrategy for distraction discovery and removal, to benefit the performance of\nestimation. Extensive experiments demonstrate that our PFNet runs in real-time\n(72 FPS) and significantly outperforms 18 cutting-edge models on three\nchallenging datasets under four standard metrics.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 11:47:59 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Mei", "Haiyang", ""], ["Ji", "Ge-Peng", ""], ["Wei", "Ziqi", ""], ["Yang", "Xin", ""], ["Wei", "Xiaopeng", ""], ["Fan", "Deng-Ping", ""]]}, {"id": "2104.10481", "submitter": "Siladittya Manna", "authors": "Siladittya Manna, Saumik Bhattacharya, Umapada Pal", "title": "SSLM: Self-Supervised Learning for Medical Diagnosis from MR Video", "comments": "This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, which this version may no longer\n  be accessible", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In medical image analysis, the cost of acquiring high-quality data and their\nannotation by experts is a barrier in many medical applications. Most of the\ntechniques used are based on supervised learning framework and need a large\namount of annotated data to achieve satisfactory performance. As an\nalternative, in this paper, we propose a self-supervised learning approach to\nlearn the spatial anatomical representations from the frames of magnetic\nresonance (MR) video clips for the diagnosis of knee medical conditions. The\npretext model learns meaningful spatial context-invariant representations. The\ndownstream task in our paper is a class imbalanced multi-label classification.\nDifferent experiments show that the features learnt by the pretext model\nprovide explainable performance in the downstream task. Moreover, the\nefficiency and reliability of the proposed pretext model in learning\nrepresentations of minority classes without applying any strategy towards\nimbalance in the dataset can be seen from the results. To the best of our\nknowledge, this work is the first work of its kind in showing the effectiveness\nand reliability of self-supervised learning algorithms in class imbalanced\nmulti-label classification tasks on MR video.\n  The code for evaluation of the proposed work is available at\nhttps://github.com/sadimanna/sslm\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 12:01:49 GMT"}, {"version": "v2", "created": "Thu, 22 Apr 2021 05:05:22 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Manna", "Siladittya", ""], ["Bhattacharya", "Saumik", ""], ["Pal", "Umapada", ""]]}, {"id": "2104.10488", "submitter": "Jiqing Zhang", "authors": "Jiqing Zhang, Chengjiang Long, Yuxin Wang, Haiyin Piao, Haiyang Mei,\n  Xin Yang, Baocai Yin", "title": "A Two-Stage Attentive Network for Single Image Super-Resolution", "comments": null, "journal-ref": null, "doi": "10.1109/TCSVT.2021.3071191", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, deep convolutional neural networks (CNNs) have been widely explored\nin single image super-resolution (SISR) and contribute remarkable progress.\nHowever, most of the existing CNNs-based SISR methods do not adequately explore\ncontextual information in the feature extraction stage and pay little attention\nto the final high-resolution (HR) image reconstruction step, hence hindering\nthe desired SR performance. To address the above two issues, in this paper, we\npropose a two-stage attentive network (TSAN) for accurate SISR in a\ncoarse-to-fine manner. Specifically, we design a novel multi-context attentive\nblock (MCAB) to make the network focus on more informative contextual features.\nMoreover, we present an essential refined attention block (RAB) which could\nexplore useful cues in HR space for reconstructing fine-detailed HR image.\nExtensive evaluations on four benchmark datasets demonstrate the efficacy of\nour proposed TSAN in terms of quantitative metrics and visual effects. Code is\navailable at https://github.com/Jee-King/TSAN.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 12:20:24 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Zhang", "Jiqing", ""], ["Long", "Chengjiang", ""], ["Wang", "Yuxin", ""], ["Piao", "Haiyin", ""], ["Mei", "Haiyang", ""], ["Yang", "Xin", ""], ["Yin", "Baocai", ""]]}, {"id": "2104.10490", "submitter": "Anthony Hu", "authors": "Anthony Hu, Zak Murez, Nikhil Mohan, Sof\\'ia Dudas, Jeff Hawke, Vijay\n  Badrinarayanan, Roberto Cipolla, Alex Kendall", "title": "FIERY: Future Instance Prediction in Bird's-Eye View from Surround\n  Monocular Cameras", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Driving requires interacting with road agents and predicting their future\nbehaviour in order to navigate safely. We present FIERY: a probabilistic future\nprediction model in bird's-eye view from monocular cameras. Our model predicts\nfuture instance segmentation and motion of dynamic agents that can be\ntransformed into non-parametric future trajectories. Our approach combines the\nperception, sensor fusion and prediction components of a traditional autonomous\ndriving stack by estimating bird's-eye-view prediction directly from surround\nRGB monocular camera inputs. FIERY learns to model the inherent stochastic\nnature of the future directly from camera driving data in an end-to-end manner,\nwithout relying on HD maps, and predicts multimodal future trajectories. We\nshow that our model outperforms previous prediction baselines on the NuScenes\nand Lyft datasets. Code is available at https://github.com/wayveai/fiery\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 12:21:40 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Hu", "Anthony", ""], ["Murez", "Zak", ""], ["Mohan", "Nikhil", ""], ["Dudas", "Sof\u00eda", ""], ["Hawke", "Jeff", ""], ["Badrinarayanan", "Vijay", ""], ["Cipolla", "Roberto", ""], ["Kendall", "Alex", ""]]}, {"id": "2104.10492", "submitter": "Ailing Zeng", "authors": "Yunyan Hong, Ailing Zeng, Min Li, Cewu Lu, Li Jiang, Qiang Xu", "title": "Skimming and Scanning for Untrimmed Video Action Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Video action recognition (VAR) is a primary task of video understanding, and\nuntrimmed videos are more common in real-life scenes. Untrimmed videos have\nredundant and diverse clips containing contextual information, so sampling\ndense clips is essential. Recently, some works attempt to train a generic model\nto select the N most representative clips. However, it is difficult to model\nthe complex relations from intra-class clips and inter-class videos within a\nsingle model and fixed selected number, and the entanglement of multiple\nrelations is also hard to explain. Thus, instead of \"only look once\", we argue\n\"divide and conquer\" strategy will be more suitable in untrimmed VAR. Inspired\nby the speed reading mechanism, we propose a simple yet effective clip-level\nsolution based on skim-scan techniques. Specifically, the proposed Skim-Scan\nframework first skims the entire video and drops those uninformative and\nmisleading clips. For the remaining clips, it scans clips with diverse features\ngradually to drop redundant clips but cover essential content. The above\nstrategies can adaptively select the necessary clips according to the\ndifficulty of the different videos. To trade off the computational complexity\nand performance, we observe the similar statistical expression between\nlightweight and heavy networks, thus it supports us to explore the combination\nof them. Comprehensive experiments are performed on ActivityNet and mini-FCVID\ndatasets, and results demonstrate that our solution surpasses the\nstate-of-the-art performance in terms of both accuracy and efficiency.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 12:23:44 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Hong", "Yunyan", ""], ["Zeng", "Ailing", ""], ["Li", "Min", ""], ["Lu", "Cewu", ""], ["Jiang", "Li", ""], ["Xu", "Qiang", ""]]}, {"id": "2104.10510", "submitter": "Shaoyu Zhang", "authors": "Shaoyu Zhang, Chen Chen, Xiyuan Hu, Silong Peng", "title": "Balanced Knowledge Distillation for Long-tailed Learning", "comments": "10 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep models trained on long-tailed datasets exhibit unsatisfactory\nperformance on tail classes. Existing methods usually modify the classification\nloss to increase the learning focus on tail classes, which unexpectedly\nsacrifice the performance on head classes. In fact, this scheme leads to a\ncontradiction between the two goals of long-tailed learning, i.e., learning\ngeneralizable representations and facilitating learning for tail classes. In\nthis work, we explore knowledge distillation in long-tailed scenarios and\npropose a novel distillation framework, named Balanced Knowledge Distillation\n(BKD), to disentangle the contradiction between the two goals and achieve both\nsimultaneously. Specifically, given a vanilla teacher model, we train the\nstudent model by minimizing the combination of an instance-balanced\nclassification loss and a class-balanced distillation loss. The former benefits\nfrom the sample diversity and learns generalizable representation, while the\nlatter considers the class priors and facilitates learning mainly for tail\nclasses. The student model trained with BKD obtains significant performance\ngain even compared with its teacher model. We conduct extensive experiments on\nseveral long-tailed benchmark datasets and demonstrate that the proposed BKD is\nan effective knowledge distillation framework in long-tailed scenarios, as well\nas a new state-of-the-art method for long-tailed learning. Code is available at\nhttps://github.com/EricZsy/BalancedKnowledgeDistillation .\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 13:07:35 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Zhang", "Shaoyu", ""], ["Chen", "Chen", ""], ["Hu", "Xiyuan", ""], ["Peng", "Silong", ""]]}, {"id": "2104.10511", "submitter": "Manh Duong Phung", "authors": "Qiuchen Zhu, Tran Hiep Dinh, Manh Duong Phung, Quang Phuc Ha", "title": "Hierarchical Convolutional Neural Network with Feature Preservation and\n  Autotuned Thresholding for Crack Detection", "comments": null, "journal-ref": "IEEE Access, 2021", "doi": "10.1109/ACCESS.2021.3073921", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Drone imagery is increasingly used in automated inspection for infrastructure\nsurface defects, especially in hazardous or unreachable environments. In\nmachine vision, the key to crack detection rests with robust and accurate\nalgorithms for image processing. To this end, this paper proposes a deep\nlearning approach using hierarchical convolutional neural networks with feature\npreservation (HCNNFP) and an intercontrast iterative thresholding algorithm for\nimage binarization. First, a set of branch networks is proposed, wherein the\noutput of previous convolutional blocks is half-sizedly concatenated to the\ncurrent ones to reduce the obscuration in the down-sampling stage taking into\naccount the overall information loss. Next, to extract the feature map\ngenerated from the enhanced HCNN, a binary contrast-based autotuned\nthresholding (CBAT) approach is developed at the post-processing step, where\npatterns of interest are clustered within the probability map of the identified\nfeatures. The proposed technique is then applied to identify surface cracks on\nthe surface of roads, bridges or pavements. An extensive comparison with\nexisting techniques is conducted on various datasets and subject to a number of\nevaluation criteria including the average F-measure (AF\\b{eta}) introduced here\nfor dynamic quantification of the performance. Experiments on crack images,\nincluding those captured by unmanned aerial vehicles inspecting a monorail\nbridge. The proposed technique outperforms the existing methods on various\ntested datasets especially for GAPs dataset with an increase of about 1.4% in\nterms of AF\\b{eta} while the mean percentage error drops by 2.2%. Such\nperformance demonstrates the merits of the proposed HCNNFP architecture for\nsurface defect inspection.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 13:07:58 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Zhu", "Qiuchen", ""], ["Dinh", "Tran Hiep", ""], ["Phung", "Manh Duong", ""], ["Ha", "Quang Phuc", ""]]}, {"id": "2104.10515", "submitter": "Max Hermann", "authors": "Max Hermann, Boitumelo Ruf, Martin Weinmann", "title": "Real-time dense 3D Reconstruction from monocular video data captured by\n  low-cost UAVs", "comments": "8 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Real-time 3D reconstruction enables fast dense mapping of the environment\nwhich benefits numerous applications, such as navigation or live evaluation of\nan emergency. In contrast to most real-time capable approaches, our approach\ndoes not need an explicit depth sensor. Instead, we only rely on a video stream\nfrom a camera and its intrinsic calibration. By exploiting the self-motion of\nthe unmanned aerial vehicle (UAV) flying with oblique view around buildings, we\nestimate both camera trajectory and depth for selected images with enough novel\ncontent. To create a 3D model of the scene, we rely on a three-stage processing\nchain. First, we estimate the rough camera trajectory using a simultaneous\nlocalization and mapping (SLAM) algorithm. Once a suitable constellation is\nfound, we estimate depth for local bundles of images using a Multi-View Stereo\n(MVS) approach and then fuse this depth into a global surfel-based model. For\nour evaluation, we use 55 video sequences with diverse settings, consisting of\nboth synthetic and real scenes. We evaluate not only the generated\nreconstruction but also the intermediate products and achieve competitive\nresults both qualitatively and quantitatively. At the same time, our method can\nkeep up with a 30 fps video for a resolution of 768x448 pixels.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 13:12:17 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Hermann", "Max", ""], ["Ruf", "Boitumelo", ""], ["Weinmann", "Martin", ""]]}, {"id": "2104.10538", "submitter": "Khurram Azeem Hashmi", "authors": "Khurram Azeem Hashmi, Didier Stricker, Marcus Liwicki, Muhammad Noman\n  Afzal and Muhammad Zeshan Afzal", "title": "Guided Table Structure Recognition through Anchor Optimization", "comments": "13 pages, 8 figures, 5 tables. Submitted to IEEE Access Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents the novel approach towards table structure recognition by\nleveraging the guided anchors. The concept differs from current\nstate-of-the-art approaches for table structure recognition that naively apply\nobject detection methods. In contrast to prior techniques, first, we estimate\nthe viable anchors for table structure recognition. Subsequently, these anchors\nare exploited to locate the rows and columns in tabular images. Furthermore,\nthe paper introduces a simple and effective method that improves the results by\nusing tabular layouts in realistic scenarios. The proposed method is\nexhaustively evaluated on the two publicly available datasets of table\nstructure recognition i.e ICDAR-2013 and TabStructDB. We accomplished\nstate-of-the-art results on the ICDAR-2013 dataset with an average F-Measure of\n95.05$\\%$ (94.6$\\%$ for rows and 96.32$\\%$ for columns) and surpassed the\nbaseline results on the TabStructDB dataset with an average F-Measure of\n94.17$\\%$ (94.08$\\%$ for rows and 95.06$\\%$ for columns).\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 13:53:09 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Hashmi", "Khurram Azeem", ""], ["Stricker", "Didier", ""], ["Liwicki", "Marcus", ""], ["Afzal", "Muhammad Noman", ""], ["Afzal", "Muhammad Zeshan", ""]]}, {"id": "2104.10546", "submitter": "Yang Liu", "authors": "Yang Liu and Zhenyue Qin and Saeed Anwar and Pan Ji and Dongwoo Kim\n  and Sabrina Caldwell and Tom Gedeon", "title": "Invertible Denoising Network: A Light Solution for Real Noise Removal", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Invertible networks have various benefits for image denoising since they are\nlightweight, information-lossless, and memory-saving during back-propagation.\nHowever, applying invertible models to remove noise is challenging because the\ninput is noisy, and the reversed output is clean, following two different\ndistributions. We propose an invertible denoising network, InvDN, to address\nthis challenge. InvDN transforms the noisy input into a low-resolution clean\nimage and a latent representation containing noise. To discard noise and\nrestore the clean image, InvDN replaces the noisy latent representation with\nanother one sampled from a prior distribution during reversion. The denoising\nperformance of InvDN is better than all the existing competitive models,\nachieving a new state-of-the-art result for the SIDD dataset while enjoying\nless run time. Moreover, the size of InvDN is far smaller, only having 4.2% of\nthe number of parameters compared to the most recently proposed DANet. Further,\nvia manipulating the noisy latent representation, InvDN is also able to\ngenerate noise more similar to the original one. Our code is available at:\nhttps://github.com/Yang-Liu1082/InvDN.git.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 14:03:48 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Liu", "Yang", ""], ["Qin", "Zhenyue", ""], ["Anwar", "Saeed", ""], ["Ji", "Pan", ""], ["Kim", "Dongwoo", ""], ["Caldwell", "Sabrina", ""], ["Gedeon", "Tom", ""]]}, {"id": "2104.10553", "submitter": "Luyang Luo", "authors": "Luyang Luo, Hao Chen, Yongjie Xiao, Yanning Zhou, Xi Wang, Varut\n  Vardhanabhuti, Mingxiang Wu, Pheng-Ann Heng", "title": "Rethinking annotation granularity for overcoming deep shortcut learning:\n  A retrospective study on chest radiographs", "comments": "22 pages of main text, 18 pages of supplementary tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Deep learning has demonstrated radiograph screening performances that are\ncomparable or superior to radiologists. However, recent studies show that deep\nmodels for thoracic disease classification usually show degraded performance\nwhen applied to external data. Such phenomena can be categorized into shortcut\nlearning, where the deep models learn unintended decision rules that can fit\nthe identically distributed training and test set but fail to generalize to\nother distributions. A natural way to alleviate this defect is explicitly\nindicating the lesions and focusing the model on learning the intended\nfeatures. In this paper, we conduct extensive retrospective experiments to\ncompare a popular thoracic disease classification model, CheXNet, and a\nthoracic lesion detection model, CheXDet. We first showed that the two models\nachieved similar image-level classification performance on the internal test\nset with no significant differences under many scenarios. Meanwhile, we found\nincorporating external training data even led to performance degradation for\nCheXNet. Then, we compared the models' internal performance on the lesion\nlocalization task and showed that CheXDet achieved significantly better\nperformance than CheXNet even when given 80% less training data. By further\nvisualizing the models' decision-making regions, we revealed that CheXNet\nlearned patterns other than the target lesions, demonstrating its shortcut\nlearning defect. Moreover, CheXDet achieved significantly better external\nperformance than CheXNet on both the image-level classification task and the\nlesion localization task. Our findings suggest improving annotation granularity\nfor training deep learning systems as a promising way to elevate future deep\nlearning-based diagnosis systems for clinical usage.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 14:21:37 GMT"}, {"version": "v2", "created": "Thu, 8 Jul 2021 08:30:43 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Luo", "Luyang", ""], ["Chen", "Hao", ""], ["Xiao", "Yongjie", ""], ["Zhou", "Yanning", ""], ["Wang", "Xi", ""], ["Vardhanabhuti", "Varut", ""], ["Wu", "Mingxiang", ""], ["Heng", "Pheng-Ann", ""]]}, {"id": "2104.10558", "submitter": "Nicholas Rhinehart", "authors": "Nicholas Rhinehart, Jeff He, Charles Packer, Matthew A. Wright, Rowan\n  McAllister, Joseph E. Gonzalez, Sergey Levine", "title": "Contingencies from Observations: Tractable Contingency Planning with\n  Learned Behavior Models", "comments": "To be published at ICRA 2021. Project page:\n  https://sites.google.com/view/contingency-planning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Humans have a remarkable ability to make decisions by accurately reasoning\nabout future events, including the future behaviors and states of mind of other\nagents. Consider driving a car through a busy intersection: it is necessary to\nreason about the physics of the vehicle, the intentions of other drivers, and\ntheir beliefs about your own intentions. If you signal a turn, another driver\nmight yield to you, or if you enter the passing lane, another driver might\ndecelerate to give you room to merge in front. Competent drivers must plan how\nthey can safely react to a variety of potential future behaviors of other\nagents before they make their next move. This requires contingency planning:\nexplicitly planning a set of conditional actions that depend on the stochastic\noutcome of future events. In this work, we develop a general-purpose\ncontingency planner that is learned end-to-end using high-dimensional scene\nobservations and low-dimensional behavioral observations. We use a conditional\nautoregressive flow model to create a compact contingency planning space, and\nshow how this model can tractably learn contingencies from behavioral\nobservations. We developed a closed-loop control benchmark of realistic\nmulti-agent scenarios in a driving simulator (CARLA), on which we compare our\nmethod to various noncontingent methods that reason about multi-agent future\nbehavior, including several state-of-the-art deep learning-based planning\napproaches. We illustrate that these noncontingent planning methods\nfundamentally fail on this benchmark, and find that our deep contingency\nplanning method achieves significantly superior performance. Code to run our\nbenchmark and reproduce our results is available at\nhttps://sites.google.com/view/contingency-planning\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 14:30:20 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Rhinehart", "Nicholas", ""], ["He", "Jeff", ""], ["Packer", "Charles", ""], ["Wright", "Matthew A.", ""], ["McAllister", "Rowan", ""], ["Gonzalez", "Joseph E.", ""], ["Levine", "Sergey", ""]]}, {"id": "2104.10563", "submitter": "Samim Ahmadi", "authors": "Samim Ahmadi, Linh K\\\"astner, Jan Christian Hauffen, Peter Jung,\n  Mathias Ziegler", "title": "Photothermal-SR-Net: A Customized Deep Unfolding Neural Network for\n  Photothermal Super Resolution Imaging", "comments": "10 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI eess.IV physics.app-ph physics.comp-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents deep unfolding neural networks to handle inverse problems\nin photothermal radiometry enabling super resolution (SR) imaging. Photothermal\nimaging is a well-known technique in active thermography for nondestructive\ninspection of defects in materials such as metals or composites. A grand\nchallenge of active thermography is to overcome the spatial resolution\nlimitation imposed by heat diffusion in order to accurately resolve each\ndefect. The photothermal SR approach enables to extract high-frequency spatial\ncomponents based on the deconvolution with the thermal point spread function.\nHowever, stable deconvolution can only be achieved by using the sparse\nstructure of defect patterns, which often requires tedious, hand-crafted tuning\nof hyperparameters and results in computationally intensive algorithms. On this\naccount, Photothermal-SR-Net is proposed in this paper, which performs\ndeconvolution by deep unfolding considering the underlying physics. This\nenables to super resolve 2D thermal images for nondestructive testing with a\nsubstantially improved convergence rate. Since defects appear sparsely in\nmaterials, Photothermal-SR-Net applies trained block-sparsity thresholding to\nthe acquired thermal images in each convolutional layer. The performance of the\nproposed approach is evaluated and discussed using various deep unfolding and\nthresholding approaches applied to 2D thermal images. Subsequently, studies are\nconducted on how to increase the reconstruction quality and the computational\nperformance of Photothermal-SR-Net is evaluated. Thereby, it was found that the\ncomputing time for creating high-resolution images could be significantly\nreduced without decreasing the reconstruction quality by using pixel binning as\na preprocessing step.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 14:41:04 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Ahmadi", "Samim", ""], ["K\u00e4stner", "Linh", ""], ["Hauffen", "Jan Christian", ""], ["Jung", "Peter", ""], ["Ziegler", "Mathias", ""]]}, {"id": "2104.10567", "submitter": "Yueming Lyu", "authors": "Yueming Lyu, Jing Dong, Bo Peng, Wei Wang, Tieniu Tan", "title": "SOGAN: 3D-Aware Shadow and Occlusion Robust GAN for Makeup Transfer", "comments": "Accepted by ACM MM 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, virtual makeup applications have become more and more\npopular. However, it is still challenging to propose a robust makeup transfer\nmethod in the real-world environment. Current makeup transfer methods mostly\nwork well on good-conditioned clean makeup images, but transferring makeup that\nexhibits shadow and occlusion is not satisfying. To alleviate it, we propose a\nnovel makeup transfer method, called 3D-Aware Shadow and Occlusion Robust GAN\n(SOGAN). Given the source and the reference faces, we first fit a 3D face model\nand then disentangle the faces into shape and texture. In the texture branch,\nwe map the texture to the UV space and design a UV texture generator to\ntransfer the makeup. Since human faces are symmetrical in the UV space, we can\nconveniently remove the undesired shadow and occlusion from the reference image\nby carefully designing a Flip Attention Module (FAM). After obtaining cleaner\nmakeup features from the reference image, a Makeup Transfer Module (MTM) is\nintroduced to perform accurate makeup transfer. The qualitative and\nquantitative experiments demonstrate that our SOGAN not only achieves superior\nresults in shadow and occlusion situations but also performs well in large pose\nand expression variations.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 14:48:49 GMT"}, {"version": "v2", "created": "Tue, 20 Jul 2021 14:16:02 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Lyu", "Yueming", ""], ["Dong", "Jing", ""], ["Peng", "Bo", ""], ["Wang", "Wei", ""], ["Tan", "Tieniu", ""]]}, {"id": "2104.10588", "submitter": "JIan Jiang", "authors": "Jian Jiang, Edoardo Cetin, Oya Celiktutan", "title": "IB-DRR: Incremental Learning with Information-Back Discrete\n  Representation Replay", "comments": "CVPR 2021 Workshop on Continual Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Incremental learning aims to enable machine learning models to continuously\nacquire new knowledge given new classes, while maintaining the knowledge\nalready learned for old classes. Saving a subset of training samples of\npreviously seen classes in the memory and replaying them during new training\nphases is proven to be an efficient and effective way to fulfil this aim. It is\nevident that the larger number of exemplars the model inherits the better\nperformance it can achieve. However, finding a trade-off between the model\nperformance and the number of samples to save for each class is still an open\nproblem for replay-based incremental learning and is increasingly desirable for\nreal-life applications. In this paper, we approach this open problem by tapping\ninto a two-step compression approach. The first step is a lossy compression, we\npropose to encode input images and save their discrete latent representations\nin the form of codes that are learned using a hierarchical Vector Quantised\nVariational Autoencoder (VQ-VAE). In the second step, we further compress codes\nlosslessly by learning a hierarchical latent variable model with bits-back\nasymmetric numeral systems (BB-ANS). To compensate for the information lost in\nthe first step compression, we introduce an Information Back (IB) mechanism\nthat utilizes real exemplars for a contrastive learning loss to regularize the\ntraining of a classifier. By maintaining all seen exemplars' representations in\nthe format of `codes', Discrete Representation Replay (DRR) outperforms the\nstate-of-art method on CIFAR-100 by a margin of 4% accuracy with a much less\nmemory cost required for saving samples. Incorporated with IB and saving a\nsmall set of old raw exemplars as well, the accuracy of DRR can be further\nimproved by 2% accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 15:32:11 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Jiang", "Jian", ""], ["Cetin", "Edoardo", ""], ["Celiktutan", "Oya", ""]]}, {"id": "2104.10596", "submitter": "Lennart Johnsson", "authors": "Nazanin Beheshti, Lennart Johnsson", "title": "Using CNNs for AD classification based on spatial correlation of BOLD\n  signals during the observation", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Resting state functional magnetic resonance images (fMRI) are commonly used\nfor classification of patients as having Alzheimer's disease (AD), mild\ncognitive impairment (MCI), or being cognitive normal (CN). Most methods use\ntime-series correlation of voxels signals during the observation period as a\nbasis for the classification. In this paper we show that Convolutional Neural\nNetwork (CNN) classification based on spatial correlation of time-averaged\nsignals yield a classification accuracy of up to 82% (sensitivity 86%,\nspecificity 80%)for a data set with 429 subjects (246 cognitive normal and 183\nAlzheimer patients). For the spatial correlation of time-averaged signal values\nwe use voxel subdomains around center points of the 90 regions AAL atlas. We\nform the subdomains as sets of voxels along a Hilbert curve of a bounding box\nin which the brain is embedded with the AAL regions center points serving as\nsubdomain seeds. The matrix resulting from the spatial correlation of the 90\narrays formed by the subdomain segments of the Hilbert curve yields a symmetric\n90x90 matrix that is used for the classification based on two different CNN\nnetworks, a 4-layer CNN network with 3x3 filters and with 4, 8, 16, and 32\noutput channels respectively, and a 2-layer CNN network with 3x3 filters and\nwith 4 and 8 output channels respectively. The results of the two networks are\nreported and compared.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 15:48:18 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Beheshti", "Nazanin", ""], ["Johnsson", "Lennart", ""]]}, {"id": "2104.10602", "submitter": "Yunzhong Hou", "authors": "Yunzhong Hou, Liang Zheng", "title": "Visualizing Adapted Knowledge in Domain Transfer", "comments": null, "journal-ref": "CVPR 2021", "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A source model trained on source data and a target model learned through\nunsupervised domain adaptation (UDA) usually encode different knowledge. To\nunderstand the adaptation process, we portray their knowledge difference with\nimage translation. Specifically, we feed a translated image and its original\nversion to the two models respectively, formulating two branches. Through\nupdating the translated image, we force similar outputs from the two branches.\nWhen such requirements are met, differences between the two images can\ncompensate for and hence represent the knowledge difference between models. To\nenforce similar outputs from the two branches and depict the adapted knowledge,\nwe propose a source-free image translation method that generates source-style\nimages using only target images and the two models. We visualize the adapted\nknowledge on several datasets with different UDA methods and find that\ngenerated images successfully capture the style difference between the two\ndomains. For application, we show that generated images enable further tuning\nof the target model without accessing source data. Code available at\nhttps://github.com/hou-yz/DA_visualization.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 17:59:05 GMT"}, {"version": "v2", "created": "Sat, 1 May 2021 11:11:24 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Hou", "Yunzhong", ""], ["Zheng", "Liang", ""]]}, {"id": "2104.10603", "submitter": "Alceu Emanuel Bissoto", "authors": "Alceu Bissoto, Eduardo Valle, Sandra Avila", "title": "GAN-Based Data Augmentation and Anonymization for Skin-Lesion Analysis:\n  A Critical Review", "comments": "Accepted to the ISIC Skin Image Analysis Workshop @ CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Despite the growing availability of high-quality public datasets, the lack of\ntraining samples is still one of the main challenges of deep-learning for skin\nlesion analysis. Generative Adversarial Networks (GANs) appear as an enticing\nalternative to alleviate the issue, by synthesizing samples indistinguishable\nfrom real images, with a plethora of works employing them for medical\napplications. Nevertheless, carefully designed experiments for skin-lesion\ndiagnosis with GAN-based data augmentation show favorable results only on\nout-of-distribution test sets. For GAN-based data anonymization $-$ where the\nsynthetic images replace the real ones $-$ favorable results also only appear\nfor out-of-distribution test sets. Because of the costs and risks associated\nwith GAN usage, those results suggest caution in their adoption for medical\napplications.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 12:47:22 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Bissoto", "Alceu", ""], ["Valle", "Eduardo", ""], ["Avila", "Sandra", ""]]}, {"id": "2104.10609", "submitter": "Gianluca Scarpellini", "authors": "Gianluca Scarpellini, Pietro Morerio, Alessio Del Bue", "title": "Lifting Monocular Events to 3D Human Poses", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents a novel 3D human pose estimation approach using a single\nstream of asynchronous events as input. Most of the state-of-the-art approaches\nsolve this task with RGB cameras, however struggling when subjects are moving\nfast. On the other hand, event-based 3D pose estimation benefits from the\nadvantages of event-cameras, especially their efficiency and robustness to\nappearance changes. Yet, finding human poses in asynchronous events is in\ngeneral more challenging than standard RGB pose estimation, since little or no\nevents are triggered in static scenes. Here we propose the first learning-based\nmethod for 3D human pose from a single stream of events. Our method consists of\ntwo steps. First, we process the event-camera stream to predict three\northogonal heatmaps per joint; each heatmap is the projection of of the joint\nonto one orthogonal plane. Next, we fuse the sets of heatmaps to estimate 3D\nlocalisation of the body joints. As a further contribution, we make available a\nnew, challenging dataset for event-based human pose estimation by simulating\nevents from the RGB Human3.6m dataset. Experiments demonstrate that our method\nachieves solid accuracy, narrowing the performance gap between standard RGB and\nevent-based vision. The code is freely available at\nhttps://iit-pavis.github.io/lifting_events_to_3d_hpe.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 16:07:12 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Scarpellini", "Gianluca", ""], ["Morerio", "Pietro", ""], ["Del Bue", "Alessio", ""]]}, {"id": "2104.10611", "submitter": "Diptodip Deb", "authors": "Diptodip Deb, Zhenfei Jiao, Alex B. Chen, Misha B. Ahrens, Kaspar\n  Podgorski, Srinivas C. Turaga", "title": "Programmable 3D snapshot microscopy with Fourier convolutional networks", "comments": "Make zebrafish Types A,B,C,D more clear", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D snapshot microscopy enables fast volumetric imaging by capturing a 3D\nvolume in a single 2D camera image, and has found a variety of biological\napplications such as whole brain imaging of fast neural activity in larval\nzebrafish. The optimal microscope design for this optical 3D-to-2D encoding is\nboth sample- and task-dependent, with no general solution known. Highly\nprogrammable optical elements create new possibilities for sample-specific\ncomputational optimization of microscope parameters, e.g. tuning the collection\nof light for a given sample structure. We perform such optimization with deep\nlearning, using a differentiable wave-optics simulation of light propagation\nthrough a programmable microscope and a neural network to reconstruct volumes\nfrom the microscope image. We introduce a class of global kernel Fourier\nconvolutional neural networks which can efficiently decode information from\nmultiple depths in the volume, globally encoded across a 3D snapshot image. We\nshow that our proposed networks succeed in large field of view volume\nreconstruction and microscope parameter optimization where traditional networks\nfail. We also show that our networks outperform the state-of-the-art learned\nreconstruction algorithms for lensless computational photography.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 16:09:56 GMT"}, {"version": "v2", "created": "Wed, 9 Jun 2021 00:15:00 GMT"}, {"version": "v3", "created": "Tue, 15 Jun 2021 05:07:16 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Deb", "Diptodip", ""], ["Jiao", "Zhenfei", ""], ["Chen", "Alex B.", ""], ["Ahrens", "Misha B.", ""], ["Podgorski", "Kaspar", ""], ["Turaga", "Srinivas C.", ""]]}, {"id": "2104.10615", "submitter": "Markus Roland Ernst", "authors": "Markus Roland Ernst, Jochen Triesch, Thomas Burwick", "title": "Recurrent Feedback Improves Recognition of Partially Occluded Objects", "comments": "6 pages, 2 figures, 28th European Symposium on Artificial Neural\n  Networks, Computational Intelligence and Machine Learning (ESANN 2020). arXiv\n  admin note: substantial text overlap with arXiv:1909.06175", "journal-ref": "Proceedings of the 28th European Symposium on Artificial Neural\n  Networks, Computational Intelligence and Machine Learning (2020) 327-332", "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Recurrent connectivity in the visual cortex is believed to aid object\nrecognition for challenging conditions such as occlusion. Here we investigate\nif and how artificial neural networks also benefit from recurrence. We compare\narchitectures composed of bottom-up, lateral and top-down connections and\nevaluate their performance using two novel stereoscopic occluded object\ndatasets. We find that classification accuracy is significantly higher for\nrecurrent models when compared to feedforward models of matched parametric\ncomplexity. Additionally we show that for challenging stimuli, the recurrent\nfeedback is able to correctly revise the initial feedforward guess.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 16:18:34 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Ernst", "Markus Roland", ""], ["Triesch", "Jochen", ""], ["Burwick", "Thomas", ""]]}, {"id": "2104.10622", "submitter": "Chenlei Lv", "authors": "Chenlei Lv, Weisi Lin, Baoquan Zhao", "title": "Voxel Structure-based Mesh Reconstruction from a 3D Point Cloud", "comments": "Accepted by IEEE Transactions on Multimedia", "journal-ref": null, "doi": "10.1109/TMM.2021.3073265", "report-no": null, "categories": "cs.GR cs.CG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mesh reconstruction from a 3D point cloud is an important topic in the fields\nof computer graphic, computer vision, and multimedia analysis. In this paper,\nwe propose a voxel structure-based mesh reconstruction framework. It provides\nthe intrinsic metric to improve the accuracy of local region detection. Based\non the detected local regions, an initial reconstructed mesh can be obtained.\nWith the mesh optimization in our framework, the initial reconstructed mesh is\noptimized into an isotropic one with the important geometric features such as\nexternal and internal edges. The experimental results indicate that our\nframework shows great advantages over peer ones in terms of mesh quality,\ngeometric feature keeping, and processing speed.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 16:31:49 GMT"}, {"version": "v2", "created": "Thu, 22 Apr 2021 12:50:41 GMT"}, {"version": "v3", "created": "Fri, 23 Apr 2021 16:55:39 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Lv", "Chenlei", ""], ["Lin", "Weisi", ""], ["Zhao", "Baoquan", ""]]}, {"id": "2104.10631", "submitter": "Chen Huang", "authors": "Chen Huang, Shuangfei Zhai, Pengsheng Guo and Josh Susskind", "title": "MetricOpt: Learning to Optimize Black-Box Evaluation Metrics", "comments": "CVPR 2021 (Oral), Supplementary Materials added", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of directly optimizing arbitrary non-differentiable task\nevaluation metrics such as misclassification rate and recall. Our method, named\nMetricOpt, operates in a black-box setting where the computational details of\nthe target metric are unknown. We achieve this by learning a differentiable\nvalue function, which maps compact task-specific model parameters to metric\nobservations. The learned value function is easily pluggable into existing\noptimizers like SGD and Adam, and is effective for rapidly finetuning a\npre-trained model. This leads to consistent improvements since the value\nfunction provides effective metric supervision during finetuning, and helps to\ncorrect the potential bias of loss-only supervision. MetricOpt achieves\nstate-of-the-art performance on a variety of metrics for (image)\nclassification, image retrieval and object detection. Solid benefits are found\nover competing methods, which often involve complex loss design or adaptation.\nMetricOpt also generalizes well to new tasks and model architectures.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 16:50:01 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Huang", "Chen", ""], ["Zhai", "Shuangfei", ""], ["Guo", "Pengsheng", ""], ["Susskind", "Josh", ""]]}, {"id": "2104.10642", "submitter": "Gang Xu", "authors": "Gang Xu and Jun Xu and Zhen Li and Liang Wang and Xing Sun and\n  Ming-Ming Cheng", "title": "Temporal Modulation Network for Controllable Space-Time Video\n  Super-Resolution", "comments": "This paper is accepted at IEEE Conference on Computer Vision and\n  Pattern Recognition (CVPR) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Space-time video super-resolution (STVSR) aims to increase the spatial and\ntemporal resolutions of low-resolution and low-frame-rate videos. Recently,\ndeformable convolution based methods have achieved promising STVSR performance,\nbut they could only infer the intermediate frame pre-defined in the training\nstage. Besides, these methods undervalued the short-term motion cues among\nadjacent frames. In this paper, we propose a Temporal Modulation Network\n(TMNet) to interpolate arbitrary intermediate frame(s) with accurate\nhigh-resolution reconstruction. Specifically, we propose a Temporal Modulation\nBlock (TMB) to modulate deformable convolution kernels for controllable feature\ninterpolation. To well exploit the temporal information, we propose a\nLocally-temporal Feature Comparison (LFC) module, along with the Bi-directional\nDeformable ConvLSTM, to extract short-term and long-term motion cues in videos.\nExperiments on three benchmark datasets demonstrate that our TMNet outperforms\nprevious STVSR methods. The code is available at\nhttps://github.com/CS-GangXu/TMNet.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 17:10:53 GMT"}, {"version": "v2", "created": "Fri, 30 Apr 2021 01:11:27 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Xu", "Gang", ""], ["Xu", "Jun", ""], ["Li", "Zhen", ""], ["Wang", "Liang", ""], ["Sun", "Xing", ""], ["Cheng", "Ming-Ming", ""]]}, {"id": "2104.10705", "submitter": "Amirsaeed Yazdani", "authors": "Amirsaeed Yazdani, Yung-Chen Sun, Nicholas B. Stephens, Timothy Ryan,\n  Vishal Monga", "title": "Multi-Class Micro-CT Image Segmentation Using Sparse Regularized Deep\n  Networks", "comments": "5 pages, 6 figures, accepted in 2020 54th Asilomar Conference on\n  Signals, Systems, and Computers", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is common in anthropology and paleontology to address questions about\nextant and extinct species through the quantification of osteological features\nobservable in micro-computed tomographic (micro-CT) scans. In cases where\nremains were buried, the grey values present in these scans may be classified\nas belonging to air, dirt, or bone. While various intensity-based methods have\nbeen proposed to segment scans into these classes, it is often the case that\nintensity values for dirt and bone are nearly indistinguishable. In these\ninstances, scientists resort to laborious manual segmentation, which does not\nscale well in practice when a large number of scans are to be analyzed. Here we\npresent a new domain-enriched network for three-class image segmentation, which\nutilizes the domain knowledge of experts familiar with manually segmenting bone\nand dirt structures. More precisely, our novel structure consists of two\ncomponents: 1) a representation network trained on special samples based on\nnewly designed custom loss terms, which extracts discriminative bone and dirt\nfeatures, 2) and a segmentation network that leverages these extracted\ndiscriminative features. These two parts are jointly trained in order to\noptimize the segmentation performance. A comparison of our network to that of\nthe current state-of-the-art U-NETs demonstrates the benefits of our proposal,\nparticularly when the number of labeled training images are limited, which is\ninvariably the case for micro-CT segmentation.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 18:06:26 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Yazdani", "Amirsaeed", ""], ["Sun", "Yung-Chen", ""], ["Stephens", "Nicholas B.", ""], ["Ryan", "Timothy", ""], ["Monga", "Vishal", ""]]}, {"id": "2104.10719", "submitter": "Biswadeep Chakraborty", "authors": "Biswadeep Chakraborty, Xueyuan She, Saibal Mukhopadhyay", "title": "A Fully Spiking Hybrid Neural Network for Energy-Efficient Object\n  Detection", "comments": "10 pages, Submitted Manuscript", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper proposes a Fully Spiking Hybrid Neural Network (FSHNN) for\nenergy-efficient and robust object detection in resource-constrained platforms.\nThe network architecture is based on Convolutional SNN using\nleaky-integrate-fire neuron models. The model combines unsupervised Spike\nTime-Dependent Plasticity (STDP) learning with back-propagation (STBP) learning\nmethods and also uses Monte Carlo Dropout to get an estimate of the uncertainty\nerror. FSHNN provides better accuracy compared to DNN based object detectors\nwhile being 150X energy-efficient. It also outperforms these object detectors,\nwhen subjected to noisy input data and less labeled training data with a lower\nuncertainty error.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 18:39:32 GMT"}, {"version": "v2", "created": "Sat, 24 Jul 2021 01:30:47 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Chakraborty", "Biswadeep", ""], ["She", "Xueyuan", ""], ["Mukhopadhyay", "Saibal", ""]]}, {"id": "2104.10729", "submitter": "Chongyi Li", "authors": "Chongyi Li and Chunle Guo and Linghao Han and Jun Jiang and Ming-Ming\n  Cheng and Jinwei Gu and Chen Change Loy", "title": "Low-Light Image and Video Enhancement Using Deep Learning: A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low-light image enhancement (LLIE) aims at improving the perception or\ninterpretability of an image captured in an environment with poor illumination.\nRecent advances in this area are dominated by deep learning-based solutions,\nwhere many learning strategies, network structures, loss functions, training\ndata, etc. have been employed. In this paper, we provide a comprehensive survey\nto cover various aspects ranging from algorithm taxonomy to unsolved open\nissues. To examine the generalization of existing methods, we propose a\nlarge-scale low-light image and video dataset, in which the images and videos\nare taken by different mobile phones' cameras under diverse illumination\nconditions. Besides, for the first time, we provide a unified online platform\nthat covers many popular LLIE methods, of which the results can be produced\nthrough a user-friendly web interface. In addition to qualitative and\nquantitative evaluation of existing methods on publicly available and our\nproposed datasets, we also validate their performance in face detection in the\ndark. This survey together with the proposed dataset and online platform could\nserve as a reference source for future study and promote the development of\nthis research field. The proposed platform and the collected methods, datasets,\nand evaluation metrics are publicly available and will be regularly updated at\nhttps://github.com/Li-Chongyi/Lighting-the-Darkness-in-the-Deep-Learning-Era-Open.\nOur low-light image and video dataset is also available.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 19:12:19 GMT"}, {"version": "v2", "created": "Thu, 10 Jun 2021 09:02:24 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Li", "Chongyi", ""], ["Guo", "Chunle", ""], ["Han", "Linghao", ""], ["Jiang", "Jun", ""], ["Cheng", "Ming-Ming", ""], ["Gu", "Jinwei", ""], ["Loy", "Chen Change", ""]]}, {"id": "2104.10745", "submitter": "Adrian Celaya", "authors": "Adrian Celaya, Jonas A. Actor, Rajarajeswari Muthusivarajan, Evan\n  Gates, Caroline Chung, Dawid Schellingerhout, Beatrice Riviere, David Fuentes", "title": "PocketNet: A Smaller Neural Network for Medical Image Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Medical imaging deep learning models are often large and complex, requiring\nspecialized hardware to train and evaluate these models. To address such\nissues, we propose the PocketNet paradigm to reduce the size of deep learning\nmodels by throttling the growth of the number of channels in convolutional\nneural networks. We demonstrate that, for a range of segmentation and\nclassification tasks, PocketNet architectures produce results comparable to\nthat of conventional neural networks while reducing the number of parameters by\nmultiple orders of magnitude, using up to 90% less GPU memory, and speeding up\ntraining times by up to 40%, thereby allowing such models to be trained and\ndeployed in resource-constrained settings.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 20:10:30 GMT"}, {"version": "v2", "created": "Thu, 24 Jun 2021 03:29:46 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Celaya", "Adrian", ""], ["Actor", "Jonas A.", ""], ["Muthusivarajan", "Rajarajeswari", ""], ["Gates", "Evan", ""], ["Chung", "Caroline", ""], ["Schellingerhout", "Dawid", ""], ["Riviere", "Beatrice", ""], ["Fuentes", "David", ""]]}, {"id": "2104.10753", "submitter": "Anssi Kanervisto", "authors": "Keishi Ishihara, Anssi Kanervisto, Jun Miura, Ville Hautam\\\"aki", "title": "Multi-task Learning with Attention for End-to-end Autonomous Driving", "comments": "Accepted to CVPR 2021 Workshop on Autonomous Driving", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous driving systems need to handle complex scenarios such as lane\nfollowing, avoiding collisions, taking turns, and responding to traffic\nsignals. In recent years, approaches based on end-to-end behavioral cloning\nhave demonstrated remarkable performance in point-to-point navigational\nscenarios, using a realistic simulator and standard benchmarks. Offline\nimitation learning is readily available, as it does not require expensive hand\nannotation or interaction with the target environment, but it is difficult to\nobtain a reliable system. In addition, existing methods have not specifically\naddressed the learning of reaction for traffic lights, which are a rare\noccurrence in the training datasets. Inspired by the previous work on\nmulti-task learning and attention modeling, we propose a novel multi-task\nattention-aware network in the conditional imitation learning (CIL) framework.\nThis does not only improve the success rate of standard benchmarks, but also\nthe ability to react to traffic lights, which we show with standard benchmarks.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 20:34:57 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Ishihara", "Keishi", ""], ["Kanervisto", "Anssi", ""], ["Miura", "Jun", ""], ["Hautam\u00e4ki", "Ville", ""]]}, {"id": "2104.10762", "submitter": "Robert Murphy", "authors": "Robert A. Murphy", "title": "Image Segmentation, Compression and Reconstruction from Edge\n  Distribution Estimation with Random Field and Random Cluster Theories", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random field and random cluster theory are used to describe certain\nmathematical results concerning the probability distribution of image pixel\nintensities characterized as generic $2D$ integer arrays. The size of the\nsmallest bounded region within an image is estimated for segmenting an image,\nfrom which, the equilibrium distribution of intensities can be recovered. From\nthe estimated bounded regions, properties of the sub-optimal and equilibrium\ndistributions of intensities are derived, which leads to an image compression\nmethodology whereby only slightly more than half of all pixels are required for\na worst-case reconstruction of the original image. A custom deep belief network\nand heuristic allows for the unsupervised segmentation, detection and\nlocalization of objects in an image. An example illustrates the mathematical\nresults.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 07:09:54 GMT"}, {"version": "v10", "created": "Sun, 4 Jul 2021 16:51:37 GMT"}, {"version": "v2", "created": "Wed, 19 May 2021 21:22:25 GMT"}, {"version": "v3", "created": "Mon, 24 May 2021 02:33:01 GMT"}, {"version": "v4", "created": "Sun, 30 May 2021 21:50:26 GMT"}, {"version": "v5", "created": "Fri, 4 Jun 2021 11:29:37 GMT"}, {"version": "v6", "created": "Tue, 15 Jun 2021 02:36:29 GMT"}, {"version": "v7", "created": "Mon, 21 Jun 2021 15:41:25 GMT"}, {"version": "v8", "created": "Mon, 28 Jun 2021 11:13:26 GMT"}, {"version": "v9", "created": "Thu, 1 Jul 2021 05:53:02 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Murphy", "Robert A.", ""]]}, {"id": "2104.10772", "submitter": "Ankit Laddha", "authors": "Ankit Laddha, Shivam Gautam, Stefan Palombo, Shreyash Pandey, Carlos\n  Vallespi-Gonzalez", "title": "MVFuseNet: Improving End-to-End Object Detection and Motion Forecasting\n  through Multi-View Fusion of LiDAR Data", "comments": "Published at CVPR 2021 Workshop on Autonomous Driving", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In this work, we propose \\textit{MVFuseNet}, a novel end-to-end method for\njoint object detection and motion forecasting from a temporal sequence of LiDAR\ndata. Most existing methods operate in a single view by projecting data in\neither range view (RV) or bird's eye view (BEV). In contrast, we propose a\nmethod that effectively utilizes both RV and BEV for spatio-temporal feature\nlearning as part of a temporal fusion network as well as for multi-scale\nfeature learning in the backbone network. Further, we propose a novel\nsequential fusion approach that effectively utilizes multiple views in the\ntemporal fusion network. We show the benefits of our multi-view approach for\nthe tasks of detection and motion forecasting on two large-scale self-driving\ndata sets, achieving state-of-the-art results. Furthermore, we show that\nMVFusenet scales well to large operating ranges while maintaining real-time\nperformance.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 21:29:08 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Laddha", "Ankit", ""], ["Gautam", "Shivam", ""], ["Palombo", "Stefan", ""], ["Pandey", "Shreyash", ""], ["Vallespi-Gonzalez", "Carlos", ""]]}, {"id": "2104.10775", "submitter": "Sara I. Garcia", "authors": "Sara I. Garcia", "title": "Meta-learning for skin cancer detection using Deep Learning Techniques", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This study focuses on automatic skin cancer detection using a Meta-learning\napproach for dermoscopic images. The aim of this study is to explore the\nbenefits of the generalization of the knowledge extracted from non-medical data\nin the classification performance of medical data and the impact of the\ndistribution shift problem within limited data by using a simple class and\ndistribution balancer algorithm. In this study, a small sample of a combined\ndataset from 3 different sources was used to fine-tune a ResNet model\npre-trained on non-medical data. The results show an increase in performance on\ndetecting melanoma, malignant (skin cancer), and benign moles with the prior\nknowledge obtained from images of everyday objects from the ImageNet dataset by\n20 points. These findings suggest that features from non-medical images can be\nused towards the classification of skin moles and that the distribution of the\ndata affects the performance of the model.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 21:44:25 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Garcia", "Sara I.", ""]]}, {"id": "2104.10780", "submitter": "Senthil Yogamani", "authors": "Sambit Mohapatra, Senthil Yogamani, Heinrich Gotzig, Stefan Milz and\n  Patrick Mader", "title": "BEVDetNet: Bird's Eye View LiDAR Point Cloud based Real-time 3D Object\n  Detection for Autonomous Driving", "comments": "Accepted for Oral Presentation at IEEE Intelligent Transportation\n  Systems Conference (ITSC) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D object detection based on LiDAR point clouds is a crucial module in\nautonomous driving particularly for long range sensing. Most of the research is\nfocused on achieving higher accuracy and these models are not optimized for\ndeployment on embedded systems from the perspective of latency and power\nefficiency. For high speed driving scenarios, latency is a crucial parameter as\nit provides more time to react to dangerous situations. Typically a voxel or\npoint-cloud based 3D convolution approach is utilized for this module. Firstly,\nthey are inefficient on embedded platforms as they are not suitable for\nefficient parallelization. Secondly, they have a variable runtime due to level\nof sparsity of the scene which is against the determinism needed in a safety\nsystem. In this work, we aim to develop a very low latency algorithm with fixed\nruntime. We propose a novel semantic segmentation architecture as a single\nunified model for object center detection using key points, box predictions and\norientation prediction using binned classification in a simpler Bird's Eye View\n(BEV) 2D representation. The proposed architecture can be trivially extended to\ninclude semantic segmentation classes like road without any additional\ncomputation. The proposed model has a latency of 4 ms on the embedded Nvidia\nXavier platform. The model is 5X faster than other top accuracy models with a\nminimal accuracy degradation of 2% in Average Precision at IoU=0.5 on KITTI\ndataset.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 22:06:39 GMT"}, {"version": "v2", "created": "Sun, 11 Jul 2021 00:42:38 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Mohapatra", "Sambit", ""], ["Yogamani", "Senthil", ""], ["Gotzig", "Heinrich", ""], ["Milz", "Stefan", ""], ["Mader", "Patrick", ""]]}, {"id": "2104.10781", "submitter": "Ren Yang", "authors": "Ren Yang, Radu Timofte, Jing Liu, Yi Xu, Xinjian Zhang, Minyi Zhao,\n  Shuigeng Zhou, Kelvin C.K. Chan, Shangchen Zhou, Xiangyu Xu, Chen Change Loy,\n  Xin Li, Fanglong Liu, He Zheng, Lielin Jiang, Qi Zhang, Dongliang He, Fu Li,\n  Qingqing Dang, Yibin Huang, Matteo Maggioni, Zhongqian Fu, Shuai Xiao, Cheng\n  li, Thomas Tanay, Fenglong Song, Wentao Chao, Qiang Guo, Yan Liu, Jiang Li,\n  Xiaochao Qu, Dewang Hou, Jiayu Yang, Lyn Jiang, Di You, Zhenyu Zhang, Chong\n  Mou, Iaroslav Koshelev, Pavel Ostyakov, Andrey Somov, Jia Hao, Xueyi Zou,\n  Shijie Zhao, Xiaopeng Sun, Yiting Liao, Yuanzhi Zhang, Qing Wang, Gen Zhan,\n  Mengxi Guo, Junlin Li, Ming Lu, Zhan Ma, Pablo Navarrete Michelini, Hai Wang,\n  Yiyun Chen, Jingyu Guo, Liliang Zhang, Wenming Yang, Sijung Kim, Syehoon Oh,\n  Yucong Wang, Minjie Cai, Wei Hao, Kangdi Shi, Liangyan Li, Jun Chen, Wei Gao,\n  Wang Liu, Xiaoyu Zhang, Linjie Zhou, Sixin Lin, Ru Wang", "title": "NTIRE 2021 Challenge on Quality Enhancement of Compressed Video: Methods\n  and Results", "comments": "Corrected the MOS values in Table 2", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This paper reviews the first NTIRE challenge on quality enhancement of\ncompressed video, with a focus on the proposed methods and results. In this\nchallenge, the new Large-scale Diverse Video (LDV) dataset is employed. The\nchallenge has three tracks. Tracks 1 and 2 aim at enhancing the videos\ncompressed by HEVC at a fixed QP, while Track 3 is designed for enhancing the\nvideos compressed by x265 at a fixed bit-rate. Besides, the quality enhancement\nof Tracks 1 and 3 targets at improving the fidelity (PSNR), and Track 2 targets\nat enhancing the perceptual quality. The three tracks totally attract 482\nregistrations. In the test phase, 12 teams, 8 teams and 11 teams submitted the\nfinal results of Tracks 1, 2 and 3, respectively. The proposed methods and\nsolutions gauge the state-of-the-art of video quality enhancement. The homepage\nof the challenge: https://github.com/RenYang-home/NTIRE21_VEnh\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 22:08:48 GMT"}, {"version": "v2", "created": "Mon, 26 Apr 2021 09:12:11 GMT"}, {"version": "v3", "created": "Wed, 28 Apr 2021 17:22:32 GMT"}, {"version": "v4", "created": "Thu, 29 Apr 2021 09:46:08 GMT"}, {"version": "v5", "created": "Sun, 2 May 2021 21:18:31 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Yang", "Ren", ""], ["Timofte", "Radu", ""], ["Liu", "Jing", ""], ["Xu", "Yi", ""], ["Zhang", "Xinjian", ""], ["Zhao", "Minyi", ""], ["Zhou", "Shuigeng", ""], ["Chan", "Kelvin C. K.", ""], ["Zhou", "Shangchen", ""], ["Xu", "Xiangyu", ""], ["Loy", "Chen Change", ""], ["Li", "Xin", ""], ["Liu", "Fanglong", ""], ["Zheng", "He", ""], ["Jiang", "Lielin", ""], ["Zhang", "Qi", ""], ["He", "Dongliang", ""], ["Li", "Fu", ""], ["Dang", "Qingqing", ""], ["Huang", "Yibin", ""], ["Maggioni", "Matteo", ""], ["Fu", "Zhongqian", ""], ["Xiao", "Shuai", ""], ["li", "Cheng", ""], ["Tanay", "Thomas", ""], ["Song", "Fenglong", ""], ["Chao", "Wentao", ""], ["Guo", "Qiang", ""], ["Liu", "Yan", ""], ["Li", "Jiang", ""], ["Qu", "Xiaochao", ""], ["Hou", "Dewang", ""], ["Yang", "Jiayu", ""], ["Jiang", "Lyn", ""], ["You", "Di", ""], ["Zhang", "Zhenyu", ""], ["Mou", "Chong", ""], ["Koshelev", "Iaroslav", ""], ["Ostyakov", "Pavel", ""], ["Somov", "Andrey", ""], ["Hao", "Jia", ""], ["Zou", "Xueyi", ""], ["Zhao", "Shijie", ""], ["Sun", "Xiaopeng", ""], ["Liao", "Yiting", ""], ["Zhang", "Yuanzhi", ""], ["Wang", "Qing", ""], ["Zhan", "Gen", ""], ["Guo", "Mengxi", ""], ["Li", "Junlin", ""], ["Lu", "Ming", ""], ["Ma", "Zhan", ""], ["Michelini", "Pablo Navarrete", ""], ["Wang", "Hai", ""], ["Chen", "Yiyun", ""], ["Guo", "Jingyu", ""], ["Zhang", "Liliang", ""], ["Yang", "Wenming", ""], ["Kim", "Sijung", ""], ["Oh", "Syehoon", ""], ["Wang", "Yucong", ""], ["Cai", "Minjie", ""], ["Hao", "Wei", ""], ["Shi", "Kangdi", ""], ["Li", "Liangyan", ""], ["Chen", "Jun", ""], ["Gao", "Wei", ""], ["Liu", "Wang", ""], ["Zhang", "Xiaoyu", ""], ["Zhou", "Linjie", ""], ["Lin", "Sixin", ""], ["Wang", "Ru", ""]]}, {"id": "2104.10782", "submitter": "Ren Yang", "authors": "Ren Yang and Radu Timofte", "title": "NTIRE 2021 Challenge on Quality Enhancement of Compressed Video: Dataset\n  and Study", "comments": "Corrected the MOS values in Figure 5-(a) and Table 2", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This paper introduces a novel dataset for video enhancement and studies the\nstate-of-the-art methods of the NTIRE 2021 challenge on quality enhancement of\ncompressed video. The challenge is the first NTIRE challenge in this direction,\nwith three competitions, hundreds of participants and tens of proposed\nsolutions. Our newly collected Large-scale Diverse Video (LDV) dataset is\nemployed in the challenge. In our study, we analyze the proposed methods of the\nchallenge and several methods in previous works on the proposed LDV dataset. We\nfind that the NTIRE 2021 challenge advances the state-of-the-art of quality\nenhancement on compressed video. The proposed LDV dataset is publicly available\nat the homepage of the challenge: https://github.com/RenYang-home/NTIRE21_VEnh\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 22:18:33 GMT"}, {"version": "v2", "created": "Mon, 26 Apr 2021 09:14:09 GMT"}, {"version": "v3", "created": "Wed, 28 Apr 2021 17:16:02 GMT"}, {"version": "v4", "created": "Thu, 29 Apr 2021 09:16:28 GMT"}, {"version": "v5", "created": "Sun, 2 May 2021 21:17:44 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Yang", "Ren", ""], ["Timofte", "Radu", ""]]}, {"id": "2104.10785", "submitter": "Reza Godaz", "authors": "Reza Godaz, Reza Monsefi, Faezeh Toutounian, Reshad Hosseini", "title": "Accurate and fast matrix factorization for low-rank learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we tackle two important challenges related to the accurate\npartial singular value decomposition (SVD) and numerical rank estimation of a\nhuge matrix to use in low-rank learning problems in a fast way. We use the\nconcepts of Krylov subspaces such as the Golub-Kahan bidiagonalization process\nas well as Ritz vectors to achieve these goals. Our experiments identify\nvarious advantages of the proposed methods compared to traditional and\nrandomized SVD (R-SVD) methods with respect to the accuracy of the singular\nvalues and corresponding singular vectors computed in a similar execution time.\nThe proposed methods are appropriate for applications involving huge matrices\nwhere accuracy in all spectrum of the desired singular values, and also all of\ncorresponding singular vectors is essential. We evaluate our method in the real\napplication of Riemannian similarity learning (RSL) between two various image\ndatasets of MNIST and USPS.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 22:35:02 GMT"}, {"version": "v2", "created": "Mon, 3 May 2021 18:27:31 GMT"}, {"version": "v3", "created": "Thu, 15 Jul 2021 18:01:59 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Godaz", "Reza", ""], ["Monsefi", "Reza", ""], ["Toutounian", "Faezeh", ""], ["Hosseini", "Reshad", ""]]}, {"id": "2104.10786", "submitter": "Senthil Yogamani", "authors": "Sugirtha T, Sridevi M, Khailash Santhakumar, B Ravi Kiran, Thomas\n  Gauthier and Senthil Yogamani", "title": "Exploring 2D Data Augmentation for 3D Monocular Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data augmentation is a key component of CNN based image recognition tasks\nlike object detection. However, it is relatively less explored for 3D object\ndetection. Many standard 2D object detection data augmentation techniques do\nnot extend to 3D box. Extension of these data augmentations for 3D object\ndetection requires adaptation of the 3D geometry of the input scene and\nsynthesis of new viewpoints. This requires accurate depth information of the\nscene which may not be always available. In this paper, we evaluate existing 2D\ndata augmentations and propose two novel augmentations for monocular 3D\ndetection without a requirement for novel view synthesis. We evaluate these\naugmentations on the RTM3D detection model firstly due to the shorter training\ntimes . We obtain a consistent improvement by 4% in the 3D AP (@IoU=0.7) for\ncars, ~1.8% scores 3D AP (@IoU=0.25) for pedestrians & cyclists, over the\nbaseline on KITTI car detection dataset. We also demonstrate a rigorous\nevaluation of the mAP scores by re-weighting them to take into account the\nclass imbalance in the KITTI validation dataset.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 22:43:42 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["T", "Sugirtha", ""], ["M", "Sridevi", ""], ["Santhakumar", "Khailash", ""], ["Kiran", "B Ravi", ""], ["Gauthier", "Thomas", ""], ["Yogamani", "Senthil", ""]]}, {"id": "2104.10824", "submitter": "Kaidong Li", "authors": "Kaidong Li, Mohammad I. Fathan, Krushi Patel, Tianxiao Zhang, Cuncong\n  Zhong, Ajay Bansal, Amit Rastogi, Jean S. Wang, Guanghui Wang", "title": "Colonoscopy Polyp Detection and Classification: Dataset Creation and\n  Comparative Evaluations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Colorectal cancer (CRC) is one of the most common types of cancer with a high\nmortality rate. Colonoscopy is the preferred procedure for CRC screening and\nhas proven to be effective in reducing CRC mortality. Thus, a reliable\ncomputer-aided polyp detection and classification system can significantly\nincrease the effectiveness of colonoscopy. In this paper, we create an\nendoscopic dataset collected from various sources and annotate the ground truth\nof polyp location and classification results with the help of experienced\ngastroenterologists. The dataset can serve as a benchmark platform to train and\nevaluate the machine learning models for polyp classification. We have also\ncompared the performance of eight state-of-the-art deep learning-based object\ndetection models. The results demonstrate that deep CNN models are promising in\nCRC screening. This work can serve as a baseline for future research in polyp\ndetection and classification.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 01:57:35 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Li", "Kaidong", ""], ["Fathan", "Mohammad I.", ""], ["Patel", "Krushi", ""], ["Zhang", "Tianxiao", ""], ["Zhong", "Cuncong", ""], ["Bansal", "Ajay", ""], ["Rastogi", "Amit", ""], ["Wang", "Jean S.", ""], ["Wang", "Guanghui", ""]]}, {"id": "2104.10826", "submitter": "YangQuan Chen Prof.", "authors": "Guoxiang Zhang and YangQuan Chen", "title": "Self-optimizing loop sifting and majorization for 3D reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual simultaneous localization and mapping (vSLAM) and 3D reconstruction\nmethods have gone through impressive progress. These methods are very promising\nfor autonomous vehicle and consumer robot applications because they can map\nlarge-scale environments such as cities and indoor environments without the\nneed for much human effort. However, when it comes to loop detection and\noptimization, there is still room for improvement. vSLAM systems tend to add\nthe loops very conservatively to reduce the severe influence of the false\nloops. These conservative checks usually lead to correct loops rejected, thus\ndecrease performance. In this paper, an algorithm that can sift and majorize\nloop detections is proposed. Our proposed algorithm can compare the usefulness\nand effectiveness of different loops with the dense map posterior (DMP) metric.\nThe algorithm tests and decides the acceptance of each loop without a single\nuser-defined threshold. Thus it is adaptive to different data conditions. The\nproposed method is general and agnostic to sensor type (as long as depth or\nLiDAR reading presents), loop detection, and optimization methods. Neither does\nit require a specific type of SLAM system. Thus it has great potential to be\napplied to various application scenarios. Experiments are conducted on public\ndatasets. Results show that the proposed method outperforms state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 01:59:19 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Zhang", "Guoxiang", ""], ["Chen", "YangQuan", ""]]}, {"id": "2104.10834", "submitter": "Xinyi Wu", "authors": "Xinyi Wu, Zhenyao Wu, Hao Guo, Lili Ju, Song Wang", "title": "DANNet: A One-Stage Domain Adaptation Network for Unsupervised Nighttime\n  Semantic Segmentation", "comments": "Accepted by CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic segmentation of nighttime images plays an equally important role as\nthat of daytime images in autonomous driving, but the former is much more\nchallenging due to poor illuminations and arduous human annotations. In this\npaper, we propose a novel domain adaptation network (DANNet) for nighttime\nsemantic segmentation without using labeled nighttime image data. It employs an\nadversarial training with a labeled daytime dataset and an unlabeled dataset\nthat contains coarsely aligned day-night image pairs. Specifically, for the\nunlabeled day-night image pairs, we use the pixel-level predictions of static\nobject categories on a daytime image as a pseudo supervision to segment its\ncounterpart nighttime image. We further design a re-weighting strategy to\nhandle the inaccuracy caused by misalignment between day-night image pairs and\nwrong predictions of daytime images, as well as boost the prediction accuracy\nof small objects. The proposed DANNet is the first one stage adaptation\nframework for nighttime semantic segmentation, which does not train additional\nday-night image transfer models as a separate pre-processing stage. Extensive\nexperiments on Dark Zurich and Nighttime Driving datasets show that our method\nachieves state-of-the-art performance for nighttime semantic segmentation.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 02:49:28 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Wu", "Xinyi", ""], ["Wu", "Zhenyao", ""], ["Guo", "Hao", ""], ["Ju", "Lili", ""], ["Wang", "Song", ""]]}, {"id": "2104.10847", "submitter": "Mehrnaz Fani", "authors": "Mehrnaz Fani, Pascale Berunelle Walters, David A. Clausi, John Zelek\n  and Alexander Wong", "title": "Localization of Ice-Rink for Broadcast Hockey Videos", "comments": "6 pages, 2 figures, Poster in WiCV2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In this work, an automatic and simple framework for hockey ice-rink\nlocalization from broadcast videos is introduced. First, video is broken into\nvideo-shots by a hierarchical partitioning of the video frames, and\nthresholding based on their histograms. To localize the frames on the ice-rink\nmodel, a ResNet18-based regressor is implemented and trained, which regresses\nto four control points on the model in a frame-by-frame fashion. This leads to\nthe projection jittering problem in the video. To overcome this, in the\ninference phase, the trajectory of the control points on the ice-rink model are\nsmoothed, for all the consecutive frames of a given video-shot, by convolving a\nHann window with the achieved coordinates. Finally, the smoothed homography\nmatrix is computed by using the direct linear transform on the four pairs of\ncorresponding points. A hockey dataset for training and testing the regressor\nis gathered. The results show success of this simple and comprehensive\nprocedure for localizing the hockey ice-rink and addressing the problem of\njittering without affecting the accuracy of homography estimation.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 03:39:43 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Fani", "Mehrnaz", ""], ["Walters", "Pascale Berunelle", ""], ["Clausi", "David A.", ""], ["Zelek", "John", ""], ["Wong", "Alexander", ""]]}, {"id": "2104.10850", "submitter": "Chuong Nguyen", "authors": "Su V. Huynh, Nam H. Nguyen, Ngoc T. Nguyen, Vinh TQ. Nguyen, Chau\n  Huynh, Chuong Nguyen", "title": "A Strong Baseline for Vehicle Re-Identification", "comments": "Accepted to CVPR Workshop 2021, 5th AI City Challenge", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Vehicle Re-Identification (Re-ID) aims to identify the same vehicle across\ndifferent cameras, hence plays an important role in modern traffic management\nsystems. The technical challenges require the algorithms must be robust in\ndifferent views, resolution, occlusion and illumination conditions. In this\npaper, we first analyze the main factors hindering the Vehicle Re-ID\nperformance. We then present our solutions, specifically targeting the dataset\nTrack 2 of the 5th AI City Challenge, including (1) reducing the domain gap\nbetween real and synthetic data, (2) network modification by stacking multi\nheads with attention mechanism, (3) adaptive loss weight adjustment. Our method\nachieves 61.34% mAP on the private CityFlow testset without using external\ndataset or pseudo labeling, and outperforms all previous works at 87.1% mAP on\nthe Veri benchmark. The code is available at\nhttps://github.com/cybercore-co-ltd/track2_aicity_2021.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 03:54:55 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Huynh", "Su V.", ""], ["Nguyen", "Nam H.", ""], ["Nguyen", "Ngoc T.", ""], ["Nguyen", "Vinh TQ.", ""], ["Huynh", "Chau", ""], ["Nguyen", "Chuong", ""]]}, {"id": "2104.10851", "submitter": "Alexander Hadjiivanov", "authors": "Alexander Hadjiivanov", "title": "Continuous Learning and Adaptation with Membrane Potential and\n  Activation Threshold Homeostasis", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Most classical (non-spiking) neural network models disregard internal neuron\ndynamics and treat neurons as simple input integrators. However, biological\nneurons have an internal state governed by complex dynamics that plays a\ncrucial role in learning, adaptation and the overall network activity and\nbehaviour. This paper presents the Membrane Potential and Activation Threshold\nHomeostasis (MPATH) neuron model, which combines several biologically inspired\nmechanisms to efficiently simulate internal neuron dynamics with a single\nparameter analogous to the membrane time constant in biological neurons. The\nmodel allows neurons to maintain a form of dynamic equilibrium by automatically\nregulating their activity when presented with fluctuating input. One\nconsequence of the MPATH model is that it imbues neurons with a sense of time\nwithout recurrent connections, paving the way for modelling processes that\ndepend on temporal aspects of neuron activity. Experiments demonstrate the\nmodel's ability to adapt to and continually learn from its input.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 04:01:32 GMT"}, {"version": "v2", "created": "Sat, 8 May 2021 13:30:55 GMT"}, {"version": "v3", "created": "Wed, 9 Jun 2021 01:24:43 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Hadjiivanov", "Alexander", ""]]}, {"id": "2104.10856", "submitter": "Ojasvi Yadav", "authors": "Ojasvi Yadav, Koustav Ghosal, Sebastian Lutz, Aljosa Smolic", "title": "Frequency Domain Loss Function for Deep Exposure Correction of Dark\n  Images", "comments": "Published version 8 pages. SIViP (2021)", "journal-ref": null, "doi": "10.1007/s11760-021-01915-4", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We address the problem of exposure correction of dark, blurry and noisy\nimages captured in low-light conditions in the wild. Classical image-denoising\nfilters work well in the frequency space but are constrained by several factors\nsuch as the correct choice of thresholds, frequency estimates etc. On the other\nhand, traditional deep networks are trained end-to-end in the RGB space by\nformulating this task as an image-translation problem. However, that is done\nwithout any explicit constraints on the inherent noise of the dark images and\nthus produce noisy and blurry outputs. To this end we propose a DCT/FFT based\nmulti-scale loss function, which when combined with traditional losses, trains\na network to translate the important features for visually pleasing output. Our\nloss function is end-to-end differentiable, scale-agnostic, and generic; i.e.,\nit can be applied to both RAW and JPEG images in most existing frameworks\nwithout additional overhead. Using this loss function, we report significant\nimprovements over the state-of-the-art using quantitative metrics and\nsubjective tests.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 04:13:40 GMT"}, {"version": "v2", "created": "Fri, 21 May 2021 12:41:57 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Yadav", "Ojasvi", ""], ["Ghosal", "Koustav", ""], ["Lutz", "Sebastian", ""], ["Smolic", "Aljosa", ""]]}, {"id": "2104.10858", "submitter": "Zihang Jiang", "authors": "Zihang Jiang, Qibin Hou, Li Yuan, Daquan Zhou, Yujun Shi, Xiaojie Jin,\n  Anran Wang, Jiashi Feng", "title": "All Tokens Matter: Token Labeling for Training Better Vision\n  Transformers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we present token labeling -- a new training objective for\ntraining high-performance vision transformers (ViTs). Different from the\nstandard training objective of ViTs that computes the classification loss on an\nadditional trainable class token, our proposed one takes advantage of all the\nimage patch tokens to compute the training loss in a dense manner.\nSpecifically, token labeling reformulates the image classification problem into\nmultiple token-level recognition problems and assigns each patch token with an\nindividual location-specific supervision generated by a machine annotator.\nExperiments show that token labeling can clearly and consistently improve the\nperformance of various ViT models across a wide spectrum. For a vision\ntransformer with 26M learnable parameters serving as an example, with token\nlabeling, the model can achieve 84.4% Top-1 accuracy on ImageNet. The result\ncan be further increased to 86.4% by slightly scaling the model size up to\n150M, delivering the minimal-sized model among previous models (250M+) reaching\n86%. We also show that token labeling can clearly improve the generalization\ncapability of the pre-trained models on downstream tasks with dense prediction,\nsuch as semantic segmentation. Our code and all the training details will be\nmade publicly available at https://github.com/zihangJiang/TokenLabeling.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 04:43:06 GMT"}, {"version": "v2", "created": "Fri, 23 Apr 2021 08:50:56 GMT"}, {"version": "v3", "created": "Wed, 9 Jun 2021 15:27:26 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Jiang", "Zihang", ""], ["Hou", "Qibin", ""], ["Yuan", "Li", ""], ["Zhou", "Daquan", ""], ["Shi", "Yujun", ""], ["Jin", "Xiaojie", ""], ["Wang", "Anran", ""], ["Feng", "Jiashi", ""]]}, {"id": "2104.10868", "submitter": "Qiming Wu", "authors": "Qiming Wu, Zhikang Zou, Pan Zhou, Xiaoqing Ye, Binghui Wang and Ang Li", "title": "Towards Adversarial Patch Analysis and Certified Defense against Crowd\n  Counting", "comments": "Accepted by ACM Multimedia 2021", "journal-ref": null, "doi": "10.1145/3474085.3475378", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowd counting has drawn much attention due to its importance in\nsafety-critical surveillance systems. Especially, deep neural network (DNN)\nmethods have significantly reduced estimation errors for crowd counting\nmissions. Recent studies have demonstrated that DNNs are vulnerable to\nadversarial attacks, i.e., normal images with human-imperceptible perturbations\ncould mislead DNNs to make false predictions. In this work, we propose a robust\nattack strategy called Adversarial Patch Attack with Momentum (APAM) to\nsystematically evaluate the robustness of crowd counting models, where the\nattacker's goal is to create an adversarial perturbation that severely degrades\ntheir performances, thus leading to public safety accidents (e.g., stampede\naccidents). Especially, the proposed attack leverages the extreme-density\nbackground information of input images to generate robust adversarial patches\nvia a series of transformations (e.g., interpolation, rotation, etc.). We\nobserve that by perturbing less than 6\\% of image pixels, our attacks severely\ndegrade the performance of crowd counting systems, both digitally and\nphysically. To better enhance the adversarial robustness of crowd counting\nmodels, we propose the first regression model-based Randomized Ablation (RA),\nwhich is more sufficient than Adversarial Training (ADT) (Mean Absolute Error\nof RA is 5 lower than ADT on clean samples and 30 lower than ADT on adversarial\nexamples). Extensive experiments on five crowd counting models demonstrate the\neffectiveness and generality of the proposed method. The supplementary\nmaterials and certificate retrained models are available at\n\\url{https://www.dropbox.com/s/hc4fdx133vht0qb/ACM_MM2021_Supp.pdf?dl=0}\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 05:10:55 GMT"}, {"version": "v2", "created": "Mon, 5 Jul 2021 14:20:22 GMT"}, {"version": "v3", "created": "Wed, 28 Jul 2021 03:43:42 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Wu", "Qiming", ""], ["Zou", "Zhikang", ""], ["Zhou", "Pan", ""], ["Ye", "Xiaoqing", ""], ["Wang", "Binghui", ""], ["Li", "Ang", ""]]}, {"id": "2104.10874", "submitter": "Savvas Karatsiolis", "authors": "Savvas Karatsiolis and Andreas Kamilaris", "title": "Focusing on Shadows for Predicting Heightmaps from Single Remotely\n  Sensed RGB Images with Deep Learning", "comments": "30 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Estimating the heightmaps of buildings and vegetation in single remotely\nsensed images is a challenging problem. Effective solutions to this problem can\ncomprise the stepping stone for solving complex and demanding problems that\nrequire 3D information of aerial imagery in the remote sensing discipline,\nwhich might be expensive or not feasible to require. We propose a task-focused\nDeep Learning (DL) model that takes advantage of the shadow map of a remotely\nsensed image to calculate its heightmap. The shadow is computed efficiently and\ndoes not add significant computation complexity. The model is trained with\naerial images and their Lidar measurements, achieving superior performance on\nthe task. We validate the model with a dataset covering a large area of\nManchester, UK, as well as the 2018 IEEE GRSS Data Fusion Contest Lidar\ndataset. Our work suggests that the proposed DL architecture and the technique\nof injecting shadows information into the model are valuable for improving the\nheightmap estimation task for single remotely sensed imagery.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 05:31:13 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Karatsiolis", "Savvas", ""], ["Kamilaris", "Andreas", ""]]}, {"id": "2104.10879", "submitter": "Xin Zheng", "authors": "Xin Zheng, Jianke Zhu", "title": "Efficient LiDAR Odometry for Autonomous Driving", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  LiDAR odometry plays an important role in self-localization and mapping for\nautonomous navigation, which is usually treated as a scan registration problem.\nAlthough having achieved promising performance on KITTI odometry benchmark, the\nconventional searching tree-based approach still has the difficulty in dealing\nwith the large scale point cloud efficiently. The recent spherical range\nimage-based method enjoys the merits of fast nearest neighbor search by\nspherical mapping. However, it is not very effective to deal with the ground\npoints nearly parallel to LiDAR beams. To address these issues, we propose a\nnovel efficient LiDAR odometry approach by taking advantage of both non-ground\nspherical range image and bird's-eye-view map for ground points. Moreover, a\nrange adaptive method is introduced to robustly estimate the local surface\nnormal. Additionally, a very fast and memory-efficient model update scheme is\nproposed to fuse the points and their corresponding normals at different\ntime-stamps. We have conducted extensive experiments on KITTI odometry\nbenchmark, whose promising results demonstrate that our proposed approach is\neffective.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 06:05:09 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Zheng", "Xin", ""], ["Zhu", "Jianke", ""]]}, {"id": "2104.10891", "submitter": "Sreetama Das", "authors": "Sreetama Das (1), Anirban Nag (1), Dhruba Adhikary (1), Ramswaroop\n  Jeevan Ram (1), Aravind BR (1), Sujit Kumar Ojha (1), Guruprasad M Hegde (2)\n  ((1) Engineering Data Sciences, (2) Research and Technology Centre, Robert\n  Bosch Engineering and Business Solutions Private Limited, Koramangala,\n  Bangalore, India)", "title": "Computer Vision-based Social Distancing Surveillance Solution with\n  Optional Automated Camera Calibration for Large Scale Deployment", "comments": "8 pages, 5 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Social distancing has been suggested as one of the most effective measures to\nbreak the chain of viral transmission in the current COVID-19 pandemic. We\nherein describe a computer vision-based AI-assisted solution to aid compliance\nwith social distancing norms. The solution consists of modules to detect and\ntrack people and to identify distance violations. It provides the flexibility\nto choose between a tool-based mode or an automated mode of camera calibration,\nmaking the latter suitable for large-scale deployments. In this paper, we\ndiscuss different metrics to assess the risk associated with social distancing\nviolations and how we can differentiate between transient or persistent\nviolations. Our proposed solution performs satisfactorily under different test\nscenarios, processes video feed at real-time speed as well as addresses data\nprivacy regulations by blurring faces of detected people, making it ideal for\ndeployments.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 06:43:02 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Das", "Sreetama", ""], ["Nag", "Anirban", ""], ["Adhikary", "Dhruba", ""], ["Ram", "Ramswaroop Jeevan", ""], ["BR", "Aravind", ""], ["Ojha", "Sujit Kumar", ""], ["Hegde", "Guruprasad M", ""]]}, {"id": "2104.10900", "submitter": "Bolivar Solarte", "authors": "Bolivar Solarte, Chin-Hsuan Wu, Kuan-Wei Lu, Min Sun, Wei-Chen Chiu,\n  Yi-Hsuan Tsai", "title": "Robust 360-8PA: Redesigning The Normalized 8-point Algorithm for 360-FoV\n  Images", "comments": "Accepted to ICRA 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents a novel preconditioning strategy for the classic 8-point\nalgorithm (8-PA) for estimating an essential matrix from 360-FoV images (i.e.,\nequirectangular images) in spherical projection. To alleviate the effect of\nuneven key-feature distributions and outlier correspondences, which can\npotentially decrease the accuracy of an essential matrix, our method optimizes\na non-rigid transformation to deform a spherical camera into a new spatial\ndomain, defining a new constraint and a more robust and accurate solution for\nan essential matrix. Through several experiments using random synthetic points,\n360-FoV, and fish-eye images, we demonstrate that our normalization can\nincrease the camera pose accuracy by about 20% without significantly overhead\nthe computation time. In addition, we present further benefits of our method\nthrough both a constant weighted least-square optimization that improves\nfurther the well known Gold Standard Method (GSM) (i.e., the non-linear\noptimization by using epipolar errors); and a relaxation of the number of\nRANSAC iterations, both showing that our normalization outcomes a more\nreliable, robust, and accurate solution.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 07:23:11 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Solarte", "Bolivar", ""], ["Wu", "Chin-Hsuan", ""], ["Lu", "Kuan-Wei", ""], ["Sun", "Min", ""], ["Chiu", "Wei-Chen", ""], ["Tsai", "Yi-Hsuan", ""]]}, {"id": "2104.10901", "submitter": "Clemens-Alexander Brust", "authors": "Clemens-Alexander Brust, Bj\\\"orn Barz, Joachim Denzler", "title": "Self-Supervised Learning from Semantically Imprecise Data", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning from imprecise labels such as \"animal\" or \"bird\", but making precise\npredictions like \"snow bunting\" at test time is an important capability when\nexpertly labeled training data is scarce. Contributions by volunteers or\nresults of web crawling lack precision in this manner, but are still valuable.\nAnd crucially, these weakly labeled examples are available in larger quantities\nfor lower cost than high-quality bespoke training data.\n  CHILLAX, a recently proposed method to tackle this task, leverages a\nhierarchical classifier to learn from imprecise labels. However, it has two\nmajor limitations. First, it is not capable of learning from effectively\nunlabeled examples at the root of the hierarchy, e.g. \"object\". Second, an\nextrapolation of annotations to precise labels is only performed at test time,\nwhere confident extrapolations could be already used as training data.\n  In this work, we extend CHILLAX with a self-supervised scheme using\nconstrained extrapolation to generate pseudo-labels. This addresses the second\nconcern, which in turn solves the first problem, enabling an even weaker\nsupervision requirement than CHILLAX. We evaluate our approach empirically and\nshow that our method allows for a consistent accuracy improvement of 0.84 to\n1.19 percent points over CHILLAX and is suitable as a drop-in replacement\nwithout any negative consequences such as longer training times.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 07:26:14 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Brust", "Clemens-Alexander", ""], ["Barz", "Bj\u00f6rn", ""], ["Denzler", "Joachim", ""]]}, {"id": "2104.10916", "submitter": "Zubair Martin", "authors": "Zubair Martin, Amir Patel and Sharief Hendricks", "title": "Automated Tackle Injury Risk Assessment in Contact-Based Sports -- A\n  Rugby Union Example", "comments": "The paper has five figures and is eight pages long (excluding\n  references). The paper has been entered into the 7th IEEE International\n  Workshop on Computer Vision in Sports (CVSports)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video analysis in tackle-collision based sports is highly subjective and\nexposed to bias, which is inherent in human observation, especially under time\nconstraints. This limitation of match analysis in tackle-collision based sports\ncan be seen as an opportunity for computer vision applications. Objectively\ntracking, detecting and recognising an athlete's movements and actions during\nmatch play from a distance using video, along with our improved understanding\nof injury aetiology and skill execution will enhance our understanding how\ninjury occurs, assist match day injury management, reduce referee subjectivity.\nIn this paper, we present a system of objectively evaluating in-game tackle\nrisk in rugby union matches. First, a ball detection model is trained using the\nYou Only Look Once (YOLO) framework, these detections are then tracked by a\nKalman Filter (KF). Following this, a separate YOLO model is used to detect\npersons/players within a tackle segment and then the ball-carrier and tackler\nare identified. Subsequently, we utilize OpenPose to determine the pose of\nball-carrier and tackle, the relative pose of these is then used to evaluate\nthe risk of the tackle. We tested the system on a diverse collection of rugby\ntackles and achieved an evaluation accuracy of 62.50%. These results will\nenable referees in tackle-contact based sports to make more subjective\ndecisions, ultimately making these sports safer.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 07:51:33 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Martin", "Zubair", ""], ["Patel", "Amir", ""], ["Hendricks", "Sharief", ""]]}, {"id": "2104.10922", "submitter": "Zander Venter", "authors": "Zander S. Venter, Markus A.K. Sydenham", "title": "Continental-scale land cover mapping at 10 m resolution over Europe\n  (ELC10)", "comments": null, "journal-ref": "Remote Sens. 2021, 13, 2301", "doi": "10.3390/rs13122301", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Widely used European land cover maps such as CORINE are produced at medium\nspatial resolutions (100 m) and rely on diverse data with complex workflows\nrequiring significant institutional capacity. We present a high resolution (10\nm) land cover map (ELC10) of Europe based on a satellite-driven machine\nlearning workflow that is annually updatable. A Random Forest classification\nmodel was trained on 70K ground-truth points from the LUCAS (Land Use/Cover\nArea frame Survey) dataset. Within the Google Earth Engine cloud computing\nenvironment, the ELC10 map can be generated from approx. 700 TB of Sentinel\nimagery within approx. 4 days from a single research user account. The map\nachieved an overall accuracy of 90% across 8 land cover classes and could\naccount for statistical unit land cover proportions within 3.9% (R2 = 0.83) of\nthe actual value. These accuracies are higher than that of CORINE (100 m) and\nother 10-m land cover maps including S2GLC and FROM-GLC10. We found that\natmospheric correction of Sentinel-2 and speckle filtering of Sentinel-1\nimagery had minimal effect on enhancing classification accuracy (< 1%).\nHowever, combining optical and radar imagery increased accuracy by 3% compared\nto Sentinel-2 alone and by 10% compared to Sentinel-1 alone. The conversion of\nLUCAS points into homogenous polygons under the Copernicus module increased\naccuracy by <1%, revealing that Random Forests are robust against contaminated\ntraining data. Furthermore, the model requires very little training data to\nachieve moderate accuracies - the difference between 5K and 50K LUCAS points is\nonly 3% (86 vs 89%). At 10-m resolution, the ELC10 map can distinguish detailed\nlandscape features like hedgerows and gardens, and therefore holds potential\nfor aerial statistics at the city borough level and monitoring property-level\nenvironmental interventions (e.g. tree planting).\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 08:24:15 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Venter", "Zander S.", ""], ["Sydenham", "Markus A. K.", ""]]}, {"id": "2104.10935", "submitter": "Peihua Li", "authors": "Jiangtao Xie, Ruiren Zeng, Qilong Wang, Ziqi Zhou, Peihua Li", "title": "So-ViT: Mind Visual Tokens for Vision Transformer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Recently the vision transformer (ViT) architecture, where the backbone purely\nconsists of self-attention mechanism, has achieved very promising performance\nin visual classification. However, the high performance of the original ViT\nheavily depends on pretraining using ultra large-scale datasets, and it\nsignificantly underperforms on ImageNet-1K if trained from scratch. This paper\nmakes the efforts toward addressing this problem, by carefully considering the\nrole of visual tokens. First, for classification head, existing ViT only\nexploits class token while entirely neglecting rich semantic information\ninherent in high-level visual tokens. Therefore, we propose a new\nclassification paradigm, where the second-order, cross-covariance pooling of\nvisual tokens is combined with class token for final classification. Meanwhile,\na fast singular value power normalization is proposed for improving the\nsecond-order pooling. Second, the original ViT employs the naive embedding of\nfixed-size image patches, lacking the ability to model translation equivariance\nand locality. To alleviate this problem, we develop a light-weight,\nhierarchical module based on off-the-shelf convolutions for visual token\nembedding. The proposed architecture, which we call So-ViT, is thoroughly\nevaluated on ImageNet-1K. The results show our models, when trained from\nscratch, outperform the competing ViT variants, while being on par with or\nbetter than state-of-the-art CNN models. Code is available at\nhttps://github.com/jiangtaoxie/So-ViT\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 09:05:09 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Xie", "Jiangtao", ""], ["Zeng", "Ruiren", ""], ["Wang", "Qilong", ""], ["Zhou", "Ziqi", ""], ["Li", "Peihua", ""]]}, {"id": "2104.10955", "submitter": "Yanbei Chen", "authors": "Yanbei Chen, Yongqin Xian, A. Sophia Koepke, Ying Shan, Zeynep Akata", "title": "Distilling Audio-Visual Knowledge by Compositional Contrastive Learning", "comments": "Accepted to CVPR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Having access to multi-modal cues (e.g. vision and audio) empowers some\ncognitive tasks to be done faster compared to learning from a single modality.\nIn this work, we propose to transfer knowledge across heterogeneous modalities,\neven though these data modalities may not be semantically correlated. Rather\nthan directly aligning the representations of different modalities, we compose\naudio, image, and video representations across modalities to uncover richer\nmulti-modal knowledge. Our main idea is to learn a compositional embedding that\ncloses the cross-modal semantic gap and captures the task-relevant semantics,\nwhich facilitates pulling together representations across modalities by\ncompositional contrastive learning. We establish a new, comprehensive\nmulti-modal distillation benchmark on three video datasets: UCF101,\nActivityNet, and VGGSound. Moreover, we demonstrate that our model\nsignificantly outperforms a variety of existing knowledge distillation methods\nin transferring audio-visual knowledge to improve video representation\nlearning. Code is released here: https://github.com/yanbeic/CCL.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 09:31:20 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Chen", "Yanbei", ""], ["Xian", "Yongqin", ""], ["Koepke", "A. Sophia", ""], ["Shan", "Ying", ""], ["Akata", "Zeynep", ""]]}, {"id": "2104.10956", "submitter": "Tai Wang", "authors": "Tai Wang, Xinge Zhu, Jiangmiao Pang, Dahua Lin", "title": "FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection", "comments": "Technical report for the best vision-only method (1st place of the\n  camera track) in the nuScenes 3D detection challenge of NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.RO", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Monocular 3D object detection is an important task for autonomous driving\nconsidering its advantage of low cost. It is much more challenging compared to\nconventional 2D case due to its inherent ill-posed property, which is mainly\nreflected on the lack of depth information. Recent progress on 2D detection\noffers opportunities to better solving this problem. However, it is non-trivial\nto make a general adapted 2D detector work in this 3D task. In this technical\nreport, we study this problem with a practice built on fully convolutional\nsingle-stage detector and propose a general framework FCOS3D. Specifically, we\nfirst transform the commonly defined 7-DoF 3D targets to image domain and\ndecouple it as 2D and 3D attributes. Then the objects are distributed to\ndifferent feature levels with the consideration of their 2D scales and assigned\nonly according to the projected 3D-center for training procedure. Furthermore,\nthe center-ness is redefined with a 2D Guassian distribution based on the\n3D-center to fit the 3D target formulation. All of these make this framework\nsimple yet effective, getting rid of any 2D detection or 2D-3D correspondence\npriors. Our solution achieves 1st place out of all the vision-only methods in\nthe nuScenes 3D detection challenge of NeurIPS 2020. Code and models are\nreleased at https://github.com/open-mmlab/mmdetection3d.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 09:35:35 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Wang", "Tai", ""], ["Zhu", "Xinge", ""], ["Pang", "Jiangmiao", ""], ["Lin", "Dahua", ""]]}, {"id": "2104.10959", "submitter": "St\\'ephanie Gu\\'erit", "authors": "St\\'ephanie Gu\\'erit, Siddharth Sivankutty, John Aldo Lee, Herv\\'e\n  Rigneault, Laurent Jacques", "title": "Compressive lensless endoscopy with partial speckle scanning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.optics cs.CV physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The lensless endoscope (LE) is a promising device to acquire in vivo images\nat a cellular scale. The tiny size of the probe enables a deep exploration of\nthe tissues. Lensless endoscopy with a multicore fiber (MCF) commonly uses a\nspatial light modulator (SLM) to coherently combine, at the output of the MCF,\nfew hundreds of beamlets into a focus spot. This spot is subsequently scanned\nacross the sample to generate a fluorescent image. We propose here a novel\nscanning scheme, partial speckle scanning (PSS), inspired by compressive\nsensing theory, that avoids the use of an SLM to perform fluorescent imaging in\nLE with reduced acquisition time. Such a strategy avoids photo-bleaching while\nkeeping high reconstruction quality. We develop our approach on two key\nproperties of the LE: (i) the ability to easily generate speckles, and (ii) the\nmemory effect in MCF that allows to use fast scan mirrors to shift light\npatterns. First, we show that speckles are sub-exponential random fields.\nDespite their granular structure, an appropriate choice of the reconstruction\nparameters makes them good candidates to build efficient sensing matrices.\nThen, we numerically validate our approach and apply it on experimental data.\nThe proposed sensing technique outperforms conventional raster scanning: higher\nreconstruction quality is achieved with far fewer observations. For a fixed\nreconstruction quality, our speckle scanning approach is faster than\ncompressive sensing schemes which require to change the speckle pattern for\neach observation.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 09:40:28 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Gu\u00e9rit", "St\u00e9phanie", ""], ["Sivankutty", "Siddharth", ""], ["Lee", "John Aldo", ""], ["Rigneault", "Herv\u00e9", ""], ["Jacques", "Laurent", ""]]}, {"id": "2104.10972", "submitter": "Tal Ridnik", "authors": "Tal Ridnik, Emanuel Ben-Baruch, Asaf Noy, Lihi Zelnik-Manor", "title": "ImageNet-21K Pretraining for the Masses", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  ImageNet-1K serves as the primary dataset for pretraining deep learning\nmodels for computer vision tasks. ImageNet-21K dataset, which is bigger and\nmore diverse, is used less frequently for pretraining, mainly due to its\ncomplexity, low accessibility, and underestimation of its added value. This\npaper aims to close this gap, and make high-quality efficient pretraining on\nImageNet-21K available for everyone. Via a dedicated preprocessing stage,\nutilization of WordNet hierarchical structure, and a novel training scheme\ncalled semantic softmax, we show that various models significantly benefit from\nImageNet-21K pretraining on numerous datasets and tasks, including small\nmobile-oriented models. We also show that we outperform previous ImageNet-21K\npretraining schemes for prominent new models like ViT and Mixer. Our proposed\npretraining pipeline is efficient, accessible, and leads to SoTA reproducible\nresults, from a publicly available dataset. The training code and pretrained\nmodels are available at: https://github.com/Alibaba-MIIL/ImageNet21K\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 10:10:14 GMT"}, {"version": "v2", "created": "Tue, 4 May 2021 16:13:28 GMT"}, {"version": "v3", "created": "Sun, 6 Jun 2021 08:29:51 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Ridnik", "Tal", ""], ["Ben-Baruch", "Emanuel", ""], ["Noy", "Asaf", ""], ["Zelnik-Manor", "Lihi", ""]]}, {"id": "2104.10984", "submitter": "C\\'edric Marco-Detchart", "authors": "Cedric Marco-Detchart, Giancarlo Lucca, Carlos Lopez-Molina, Laura De\n  Miguel, Gra\\c{c}aliz Pereira Dimuro, Humberto Bustince", "title": "Neuro-inspired edge feature fusion using Choquet integrals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is known that the human visual system performs a hierarchical information\nprocess in which early vision cues (or primitives) are fused in the visual\ncortex to compose complex shapes and descriptors. While different aspects of\nthe process have been extensively studied, as the lens adaptation or the\nfeature detection, some other,as the feature fusion, have been mostly left\naside. In this work we elaborate on the fusion of early vision primitives using\ngeneralizations of the Choquet integral, and novel aggregation operators that\nhave been extensively studied in recent years. We propose to use\ngeneralizations of the Choquet integral to sensibly fuse elementary edge cues,\nin an attempt to model the behaviour of neurons in the early visual cortex. Our\nproposal leads to a full-framed edge detection algorithm, whose performance is\nput to the test in state-of-the-art boundary detection datasets.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 10:45:52 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Marco-Detchart", "Cedric", ""], ["Lucca", "Giancarlo", ""], ["Lopez-Molina", "Carlos", ""], ["De Miguel", "Laura", ""], ["Dimuro", "Gra\u00e7aliz Pereira", ""], ["Bustince", "Humberto", ""]]}, {"id": "2104.10985", "submitter": "Senthil Yogamani", "authors": "Hazem Rashed, Ahmad El Sallab and Senthil Yogamani", "title": "VM-MODNet: Vehicle Motion aware Moving Object Detection for Autonomous\n  Driving", "comments": "Accepted for Oral Presentation at IEEE Intelligent Transportation\n  Systems Conference (ITSC) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Moving object Detection (MOD) is a critical task in autonomous driving as\nmoving agents around the ego-vehicle need to be accurately detected for safe\ntrajectory planning. It also enables appearance agnostic detection of objects\nbased on motion cues. There are geometric challenges like motion-parallax\nambiguity which makes it a difficult problem. In this work, we aim to leverage\nthe vehicle motion information and feed it into the model to have an adaptation\nmechanism based on ego-motion. The motivation is to enable the model to\nimplicitly perform ego-motion compensation to improve performance. We convert\nthe six degrees of freedom vehicle motion into a pixel-wise tensor which can be\nfed as input to the CNN model. The proposed model using Vehicle Motion Tensor\n(VMT) achieves an absolute improvement of 5.6% in mIoU over the baseline\narchitecture. We also achieve state-of-the-art results on the public\nKITTI_MoSeg_Extended dataset even compared to methods which make use of LiDAR\nand additional input frames. Our model is also lightweight and runs at 85 fps\non a TitanX GPU. Qualitative results are provided in\nhttps://youtu.be/ezbfjti-kTk.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 10:46:55 GMT"}, {"version": "v2", "created": "Sun, 11 Jul 2021 00:24:49 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Rashed", "Hazem", ""], ["Sallab", "Ahmad El", ""], ["Yogamani", "Senthil", ""]]}, {"id": "2104.10993", "submitter": "Johannes C. Paetzold", "authors": "Izabela Horvath, Johannes C. Paetzold, Oliver Schoppe, Rami\n  Al-Maskari, Ivan Ezhov, Suprosanna Shit, Hongwei Li, Ali Ertuerk, Bjoern H.\n  Menze", "title": "METGAN: Generative Tumour Inpainting and Modality Synthesis in Light\n  Sheet Microscopy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Novel multimodal imaging methods are capable of generating extensive, super\nhigh resolution datasets for preclinical research. Yet, a massive lack of\nannotations prevents the broad use of deep learning to analyze such data. So\nfar, existing generative models fail to mitigate this problem because of\nfrequent labeling errors. In this paper, we introduce a novel generative method\nwhich leverages real anatomical information to generate realistic image-label\npairs of tumours. We construct a dual-pathway generator, for the anatomical\nimage and label, trained in a cycle-consistent setup, constrained by an\nindependent, pretrained segmentor. The generated images yield significant\nquantitative improvement compared to existing methods. To validate the quality\nof synthesis, we train segmentation networks on a dataset augmented with the\nsynthetic data, substantially improving the segmentation over baseline.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 11:18:17 GMT"}, {"version": "v2", "created": "Fri, 23 Apr 2021 10:50:07 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Horvath", "Izabela", ""], ["Paetzold", "Johannes C.", ""], ["Schoppe", "Oliver", ""], ["Al-Maskari", "Rami", ""], ["Ezhov", "Ivan", ""], ["Shit", "Suprosanna", ""], ["Li", "Hongwei", ""], ["Ertuerk", "Ali", ""], ["Menze", "Bjoern H.", ""]]}, {"id": "2104.11004", "submitter": "Jian Pang", "authors": "Jian Pang, Dacheng Zhang, Huafeng Li, Weifeng Liu, Zhengtao Yu", "title": "Hazy Re-ID: An Interference Suppression Model For Domain Adaptation\n  Person Re-identification Under Inclement Weather Condition", "comments": "Accepted by ICME2021 as oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In a conventional domain adaptation person Re-identification (Re-ID) task,\nboth the training and test images in target domain are collected under the\nsunny weather. However, in reality, the pedestrians to be retrieved may be\nobtained under severe weather conditions such as hazy, dusty and snowing, etc.\nThis paper proposes a novel Interference Suppression Model (ISM) to deal with\nthe interference caused by the hazy weather in domain adaptation person Re-ID.\nA teacherstudent model is used in the ISM to distill the interference\ninformation at the feature level by reducing the discrepancy between the clear\nand the hazy intrinsic similarity matrix. Furthermore, in the distribution\nlevel, the extra discriminator is introduced to assist the student model make\nthe interference feature distribution more clear. The experimental results show\nthat the proposed method achieves the superior performance on two synthetic\ndatasets than the stateof-the-art methods. The related code will be released\nonline https://github.com/pangjian123/ISM-ReID.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 11:59:27 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Pang", "Jian", ""], ["Zhang", "Dacheng", ""], ["Li", "Huafeng", ""], ["Liu", "Weifeng", ""], ["Yu", "Zhengtao", ""]]}, {"id": "2104.11008", "submitter": "Dinesh Jackson Samuel", "authors": "Dinesh Jackson Samuel and Fabio Cuzzolin", "title": "Unsupervised anomaly detection for a Smart Autonomous Robotic Assistant\n  Surgeon (SARAS)using a deep residual autoencoder", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Anomaly detection in Minimally-Invasive Surgery (MIS) traditionally requires\na human expert monitoring the procedure from a console. Data scarcity, on the\nother hand, hinders what would be a desirable migration towards autonomous\nrobotic-assisted surgical systems. Automated anomaly detection systems in this\narea typically rely on classical supervised learning. Anomalous events in a\nsurgical setting, however, are rare, making it difficult to capture data to\ntrain a detection model in a supervised fashion. In this work we thus propose\nan unsupervised approach to anomaly detection for robotic-assisted surgery\nbased on deep residual autoencoders. The idea is to make the autoencoder learn\nthe 'normal' distribution of the data and detect abnormal events deviating from\nthis distribution by measuring the reconstruction error. The model is trained\nand validated upon both the publicly available Cholec80 dataset, provided with\nextra annotation, and on a set of videos captured on procedures using\nartificial anatomies ('phantoms') produced as part of the Smart Autonomous\nRobotic Assistant Surgeon (SARAS) project. The system achieves recall and\nprecision equal to 78.4%, 91.5%, respectively, on Cholec80 and of 95.6%, 88.1%\non the SARAS phantom dataset. The end-to-end system was developed and deployed\nas part of the SARAS demonstration platform for real-time anomaly detection\nwith a processing time of about 25 ms per frame.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 12:10:38 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Samuel", "Dinesh Jackson", ""], ["Cuzzolin", "Fabio", ""]]}, {"id": "2104.11014", "submitter": "Min-Hung Chen", "authors": "Min-Fong Hong, Hao-Yun Chen, Min-Hung Chen, Yu-Syuan Xu, Hsien-Kai\n  Kuo, Yi-Min Tsai, Hung-Jen Chen, Kevin Jou", "title": "Network Space Search for Pareto-Efficient Spaces", "comments": "CVPRW2021 [Oral] (Efficient Deep Learning for Computer Vision\n  Workshop). Website: https://minhungchen.netlify.app/publication/nss", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Network spaces have been known as a critical factor in both handcrafted\nnetwork designs or defining search spaces for Neural Architecture Search (NAS).\nHowever, an effective space involves tremendous prior knowledge and/or manual\neffort, and additional constraints are required to discover efficiency-aware\narchitectures. In this paper, we define a new problem, Network Space Search\n(NSS), as searching for favorable network spaces instead of a single\narchitecture. We propose an NSS method to directly search for efficient-aware\nnetwork spaces automatically, reducing the manual effort and immense cost in\ndiscovering satisfactory ones. The resultant network spaces, named Elite\nSpaces, are discovered from Expanded Search Space with minimal human expertise\nimposed. The Pareto-efficient Elite Spaces are aligned with the Pareto front\nunder various complexity constraints and can be further served as NAS search\nspaces, benefiting differentiable NAS approaches (e.g. In CIFAR-100, an\naveragely 2.3% lower error rate and 3.7% closer to target constraint than the\nbaseline with around 90% fewer samples required to find satisfactory networks).\nMoreover, our NSS approach is capable of searching for superior spaces in\nfuture unexplored spaces, revealing great potential in searching for network\nspaces automatically. Website: https://minhungchen.netlify.app/publication/nss.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 12:23:53 GMT"}, {"version": "v2", "created": "Thu, 10 Jun 2021 03:31:32 GMT"}, {"version": "v3", "created": "Sat, 12 Jun 2021 03:39:42 GMT"}, {"version": "v4", "created": "Tue, 15 Jun 2021 07:36:27 GMT"}, {"version": "v5", "created": "Wed, 16 Jun 2021 00:22:11 GMT"}, {"version": "v6", "created": "Sun, 20 Jun 2021 01:51:58 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Hong", "Min-Fong", ""], ["Chen", "Hao-Yun", ""], ["Chen", "Min-Hung", ""], ["Xu", "Yu-Syuan", ""], ["Kuo", "Hsien-Kai", ""], ["Tsai", "Yi-Min", ""], ["Chen", "Hung-Jen", ""], ["Jou", "Kevin", ""]]}, {"id": "2104.11017", "submitter": "Jingnan Jia", "authors": "Jingnan Jia, Zhiwei Zhai, M. Els Bakker, I. Hernandez Giron, Marius\n  Staring, Berend C. Stoel", "title": "Multi-task Semi-supervised Learning for Pulmonary Lobe Segmentation", "comments": "4 pages, to be published in ISBI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Pulmonary lobe segmentation is an important preprocessing task for the\nanalysis of lung diseases. Traditional methods relying on fissure detection or\nother anatomical features, such as the distribution of pulmonary vessels and\nairways, could provide reasonably accurate lobe segmentations. Deep learning\nbased methods can outperform these traditional approaches, but require large\ndatasets. Deep multi-task learning is expected to utilize labels of multiple\ndifferent structures. However, commonly such labels are distributed over\nmultiple datasets. In this paper, we proposed a multi-task semi-supervised\nmodel that can leverage information of multiple structures from unannotated\ndatasets and datasets annotated with different structures. A focused\nalternating training strategy is presented to balance the different tasks. We\nevaluated the trained model on an external independent CT dataset. The results\nshow that our model significantly outperforms single-task alternatives,\nimproving the mean surface distance from 7.174 mm to 4.196 mm. We also\ndemonstrated that our approach is successful for different network\narchitectures as backbones.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 12:33:30 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Jia", "Jingnan", ""], ["Zhai", "Zhiwei", ""], ["Bakker", "M. Els", ""], ["Giron", "I. Hernandez", ""], ["Staring", "Marius", ""], ["Stoel", "Berend C.", ""]]}, {"id": "2104.11020", "submitter": "Minh Vu", "authors": "Minh H. Vu and Gabriella Norman and Tufve Nyholm and Tommy L\\\"ofstedt", "title": "A Data-Adaptive Loss Function for Incomplete Data and Incremental\n  Learning in Semantic Image Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the last years, deep learning has dramatically improved the performances\nin a variety of medical image analysis applications. Among different types of\ndeep learning models, convolutional neural networks have been among the most\nsuccessful and they have been used in many applications in medical imaging.\n  Training deep convolutional neural networks often requires large amounts of\nimage data to generalize well to new unseen images. It is often time-consuming\nand expensive to collect large amounts of data in the medical image domain due\nto expensive imaging systems, and the need for experts to manually make ground\ntruth annotations. A potential problem arises if new structures are added when\na decision support system is already deployed and in use. Since the field of\nradiation therapy is constantly developing, the new structures would also have\nto be covered by the decision support system.\n  In the present work, we propose a novel loss function, that adapts to the\navailable data in order to utilize all available data, even when some have\nmissing annotations. We demonstrate that the proposed loss function also works\nwell in an incremental learning setting, where it can automatically incorporate\nnew structures as they appear. Experiments on a large in-house data set show\nthat the proposed method performs on par with baseline models, while greatly\nreducing the training time.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 12:46:50 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Vu", "Minh H.", ""], ["Norman", "Gabriella", ""], ["Nyholm", "Tufve", ""], ["L\u00f6fstedt", "Tommy", ""]]}, {"id": "2104.11021", "submitter": "Alejandro Barrera", "authors": "Alejandro Barrera, Jorge Beltr\\'an, Carlos Guindel, Jose Antonio\n  Iglesias, Fernando Garc\\'ia", "title": "Cycle and Semantic Consistent Adversarial Domain Adaptation for Reducing\n  Simulation-to-Real Domain Shift in LiDAR Bird's Eye View", "comments": "Submitted to IEEE International Conference on Intelligent\n  Transportation Systems (ITSC2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance of object detection methods based on LiDAR information is\nheavily impacted by the availability of training data, usually limited to\ncertain laser devices. As a result, the use of synthetic data is becoming\npopular when training neural network models, as both sensor specifications and\ndriving scenarios can be generated ad-hoc. However, bridging the gap between\nvirtual and real environments is still an open challenge, as current simulators\ncannot completely mimic real LiDAR operation. To tackle this issue, domain\nadaptation strategies are usually applied, obtaining remarkable results on\nvehicle detection when applied to range view (RV) and bird's eye view (BEV)\nprojections while failing for smaller road agents. In this paper, we present a\nBEV domain adaptation method based on CycleGAN that uses prior semantic\nclassification in order to preserve the information of small objects of\ninterest during the domain adaptation process. The quality of the generated\nBEVs has been evaluated using a state-of-the-art 3D object detection framework\nat KITTI 3D Object Detection Benchmark. The obtained results show the\nadvantages of the proposed method over the existing alternatives.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 12:47:37 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Barrera", "Alejandro", ""], ["Beltr\u00e1n", "Jorge", ""], ["Guindel", "Carlos", ""], ["Iglesias", "Jose Antonio", ""], ["Garc\u00eda", "Fernando", ""]]}, {"id": "2104.11028", "submitter": "Max Coenen", "authors": "Max Coenen, Tobias Schack, Dries Beyer, Christian Heipke, Michael\n  Haist", "title": "Semi-Supervised Segmentation of Concrete Aggregate Using Consensus\n  Regularisation and Prior Guidance", "comments": null, "journal-ref": null, "doi": "10.5194/isprs-annals-V-2-2021-83-2021", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to leverage and profit from unlabelled data, semi-supervised\nframeworks for semantic segmentation based on consistency training have been\nproven to be powerful tools to significantly improve the performance of purely\nsupervised segmentation learning. However, the consensus principle behind\nconsistency training has at least one drawback, which we identify in this\npaper: imbalanced label distributions within the data. To overcome the\nlimitations of standard consistency training, we propose a novel\nsemi-supervised framework for semantic segmentation, introducing additional\nlosses based on prior knowledge. Specifically, we propose a light-weight\narchitecture consisting of a shared encoder and a main decoder, which is\ntrained in a supervised manner. An auxiliary decoder is added as additional\nbranch in order to make use of unlabelled data based on consensus training, and\nwe add additional constraints derived from prior information on the class\ndistribution and on auto-encoder regularisation. Experiments performed on our\n\"concrete aggregate dataset\" presented in this paper demonstrate the\neffectiveness of the proposed approach, outperforming the segmentation results\nachieved by purely supervised segmentation and standard consistency training.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 13:01:28 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Coenen", "Max", ""], ["Schack", "Tobias", ""], ["Beyer", "Dries", ""], ["Heipke", "Christian", ""], ["Haist", "Michael", ""]]}, {"id": "2104.11056", "submitter": "Weizhe Liu", "authors": "Weizhe Liu, David Ferstl, Samuel Schulter, Lukas Zebedin, Pascal Fua,\n  Christian Leistner", "title": "Domain Adaptation for Semantic Segmentation via Patch-Wise Contrastive\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel approach to unsupervised and semi-supervised domain\nadaptation for semantic segmentation. Unlike many earlier methods that rely on\nadversarial learning for feature alignment, we leverage contrastive learning to\nbridge the domain gap by aligning the features of structurally similar label\npatches across domains. As a result, the networks are easier to train and\ndeliver better performance. Our approach consistently outperforms\nstate-of-the-art unsupervised and semi-supervised methods on two challenging\ndomain adaptive segmentation tasks, particularly with a small number of target\ndomain annotations. It can also be naturally extended to weakly-supervised\ndomain adaptation, where only a minor drop in accuracy can save up to 75% of\nannotation cost.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 13:39:12 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Liu", "Weizhe", ""], ["Ferstl", "David", ""], ["Schulter", "Samuel", ""], ["Zebedin", "Lukas", ""], ["Fua", "Pascal", ""], ["Leistner", "Christian", ""]]}, {"id": "2104.11057", "submitter": "Lie Ju", "authors": "Lie Ju, Xin Wang, Lin Wang, Tongliang Liu, Xin Zhao, Tom Drummond,\n  Dwarikanath Mahapatra, Zongyuan Ge", "title": "Relational Subsets Knowledge Distillation for Long-tailed Retinal\n  Diseases Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the real world, medical datasets often exhibit a long-tailed data\ndistribution (i.e., a few classes occupy most of the data, while most classes\nhave rarely few samples), which results in a challenging imbalance learning\nscenario. For example, there are estimated more than 40 different kinds of\nretinal diseases with variable morbidity, however with more than 30+ conditions\nare very rare from the global patient cohorts, which results in a typical\nlong-tailed learning problem for deep learning-based screening models. In this\nstudy, we propose class subset learning by dividing the long-tailed data into\nmultiple class subsets according to prior knowledge, such as regions and\nphenotype information. It enforces the model to focus on learning the\nsubset-specific knowledge. More specifically, there are some relational classes\nthat reside in the fixed retinal regions, or some common pathological features\nare observed in both the majority and minority conditions. With those subsets\nlearnt teacher models, then we are able to distill the multiple teacher models\ninto a unified model with weighted knowledge distillation loss. The proposed\nframework proved to be effective for the long-tailed retinal diseases\nrecognition task. The experimental results on two different datasets\ndemonstrate that our method is flexible and can be easily plugged into many\nother state-of-the-art techniques with significant improvements.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 13:39:33 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Ju", "Lie", ""], ["Wang", "Xin", ""], ["Wang", "Lin", ""], ["Liu", "Tongliang", ""], ["Zhao", "Xin", ""], ["Drummond", "Tom", ""], ["Mahapatra", "Dwarikanath", ""], ["Ge", "Zongyuan", ""]]}, {"id": "2104.11100", "submitter": "Bin Jiang", "authors": "Bin Jiang and Chris de Rijke", "title": "Structural Beauty: A Structure-based Approach to Quantifying the Beauty\n  of an Image", "comments": "15 pages, 8 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  To say that beauty is in the eye of the beholder means that beauty is largely\nsubjective so varies from person to person. While the subjectivity view is\ncommonly held, there is also an objectivity view that seeks to measure beauty\nor aesthetics in some quantitative manners. Christopher Alexander has long\ndiscovered that beauty or coherence highly correlates to the number of\nsubsymmetries or substructures and demonstrated that there is a shared notion\nof beauty - structural beauty - among people and even different peoples,\nregardless of their faiths, cultures, and ethnicities. This notion of\nstructural beauty arises directly out of living structure or wholeness, a\nphysical and mathematical structure that underlies all space and matter. Based\non the concept of living structure, this paper develops an approach for\ncomputing the structural beauty or life of an image (L) based on the number of\nautomatically derived substructures (S) and their inherent hierarchy (H). To\nverify this approach, we conducted a series of case studies applied to eight\npairs of images including Leonardo da Vinci's Mona Lisa and Jackson Pollock's\nBlue Poles. We discovered among others that Blue Poles is more structurally\nbeautiful than the Mona Lisa, and traditional buildings are in general more\nstructurally beautiful than their modernist counterparts. This finding implies\nthat goodness of things or images is largely a matter of fact rather than an\nopinion or personal preference as conventionally conceived. The research on\nstructural beauty has deep implications on many disciplines, where beauty or\naesthetics is a major concern such as image understanding and computer vision,\narchitecture and urban design, humanities and arts, neurophysiology, and\npsychology.\n  Keywords: Life; wholeness; figural goodness; head/tail breaks; computer\nvision\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 08:48:34 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Jiang", "Bin", ""], ["de Rijke", "Chris", ""]]}, {"id": "2104.11101", "submitter": "Yi Wang", "authors": "Arman Maesumi and Mingkang Zhu and Yi Wang and Tianlong Chen and\n  Zhangyang Wang and Chandrajit Bajaj", "title": "Learning Transferable 3D Adversarial Cloaks for Deep Trained Detectors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper presents a novel patch-based adversarial attack pipeline that\ntrains adversarial patches on 3D human meshes. We sample triangular faces on a\nreference human mesh, and create an adversarial texture atlas over those faces.\nThe adversarial texture is transferred to human meshes in various poses, which\nare rendered onto a collection of real-world background images. Contrary to the\ntraditional patch-based adversarial attacks, where prior work attempts to fool\ntrained object detectors using appended adversarial patches, this new form of\nattack is mapped into the 3D object world and back-propagated to the texture\natlas through differentiable rendering. As such, the adversarial patch is\ntrained under deformation consistent with real-world materials. In addition,\nand unlike existing adversarial patches, our new 3D adversarial patch is shown\nto fool state-of-the-art deep object detectors robustly under varying views,\npotentially leading to an attacking scheme that is persistently strong in the\nphysical world.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 14:36:08 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Maesumi", "Arman", ""], ["Zhu", "Mingkang", ""], ["Wang", "Yi", ""], ["Chen", "Tianlong", ""], ["Wang", "Zhangyang", ""], ["Bajaj", "Chandrajit", ""]]}, {"id": "2104.11116", "submitter": "Hang Zhou", "authors": "Hang Zhou, Yasheng Sun, Wayne Wu, Chen Change Loy, Xiaogang Wang,\n  Ziwei Liu", "title": "Pose-Controllable Talking Face Generation by Implicitly Modularized\n  Audio-Visual Representation", "comments": "Accepted to IEEE/CVF Conference on Computer Vision and Pattern\n  Recognition (CVPR), 2021. Code and models are available at\n  https://github.com/Hangz-nju-cuhk/Talking-Face_PC-AVS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM cs.SD eess.AS eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  While accurate lip synchronization has been achieved for arbitrary-subject\naudio-driven talking face generation, the problem of how to efficiently drive\nthe head pose remains. Previous methods rely on pre-estimated structural\ninformation such as landmarks and 3D parameters, aiming to generate\npersonalized rhythmic movements. However, the inaccuracy of such estimated\ninformation under extreme conditions would lead to degradation problems. In\nthis paper, we propose a clean yet effective framework to generate\npose-controllable talking faces. We operate on raw face images, using only a\nsingle photo as an identity reference. The key is to modularize audio-visual\nrepresentations by devising an implicit low-dimension pose code. Substantially,\nboth speech content and head pose information lie in a joint non-identity\nembedding space. While speech content information can be defined by learning\nthe intrinsic synchronization between audio-visual modalities, we identify that\na pose code will be complementarily learned in a modulated convolution-based\nreconstruction framework.\n  Extensive experiments show that our method generates accurately lip-synced\ntalking faces whose poses are controllable by other videos. Moreover, our model\nhas multiple advanced capabilities including extreme view robustness and\ntalking face frontalization. Code, models, and demo videos are available at\nhttps://hangz-nju-cuhk.github.io/projects/PC-AVS.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 15:10:26 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Zhou", "Hang", ""], ["Sun", "Yasheng", ""], ["Wu", "Wayne", ""], ["Loy", "Chen Change", ""], ["Wang", "Xiaogang", ""], ["Liu", "Ziwei", ""]]}, {"id": "2104.11130", "submitter": "Jose M. Saavedra PhD", "authors": "Anibal Fuentes and Jose M. Saavedra", "title": "Sketch-QNet: A Quadruplet ConvNet for Color Sketch-based Image Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Architectures based on siamese networks with triplet loss have shown\noutstanding performance on the image-based similarity search problem. This\napproach attempts to discriminate between positive (relevant) and negative\n(irrelevant) items. However, it undergoes a critical weakness. Given a query,\nit cannot discriminate weakly relevant items, for instance, items of the same\ntype but different color or texture as the given query, which could be a\nserious limitation for many real-world search applications. Therefore, in this\nwork, we present a quadruplet-based architecture that overcomes the\naforementioned weakness. Moreover, we present an instance of this quadruplet\nnetwork, which we call Sketch-QNet, to deal with the color sketch-based image\nretrieval (CSBIR) problem, achieving new state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 15:28:39 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Fuentes", "Anibal", ""], ["Saavedra", "Jose M.", ""]]}, {"id": "2104.11138", "submitter": "Debesh Jha", "authors": "Debesh Jha, Nikhil Kumar Tomar, Sharib Ali, Michael A. Riegler,\n  H{\\aa}vard D. Johansen, Dag Johansen, Thomas de Lange, P{\\aa}l Halvorsen", "title": "NanoNet: Real-Time Polyp Segmentation in Video Capsule Endoscopy and\n  Colonoscopy", "comments": "Accepted at CBMS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep learning in gastrointestinal endoscopy can assist to improve clinical\nperformance and be helpful to assess lesions more accurately. To this extent,\nsemantic segmentation methods that can perform automated real-time delineation\nof a region-of-interest, e.g., boundary identification of cancer or\nprecancerous lesions, can benefit both diagnosis and interventions. However,\naccurate and real-time segmentation of endoscopic images is extremely\nchallenging due to its high operator dependence and high-definition image\nquality. To utilize automated methods in clinical settings, it is crucial to\ndesign lightweight models with low latency such that they can be integrated\nwith low-end endoscope hardware devices. In this work, we propose NanoNet, a\nnovel architecture for the segmentation of video capsule endoscopy and\ncolonoscopy images. Our proposed architecture allows real-time performance and\nhas higher segmentation accuracy compared to other more complex ones. We use\nvideo capsule endoscopy and standard colonoscopy datasets with polyps, and a\ndataset consisting of endoscopy biopsies and surgical instruments, to evaluate\nthe effectiveness of our approach. Our experiments demonstrate the increased\nperformance of our architecture in terms of a trade-off between model\ncomplexity, speed, model parameters, and metric performances. Moreover, the\nresulting model size is relatively tiny, with only nearly 36,000 parameters\ncompared to traditional deep learning approaches having millions of parameters.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 15:40:28 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Jha", "Debesh", ""], ["Tomar", "Nikhil Kumar", ""], ["Ali", "Sharib", ""], ["Riegler", "Michael A.", ""], ["Johansen", "H\u00e5vard D.", ""], ["Johansen", "Dag", ""], ["de Lange", "Thomas", ""], ["Halvorsen", "P\u00e5l", ""]]}, {"id": "2104.11159", "submitter": "Matan Rusanovsky", "authors": "Matan Rusanovsky, Ofer Beeri, Sigalit Ifergane and Gal Oren", "title": "An End-to-End Computer Vision Methodology for Quantitative Metallography", "comments": "arXiv admin note: substantial text overlap with arXiv:2003.04226", "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.mtrl-sci cs.CV cs.LG physics.data-an", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Metallography is crucial for a proper assessment of material's properties. It\ninvolves mainly the investigation of spatial distribution of grains and the\noccurrence and characteristics of inclusions or precipitates. This work\npresents an holistic artificial intelligence model for Anomaly Detection that\nautomatically quantifies the degree of anomaly of impurities in alloys. We\nsuggest the following examination process: (1) Deep semantic segmentation is\nperformed on the inclusions (based on a suitable metallographic database of\nalloys and corresponding tags of inclusions), producing inclusions masks that\nare saved into a separated database. (2) Deep image inpainting is performed to\nfill the removed inclusions parts, resulting in 'clean' metallographic images,\nwhich contain the background of grains. (3) Grains' boundaries are marked using\ndeep semantic segmentation (based on another metallographic database of\nalloys), producing boundaries that are ready for further inspection on the\ndistribution of grains' size. (4) Deep anomaly detection and pattern\nrecognition is performed on the inclusions masks to determine spatial, shape\nand area anomaly detection of the inclusions. Finally, the system recommends to\nan expert on areas of interests for further examination. The performance of the\nmodel is presented and analyzed based on few representative cases. Although the\nmodels presented here were developed for metallography analysis, most of them\ncan be generalized to a wider set of problems in which anomaly detection of\ngeometrical objects is desired. All models as well as the data-sets that were\ncreated for this work, are publicly available at\nhttps://github.com/Scientific-Computing-Lab-NRCN/MLography.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 16:29:44 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Rusanovsky", "Matan", ""], ["Beeri", "Ofer", ""], ["Ifergane", "Sigalit", ""], ["Oren", "Gal", ""]]}, {"id": "2104.11165", "submitter": "Zahra Gharaee", "authors": "Zahra Gharaee", "title": "Hierarchical growing grid networks for skeleton based action recognition", "comments": null, "journal-ref": "Cognitive Systems Research, vol.63, pp.11-29 (2020)", "doi": "10.1016/j.cogsys.2020.05.002", "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.RO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, a novel cognitive architecture for action recognition is\ndeveloped by applying layers of growing grid neural networks.Using these layers\nmakes the system capable of automatically arranging its representational\nstructure. In addition to the expansion of the neural map during the growth\nphase, the system is provided with a prior knowledge of the input space, which\nincreases the processing speed of the learning phase. Apart from two layers of\ngrowing grid networks the architecture is composed of a preprocessing layer, an\nordered vector representation layer and a one-layer supervised neural network.\nThese layers are designed to solve the action recognition problem. The\nfirst-layer growing grid receives the input data of human actions and the\nneural map generates an action pattern vector representing each action sequence\nby connecting the elicited activation of the trained map. The pattern vectors\nare then sent to the ordered vector representation layer to build the\ntime-invariant input vectors of key activations for the second-layer growing\ngrid. The second-layer growing grid categorizes the input vectors to the\ncorresponding action clusters/sub-clusters and finally the one-layer supervised\nneural network labels the shaped clusters with action labels. Three experiments\nusing different datasets of actions show that the system is capable of learning\nto categorize the actions quickly and efficiently. The performance of the\ngrowing grid architecture is com-pared with the results from a system based on\nSelf-Organizing Maps, showing that the growing grid architecture performs\nsignificantly superior on the action recognition tasks.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 16:35:32 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Gharaee", "Zahra", ""]]}, {"id": "2104.11176", "submitter": "Ryuhei Hamaguchi", "authors": "Ryuhei Hamaguchi, Yasutaka Furukawa, Masaki Onishi, Ken Sakurada", "title": "Heterogeneous Grid Convolution for Adaptive, Efficient, and Controllable\n  Computation", "comments": "Accepted to CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel heterogeneous grid convolution that builds a\ngraph-based image representation by exploiting heterogeneity in the image\ncontent, enabling adaptive, efficient, and controllable computations in a\nconvolutional architecture. More concretely, the approach builds a\ndata-adaptive graph structure from a convolutional layer by a differentiable\nclustering method, pools features to the graph, performs a novel\ndirection-aware graph convolution, and unpool features back to the\nconvolutional layer. By using the developed module, the paper proposes\nheterogeneous grid convolutional networks, highly efficient yet strong\nextension of existing architectures. We have evaluated the proposed approach on\nfour image understanding tasks, semantic segmentation, object localization,\nroad extraction, and salient object detection. The proposed method is effective\non three of the four tasks. Especially, the method outperforms a strong\nbaseline with more than 90% reduction in floating-point operations for semantic\nsegmentation, and achieves the state-of-the-art result for road extraction. We\nwill share our code, model, and data.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 17:02:59 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Hamaguchi", "Ryuhei", ""], ["Furukawa", "Yasutaka", ""], ["Onishi", "Masaki", ""], ["Sakurada", "Ken", ""]]}, {"id": "2104.11178", "submitter": "Hassan Akbari", "authors": "Hassan Akbari, Linagzhe Yuan, Rui Qian, Wei-Hong Chuang, Shih-Fu\n  Chang, Yin Cui, Boqing Gong", "title": "VATT: Transformers for Multimodal Self-Supervised Learning from Raw\n  Video, Audio and Text", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a framework for learning multimodal representations from unlabeled\ndata using convolution-free Transformer architectures. Specifically, our\nVideo-Audio-Text Transformer (VATT) takes raw signals as inputs and extracts\nmultimodal representations that are rich enough to benefit a variety of\ndownstream tasks. We train VATT end-to-end from scratch using multimodal\ncontrastive losses and evaluate its performance by the downstream tasks of\nvideo action recognition, audio event classification, image classification, and\ntext-to-video retrieval. Furthermore, we study a modality-agnostic\nsingle-backbone Transformer by sharing weights among the three modalities. We\nshow that the convolution-free VATT outperforms state-of-the-art ConvNet-based\narchitectures in the downstream tasks. Especially, VATT's vision Transformer\nachieves the top-1 accuracy of 82.1% on Kinetics-400, 83.6% on Kinetics-600,and\n41.1% on Moments in Time, new records while avoiding supervised pre-training.\nTransferring to image classification leads to 78.7% top-1 accuracy on ImageNet\ncompared to 64.7% by training the same Transformer from scratch, showing the\ngeneralizability of our model despite the domain gap between videos and images.\nVATT's audio Transformer also sets a new record on waveform-based audio event\nrecognition by achieving the mAP of 39.4% on AudioSet without any supervised\npre-training.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 17:07:41 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Akbari", "Hassan", ""], ["Yuan", "Linagzhe", ""], ["Qian", "Rui", ""], ["Chuang", "Wei-Hong", ""], ["Chang", "Shih-Fu", ""], ["Cui", "Yin", ""], ["Gong", "Boqing", ""]]}, {"id": "2104.11180", "submitter": "Mohamed Hasan Dr", "authors": "Mohamed Hasan, Evangelos Paschalidis, Albert Solernou, He Wang, Gustav\n  Markkula and Richard Romano", "title": "Maneuver-based Anchor Trajectory Hypotheses at Roundabouts", "comments": "Under Review IROS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting future behavior of the surrounding vehicles is crucial for\nself-driving platforms to safely navigate through other traffic. This is\ncritical when making decisions like crossing an unsignalized intersection. We\naddress the problem of vehicle motion prediction in a challenging roundabout\nenvironment by learning from human driver data. We extend existing recurrent\nencoder-decoder models to be advantageously combined with anchor trajectories\nto predict vehicle behaviors on a roundabout. Drivers' intentions are encoded\nby a set of maneuvers that correspond to semantic driving concepts.\nAccordingly, our model employs a set of maneuver-specific anchor trajectories\nthat cover the space of possible outcomes at the roundabout. The proposed model\ncan output a multi-modal distribution over the predicted future trajectories\nbased on the maneuver-specific anchors. We evaluate our model using the public\nRounD dataset and the experiment results show the effectiveness of the proposed\nmaneuver-based anchor regression in improving prediction accuracy, reducing the\naverage RMSE to 28% less than the best baseline. Our code is available at\nhttps://github.com/m-hasan-n/roundabout.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 17:08:29 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Hasan", "Mohamed", ""], ["Paschalidis", "Evangelos", ""], ["Solernou", "Albert", ""], ["Wang", "He", ""], ["Markkula", "Gustav", ""], ["Romano", "Richard", ""]]}, {"id": "2104.11181", "submitter": "Taein Kwon", "authors": "Taein Kwon, Bugra Tekin, Jan Stuhmer, Federica Bogo, Marc Pollefeys", "title": "H2O: Two Hands Manipulating Objects for First Person Interaction\n  Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present, for the first time, a comprehensive framework for egocentric\ninteraction recognition using markerless 3D annotations of two hands\nmanipulating objects. To this end, we propose a method to create a unified\ndataset for egocentric 3D interaction recognition. Our method produces\nannotations of the 3D pose of two hands and the 6D pose of the manipulated\nobjects, along with their interaction labels for each frame. Our dataset,\ncalled H2O (2 Hands and Objects), provides synchronized multi-view RGB-D\nimages, interaction labels, object classes, ground-truth 3D poses for left &\nright hands, 6D object poses, ground-truth camera poses, object meshes and\nscene point clouds. To the best of our knowledge, this is the first benchmark\nthat enables the study of first-person actions with the use of the pose of both\nleft and right hands manipulating objects and presents an unprecedented level\nof detail for egocentric 3D interaction recognition. We further propose the\nfirst method to predict interaction classes by estimating the 3D pose of two\nhands and the 6D pose of the manipulated objects, jointly from RGB images. Our\nmethod models both inter- and intra-dependencies between both hands and objects\nby learning the topology of a graph convolutional network that predicts\ninteractions. We show that our method facilitated by this dataset establishes a\nstrong baseline for joint hand-object pose estimation and achieves\nstate-of-the-art accuracy for first person interaction recognition.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 17:10:42 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Kwon", "Taein", ""], ["Tekin", "Bugra", ""], ["Stuhmer", "Jan", ""], ["Bogo", "Federica", ""], ["Pollefeys", "Marc", ""]]}, {"id": "2104.11200", "submitter": "Yuansheng Hua", "authors": "Yuansheng Hua, Lichao Moua, Jianzhe Lin, Konrad Heidler, Xiao Xiang\n  Zhu", "title": "Aerial Scene Understanding in The Wild: Multi-Scene Recognition via\n  Prototype-based Memory Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aerial scene recognition is a fundamental visual task and has attracted an\nincreasing research interest in the last few years. Most of current researches\nmainly deploy efforts to categorize an aerial image into one scene-level label,\nwhile in real-world scenarios, there often exist multiple scenes in a single\nimage. Therefore, in this paper, we propose to take a step forward to a more\npractical and challenging task, namely multi-scene recognition in single\nimages. Moreover, we note that manually yielding annotations for such a task is\nextraordinarily time- and labor-consuming. To address this, we propose a\nprototype-based memory network to recognize multiple scenes in a single image\nby leveraging massive well-annotated single-scene images. The proposed network\nconsists of three key components: 1) a prototype learning module, 2) a\nprototype-inhabiting external memory, and 3) a multi-head attention-based\nmemory retrieval module. To be more specific, we first learn the prototype\nrepresentation of each aerial scene from single-scene aerial image datasets and\nstore it in an external memory. Afterwards, a multi-head attention-based memory\nretrieval module is devised to retrieve scene prototypes relevant to query\nmulti-scene images for final predictions. Notably, only a limited number of\nannotated multi-scene images are needed in the training phase. To facilitate\nthe progress of aerial scene recognition, we produce a new multi-scene aerial\nimage (MAI) dataset. Experimental results on variant dataset configurations\ndemonstrate the effectiveness of our network. Our dataset and codes are\npublicly available.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 17:32:14 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Hua", "Yuansheng", ""], ["Moua", "Lichao", ""], ["Lin", "Jianzhe", ""], ["Heidler", "Konrad", ""], ["Zhu", "Xiao Xiang", ""]]}, {"id": "2104.11207", "submitter": "Xili Dai", "authors": "Xili Dai, Xiaojun Yuan, Haigang Gong, Yi Ma", "title": "Fully Convolutional Line Parsing", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  We present a one-stage Fully Convolutional Line Parsing network (F-Clip) that\ndetects line segments from images. The proposed network is very simple and\nflexible with variations that gracefully trade off between speed and accuracy\nfor different applications. F-Clip detects line segments in an end-to-end\nfashion by predicting them with each line's center position, length, and angle.\nBased on empirical observation of the distribution of line angles in real image\ndatasets, we further customize the design of convolution kernels of our fully\nconvolutional network to effectively exploit such statistical priors. We\nconduct extensive experiments and show that our method achieves a significantly\nbetter trade-off between efficiency and accuracy, resulting in a real-time line\ndetector at up to 73 FPS on a single GPU. Such inference speed makes our method\nreadily applicable to real-time tasks without compromising any accuracy of\nprevious methods. Moreover, when equipped with a performance-improving backbone\nnetwork, F-Clip is able to significantly outperform all state-of-the-art line\ndetectors on accuracy at a similar or even higher frame rate. Source code\nhttps://github.com/Delay-Xili/F-Clip.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 17:41:12 GMT"}, {"version": "v2", "created": "Fri, 23 Apr 2021 03:37:21 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Dai", "Xili", ""], ["Yuan", "Xiaojun", ""], ["Gong", "Haigang", ""], ["Ma", "Yi", ""]]}, {"id": "2104.11208", "submitter": "Yanan Sun", "authors": "Yanan Sun, Guanzhi Wang, Qiao Gu, Chi-Keung Tang, Yu-Wing Tai", "title": "Deep Video Matting via Spatio-Temporal Alignment and Aggregation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the significant progress made by deep learning in natural image\nmatting, there has been so far no representative work on deep learning for\nvideo matting due to the inherent technical challenges in reasoning temporal\ndomain and lack of large-scale video matting datasets. In this paper, we\npropose a deep learning-based video matting framework which employs a novel and\neffective spatio-temporal feature aggregation module (ST-FAM). As optical flow\nestimation can be very unreliable within matting regions, ST-FAM is designed to\neffectively align and aggregate information across different spatial scales and\ntemporal frames within the network decoder. To eliminate frame-by-frame trimap\nannotations, a lightweight interactive trimap propagation network is also\nintroduced. The other contribution consists of a large-scale video matting\ndataset with groundtruth alpha mattes for quantitative evaluation and\nreal-world high-resolution videos with trimaps for qualitative evaluation.\nQuantitative and qualitative experimental results show that our framework\nsignificantly outperforms conventional video matting and deep image matting\nmethods applied to video in presence of multi-frame temporal information.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 17:42:08 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Sun", "Yanan", ""], ["Wang", "Guanzhi", ""], ["Gu", "Qiao", ""], ["Tang", "Chi-Keung", ""], ["Tai", "Yu-Wing", ""]]}, {"id": "2104.11213", "submitter": "Kiana Ehsani", "authors": "Kiana Ehsani, Winson Han, Alvaro Herrasti, Eli VanderBilt, Luca Weihs,\n  Eric Kolve, Aniruddha Kembhavi, Roozbeh Mottaghi", "title": "ManipulaTHOR: A Framework for Visual Object Manipulation", "comments": "CVPR 2021 -- (Oral presentation)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The domain of Embodied AI has recently witnessed substantial progress,\nparticularly in navigating agents within their environments. These early\nsuccesses have laid the building blocks for the community to tackle tasks that\nrequire agents to actively interact with objects in their environment. Object\nmanipulation is an established research domain within the robotics community\nand poses several challenges including manipulator motion, grasping and\nlong-horizon planning, particularly when dealing with oft-overlooked practical\nsetups involving visually rich and complex scenes, manipulation using mobile\nagents (as opposed to tabletop manipulation), and generalization to unseen\nenvironments and objects. We propose a framework for object manipulation built\nupon the physics-enabled, visually rich AI2-THOR framework and present a new\nchallenge to the Embodied AI community known as ArmPointNav. This task extends\nthe popular point navigation task to object manipulation and offers new\nchallenges including 3D obstacle avoidance, manipulating objects in the\npresence of occlusion, and multi-object manipulation that necessitates long\nterm planning. Popular learning paradigms that are successful on PointNav\nchallenges show promise, but leave a large room for improvement.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 17:49:04 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Ehsani", "Kiana", ""], ["Han", "Winson", ""], ["Herrasti", "Alvaro", ""], ["VanderBilt", "Eli", ""], ["Weihs", "Luca", ""], ["Kolve", "Eric", ""], ["Kembhavi", "Aniruddha", ""], ["Mottaghi", "Roozbeh", ""]]}, {"id": "2104.11216", "submitter": "Sumith Kulal", "authors": "Sumith Kulal, Jiayuan Mao, Alex Aiken, Jiajun Wu", "title": "Hierarchical Motion Understanding via Motion Programs", "comments": "CVPR 2021. First two authors contributed equally. Project page:\n  https://sumith1896.github.io/motion2prog/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current approaches to video analysis of human motion focus on raw pixels or\nkeypoints as the basic units of reasoning. We posit that adding higher-level\nmotion primitives, which can capture natural coarser units of motion such as\nbackswing or follow-through, can be used to improve downstream analysis tasks.\nThis higher level of abstraction can also capture key features, such as loops\nof repeated primitives, that are currently inaccessible at lower levels of\nrepresentation. We therefore introduce Motion Programs, a neuro-symbolic,\nprogram-like representation that expresses motions as a composition of\nhigh-level primitives. We also present a system for automatically inducing\nmotion programs from videos of human motion and for leveraging motion programs\nin video synthesis. Experiments show that motion programs can accurately\ndescribe a diverse set of human motions and the inferred programs contain\nsemantically meaningful motion primitives, such as arm swings and jumping\njacks. Our representation also benefits downstream tasks such as video\ninterpolation and video prediction and outperforms off-the-shelf models. We\nfurther demonstrate how these programs can detect diverse kinds of repetitive\nmotion and facilitate interactive video editing.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 17:49:59 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Kulal", "Sumith", ""], ["Mao", "Jiayuan", ""], ["Aiken", "Alex", ""], ["Wu", "Jiajun", ""]]}, {"id": "2104.11221", "submitter": "Jonathon Luiten", "authors": "Yang Liu and Idil Esen Zulfikar and Jonathon Luiten and Achal Dave and\n  Aljo\\v{s}a O\\v{s}ep and Deva Ramanan and Bastian Leibe and Laura Leal-Taix\\'e", "title": "Opening up Open-World Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose and study Open-World Tracking (OWT). Open-world\ntracking goes beyond current multi-object tracking benchmarks and methods which\nfocus on tracking object classes that belong to a predefined closed-set of\nfrequently observed object classes. In OWT, we relax this assumption: we may\nencounter objects at inference time that were not labeled for training. The\nmain contribution of this paper is the formalization of the OWT task, along\nwith an evaluation protocol and metric (Open-World Tracking Accuracy, OWTA),\nwhich decomposes into two intuitive terms, one for measuring recall, and\nanother for measuring track association accuracy. This allows us to perform a\nrigorous evaluation of several different baselines that follow design patterns\nproposed in the multi-object tracking community. Further we show that our\nOpen-World Tracking Baseline, while performing well in the OWT setting, also\nachieves near state-of-the-art results on traditional closed-world benchmarks,\nwithout any adjustments or tuning. We believe that this paper is an initial\nstep towards studying multi-object tracking in the open world, a task of\ncrucial importance for future intelligent agents that will need to understand,\nreact to, and learn from, an infinite variety of objects that can appear in an\nopen world.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 17:58:15 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Liu", "Yang", ""], ["Zulfikar", "Idil Esen", ""], ["Luiten", "Jonathon", ""], ["Dave", "Achal", ""], ["O\u0161ep", "Aljo\u0161a", ""], ["Ramanan", "Deva", ""], ["Leibe", "Bastian", ""], ["Leal-Taix\u00e9", "Laura", ""]]}, {"id": "2104.11222", "submitter": "Gaurav Parmar", "authors": "Gaurav Parmar, Richard Zhang, Jun-Yan Zhu", "title": "On Buggy Resizing Libraries and Surprising Subtleties in FID Calculation", "comments": "GitHub: https://www.github.com/GaParmar/clean-fid Website:\n  https://www.cs.cmu.edu/~clean-fid/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the sensitivity of the Fr\\'echet Inception Distance (FID)\nscore to inconsistent and often incorrect implementations across different\nimage processing libraries. FID score is widely used to evaluate generative\nmodels, but each FID implementation uses a different low-level image processing\nprocess. Image resizing functions in commonly-used deep learning libraries\noften introduce aliasing artifacts. We observe that numerous subtle choices\nneed to be made for FID calculation and a lack of consistencies in these\nchoices can lead to vastly different FID scores. In particular, we show that\nthe following choices are significant: (1) selecting what image resizing\nlibrary to use, (2) choosing what interpolation kernel to use, (3) what\nencoding to use when representing images. We additionally outline numerous\ncommon pitfalls that should be avoided and provide recommendations for\ncomputing the FID score accurately. We provide an easy-to-use optimized\nimplementation of our proposed recommendations in the accompanying code.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 17:58:38 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Parmar", "Gaurav", ""], ["Zhang", "Richard", ""], ["Zhu", "Jun-Yan", ""]]}, {"id": "2104.11224", "submitter": "Tomas Jakab", "authors": "Tomas Jakab, Richard Tucker, Ameesh Makadia, Jiajun Wu, Noah Snavely,\n  Angjoo Kanazawa", "title": "KeypointDeformer: Unsupervised 3D Keypoint Discovery for Shape Control", "comments": "CVPR 2021 (oral). Project page:\n  http://tomasjakab.github.io/KeypointDeformer", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce KeypointDeformer, a novel unsupervised method for shape control\nthrough automatically discovered 3D keypoints. We cast this as the problem of\naligning a source 3D object to a target 3D object from the same object\ncategory. Our method analyzes the difference between the shapes of the two\nobjects by comparing their latent representations. This latent representation\nis in the form of 3D keypoints that are learned in an unsupervised way. The\ndifference between the 3D keypoints of the source and the target objects then\ninforms the shape deformation algorithm that deforms the source object into the\ntarget object. The whole model is learned end-to-end and simultaneously\ndiscovers 3D keypoints while learning to use them for deforming object shapes.\nOur approach produces intuitive and semantically consistent control of shape\ndeformations. Moreover, our discovered 3D keypoints are consistent across\nobject category instances despite large shape variations. As our method is\nunsupervised, it can be readily deployed to new object categories without\nrequiring annotations for 3D keypoints and deformations.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 17:59:08 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Jakab", "Tomas", ""], ["Tucker", "Richard", ""], ["Makadia", "Ameesh", ""], ["Wu", "Jiajun", ""], ["Snavely", "Noah", ""], ["Kanazawa", "Angjoo", ""]]}, {"id": "2104.11225", "submitter": "Ji Hou", "authors": "Ji Hou, Saining Xie, Benjamin Graham, Angela Dai, Matthias Nie{\\ss}ner", "title": "Pri3D: Can 3D Priors Help 2D Representation Learning?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in 3D perception have shown impressive progress in\nunderstanding geometric structures of 3Dshapes and even scenes. Inspired by\nthese advances in geometric understanding, we aim to imbue image-based\nperception with representations learned under geometric constraints. We\nintroduce an approach to learn view-invariant,geometry-aware representations\nfor network pre-training, based on multi-view RGB-D data, that can then be\neffectively transferred to downstream 2D tasks. We propose to employ\ncontrastive learning under both multi-view im-age constraints and\nimage-geometry constraints to encode3D priors into learned 2D representations.\nThis results not only in improvement over 2D-only representation learning on\nthe image-based tasks of semantic segmentation, instance segmentation, and\nobject detection on real-world in-door datasets, but moreover, provides\nsignificant improvement in the low data regime. We show a significant\nimprovement of 6.0% on semantic segmentation on full data as well as 11.9% on\n20% data against baselines on ScanNet.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 17:59:30 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Hou", "Ji", ""], ["Xie", "Saining", ""], ["Graham", "Benjamin", ""], ["Dai", "Angela", ""], ["Nie\u00dfner", "Matthias", ""]]}, {"id": "2104.11227", "submitter": "Christoph Feichtenhofer", "authors": "Haoqi Fan, Bo Xiong, Karttikeya Mangalam, Yanghao Li, Zhicheng Yan,\n  Jitendra Malik, Christoph Feichtenhofer", "title": "Multiscale Vision Transformers", "comments": "Technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Multiscale Vision Transformers (MViT) for video and image\nrecognition, by connecting the seminal idea of multiscale feature hierarchies\nwith transformer models. Multiscale Transformers have several\nchannel-resolution scale stages. Starting from the input resolution and a small\nchannel dimension, the stages hierarchically expand the channel capacity while\nreducing the spatial resolution. This creates a multiscale pyramid of features\nwith early layers operating at high spatial resolution to model simple\nlow-level visual information, and deeper layers at spatially coarse, but\ncomplex, high-dimensional features. We evaluate this fundamental architectural\nprior for modeling the dense nature of visual signals for a variety of video\nrecognition tasks where it outperforms concurrent vision transformers that rely\non large scale external pre-training and are 5-10x more costly in computation\nand parameters. We further remove the temporal dimension and apply our model\nfor image classification where it outperforms prior work on vision\ntransformers. Code is available at:\nhttps://github.com/facebookresearch/SlowFast\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 17:59:45 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Fan", "Haoqi", ""], ["Xiong", "Bo", ""], ["Mangalam", "Karttikeya", ""], ["Li", "Yanghao", ""], ["Yan", "Zhicheng", ""], ["Malik", "Jitendra", ""], ["Feichtenhofer", "Christoph", ""]]}, {"id": "2104.11228", "submitter": "Dongdong Chen", "authors": "Can Wang and Menglei Chai and Mingming He and Dongdong Chen and Jing\n  Liao", "title": "Cross-Domain and Disentangled Face Manipulation with 3D Guidance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face image manipulation via three-dimensional guidance has been widely\napplied in various interactive scenarios due to its semantically-meaningful\nunderstanding and user-friendly controllability. However, existing\n3D-morphable-model-based manipulation methods are not directly applicable to\nout-of-domain faces, such as non-photorealistic paintings, cartoon portraits,\nor even animals, mainly due to the formidable difficulties in building the\nmodel for each specific face domain. To overcome this challenge, we propose, as\nfar as we know, the first method to manipulate faces in arbitrary domains using\nhuman 3DMM. This is achieved through two major steps: 1) disentangled mapping\nfrom 3DMM parameters to the latent space embedding of a pre-trained StyleGAN2\nthat guarantees disentangled and precise controls for each semantic attribute;\nand 2) cross-domain adaptation that bridges domain discrepancies and makes\nhuman 3DMM applicable to out-of-domain faces by enforcing a consistent latent\nspace embedding. Experiments and comparisons demonstrate the superiority of our\nhigh-quality semantic manipulation method on a variety of face domains with all\nmajor 3D facial attributes controllable: pose, expression, shape, albedo, and\nillumination. Moreover, we develop an intuitive editing interface to support\nuser-friendly control and instant feedback. Our project page is\nhttps://cassiepython.github.io/sigasia/cddfm3d.html.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 17:59:50 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Wang", "Can", ""], ["Chai", "Menglei", ""], ["He", "Mingming", ""], ["Chen", "Dongdong", ""], ["Liao", "Jing", ""]]}, {"id": "2104.11231", "submitter": "Tekin Evrim Ozmermer", "authors": "Tekin Evrim Ozmermer, Viktors Roze, Stanislavs Hilcuks, Alina\n  Nescerecka", "title": "VeriMedi: Pill Identification using Proxy-based Deep Metric Learning and\n  Exact Solution", "comments": "31 pages, 21 figures, 10 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.IT cs.LG math.IT", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We present the system that we have developed for the identification and\nverification of pills using images that are taken by the VeriMedi device. The\nVeriMedi device is an Internet of Things device that takes pictures of a filled\npill vial from the bottom of the vial and uses the solution that is presented\nin this research to identify the pills in the vials. The solution has two\nserially connected deep learning solutions which do segmentation and\nidentification. The segmentation solution creates the masks for each pill in\nthe vial image by using the Mask R-CNN model, then segments and crops the pills\nand blurs the background. After that, the segmented pill images are sent to the\nidentification solution where a Deep Metric Learning model that is trained with\nProxy Anchor Loss (PAL) function generates embedding vectors for each pill\nimage. The generated embedding vectors are fed into a one-layer fully connected\nnetwork that is trained with the exact solution to predict each single pill\nimage. Then, the aggregation/verification function aggregates the multiple\npredictions coming from multiple single pill images and verifies the\ncorrectness of the final prediction with respect to predefined rules. Besides,\nwe enhanced the PAL with a better proxy initialization that increased the\nperformance of the models and let the model learn the new classes of images\ncontinually without retraining the model with the whole dataset. When the model\nthat is trained with initial classes is retrained only with new classes, the\naccuracy of the model increases for both old and new classes. The\nidentification solution that we have presented in this research can also be\nreused for other problem domains which require continual learning and/or\nFine-Grained Visual Categorization.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 06:52:30 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Ozmermer", "Tekin Evrim", ""], ["Roze", "Viktors", ""], ["Hilcuks", "Stanislavs", ""], ["Nescerecka", "Alina", ""]]}, {"id": "2104.11244", "submitter": "Andrew Saydjari", "authors": "Andrew K. Saydjari, Douglas P. Finkbeiner", "title": "Equivariant Wavelets: Fast Rotation and Translation Invariant Wavelet\n  Scattering Transforms", "comments": "20 pages, 14 figures, submitted to IEEE Signal Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV astro-ph.IM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wavelet scattering networks, which are convolutional neural networks (CNNs)\nwith fixed filters and weights, are promising tools for image analysis.\nImposing symmetry on image statistics can improve human interpretability, aid\nin generalization, and provide dimension reduction. In this work, we introduce\na fast-to-compute, translationally invariant and rotationally equivariant\nwavelet scattering network (EqWS) and filter bank of wavelets (triglets). We\ndemonstrate the interpretability and quantify the invariance/equivariance of\nthe coefficients, briefly commenting on difficulties with implementing scale\nequivariance. On MNIST, we show that training on a rotationally invariant\nreduction of the coefficients maintains rotational invariance when generalized\nto test data and visualize residual symmetry breaking terms. Rotation\nequivariance is leveraged to estimate the rotation angle of digits and\nreconstruct the full rotation dependence of each coefficient from a single\nangle. We benchmark EqWS with linear classifiers on EMNIST and CIFAR-10/100,\nintroducing a new second-order, cross-color channel coupling for the color\nimages. We conclude by comparing the performance of an isotropic reduction of\nthe scattering coefficients and RWST, a previous coefficient reduction, on an\nisotropic classification of magnetohydrodynamic simulations with astrophysical\nrelevance.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 18:00:01 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Saydjari", "Andrew K.", ""], ["Finkbeiner", "Douglas P.", ""]]}, {"id": "2104.11274", "submitter": "Tapan Gandhi Prof", "authors": "Rohan Wadhawan and Tapan K. Gandhi", "title": "Landmark-Aware and Part-based Ensemble Transfer Learning Network for\n  Facial Expression Recognition from Static images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial Expression Recognition from static images is a challenging problem in\ncomputer vision applications. Convolutional Neural Network (CNN), the\nstate-of-the-art method for various computer vision tasks, has had limited\nsuccess in predicting expressions from faces having extreme poses,\nillumination, and occlusion conditions. To mitigate this issue, CNNs are often\naccompanied by techniques like transfer, multi-task, or ensemble learning that\noften provide high accuracy at the cost of high computational complexity. In\nthis work, we propose a Part-based Ensemble Transfer Learning network, which\nmodels how humans recognize facial expressions by correlating the spatial\norientation pattern of the facial features with a specific expression. It\nconsists of 5 sub-networks, in which each sub-network performs transfer\nlearning from one of the five subsets of facial landmarks: eyebrows, eyes,\nnose, mouth, or jaw to expression classification. We test the proposed network\non the CK+, JAFFE, and SFEW datasets, and it outperforms the benchmark for CK+\nand JAFFE datasets by 0.51\\% and 5.34\\%, respectively. Additionally, it\nconsists of a total of 1.65M model parameters and requires only 3.28 $\\times$\n$10^{6}$ FLOPS, which ensures computational efficiency for real-time\ndeployment. Grad-CAM visualizations of our proposed ensemble highlight the\ncomplementary nature of its sub-networks, a key design parameter of an\neffective ensemble network. Lastly, cross-dataset evaluation results reveal\nthat our proposed ensemble has a high generalization capacity. Our model\ntrained on the SFEW Train dataset achieves an accuracy of 47.53\\% on the CK+\ndataset, which is higher than what it achieves on the SFEW Valid dataset.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 18:38:33 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Wadhawan", "Rohan", ""], ["Gandhi", "Tapan K.", ""]]}, {"id": "2104.11280", "submitter": "Aliaksandr Siarohin", "authors": "Aliaksandr Siarohin, Oliver J. Woodford, Jian Ren, Menglei Chai and\n  Sergey Tulyakov", "title": "Motion Representations for Articulated Animation", "comments": null, "journal-ref": "CVPR 2021", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose novel motion representations for animating articulated objects\nconsisting of distinct parts. In a completely unsupervised manner, our method\nidentifies object parts, tracks them in a driving video, and infers their\nmotions by considering their principal axes. In contrast to the previous\nkeypoint-based works, our method extracts meaningful and consistent regions,\ndescribing locations, shape, and pose. The regions correspond to semantically\nrelevant and distinct object parts, that are more easily detected in frames of\nthe driving video. To force decoupling of foreground from background, we model\nnon-object related global motion with an additional affine transformation. To\nfacilitate animation and prevent the leakage of the shape of the driving\nobject, we disentangle shape and pose of objects in the region space. Our model\ncan animate a variety of objects, surpassing previous methods by a large margin\non existing benchmarks. We present a challenging new benchmark with\nhigh-resolution videos and show that the improvement is particularly pronounced\nwhen articulated objects are considered, reaching 96.6% user preference vs. the\nstate of the art.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 18:53:56 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Siarohin", "Aliaksandr", ""], ["Woodford", "Oliver J.", ""], ["Ren", "Jian", ""], ["Chai", "Menglei", ""], ["Tulyakov", "Sergey", ""]]}, {"id": "2104.11285", "submitter": "Tingwei Meng", "authors": "J\\'er\\^ome Darbon and Gabriel P. Langlois and Tingwei Meng", "title": "Connecting Hamilton--Jacobi partial differential equations with maximum\n  a posteriori and posterior mean estimators for some non-convex priors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many imaging problems can be formulated as inverse problems expressed as\nfinite-dimensional optimization problems. These optimization problems generally\nconsist of minimizing the sum of a data fidelity and regularization terms. In\n[23,26], connections between these optimization problems and (multi-time)\nHamilton--Jacobi partial differential equations have been proposed under the\nconvexity assumptions of both the data fidelity and regularization terms. In\nparticular, under these convexity assumptions, some representation formulas for\na minimizer can be obtained. From a Bayesian perspective, such a minimizer can\nbe seen as a maximum a posteriori estimator. In this chapter, we consider a\ncertain class of non-convex regularizations and show that similar\nrepresentation formulas for the minimizer can also be obtained. This is\nachieved by leveraging min-plus algebra techniques that have been originally\ndeveloped for solving certain Hamilton--Jacobi partial differential equations\narising in optimal control. Note that connections between viscous\nHamilton--Jacobi partial differential equations and Bayesian posterior mean\nestimators with Gaussian data fidelity terms and log-concave priors have been\nhighlighted in [25]. We also present similar results for certain Bayesian\nposterior mean estimators with Gaussian data fidelity and certain\nnon-log-concave priors using an analogue of min-plus algebra techniques.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 19:00:37 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Darbon", "J\u00e9r\u00f4me", ""], ["Langlois", "Gabriel P.", ""], ["Meng", "Tingwei", ""]]}, {"id": "2104.11288", "submitter": "Baoru Huang", "authors": "Baoru Huang, Jian-Qing Zheng, Stamatia Giannarou, Daniel S. Elson", "title": "H-Net: Unsupervised Attention-based Stereo Depth Estimation Leveraging\n  Epipolar Geometry", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Depth estimation from a stereo image pair has become one of the most explored\napplications in computer vision, with most of the previous methods relying on\nfully supervised learning settings. However, due to the difficulty in acquiring\naccurate and scalable ground truth data, the training of fully supervised\nmethods is challenging. As an alternative, self-supervised methods are becoming\nmore popular to mitigate this challenge. In this paper, we introduce the H-Net,\na deep-learning framework for unsupervised stereo depth estimation that\nleverages epipolar geometry to refine stereo matching. For the first time, a\nSiamese autoencoder architecture is used for depth estimation which allows\nmutual information between the rectified stereo images to be extracted. To\nenforce the epipolar constraint, the mutual epipolar attention mechanism has\nbeen designed which gives more emphasis to correspondences of features which\nlie on the same epipolar line while learning mutual information between the\ninput stereo pair. Stereo correspondences are further enhanced by incorporating\nsemantic information to the proposed attention mechanism. More specifically,\nthe optimal transport algorithm is used to suppress attention and eliminate\noutliers in areas not visible in both cameras. Extensive experiments on\nKITTI2015 and Cityscapes show that our method outperforms the state-ofthe-art\nunsupervised stereo depth estimation methods while closing the gap with the\nfully supervised approaches.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 19:16:35 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Huang", "Baoru", ""], ["Zheng", "Jian-Qing", ""], ["Giannarou", "Stamatia", ""], ["Elson", "Daniel S.", ""]]}, {"id": "2104.11379", "submitter": "Mahmood Amintoosi", "authors": "Mahmood Amintoosi, Farzam Farbiz", "title": "Eigenbackground Revisited: Can We Model the Background with\n  Eigenvectors?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using dominant eigenvectors for background modeling (usually known as\nEigenbackground) is a common technique in the literature. However, its results\nsuffer from noticeable artifacts. Thus have been many attempts to reduce the\nartifacts by making some improvements/enhancement in the Eigenbackground\nalgorithm.\n  In this paper, we show the main problem of the Eigenbackground is in its own\ncore and in fact, it is not a good idea to use strongest eigenvectors for\nmodeling the background. Instead, we propose an alternative solution by\nexploiting the weakest eigenvectors (which are usually thrown away and treated\nas garbage data) for background modeling. MATLAB codes are available at\n\\url{https://github.com/mamintoosi/Eigenbackground-Revisited}\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 02:21:03 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Amintoosi", "Mahmood", ""], ["Farbiz", "Farzam", ""]]}, {"id": "2104.11401", "submitter": "Jaehee Chun", "authors": "Jaehee Chun (3), Justin C. Park (1), Sven Olberg (1 and 2), You Zhang\n  (1), Dan Nguyen (1), Jing Wang (1), Jin Sung Kim (3), Steve Jiang (1) ((1)\n  Medical Artificial Intelligence and Automation (MAIA) Laboratory, Department\n  of Radiation Oncology, University of Texas Southwestern Medical Center,\n  Dallas, USA, (2) Department of Biomedical Engineering, Washington University\n  in St. Louis, St. Louis, USA, (3) Department of Radiation Oncology, Yonsei\n  Cancer Center, Yonsei University College of Medicine, Seoul, South Korea)", "title": "Intentional Deep Overfit Learning (IDOL): A Novel Deep Learning Strategy\n  for Adaptive Radiation Therapy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, we propose a tailored DL framework for patient-specific\nperformance that leverages the behavior of a model intentionally overfitted to\na patient-specific training dataset augmented from the prior information\navailable in an ART workflow - an approach we term Intentional Deep Overfit\nLearning (IDOL). Implementing the IDOL framework in any task in radiotherapy\nconsists of two training stages: 1) training a generalized model with a diverse\ntraining dataset of N patients, just as in the conventional DL approach, and 2)\nintentionally overfitting this general model to a small training\ndataset-specific the patient of interest (N+1) generated through perturbations\nand augmentations of the available task- and patient-specific prior information\nto establish a personalized IDOL model. The IDOL framework itself is\ntask-agnostic and is thus widely applicable to many components of the ART\nworkflow, three of which we use as a proof of concept here: the auto-contouring\ntask on re-planning CTs for traditional ART, the MRI super-resolution (SR) task\nfor MRI-guided ART, and the synthetic CT (sCT) reconstruction task for MRI-only\nART. In the re-planning CT auto-contouring task, the accuracy measured by the\nDice similarity coefficient improves from 0.847 with the general model to 0.935\nby adopting the IDOL model. In the case of MRI SR, the mean absolute error\n(MAE) is improved by 40% using the IDOL framework over the conventional model.\nFinally, in the sCT reconstruction task, the MAE is reduced from 68 to 22 HU by\nutilizing the IDOL framework.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 03:41:49 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Chun", "Jaehee", "", "1 and 2"], ["Park", "Justin C.", "", "1 and 2"], ["Olberg", "Sven", "", "1 and 2"], ["Zhang", "You", ""], ["Nguyen", "Dan", ""], ["Wang", "Jing", ""], ["Kim", "Jin Sung", ""], ["Jiang", "Steve", ""]]}, {"id": "2104.11403", "submitter": "Cece Jin", "authors": "Cece Jin, Yuanqi Chen, Ge Li, Tao Zhang, Thomas Li", "title": "Low Pass Filter for Anti-aliasing in Temporal Action Localization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In temporal action localization methods, temporal downsampling operations are\nwidely used to extract proposal features, but they often lead to the aliasing\nproblem, due to lacking consideration of sampling rates. This paper aims to\nverify the existence of aliasing in TAL methods and investigate utilizing low\npass filters to solve this problem by inhibiting the high-frequency band.\nHowever, the high-frequency band usually contains large amounts of specific\ninformation, which is important for model inference. Therefore, it is necessary\nto make a tradeoff between anti-aliasing and reserving high-frequency\ninformation. To acquire optimal performance, this paper learns different cutoff\nfrequencies for different instances dynamically. This design can be plugged\ninto most existing temporal modeling programs requiring only one additional\ncutoff frequency parameter. Integrating low pass filters to the downsampling\noperations significantly improves the detection performance and achieves\ncomparable results on THUMOS'14, ActivityNet~1.3, and Charades datasets.\nExperiments demonstrate that anti-aliasing with low pass filters in TAL is\nadvantageous and efficient.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 03:57:34 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Jin", "Cece", ""], ["Chen", "Yuanqi", ""], ["Li", "Ge", ""], ["Zhang", "Tao", ""], ["Li", "Thomas", ""]]}, {"id": "2104.11416", "submitter": "Yige Peng", "authors": "Yige Peng, Lei Bi, Ashnil Kumar, Michael Fulham, Dagan Feng, Jinman\n  Kim", "title": "Predicting Distant Metastases in Soft-Tissue Sarcomas from PET-CT scans\n  using Constrained Hierarchical Multi-Modality Feature Learning", "comments": "Under Review", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Distant metastases (DM) refer to the dissemination of tumors, usually, beyond\nthe organ where the tumor originated. They are the leading cause of death in\npatients with soft-tissue sarcomas (STSs). Positron emission\ntomography-computed tomography (PET-CT) is regarded as the imaging modality of\nchoice for the management of STSs. It is difficult to determine from imaging\nstudies which STS patients will develop metastases. 'Radiomics' refers to the\nextraction and analysis of quantitative features from medical images and it has\nbeen employed to help identify such tumors. The state-of-the-art in radiomics\nis based on convolutional neural networks (CNNs). Most CNNs are designed for\nsingle-modality imaging data (CT or PET alone) and do not exploit the\ninformation embedded in PET-CT where there is a combination of an anatomical\nand functional imaging modality. Furthermore, most radiomic methods rely on\nmanual input from imaging specialists for tumor delineation, definition and\nselection of radiomic features. This approach, however, may not be scalable to\ntumors with complex boundaries and where there are multiple other sites of\ndisease. We outline a new 3D CNN to help predict DM in STS patients from PET-CT\ndata. The 3D CNN uses a constrained feature learning module and a hierarchical\nmulti-modality feature learning module that leverages the complementary\ninformation from the modalities to focus on semantically important regions. Our\nresults on a public PET-CT dataset of STS patients show that multi-modal\ninformation improves the ability to identify those patients who develop DM.\nFurther our method outperformed all other related state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 05:12:02 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Peng", "Yige", ""], ["Bi", "Lei", ""], ["Kumar", "Ashnil", ""], ["Fulham", "Michael", ""], ["Feng", "Dagan", ""], ["Kim", "Jinman", ""]]}, {"id": "2104.11435", "submitter": "Beomyoung Kim", "authors": "Beomyoung Kim, Janghyeon Lee, Sihaeng Lee, Doyeon Kim, and Junmo Kim", "title": "TricubeNet: 2D Kernel-Based Object Representation for Weakly-Occluded\n  Oriented Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new approach for oriented object detection, an anchor-free\none-stage detector. This approach, named TricubeNet, represents each object as\na 2D Tricube kernel and extracts bounding boxes using appearance-based\npost-processing. Unlike existing anchor-based oriented object detectors, we can\nsave the computational complexity and the number of hyperparameters by\neliminating the anchor box in the network design. In addition, by adopting a\nheatmap-based detection process instead of the box offset regression, we simply\nand effectively solve the angle discontinuity problem, which is one of the\nimportant problems for oriented object detection. To further boost the\nperformance, we propose some effective techniques for the loss balancing,\nextracting the rotation-invariant feature, and heatmap refinement. To\ndemonstrate the effectiveness of our TricueNet, we experiment on various tasks\nfor the weakly-occluded oriented object detection. The extensive experimental\nresults show that our TricueNet is highly effective and competitive for\noriented object detection. The code is available at\nhttps://github.com/qjadud1994/TricubeNet.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 06:50:28 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Kim", "Beomyoung", ""], ["Lee", "Janghyeon", ""], ["Lee", "Sihaeng", ""], ["Kim", "Doyeon", ""], ["Kim", "Junmo", ""]]}, {"id": "2104.11436", "submitter": "Zehui Liao", "authors": "Zehui Liao, Yutong Xie, Shishuai Hu, Yong Xia", "title": "Learning from Ambiguous Labels for Lung Nodule Malignancy Prediction", "comments": "Submitted to IEEE-TMI", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Lung nodule malignancy prediction is an essential step in the early diagnosis\nof lung cancer. Besides the difficulties commonly discussed, the challenges of\nthis task also come from the ambiguous labels provided by annotators, since\ndeep learning models may learn, even amplify, the bias embedded in them. In\nthis paper, we propose a multi-view \"divide-and-rule\" (MV-DAR) model to learn\nfrom both reliable and ambiguous annotations for lung nodule malignancy\nprediction. According to the consistency and reliability of their annotations,\nwe divide nodules into three sets: a consistent and reliable set (CR-Set), an\ninconsistent set (IC-Set), and a low reliable set (LR-Set). The nodule in\nIC-Set is annotated by multiple radiologists inconsistently, and the nodule in\nLR-Set is annotated by only one radiologist. The proposed MV-DAR contains three\nDAR submodels to characterize a lung nodule from three orthographic views. Each\nDAR consists of a prediction network (Prd-Net), a counterfactual network\n(CF-Net), and a low reliable network (LR-Net), learning on CR-Set, IC-Set, and\nLR-Set, respectively. The image representation ability learned by CF-Net and\nLR-Net is then transferred to Prd-Net by negative-attention module (NA-Module)\nand consistent-attention module (CA-Module), aiming to boost the prediction\nability of Prd-Net. The MV-DAR model has been evaluated on the LIDC-IDRI\ndataset and LUNGx dataset. Our results indicate not only the effectiveness of\nthe proposed MV-DAR model in learning from ambiguous labels but also its\nsuperiority over present noisy label-learning models in lung nodule malignancy\nprediction.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 07:01:10 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Liao", "Zehui", ""], ["Xie", "Yutong", ""], ["Hu", "Shishuai", ""], ["Xia", "Yong", ""]]}, {"id": "2104.11452", "submitter": "Xin Chen", "authors": "Xin Chen, Anqi Pang, Wei Yang, Yuexin Ma, Lan Xu, Jingyi Yu", "title": "SportsCap: Monocular 3D Human Motion Capture and Fine-grained\n  Understanding in Challenging Sports Videos", "comments": "18 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markerless motion capture and understanding of professional non-daily human\nmovements is an important yet unsolved task, which suffers from complex motion\npatterns and severe self-occlusion, especially for the monocular setting. In\nthis paper, we propose SportsCap -- the first approach for simultaneously\ncapturing 3D human motions and understanding fine-grained actions from\nmonocular challenging sports video input. Our approach utilizes the semantic\nand temporally structured sub-motion prior in the embedding space for motion\ncapture and understanding in a data-driven multi-task manner. To enable robust\ncapture under complex motion patterns, we propose an effective motion embedding\nmodule to recover both the implicit motion embedding and explicit 3D motion\ndetails via a corresponding mapping function as well as a sub-motion\nclassifier. Based on such hybrid motion information, we introduce a\nmulti-stream spatial-temporal Graph Convolutional Network(ST-GCN) to predict\nthe fine-grained semantic action attributes, and adopt a semantic attribute\nmapping block to assemble various correlated action attributes into a\nhigh-level action label for the overall detailed understanding of the whole\nsequence, so as to enable various applications like action assessment or motion\nscoring. Comprehensive experiments on both public and our proposed datasets\nshow that with a challenging monocular sports video input, our novel approach\nnot only significantly improves the accuracy of 3D human motion capture, but\nalso recovers accurate fine-grained semantic action attributes.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 07:52:03 GMT"}, {"version": "v2", "created": "Mon, 26 Apr 2021 14:06:25 GMT"}, {"version": "v3", "created": "Thu, 15 Jul 2021 02:45:18 GMT"}, {"version": "v4", "created": "Fri, 16 Jul 2021 02:44:10 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Chen", "Xin", ""], ["Pang", "Anqi", ""], ["Yang", "Wei", ""], ["Ma", "Yuexin", ""], ["Xu", "Lan", ""], ["Yu", "Jingyi", ""]]}, {"id": "2104.11466", "submitter": "Ruolin Ye", "authors": "Ruolin Ye, Wenqiang Xu, Zhendong Xue, Tutian Tang, Yanfeng Wang, Cewu\n  Lu", "title": "H2O: A Benchmark for Visual Human-human Object Handover Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Object handover is a common human collaboration behavior that attracts\nattention from researchers in Robotics and Cognitive Science. Though visual\nperception plays an important role in the object handover task, the whole\nhandover process has been specifically explored. In this work, we propose a\nnovel rich-annotated dataset, H2O, for visual analysis of human-human object\nhandovers. The H2O, which contains 18K video clips involving 15 people who hand\nover 30 objects to each other, is a multi-purpose benchmark. It can support\nseveral vision-based tasks, from which, we specifically provide a baseline\nmethod, RGPNet, for a less-explored task named Receiver Grasp Prediction.\nExtensive experiments show that the RGPNet can produce plausible grasps based\non the giver's hand-object states in the pre-handover phase. Besides, we also\nreport the hand and object pose errors with existing baselines and show that\nthe dataset can serve as the video demonstrations for robot imitation learning\non the handover task. Dataset, model and code will be made public.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 08:30:54 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Ye", "Ruolin", ""], ["Xu", "Wenqiang", ""], ["Xue", "Zhendong", ""], ["Tang", "Tutian", ""], ["Wang", "Yanfeng", ""], ["Lu", "Cewu", ""]]}, {"id": "2104.11467", "submitter": "Robin Karlsson", "authors": "Robin Karlsson, David Robert Wong, Kazunari Kawabata, Simon Thompson,\n  Naoki Sakai", "title": "Probabilistic Rainfall Estimation from Automotive Lidar", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust sensing and perception in adverse weather conditions remains one of\nthe biggest challenges for realizing reliable autonomous vehicle mobility\nservices. Prior work has established that rainfall rate is a useful measure for\nadversity of atmospheric weather conditions. This work presents a probabilistic\nhierarchical Bayesian model that infers rainfall rate from automotive lidar\npoint cloud sequences with high accuracy and reliability. The model is a\nhierarchical mixture of expert model, or a probabilistic decision tree, with\ngating and expert nodes consisting of variational logistic and linear\nregression models. Experimental data used to train and evaluate the model is\ncollected in a large-scale rainfall experiment facility from both stationary\nand moving vehicle platforms. The results show prediction accuracy comparable\nto the measurement resolution of a disdrometer, and the soundness and\nusefulness of the uncertainty estimation. The model achieves RMSE 2.42 mm/h\nafter filtering out uncertain predictions. The error is comparable to the mean\nrainfall rate change of 3.5 mm/h between measurements. Model parameter studies\nshow how predictive performance changes with tree depth, sampling duration, and\ncrop box dimension. A second experiment demonstrate the predictability of\nhigher rainfall above 300 mm/h using a different lidar sensor, demonstrating\nsensor independence.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 08:35:54 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Karlsson", "Robin", ""], ["Wong", "David Robert", ""], ["Kawabata", "Kazunari", ""], ["Thompson", "Simon", ""], ["Sakai", "Naoki", ""]]}, {"id": "2104.11473", "submitter": "Kejun Wang", "authors": "Xinnan Ding, Kejun Wang, Chenhui Wang, Tianyi Lan, Liangliang Liu", "title": "Sequential convolutional network for behavioral pattern extraction in\n  gait recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  As a unique and promising biometric, video-based gait recognition has broad\napplications. The key step of this methodology is to learn the walking pattern\nof individuals, which, however, often suffers challenges to extract the\nbehavioral feature from a sequence directly. Most existing methods just focus\non either the appearance or the motion pattern. To overcome these limitations,\nwe propose a sequential convolutional network (SCN) from a novel perspective,\nwhere spatiotemporal features can be learned by a basic convolutional backbone.\nIn SCN, behavioral information extractors (BIE) are constructed to comprehend\nintermediate feature maps in time series through motion templates where the\nrelationship between frames can be analyzed, thereby distilling the information\nof the walking pattern. Furthermore, a multi-frame aggregator in SCN performs\nfeature integration on a sequence whose length is uncertain, via a mobile 3D\nconvolutional layer. To demonstrate the effectiveness, experiments have been\nconducted on two popular public benchmarks, CASIA-B and OU-MVLP, and our\napproach is demonstrated superior performance, comparing with the state-of-art\nmethods.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 08:44:10 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Ding", "Xinnan", ""], ["Wang", "Kejun", ""], ["Wang", "Chenhui", ""], ["Lan", "Tianyi", ""], ["Liu", "Liangliang", ""]]}, {"id": "2104.11487", "submitter": "Davide Abati", "authors": "Amirhossein Habibian, Davide Abati, Taco S. Cohen, Babak Ehteshami\n  Bejnordi", "title": "Skip-Convolutions for Efficient Video Processing", "comments": "CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose Skip-Convolutions to leverage the large amount of redundancies in\nvideo streams and save computations. Each video is represented as a series of\nchanges across frames and network activations, denoted as residuals. We\nreformulate standard convolution to be efficiently computed on residual frames:\neach layer is coupled with a binary gate deciding whether a residual is\nimportant to the model prediction,~\\eg foreground regions, or it can be safely\nskipped, e.g. background regions. These gates can either be implemented as an\nefficient network trained jointly with convolution kernels, or can simply skip\nthe residuals based on their magnitude. Gating functions can also incorporate\nblock-wise sparsity structures, as required for efficient implementation on\nhardware platforms. By replacing all convolutions with Skip-Convolutions in two\nstate-of-the-art architectures, namely EfficientDet and HRNet, we reduce their\ncomputational cost consistently by a factor of 3~4x for two different tasks,\nwithout any accuracy drop. Extensive comparisons with existing model\ncompression, as well as image and video efficiency methods demonstrate that\nSkip-Convolutions set a new state-of-the-art by effectively exploiting the\ntemporal redundancies in videos.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 09:10:39 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Habibian", "Amirhossein", ""], ["Abati", "Davide", ""], ["Cohen", "Taco S.", ""], ["Bejnordi", "Babak Ehteshami", ""]]}, {"id": "2104.11489", "submitter": "Akshay Rangesh", "authors": "Akshay Rangesh, Nachiket Deo, Ross Greer, Pujitha Gunaratne, Mohan M.\n  Trivedi", "title": "Autonomous Vehicles that Alert Humans to Take-Over Controls: Modeling\n  with Real-World Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  With increasing automation in passenger vehicles, the study of safe and\nsmooth occupant-vehicle interaction and control transitions is key. In this\nstudy, we focus on the development of contextual, semantically meaningful\nrepresentations of the driver state, which can then be used to determine the\nappropriate timing and conditions for transfer of control between driver and\nvehicle. To this end, we conduct a large-scale real-world controlled data study\nwhere participants are instructed to take-over control from an autonomous agent\nunder different driving conditions while engaged in a variety of distracting\nactivities. These take-over events are captured using multiple driver-facing\ncameras, which when labelled result in a dataset of control transitions and\ntheir corresponding take-over times (TOTs). We then develop and train TOT\nmodels that operate sequentially on mid to high-level features produced by\ncomputer vision algorithms operating on different driver-facing camera views.\nThe proposed TOT model produces continuous predictions of take-over times\nwithout delay, and shows promising qualitative and quantitative results in\ncomplex real-world scenarios.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 09:16:53 GMT"}, {"version": "v2", "created": "Sat, 10 Jul 2021 19:52:23 GMT"}, {"version": "v3", "created": "Thu, 22 Jul 2021 23:03:52 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Rangesh", "Akshay", ""], ["Deo", "Nachiket", ""], ["Greer", "Ross", ""], ["Gunaratne", "Pujitha", ""], ["Trivedi", "Mohan M.", ""]]}, {"id": "2104.11493", "submitter": "Zhengmi Tang", "authors": "Zhengmi Tang, Tomo Miyazaki, Yoshihiro Sugaya, and Shinichiro Omachi", "title": "Stroke-Based Scene Text Erasing Using Synthetic Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scene text erasing, which replaces text regions with reasonable content in\nnatural images, has drawn attention in the computer vision community in recent\nyears. There are two potential subtasks in scene text erasing: text detection\nand image inpainting. Either sub-task requires considerable data to achieve\nbetter performance; however, the lack of a large-scale real-world scene-text\nremoval dataset allows the existing methods to not work in full strength. To\navoid the limitation of the lack of pairwise real-world data, we enhance and\nmake full use of the synthetic text and consequently train our model only on\nthe dataset generated by the improved synthetic text engine. Our proposed\nnetwork contains a stroke mask prediction module and background inpainting\nmodule that can extract the text stroke as a relatively small hole from the\ntext image patch to maintain more background content for better inpainting\nresults. This model can partially erase text instances in a scene image with a\nbounding box provided or work with an existing scene text detector for\nautomatic scene text erasing. The experimental results of qualitative\nevaluation and quantitative evaluation on the SCUT-Syn, ICDAR2013, and\nSCUT-EnsText datasets demonstrate that our method significantly outperforms\nexisting state-of-the-art methods even when trained on real-world data.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 09:29:41 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Tang", "Zhengmi", ""], ["Miyazaki", "Tomo", ""], ["Sugaya", "Yoshihiro", ""], ["Omachi", "Shinichiro", ""]]}, {"id": "2104.11502", "submitter": "Kai Wang", "authors": "Jinxing Ye, Xioajiang Peng, Baigui Sun, Kai Wang, Xiuyu Sun, Hao Li,\n  Hanqing Wu", "title": "Learning to Cluster Faces via Transformer", "comments": "Face Transformer", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face clustering is a useful tool for applications like automatic face\nannotation and retrieval. The main challenge is that it is difficult to cluster\nimages from the same identity with different face poses, occlusions, and image\nquality. Traditional clustering methods usually ignore the relationship between\nindividual images and their neighbors which may contain useful context\ninformation. In this paper, we repurpose the well-known Transformer and\nintroduce a Face Transformer for supervised face clustering. In Face\nTransformer, we decompose the face clustering into two steps: relation encoding\nand linkage predicting. Specifically, given a face image, a \\textbf{relation\nencoder} module aggregates local context information from its neighbors and a\n\\textbf{linkage predictor} module judges whether a pair of images belong to the\nsame cluster or not. In the local linkage graph view, Face Transformer can\ngenerate more robust node and edge representations compared to existing\nmethods. Experiments on both MS-Celeb-1M and DeepFashion show that our method\nachieves state-of-the-art performance, e.g., 91.12\\% in pairwise F-score on\nMS-Celeb-1M.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 09:43:36 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Ye", "Jinxing", ""], ["Peng", "Xioajiang", ""], ["Sun", "Baigui", ""], ["Wang", "Kai", ""], ["Sun", "Xiuyu", ""], ["Li", "Hao", ""], ["Wu", "Hanqing", ""]]}, {"id": "2104.11507", "submitter": "Xuequan Lu", "authors": "Sheldon Fung, Xuequan Lu, Chao Zhang, Chang-Tsun Li", "title": "DeepfakeUCL: Deepfake Detection via Unsupervised Contrastive Learning", "comments": "accepted to IJCNN2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face deepfake detection has seen impressive results recently. Nearly all\nexisting deep learning techniques for face deepfake detection are fully\nsupervised and require labels during training. In this paper, we design a novel\ndeepfake detection method via unsupervised contrastive learning. We first\ngenerate two different transformed versions of an image and feed them into two\nsequential sub-networks, i.e., an encoder and a projection head. The\nunsupervised training is achieved by maximizing the correspondence degree of\nthe outputs of the projection head. To evaluate the detection performance of\nour unsupervised method, we further use the unsupervised features to train an\nefficient linear classification network. Extensive experiments show that our\nunsupervised learning method enables comparable detection performance to\nstate-of-the-art supervised techniques, in both the intra- and inter-dataset\nsettings. We also conduct ablation studies for our method.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 09:48:10 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Fung", "Sheldon", ""], ["Lu", "Xuequan", ""], ["Zhang", "Chao", ""], ["Li", "Chang-Tsun", ""]]}, {"id": "2104.11520", "submitter": "Mariella Dimiccoli", "authors": "Alejandro Cartas, Petia Radeva, Mariella Dimiccoli", "title": "Modeling long-term interactions to enhance action recognition", "comments": "Accepted to the 25th International Conference on Pattern Recognition\n  (ICPR), 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a new approach to under-stand actions in egocentric\nvideos that exploits the semantics of object interactions at both frame and\ntemporal levels. At the frame level, we use a region-based approach that takes\nas input a primary region roughly corresponding to the user hands and a set of\nsecondary regions potentially corresponding to the interacting objects and\ncalculates the action score through a CNN formulation. This information is then\nfed to a Hierarchical LongShort-Term Memory Network (HLSTM) that captures\ntemporal dependencies between actions within and across shots. Ablation studies\nthoroughly validate the proposed approach, showing in particular that both\nlevels of the HLSTM architecture contribute to performance improvement.\nFurthermore, quantitative comparisons show that the proposed approach\noutperforms the state-of-the-art in terms of action recognition on standard\nbenchmarks,without relying on motion information\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 10:08:15 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Cartas", "Alejandro", ""], ["Radeva", "Petia", ""], ["Dimiccoli", "Mariella", ""]]}, {"id": "2104.11530", "submitter": "Junaid Ahmed Ghauri", "authors": "Junaid Ahmed Ghauri, Sherzod Hakimov, Ralph Ewerth", "title": "Supervised Video Summarization via Multiple Feature Sets with Parallel\n  Attention", "comments": "Accepted in IEEE International Conference on Multimedia and Expo\n  (ICME) 2021 (They have copyright to publish camera ready version of this\n  work)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.IR cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The assignment of importance scores to particular frames or (short) segments\nin a video is crucial for summarization, but also a difficult task. Previous\nwork utilizes only one source of visual features. In this paper, we suggest a\nnovel model architecture that combines three feature sets for visual content\nand motion to predict importance scores. The proposed architecture utilizes an\nattention mechanism before fusing motion features and features representing the\n(static) visual content, i.e., derived from an image classification model.\nComprehensive experimental evaluations are reported for two well-known\ndatasets, SumMe and TVSum. In this context, we identify methodological issues\non how previous work used these benchmark datasets, and present a fair\nevaluation scheme with appropriate data splits that can be used in future work.\nWhen using static and motion features with parallel attention mechanism, we\nimprove state-of-the-art results for SumMe, while being on par with the state\nof the art for the other dataset.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 10:46:35 GMT"}, {"version": "v2", "created": "Thu, 13 May 2021 16:07:41 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Ghauri", "Junaid Ahmed", ""], ["Hakimov", "Sherzod", ""], ["Ewerth", "Ralph", ""]]}, {"id": "2104.11536", "submitter": "Qian Bao", "authors": "Wu Liu, Qian Bao, Yu Sun, Tao Mei", "title": "Recent Advances in Monocular 2D and 3D Human Pose Estimation: A Deep\n  Learning Perspective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimation of the human pose from a monocular camera has been an emerging\nresearch topic in the computer vision community with many applications.\nRecently, benefited from the deep learning technologies, a significant amount\nof research efforts have greatly advanced the monocular human pose estimation\nboth in 2D and 3D areas. Although there have been some works to summarize the\ndifferent approaches, it still remains challenging for researchers to have an\nin-depth view of how these approaches work. In this paper, we provide a\ncomprehensive and holistic 2D-to-3D perspective to tackle this problem. We\ncategorize the mainstream and milestone approaches since the year 2014 under\nunified frameworks. By systematically summarizing the differences and\nconnections between these approaches, we further analyze the solutions for\nchallenging cases, such as the lack of data, the inherent ambiguity between 2D\nand 3D, and the complex multi-person scenarios. We also summarize the pose\nrepresentation styles, benchmarks, evaluation metrics, and the quantitative\nperformance of popular approaches. Finally, we discuss the challenges and give\ndeep thinking of promising directions for future research. We believe this\nsurvey will provide the readers with a deep and insightful understanding of\nmonocular human pose estimation.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 11:07:07 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Liu", "Wu", ""], ["Bao", "Qian", ""], ["Sun", "Yu", ""], ["Mei", "Tao", ""]]}, {"id": "2104.11539", "submitter": "Nianchang Huang", "authors": "Nianchang Huang, Jianan Liu, Qiang Zhang, Jungong Han", "title": "Exploring Modality-shared Appearance Features and Modality-invariant\n  Relation Features for Cross-modality Person Re-Identification", "comments": "13 pages, 8 figures, submitted to TIP", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Most existing cross-modality person re-identification works rely on\ndiscriminative modality-shared features for reducing cross-modality variations\nand intra-modality variations. Despite some initial success, such\nmodality-shared appearance features cannot capture enough modality-invariant\ndiscriminative information due to a massive discrepancy between RGB and\ninfrared images. To address this issue, on the top of appearance features, we\nfurther capture the modality-invariant relations among different person parts\n(referred to as modality-invariant relation features), which are the complement\nto those modality-shared appearance features and help to identify persons with\nsimilar appearances but different body shapes. To this end, a Multi-level\nTwo-streamed Modality-shared Feature Extraction (MTMFE) sub-network is\ndesigned, where the modality-shared appearance features and modality-invariant\nrelation features are first extracted in a shared 2D feature space and a shared\n3D feature space, respectively. The two features are then fused into the final\nmodality-shared features such that both cross-modality variations and\nintra-modality variations can be reduced. Besides, a novel cross-modality\nquadruplet loss is proposed to further reduce the cross-modality variations.\nExperimental results on several benchmark datasets demonstrate that our\nproposed method exceeds state-of-the-art algorithms by a noticeable margin.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 11:14:07 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Huang", "Nianchang", ""], ["Liu", "Jianan", ""], ["Zhang", "Qiang", ""], ["Han", "Jungong", ""]]}, {"id": "2104.11543", "submitter": "Nianchang Huang", "authors": "Nianchang Huang, Qiang Zhang, Jungong Han", "title": "Middle-level Fusion for Lightweight RGB-D Salient Object Detection", "comments": "11 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Most existing lightweight RGB-D salient object detection (SOD) models are\nbased on two-stream structure or single-stream structure. The former one first\nuses two sub-networks to extract unimodal features from RGB and depth images,\nrespectively, and then fuses them for SOD. While, the latter one directly\nextracts multi-modal features from the input RGB-D images and then focuses on\nexploiting cross-level complementary information. However, two-stream structure\nbased models inevitably require more parameters and single-stream structure\nbased ones cannot well exploit the cross-modal complementary information since\nthey ignore the modality difference. To address these issues, we propose to\nemploy the middle-level fusion structure for designing lightweight RGB-D SOD\nmodel in this paper, which first employs two sub-networks to extract low- and\nmiddle-level unimodal features, respectively, and then fuses those extracted\nmiddle-level unimodal features for extracting corresponding high-level\nmulti-modal features in the subsequent sub-network. Different from existing\nmodels, this structure can effectively exploit the cross-modal complementary\ninformation and significantly reduce the network's parameters, simultaneously.\nTherefore, a novel lightweight SOD model is designed, which contains a\ninformation-aware multi-modal feature fusion (IMFF) module for effectively\ncapturing the cross-modal complementary information and a lightweight\nfeature-level and decision-level feature fusion (LFDF) module for aggregating\nthe feature-level and the decision-level saliency information in different\nstages with less parameters. Our proposed model has only 3.9M parameters and\nruns at 33 FPS. The experimental results on several benchmark datasets verify\nthe effectiveness and superiority of the proposed method over some\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 11:37:15 GMT"}, {"version": "v2", "created": "Thu, 6 May 2021 08:15:32 GMT"}, {"version": "v3", "created": "Sat, 5 Jun 2021 09:50:02 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Huang", "Nianchang", ""], ["Zhang", "Qiang", ""], ["Han", "Jungong", ""]]}, {"id": "2104.11548", "submitter": "Junsong Wang", "authors": "Junsong Wang, Yubo Li, Zhiyong Chang, Haitao Yue, Yonghua Lin", "title": "Fine-Grained Texture Identification for Reliable Product Traceability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Texture exists in lots of the products, such as wood, beef and compression\ntea. These abundant and stochastic texture patterns are significantly different\nbetween any two products. Unlike the traditional digital ID tracking, in this\npaper, we propose a novel approach for product traceability, which directly\nuses the natural texture of the product itself as the unique identifier. A\ntexture identification based traceability system for Pu'er compression tea is\ndeveloped to demonstrate the feasibility of the proposed solution. With\ntea-brick images collected from manufactures and individual users, a\nlarge-scale dataset has been formed to evaluate the performance of tea-brick\ntexture verification and searching algorithm. The texture similarity approach\nwith local feature extraction and matching achieves the verification accuracy\nof 99.6% and the top-1 searching accuracy of 98.9%, respectively.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 11:53:43 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Wang", "Junsong", ""], ["Li", "Yubo", ""], ["Chang", "Zhiyong", ""], ["Yue", "Haitao", ""], ["Lin", "Yonghua", ""]]}, {"id": "2104.11551", "submitter": "Mengfan Li", "authors": "Mengfan Li", "title": "Research on the Detection Method of Breast Cancer Deep Convolutional\n  Neural Network Based on Computer Aid", "comments": "\\c{opyright}2021 IEEE", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Traditional breast cancer image classification methods require manual\nextraction of features from medical images, which not only require professional\nmedical knowledge, but also have problems such as time-consuming and\nlabor-intensive and difficulty in extracting high-quality features. Therefore,\nthe paper proposes a computer-based feature fusion Convolutional neural network\nbreast cancer image classification and detection method. The paper pre-trains\ntwo convolutional neural networks with different structures, and then uses the\nconvolutional neural network to automatically extract the characteristics of\nfeatures, fuse the features extracted from the two structures, and finally use\nthe classifier classifies the fused features. The experimental results show\nthat the accuracy of this method in the classification of breast cancer image\ndata sets is 89%, and the classification accuracy of breast cancer images is\nsignificantly improved compared with traditional methods.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 12:03:53 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Li", "Mengfan", ""]]}, {"id": "2104.11554", "submitter": "Haoran Xie", "authors": "Yi He, Haoran Xie, Chao Zhang, Xi Yang, Kazunori Miyata", "title": "Sketch-based Normal Map Generation with Geometric Sampling", "comments": "accepted in International Workshop on Advanced Image Technology 2021,\n  5 pages, 2 figures", "journal-ref": null, "doi": "10.1117/12.2590760", "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Normal map is an important and efficient way to represent complex 3D models.\nA designer may benefit from the auto-generation of high quality and accurate\nnormal maps from freehand sketches in 3D content creation. This paper proposes\na deep generative model for generating normal maps from users sketch with\ngeometric sampling. Our generative model is based on Conditional Generative\nAdversarial Network with the curvature-sensitive points sampling of conditional\nmasks. This sampling process can help eliminate the ambiguity of generation\nresults as network input. In addition, we adopted a U-Net structure\ndiscriminator to help the generator be better trained. It is verified that the\nproposed framework can generate more accurate normal maps.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 12:30:22 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["He", "Yi", ""], ["Xie", "Haoran", ""], ["Zhang", "Chao", ""], ["Yang", "Xi", ""], ["Miyata", "Kazunori", ""]]}, {"id": "2104.11560", "submitter": "Wenliang Dai", "authors": "Wenliang Dai, Samuel Cahyawijaya, Yejin Bang, Pascale Fung", "title": "Weakly-supervised Multi-task Learning for Multimodal Affect Recognition", "comments": "13 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multimodal affect recognition constitutes an important aspect for enhancing\ninterpersonal relationships in human-computer interaction. However, relevant\ndata is hard to come by and notably costly to annotate, which poses a\nchallenging barrier to build robust multimodal affect recognition systems.\nModels trained on these relatively small datasets tend to overfit and the\nimprovement gained by using complex state-of-the-art models is marginal\ncompared to simple baselines. Meanwhile, there are many different multimodal\naffect recognition datasets, though each may be small. In this paper, we\npropose to leverage these datasets using weakly-supervised multi-task learning\nto improve the generalization performance on each of them. Specifically, we\nexplore three multimodal affect recognition tasks: 1) emotion recognition; 2)\nsentiment analysis; and 3) sarcasm recognition. Our experimental results show\nthat multi-tasking can benefit all these tasks, achieving an improvement up to\n2.9% accuracy and 3.3% F1-score. Furthermore, our method also helps to improve\nthe stability of model performance. In addition, our analysis suggests that\nweak supervision can provide a comparable contribution to strong supervision if\nthe tasks are highly correlated.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 12:36:19 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Dai", "Wenliang", ""], ["Cahyawijaya", "Samuel", ""], ["Bang", "Yejin", ""], ["Fung", "Pascale", ""]]}, {"id": "2104.11568", "submitter": "Lorin Sweeney", "authors": "Lorin Sweeney, Graham Healy, Alan F. Smeaton", "title": "The Influence of Audio on Video Memorability with an Audio Gestalt\n  Regulated Video Memorability System", "comments": "6 pages, 3 figures, 4 tables, paper accepted in CBMI 2021 for\n  publication and oral presentation", "journal-ref": null, "doi": "10.1109/CBMI50038.2021.9461903", "report-no": null, "categories": "cs.MM cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Memories are the tethering threads that tie us to the world, and memorability\nis the measure of their tensile strength. The threads of memory are spun from\nfibres of many modalities, obscuring the contribution of a single fibre to a\nthread's overall tensile strength. Unfurling these fibres is the key to\nunderstanding the nature of their interaction, and how we can ultimately create\nmore meaningful media content. In this paper, we examine the influence of audio\non video recognition memorability, finding evidence to suggest that it can\nfacilitate overall video recognition memorability rich in high-level (gestalt)\naudio features. We introduce a novel multimodal deep learning-based late-fusion\nsystem that uses audio gestalt to estimate the influence of a given video's\naudio on its overall short-term recognition memorability, and selectively\nleverages audio features to make a prediction accordingly. We benchmark our\naudio gestalt based system on the Memento10k short-term video memorability\ndataset, achieving top-2 state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 12:53:33 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Sweeney", "Lorin", ""], ["Healy", "Graham", ""], ["Smeaton", "Alan F.", ""]]}, {"id": "2104.11571", "submitter": "Ran Ben Izhak", "authors": "Ran Ben Izhak, Alon Lahav and Ayellet Tal", "title": "AttWalk: Attentive Cross-Walks for Deep Mesh Analysis", "comments": "8 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Mesh representation by random walks has been shown to benefit deep learning.\nRandomness is indeed a powerful concept. However, it comes with a price: some\nwalks might wander around non-characteristic regions of the mesh, which might\nbe harmful to shape analysis, especially when only a few walks are utilized. We\npropose a novel walk-attention mechanism that leverages the fact that multiple\nwalks are used. The key idea is that the walks may provide each other with\ninformation regarding the meaningful (attentive) features of the mesh. We\nutilize this mutual information to extract a single descriptor of the mesh.\nThis differs from common attention mechanisms that use attention to improve the\nrepresentation of each individual descriptor. Our approach achieves SOTA\nresults for two basic 3D shape analysis tasks: classification and retrieval.\nEven a handful of walks along a mesh suffice for learning.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 13:02:39 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Izhak", "Ran Ben", ""], ["Lahav", "Alon", ""], ["Tal", "Ayellet", ""]]}, {"id": "2104.11574", "submitter": "Maged Abdalla Helmy Abdou", "authors": "Maged Helmy, Anastasiya Dykyy, Tuyen Trung Truong, Paulo Ferreira,\n  Eric Jul", "title": "CapillaryNet: An Automated System to Analyze Microcirculation Videos\n  from Handheld Vital Microscopy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Capillaries are the smallest vessels in the body responsible for the delivery\nof oxygen and nutrients to the surrounding cells. Various diseases have been\nshown to alter the density of nutritive capillaries and the flow velocity of\nerythrocytes. In previous studies, capillary density and flow velocity have\nbeen assessed manually by trained specialists. Manual analysis of a 20-second\nlong microvascular video takes on average 20 minutes and requires extensive\ntraining. Several studies have reported that manual analysis hinders the\napplication of microvascular microscopy in a clinical setting. In this paper,\nwe present a fully automated system, called CapillaryNet, that can automate\nmicrovascular microscopy analysis so it can be used as a clinical application.\nMoreover, CapillaryNet measures several microvascular parameters that\nresearchers were previously unable to quantify, i.e. capillary hematocrit and\nintra-capillary flow velocity heterogeneity.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 13:14:47 GMT"}, {"version": "v2", "created": "Fri, 21 May 2021 15:08:25 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Helmy", "Maged", ""], ["Dykyy", "Anastasiya", ""], ["Truong", "Tuyen Trung", ""], ["Ferreira", "Paulo", ""], ["Jul", "Eric", ""]]}, {"id": "2104.11585", "submitter": "Qing Guo", "authors": "Ziyi Cheng and Xuhong Ren and Felix Juefei-Xu and Wanli Xue and Qing\n  Guo and Lei Ma and Jianjun Zhao", "title": "DeepMix: Online Auto Data Augmentation for Robust Visual Object Tracking", "comments": "6 pages, 2 figures. This work has been accepted to ICME 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online updating of the object model via samples from historical frames is of\ngreat importance for accurate visual object tracking. Recent works mainly focus\non constructing effective and efficient updating methods while neglecting the\ntraining samples for learning discriminative object models, which is also a key\npart of a learning problem. In this paper, we propose the DeepMix that takes\nhistorical samples' embeddings as input and generates augmented embeddings\nonline, enhancing the state-of-the-art online learning methods for visual\nobject tracking. More specifically, we first propose the online data\naugmentation for tracking that online augments the historical samples through\nobject-aware filtering. Then, we propose MixNet which is an offline trained\nnetwork for performing online data augmentation within one-step, enhancing the\ntracking accuracy while preserving high speeds of the state-of-the-art online\nlearning methods. The extensive experiments on three different tracking\nframeworks, i.e., DiMP, DSiam, and SiamRPN++, and three large-scale and\nchallenging datasets, \\ie, OTB-2015, LaSOT, and VOT, demonstrate the\neffectiveness and advantages of the proposed method.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 13:37:47 GMT"}, {"version": "v2", "created": "Mon, 3 May 2021 03:37:04 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Cheng", "Ziyi", ""], ["Ren", "Xuhong", ""], ["Juefei-Xu", "Felix", ""], ["Xue", "Wanli", ""], ["Guo", "Qing", ""], ["Ma", "Lei", ""], ["Zhao", "Jianjun", ""]]}, {"id": "2104.11589", "submitter": "Sang Hun Lee", "authors": "Sangrok Lee, Taekang Woo, Sang Hun Lee", "title": "SBNet: Segmentation-based Network for Natural Language-based Vehicle\n  Search", "comments": "7 pages, 4 figures, CVPR Workshop Paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Natural language-based vehicle retrieval is a task to find a target vehicle\nwithin a given image based on a natural language description as a query. This\ntechnology can be applied to various areas including police searching for a\nsuspect vehicle. However, it is challenging due to the ambiguity of language\ndescriptions and the difficulty of processing multi-modal data. To tackle this\nproblem, we propose a deep neural network called SBNet that performs natural\nlanguage-based segmentation for vehicle retrieval. We also propose two\ntask-specific modules to improve performance: a substitution module that helps\nfeatures from different domains to be embedded in the same space and a future\nprediction module that learns temporal information. SBnet has been trained\nusing the CityFlow-NL dataset that contains 2,498 tracks of vehicles with three\nunique natural language descriptions each and tested 530 unique vehicle tracks\nand their corresponding query sets. SBNet achieved a significant improvement\nover the baseline in the natural language-based vehicle tracking track in the\nAI City Challenge 2021.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 08:06:17 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Lee", "Sangrok", ""], ["Woo", "Taekang", ""], ["Lee", "Sang Hun", ""]]}, {"id": "2104.11596", "submitter": "Anne-Marie Rickmann", "authors": "Fabian Gr\\\"oger, Anne-Marie Rickmann, Christian Wachinger", "title": "STRUDEL: Self-Training with Uncertainty Dependent Label Refinement\n  across Domains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We propose an unsupervised domain adaptation (UDA) approach for white matter\nhyperintensity (WMH) segmentation, which uses Self-Training with Uncertainty\nDEpendent Label refinement (STRUDEL). Self-training has recently been\nintroduced as a highly effective method for UDA, which is based on\nself-generated pseudo labels. However, pseudo labels can be very noisy and\ntherefore deteriorate model performance. We propose to predict the uncertainty\nof pseudo labels and integrate it in the training process with an\nuncertainty-guided loss function to highlight labels with high certainty.\nSTRUDEL is further improved by incorporating the segmentation output of an\nexisting method in the pseudo label generation that showed high robustness for\nWMH segmentation. In our experiments, we evaluate STRUDEL with a standard U-Net\nand a modified network with a higher receptive field. Our results on WMH\nsegmentation across datasets demonstrate the significant improvement of STRUDEL\nwith respect to standard self-training.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 13:46:26 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Gr\u00f6ger", "Fabian", ""], ["Rickmann", "Anne-Marie", ""], ["Wachinger", "Christian", ""]]}, {"id": "2104.11599", "submitter": "Weihao Xia", "authors": "Shuwei Shi, Qingyan Bai, Mingdeng Cao, Weihao Xia, Jiahao Wang, Yifan\n  Chen, Yujiu Yang", "title": "Region-Adaptive Deformable Network for Image Quality Assessment", "comments": "CVPR NTIRE Workshop 2021. The first two authors contribute equally to\n  this work. Code is available at https://github.com/IIGROUP/RADN", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image quality assessment (IQA) aims to assess the perceptual quality of\nimages. The outputs of the IQA algorithms are expected to be consistent with\nhuman subjective perception. In image restoration and enhancement tasks, images\ngenerated by generative adversarial networks (GAN) can achieve better visual\nperformance than traditional CNN-generated images, although they have spatial\nshift and texture noise. Unfortunately, the existing IQA methods have\nunsatisfactory performance on the GAN-based distortion partially because of\ntheir low tolerance to spatial misalignment. To this end, we propose the\nreference-oriented deformable convolution, which can improve the performance of\nan IQA network on GAN-based distortion by adaptively considering this\nmisalignment. We further propose a patch-level attention module to enhance the\ninteraction among different patch regions, which are processed independently in\nprevious patch-based methods. The modified residual block is also proposed by\napplying modifications to the classic residual block to construct a\npatch-region-based baseline called WResNet. Equipping this baseline with the\ntwo proposed modules, we further propose Region-Adaptive Deformable Network\n(RADN). The experiment results on the NTIRE 2021 Perceptual Image Quality\nAssessment Challenge dataset show the superior performance of RADN, and the\nensemble approach won fourth place in the final testing phase of the challenge.\nCode is available at https://github.com/IIGROUP/RADN.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 13:47:20 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Shi", "Shuwei", ""], ["Bai", "Qingyan", ""], ["Cao", "Mingdeng", ""], ["Xia", "Weihao", ""], ["Wang", "Jiahao", ""], ["Chen", "Yifan", ""], ["Yang", "Yujiu", ""]]}, {"id": "2104.11600", "submitter": "Patrick Zschech", "authors": "Patrick Zschech, Jannis Walk, Kai Heinrich, Michael V\\\"ossing, Niklas\n  K\\\"uhl", "title": "A Picture is Worth a Collaboration: Accumulating Design Knowledge for\n  Computer-Vision-based Hybrid Intelligence Systems", "comments": "Preprint accepted for archival and presentation at the 29th European\n  Conference on Information Systems (ECIS 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Computer vision (CV) techniques try to mimic human capabilities of visual\nperception to support labor-intensive and time-consuming tasks like the\nrecognition and localization of critical objects. Nowadays, CV increasingly\nrelies on artificial intelligence (AI) to automatically extract useful\ninformation from images that can be utilized for decision support and business\nprocess automation. However, the focus of extant research is often exclusively\non technical aspects when designing AI-based CV systems while neglecting\nsocio-technical facets, such as trust, control, and autonomy. For this purpose,\nwe consider the design of such systems from a hybrid intelligence (HI)\nperspective and aim to derive prescriptive design knowledge for CV-based HI\nsystems. We apply a reflective, practice-inspired design science approach and\naccumulate design knowledge from six comprehensive CV projects. As a result, we\nidentify four design-related mechanisms (i.e., automation, signaling,\nmodification, and collaboration) that inform our derived meta-requirements and\ndesign principles. This can serve as a basis for further socio-technical\nresearch on CV-based HI systems.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 13:47:57 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Zschech", "Patrick", ""], ["Walk", "Jannis", ""], ["Heinrich", "Kai", ""], ["V\u00f6ssing", "Michael", ""], ["K\u00fchl", "Niklas", ""]]}, {"id": "2104.11619", "submitter": "Jose Luis Gomez", "authors": "Jose L. G\\'omez, Gabriel Villalonga, Antonio M. L\\'opez", "title": "Co-training for Deep Object Detection: Comparing Single-modal and\n  Multi-modal Approaches", "comments": null, "journal-ref": "special issue of Sensors (ISSN 1424-8220) \"Feature Papers in\n  Physical Sensors Section 2020\"", "doi": "10.3390/s21093185", "report-no": "sensors-1185064", "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Top-performing computer vision models are powered by convolutional neural\nnetworks (CNNs). Training an accurate CNN highly depends on both the raw sensor\ndata and their associated ground truth (GT). Collecting such GT is usually done\nthrough human labeling, which is time-consuming and does not scale as we wish.\nThis data labeling bottleneck may be intensified due to domain shifts among\nimage sensors, which could force per-sensor data labeling. In this paper, we\nfocus on the use of co-training, a semi-supervised learning (SSL) method, for\nobtaining self-labeled object bounding boxes (BBs), i.e., the GT to train deep\nobject detectors. In particular, we assess the goodness of multi-modal\nco-training by relying on two different views of an image, namely, appearance\n(RGB) and estimated depth (D). Moreover, we compare appearance-based\nsingle-modal co-training with multi-modal. Our results suggest that in a\nstandard SSL setting (no domain shift, a few human-labeled data) and under\nvirtual-to-real domain shift (many virtual-world labeled data, no human-labeled\ndata) multi-modal co-training outperforms single-modal. In the latter case, by\nperforming GAN-based domain translation both co-training modalities are on\npair; at least, when using an off-the-shelf depth estimation model not\nspecifically trained on the translated images.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 14:13:59 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["G\u00f3mez", "Jose L.", ""], ["Villalonga", "Gabriel", ""], ["L\u00f3pez", "Antonio M.", ""]]}, {"id": "2104.11620", "submitter": "Nibaran Das", "authors": "Bodhisatwa Mandal, Swarnendu Ghosh, Teresa Gon\\c{c}alves, Paulo\n  Quaresma, Mita Nasipuri, Nibaran Das", "title": "GuideBP: Guiding Backpropagation Through Weaker Pathways of Parallel\n  Logits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks often generate multiple logits and use simple\ntechniques like addition or averaging for loss computation. But this allows\ngradients to be distributed equally among all paths. The proposed approach\nguides the gradients of backpropagation along weakest concept representations.\nA weakness scores defines the class specific performance of individual pathways\nwhich is then used to create a logit that would guide gradients along the\nweakest pathways. The proposed approach has been shown to perform better than\ntraditional column merging techniques and can be used in several application\nscenarios. Not only can the proposed model be used as an efficient technique\nfor training multiple instances of a model parallely, but also CNNs with\nmultiple output branches have been shown to perform better with the proposed\nupgrade. Various experiments establish the flexibility of the learning\ntechnique which is simple yet effective in various multi-objective scenarios\nboth empirically and statistically.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 14:14:00 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Mandal", "Bodhisatwa", ""], ["Ghosh", "Swarnendu", ""], ["Gon\u00e7alves", "Teresa", ""], ["Quaresma", "Paulo", ""], ["Nasipuri", "Mita", ""], ["Das", "Nibaran", ""]]}, {"id": "2104.11637", "submitter": "Zahra Gharaee", "authors": "Zahra Gharaee", "title": "Online recognition of unsegmented actions with hierarchical SOM\n  architecture", "comments": null, "journal-ref": "Cogn Process 22, 77-91 (2021)", "doi": "10.1007/s10339-020-00986-4", "report-no": null, "categories": "cs.CV cs.AI cs.HC cs.LG cs.RO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Automatic recognition of an online series of unsegmented actions requires a\nmethod for segmentation that determines when an action starts and when it ends.\nIn this paper, a novel approach for recognizing unsegmented actions in online\ntest experiments is proposed. The method uses self-organizing neural networks\nto build a three-layer cognitive architecture. The unique features of an action\nsequence are represented as a series of elicited key activations by the\nfirst-layer self-organizing map. An average length of a key activation vector\nis calculated for all action sequences in a training set and adjusted in\nlearning trials to generate input patterns to the second-layer self-organizing\nmap. The pattern vectors are clustered in the second layer, and the clusters\nare then labeled by an action identity in the third layer neural network. The\nexperiment results show that although the performance drops slightly in online\nexperiments compared to the offline tests, the ability of the proposed\narchitecture to deal with the unsegmented action sequences as well as the\nonline performance makes the system more plausible and practical in real-case\nscenarios.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 14:41:46 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Gharaee", "Zahra", ""]]}, {"id": "2104.11653", "submitter": "Siham Tabik", "authors": "Roberto Olmos, Siham Tabik, Francisco Perez-Hernandez, Alberto Lamas,\n  Francisco Herrera", "title": "MULTICAST: MULTI Confirmation-level Alarm SysTem based on CNN and LSTM\n  to mitigate false alarms for handgun detection in video-surveillance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the constant advances in computer vision, integrating modern\nsingle-image detectors in real-time handgun alarm systems in video-surveillance\nis still debatable. Using such detectors still implies a high number of false\nalarms and false negatives. In this context, most existent studies select one\nof the latest single-image detectors and train it on a better dataset or use\nsome pre-processing, post-processing or data-fusion approach to further reduce\nfalse alarms. However, none of these works tried to exploit the temporal\ninformation present in the videos to mitigate false detections. This paper\npresents a new system, called MULTI Confirmation-level Alarm SysTem based on\nConvolutional Neural Networks (CNN) and Long Short Term Memory networks (LSTM)\n(MULTICAST), that leverages not only the spacial information but also the\ntemporal information existent in the videos for a more reliable handgun\ndetection. MULTICAST consists of three stages, i) a handgun detection stage,\nii) a CNN-based spacial confirmation stage and iii) LSTM-based temporal\nconfirmation stage. The temporal confirmation stage uses the positions of the\ndetected handgun in previous instants to predict its trajectory in the next\nframe. Our experiments show that MULTICAST reduces by 80% the number of false\nalarms with respect to Faster R-CNN based-single-image detector, which makes it\nmore useful in providing more effective and rapid security responses.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 15:07:58 GMT"}, {"version": "v2", "created": "Mon, 3 May 2021 07:14:19 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Olmos", "Roberto", ""], ["Tabik", "Siham", ""], ["Perez-Hernandez", "Francisco", ""], ["Lamas", "Alberto", ""], ["Herrera", "Francisco", ""]]}, {"id": "2104.11677", "submitter": "Arsalan Tahir", "authors": "Arsalan Tahir, Muhammad Adil and Arslan Ali", "title": "Rapid Detection of Aircrafts in Satellite Imagery based on Deep Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Object detection is one of the fundamental objectives in Applied Computer\nVision. In some of the applications, object detection becomes very challenging\nsuch as in the case of satellite image processing. Satellite image processing\nhas remained the focus of researchers in domains of Precision Agriculture,\nClimate Change, Disaster Management, etc. Therefore, object detection in\nsatellite imagery is one of the most researched problems in this domain. This\npaper focuses on aircraft detection. in satellite imagery using deep learning\ntechniques. In this paper, we used YOLO deep learning framework for aircraft\ndetection. This method uses satellite images collected by different sources as\nlearning for the model to perform detection. Object detection in satellite\nimages is mostly complex because objects have many variations, types, poses,\nsizes, complex and dense background. YOLO has some limitations for small size\nobjects (less than$\\sim$32 pixels per object), therefore we upsample the\nprediction grid to reduce the coarseness of the model and to accurately detect\nthe densely clustered objects. The improved model shows good accuracy and\nperformance on different unknown images having small, rotating, and dense\nobjects to meet the requirements in real-time.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 18:13:16 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Tahir", "Arsalan", ""], ["Adil", "Muhammad", ""], ["Ali", "Arslan", ""]]}, {"id": "2104.11691", "submitter": "Julia Rosenzweig", "authors": "Julia Rosenzweig, Joachim Sicking, Sebastian Houben, Michael Mock,\n  Maram Akila", "title": "Patch Shortcuts: Interpretable Proxy Models Efficiently Find Black-Box\n  Vulnerabilities", "comments": "Under IEEE Copyright; accepted at the SAIAD (Safe Artificial\n  Intelligence for Automated Driving) Workshop at CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important pillar for safe machine learning (ML) is the systematic\nmitigation of weaknesses in neural networks to afford their deployment in\ncritical applications. An ubiquitous class of safety risks are learned\nshortcuts, i.e. spurious correlations a network exploits for its decisions that\nhave no semantic connection to the actual task. Networks relying on such\nshortcuts bear the risk of not generalizing well to unseen inputs.\nExplainability methods help to uncover such network vulnerabilities. However,\nmany of these techniques are not directly applicable if access to the network\nis constrained, in so-called black-box setups. These setups are prevalent when\nusing third-party ML components. To address this constraint, we present an\napproach to detect learned shortcuts using an interpretable-by-design network\nas a proxy to the black-box model of interest. Leveraging the proxy's\nguarantees on introspection we automatically extract candidates for learned\nshortcuts. Their transferability to the black box is validated in a systematic\nfashion. Concretely, as proxy model we choose a BagNet, which bases its\ndecisions purely on local image patches. We demonstrate on the autonomous\ndriving dataset A2D2 that extracted patch shortcuts significantly influence the\nblack box model. By efficiently identifying such patch-based vulnerabilities,\nwe contribute to safer ML models.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 05:44:40 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Rosenzweig", "Julia", ""], ["Sicking", "Joachim", ""], ["Houben", "Sebastian", ""], ["Mock", "Michael", ""], ["Akila", "Maram", ""]]}, {"id": "2104.11692", "submitter": "Giuseppe Pastore", "authors": "Giuseppe Pastore, Fabio Cermelli, Yongqin Xian, Massimiliano Mancini,\n  Zeynep Akata, Barbara Caputo", "title": "A Closer Look at Self-training for Zero-Label Semantic Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Being able to segment unseen classes not observed during training is an\nimportant technical challenge in deep learning, because of its potential to\nreduce the expensive annotation required for semantic segmentation. Prior\nzero-label semantic segmentation works approach this task by learning\nvisual-semantic embeddings or generative models. However, they are prone to\noverfitting on the seen classes because there is no training signal for them.\nIn this paper, we study the challenging generalized zero-label semantic\nsegmentation task where the model has to segment both seen and unseen classes\nat test time. We assume that pixels of unseen classes could be present in the\ntraining images but without being annotated. Our idea is to capture the latent\ninformation on unseen classes by supervising the model with self-produced\npseudo-labels for unlabeled pixels. We propose a consistency regularizer to\nfilter out noisy pseudo-labels by taking the intersections of the pseudo-labels\ngenerated from different augmentations of the same image. Our framework\ngenerates pseudo-labels and then retrain the model with human-annotated and\npseudo-labelled data. This procedure is repeated for several iterations. As a\nresult, our approach achieves the new state-of-the-art on PascalVOC12 and\nCOCO-stuff datasets in the challenging generalized zero-label semantic\nsegmentation setting, surpassing other existing methods addressing this task\nwith more complex strategies.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 14:34:33 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Pastore", "Giuseppe", ""], ["Cermelli", "Fabio", ""], ["Xian", "Yongqin", ""], ["Mancini", "Massimiliano", ""], ["Akata", "Zeynep", ""], ["Caputo", "Barbara", ""]]}, {"id": "2104.11693", "submitter": "Yiming Zhao", "authors": "Yiming Zhao, Xinming Huang and Ziming Zhang", "title": "Deep Lucas-Kanade Homography for Multimodal Image Alignment", "comments": "Accepted by CVPR2021, codelink:\n  https://github.com/placeforyiming/CVPR21-Deep-Lucas-Kanade-Homography", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating homography to align image pairs captured by different sensors or\nimage pairs with large appearance changes is an important and general challenge\nfor many computer vision applications. In contrast to others, we propose a\ngeneric solution to pixel-wise align multimodal image pairs by extending the\ntraditional Lucas-Kanade algorithm with networks. The key contribution in our\nmethod is how we construct feature maps, named as deep Lucas-Kanade feature map\n(DLKFM). The learned DLKFM can spontaneously recognize invariant features under\nvarious appearance-changing conditions. It also has two nice properties for the\nLucas-Kanade algorithm: (1) The template feature map keeps brightness\nconsistency with the input feature map, thus the color difference is very small\nwhile they are well-aligned. (2) The Lucas-Kanade objective function built on\nDLKFM has a smooth landscape around ground truth homography parameters, so the\niterative solution of the Lucas-Kanade can easily converge to the ground truth.\nWith those properties, directly updating the Lucas-Kanade algorithm on our\nfeature maps will precisely align image pairs with large appearance changes. We\nshare the datasets, code, and demo video online.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 04:11:29 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Zhao", "Yiming", ""], ["Huang", "Xinming", ""], ["Zhang", "Ziming", ""]]}, {"id": "2104.11712", "submitter": "Necati Cihan Camgoz Dr.", "authors": "Tao Jiang, Necati Cihan Camgoz, Richard Bowden", "title": "Skeletor: Skeletal Transformers for Robust Body-Pose Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Predicting 3D human pose from a single monoscopic video can be highly\nchallenging due to factors such as low resolution, motion blur and occlusion,\nin addition to the fundamental ambiguity in estimating 3D from 2D. Approaches\nthat directly regress the 3D pose from independent images can be particularly\nsusceptible to these factors and result in jitter, noise and/or inconsistencies\nin skeletal estimation. Much of which can be overcome if the temporal evolution\nof the scene and skeleton are taken into account. However, rather than tracking\nbody parts and trying to temporally smooth them, we propose a novel transformer\nbased network that can learn a distribution over both pose and motion in an\nunsupervised fashion. We call our approach Skeletor. Skeletor overcomes\ninaccuracies in detection and corrects partial or entire skeleton corruption.\nSkeletor uses strong priors learn from on 25 million frames to correct skeleton\nsequences smoothly and consistently. Skeletor can achieve this as it implicitly\nlearns the spatio-temporal context of human motion via a transformer based\nneural network. Extensive experiments show that Skeletor achieves improved\nperformance on 3D human pose estimation and further provides benefits for\ndownstream tasks such as sign language translation.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 16:56:42 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Jiang", "Tao", ""], ["Camgoz", "Necati Cihan", ""], ["Bowden", "Richard", ""]]}, {"id": "2104.11721", "submitter": "Sander Klomp", "authors": "Sander R. Klomp (1 and 2), Matthew van Rijn (3), Rob G.J. Wijnhoven\n  (2), Cees G.M. Snoek (3), Peter H.N. de With (1) ((1) Eindhoven University of\n  Technology, (2) ViNotion B.V., (3) University of Amsterdam)", "title": "Safe Fakes: Evaluating Face Anonymizers for Face Detectors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Since the introduction of the GDPR and CCPA legislation, both public and\nprivate facial image datasets are increasingly scrutinized. Several datasets\nhave been taken offline completely and some have been anonymized. However, it\nis unclear how anonymization impacts face detection performance. To our\nknowledge, this paper presents the first empirical study on the effect of image\nanonymization on supervised training of face detectors. We compare conventional\nface anonymizers with three state-of-the-art Generative Adversarial\nNetwork-based (GAN) methods, by training an off-the-shelf face detector on\nanonymized data. Our experiments investigate the suitability of anonymization\nmethods for maintaining face detector performance, the effect of detectors\novertraining on anonymization artefacts, dataset size for training an\nanonymizer, and the effect of training time of anonymization GANs. A final\nexperiment investigates the correlation between common GAN evaluation metrics\nand the performance of a trained face detector. Although all tested\nanonymization methods lower the performance of trained face detectors, faces\nanonymized using GANs cause far smaller performance degradation than\nconventional methods. As the most important finding, the best-performing GAN,\nDeepPrivacy, removes identifiable faces for a face detector trained on\nanonymized data, resulting in a modest decrease from 91.0 to 88.3 mAP. In the\nlast few years, there have been rapid improvements in realism of GAN-generated\nfaces. We expect that further progression in GAN research will allow the use of\nDeep Fake technology for privacy-preserving Safe Fakes, without any performance\ndegradation for training face detectors.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 17:16:23 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Klomp", "Sander R.", "", "1 and 2"], ["van Rijn", "Matthew", ""], ["Wijnhoven", "Rob G. J.", ""], ["Snoek", "Cees G. M.", ""], ["de With", "Peter H. N.", ""]]}, {"id": "2104.11746", "submitter": "Xinyu Li", "authors": "Xinyu Li, Yanyi Zhang, Chunhui Liu, Bing Shuai, Yi Zhu, Biagio\n  Brattoli, Hao Chen, Ivan Marsic, Joseph Tighe", "title": "VidTr: Video Transformer Without Convolutions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce Video Transformer (VidTr) with separable-attention for video\nclassification. Comparing with commonly used 3D networks, VidTr is able to\naggregate spatio-temporal information via stacked attentions and provide better\nperformance with higher efficiency. We first introduce the vanilla video\ntransformer and show that transformer module is able to perform spatio-temporal\nmodeling from raw pixels, but with heavy memory usage. We then present VidTr\nwhich reduces the memory cost by 3.3$\\times$ while keeping the same\nperformance. To further compact the model, we propose the standard deviation\nbased topK pooling attention, which reduces the computation by dropping\nnon-informative features. VidTr achieves state-of-the-art performance on five\ncommonly used dataset with lower computational requirement, showing both the\nefficiency and effectiveness of our design. Finally, error analysis and\nvisualization show that VidTr is especially good at predicting actions that\nrequire long-term temporal reasoning. The code and pre-trained weights will be\nreleased.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 17:59:01 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Li", "Xinyu", ""], ["Zhang", "Yanyi", ""], ["Liu", "Chunhui", ""], ["Shuai", "Bing", ""], ["Zhu", "Yi", ""], ["Brattoli", "Biagio", ""], ["Chen", "Hao", ""], ["Marsic", "Ivan", ""], ["Tighe", "Joseph", ""]]}, {"id": "2104.11747", "submitter": "Jan-Nico Zaech", "authors": "Jan-Nico Zaech, Dengxin Dai, Alexander Liniger, Martin Danelljan, Luc\n  Van Gool", "title": "Learnable Online Graph Representations for 3D Multi-Object Tracking", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tracking of objects in 3D is a fundamental task in computer vision that finds\nuse in a wide range of applications such as autonomous driving, robotics or\naugmented reality. Most recent approaches for 3D multi object tracking (MOT)\nfrom LIDAR use object dynamics together with a set of handcrafted features to\nmatch detections of objects. However, manually designing such features and\nheuristics is cumbersome and often leads to suboptimal performance. In this\nwork, we instead strive towards a unified and learning based approach to the 3D\nMOT problem. We design a graph structure to jointly process detection and track\nstates in an online manner. To this end, we employ a Neural Message Passing\nnetwork for data association that is fully trainable. Our approach provides a\nnatural way for track initialization and handling of false positive detections,\nwhile significantly improving track stability. We show the merit of the\nproposed approach on the publicly available nuScenes dataset by achieving\nstate-of-the-art performance of 65.6% AMOTA and 58% fewer ID-switches.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 17:59:28 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Zaech", "Jan-Nico", ""], ["Dai", "Dengxin", ""], ["Liniger", "Alexander", ""], ["Danelljan", "Martin", ""], ["Van Gool", "Luc", ""]]}, {"id": "2104.11776", "submitter": "Pablo Martinez-Gonzalez", "authors": "Pablo Martinez-Gonzalez, Sergiu Oprea, John Alejandro Castro-Vargas,\n  Alberto Garcia-Garcia, Sergio Orts-Escolano, Jose Garcia-Rodriguez and Markus\n  Vincze", "title": "UnrealROX+: An Improved Tool for Acquiring Synthetic Data from Virtual\n  3D Environments", "comments": "Accepted at International Joint Conference on Neural Networks (IJCNN)\n  2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Synthetic data generation has become essential in last years for feeding\ndata-driven algorithms, which surpassed traditional techniques performance in\nalmost every computer vision problem. Gathering and labelling the amount of\ndata needed for these data-hungry models in the real world may become\nunfeasible and error-prone, while synthetic data give us the possibility of\ngenerating huge amounts of data with pixel-perfect annotations. However, most\nsynthetic datasets lack from enough realism in their rendered images. In that\ncontext UnrealROX generation tool was presented in 2019, allowing to generate\nhighly realistic data, at high resolutions and framerates, with an efficient\npipeline based on Unreal Engine, a cutting-edge videogame engine. UnrealROX\nenabled robotic vision researchers to generate realistic and visually plausible\ndata with full ground truth for a wide variety of problems such as class and\ninstance semantic segmentation, object detection, depth estimation, visual\ngrasping, and navigation. Nevertheless, its workflow was very tied to generate\nimage sequences from a robotic on-board camera, making hard to generate data\nfor other purposes. In this work, we present UnrealROX+, an improved version of\nUnrealROX where its decoupled and easy-to-use data acquisition system allows to\nquickly design and generate data in a much more flexible and customizable way.\nMoreover, it is packaged as an Unreal plug-in, which makes it more comfortable\nto use with already existing Unreal projects, and it also includes new features\nsuch as generating albedo or a Python API for interacting with the virtual\nenvironment from Deep Learning frameworks.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 18:45:42 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Martinez-Gonzalez", "Pablo", ""], ["Oprea", "Sergiu", ""], ["Castro-Vargas", "John Alejandro", ""], ["Garcia-Garcia", "Alberto", ""], ["Orts-Escolano", "Sergio", ""], ["Garcia-Rodriguez", "Jose", ""], ["Vincze", "Markus", ""]]}, {"id": "2104.11785", "submitter": "Marco Giordani", "authors": "Valentina Rossi, Paolo Testolina, Marco Giordani, Michele Zorzi", "title": "On the Role of Sensor Fusion for Object Detection in Future Vehicular\n  Networks", "comments": "This paper has been accepted for presentation at the Joint European\n  Conference on Networks and Communications & 6G Summit (EuCNC/6G Summit). 6\n  pages, 6 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fully autonomous driving systems require fast detection and recognition of\nsensitive objects in the environment. In this context, intelligent vehicles\nshould share their sensor data with computing platforms and/or other vehicles,\nto detect objects beyond their own sensors' fields of view. However, the\nresulting huge volumes of data to be exchanged can be challenging to handle for\nstandard communication technologies. In this paper, we evaluate how using a\ncombination of different sensors affects the detection of the environment in\nwhich the vehicles move and operate. The final objective is to identify the\noptimal setup that would minimize the amount of data to be distributed over the\nchannel, with negligible degradation in terms of object detection accuracy. To\nthis aim, we extend an already available object detection algorithm so that it\ncan consider, as an input, camera images, LiDAR point clouds, or a combination\nof the two, and compare the accuracy performance of the different approaches\nusing two realistic datasets. Our results show that, although sensor fusion\nalways achieves more accurate detections, LiDAR only inputs can obtain similar\nresults for large objects while mitigating the burden on the channel.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 18:58:37 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Rossi", "Valentina", ""], ["Testolina", "Paolo", ""], ["Giordani", "Marco", ""], ["Zorzi", "Michele", ""]]}, {"id": "2104.11797", "submitter": "Gabriel Eilertsen", "authors": "Gabriel Eilertsen, Apostolia Tsirikoglou, Claes Lundstr\\\"om, Jonas\n  Unger", "title": "Ensembles of GANs for synthetic training data generation", "comments": "ICLR 2021 workshop on Synthetic Data Generation: Quality, Privacy,\n  Bias", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Insufficient training data is a major bottleneck for most deep learning\npractices, not least in medical imaging where data is difficult to collect and\npublicly available datasets are scarce due to ethics and privacy. This work\ninvestigates the use of synthetic images, created by generative adversarial\nnetworks (GANs), as the only source of training data. We demonstrate that for\nthis application, it is of great importance to make use of multiple GANs to\nimprove the diversity of the generated data, i.e. to sufficiently cover the\ndata distribution. While a single GAN can generate seemingly diverse image\ncontent, training on this data in most cases lead to severe over-fitting. We\ntest the impact of ensembled GANs on synthetic 2D data as well as common image\ndatasets (SVHN and CIFAR-10), and using both DCGANs and progressively growing\nGANs. As a specific use case, we focus on synthesizing digital pathology\npatches to provide anonymized training data.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 19:38:48 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Eilertsen", "Gabriel", ""], ["Tsirikoglou", "Apostolia", ""], ["Lundstr\u00f6m", "Claes", ""], ["Unger", "Jonas", ""]]}, {"id": "2104.11832", "submitter": "Zhe Gan", "authors": "Zhe Gan, Yen-Chun Chen, Linjie Li, Tianlong Chen, Yu Cheng, Shuohang\n  Wang, Jingjing Liu", "title": "Playing Lottery Tickets with Vision and Language", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Large-scale transformer-based pre-training has recently revolutionized\nvision-and-language (V+L) research. Models such as LXMERT, ViLBERT and UNITER\nhave significantly lifted the state of the art over a wide range of V+L tasks.\nHowever, the large number of parameters in such models hinders their\napplication in practice. In parallel, work on the lottery ticket hypothesis has\nshown that deep neural networks contain small matching subnetworks that can\nachieve on par or even better performance than the dense networks when trained\nin isolation. In this work, we perform the first empirical study to assess\nwhether such trainable subnetworks also exist in pre-trained V+L models. We use\nUNITER, one of the best-performing V+L models, as the testbed, and consolidate\n7 representative V+L tasks for experiments, including visual question\nanswering, visual commonsense reasoning, visual entailment, referring\nexpression comprehension, image-text retrieval, GQA, and NLVR$^2$. Through\ncomprehensive analysis, we summarize our main findings as follows. ($i$) It is\ndifficult to find subnetworks (i.e., the tickets) that strictly match the\nperformance of the full UNITER model. However, it is encouraging to confirm\nthat we can find \"relaxed\" winning tickets at 50%-70% sparsity that maintain\n99% of the full accuracy. ($ii$) Subnetworks found by task-specific pruning\ntransfer reasonably well to the other tasks, while those found on the\npre-training tasks at 60%/70% sparsity transfer universally, matching 98%/96%\nof the full accuracy on average over all the tasks. ($iii$) Adversarial\ntraining can be further used to enhance the performance of the found lottery\ntickets.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 22:24:33 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Gan", "Zhe", ""], ["Chen", "Yen-Chun", ""], ["Li", "Linjie", ""], ["Chen", "Tianlong", ""], ["Cheng", "Yu", ""], ["Wang", "Shuohang", ""], ["Liu", "Jingjing", ""]]}, {"id": "2104.11849", "submitter": "Stone Yun", "authors": "Stone Yun and Alexander Wong", "title": "Do All MobileNets Quantize Poorly? Gaining Insights into the Effect of\n  Quantization on Depthwise Separable Convolutional Networks Through the Eyes\n  of Multi-scale Distributional Dynamics", "comments": "Accepted for publication in Mobile AI (MAI) Workshop 2021 at CVPR", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As the \"Mobile AI\" revolution continues to grow, so does the need to\nunderstand the behaviour of edge-deployed deep neural networks. In particular,\nMobileNets are the go-to family of deep convolutional neural networks (CNN) for\nmobile. However, they often have significant accuracy degradation under\npost-training quantization. While studies have introduced quantization-aware\ntraining and other methods to tackle this challenge, there is limited\nunderstanding into why MobileNets (and potentially depthwise-separable CNNs\n(DWSCNN) in general) quantize so poorly compared to other CNN architectures.\nMotivated to gain deeper insights into this phenomenon, we take a different\nstrategy and study the multi-scale distributional dynamics of MobileNet-V1, a\nset of smaller DWSCNNs, and regular CNNs. Specifically, we investigate the\nimpact of quantization on the weight and activation distributional dynamics as\ninformation propagates from layer to layer, as well as overall changes in\ndistributional dynamics at the network level. This fine-grained analysis\nrevealed significant dynamic range fluctuations and a \"distributional mismatch\"\nbetween channelwise and layerwise distributions in DWSCNNs that lead to\nincreasing quantized degradation and distributional shift during information\npropagation. Furthermore, analysis of the activation quantization errors show\nthat there is greater quantization error accumulation in DWSCNN compared to\nregular CNNs. The hope is that such insights can lead to innovative strategies\nfor reducing such distributional dynamics changes and improve post-training\nquantization for mobile.\n", "versions": [{"version": "v1", "created": "Sat, 24 Apr 2021 01:28:29 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Yun", "Stone", ""], ["Wong", "Alexander", ""]]}, {"id": "2104.11854", "submitter": "Mohsen Zand", "authors": "Mohsen Zand, Ali Etemad, and Michael Greenspan", "title": "Oriented Bounding Boxes for Small and Freely Rotated Objects", "comments": "IEEE Transactions on Geoscience and Remote Sensing, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel object detection method is presented that handles freely rotated\nobjects of arbitrary sizes, including tiny objects as small as $2\\times 2$\npixels. Such tiny objects appear frequently in remotely sensed images, and\npresent a challenge to recent object detection algorithms. More importantly,\ncurrent object detection methods have been designed originally to accommodate\naxis-aligned bounding box detection, and therefore fail to accurately localize\noriented boxes that best describe freely rotated objects. In contrast, the\nproposed CNN-based approach uses potential pixel information at multiple scale\nlevels without the need for any external resources, such as anchor boxes.The\nmethod encodes the precise location and orientation of features of the target\nobjects at grid cell locations. Unlike existing methods which regress the\nbounding box location and dimension,the proposed method learns all the required\ninformation by classification, which has the added benefit of enabling oriented\nbounding box detection without any extra computation. It thus infers the\nbounding boxes only at inference time by finding the minimum surrounding box\nfor every set of the same predicted class labels. Moreover, a\nrotation-invariant feature representation is applied to each scale, which\nimposes a regularization constraint to enforce covering the 360 degree range of\nin-plane rotation of the training samples to share similar features.\nEvaluations on the xView and DOTA datasets show that the proposed method\nuniformly improves performance over existing state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sat, 24 Apr 2021 02:04:49 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Zand", "Mohsen", ""], ["Etemad", "Ali", ""], ["Greenspan", "Michael", ""]]}, {"id": "2104.11861", "submitter": "Lukasz Korycki", "authors": "{\\L}ukasz Korycki, Bartosz Krawczyk", "title": "Class-Incremental Experience Replay for Continual Learning under Concept\n  Drift", "comments": "IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\n  Workshops, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern machine learning systems need to be able to cope with constantly\narriving and changing data. Two main areas of research dealing with such\nscenarios are continual learning and data stream mining. Continual learning\nfocuses on accumulating knowledge and avoiding forgetting, assuming information\nonce learned should be stored. Data stream mining focuses on adaptation to\nconcept drift and discarding outdated information, assuming that only the most\nrecent data is relevant. While these two areas are mainly being developed in\nseparation, they offer complementary views on the problem of learning from\ndynamic data. There is a need for unifying them, by offering architectures\ncapable of both learning and storing new information, as well as revisiting and\nadapting to changes in previously seen concepts. We propose a novel continual\nlearning approach that can handle both tasks. Our experience replay method is\nfueled by a centroid-driven memory storing diverse instances of incrementally\narriving classes. This is enhanced with a reactive subspace buffer that tracks\nconcept drift occurrences in previously seen classes and adapts clusters\naccordingly. The proposed architecture is thus capable of both remembering\nvalid and forgetting outdated information, offering a holistic framework for\ncontinual learning under concept drift.\n", "versions": [{"version": "v1", "created": "Sat, 24 Apr 2021 02:36:38 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Korycki", "\u0141ukasz", ""], ["Krawczyk", "Bartosz", ""]]}, {"id": "2104.11883", "submitter": "Yuxin Zhang", "authors": "Yuxin Zhang, Mingbao Lin, Chia-Wen Lin, Jie Chen, Feiyue Huang,\n  Yongjian Wu, Yonghong Tian, Rongrong Ji", "title": "Channel Pruning in a White Box for Efficient Image Classification", "comments": "10 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Channel Pruning has been long studied to compress CNNs for efficient image\nclassification. Prior works implement channel pruning in an unexplainable\nmanner, which tends to reduce the final classification errors while failing to\nconsider the internal influence of each channel. In this paper, we conduct\nchannel pruning in a white box. Through deep visualization of feature maps\nactivated by different channels, we observe that different channels have a\nvarying contribution to different categories in image classification. Inspired\nby this, we choose to preserve channels contributing to most categories.\nSpecifically, to model the contribution of each channel to differentiating\ncategories, we develop a class-wise mask for each channel, implemented in a\ndynamic training manner w.r.t. the input image's category. On the basis of the\nlearned class-wise mask, we perform a global voting mechanism to remove\nchannels with less category discrimination. Lastly, a fine-tuning process is\nconducted to recover the performance of the pruned model. To our best\nknowledge, it is the first time that CNN interpretability theory is considered\nto guide channel pruning. Extensive experiments on representative image\nclassification tasks demonstrate the superiority of our White-Box over many\nstate-of-the-arts. For instance, on CIFAR-10, it reduces 65.23% FLOPs with even\n0.62% accuracy improvement for ResNet-110. On ILSVRC-2012, White-Box achieves a\n45.6% FLOPs reduction with only a small loss of 0.83% in the top-1 accuracy for\nResNet-50.\n", "versions": [{"version": "v1", "created": "Sat, 24 Apr 2021 04:59:03 GMT"}, {"version": "v2", "created": "Sat, 26 Jun 2021 14:17:34 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Zhang", "Yuxin", ""], ["Lin", "Mingbao", ""], ["Lin", "Chia-Wen", ""], ["Chen", "Jie", ""], ["Huang", "Feiyue", ""], ["Wu", "Yongjian", ""], ["Tian", "Yonghong", ""], ["Ji", "Rongrong", ""]]}, {"id": "2104.11892", "submitter": "Mohammad Samar Ansari", "authors": "Syed Sahil Abbas Zaidi, Mohammad Samar Ansari, Asra Aslam, Nadia\n  Kanwal, Mamoona Asghar, and Brian Lee", "title": "A Survey of Modern Deep Learning based Object Detection Models", "comments": "Preprint submitted to IET Computer Vision", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Object Detection is the task of classification and localization of objects in\nan image or video. It has gained prominence in recent years due to its\nwidespread applications. This article surveys recent developments in deep\nlearning based object detectors. Concise overview of benchmark datasets and\nevaluation metrics used in detection is also provided along with some of the\nprominent backbone architectures used in recognition tasks. It also covers\ncontemporary lightweight classification models used on edge devices. Lastly, we\ncompare the performances of these architectures on multiple metrics.\n", "versions": [{"version": "v1", "created": "Sat, 24 Apr 2021 06:33:54 GMT"}, {"version": "v2", "created": "Wed, 12 May 2021 16:45:20 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Zaidi", "Syed Sahil Abbas", ""], ["Ansari", "Mohammad Samar", ""], ["Aslam", "Asra", ""], ["Kanwal", "Nadia", ""], ["Asghar", "Mamoona", ""], ["Lee", "Brian", ""]]}, {"id": "2104.11896", "submitter": "Tianrui Guan", "authors": "Tianrui Guan, Jun Wang, Shiyi Lan, Rohan Chandra, Zuxuan Wu, Larry\n  Davis, Dinesh Manocha", "title": "M3DeTR: Multi-representation, Multi-scale, Mutual-relation 3D Object\n  Detection with Transformers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel architecture for 3D object detection, M3DeTR, which\ncombines different point cloud representations (raw, voxels, bird-eye view)\nwith different feature scales based on multi-scale feature pyramids. M3DeTR is\nthe first approach that unifies multiple point cloud representations, feature\nscales, as well as models mutual relationships between point clouds\nsimultaneously using transformers. We perform extensive ablation experiments\nthat highlight the benefits of fusing representation and scale, and modeling\nthe relationships. Our method achieves state-of-the-art performance on the\nKITTI 3D object detection dataset and Waymo Open Dataset. Results show that\nM3DeTR improves the baseline significantly by 1.48% mAP for all classes on\nWaymo Open Dataset. In particular, our approach ranks 1st on the well-known\nKITTI 3D Detection Benchmark for both car and cyclist classes, and ranks 1st on\nWaymo Open Dataset with single frame point cloud input.\n", "versions": [{"version": "v1", "created": "Sat, 24 Apr 2021 06:48:23 GMT"}, {"version": "v2", "created": "Fri, 30 Apr 2021 17:53:25 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Guan", "Tianrui", ""], ["Wang", "Jun", ""], ["Lan", "Shiyi", ""], ["Chandra", "Rohan", ""], ["Wu", "Zuxuan", ""], ["Davis", "Larry", ""], ["Manocha", "Dinesh", ""]]}, {"id": "2104.11904", "submitter": "Yanling Miao", "authors": "Qi Wang, Yanling Miao, Mulin Chen, Xuelong Li", "title": "Spatial-Spectral Clustering with Anchor Graph for Hyperspectral Image", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hyperspectral image (HSI) clustering, which aims at dividing hyperspectral\npixels into clusters, has drawn significant attention in practical\napplications. Recently, many graph-based clustering methods, which construct an\nadjacent graph to model the data relationship, have shown dominant performance.\nHowever, the high dimensionality of HSI data makes it hard to construct the\npairwise adjacent graph. Besides, abundant spatial structures are often\noverlooked during the clustering procedure. In order to better handle the high\ndimensionality problem and preserve the spatial structures, this paper proposes\na novel unsupervised approach called spatial-spectral clustering with anchor\ngraph (SSCAG) for HSI data clustering. The SSCAG has the following\ncontributions: 1) the anchor graph-based strategy is used to construct a\ntractable large graph for HSI data, which effectively exploits all data points\nand reduces the computational complexity; 2) a new similarity metric is\npresented to embed the spatial-spectral information into the combined adjacent\ngraph, which can mine the intrinsic property structure of HSI data; 3) an\neffective neighbors assignment strategy is adopted in the optimization, which\nperforms the singular value decomposition (SVD) on the adjacent graph to get\nsolutions efficiently. Extensive experiments on three public HSI datasets show\nthat the proposed SSCAG is competitive against the state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Sat, 24 Apr 2021 08:09:27 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Wang", "Qi", ""], ["Miao", "Yanling", ""], ["Chen", "Mulin", ""], ["Li", "Xuelong", ""]]}, {"id": "2104.11914", "submitter": "Natalia D\\'iaz-Rodr\\'iguez PhD", "authors": "Natalia D\\'iaz-Rodr\\'iguez, Alberto Lamas, Jules Sanchez, Gianni\n  Franchi, Ivan Donadello, Siham Tabik, David Filliat, Policarpo Cruz, Rosana\n  Montes, Francisco Herrera", "title": "EXplainable Neural-Symbolic Learning (X-NeSyL) methodology to fuse deep\n  learning representations with expert knowledge graphs: the MonuMAI cultural\n  heritage use case", "comments": "Under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.SC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The latest Deep Learning (DL) models for detection and classification have\nachieved an unprecedented performance over classical machine learning\nalgorithms. However, DL models are black-box methods hard to debug, interpret,\nand certify. DL alone cannot provide explanations that can be validated by a\nnon technical audience. In contrast, symbolic AI systems that convert concepts\ninto rules or symbols -- such as knowledge graphs -- are easier to explain.\nHowever, they present lower generalisation and scaling capabilities. A very\nimportant challenge is to fuse DL representations with expert knowledge. One\nway to address this challenge, as well as the performance-explainability\ntrade-off is by leveraging the best of both streams without obviating domain\nexpert knowledge. We tackle such problem by considering the symbolic knowledge\nis expressed in form of a domain expert knowledge graph. We present the\neXplainable Neural-symbolic learning (X-NeSyL) methodology, designed to learn\nboth symbolic and deep representations, together with an explainability metric\nto assess the level of alignment of machine and human expert explanations. The\nultimate objective is to fuse DL representations with expert domain knowledge\nduring the learning process to serve as a sound basis for explainability.\nX-NeSyL methodology involves the concrete use of two notions of explanation at\ninference and training time respectively: 1) EXPLANet: Expert-aligned\neXplainable Part-based cLAssifier NETwork Architecture, a compositional CNN\nthat makes use of symbolic representations, and 2) SHAP-Backprop, an\nexplainable AI-informed training procedure that guides the DL process to align\nwith such symbolic representations in form of knowledge graphs. We showcase\nX-NeSyL methodology using MonuMAI dataset for monument facade image\nclassification, and demonstrate that our approach improves explainability and\nperformance.\n", "versions": [{"version": "v1", "created": "Sat, 24 Apr 2021 09:06:08 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["D\u00edaz-Rodr\u00edguez", "Natalia", ""], ["Lamas", "Alberto", ""], ["Sanchez", "Jules", ""], ["Franchi", "Gianni", ""], ["Donadello", "Ivan", ""], ["Tabik", "Siham", ""], ["Filliat", "David", ""], ["Cruz", "Policarpo", ""], ["Montes", "Rosana", ""], ["Herrera", "Francisco", ""]]}, {"id": "2104.11927", "submitter": "Furkan Ulger Mr.", "authors": "Furkan Ulger, Seniha Esen Yuksel, Atila Yilmaz", "title": "Anomaly Detection for Solder Joints Using $\\beta$-VAE", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In the assembly process of printed circuit boards (PCB), most of the errors\nare caused by solder joints in Surface Mount Devices (SMD). In the literature,\ntraditional feature extraction based methods require designing hand-crafted\nfeatures and rely on the tiered RGB illumination to detect solder joint errors,\nwhereas the supervised Convolutional Neural Network (CNN) based approaches\nrequire a lot of labelled abnormal samples (defective solder joints) to achieve\nhigh accuracy. To solve the optical inspection problem in unrestricted\nenvironments with no special lighting and without the existence of error-free\nreference boards, we propose a new beta-Variational Autoencoders (beta-VAE)\narchitecture for anomaly detection that can work on both IC and non-IC\ncomponents. We show that the proposed model learns disentangled representation\nof data, leading to more independent features and improved latent space\nrepresentations. We compare the activation and gradient-based representations\nthat are used to characterize anomalies; and observe the effect of different\nbeta parameters on accuracy and on untwining the feature representations in\nbeta-VAE. Finally, we show that anomalies on solder joints can be detected with\nhigh accuracy via a model trained on directly normal samples without designated\nhardware or feature engineering.\n", "versions": [{"version": "v1", "created": "Sat, 24 Apr 2021 11:19:27 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Ulger", "Furkan", ""], ["Yuksel", "Seniha Esen", ""], ["Yilmaz", "Atila", ""]]}, {"id": "2104.11931", "submitter": "Mengyao Zhai", "authors": "Mengyao Zhai, Ruizhi Deng, Jiacheng Chen, Lei Chen, Zhiwei Deng, Greg\n  Mori", "title": "Adaptive Appearance Rendering", "comments": "Accepted to BMVC 2018. arXiv admin note: substantial text overlap\n  with arXiv:1712.01955", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an approach to generate images of people given a desired\nappearance and pose. Disentangled representations of pose and appearance are\nnecessary to handle the compound variability in the resulting generated images.\nHence, we develop an approach based on intermediate representations of poses\nand appearance: our pose-guided appearance rendering network firstly encodes\nthe targets' poses using an encoder-decoder neural network. Then the targets'\nappearances are encoded by learning adaptive appearance filters using a fully\nconvolutional network. Finally, these filters are placed in the encoder-decoder\nneural networks to complete the rendering. We demonstrate that our model can\ngenerate images and videos that are superior to state-of-the-art methods, and\ncan handle pose guided appearance rendering in both image and video generation.\n", "versions": [{"version": "v1", "created": "Sat, 24 Apr 2021 11:53:09 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Zhai", "Mengyao", ""], ["Deng", "Ruizhi", ""], ["Chen", "Jiacheng", ""], ["Chen", "Lei", ""], ["Deng", "Zhiwei", ""], ["Mori", "Greg", ""]]}, {"id": "2104.11934", "submitter": "Jun Chen", "authors": "Jun Chen, Aniket Agarwal, Sherif Abdelkarim, Deyao Zhu, Mohamed\n  Elhoseiny", "title": "RelTransformer: Balancing the Visual Relationship Detection from Local\n  Context, Scene and Memory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Visual relationship recognition (VRR) is a fundamental scene understanding\ntask. The structure that VRR provides is essential to improve the AI\ninterpretability in downstream tasks such as image captioning and visual\nquestion answering. Several recent studies showed that the long-tail problem in\nVRR is even more critical than that in object recognition due to the\ncompositional complexity and structure. To overcome this limitation, we propose\na novel transformer-based framework, dubbed as RelTransformer, which performs\nrelationship prediction using rich semantic features from multiple image\nlevels. We assume that more abundantcon textual features can generate more\naccurate and discriminative relationships, which can be useful when sufficient\ntraining data are lacking. The key feature of our model is its ability to\naggregate three different-level features (local context, scene, and\ndataset-level) to compositionally predict the visual relationship. We evaluate\nour model on the visual genome and two \"long-tail\" VRR datasets, GQA-LT and\nVG8k-LT. Extensive experiments demonstrate that our RelTransformer could\nimprove over the state-of-the-art baselines on all the datasets. In addition,\nour model significantly improves the accuracy of GQA-LT by 27.4% upon the best\nbaselines on tail-relationship prediction. Our code is available in\nhttps://github.com/Vision-CAIR/RelTransformer.\n", "versions": [{"version": "v1", "created": "Sat, 24 Apr 2021 12:04:04 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Chen", "Jun", ""], ["Agarwal", "Aniket", ""], ["Abdelkarim", "Sherif", ""], ["Zhu", "Deyao", ""], ["Elhoseiny", "Mohamed", ""]]}, {"id": "2104.11939", "submitter": "Mengyao Zhai", "authors": "Mengyao Zhai, Lei Chen, Jiawei He, Megha Nawhal, Frederick Tung, Greg\n  Mori", "title": "Piggyback GAN: Efficient Lifelong Learning for Image Conditioned\n  Generation", "comments": "Accepted to ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans accumulate knowledge in a lifelong fashion. Modern deep neural\nnetworks, on the other hand, are susceptible to catastrophic forgetting: when\nadapted to perform new tasks, they often fail to preserve their performance on\npreviously learned tasks. Given a sequence of tasks, a naive approach\naddressing catastrophic forgetting is to train a separate standalone model for\neach task, which scales the total number of parameters drastically without\nefficiently utilizing previous models. In contrast, we propose a parameter\nefficient framework, Piggyback GAN, which learns the current task by building a\nset of convolutional and deconvolutional filters that are factorized into\nfilters of the models trained on previous tasks. For the current task, our\nmodel achieves high generation quality on par with a standalone model at a\nlower number of parameters. For previous tasks, our model can also preserve\ngeneration quality since the filters for previous tasks are not altered. We\nvalidate Piggyback GAN on various image-conditioned generation tasks across\ndifferent domains, and provide qualitative and quantitative results to show\nthat the proposed approach can address catastrophic forgetting effectively and\nefficiently.\n", "versions": [{"version": "v1", "created": "Sat, 24 Apr 2021 12:09:52 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Zhai", "Mengyao", ""], ["Chen", "Lei", ""], ["He", "Jiawei", ""], ["Nawhal", "Megha", ""], ["Tung", "Frederick", ""], ["Mori", "Greg", ""]]}, {"id": "2104.11949", "submitter": "Navid Ghassemi", "authors": "Navid Ghassemi, Afshin Shoeibi, Marjane Khodatars, Jonathan Heras,\n  Alireza Rahimi, Assef Zare, Ram Bilas Pachori, J. Manuel Gorriz", "title": "Automatic Diagnosis of COVID-19 from CT Images using CycleGAN and\n  Transfer Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The outbreak of the corona virus disease (COVID-19) has changed the lives of\nmost people on Earth. Given the high prevalence of this disease, its correct\ndiagnosis in order to quarantine patients is of the utmost importance in steps\nof fighting this pandemic. Among the various modalities used for diagnosis,\nmedical imaging, especially computed tomography (CT) imaging, has been the\nfocus of many previous studies due to its accuracy and availability. In\naddition, automation of diagnostic methods can be of great help to physicians.\nIn this paper, a method based on pre-trained deep neural networks is presented,\nwhich, by taking advantage of a cyclic generative adversarial net (CycleGAN)\nmodel for data augmentation, has reached state-of-the-art performance for the\ntask at hand, i.e., 99.60% accuracy. Also, in order to evaluate the method, a\ndataset containing 3163 images from 189 patients has been collected and labeled\nby physicians. Unlike prior datasets, normal data have been collected from\npeople suspected of having COVID-19 disease and not from data from other\ndiseases, and this database is made available publicly.\n", "versions": [{"version": "v1", "created": "Sat, 24 Apr 2021 13:12:20 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Ghassemi", "Navid", ""], ["Shoeibi", "Afshin", ""], ["Khodatars", "Marjane", ""], ["Heras", "Jonathan", ""], ["Rahimi", "Alireza", ""], ["Zare", "Assef", ""], ["Pachori", "Ram Bilas", ""], ["Gorriz", "J. Manuel", ""]]}, {"id": "2104.11977", "submitter": "Salvatore Ivan Trapasso", "authors": "Fabio Nicola and S. Ivan Trapasso", "title": "On the stability of deep convolutional neural networks under irregular\n  or random deformations", "comments": "36 pages, 6 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.FA cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of robustness under location deformations for deep convolutional\nneural networks (DCNNs) is of great theoretical and practical interest. This\nissue has been studied in pioneering works, especially for scattering-type\narchitectures, for deformation vector fields $\\tau(x)$ with some regularity -\nat least $C^1$. Here we address this issue for any field $\\tau\\in\nL^\\infty(\\mathbb{R}^d;\\mathbb{R}^d)$, without any additional regularity\nassumption, hence including the case of wild irregular deformations such as a\nnoise on the pixel location of an image. We prove that for signals in\nmultiresolution approximation spaces $U_s$ at scale $s$, whenever the network\nis Lipschitz continuous (regardless of its architecture), stability in $L^2$\nholds in the regime $\\|\\tau\\|_{L^\\infty}/s\\ll 1$, essentially as a consequence\nof the uncertainty principle. When $\\|\\tau\\|_{L^\\infty}/s\\gg 1$ instability can\noccur even for well-structured DCNNs such as the wavelet scattering networks,\nand we provide a sharp upper bound for the asymptotic growth rate. The\nstability results are then extended to signals in the Besov space\n$B^{d/2}_{2,1}$ tailored to the given multiresolution approximation. We also\nconsider the case of more general time-frequency deformations. Finally, we\nprovide stochastic versions of the aforementioned results, namely we study the\nissue of stability in mean when $\\tau(x)$ is modeled as a random field (not\nbounded, in general) with with identically distributed variables $|\\tau(x)|$,\n$x\\in\\mathbb{R}^d$.\n", "versions": [{"version": "v1", "created": "Sat, 24 Apr 2021 16:16:30 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Nicola", "Fabio", ""], ["Trapasso", "S. Ivan", ""]]}, {"id": "2104.12023", "submitter": "Peng Jiang", "authors": "Peng Jiang, Philip Osteen, Srikanth Saripalli", "title": "Calibrating LiDAR and Camera using Semantic Mutual information", "comments": "5 pages, 6 figures, submitted to ICRA 2021 workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose an algorithm for automatic, targetless, extrinsic calibration of a\nLiDAR and camera system using semantic information. We achieve this goal by\nmaximizing mutual information (MI) of semantic information between sensors,\nleveraging a neural network to estimate semantic mutual information, and matrix\nexponential for calibration computation. Using kernel-based sampling to sample\ndata from camera measurement based on LiDAR projected points, we formulate the\nproblem as a novel differentiable objective function which supports the use of\ngradient-based optimization methods. We also introduce an initial calibration\nmethod using 2D MI-based image registration. Finally, we demonstrate the\nrobustness of our method and quantitatively analyze the accuracy on a synthetic\ndataset and also evaluate our algorithm qualitatively on KITTI360 and RELLIS-3D\nbenchmark datasets, showing improvement over recent comparable approaches.\n", "versions": [{"version": "v1", "created": "Sat, 24 Apr 2021 21:04:33 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Jiang", "Peng", ""], ["Osteen", "Philip", ""], ["Saripalli", "Srikanth", ""]]}, {"id": "2104.12034", "submitter": "Eduard Durech", "authors": "Eduard F. Durech", "title": "Deep Convolutional Neural Network for Non-rigid Image Registration", "comments": "10 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Images taken at different times or positions undergo transformations such as\nrotation, scaling, skewing, and more. The process of aligning different images\nwhich have undergone transformations can be done via registration. Registration\nis desirable when analyzing time-series data for tracking, averaging, or\ndifferential diagnoses of diseases. Efficient registration methods exist for\nrigid (including linear or affine) transformations; however, for non-rigid\n(also known as non-affine) transformations, current methods are computationally\nexpensive and time-consuming. In this report, I will explore the ability of a\ndeep neural network (DNN) and, more specifically, a deep convolutional neural\nnetwork (CNN) to efficiently perform non-rigid image registration. The\nexperimental results show that a CNN can be used for efficient non-rigid image\nregistration and in significantly less computational time than a conventional\nDiffeomorphic Demons or Pyramiding approach.\n", "versions": [{"version": "v1", "created": "Sat, 24 Apr 2021 23:24:29 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Durech", "Eduard F.", ""]]}, {"id": "2104.12041", "submitter": "Zikai Zhang", "authors": "Zikai Zhang, Bineng Zhong, Shengping Zhang, Zhenjun Tang, Xin Liu,\n  Zhaoxiang Zhang", "title": "Distractor-Aware Fast Tracking via Dynamic Convolutions and MOT\n  Philosophy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A practical long-term tracker typically contains three key properties, i.e.\nan efficient model design, an effective global re-detection strategy and a\nrobust distractor awareness mechanism. However, most state-of-the-art long-term\ntrackers (e.g., Pseudo and re-detecting based ones) do not take all three key\nproperties into account and therefore may either be time-consuming or drift to\ndistractors. To address the issues, we propose a two-task tracking frame work\n(named DMTrack), which utilizes two core components (i.e., one-shot detection\nand re-identification (re-id) association) to achieve distractor-aware fast\ntracking via Dynamic convolutions (d-convs) and Multiple object tracking (MOT)\nphilosophy. To achieve precise and fast global detection, we construct a\nlightweight one-shot detector using a novel dynamic convolutions generation\nmethod, which provides a unified and more flexible way for fusing target\ninformation into the search field. To distinguish the target from distractors,\nwe resort to the philosophy of MOT to reason distractors explicitly by\nmaintaining all potential similarities' tracklets. Benefited from the strength\nof high recall detection and explicit object association, our tracker achieves\nstate-of-the-art performance on the LaSOT, OxUvA, TLP, VOT2018LT and VOT2019LT\nbenchmarks and runs in real-time (3x faster than comparisons).\n", "versions": [{"version": "v1", "created": "Sun, 25 Apr 2021 00:59:53 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Zhang", "Zikai", ""], ["Zhong", "Bineng", ""], ["Zhang", "Shengping", ""], ["Tang", "Zhenjun", ""], ["Liu", "Xin", ""], ["Zhang", "Zhaoxiang", ""]]}, {"id": "2104.12044", "submitter": "Xiaowei Xu", "authors": "Xiaowe Xu, Jiawei Zhang, Jinglan Liu, Yukun Ding, Tianchen Wang,\n  Hailong Qiu, Haiyun Yuan, Jian Zhuang, and Wen Xie, Yuhao Dong, Qianjun Jia,\n  Meiping Huang, Yiyu Shi", "title": "Multi-Cycle-Consistent Adversarial Networks for Edge Denoising of\n  Computed Tomography Images", "comments": "16 pages, 7 figures, 4 tables, accepted by the ACM Journal on\n  Emerging Technologies in Computing Systems (JETC). arXiv admin note:\n  substantial text overlap with arXiv:2002.12130", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As one of the most commonly ordered imaging tests, computed tomography (CT)\nscan comes with inevitable radiation exposure that increases the cancer risk to\npatients. However, CT image quality is directly related to radiation dose, thus\nit is desirable to obtain high-quality CT images with as little dose as\npossible. CT image denoising tries to obtain high dose like high-quality CT\nimages (domain X) from low dose low-quality CTimages (domain Y), which can be\ntreated as an image-to-image translation task where the goal is to learn the\ntransform between a source domain X (noisy images) and a target domain Y (clean\nimages). In this paper, we propose a multi-cycle-consistent adversarial network\n(MCCAN) that builds intermediate domains and enforces both local and global\ncycle-consistency for edge denoising of CT images. The global cycle-consistency\ncouples all generators together to model the whole denoising process, while the\nlocal cycle-consistency imposes effective supervision on the process between\nadjacent domains. Experiments show that both local and global cycle-consistency\nare important for the success of MCCAN, which outperformsCCADN in terms of\ndenoising quality with slightly less computation resource consumption.\n", "versions": [{"version": "v1", "created": "Sun, 25 Apr 2021 01:53:46 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Xu", "Xiaowe", ""], ["Zhang", "Jiawei", ""], ["Liu", "Jinglan", ""], ["Ding", "Yukun", ""], ["Wang", "Tianchen", ""], ["Qiu", "Hailong", ""], ["Yuan", "Haiyun", ""], ["Zhuang", "Jian", ""], ["Xie", "Wen", ""], ["Dong", "Yuhao", ""], ["Jia", "Qianjun", ""], ["Huang", "Meiping", ""], ["Shi", "Yiyu", ""]]}, {"id": "2104.12046", "submitter": "Xiaowei Xu", "authors": "Wentao Chen, Hailong Qiu, Jian Zhuang, Chutong Zhang, Yu Hu, Qing Lu,\n  Tianchen Wang, Yiyu Shi{\\dag}, Meiping Huang, Xiaowe Xu", "title": "Quantization of Deep Neural Networks for Accurate EdgeComputing", "comments": "11 pages, 3 figures, 10 tables, accepted by the ACM Journal on\n  Emerging Technologies in Computing Systems (JETC)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) have demonstrated their great potential in recent\nyears, exceeding the per-formance of human experts in a wide range of\napplications. Due to their large sizes, however, compressiontechniques such as\nweight quantization and pruning are usually applied before they can be\naccommodated onthe edge. It is generally believed that quantization leads to\nperformance degradation, and plenty of existingworks have explored quantization\nstrategies aiming at minimum accuracy loss. In this paper, we argue\nthatquantization, which essentially imposes regularization on weight\nrepresentations, can sometimes help toimprove accuracy. We conduct\ncomprehensive experiments on three widely used applications: fully con-nected\nnetwork (FCN) for biomedical image segmentation, convolutional neural network\n(CNN) for imageclassification on ImageNet, and recurrent neural network (RNN)\nfor automatic speech recognition, and experi-mental results show that\nquantization can improve the accuracy by 1%, 1.95%, 4.23% on the three\napplicationsrespectively with 3.5x-6.4x memory reduction.\n", "versions": [{"version": "v1", "created": "Sun, 25 Apr 2021 02:05:12 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Chen", "Wentao", ""], ["Qiu", "Hailong", ""], ["Zhuang", "Jian", ""], ["Zhang", "Chutong", ""], ["Hu", "Yu", ""], ["Lu", "Qing", ""], ["Wang", "Tianchen", ""], ["Shi\u2020", "Yiyu", ""], ["Huang", "Meiping", ""], ["Xu", "Xiaowe", ""]]}, {"id": "2104.12051", "submitter": "Wang Qianyun", "authors": "Qianyun Wang, Zhenfeng Fan, Shihong Xia", "title": "3D-TalkEmo: Learning to Synthesize 3D Emotional Talking Head", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Impressive progress has been made in audio-driven 3D facial animation\nrecently, but synthesizing 3D talking-head with rich emotion is still unsolved.\nThis is due to the lack of 3D generative models and available 3D emotional\ndataset with synchronized audios. To address this, we introduce 3D-TalkEmo, a\ndeep neural network that generates 3D talking head animation with various\nemotions. We also create a large 3D dataset with synchronized audios and\nvideos, rich corpus, as well as various emotion states of different persons\nwith the sophisticated 3D face reconstruction methods. In the emotion\ngeneration network, we propose a novel 3D face representation structure -\ngeometry map by classical multi-dimensional scaling analysis. It maps the\ncoordinates of vertices on a 3D face to a canonical image plane, while\npreserving the vertex-to-vertex geodesic distance metric in a least-square\nsense. This maintains the adjacency relationship of each vertex and holds the\neffective convolutional structure for the 3D facial surface. Taking a neutral\n3D mesh and a speech signal as inputs, the 3D-TalkEmo is able to generate vivid\nfacial animations. Moreover, it provides access to change the emotion state of\nthe animated speaker.\n  We present extensive quantitative and qualitative evaluation of our method,\nin addition to user studies, demonstrating the generated talking-heads of\nsignificantly higher quality compared to previous state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sun, 25 Apr 2021 02:48:19 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Wang", "Qianyun", ""], ["Fan", "Zhenfeng", ""], ["Xia", "Shihong", ""]]}, {"id": "2104.12056", "submitter": "Ivan Bajic", "authors": "Timothy Woinoski and Ivan V. Baji\\'c", "title": "Swimmer Stroke Rate Estimation From Overhead Race Video", "comments": "6 pages, 4 figures, to be presented at the IEEE ICME Workshop on\n  Artificial Intelligence in Sports (AI-Sports), July 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work, we propose a swimming analytics system for automatically\ndetermining swimmer stroke rates from overhead race video (ORV). General ORV is\ndefined as any footage of swimmers in competition, taken for the purposes of\nviewing or analysis. Examples of this are footage from live streams,\nbroadcasts, or specialized camera equipment, with or without camera motion.\nThese are the most typical forms of swimming competition footage. We detail how\nto create a system that will automatically collect swimmer stroke rates in any\ncompetition, given the video of the competition of interest. With this\ninformation, better systems can be created and additions to our analytics\nsystem can be proposed to automatically extract other swimming metrics of\ninterest.\n", "versions": [{"version": "v1", "created": "Sun, 25 Apr 2021 04:20:38 GMT"}, {"version": "v2", "created": "Thu, 20 May 2021 20:55:46 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Woinoski", "Timothy", ""], ["Baji\u0107", "Ivan V.", ""]]}, {"id": "2104.12069", "submitter": "Xinwei Zhao", "authors": "Xinwei Zhao, Matthew C. Stamm", "title": "Making GAN-Generated Images Difficult To Spot: A New Attack Against\n  Synthetic Image Detectors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Visually realistic GAN-generated images have recently emerged as an important\nmisinformation threat. Research has shown that these synthetic images contain\nforensic traces that are readily identifiable by forensic detectors.\nUnfortunately, these detectors are built upon neural networks, which are\nvulnerable to recently developed adversarial attacks. In this paper, we propose\na new anti-forensic attack capable of fooling GAN-generated image detectors.\nOur attack uses an adversarially trained generator to synthesize traces that\nthese detectors associate with real images. Furthermore, we propose a technique\nto train our attack so that it can achieve transferability, i.e. it can fool\nunknown CNNs that it was not explicitly trained against. We demonstrate the\nperformance of our attack through an extensive set of experiments, where we\nshow that our attack can fool eight state-of-the-art detection CNNs with\nsynthetic images created using seven different GANs.\n", "versions": [{"version": "v1", "created": "Sun, 25 Apr 2021 05:56:57 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Zhao", "Xinwei", ""], ["Stamm", "Matthew C.", ""]]}, {"id": "2104.12076", "submitter": "Usman Sajid", "authors": "Usman Sajid, Michael Chow, Jin Zhang, Taejoon Kim, Guanghui Wang", "title": "Parallel Scale-wise Attention Network for Effective Scene Text\n  Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper proposes a new text recognition network for scene-text images. Many\nstate-of-the-art methods employ the attention mechanism either in the text\nencoder or decoder for the text alignment. Although the encoder-based attention\nyields promising results, these schemes inherit noticeable limitations. They\nperform the feature extraction (FE) and visual attention (VA) sequentially,\nwhich bounds the attention mechanism to rely only on the FE final single-scale\noutput. Moreover, the utilization of the attention process is limited by only\napplying it directly to the single scale feature-maps. To address these issues,\nwe propose a new multi-scale and encoder-based attention network for text\nrecognition that performs the multi-scale FE and VA in parallel. The\nmulti-scale channels also undergo regular fusion with each other to develop the\ncoordinated knowledge together. Quantitative evaluation and robustness analysis\non the standard benchmarks demonstrate that the proposed network outperforms\nthe state-of-the-art in most cases.\n", "versions": [{"version": "v1", "created": "Sun, 25 Apr 2021 06:44:26 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Sajid", "Usman", ""], ["Chow", "Michael", ""], ["Zhang", "Jin", ""], ["Kim", "Taejoon", ""], ["Wang", "Guanghui", ""]]}, {"id": "2104.12081", "submitter": "Dapeng Hu", "authors": "Dapeng Hu, Qizhengqiu Lu, Lanqing Hong, Hailin Hu, Yifan Zhang,\n  Zhenguo Li, Alfred Shen, Jiashi Feng", "title": "How Well Self-Supervised Pre-Training Performs with Streaming Data?", "comments": "16 pages; 16figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The common self-supervised pre-training practice requires collecting massive\nunlabeled data together and then trains a representation model, dubbed\n\\textbf{joint training}. However, in real-world scenarios where data are\ncollected in a streaming fashion, the joint training scheme is usually\nstorage-heavy and time-consuming. A more efficient alternative is to train a\nmodel continually with streaming data, dubbed \\textbf{sequential training}.\nNevertheless, it is unclear how well sequential self-supervised pre-training\nperforms with streaming data. In this paper, we conduct thorough experiments to\ninvestigate self-supervised pre-training with streaming data. Specifically, we\nevaluate the transfer performance of sequential self-supervised pre-training\nwith four different data sequences on three different downstream tasks and make\ncomparisons with joint self-supervised pre-training. Surprisingly, we find\nsequential self-supervised learning exhibits almost the same performance as the\njoint training when the distribution shifts within streaming data are mild.\nEven for data sequences with large distribution shifts, sequential\nself-supervised training with simple techniques, e.g., parameter regularization\nor data replay, still performs comparably to joint training. Based on our\nfindings, we recommend using sequential self-supervised training as a\n\\textbf{more efficient yet performance-competitive} representation learning\npractice for real-world applications.\n", "versions": [{"version": "v1", "created": "Sun, 25 Apr 2021 06:56:48 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Hu", "Dapeng", ""], ["Lu", "Qizhengqiu", ""], ["Hong", "Lanqing", ""], ["Hu", "Hailin", ""], ["Zhang", "Yifan", ""], ["Li", "Zhenguo", ""], ["Shen", "Alfred", ""], ["Feng", "Jiashi", ""]]}, {"id": "2104.12085", "submitter": "Canqun Xiang", "authors": "Jinping Wang, Xiaojun Tan, Jianhuang Lai, Jun Li, Canqun Xiang", "title": "ASPCNet: A Deep Adaptive Spatial Pattern Capsule Network for\n  Hyperspectral Image Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous studies have shown the great potential of capsule networks for the\nspatial contextual feature extraction from {hyperspectral images (HSIs)}.\nHowever, the sampling locations of the convolutional kernels of capsules are\nfixed and cannot be adaptively changed according to the inconsistent semantic\ninformation of HSIs. Based on this observation, this paper proposes an adaptive\nspatial pattern capsule network (ASPCNet) architecture by developing an\nadaptive spatial pattern (ASP) unit, that can rotate the sampling location of\nconvolutional kernels on the basis of an enlarged receptive field. Note that\nthis unit can learn more discriminative representations of HSIs with fewer\nparameters. Specifically, two cascaded ASP-based convolution operations\n(ASPConvs) are applied to input images to learn relatively high-level semantic\nfeatures, transmitting hierarchical structures among capsules more accurately\nthan the use of the most fundamental features. Furthermore, the semantic\nfeatures are fed into ASP-based conv-capsule operations (ASPCaps) to explore\nthe shapes of objects among the capsules in an adaptive manner, further\nexploring the potential of capsule networks. Finally, the class labels of image\npatches centered on test samples can be determined according to the fully\nconnected capsule layer. Experiments on three public datasets demonstrate that\nASPCNet can yield competitive performance with higher accuracies than\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sun, 25 Apr 2021 07:10:55 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Wang", "Jinping", ""], ["Tan", "Xiaojun", ""], ["Lai", "Jianhuang", ""], ["Li", "Jun", ""], ["Xiang", "Canqun", ""]]}, {"id": "2104.12087", "submitter": "Dongsheng Wang", "authors": "Dongsheng Wang, Chaohao Xie, Shaohui Liu, Zhenxing Niu, Wangmeng Zuo", "title": "Image Inpainting with Edge-guided Learnable Bidirectional Attention Maps", "comments": "16 pages,13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For image inpainting, the convolutional neural networks (CNN) in previous\nmethods often adopt standard convolutional operator, which treats valid pixels\nand holes indistinguishably. As a result, they are limited in handling\nirregular holes and tend to produce color-discrepant and blurry inpainting\nresult. Partial convolution (PConv) copes with this issue by conducting masked\nconvolution and feature re-normalization conditioned only on valid pixels, but\nthe mask-updating is handcrafted and independent with image structural\ninformation. In this paper, we present an edge-guided learnable bidirectional\nattention map (Edge-LBAM) for improving image inpainting of irregular holes\nwith several distinct merits. Instead of using a hard 0-1 mask, a learnable\nattention map module is introduced for learning feature re-normalization and\nmask-updating in an end-to-end manner. Learnable reverse attention maps are\nfurther proposed in the decoder for emphasizing on filling in unknown pixels\ninstead of reconstructing all pixels. Motivated by that the filling-in order is\ncrucial to inpainting results and largely depends on image structures in\nexemplar-based methods, we further suggest a multi-scale edge completion\nnetwork to predict coherent edges. Our Edge-LBAM method contains dual\nprocedures,including structure-aware mask-updating guided by predict edges and\nattention maps generated by masks for feature re-normalization.Extensive\nexperiments show that our Edge-LBAM is effective in generating coherent image\nstructures and preventing color discrepancy and blurriness, and performs\nfavorably against the state-of-the-art methods in terms of qualitative metrics\nand visual quality.\n", "versions": [{"version": "v1", "created": "Sun, 25 Apr 2021 07:25:16 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Wang", "Dongsheng", ""], ["Xie", "Chaohao", ""], ["Liu", "Shaohui", ""], ["Niu", "Zhenxing", ""], ["Zuo", "Wangmeng", ""]]}, {"id": "2104.12099", "submitter": "Ni Zhang", "authors": "Nian Liu and Ni Zhang and Kaiyuan Wan and Junwei Han and Ling Shao", "title": "Visual Saliency Transformer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, massive saliency detection methods have achieved promising results\nby relying on CNN-based architectures. Alternatively, we rethink this task from\na convolution-free sequence-to-sequence perspective and predict saliency by\nmodeling long-range dependencies, which can not be achieved by convolution.\nSpecifically, we develop a novel unified model based on a pure transformer,\nnamely, Visual Saliency Transformer (VST), for both RGB and RGB-D salient\nobject detection (SOD). It takes image patches as inputs and leverages the\ntransformer to propagate global contexts among image patches. Apart from the\ntraditional transformer architecture used in Vision Transformer (ViT), we\nleverage multi-level token fusion and propose a new token upsampling method\nunder the transformer framework to get high-resolution detection results. We\nalso develop a token-based multi-task decoder to simultaneously perform\nsaliency and boundary detection by introducing task-related tokens and a novel\npatch-task-attention mechanism. Experimental results show that our model\noutperforms existing state-of-the-art results on both RGB and RGB-D SOD\nbenchmark datasets. Most importantly, our whole framework not only provides a\nnew perspective for the SOD field but also shows a new paradigm for\ntransformer-based dense prediction models.\n", "versions": [{"version": "v1", "created": "Sun, 25 Apr 2021 08:24:06 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Liu", "Nian", ""], ["Zhang", "Ni", ""], ["Wan", "Kaiyuan", ""], ["Han", "Junwei", ""], ["Shao", "Ling", ""]]}, {"id": "2104.12100", "submitter": "Xiang Chen", "authors": "Xiang Chen, Yufeng Huang, Lei Xu", "title": "Multi-Scale Hourglass Hierarchical Fusion Network for Single Image\n  Deraining", "comments": "Accepted in CVPRW 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rain streaks bring serious blurring and visual quality degradation, which\noften vary in size, direction and density. Current CNN-based methods achieve\nencouraging performance, while are limited to depict rain characteristics and\nrecover image details in the poor visibility environment. To address these\nissues, we present a Multi-scale Hourglass Hierarchical Fusion Network\n(MH2F-Net) in end-to-end manner, to exactly captures rain streak features with\nmulti-scale extraction, hierarchical distillation and information aggregation.\nFor better extracting the features, a novel Multi-scale Hourglass Extraction\nBlock (MHEB) is proposed to get local and global features across different\nscales through down- and up-sample process. Besides, a Hierarchical Attentive\nDistillation Block (HADB) then employs the dual attention feature responses to\nadaptively recalibrate the hierarchical features and eliminate the redundant\nones. Further, we introduce a Residual Projected Feature Fusion (RPFF) strategy\nto progressively discriminate feature learning and aggregate different features\ninstead of directly concatenating or adding. Extensive experiments on both\nsynthetic and real rainy datasets demonstrate the effectiveness of the designed\nMH2F-Net by comparing with recent state-of-the-art deraining algorithms. Our\nsource code will be available on the GitHub:\nhttps://github.com/cxtalk/MH2F-Net.\n", "versions": [{"version": "v1", "created": "Sun, 25 Apr 2021 08:27:01 GMT"}, {"version": "v2", "created": "Mon, 14 Jun 2021 01:58:39 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Chen", "Xiang", ""], ["Huang", "Yufeng", ""], ["Xu", "Lei", ""]]}, {"id": "2104.12102", "submitter": "Songmin Dai", "authors": "Songmin Dai, Jide Li, Lu Wang, Congcong Zhu, Yifan Wu, Xiaoqiang Li", "title": "Unsupervised Learning of Multi-level Structures for Anomaly Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main difficulty in high-dimensional anomaly detection tasks is the lack\nof anomalous data for training. And simply collecting anomalous data from the\nreal world, common distributions, or the boundary of normal data manifold may\nface the problem of missing anomaly modes. This paper first introduces a novel\nmethod to generate anomalous data by breaking up global structures while\npreserving local structures of normal data at multiple levels. It can\nefficiently expose local abnormal structures of various levels. To fully\nexploit the exposed multi-level abnormal structures, we propose to train\nmultiple level-specific patch-based detectors with contrastive losses. Each\ndetector learns to detect local abnormal structures of corresponding level at\nall locations and outputs patchwise anomaly scores. By aggregating the outputs\nof all level-specific detectors, we obtain a model that can detect all\npotential anomalies. The effectiveness is evaluated on MNIST, CIFAR10, and\nImageNet10 dataset, where the results surpass the accuracy of state-of-the-art\nmethods. Qualitative experiments demonstrate our model is robust that it\nunbiasedly detects all anomaly modes.\n", "versions": [{"version": "v1", "created": "Sun, 25 Apr 2021 08:38:41 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Dai", "Songmin", ""], ["Li", "Jide", ""], ["Wang", "Lu", ""], ["Zhu", "Congcong", ""], ["Wu", "Yifan", ""], ["Li", "Xiaoqiang", ""]]}, {"id": "2104.12106", "submitter": "Eme\\c{c} Er\\c{c}elik", "authors": "Eme\\c{c} Er\\c{c}elik, Ekim Yurtsever and Alois Knoll", "title": "Temp-Frustum Net: 3D Object Detection with Temporal Fusion", "comments": "To be published in 32nd IEEE Intelligent Vehicles Symposium", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D object detection is a core component of automated driving systems.\nState-of-the-art methods fuse RGB imagery and LiDAR point cloud data\nframe-by-frame for 3D bounding box regression. However, frame-by-frame 3D\nobject detection suffers from noise, field-of-view obstruction, and sparsity.\nWe propose a novel Temporal Fusion Module (TFM) to use information from\nprevious time-steps to mitigate these problems. First, a state-of-the-art\nfrustum network extracts point cloud features from raw RGB and LiDAR point\ncloud data frame-by-frame. Then, our TFM module fuses these features with a\nrecurrent neural network. As a result, 3D object detection becomes robust\nagainst single frame failures and transient occlusions. Experiments on the\nKITTI object tracking dataset show the efficiency of the proposed TFM, where we\nobtain ~6%, ~4%, and ~6% improvements on Car, Pedestrian, and Cyclist classes,\nrespectively, compared to frame-by-frame baselines. Furthermore, ablation\nstudies reinforce that the subject of improvement is temporal fusion and show\nthe effects of different placements of TFM in the object detection pipeline.\nOur code is open-source and available at\nhttps://github.com/emecercelik/Temp-Frustum-Net.git.\n", "versions": [{"version": "v1", "created": "Sun, 25 Apr 2021 09:08:14 GMT"}, {"version": "v2", "created": "Fri, 21 May 2021 10:37:01 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Er\u00e7elik", "Eme\u00e7", ""], ["Yurtsever", "Ekim", ""], ["Knoll", "Alois", ""]]}, {"id": "2104.12123", "submitter": "Guy Ben-Yosef", "authors": "Uri Wollner and Guy Ben-Yosef", "title": "Parallel mesh reconstruction streams for pose estimation of interacting\n  hands", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new multi-stream 3D mesh reconstruction network (MSMR-Net) for\nhand pose estimation from a single RGB image. Our model consists of an image\nencoder followed by a mesh-convolution decoder composed of connected graph\nconvolution layers. In contrast to previous models that form a single mesh\ndecoding path, our decoder network incorporates multiple cross-resolution\ntrajectories that are executed in parallel. Thus, global and local information\nare shared to form rich decoding representations at minor additional parameter\ncost compared to the single trajectory network. We demonstrate the\neffectiveness of our method in hand-hand and hand-object interaction scenarios\nat various levels of interaction. To evaluate the former scenario, we propose a\nmethod to generate RGB images of closely interacting hands. Moreoever, we\nsuggest a metric to quantify the degree of interaction and show that close hand\ninteractions are particularly challenging. Experimental results show that the\nMSMR-Net outperforms existing algorithms on the hand-object FreiHAND dataset as\nwell as on our own hand-hand dataset.\n", "versions": [{"version": "v1", "created": "Sun, 25 Apr 2021 10:14:15 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Wollner", "Uri", ""], ["Ben-Yosef", "Guy", ""]]}, {"id": "2104.12136", "submitter": "Muhammad Ahmad", "authors": "Muhammad Ahmad, Manuel Mazzara, and Salvatore Distefano", "title": "3D/2D regularized CNN feature hierarchy for Hyperspectral image\n  classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Convolutional Neural Networks (CNN) have been rigorously studied for\nHyperspectral Image Classification (HSIC) and are known to be effective in\nexploiting joint spatial-spectral information with the expense of lower\ngeneralization performance and learning speed due to the hard labels and\nnon-uniform distribution over labels. Several regularization techniques have\nbeen used to overcome the aforesaid issues. However, sometimes models learn to\npredict the samples extremely confidently which is not good from a\ngeneralization point of view. Therefore, this paper proposed an idea to enhance\nthe generalization performance of a hybrid CNN for HSIC using soft labels that\nare a weighted average of the hard labels and uniform distribution over ground\nlabels. The proposed method helps to prevent CNN from becoming over-confident.\nWe empirically show that in improving generalization performance, label\nsmoothing also improves model calibration which significantly improves\nbeam-search. Several publicly available Hyperspectral datasets are used to\nvalidate the experimental evaluation which reveals improved generalization\nperformance, statistical significance, and computational complexity as compared\nto the state-of-the-art models. The code will be made available at\nhttps://github.com/mahmad00.\n", "versions": [{"version": "v1", "created": "Sun, 25 Apr 2021 11:26:56 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Ahmad", "Muhammad", ""], ["Mazzara", "Manuel", ""], ["Distefano", "Salvatore", ""]]}, {"id": "2104.12137", "submitter": "Libo Wang", "authors": "Libo Wang, Rui Li, Chenxi Duan, Ce Zhang, Xiaoliang Meng and Shenghui\n  Fang", "title": "A Novel Transformer based Semantic Segmentation Scheme for\n  Fine-Resolution Remote Sensing Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The fully-convolutional network (FCN) with an encoder-decoder architecture\nhas been the standard paradigm for semantic segmentation. The encoder-decoder\narchitecture utilizes an encoder to capture multi-level feature maps, which are\nincorporated into the final prediction by a decoder. As the context is crucial\nfor precise segmentation, tremendous effort has been made to extract such\ninformation in an intelligent fashion, including employing dilated/atrous\nconvolutions or inserting attention modules. However, these endeavours are all\nbased on the FCN architecture with ResNet or other backbones, which cannot\nfully exploit the context from the theoretical concept. By contrast, we propose\nthe Swin Transformer as the backbone to extract the context information and\ndesign a novel decoder of densely connected feature aggregation module (DCFAM)\nto restore the resolution and produce the segmentation map. The experimental\nresults on two remotely sensed semantic segmentation datasets demonstrate the\neffectiveness of the proposed scheme.\n", "versions": [{"version": "v1", "created": "Sun, 25 Apr 2021 11:34:22 GMT"}, {"version": "v2", "created": "Mon, 10 May 2021 14:20:46 GMT"}, {"version": "v3", "created": "Tue, 11 May 2021 15:03:00 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Wang", "Libo", ""], ["Li", "Rui", ""], ["Duan", "Chenxi", ""], ["Zhang", "Ce", ""], ["Meng", "Xiaoliang", ""], ["Fang", "Shenghui", ""]]}, {"id": "2104.12138", "submitter": "Yukun Zhou", "authors": "Yukun Zhou, Moucheng Xu, Yipeng Hu, Hongxiang Lin, Joseph Jacob,\n  Pearse Keane, Daniel Alexander", "title": "Learning to Address Intra-segment Misclassification in Retinal Imaging", "comments": "10 pages, 5 figures, and 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate multi-class segmentation is a long-standing challenge in medical\nimaging, especially in scenarios where classes share strong similarity.\nSegmenting retinal blood vessels in retinal photographs is one such scenario,\nin which arteries and veins need to be identified and differentiated from each\nother and from the background. Intra-segment misclassification, i.e. veins\nclassified as arteries or vice versa, frequently occurs when arteries and veins\nintersect, whereas in binary retinal vessel segmentation, error rates are much\nlower. We thus propose a new approach that decomposes multi-class segmentation\ninto multiple binary, followed by a binary-to-multi-class fusion network. The\nnetwork merges representations of artery, vein, and multi-class feature maps,\neach of which are supervised by expert vessel annotation in adversarial\ntraining. A skip-connection based merging process explicitly maintains\nclass-specific gradients to avoid gradient vanishing in deep layers, to favor\nthe discriminative features. The results show that, our model respectively\nimproves F1-score by 4.4\\%, 5.1\\%, and 4.2\\% compared with three\nstate-of-the-art deep learning based methods on DRIVE-AV, LES-AV, and HRF-AV\ndata sets.\n", "versions": [{"version": "v1", "created": "Sun, 25 Apr 2021 11:57:26 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Zhou", "Yukun", ""], ["Xu", "Moucheng", ""], ["Hu", "Yipeng", ""], ["Lin", "Hongxiang", ""], ["Jacob", "Joseph", ""], ["Keane", "Pearse", ""], ["Alexander", "Daniel", ""]]}, {"id": "2104.12146", "submitter": "Jinlai Zhang", "authors": "Jinlai Zhang, Lyujie Chen, Binbin Liu, Bo Ouyang, Qizhi Xie, Jihong\n  Zhu, Weiming Li, Yanmei Meng", "title": "3D Adversarial Attacks Beyond Point Cloud", "comments": "9 pages, 5 figs", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently, 3D deep learning models have been shown to be susceptible to\nadversarial attacks like their 2D counterparts. Most of the state-of-the-art\n(SOTA) 3D adversarial attacks perform perturbation to 3D point clouds. To\nreproduce these attacks in pseudo physical scenario, a generated adversarial 3D\npoint cloud need to be reconstructed to mesh, which leads to a significant drop\nin its adversarial effect. In this paper, we propose a strong 3D adversarial\nattack named Mesh Attack to address this problem by directly performing\nperturbation on mesh of a 3D object. Specifically, in each iteration of our\nmethod, the mesh is first sampled to point cloud by a differentiable sample\nmodule. Then a point cloud classifier is used to back-propagate a combined loss\nto update the mesh vertices. The combined loss includes an adversarial loss to\nmislead the point cloud classifier and three mesh losses to regularize the mesh\nto be smooth. Extensive experiments demonstrate that the proposed scheme\noutperforms SOTA 3D attacks by a significant margin in the pseudo physical\nscenario. We also achieved SOTA performance under various defenses. Moreover,\nto the best of our knowledge, our Mesh Attack is the first attempt of\nadversarial attack on mesh classifier. Our code is available at:\n{\\footnotesize{\\url{https://github.com/cuge1995/Mesh-Attack}}}.\n", "versions": [{"version": "v1", "created": "Sun, 25 Apr 2021 13:01:41 GMT"}, {"version": "v2", "created": "Mon, 31 May 2021 13:09:19 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Zhang", "Jinlai", ""], ["Chen", "Lyujie", ""], ["Liu", "Binbin", ""], ["Ouyang", "Bo", ""], ["Xie", "Qizhi", ""], ["Zhu", "Jihong", ""], ["Li", "Weiming", ""], ["Meng", "Yanmei", ""]]}, {"id": "2104.12166", "submitter": "Guotai Wang", "authors": "Xiangde Luo, Guotai Wang, Tao Song, Jingyang Zhang, Michael Aertsen,\n  Jan Deprest, Sebastien Ourselin, Tom Vercauteren, Shaoting Zhang", "title": "MIDeepSeg: Minimally Interactive Segmentation of Unseen Objects from\n  Medical Images Using Deep Learning", "comments": "17 pages, 16 figures", "journal-ref": null, "doi": "10.1016/j.media.2021.102102", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Segmentation of organs or lesions from medical images plays an essential role\nin many clinical applications such as diagnosis and treatment planning. Though\nConvolutional Neural Networks (CNN) have achieved the state-of-the-art\nperformance for automatic segmentation, they are often limited by the lack of\nclinically acceptable accuracy and robustness in complex cases. Therefore,\ninteractive segmentation is a practical alternative to these methods. However,\ntraditional interactive segmentation methods require a large amount of user\ninteractions, and recently proposed CNN-based interactive segmentation methods\nare limited by poor performance on previously unseen objects. To solve these\nproblems, we propose a novel deep learning-based interactive segmentation\nmethod that not only has high efficiency due to only requiring clicks as user\ninputs but also generalizes well to a range of previously unseen objects.\nSpecifically, we first encode user-provided interior margin points via our\nproposed exponentialized geodesic distance that enables a CNN to achieve a good\ninitial segmentation result of both previously seen and unseen objects, then we\nuse a novel information fusion method that combines the initial segmentation\nwith only few additional user clicks to efficiently obtain a refined\nsegmentation. We validated our proposed framework through extensive experiments\non 2D and 3D medical image segmentation tasks with a wide range of previous\nunseen objects that were not present in the training set. Experimental results\nshowed that our proposed framework 1) achieves accurate results with fewer user\ninteractions and less time compared with state-of-the-art interactive\nframeworks and 2) generalizes well to previously unseen objects.\n", "versions": [{"version": "v1", "created": "Sun, 25 Apr 2021 14:15:17 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Luo", "Xiangde", ""], ["Wang", "Guotai", ""], ["Song", "Tao", ""], ["Zhang", "Jingyang", ""], ["Aertsen", "Michael", ""], ["Deprest", "Jan", ""], ["Ourselin", "Sebastien", ""], ["Vercauteren", "Tom", ""], ["Zhang", "Shaoting", ""]]}, {"id": "2104.12167", "submitter": "Jinglin Sun", "authors": "Sunjing Lin, Yu Liu, Shaochu Wang, Chang Li, Han Wang", "title": "A Novel Unified Stereo Stimuli based Binocular Eye-Tracking System for\n  Accurate 3D Gaze Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In addition to the high cost and complex setup, the main reason for the\nlimitation of the three-dimensional (3D) display is the problem of accurately\nestimating the user's current point-of-gaze (PoG) in a 3D space. In this paper,\nwe present a novel noncontact technique for the PoG estimation in a\nstereoscopic environment, which integrates a 3D stereoscopic display system and\nan eye-tracking system. The 3D stereoscopic display system can provide users\nwith a friendly and immersive high-definition viewing experience without\nwearing any equipment. To accurately locate the user's 3D PoG in the field of\nview, we build a regression-based 3D eye-tracking model with the eye movement\ndata and stereo stimulus videos as input. Besides, to train an optimal\nregression model, we also design and annotate a dataset that contains 30 users'\neye-tracking data corresponding to two designed stereo test scenes.\nInnovatively, this dataset introduces feature vectors between eye region\nlandmarks for the gaze vector estimation and a combined feature set for the\ngaze depth estimation. Moreover, five traditional regression models are trained\nand evaluated based on this dataset. Experimental results show that the average\nerrors of the 3D PoG are about 0.90~cm on the X-axis, 0.83~cm on the Y-axis,\nand 1.48~cm$/$0.12~m along the Z-axis with the scene-depth range in\n75~cm$/$8~m, respectively.\n", "versions": [{"version": "v1", "created": "Sun, 25 Apr 2021 14:17:07 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Lin", "Sunjing", ""], ["Liu", "Yu", ""], ["Wang", "Shaochu", ""], ["Li", "Chang", ""], ["Wang", "Han", ""]]}, {"id": "2104.12203", "submitter": "Muhammad Saif Ullah Khan", "authors": "Muhammad Saif Ullah Khan", "title": "A novel segmentation dataset for signatures on bank checks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  The dataset presented provides high-resolution images of real, filled out\nbank checks containing various complex backgrounds, and handwritten text and\nsignatures in the respective fields, along with both pixel-level and\npatch-level segmentation masks for the signatures on the checks. The images of\nbank checks were obtained from different sources, including other publicly\navailable check datasets, publicly available images on the internet, as well as\nscans and images of real checks. Using the GIMP graphics software, pixel-level\nsegmentation masks for signatures on these checks were manually generated as\nbinary images. An automated script was then used to generate patch-level masks.\nThe dataset was created to train and test networks for extracting signatures\nfrom bank checks and other similar documents with very complex backgrounds.\n", "versions": [{"version": "v1", "created": "Sun, 25 Apr 2021 16:56:09 GMT"}, {"version": "v2", "created": "Wed, 28 Apr 2021 11:06:40 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Khan", "Muhammad Saif Ullah", ""]]}, {"id": "2104.12218", "submitter": "Lia Morra", "authors": "Sina Famouri, Lia Morra, Leonardo Mangia, Fabrizio Lamberti", "title": "Breast Mass Detection with Faster R-CNN: On the Feasibility of Learning\n  from Noisy Annotations", "comments": null, "journal-ref": "IEEE Access, 2021", "doi": "10.1109/ACCESS.2021.3072997", "report-no": null, "categories": "cs.CV physics.med-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work we study the impact of noise on the training of object detection\nnetworks for the medical domain, and how it can be mitigated by improving the\ntraining procedure. Annotating large medical datasets for training data-hungry\ndeep learning models is expensive and time consuming. Leveraging information\nthat is already collected in clinical practice, in the form of text reports,\nbookmarks or lesion measurements would substantially reduce this cost.\nObtaining precise lesion bounding boxes through automatic mining procedures,\nhowever, is difficult. We provide here a quantitative evaluation of the effect\nof bounding box coordinate noise on the performance of Faster R-CNN object\ndetection networks for breast mass detection. Varying degrees of noise are\nsimulated by randomly modifying the bounding boxes: in our experiments,\nbounding boxes could be enlarged up to six times the original size. The noise\nis injected in the CBIS-DDSM collection, a well curated public mammography\ndataset for which accurate lesion location is available. We show how, due to an\nimperfect matching between the ground truth and the network bounding box\nproposals, the noise is propagated during training and reduces the ability of\nthe network to correctly classify lesions from background. When using the\nstandard Intersection over Union criterion, the area under the FROC curve\ndecreases by up to 9%. A novel matching criterion is proposed to improve\ntolerance to noise.\n", "versions": [{"version": "v1", "created": "Sun, 25 Apr 2021 17:43:58 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Famouri", "Sina", ""], ["Morra", "Lia", ""], ["Mangia", "Leonardo", ""], ["Lamberti", "Fabrizio", ""]]}, {"id": "2104.12229", "submitter": "Congyue Deng", "authors": "Congyue Deng, Or Litany, Yueqi Duan, Adrien Poulenard, Andrea\n  Tagliasacchi, Leonidas Guibas", "title": "Vector Neurons: A General Framework for SO(3)-Equivariant Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Invariance and equivariance to the rotation group have been widely discussed\nin the 3D deep learning community for pointclouds. Yet most proposed methods\neither use complex mathematical tools that may limit their accessibility, or\nare tied to specific input data types and network architectures. In this paper,\nwe introduce a general framework built on top of what we call Vector Neuron\nrepresentations for creating SO(3)-equivariant neural networks for pointcloud\nprocessing. Extending neurons from 1D scalars to 3D vectors, our vector neurons\nenable a simple mapping of SO(3) actions to latent spaces thereby providing a\nframework for building equivariance in common neural operations -- including\nlinear layers, non-linearities, pooling, and normalizations. Due to their\nsimplicity, vector neurons are versatile and, as we demonstrate, can be\nincorporated into diverse network architecture backbones, allowing them to\nprocess geometry inputs in arbitrary poses. Despite its simplicity, our method\nperforms comparably well in accuracy and generalization with other more complex\nand specialized state-of-the-art methods on classification and segmentation\ntasks. We also show for the first time a rotation equivariant reconstruction\nnetwork.\n", "versions": [{"version": "v1", "created": "Sun, 25 Apr 2021 18:48:15 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Deng", "Congyue", ""], ["Litany", "Or", ""], ["Duan", "Yueqi", ""], ["Poulenard", "Adrien", ""], ["Tagliasacchi", "Andrea", ""], ["Guibas", "Leonidas", ""]]}, {"id": "2104.12233", "submitter": "Zheng Tang", "authors": "Milind Naphade, Shuo Wang, David C. Anastasiu, Zheng Tang, Ming-Ching\n  Chang, Xiaodong Yang, Yue Yao, Liang Zheng, Pranamesh Chakraborty, Christian\n  E. Lopez, Anuj Sharma, Qi Feng, Vitaly Ablavsky, Stan Sclaroff", "title": "The 5th AI City Challenge", "comments": "Summary of the 5th AI City Challenge Workshop in conjunction with\n  CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The AI City Challenge was created with two goals in mind: (1) pushing the\nboundaries of research and development in intelligent video analysis for\nsmarter cities use cases, and (2) assessing tasks where the level of\nperformance is enough to cause real-world adoption. Transportation is a segment\nripe for such adoption. The fifth AI City Challenge attracted 305 participating\nteams across 38 countries, who leveraged city-scale real traffic data and\nhigh-quality synthetic data to compete in five challenge tracks. Track 1\naddressed video-based automatic vehicle counting, where the evaluation being\nconducted on both algorithmic effectiveness and computational efficiency. Track\n2 addressed city-scale vehicle re-identification with augmented synthetic data\nto substantially increase the training set for the task. Track 3 addressed\ncity-scale multi-target multi-camera vehicle tracking. Track 4 addressed\ntraffic anomaly detection. Track 5 was a new track addressing vehicle retrieval\nusing natural language descriptions. The evaluation system shows a general\nleader board of all submitted results, and a public leader board of results\nlimited to the contest participation rules, where teams are not allowed to use\nexternal data in their work. The public leader board shows results more close\nto real-world situations where annotated data is limited. Results show the\npromise of AI in Smarter Transportation. State-of-the-art performance for some\ntasks shows that these technologies are ready for adoption in real-world\nsystems.\n", "versions": [{"version": "v1", "created": "Sun, 25 Apr 2021 19:15:27 GMT"}, {"version": "v2", "created": "Mon, 24 May 2021 19:26:35 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Naphade", "Milind", ""], ["Wang", "Shuo", ""], ["Anastasiu", "David C.", ""], ["Tang", "Zheng", ""], ["Chang", "Ming-Ching", ""], ["Yang", "Xiaodong", ""], ["Yao", "Yue", ""], ["Zheng", "Liang", ""], ["Chakraborty", "Pranamesh", ""], ["Lopez", "Christian E.", ""], ["Sharma", "Anuj", ""], ["Feng", "Qi", ""], ["Ablavsky", "Vitaly", ""], ["Sclaroff", "Stan", ""]]}, {"id": "2104.12245", "submitter": "Chuong Nguyen", "authors": "Chuong H. Nguyen, Thuy C. Nguyen, Anh H. Vo, Yamazaki Masayuki", "title": "Single Stage Class Agnostic Common Object Detection: A Simple Baseline", "comments": "This paper is accepted to International Conference on Pattern\n  Recognition Applications and Methods (ICPRAM) 2021", "journal-ref": null, "doi": "10.5220/0010242303960407", "report-no": "ISBN 978-989-758-486-2 ISSN 2184-4313, pages 396-407", "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper addresses the problem of common object detection, which aims to\ndetect objects of similar categories from a set of images. Although it shares\nsome similarities with the standard object detection and co-segmentation,\ncommon object detection, recently promoted by \\cite{Jiang2019a}, has some\nunique advantages and challenges. First, it is designed to work on both\nclosed-set and open-set conditions, a.k.a. known and unknown objects. Second,\nit must be able to match objects of the same category but not restricted to the\nsame instance, texture, or posture. Third, it can distinguish multiple objects.\nIn this work, we introduce the Single Stage Common Object Detection (SSCOD) to\ndetect class-agnostic common objects from an image set. The proposed method is\nbuilt upon the standard single-stage object detector. Furthermore, an embedded\nbranch is introduced to generate the object's representation feature, and their\nsimilarity is measured by cosine distance. Experiments are conducted on PASCAL\nVOC 2007 and COCO 2014 datasets. While being simple and flexible, our proposed\nSSCOD built upon ATSSNet performs significantly better than the baseline of the\nstandard object detection, while still be able to match objects of unknown\ncategories. Our source code can be found at\n\\href{https://github.com/cybercore-co-ltd/Single-Stage-Common-Object-Detection}{(URL)}\n", "versions": [{"version": "v1", "created": "Sun, 25 Apr 2021 20:14:28 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Nguyen", "Chuong H.", ""], ["Nguyen", "Thuy C.", ""], ["Vo", "Anh H.", ""], ["Masayuki", "Yamazaki", ""]]}, {"id": "2104.12276", "submitter": "Yuming Du", "authors": "Yuming Du, Yang Xiao, Vincent Lepetit", "title": "Learning to Better Segment Objects from Unseen Classes with Unlabeled\n  Videos", "comments": "18 pages, 14 figures. See project page\n  https://dulucas.github.io/Homepage/gbopt/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The ability to localize and segment objects from unseen classes would open\nthe door to new applications, such as autonomous object learning in active\nvision. Nonetheless, improving the performance on unseen classes requires\nadditional training data, while manually annotating the objects of the unseen\nclasses can be labor-extensive and expensive. In this paper, we explore the use\nof unlabeled video sequences to automatically generate training data for\nobjects of unseen classes. It is in principle possible to apply existing video\nsegmentation methods to unlabeled videos and automatically obtain object masks,\nwhich can then be used as a training set even for classes with no manual labels\navailable. However, our experiments show that these methods do not perform well\nenough for this purpose. We therefore introduce a Bayesian method that is\nspecifically designed to automatically create such a training set: Our method\nstarts from a set of object proposals and relies on (non-realistic)\nanalysis-by-synthesis to select the correct ones by performing an efficient\noptimization over all the frames simultaneously. Through extensive experiments,\nwe show that our method can generate a high-quality training set which\nsignificantly boosts the performance of segmenting objects of unseen classes.\nWe thus believe that our method could open the door for open-world instance\nsegmentation using abundant Internet videos.\n", "versions": [{"version": "v1", "created": "Sun, 25 Apr 2021 22:05:44 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Du", "Yuming", ""], ["Xiao", "Yang", ""], ["Lepetit", "Vincent", ""]]}, {"id": "2104.12287", "submitter": "Saheb Chhabra", "authors": "Saheb Chhabra, Puspita Majumdar, Mayank Vatsa, Richa Singh", "title": "Class Equilibrium using Coulomb's Law", "comments": "Accepted at IJCNN 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Projection algorithms learn a transformation function to project the data\nfrom input space to the feature space, with the objective of increasing the\ninter-class distance. However, increasing the inter-class distance can affect\nthe intra-class distance. Maintaining an optimal inter-class separation among\nthe classes without affecting the intra-class distance of the data distribution\nis a challenging task. In this paper, inspired by the Coulomb's law of\nElectrostatics, we propose a new algorithm to compute the equilibrium space of\nany data distribution where the separation among the classes is optimal. The\nalgorithm further learns the transformation between the input space and\nequilibrium space to perform classification in the equilibrium space. The\nperformance of the proposed algorithm is evaluated on four publicly available\ndatasets at three different resolutions. It is observed that the proposed\nalgorithm performs well for low-resolution images.\n", "versions": [{"version": "v1", "created": "Sun, 25 Apr 2021 23:38:06 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Chhabra", "Saheb", ""], ["Majumdar", "Puspita", ""], ["Vatsa", "Mayank", ""], ["Singh", "Richa", ""]]}, {"id": "2104.12290", "submitter": "Gokhan Egri", "authors": "Gokhan Egri, Todd Zickler", "title": "StegaPos: Preventing Crops and Splices with Imperceptible Positional\n  Encodings", "comments": "For NeurIPS 2021 submission, 8 pages (main), 18 pages (total)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a model for differentiating between images that are authentic\ncopies of ones published by photographers, and images that have been\nmanipulated by cropping, splicing or downsampling after publication. The model\ncomprises an encoder that resides with the photographer and a matching decoder\nthat is available to observers. The encoder learns to embed imperceptible\npositional signatures into image values prior to publication. The decoder\nlearns to use these steganographic positional (\"stegapos\") signatures to\ndetermine, for each small image patch, the 2D positional coordinates that were\nheld by the patch in its originally-published image. Crop, splice and\ndownsample edits become detectable by the inconsistencies they cause in the\nhidden positional signatures. We find that training the encoder and decoder\ntogether produces a model that imperceptibly encodes position, and that enables\nsuperior performance on established benchmarks for splice detection and high\naccuracy on a new benchmark for crop detection.\n", "versions": [{"version": "v1", "created": "Sun, 25 Apr 2021 23:42:29 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Egri", "Gokhan", ""], ["Zickler", "Todd", ""]]}, {"id": "2104.12294", "submitter": "Mohammad Rahimzadeh", "authors": "Mohammad Rahimzadeh, Soroush Parvin, Elnaz Safi, Mohammad Reza\n  Mohammadi", "title": "Wise-SrNet: A Novel Architecture for Enhancing Image Classification by\n  Learning Spatial Resolution of Feature Maps", "comments": "The code is shared at\n  https://github.com/mr7495/image-classification-spatial", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  One of the main challenges since the advancement of convolutional neural\nnetworks is how to connect the extracted feature map to the final\nclassification layer. VGG models used two sets of fully connected layers for\nthe classification part of their architectures, which significantly increases\nthe number of models' weights. ResNet and next deep convolutional models used\nthe Global Average Pooling (GAP) layer to compress the feature map and feed it\nto the classification layer. Although using the GAP layer reduces the\ncomputational cost, but also causes losing spatial resolution of the feature\nmap, which results in decreasing learning efficiency. In this paper, we aim to\ntackle this problem by replacing the GAP layer with a new architecture called\nWise-SrNet. It is inspired by the depthwise convolutional idea and is designed\nfor processing spatial resolution and also not increasing computational cost.\nWe have evaluated our method using three different datasets: Intel Image\nClassification Challenge, MIT Indoors Scenes, and a part of the ImageNet\ndataset. We investigated the implementation of our architecture on several\nmodels of Inception, ResNet and DensNet families. Applying our architecture has\nrevealed a significant effect on increasing convergence speed and accuracy. Our\nExperiments on images with 224x224 resolution increased the Top-1 accuracy\nbetween 2% to 8% on different datasets and models. Running our models on\n512x512 resolution images of the MIT Indoors Scenes dataset showed a notable\nresult of improving the Top-1 accuracy within 3% to 26%. We will also\ndemonstrate the GAP layer's disadvantage when the input images are large and\nthe number of classes is not few. In this circumstance, our proposed\narchitecture can do a great help in enhancing classification results. The code\nis shared at https://github.com/mr7495/image-classification-spatial.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 00:37:11 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Rahimzadeh", "Mohammad", ""], ["Parvin", "Soroush", ""], ["Safi", "Elnaz", ""], ["Mohammadi", "Mohammad Reza", ""]]}, {"id": "2104.12297", "submitter": "Haoran Xie", "authors": "Zhengyu Huang, Yichen Peng, Tomohiro Hibino, Chunqi Zhao, Haoran Xie,\n  Tsukasa Fukusato, Kazunori Miyata", "title": "dualFace:Two-Stage Drawing Guidance for Freehand Portrait Sketching", "comments": "Accepted in the Journal of Computational Visual Media Conference\n  2021. 13 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we propose dualFace, a portrait drawing interface to assist\nusers with different levels of drawing skills to complete recognizable and\nauthentic face sketches. dualFace consists of two-stage drawing assistance to\nprovide global and local visual guidance: global guidance, which helps users\ndraw contour lines of portraits (i.e., geometric structure), and local\nguidance, which helps users draws details of facial parts (which conform to\nuser-drawn contour lines), inspired by traditional artist workflows in portrait\ndrawing. In the stage of global guidance, the user draws several contour lines,\nand dualFace then searches several relevant images from an internal database\nand displays the suggested face contour lines over the background of the\ncanvas. In the stage of local guidance, we synthesize detailed portrait images\nwith a deep generative model from user-drawn contour lines, but use the\nsynthesized results as detailed drawing guidance. We conducted a user study to\nverify the effectiveness of dualFace, and we confirmed that dualFace\nsignificantly helps achieve a detailed portrait sketch. see\nhttp://www.jaist.ac.jp/~xie/dualface.html\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 00:56:37 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Huang", "Zhengyu", ""], ["Peng", "Yichen", ""], ["Hibino", "Tomohiro", ""], ["Zhao", "Chunqi", ""], ["Xie", "Haoran", ""], ["Fukusato", "Tsukasa", ""], ["Miyata", "Kazunori", ""]]}, {"id": "2104.12300", "submitter": "Ricky Ma", "authors": "Ricky Ma (The University of British Columbia)", "title": "ODDObjects: A Framework for Multiclass Unsupervised Anomaly Detection on\n  Masked Objects", "comments": "11 pages, 15 Postscript figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper presents a novel framework for unsupervised anomaly detection on\nmasked objects called ODDObjects, which stands for Out-of-Distribution\nDetection on Objects. ODDObjects is designed to detect anomalies of various\ncategories using unsupervised autoencoders trained on COCO-style datasets. The\nmethod utilizes autoencoder-based image reconstruction, where high\nreconstruction error indicates the possibility of an anomaly. The framework\nextends previous work on anomaly detection with autoencoders, comparing\nstate-of-the-art models trained on object recognition datasets. Various model\narchitectures were compared, and experimental results show that\nmemory-augmented deep convolutional autoencoders perform the best at detecting\nout-of-distribution objects.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 01:13:28 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Ma", "Ricky", "", "The University of British Columbia"]]}, {"id": "2104.12335", "submitter": "Yingchen Yu", "authors": "Yingchen Yu, Fangneng Zhan, Rongliang Wu, Jianxiong Pan, Kaiwen Cui,\n  Shijian Lu, Feiying Ma, Xuansong Xie, Chunyan Miao", "title": "Diverse Image Inpainting with Bidirectional and Autoregressive\n  Transformers", "comments": "11 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image inpainting is an underdetermined inverse problem, which naturally\nallows diverse contents to fill up the missing or corrupted regions\nrealistically. Prevalent approaches using convolutional neural networks (CNNs)\ncan synthesize visually pleasant contents, but CNNs suffer from limited\nperception fields for capturing global features. With image-level attention,\ntransformers enable to model long-range dependencies and generate diverse\ncontents with autoregressive modeling of pixel-sequence distributions. However,\nthe unidirectional attention in autoregressive transformers is suboptimal as\ncorrupted image regions may have arbitrary shapes with contexts from any\ndirection. We propose BAT-Fill, an innovative image inpainting framework that\nintroduces a novel bidirectional autoregressive transformer (BAT) for image\ninpainting. BAT utilizes the transformers to learn autoregressive\ndistributions, which naturally allows the diverse generation of missing\ncontents. In addition, it incorporates the masked language model like BERT,\nwhich enables bidirectionally modeling of contextual information of missing\nregions for better image completion. Extensive experiments over multiple\ndatasets show that BAT-Fill achieves superior diversity and fidelity in image\ninpainting qualitatively and quantitatively.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 03:52:27 GMT"}, {"version": "v2", "created": "Fri, 30 Apr 2021 10:54:10 GMT"}, {"version": "v3", "created": "Tue, 1 Jun 2021 04:10:48 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Yu", "Yingchen", ""], ["Zhan", "Fangneng", ""], ["Wu", "Rongliang", ""], ["Pan", "Jianxiong", ""], ["Cui", "Kaiwen", ""], ["Lu", "Shijian", ""], ["Ma", "Feiying", ""], ["Xie", "Xuansong", ""], ["Miao", "Chunyan", ""]]}, {"id": "2104.12345", "submitter": "Decky Aspandi", "authors": "Nuria Rodriguez-Diaz, Decky Aspandi, Federico Sukno, Xavier Binefa", "title": "Machine Learning-based Lie Detector applied to a Novel Annotated Game\n  Dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Lie detection is considered a concern for everyone in their day to day life\ngiven its impact on human interactions. Thus, people normally pay attention to\nboth what their interlocutors are saying and also to their visual appearances,\nincluding faces, to try to find any signs that indicate whether the person is\ntelling the truth or not. While automatic lie detection may help us to\nunderstand this lying characteristics, current systems are still fairly\nlimited, partly due to lack of adequate datasets to evaluate their performance\nin realistic scenarios. In this work, we have collected an annotated dataset of\nfacial images, comprising both 2D and 3D information of several participants\nduring a card game that encourages players to lie. Using our collected dataset,\nWe evaluated several types of machine learning-based lie detectors in terms of\ntheir generalization, person-specific and cross-domain experiments. Our results\nshow that models based on deep learning achieve the best accuracy, reaching up\nto 57\\% for the generalization task and 63\\% when dealing with a single\nparticipant. Finally, we also highlight the limitation of the deep learning\nbased lie detector when dealing with cross-domain lie detection tasks.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 04:48:42 GMT"}, {"version": "v2", "created": "Wed, 30 Jun 2021 04:00:37 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Rodriguez-Diaz", "Nuria", ""], ["Aspandi", "Decky", ""], ["Sukno", "Federico", ""], ["Binefa", "Xavier", ""]]}, {"id": "2104.12347", "submitter": "Fang Aiqing", "authors": "Aiqing Fang, Xinbo Zhao, Jiaqi Yang, Yanning Zhang", "title": "Dynamic Image Restoration and Fusion Based on Dynamic Degradation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  The deep-learning-based image restoration and fusion methods have achieved\nremarkable results. However, the existing restoration and fusion methods paid\nlittle research attention to the robustness problem caused by dynamic\ndegradation. In this paper, we propose a novel dynamic image restoration and\nfusion neural network, termed as DDRF-Net, which is capable of solving two\nproblems, i.e., static restoration and fusion, dynamic degradation. In order to\nsolve the static fusion problem of existing methods, dynamic convolution is\nintroduced to learn dynamic restoration and fusion weights. In addition, a\ndynamic degradation kernel is proposed to improve the robustness of image\nrestoration and fusion. Our network framework can effectively combine image\ndegradation with image fusion tasks, provide more detailed information for\nimage fusion tasks through image restoration loss, and optimize image\nrestoration tasks through image fusion loss. Therefore, the stumbling blocks of\ndeep learning in image fusion, e.g., static fusion weight and specifically\ndesigned network architecture, are greatly mitigated. Extensive experiments\nshow that our method is more superior compared with the state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 04:59:55 GMT"}, {"version": "v2", "created": "Wed, 28 Apr 2021 08:57:03 GMT"}, {"version": "v3", "created": "Fri, 30 Apr 2021 12:02:03 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Fang", "Aiqing", ""], ["Zhao", "Xinbo", ""], ["Yang", "Jiaqi", ""], ["Zhang", "Yanning", ""]]}, {"id": "2104.12357", "submitter": "Yuzhi Zhao", "authors": "Yuzhi Zhao, Lai-Man Po, Wing-Yin Yu, Yasar Abbas Ur Rehman, Mengyang\n  Liu, Yujia Zhang, Weifeng Ou", "title": "VCGAN: Video Colorization with Hybrid Generative Adversarial Network", "comments": "Submitted Major Revision Manuscript of IEEE Transactions on\n  Multimedia (TMM)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a hybrid recurrent Video Colorization with Hybrid Generative\nAdversarial Network (VCGAN), an improved approach to video colorization using\nend-to-end learning. The VCGAN addresses two prevalent issues in the video\ncolorization domain: Temporal consistency and unification of colorization\nnetwork and refinement network into a single architecture. To enhance\ncolorization quality and spatiotemporal consistency, the mainstream of\ngenerator in VCGAN is assisted by two additional networks, i.e., global feature\nextractor and placeholder feature extractor, respectively. The global feature\nextractor encodes the global semantics of grayscale input to enhance\ncolorization quality, whereas the placeholder feature extractor acts as a\nfeedback connection to encode the semantics of the previous colorized frame in\norder to maintain spatiotemporal consistency. If changing the input for\nplaceholder feature extractor as grayscale input, the hybrid VCGAN also has the\npotential to perform image colorization. To improve the consistency of far\nframes, we propose a dense long-term loss that smooths the temporal disparity\nof every two remote frames. Trained with colorization and temporal losses\njointly, VCGAN strikes a good balance between color vividness and video\ncontinuity. Experimental results demonstrate that VCGAN produces higher-quality\nand temporally more consistent colorful videos than existing approaches.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 05:50:53 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Zhao", "Yuzhi", ""], ["Po", "Lai-Man", ""], ["Yu", "Wing-Yin", ""], ["Rehman", "Yasar Abbas Ur", ""], ["Liu", "Mengyang", ""], ["Zhang", "Yujia", ""], ["Ou", "Weifeng", ""]]}, {"id": "2104.12376", "submitter": "Max-Heinrich Laves", "authors": "Max-Heinrich Laves, Sontje Ihler, Jacob F. Fast, L\\\"uder A. Kahrs,\n  Tobias Ortmaier", "title": "Recalibration of Aleatoric and Epistemic Regression Uncertainty in\n  Medical Imaging", "comments": "Accepted for publication at the Journal of Machine Learning for\n  Biomedical Imaging (MELBA) https://melba-journal.org", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The consideration of predictive uncertainty in medical imaging with deep\nlearning is of utmost importance. We apply estimation of both aleatoric and\nepistemic uncertainty by variational Bayesian inference with Monte Carlo\ndropout to regression tasks and show that predictive uncertainty is\nsystematically underestimated. We apply $ \\sigma $ scaling with a single scalar\nvalue; a simple, yet effective calibration method for both types of\nuncertainty. The performance of our approach is evaluated on a variety of\ncommon medical regression data sets using different state-of-the-art\nconvolutional network architectures. In our experiments, $ \\sigma $ scaling is\nable to reliably recalibrate predictive uncertainty. It is easy to implement\nand maintains the accuracy. Well-calibrated uncertainty in regression allows\nrobust rejection of unreliable predictions or detection of out-of-distribution\nsamples. Our source code is available at\nhttps://github.com/mlaves/well-calibrated-regression-uncertainty\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 07:18:58 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Laves", "Max-Heinrich", ""], ["Ihler", "Sontje", ""], ["Fast", "Jacob F.", ""], ["Kahrs", "L\u00fcder A.", ""], ["Ortmaier", "Tobias", ""]]}, {"id": "2104.12378", "submitter": "Bangjie Yin", "authors": "Wenxuan Wang and Bangjie Yin and Taiping Yao and Li Zhang and Yanwei\n  Fu and Shouhong Ding and Jilin Li and Feiyue Huang and Xiangyang Xue", "title": "Delving into Data: Effectively Substitute Training for Black-box Attack", "comments": "10 pages, 6 figures, 6 tables, 1 algorithm, To appear in CVPR 2021 as\n  a poster paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Deep models have shown their vulnerability when processing adversarial\nsamples. As for the black-box attack, without access to the architecture and\nweights of the attacked model, training a substitute model for adversarial\nattacks has attracted wide attention. Previous substitute training approaches\nfocus on stealing the knowledge of the target model based on real training data\nor synthetic data, without exploring what kind of data can further improve the\ntransferability between the substitute and target models. In this paper, we\npropose a novel perspective substitute training that focuses on designing the\ndistribution of data used in the knowledge stealing process. More specifically,\na diverse data generation module is proposed to synthesize large-scale data\nwith wide distribution. And adversarial substitute training strategy is\nintroduced to focus on the data distributed near the decision boundary. The\ncombination of these two modules can further boost the consistency of the\nsubstitute model and target model, which greatly improves the effectiveness of\nadversarial attack. Extensive experiments demonstrate the efficacy of our\nmethod against state-of-the-art competitors under non-target and target attack\nsettings. Detailed visualization and analysis are also provided to help\nunderstand the advantage of our method.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 07:26:29 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Wang", "Wenxuan", ""], ["Yin", "Bangjie", ""], ["Yao", "Taiping", ""], ["Zhang", "Li", ""], ["Fu", "Yanwei", ""], ["Ding", "Shouhong", ""], ["Li", "Jilin", ""], ["Huang", "Feiyue", ""], ["Xue", "Xiangyang", ""]]}, {"id": "2104.12389", "submitter": "Huanyu He", "authors": "Yuang Zhang, Huanyu He, Jianguo Li, Yuxi Li, John See, Weiyao Lin", "title": "Variational Pedestrian Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Pedestrian detection in a crowd is a challenging task due to a high number of\nmutually-occluding human instances, which brings ambiguity and optimization\ndifficulties to the current IoU-based ground truth assignment procedure in\nclassical object detection methods. In this paper, we develop a unique\nperspective of pedestrian detection as a variational inference problem. We\nformulate a novel and efficient algorithm for pedestrian detection by modeling\nthe dense proposals as a latent variable while proposing a customized Auto\nEncoding Variational Bayes (AEVB) algorithm. Through the optimization of our\nproposed algorithm, a classical detector can be fashioned into a variational\npedestrian detector. Experiments conducted on CrowdHuman and CityPersons\ndatasets show that the proposed algorithm serves as an efficient solution to\nhandle the dense pedestrian detection problem for the case of single-stage\ndetectors. Our method can also be flexibly applied to two-stage detectors,\nachieving notable performance enhancement.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 08:06:41 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Zhang", "Yuang", ""], ["He", "Huanyu", ""], ["Li", "Jianguo", ""], ["Li", "Yuxi", ""], ["See", "John", ""], ["Lin", "Weiyao", ""]]}, {"id": "2104.12404", "submitter": "Ciaran Eising", "authors": "Letizia Mariotti and Ciaran Eising", "title": "Spherical formulation of geometric motion segmentation constraints in\n  fisheye cameras", "comments": "arXiv admin note: text overlap with arXiv:2003.03262", "journal-ref": "IEEE Transactions on Intelligent Transportation Systems, 2021", "doi": "10.1109/TITS.2020.3042759", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce a visual motion segmentation method employing spherical geometry\nfor fisheye cameras and automoated driving. Three commonly used geometric\nconstraints in pin-hole imagery (the positive height, positive depth and\nepipolar constraints) are reformulated to spherical coordinates, making them\ninvariant to specific camera configurations as long as the camera calibration\nis known. A fourth constraint, known as the anti-parallel constraint, is added\nto resolve motion-parallax ambiguity, to support the detection of moving\nobjects undergoing parallel or near-parallel motion with respect to the host\nvehicle. A final constraint constraint is described, known as the spherical\nthree-view constraint, is described though not employed in our proposed\nalgorithm. Results are presented and analyzed that demonstrate that the\nproposal is an effective motion segmentation approach for direct employment on\nfisheye imagery.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 08:48:12 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Mariotti", "Letizia", ""], ["Eising", "Ciaran", ""]]}, {"id": "2104.12417", "submitter": "Augusto Luis Ballardini PhD", "authors": "Augusto Luis Ballardini and \\'Alvaro Hern\\'andez and Miguel \\'Angel\n  Sotelo", "title": "Model Guided Road Intersection Classification", "comments": "To be presented at the 2021 32nd IEEE Intelligent Vehicles Symposium\n  (IV) (IV 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Understanding complex scenarios from in-vehicle cameras is essential for\nsafely operating autonomous driving systems in densely populated areas. Among\nthese, intersection areas are one of the most critical as they concentrate a\nconsiderable number of traffic accidents and fatalities. Detecting and\nunderstanding the scene configuration of these usually crowded areas is then of\nextreme importance for both autonomous vehicles and modern ADAS aimed at\npreventing road crashes and increasing the safety of vulnerable road users.\nThis work investigates inter-section classification from RGB images using\nwell-consolidate neural network approaches along with a method to enhance the\nresults based on the teacher/student training paradigm. An extensive\nexperimental activity aimed at identifying the best input configuration and\nevaluating different network parameters on both the well-known KITTI dataset\nand the new KITTI-360 sequences shows that our method outperforms current\nstate-of-the-art approaches on a per-frame basis and prove the effectiveness of\nthe proposed learning scheme.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 09:15:28 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Ballardini", "Augusto Luis", ""], ["Hern\u00e1ndez", "\u00c1lvaro", ""], ["Sotelo", "Miguel \u00c1ngel", ""]]}, {"id": "2104.12419", "submitter": "Quentin Paletta", "authors": "Quentin Paletta, Anthony Hu, Guillaume Arbod, Joan Lasenby", "title": "ECLIPSE : Envisioning Cloud Induced Perturbations in Solar Energy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient integration of solar energy into the electricity mix depends on a\nreliable anticipation of its intermittency. A promising approach to forecast\nthe temporal variability of solar irradiance resulting from the cloud cover\ndynamics, is based on the analysis of sequences of ground-taken sky images.\nDespite encouraging results, a recurrent limitation of current Deep Learning\napproaches lies in the ubiquitous tendency of reacting to past observations\nrather than actively anticipating future events. This leads to a systematic\ntemporal lag and little ability to predict sudden events. To address this\nchallenge, we introduce ECLIPSE, a spatio-temporal neural network architecture\nthat models cloud motion from sky images to predict both future segmented\nimages and corresponding irradiance levels. We show that ECLIPSE anticipates\ncritical events and considerably reduces temporal delay while generating\nvisually realistic futures.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 09:19:43 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Paletta", "Quentin", ""], ["Hu", "Anthony", ""], ["Arbod", "Guillaume", ""], ["Lasenby", "Joan", ""]]}, {"id": "2104.12446", "submitter": "Boris Ivanovic", "authors": "Boris Ivanovic, Kuan-Hui Lee, Pavel Tokmakov, Blake Wulfe, Rowan\n  McAllister, Adrien Gaidon, Marco Pavone", "title": "Heterogeneous-Agent Trajectory Forecasting Incorporating Class\n  Uncertainty", "comments": "17 pages, 12 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reasoning about the future behavior of other agents is critical to safe robot\nnavigation. The multiplicity of plausible futures is further amplified by the\nuncertainty inherent to agent state estimation from data, including positions,\nvelocities, and semantic class. Forecasting methods, however, typically neglect\nclass uncertainty, conditioning instead only on the agent's most likely class,\neven though perception models often return full class distributions. To exploit\nthis information, we present HAICU, a method for heterogeneous-agent trajectory\nforecasting that explicitly incorporates agents' class probabilities. We\nadditionally present PUP, a new challenging real-world autonomous driving\ndataset, to investigate the impact of Perceptual Uncertainty in Prediction. It\ncontains challenging crowded scenes with unfiltered agent class probabilities\nthat reflect the long-tail of current state-of-the-art perception systems. We\ndemonstrate that incorporating class probabilities in trajectory forecasting\nsignificantly improves performance in the face of uncertainty, and enables new\nforecasting capabilities such as counterfactual predictions.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 10:28:34 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Ivanovic", "Boris", ""], ["Lee", "Kuan-Hui", ""], ["Tokmakov", "Pavel", ""], ["Wulfe", "Blake", ""], ["McAllister", "Rowan", ""], ["Gaidon", "Adrien", ""], ["Pavone", "Marco", ""]]}, {"id": "2104.12456", "submitter": "Thomas Bird", "authors": "Thomas Bird, Johannes Ball\\'e, Saurabh Singh, Philip A. Chou", "title": "3D Scene Compression through Entropy Penalized Neural Representation\n  Functions", "comments": "accepted (in an abridged format) as a contribution to the\n  Learning-based Image Coding special session of the Picture Coding Symposium\n  2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Some forms of novel visual media enable the viewer to explore a 3D scene from\narbitrary viewpoints, by interpolating between a discrete set of original\nviews. Compared to 2D imagery, these types of applications require much larger\namounts of storage space, which we seek to reduce. Existing approaches for\ncompressing 3D scenes are based on a separation of compression and rendering:\neach of the original views is compressed using traditional 2D image formats;\nthe receiver decompresses the views and then performs the rendering. We unify\nthese steps by directly compressing an implicit representation of the scene, a\nfunction that maps spatial coordinates to a radiance vector field, which can\nthen be queried to render arbitrary viewpoints. The function is implemented as\na neural network and jointly trained for reconstruction as well as\ncompressibility, in an end-to-end manner, with the use of an entropy penalty on\nthe parameters. Our method significantly outperforms a state-of-the-art\nconventional approach for scene compression, achieving simultaneously higher\nquality reconstructions and lower bitrates. Furthermore, we show that the\nperformance at lower bitrates can be improved by jointly representing multiple\nscenes using a soft form of parameter sharing.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 10:36:47 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Bird", "Thomas", ""], ["Ball\u00e9", "Johannes", ""], ["Singh", "Saurabh", ""], ["Chou", "Philip A.", ""]]}, {"id": "2104.12462", "submitter": "Francesc Llu\\'is", "authors": "Francesc Llu\\'is, Vasileios Chatziioannou, Alex Hofmann", "title": "Points2Sound: From mono to binaural audio using 3D point cloud scenes", "comments": "Demo: https://youtu.be/oy7DCMMC3Lk", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CV eess.AS", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Binaural sound that matches the visual counterpart is crucial to bring\nmeaningful and immersive experiences to people in augmented reality (AR) and\nvirtual reality (VR) applications. Recent works have shown the possibility to\ngenerate binaural audio from mono using 2D visual information as guidance.\nUsing 3D visual information may allow for a more accurate representation of a\nvirtual audio scene for VR/AR applications. This paper proposes Points2Sound, a\nmulti-modal deep learning model which generates a binaural version from mono\naudio using 3D point cloud scenes. Specifically, Points2Sound consist of a\nvision network which extracts visual features from the point cloud scene to\ncondition an audio network, which operates in the waveform domain, to\nsynthesize the binaural version. Both quantitative and perceptual evaluations\nindicate that our proposed model is preferred over a reference case, based on a\nrecent 2D mono-to-binaural model.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 10:44:01 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Llu\u00eds", "Francesc", ""], ["Chatziioannou", "Vasileios", ""], ["Hofmann", "Alex", ""]]}, {"id": "2104.12464", "submitter": "Jing Tan", "authors": "Jing Tan, Shan Zhao, Pengfei Xiong, Jiangyu Liu, Haoqiang Fan,\n  Shuaicheng Liu", "title": "Practical Wide-Angle Portraits Correction with Deep Structured Models", "comments": "This work has been accepted to CVPR2021. The project link is\n  https://github.com/TanJing94/Deep_Portraits_Correction", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wide-angle portraits often enjoy expanded views. However, they contain\nperspective distortions, especially noticeable when capturing group portrait\nphotos, where the background is skewed and faces are stretched. This paper\nintroduces the first deep learning based approach to remove such artifacts from\nfreely-shot photos. Specifically, given a wide-angle portrait as input, we\nbuild a cascaded network consisting of a LineNet, a ShapeNet, and a transition\nmodule (TM), which corrects perspective distortions on the background, adapts\nto the stereographic projection on facial regions, and achieves smooth\ntransitions between these two projections, accordingly. To train our network,\nwe build the first perspective portrait dataset with a large diversity in\nidentities, scenes and camera modules. For the quantitative evaluation, we\nintroduce two novel metrics, line consistency and face congruence. Compared to\nthe previous state-of-the-art approach, our method does not require camera\ndistortion parameters. We demonstrate that our approach significantly\noutperforms the previous state-of-the-art approach both qualitatively and\nquantitatively.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 10:47:35 GMT"}, {"version": "v2", "created": "Tue, 27 Apr 2021 12:18:53 GMT"}, {"version": "v3", "created": "Wed, 28 Apr 2021 06:22:21 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Tan", "Jing", ""], ["Zhao", "Shan", ""], ["Xiong", "Pengfei", ""], ["Liu", "Jiangyu", ""], ["Fan", "Haoqiang", ""], ["Liu", "Shuaicheng", ""]]}, {"id": "2104.12465", "submitter": "Jia-Hong Huang", "authors": "Jia-Hong Huang, Luka Murn, Marta Mrak, Marcel Worring", "title": "GPT2MVS: Generative Pre-trained Transformer-2 for Multi-modal Video\n  Summarization", "comments": "This paper is accepted by ACM International Conference on Multimedia\n  Retrieval (ICMR), 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Traditional video summarization methods generate fixed video representations\nregardless of user interest. Therefore such methods limit users' expectations\nin content search and exploration scenarios. Multi-modal video summarization is\none of the methods utilized to address this problem. When multi-modal video\nsummarization is used to help video exploration, a text-based query is\nconsidered as one of the main drivers of video summary generation, as it is\nuser-defined. Thus, encoding the text-based query and the video effectively are\nboth important for the task of multi-modal video summarization. In this work, a\nnew method is proposed that uses a specialized attention network and\ncontextualized word representations to tackle this task. The proposed model\nconsists of a contextualized video summary controller, multi-modal attention\nmechanisms, an interactive attention network, and a video summary generator.\nBased on the evaluation of the existing multi-modal video summarization\nbenchmark, experimental results show that the proposed model is effective with\nthe increase of +5.88% in accuracy and +4.06% increase of F1-score, compared\nwith the state-of-the-art method.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 10:50:37 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Huang", "Jia-Hong", ""], ["Murn", "Luka", ""], ["Mrak", "Marta", ""], ["Worring", "Marcel", ""]]}, {"id": "2104.12468", "submitter": "Subhankar Ghosh", "authors": "Subhankar Ghosh", "title": "Dynamic VAEs with Generative Replay for Continual Zero-shot Learning", "comments": "10 pages, 10 figures. arXiv admin note: text overlap with\n  arXiv:2102.03778", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continual zero-shot learning(CZSL) is a new domain to classify objects\nsequentially the model has not seen during training. It is more suitable than\nzero-shot and continual learning approaches in real-case scenarios when data\nmay come continually with only attributes for a few classes and attributes and\nfeatures for other classes. Continual learning(CL) suffers from catastrophic\nforgetting, and zero-shot learning(ZSL) models cannot classify objects like\nstate-of-the-art supervised classifiers due to lack of actual data(or features)\nduring training. This paper proposes a novel continual zero-shot learning\n(DVGR-CZSL) model that grows in size with each task and uses generative replay\nto update itself with previously learned classes to avoid forgetting. We\ndemonstrate our hybrid model(DVGR-CZSL) outperforms the baselines and is\neffective on several datasets, i.e., CUB, AWA1, AWA2, and aPY. We show our\nmethod is superior in task sequentially learning with ZSL(Zero-Shot Learning).\nWe also discuss our results on the SUN dataset.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 10:56:43 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Ghosh", "Subhankar", ""]]}, {"id": "2104.12469", "submitter": "Konstantin Klemmer", "authors": "Konstantin Klemmer, Sudipan Saha, Matthias Kahl, Tianlin Xu, Xiao\n  Xiang Zhu", "title": "Generative modeling of spatio-temporal weather patterns with extreme\n  event conditioning", "comments": "ICLR'21 Workshop AI: Modeling Oceans and Climate Change (AIMOCC)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG physics.ao-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep generative models are increasingly used to gain insights in the\ngeospatial data domain, e.g., for climate data. However, most existing\napproaches work with temporal snapshots or assume 1D time-series; few are able\nto capture spatio-temporal processes simultaneously. Beyond this, Earth-systems\ndata often exhibit highly irregular and complex patterns, for example caused by\nextreme weather events. Because of climate change, these phenomena are only\nincreasing in frequency. Here, we proposed a novel GAN-based approach for\ngenerating spatio-temporal weather patterns conditioned on detected extreme\nevents. Our approach augments GAN generator and discriminator with an encoded\nextreme weather event segmentation mask. These segmentation masks can be\ncreated from raw input using existing event detection frameworks. As such, our\napproach is highly modular and can be combined with custom GAN architectures.\nWe highlight the applicability of our proposed approach in experiments with\nreal-world surface radiation and zonal wind data.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 10:58:44 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Klemmer", "Konstantin", ""], ["Saha", "Sudipan", ""], ["Kahl", "Matthias", ""], ["Xu", "Tianlin", ""], ["Zhu", "Xiao Xiang", ""]]}, {"id": "2104.12471", "submitter": "Jia-Hong Huang", "authors": "Jia-Hong Huang, Ting-Wei Wu, Marcel Worring", "title": "Contextualized Keyword Representations for Multi-modal Retinal Image\n  Captioning", "comments": "This paper is accepted by ACM International Conference on Multimedia\n  Retrieval (ICMR), 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.IR cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Medical image captioning automatically generates a medical description to\ndescribe the content of a given medical image. A traditional medical image\ncaptioning model creates a medical description only based on a single medical\nimage input. Hence, an abstract medical description or concept is hard to be\ngenerated based on the traditional approach. Such a method limits the\neffectiveness of medical image captioning. Multi-modal medical image captioning\nis one of the approaches utilized to address this problem. In multi-modal\nmedical image captioning, textual input, e.g., expert-defined keywords, is\nconsidered as one of the main drivers of medical description generation. Thus,\nencoding the textual input and the medical image effectively are both important\nfor the task of multi-modal medical image captioning. In this work, a new\nend-to-end deep multi-modal medical image captioning model is proposed.\nContextualized keyword representations, textual feature reinforcement, and\nmasked self-attention are used to develop the proposed approach. Based on the\nevaluation of the existing multi-modal medical image captioning dataset,\nexperimental results show that the proposed model is effective with the\nincrease of +53.2% in BLEU-avg and +18.6% in CIDEr, compared with the\nstate-of-the-art method.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 11:08:13 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Huang", "Jia-Hong", ""], ["Wu", "Ting-Wei", ""], ["Worring", "Marcel", ""]]}, {"id": "2104.12476", "submitter": "Zhenliang He", "authors": "Zhenliang He, Meina Kan, Shiguang Shan", "title": "EigenGAN: Layer-Wise Eigen-Learning for GANs", "comments": "Code: https://github.com/LynnHo/EigenGAN-Tensorflow", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies on Generative Adversarial Network (GAN) reveal that different\nlayers of a generative CNN hold different semantics of the synthesized images.\nHowever, few GAN models have explicit dimensions to control the semantic\nattributes represented in a specific layer. This paper proposes EigenGAN which\nis able to unsupervisedly mine interpretable and controllable dimensions from\ndifferent generator layers. Specifically, EigenGAN embeds one linear subspace\nwith orthogonal basis into each generator layer. Via the adversarial training\nto learn a target distribution, these layer-wise subspaces automatically\ndiscover a set of \"eigen-dimensions\" at each layer corresponding to a set of\nsemantic attributes or interpretable variations. By traversing the coefficient\nof a specific eigen-dimension, the generator can produce samples with\ncontinuous changes corresponding to a specific semantic attribute. Taking the\nhuman face for example, EigenGAN can discover controllable dimensions for\nhigh-level concepts such as pose and gender in the subspace of deep layers, as\nwell as low-level concepts such as hue and color in the subspace of shallow\nlayers. Moreover, under the linear circumstance, we theoretically prove that\nour algorithm derives the principal components as PCA does. Codes can be found\nin https://github.com/LynnHo/EigenGAN-Tensorflow.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 11:14:37 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["He", "Zhenliang", ""], ["Kan", "Meina", ""], ["Shan", "Shiguang", ""]]}, {"id": "2104.12478", "submitter": "Fahdi Kanavati", "authors": "Fahdi Kanavati, Masayuki Tsuneki", "title": "A deep learning model for gastric diffuse-type adenocarcinoma\n  classification in whole slide images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gastric diffuse-type adenocarcinoma represents a disproportionately high\npercentage of cases of gastric cancers occurring in the young, and its relative\nincidence seems to be on the rise. Usually it affects the body of the stomach,\nand presents shorter duration and worse prognosis compared with the\ndifferentiated (intestinal) type adenocarcinoma. The main difficulty\nencountered in the differential diagnosis of gastric adenocarcinomas occurs\nwith the diffuse-type. As the cancer cells of diffuse-type adenocarcinoma are\noften single and inconspicuous in a background desmoplaia and inflammation, it\ncan often be mistaken for a wide variety of non-neoplastic lesions including\ngastritis or reactive endothelial cells seen in granulation tissue. In this\nstudy we trained deep learning models to classify gastric diffuse-type\nadenocarcinoma from WSIs. We evaluated the models on five test sets obtained\nfrom distinct sources, achieving receiver operator curve (ROC) area under the\ncurves (AUCs) in the range of 0.95-0.99. The highly promising results\ndemonstrate the potential of AI-based computational pathology for aiding\npathologists in their diagnostic workflow system.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 11:22:20 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Kanavati", "Fahdi", ""], ["Tsuneki", "Masayuki", ""]]}, {"id": "2104.12505", "submitter": "Yi Wang", "authors": "Yi Wang, Xinyu Hou, and Lap-Pui Chau", "title": "Dense Point Prediction: A Simple Baseline for Crowd Counting and\n  Localization", "comments": "6 pages. Accepted at ICME Workshop 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a simple yet effective crowd counting and\nlocalization network named SCALNet. Unlike most existing works that separate\nthe counting and localization tasks, we consider those tasks as a pixel-wise\ndense prediction problem and integrate them into an end-to-end framework.\nSpecifically, for crowd counting, we adopt a counting head supervised by the\nMean Square Error (MSE) loss. For crowd localization, the key insight is to\nrecognize the keypoint of people, i.e., the center point of heads. We propose a\nlocalization head to distinguish dense crowds trained by two loss functions,\ni.e., Negative-Suppressed Focal (NSF) loss and False-Positive (FP) loss, which\nbalances the positive/negative examples and handles the false-positive\npredictions. Experiments on the recent and large-scale benchmark, NWPU-Crowd,\nshow that our approach outperforms the state-of-the-art methods by more than 5%\nand 10% improvement in crowd localization and counting tasks, respectively. The\ncode is publicly available at https://github.com/WangyiNTU/SCALNet.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 12:08:08 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Wang", "Yi", ""], ["Hou", "Xinyu", ""], ["Chau", "Lap-Pui", ""]]}, {"id": "2104.12510", "submitter": "Zihao Wang", "authors": "Wang Zihao, Vandersteen Clair, Demarcy Thomas, Gnansia Dan, Raffaelli\n  Charles, Guevara Nicolas, Delingette Herve", "title": "Inner-ear Augmented Metal Artifact Reduction with Simulation-based 3D\n  Generative Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Metal Artifacts creates often difficulties for a high quality visual\nassessment of post-operative imaging in {c}omputed {t}omography (CT). A vast\nbody of methods have been proposed to tackle this issue, but {these} methods\nwere designed for regular CT scans and their performance is usually\ninsufficient when imaging tiny implants. In the context of post-operative\nhigh-resolution {CT} imaging, we propose a 3D metal {artifact} reduction\nalgorithm based on a generative adversarial neural network. It is based on the\nsimulation of physically realistic CT metal artifacts created by cochlea\nimplant electrodes on preoperative images. The generated images serve to train\na 3D generative adversarial networks for artifacts reduction. The proposed\napproach was assessed qualitatively and quantitatively on clinical conventional\nand cone-beam CT of cochlear implant postoperative images. These experiments\nshow that the proposed method {outperforms other} general metal artifact\nreduction approaches.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 12:22:56 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Zihao", "Wang", ""], ["Clair", "Vandersteen", ""], ["Thomas", "Demarcy", ""], ["Dan", "Gnansia", ""], ["Charles", "Raffaelli", ""], ["Nicolas", "Guevara", ""], ["Herve", "Delingette", ""]]}, {"id": "2104.12533", "submitter": "Zhengsu Chen", "authors": "Zhengsu Chen, Lingxi Xie, Jianwei Niu, Xuefeng Liu, Longhui Wei, Qi\n  Tian", "title": "Visformer: The Vision-friendly Transformer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The past year has witnessed the rapid development of applying the Transformer\nmodule to vision problems. While some researchers have demonstrated that\nTransformer-based models enjoy a favorable ability of fitting data, there are\nstill growing number of evidences showing that these models suffer over-fitting\nespecially when the training data is limited. This paper offers an empirical\nstudy by performing step-by-step operations to gradually transit a\nTransformer-based model to a convolution-based model. The results we obtain\nduring the transition process deliver useful messages for improving visual\nrecognition. Based on these observations, we propose a new architecture named\nVisformer, which is abbreviated from the `Vision-friendly Transformer'. With\nthe same computational complexity, Visformer outperforms both the\nTransformer-based and convolution-based models in terms of ImageNet\nclassification accuracy, and the advantage becomes more significant when the\nmodel complexity is lower or the training set is smaller. The code is available\nat https://github.com/danczs/Visformer.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 13:13:03 GMT"}, {"version": "v2", "created": "Tue, 27 Apr 2021 05:03:12 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Chen", "Zhengsu", ""], ["Xie", "Lingxi", ""], ["Niu", "Jianwei", ""], ["Liu", "Xuefeng", ""], ["Wei", "Longhui", ""], ["Tian", "Qi", ""]]}, {"id": "2104.12537", "submitter": "Ciaran Eising", "authors": "Markus Heimberger, Jonathan Horgan, Ciaran Hughes, John McDonald,\n  Senthil Yogamani", "title": "Computer vision in automated parking systems: Design, implementation and\n  challenges", "comments": null, "journal-ref": "Image and Vision Computing, Volume 68, December 2017, Pages 88-101", "doi": "10.1016/j.imavis.2017.07.002", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Automated driving is an active area of research in both industry and\nacademia. Automated Parking, which is automated driving in a restricted\nscenario of parking with low speed manoeuvring, is a key enabling product for\nfully autonomous driving systems. It is also an important milestone from the\nperspective of a higher end system built from the previous generation driver\nassistance systems comprising of collision warning, pedestrian detection, etc.\nIn this paper, we discuss the design and implementation of an automated parking\nsystem from the perspective of computer vision algorithms. Designing a low-cost\nsystem with functional safety is challenging and leads to a large gap between\nthe prototype and the end product, in order to handle all the corner cases. We\ndemonstrate how camera systems are crucial for addressing a range of automated\nparking use cases and also, to add robustness to systems based on active\ndistance measuring sensors, such as ultrasonics and radar. The key vision\nmodules which realize the parking use cases are 3D reconstruction, parking slot\nmarking recognition, freespace and vehicle/pedestrian detection. We detail the\nimportant parking use cases and demonstrate how to combine the vision modules\nto form a robust parking system. To the best of the authors' knowledge, this is\nthe first detailed discussion of a systemic view of a commercial automated\nparking system.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 13:18:02 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Heimberger", "Markus", ""], ["Horgan", "Jonathan", ""], ["Hughes", "Ciaran", ""], ["McDonald", "John", ""], ["Yogamani", "Senthil", ""]]}, {"id": "2104.12564", "submitter": "Meher Shashwat Nigam", "authors": "Stanislava Fedorova, Alberto Tono, Meher Shashwat Nigam, Jiayao Zhang,\n  Amirhossein Ahmadnia, Cecilia Bolognesi, Dominik L. Michels", "title": "Synthetic 3D Data Generation Pipeline for Geometric Deep Learning in\n  Architecture", "comments": "Project Page:\n  https://cdinstitute.github.io/Building-Dataset-Generator/", "journal-ref": null, "doi": "10.5194/isprs-archives-XLIII-B2-2021-337-2021", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the growing interest in deep learning algorithms and computational\ndesign in the architectural field, the need for large, accessible and diverse\narchitectural datasets increases. We decided to tackle this problem by\nconstructing a field-specific synthetic data generation pipeline that generates\nan arbitrary amount of 3D data along with the associated 2D and 3D annotations.\nThe variety of annotations, the flexibility to customize the generated building\nand dataset parameters make this framework suitable for multiple deep learning\ntasks, including geometric deep learning that requires direct 3D supervision.\nCreating our building data generation pipeline we leveraged architectural\nknowledge from experts in order to construct a framework that would be modular,\nextendable and would provide a sufficient amount of class-balanced data\nsamples. Moreover, we purposefully involve the researcher in the dataset\ncustomization allowing the introduction of additional building components,\nmaterial textures, building classes, number and type of annotations as well as\nthe number of views per 3D model sample. In this way, the framework would\nsatisfy different research requirements and would be adaptable to a large\nvariety of tasks. All code and data are made publicly available.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 13:32:03 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Fedorova", "Stanislava", ""], ["Tono", "Alberto", ""], ["Nigam", "Meher Shashwat", ""], ["Zhang", "Jiayao", ""], ["Ahmadnia", "Amirhossein", ""], ["Bolognesi", "Cecilia", ""], ["Michels", "Dominik L.", ""]]}, {"id": "2104.12565", "submitter": "Chuanguang Yang", "authors": "Chuanguang Yang, Zhulin An, Linhang Cai, Yongjun Xu", "title": "Mutual Contrastive Learning for Visual Representation Learning", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a collaborative learning method called Mutual Contrastive Learning\n(MCL) for general visual representation learning. The core idea of MCL is to\nperform mutual interaction and transfer of contrastive distributions among a\ncohort of models. Benefiting from MCL, each model can learn extra contrastive\nknowledge from others, leading to more meaningful feature representations for\nvisual recognition tasks. We emphasize that MCL is conceptually simple yet\nempirically powerful. It is a generic framework that can be applied to both\nsupervised and self-supervised representation learning. Experimental results on\nsupervised and self-supervised image classification, transfer learning and\nfew-shot learning show that MCL can lead to consistent performance gains,\ndemonstrating that MCL can guide the network to generate better feature\nrepresentations.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 13:32:33 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Yang", "Chuanguang", ""], ["An", "Zhulin", ""], ["Cai", "Linhang", ""], ["Xu", "Yongjun", ""]]}, {"id": "2104.12574", "submitter": "Yang Liu", "authors": "Yang Liu, Luiz G. Hafemann, Michael Jamieson, Mehrsan Javan", "title": "Detecting and Matching Related Objects with One Proposal Multiple\n  Predictions", "comments": "CVPR workshop 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tracking players in sports videos is commonly done in a tracking-by-detection\nframework, first detecting players in each frame, and then performing\nassociation over time. While for some sports tracking players is sufficient for\ngame analysis, sports like hockey, tennis and polo may require additional\ndetections, that include the object the player is holding (e.g. racket, stick).\nThe baseline solution for this problem involves detecting these objects as\nseparate classes, and matching them to player detections based on the\nintersection over union (IoU). This approach, however, leads to poor matching\nperformance in crowded situations, as it does not model the relationship\nbetween players and objects. In this paper, we propose a simple yet efficient\nway to detect and match players and related objects at once without extra cost,\nby considering an implicit association for prediction of multiple objects\nthrough the same proposal box. We evaluate the method on a dataset of broadcast\nice hockey videos, and also a new public dataset we introduce called COCO\n+Torso. On the ice hockey dataset, the proposed method boosts matching\nperformance from 57.1% to 81.4%, while also improving the meanAP of\nplayer+stick detections from 68.4% to 88.3%. On the COCO +Torso dataset, we see\nmatching improving from 47.9% to 65.2%. The COCO +Torso dataset, code and\npre-trained models will be released at\nhttps://github.com/foreverYoungGitHub/detect-and-match-related-objects.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 14:37:10 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Liu", "Yang", ""], ["Hafemann", "Luiz G.", ""], ["Jamieson", "Michael", ""], ["Javan", "Mehrsan", ""]]}, {"id": "2104.12579", "submitter": "Lo\\\"ic Cordone", "authors": "Lo\\\"ic Cordone, Beno\\^it Miramond and Sonia Ferrante", "title": "Learning from Event Cameras with Sparse Spiking Convolutional Neural\n  Networks", "comments": "Accepted to the International Joint Conference on Neural Networks\n  (IJCNN) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Convolutional neural networks (CNNs) are now the de facto solution for\ncomputer vision problems thanks to their impressive results and ease of\nlearning. These networks are composed of layers of connected units called\nartificial neurons, loosely modeling the neurons in a biological brain.\nHowever, their implementation on conventional hardware (CPU/GPU) results in\nhigh power consumption, making their integration on embedded systems difficult.\nIn a car for example, embedded algorithms have very high constraints in term of\nenergy, latency and accuracy. To design more efficient computer vision\nalgorithms, we propose to follow an end-to-end biologically inspired approach\nusing event cameras and spiking neural networks (SNNs). Event cameras output\nasynchronous and sparse events, providing an incredibly efficient data source,\nbut processing these events with synchronous and dense algorithms such as CNNs\ndoes not yield any significant benefits. To address this limitation, we use\nspiking neural networks (SNNs), which are more biologically realistic neural\nnetworks where units communicate using discrete spikes. Due to the nature of\ntheir operations, they are hardware friendly and energy-efficient, but training\nthem still remains a challenge. Our method enables the training of sparse\nspiking convolutional neural networks directly on event data, using the popular\ndeep learning framework PyTorch. The performances in terms of accuracy,\nsparsity and training time on the popular DVS128 Gesture Dataset make it\npossible to use this bio-inspired approach for the future embedding of\nreal-time applications on low-power neuromorphic hardware.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 13:52:01 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Cordone", "Lo\u00efc", ""], ["Miramond", "Beno\u00eet", ""], ["Ferrante", "Sonia", ""]]}, {"id": "2104.12581", "submitter": "Bochen Shen", "authors": "Longling Zhang, Bochen Shen, Ahmed Barnawi, Shan Xi, Neeraj Kumar, Yi\n  Wu", "title": "FedDPGAN: Federated Differentially Private Generative Adversarial\n  Networks Framework for the Detection of COVID-19 Pneumonia", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Existing deep learning technologies generally learn the features of chest\nX-ray data generated by Generative Adversarial Networks (GAN) to diagnose\nCOVID-19 pneumonia. However, the above methods have a critical challenge: data\nprivacy. GAN will leak the semantic information of the training data which can\nbe used to reconstruct the training samples by attackers, thereby this method\nwill leak the privacy of the patient. Furthermore, for this reason that is the\nlimitation of the training data sample, different hospitals jointly train the\nmodel through data sharing, which will also cause the privacy leakage. To solve\nthis problem, we adopt the Federated Learning (FL) frame-work which is a new\ntechnique being used to protect the data privacy. Under the FL framework and\nDifferentially Private thinking, we propose a FederatedDifferentially Private\nGenerative Adversarial Network (FedDPGAN) to detectCOVID-19 pneumonia for\nsustainable smart cities. Specifically, we use DP-GAN to privately generate\ndiverse patient data in which differential privacy technology is introduced to\nmake sure the privacy protection of the semantic information of training\ndataset. Furthermore, we leverage FL to allow hospitals to collaboratively\ntrain COVID-19 models without sharing the original data. Under Independent and\nIdentically Distributed (IID) and non-IID settings, The evaluation of the\nproposed model is on three types of chest X-ray (CXR) images dataset (COVID-19,\nnormal, and normal pneumonia). A large number of the truthful reports make the\nverification of our model can effectively diagnose COVID-19 without\ncompromising privacy.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 13:52:12 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Zhang", "Longling", ""], ["Shen", "Bochen", ""], ["Barnawi", "Ahmed", ""], ["Xi", "Shan", ""], ["Kumar", "Neeraj", ""], ["Wu", "Yi", ""]]}, {"id": "2104.12583", "submitter": "Ciaran Eising", "authors": "Jonathan Horgan, Ciar\\'an Hughes, John McDonald, Senthil Yogamani", "title": "Vision-based Driver Assistance Systems: Survey, Taxonomy and Advances", "comments": null, "journal-ref": "2015 IEEE 18th International Conference on Intelligent\n  Transportation Systems", "doi": "10.1109/ITSC.2015.329", "report-no": null, "categories": "cs.RO cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Vision-based driver assistance systems is one of the rapidly growing research\nareas of ITS, due to various factors such as the increased level of safety\nrequirements in automotive, computational power in embedded systems, and desire\nto get closer to autonomous driving. It is a cross disciplinary area\nencompassing specialised fields like computer vision, machine learning, robotic\nnavigation, embedded systems, automotive electronics and safety critical\nsoftware. In this paper, we survey the list of vision based advanced driver\nassistance systems with a consistent terminology and propose a taxonomy. We\nalso propose an abstract model in an attempt to formalize a top-down view of\napplication development to scale towards autonomous driving system.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 13:53:00 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Horgan", "Jonathan", ""], ["Hughes", "Ciar\u00e1n", ""], ["McDonald", "John", ""], ["Yogamani", "Senthil", ""]]}, {"id": "2104.12609", "submitter": "Chong Xiang", "authors": "Chong Xiang, Prateek Mittal", "title": "PatchGuard++: Efficient Provable Attack Detection against Adversarial\n  Patches", "comments": "ICLR 2021 Workshop on Security and Safety in Machine Learning Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An adversarial patch can arbitrarily manipulate image pixels within a\nrestricted region to induce model misclassification. The threat of this\nlocalized attack has gained significant attention because the adversary can\nmount a physically-realizable attack by attaching patches to the victim object.\nRecent provably robust defenses generally follow the PatchGuard framework by\nusing CNNs with small receptive fields and secure feature aggregation for\nrobust model predictions. In this paper, we extend PatchGuard to PatchGuard++\nfor provably detecting the adversarial patch attack to boost both provable\nrobust accuracy and clean accuracy. In PatchGuard++, we first use a CNN with\nsmall receptive fields for feature extraction so that the number of features\ncorrupted by the adversarial patch is bounded. Next, we apply masks in the\nfeature space and evaluate predictions on all possible masked feature maps.\nFinally, we extract a pattern from all masked predictions to catch the\nadversarial patch attack. We evaluate PatchGuard++ on ImageNette (a 10-class\nsubset of ImageNet), ImageNet, and CIFAR-10 and demonstrate that PatchGuard++\nsignificantly improves the provable robustness and clean performance.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 14:22:33 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Xiang", "Chong", ""], ["Mittal", "Prateek", ""]]}, {"id": "2104.12623", "submitter": "Sebastian Szyller", "authors": "Sebastian Szyller, Vasisht Duddu, Tommi Gr\\\"ondahl, N. Asokan", "title": "Good Artists Copy, Great Artists Steal: Model Extraction Attacks Against\n  Image Translation Generative Adversarial Networks", "comments": "9 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning models are typically made available to potential client\nusers via inference APIs. Model extraction attacks occur when a malicious\nclient uses information gleaned from queries to the inference API of a victim\nmodel $F_V$ to build a surrogate model $F_A$ that has comparable functionality.\nRecent research has shown successful model extraction attacks against image\nclassification, and NLP models. In this paper, we show the first model\nextraction attack against real-world generative adversarial network (GAN) image\ntranslation models. We present a framework for conducting model extraction\nattacks against image translation models, and show that the adversary can\nsuccessfully extract functional surrogate models. The adversary is not required\nto know $F_V$'s architecture or any other information about it beyond its\nintended image translation task, and queries $F_V$'s inference interface using\ndata drawn from the same domain as the training data for $F_V$. We evaluate the\neffectiveness of our attacks using three different instances of two popular\ncategories of image translation: (1) Selfie-to-Anime and (2) Monet-to-Photo\n(image style transfer), and (3) Super-Resolution (super resolution). Using\nstandard performance metrics for GANs, we show that our attacks are effective\nin each of the three cases -- the differences between $F_V$ and $F_A$, compared\nto the target are in the following ranges: Selfie-to-Anime: FID $13.36-68.66$,\nMonet-to-Photo: FID $3.57-4.40$, and Super-Resolution: SSIM: $0.06-0.08$ and\nPSNR: $1.43-4.46$. Furthermore, we conducted a large scale (125 participants)\nuser study on Selfie-to-Anime and Monet-to-Photo to show that human perception\nof the images produced by the victim and surrogate models can be considered\nequivalent, within an equivalence bound of Cohen's $d=0.3$.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 14:50:59 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Szyller", "Sebastian", ""], ["Duddu", "Vasisht", ""], ["Gr\u00f6ndahl", "Tommi", ""], ["Asokan", "N.", ""]]}, {"id": "2104.12627", "submitter": "Jiahao Zhang", "authors": "Anqi Hu, Jiahao Zhang and Hiroyuki Kaga", "title": "Green View Index Analysis and Optimal Green View Index Path Based on\n  Street View and Deep Learning", "comments": "8 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Streetscapes are an important part of the urban landscape, analysing and\nstudying them can increase the understanding of the cities' infrastructure,\nwhich can lead to better planning and design of the urban living environment.\nIn this paper, we used Google API to obtain street view images of Osaka City.\nThe semantic segmentation model PSPNet is used to segment the Osaka City street\nview images and analyse the Green View Index (GVI) data of Osaka area. Based on\nthe GVI data, three methods, namely corridor analysis, geometric network and a\ncombination of them, were then used to calculate the optimal GVI paths in Osaka\nCity. The corridor analysis and geometric network methods allow for a more\ndetailed delineation of the optimal GVI path from general areas to specific\nroutes. Our analysis not only allows for the calculation of specific routes for\nthe optimal GVI paths, but also allows for the visualisation and integration of\nneighbourhood landscape data. By summarising all the data, a more specific and\nobjective analysis of the landscape in the study area can be carried out and\nbased on this, the available natural resources can be maximised for a better\nlife.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 14:53:21 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Hu", "Anqi", ""], ["Zhang", "Jiahao", ""], ["Kaga", "Hiroyuki", ""]]}, {"id": "2104.12642", "submitter": "Manas Sahni", "authors": "Manas Sahni, Shreya Varshini, Alind Khare, Alexey Tumanov", "title": "CompOFA: Compound Once-For-All Networks for Faster Multi-Platform\n  Deployment", "comments": "Published as a conference paper at ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The emergence of CNNs in mainstream deployment has necessitated methods to\ndesign and train efficient architectures tailored to maximize the accuracy\nunder diverse hardware & latency constraints. To scale these resource-intensive\ntasks with an increasing number of deployment targets, Once-For-All (OFA)\nproposed an approach to jointly train several models at once with a constant\ntraining cost. However, this cost remains as high as 40-50 GPU days and also\nsuffers from a combinatorial explosion of sub-optimal model configurations. We\nseek to reduce this search space -- and hence the training budget -- by\nconstraining search to models close to the accuracy-latency Pareto frontier. We\nincorporate insights of compound relationships between model dimensions to\nbuild CompOFA, a design space smaller by several orders of magnitude. Through\nexperiments on ImageNet, we demonstrate that even with simple heuristics we can\nachieve a 2x reduction in training time and 216x speedup in model\nsearch/extraction time compared to the state of the art, without loss of Pareto\noptimality! We also show that this smaller design space is dense enough to\nsupport equally accurate models for a similar diversity of hardware and latency\ntargets, while also reducing the complexity of the training and subsequent\nextraction algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 15:10:48 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Sahni", "Manas", ""], ["Varshini", "Shreya", ""], ["Khare", "Alind", ""], ["Tumanov", "Alexey", ""]]}, {"id": "2104.12663", "submitter": "Henning Schulze", "authors": "Henning Schulze and Dogucan Yaman and Alexander Waibel", "title": "CAGAN: Text-To-Image Generation with Combined Attention GANs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Generating images according to natural language descriptions is a challenging\ntask. Prior research has mainly focused to enhance the quality of generation by\ninvestigating the use of spatial attention and/or textual attention thereby\nneglecting the relationship between channels. In this work, we propose the\nCombined Attention Generative Adversarial Network (CAGAN) to generate\nphoto-realistic images according to textual descriptions. The proposed CAGAN\nutilises two attention models: word attention to draw different sub-regions\nconditioned on related words; and squeeze-and-excitation attention to capture\nnon-linear interaction among channels. With spectral normalisation to stabilise\ntraining, our proposed CAGAN improves the state of the art on the IS and FID on\nthe CUB dataset and the FID on the more challenging COCO dataset. Furthermore,\nwe demonstrate that judging a model by a single evaluation metric can be\nmisleading by developing an additional model adding local self-attention which\nscores a higher IS, outperforming the state of the art on the CUB dataset, but\ngenerates unrealistic images through feature repetition.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 15:46:40 GMT"}, {"version": "v2", "created": "Wed, 23 Jun 2021 18:57:03 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Schulze", "Henning", ""], ["Yaman", "Dogucan", ""], ["Waibel", "Alexander", ""]]}, {"id": "2104.12665", "submitter": "Seungjun Nah", "authors": "Seungjun Nah, Sanghyun Son, Jaerin Lee, Kyoung Mu Lee", "title": "Clean Images are Hard to Reblur: A New Clue for Deblurring", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of dynamic scene deblurring is to remove the motion blur present in\na given image. Most learning-based approaches implement their solutions by\nminimizing the L1 or L2 distance between the output and reference sharp image.\nRecent attempts improve the perceptual quality of the deblurred image by using\nfeatures learned from visual recognition tasks. However, those features are\noriginally designed to capture the high-level contexts rather than the\nlow-level structures of the given image, such as blurriness. We propose a novel\nlow-level perceptual loss to make image sharper. To better focus on image\nblurriness, we train a reblurring module amplifying the unremoved motion blur.\nMotivated that a well-deblurred clean image should contain zero-magnitude\nmotion blur that is hard to be amplified, we design two types of reblurring\nloss functions. The supervised reblurring loss at training stage compares the\namplified blur between the deblurred image and the reference sharp image. The\nself-supervised reblurring loss at inference stage inspects if the deblurred\nimage still contains noticeable blur to be amplified. Our experimental results\ndemonstrate the proposed reblurring losses improve the perceptual quality of\nthe deblurred images in terms of NIQE and LPIPS scores as well as visual\nsharpness.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 15:49:21 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Nah", "Seungjun", ""], ["Son", "Sanghyun", ""], ["Lee", "Jaerin", ""], ["Lee", "Kyoung Mu", ""]]}, {"id": "2104.12668", "submitter": "Yihua Cheng", "authors": "Yihua Cheng, Haofei Wang, Yiwei Bao and Feng Lu", "title": "Appearance-based Gaze Estimation With Deep Learning: A Review and\n  Benchmark", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaze estimation reveals where a person is looking. It is an important clue\nfor understanding human intention. The recent development of deep learning has\nrevolutionized many computer vision tasks, the appearance-based gaze estimation\nis no exception. However, it lacks a guideline for designing deep learning\nalgorithms for gaze estimation tasks. In this paper, we present a comprehensive\nreview of the appearance-based gaze estimation methods with deep learning. We\nsummarize the processing pipeline and discuss these methods from four\nperspectives: deep feature extraction, deep neural network architecture design,\npersonal calibration as well as device and platform. Since the data\npre-processing and post-processing methods are crucial for gaze estimation, we\nalso survey face/eye detection method, data rectification method, 2D/3D gaze\nconversion method, and gaze origin conversion method. To fairly compare the\nperformance of various gaze estimation approaches, we characterize all the\npublicly available gaze estimation datasets and collect the code of typical\ngaze estimation algorithms. We implement these codes and set up a benchmark of\nconverting the results of different methods into the same evaluation metrics.\nThis paper not only serves as a reference to develop deep learning-based gaze\nestimation methods but also a guideline for future gaze estimation research.\nImplemented methods and data processing codes are available at\nhttp://phi-ai.org/GazeHub.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 15:53:03 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Cheng", "Yihua", ""], ["Wang", "Haofei", ""], ["Bao", "Yiwei", ""], ["Lu", "Feng", ""]]}, {"id": "2104.12669", "submitter": "Brian Lim", "authors": "Xuejun Zhao, Wencan Zhang, Xiaokui Xiao, Brian Y. Lim", "title": "Exploiting Explanations for Model Inversion Attacks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The successful deployment of artificial intelligence (AI) in many domains\nfrom healthcare to hiring requires their responsible use, particularly in model\nexplanations and privacy. Explainable artificial intelligence (XAI) provides\nmore information to help users to understand model decisions, yet this\nadditional knowledge exposes additional risks for privacy attacks. Hence,\nproviding explanation harms privacy. We study this risk for image-based model\ninversion attacks and identified several attack architectures with increasing\nperformance to reconstruct private image data from model explanations. We have\ndeveloped several multi-modal transposed CNN architectures that achieve\nsignificantly higher inversion performance than using the target model\nprediction only. These XAI-aware inversion models were designed to exploit the\nspatial knowledge in image explanations. To understand which explanations have\nhigher privacy risk, we analyzed how various explanation types and factors\ninfluence inversion performance. In spite of some models not providing\nexplanations, we further demonstrate increased inversion performance even for\nnon-explainable target models by exploiting explanations of surrogate models\nthrough attention transfer. This method first inverts an explanation from the\ntarget prediction, then reconstructs the target image. These threats highlight\nthe urgent and significant privacy risks of explanations and calls attention\nfor new privacy preservation techniques that balance the dual-requirement for\nAI explainability and privacy.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 15:53:57 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Zhao", "Xuejun", ""], ["Zhang", "Wencan", ""], ["Xiao", "Xiaokui", ""], ["Lim", "Brian Y.", ""]]}, {"id": "2104.12671", "submitter": "Brian Chen", "authors": "Brian Chen, Andrew Rouditchenko, Kevin Duarte, Hilde Kuehne, Samuel\n  Thomas, Angie Boggust, Rameswar Panda, Brian Kingsbury, Rogerio Feris, David\n  Harwath, James Glass, Michael Picheny, Shih-Fu Chang", "title": "Multimodal Clustering Networks for Self-supervised Learning from\n  Unlabeled Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multimodal self-supervised learning is getting more and more attention as it\nallows not only to train large networks without human supervision but also to\nsearch and retrieve data across various modalities. In this context, this paper\nproposes a self-supervised training framework that learns a common multimodal\nembedding space that, in addition to sharing representations across different\nmodalities, enforces a grouping of semantically similar instances. To this end,\nwe extend the concept of instance-level contrastive learning with a multimodal\nclustering step in the training pipeline to capture semantic similarities\nacross modalities. The resulting embedding space enables retrieval of samples\nacross all modalities, even from unseen datasets and different domains. To\nevaluate our approach, we train our model on the HowTo100M dataset and evaluate\nits zero-shot retrieval capabilities in two challenging domains, namely\ntext-to-video retrieval, and temporal action localization, showing\nstate-of-the-art results on four different datasets.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 15:55:01 GMT"}, {"version": "v2", "created": "Wed, 5 May 2021 14:20:16 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Chen", "Brian", ""], ["Rouditchenko", "Andrew", ""], ["Duarte", "Kevin", ""], ["Kuehne", "Hilde", ""], ["Thomas", "Samuel", ""], ["Boggust", "Angie", ""], ["Panda", "Rameswar", ""], ["Kingsbury", "Brian", ""], ["Feris", "Rogerio", ""], ["Harwath", "David", ""], ["Glass", "James", ""], ["Picheny", "Michael", ""], ["Chang", "Shih-Fu", ""]]}, {"id": "2104.12672", "submitter": "Yiqiao Yin", "authors": "Shaw-Hwa Lo, Yiqiao Yin", "title": "A Novel Interaction-based Methodology Towards Explainable AI with Better\n  Understanding of Pneumonia Chest X-ray Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In the field of eXplainable AI (XAI), robust \"blackbox\" algorithms such as\nConvolutional Neural Networks (CNNs) are known for making high prediction\nperformance. However, the ability to explain and interpret these algorithms\nstill require innovation in the understanding of influential and, more\nimportantly, explainable features that directly or indirectly impact the\nperformance of predictivity. A number of methods existing in literature focus\non visualization techniques but the concepts of explainability and\ninterpretability still require rigorous definition. In view of the above needs,\nthis paper proposes an interaction-based methodology -- Influence Score\n(I-score) -- to screen out the noisy and non-informative variables in the\nimages hence it nourishes an environment with explainable and interpretable\nfeatures that are directly associated to feature predictivity. We apply the\nproposed method on a real world application in Pneumonia Chest X-ray Image data\nset and produced state-of-the-art results. We demonstrate how to apply the\nproposed approach for more general big data problems by improving the\nexplainability and interpretability without sacrificing the prediction\nperformance. The contribution of this paper opens a novel angle that moves the\ncommunity closer to the future pipelines of XAI problems.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 23:02:43 GMT"}, {"version": "v2", "created": "Sun, 13 Jun 2021 03:57:08 GMT"}, {"version": "v3", "created": "Tue, 15 Jun 2021 05:29:26 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Lo", "Shaw-Hwa", ""], ["Yin", "Yiqiao", ""]]}, {"id": "2104.12673", "submitter": "Kai Han", "authors": "Xuhui Jia and Kai Han and Yukun Zhu and Bradley Green", "title": "Joint Representation Learning and Novel Category Discovery on Single-\n  and Multi-modal Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the problem of novel category discovery on single- and\nmulti-modal data with labels from different but relevant categories. We present\na generic, end-to-end framework to jointly learn a reliable representation and\nassign clusters to unlabelled data. To avoid over-fitting the learnt embedding\nto labelled data, we take inspiration from self-supervised representation\nlearning by noise-contrastive estimation and extend it to jointly handle\nlabelled and unlabelled data. In particular, we propose using category\ndiscrimination on labelled data and cross-modal discrimination on multi-modal\ndata to augment instance discrimination used in conventional contrastive\nlearning approaches. We further employ Winner-Take-All (WTA) hashing algorithm\non the shared representation space to generate pairwise pseudo labels for\nunlabelled data to better predict cluster assignments. We thoroughly evaluate\nour framework on large-scale multi-modal video benchmarks Kinetics-400 and\nVGG-Sound, and image benchmarks CIFAR10, CIFAR100 and ImageNet, obtaining\nstate-of-the-art results.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 15:56:16 GMT"}, {"version": "v2", "created": "Tue, 27 Apr 2021 09:00:44 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Jia", "Xuhui", ""], ["Han", "Kai", ""], ["Zhu", "Yukun", ""], ["Green", "Bradley", ""]]}, {"id": "2104.12686", "submitter": "Benedikt Pf\\\"ulb", "authors": "Alexander Gepperth, Benedikt Pf\\\"ulb", "title": "Image Modeling with Deep Convolutional Gaussian Mixture Models", "comments": "accepted at IJCNN2021, 9 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this conceptual work, we present Deep Convolutional Gaussian Mixture\nModels (DCGMMs): a new formulation of deep hierarchical Gaussian Mixture Models\n(GMMs) that is particularly suitable for describing and generating images.\nVanilla (i.e., flat) GMMs require a very large number of components to describe\nimages well, leading to long training times and memory issues. DCGMMs avoid\nthis by a stacked architecture of multiple GMM layers, linked by convolution\nand pooling operations. This allows to exploit the compositionality of images\nin a similar way as deep CNNs do. DCGMMs can be trained end-to-end by\nStochastic Gradient Descent. This sets them apart from vanilla GMMs which are\ntrained by Expectation-Maximization, requiring a prior k-means initialization\nwhich is infeasible in a layered structure. For generating sharp images with\nDCGMMs, we introduce a new gradient-based technique for sampling through\nnon-invertible operations like convolution and pooling. Based on the MNIST and\nFashionMNIST datasets, we validate the DCGMMs model by demonstrating its\nsuperiority over flat GMMs for clustering, sampling and outlier detection.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 12:08:53 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Gepperth", "Alexander", ""], ["Pf\u00fclb", "Benedikt", ""]]}, {"id": "2104.12690", "submitter": "Yuan-Hong Liao", "authors": "Yuan-Hong Liao, Amlan Kar, Sanja Fidler", "title": "Towards Good Practices for Efficiently Annotating Large-Scale Image\n  Classification Datasets", "comments": "CVPR 2021 Oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Data is the engine of modern computer vision, which necessitates collecting\nlarge-scale datasets. This is expensive, and guaranteeing the quality of the\nlabels is a major challenge. In this paper, we investigate efficient annotation\nstrategies for collecting multi-class classification labels for a large\ncollection of images. While methods that exploit learnt models for labeling\nexist, a surprisingly prevalent approach is to query humans for a fixed number\nof labels per datum and aggregate them, which is expensive. Building on prior\nwork on online joint probabilistic modeling of human annotations and\nmachine-generated beliefs, we propose modifications and best practices aimed at\nminimizing human labeling effort. Specifically, we make use of advances in\nself-supervised learning, view annotation as a semi-supervised learning\nproblem, identify and mitigate pitfalls and ablate several key design choices\nto propose effective guidelines for labeling. Our analysis is done in a more\nrealistic simulation that involves querying human labelers, which uncovers\nissues with evaluation using existing worker simulation methods. Simulated\nexperiments on a 125k image subset of the ImageNet100 show that it can be\nannotated to 80% top-1 accuracy with 0.35 annotations per image on average, a\n2.7x and 6.7x improvement over prior work and manual annotation, respectively.\nProject page: https://fidler-lab.github.io/efficient-annotation-cookbook\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 16:29:32 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Liao", "Yuan-Hong", ""], ["Kar", "Amlan", ""], ["Fidler", "Sanja", ""]]}, {"id": "2104.12709", "submitter": "Mohamed Afham", "authors": "Mohamed Afham, Salman Khan, Muhammad Haris Khan, Muzammal Naseer,\n  Fahad Shahbaz Khan", "title": "Rich Semantics Improve Few-shot Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human learning benefits from multi-modal inputs that often appear as rich\nsemantics (e.g., description of an object's attributes while learning about\nit). This enables us to learn generalizable concepts from very limited visual\nexamples. However, current few-shot learning (FSL) methods use numerical class\nlabels to denote object classes which do not provide rich semantic meanings\nabout the learned concepts. In this work, we show that by using 'class-level'\nlanguage descriptions, that can be acquired with minimal annotation cost, we\ncan improve the FSL performance. Given a support set and queries, our main idea\nis to create a bottleneck visual feature (hybrid prototype) which is then used\nto generate language descriptions of the classes as an auxiliary task during\ntraining. We develop a Transformer based forward and backward encoding\nmechanism to relate visual and semantic tokens that can encode intricate\nrelationships between the two modalities. Forcing the prototypes to retain\nsemantic information about class description acts as a regularizer on the\nvisual features, improving their generalization to novel classes at inference.\nFurthermore, this strategy imposes a human prior on the learned\nrepresentations, ensuring that the model is faithfully relating visual and\nsemantic concepts, thereby improving model interpretability. Our experiments on\nfour datasets and ablation studies show the benefit of effectively modeling\nrich semantics for FSL.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 16:48:27 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Afham", "Mohamed", ""], ["Khan", "Salman", ""], ["Khan", "Muhammad Haris", ""], ["Naseer", "Muzammal", ""], ["Khan", "Fahad Shahbaz", ""]]}, {"id": "2104.12727", "submitter": "Yu-Chuan Su", "authors": "Yu-Chuan Su, Soravit Changpinyo, Xiangning Chen, Sathish Thoppay,\n  Cho-Jui Hsieh, Lior Shapira, Radu Soricut, Hartwig Adam, Matthew Brown,\n  Ming-Hsuan Yang, Boqing Gong", "title": "2.5D Visual Relationship Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Visual 2.5D perception involves understanding the semantics and geometry of a\nscene through reasoning about object relationships with respect to the viewer\nin an environment. However, existing works in visual recognition primarily\nfocus on the semantics. To bridge this gap, we study 2.5D visual relationship\ndetection (2.5VRD), in which the goal is to jointly detect objects and predict\ntheir relative depth and occlusion relationships. Unlike general VRD, 2.5VRD is\negocentric, using the camera's viewpoint as a common reference for all 2.5D\nrelationships. Unlike depth estimation, 2.5VRD is object-centric and not only\nfocuses on depth. To enable progress on this task, we create a new dataset\nconsisting of 220k human-annotated 2.5D relationships among 512K objects from\n11K images. We analyze this dataset and conduct extensive experiments including\nbenchmarking multiple state-of-the-art VRD models on this task. Our results\nshow that existing models largely rely on semantic cues and simple heuristics\nto solve 2.5VRD, motivating further research on models for 2.5D perception. The\nnew dataset is available at https://github.com/google-research-datasets/2.5vrd.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 17:19:10 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Su", "Yu-Chuan", ""], ["Changpinyo", "Soravit", ""], ["Chen", "Xiangning", ""], ["Thoppay", "Sathish", ""], ["Hsieh", "Cho-Jui", ""], ["Shapira", "Lior", ""], ["Soricut", "Radu", ""], ["Adam", "Hartwig", ""], ["Brown", "Matthew", ""], ["Yang", "Ming-Hsuan", ""], ["Gong", "Boqing", ""]]}, {"id": "2104.12753", "submitter": "Chengyue Gong", "authors": "Chengyue Gong, Dilin Wang, Meng Li, Vikas Chandra, Qiang Liu", "title": "Vision Transformers with Patch Diversification", "comments": "preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vision transformer has demonstrated promising performance on challenging\ncomputer vision tasks. However, directly training the vision transformers may\nyield unstable and sub-optimal results. Recent works propose to improve the\nperformance of the vision transformers by modifying the transformer structures,\ne.g., incorporating convolution layers. In contrast, we investigate an\northogonal approach to stabilize the vision transformer training without\nmodifying the networks. We observe the instability of the training can be\nattributed to the significant similarity across the extracted patch\nrepresentations. More specifically, for deep vision transformers, the\nself-attention blocks tend to map different patches into similar latent\nrepresentations, yielding information loss and performance degradation. To\nalleviate this problem, in this work, we introduce novel loss functions in\nvision transformer training to explicitly encourage diversity across patch\nrepresentations for more discriminative feature extraction. We empirically show\nthat our proposed techniques stabilize the training and allow us to train wider\nand deeper vision transformers. We further show the diversified features\nsignificantly benefit the downstream tasks in transfer learning. For semantic\nsegmentation, we enhance the state-of-the-art (SOTA) results on Cityscapes and\nADE20k. Our code is available at\nhttps://github.com/ChengyueGongR/PatchVisionTransformer.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 17:43:04 GMT"}, {"version": "v2", "created": "Thu, 10 Jun 2021 05:55:42 GMT"}, {"version": "v3", "created": "Fri, 11 Jun 2021 01:35:08 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Gong", "Chengyue", ""], ["Wang", "Dilin", ""], ["Li", "Meng", ""], ["Chandra", "Vikas", ""], ["Liu", "Qiang", ""]]}, {"id": "2104.12756", "submitter": "Minesh Mathew", "authors": "Minesh Mathew, Viraj Bagal, Rub\\`en P\\'erez Tito, Dimosthenis\n  Karatzas, Ernest Valveny, C.V Jawahar", "title": "InfographicVQA", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Infographics are documents designed to effectively communicate information\nusing a combination of textual, graphical and visual elements. In this work, we\nexplore the automatic understanding of infographic images by using Visual\nQuestion Answering technique.To this end, we present InfographicVQA, a new\ndataset that comprises a diverse collection of infographics along with natural\nlanguage questions and answers annotations. The collected questions require\nmethods to jointly reason over the document layout, textual content, graphical\nelements, and data visualizations. We curate the dataset with emphasis on\nquestions that require elementary reasoning and basic arithmetic skills.\nFinally, we evaluate two strong baselines based on state of the art multi-modal\nVQA models, and establish baseline performance for the new task. The dataset,\ncode and leaderboard will be made available at http://docvqa.org\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 17:45:54 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Mathew", "Minesh", ""], ["Bagal", "Viraj", ""], ["Tito", "Rub\u00e8n P\u00e9rez", ""], ["Karatzas", "Dimosthenis", ""], ["Valveny", "Ernest", ""], ["Jawahar", "C. V", ""]]}, {"id": "2104.12763", "submitter": "Aishwarya Kamath", "authors": "Aishwarya Kamath, Mannat Singh, Yann LeCun, Ishan Misra, Gabriel\n  Synnaeve, Nicolas Carion", "title": "MDETR -- Modulated Detection for End-to-End Multi-Modal Understanding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multi-modal reasoning systems rely on a pre-trained object detector to\nextract regions of interest from the image. However, this crucial module is\ntypically used as a black box, trained independently of the downstream task and\non a fixed vocabulary of objects and attributes. This makes it challenging for\nsuch systems to capture the long tail of visual concepts expressed in free form\ntext. In this paper we propose MDETR, an end-to-end modulated detector that\ndetects objects in an image conditioned on a raw text query, like a caption or\na question. We use a transformer-based architecture to reason jointly over text\nand image by fusing the two modalities at an early stage of the model. We\npre-train the network on 1.3M text-image pairs, mined from pre-existing\nmulti-modal datasets having explicit alignment between phrases in text and\nobjects in the image. We then fine-tune on several downstream tasks such as\nphrase grounding, referring expression comprehension and segmentation,\nachieving state-of-the-art results on popular benchmarks. We also investigate\nthe utility of our model as an object detector on a given label set when\nfine-tuned in a few-shot setting. We show that our pre-training approach\nprovides a way to handle the long tail of object categories which have very few\nlabelled instances. Our approach can be easily extended for visual question\nanswering, achieving competitive performance on GQA and CLEVR. The code and\nmodels are available at https://github.com/ashkamath/mdetr.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 17:55:33 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Kamath", "Aishwarya", ""], ["Singh", "Mannat", ""], ["LeCun", "Yann", ""], ["Misra", "Ishan", ""], ["Synnaeve", "Gabriel", ""], ["Carion", "Nicolas", ""]]}, {"id": "2104.12766", "submitter": "Zhen Dong", "authors": "Zhen Dong, Yizhao Gao, Qijing Huang, John Wawrzynek, Hayden K.H. So,\n  Kurt Keutzer", "title": "HAO: Hardware-aware neural Architecture Optimization for Efficient\n  Inference", "comments": null, "journal-ref": "FCCM 2021", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Automatic algorithm-hardware co-design for DNN has shown great success in\nimproving the performance of DNNs on FPGAs. However, this process remains\nchallenging due to the intractable search space of neural network architectures\nand hardware accelerator implementation. Differing from existing hardware-aware\nneural architecture search (NAS) algorithms that rely solely on the expensive\nlearning-based approaches, our work incorporates integer programming into the\nsearch algorithm to prune the design space. Given a set of hardware resource\nconstraints, our integer programming formulation directly outputs the optimal\naccelerator configuration for mapping a DNN subgraph that minimizes latency. We\nuse an accuracy predictor for different DNN subgraphs with different\nquantization schemes and generate accuracy-latency pareto frontiers. With low\ncomputational cost, our algorithm can generate quantized networks that achieve\nstate-of-the-art accuracy and hardware performance on Xilinx Zynq (ZU3EG) FPGA\nfor image classification on ImageNet dataset. The solution searched by our\nalgorithm achieves 72.5% top-1 accuracy on ImageNet at framerate 50, which is\n60% faster than MnasNet and 135% faster than FBNet with comparable accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 17:59:29 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Dong", "Zhen", ""], ["Gao", "Yizhao", ""], ["Huang", "Qijing", ""], ["Wawrzynek", "John", ""], ["So", "Hayden K. H.", ""], ["Keutzer", "Kurt", ""]]}, {"id": "2104.12835", "submitter": "Daniel Glasner", "authors": "Srikumar Ramalingam, Daniel Glasner, Kaushal Patel, Raviteja\n  Vemulapalli, Sadeep Jayasumana, Sanjiv Kumar", "title": "Balancing Constraints and Submodularity in Data Subset Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep learning has yielded extraordinary results in vision and natural\nlanguage processing, but this achievement comes at a cost. Most deep learning\nmodels require enormous resources during training, both in terms of computation\nand in human labeling effort. In this paper, we show that one can achieve\nsimilar accuracy to traditional deep-learning models, while using less training\ndata. Much of the previous work in this area relies on using uncertainty or\nsome form of diversity to select subsets of a larger training set.\nSubmodularity, a discrete analogue of convexity, has been exploited to model\ndiversity in various settings including data subset selection. In contrast to\nprior methods, we propose a novel diversity driven objective function, and\nbalancing constraints on class labels and decision boundaries using matroids.\nThis allows us to use efficient greedy algorithms with approximation guarantees\nfor subset selection. We outperform baselines on standard image classification\ndatasets such as CIFAR-10, CIFAR-100, and ImageNet. In addition, we also show\nthat the proposed balancing constraints can play a key role in boosting the\nperformance in long-tailed datasets such as CIFAR-100-LT.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 19:22:27 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Ramalingam", "Srikumar", ""], ["Glasner", "Daniel", ""], ["Patel", "Kaushal", ""], ["Vemulapalli", "Raviteja", ""], ["Jayasumana", "Sadeep", ""], ["Kumar", "Sanjiv", ""]]}, {"id": "2104.12836", "submitter": "Xin Yuan", "authors": "Xin Yuan, Zhe Lin, Jason Kuen, Jianming Zhang, Yilin Wang, Michael\n  Maire, Ajinkya Kale, and Baldo Faieta", "title": "Multimodal Contrastive Training for Visual Representation Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop an approach to learning visual representations that embraces\nmultimodal data, driven by a combination of intra- and inter-modal similarity\npreservation objectives. Unlike existing visual pre-training methods, which\nsolve a proxy prediction task in a single domain, our method exploits intrinsic\ndata properties within each modality and semantic information from cross-modal\ncorrelation simultaneously, hence improving the quality of learned visual\nrepresentations. By including multimodal training in a unified framework with\ndifferent types of contrastive losses, our method can learn more powerful and\ngeneric visual features. We first train our model on COCO and evaluate the\nlearned visual representations on various downstream tasks including image\nclassification, object detection, and instance segmentation. For example, the\nvisual representations pre-trained on COCO by our method achieve\nstate-of-the-art top-1 validation accuracy of $55.3\\%$ on ImageNet\nclassification, under the common transfer protocol. We also evaluate our method\non the large-scale Stock images dataset and show its effectiveness on\nmulti-label image tagging, and cross-modal retrieval tasks.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 19:23:36 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Yuan", "Xin", ""], ["Lin", "Zhe", ""], ["Kuen", "Jason", ""], ["Zhang", "Jianming", ""], ["Wang", "Yilin", ""], ["Maire", "Michael", ""], ["Kale", "Ajinkya", ""], ["Faieta", "Baldo", ""]]}, {"id": "2104.12839", "submitter": "M. Hamed Mozaffari", "authors": "M. Hamed Mozaffari and Li-Lin Tay", "title": "One-dimensional Active Contour Models for Raman Spectrum Baseline\n  Correction", "comments": "4 figures, and 9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.CV cs.LG cs.NE eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Raman spectroscopy is a powerful and non-invasive method for analysis of\nchemicals and detection of unknown substances. However, Raman signal is so weak\nthat background noise can distort the actual Raman signal. These baseline\nshifts that exist in the Raman spectrum might deteriorate analytical results.\nIn this paper, a modified version of active contour models in one-dimensional\nspace has been proposed for the baseline correction of Raman spectra. Our\ntechnique, inspired by principles of physics and heuristic optimization\nmethods, iteratively deforms an initialized curve toward the desired baseline.\nThe performance of the proposed algorithm was evaluated and compared with\nsimilar techniques using simulated Raman spectra. The results showed that the\n1D active contour model outperforms many iterative baseline correction methods.\nThe proposed algorithm was successfully applied to experimental Raman spectral\ndata, and the results indicate that the baseline of Raman spectra can be\nautomatically subtracted.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 19:30:34 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Mozaffari", "M. Hamed", ""], ["Tay", "Li-Lin", ""]]}, {"id": "2104.12861", "submitter": "Hengli Wang", "authors": "Hengli Wang, Peide Cai, Yuxiang Sun, Lujia Wang, Ming Liu", "title": "Learning Interpretable End-to-End Vision-Based Motion Planning for\n  Autonomous Driving with Optical Flow Distillation", "comments": "7 pages, 5 figures and 1 table. This paper is accepted by ICRA 2021.\n  arXiv admin note: text overlap with arXiv:2104.08862", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, deep-learning based approaches have achieved impressive performance\nfor autonomous driving. However, end-to-end vision-based methods typically have\nlimited interpretability, making the behaviors of the deep networks difficult\nto explain. Hence, their potential applications could be limited in practice.\nTo address this problem, we propose an interpretable end-to-end vision-based\nmotion planning approach for autonomous driving, referred to as IVMP. Given a\nset of past surrounding-view images, our IVMP first predicts future egocentric\nsemantic maps in bird's-eye-view space, which are then employed to plan\ntrajectories for self-driving vehicles. The predicted future semantic maps not\nonly provide useful interpretable information, but also allow our motion\nplanning module to handle objects with low probability, thus improving the\nsafety of autonomous driving. Moreover, we also develop an optical flow\ndistillation paradigm, which can effectively enhance the network while still\nmaintaining its real-time performance. Extensive experiments on the nuScenes\ndataset and closed-loop simulation show that our IVMP significantly outperforms\nthe state-of-the-art approaches in imitating human drivers with a much higher\nsuccess rate. Our project page is available at\nhttps://sites.google.com/view/ivmp.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 13:51:25 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Wang", "Hengli", ""], ["Cai", "Peide", ""], ["Sun", "Yuxiang", ""], ["Wang", "Lujia", ""], ["Liu", "Ming", ""]]}, {"id": "2104.12862", "submitter": "Muhammad Shaban", "authors": "Muhammad Shaban, Shan E Ahmed Raza, Mariam Hassan, Arif Jamshed, Sajid\n  Mushtaq, Asif Loya, Nikolaos Batis, Jill Brooks, Paul Nankivell, Neil Sharma,\n  Max Robinson, Hisham Mehanna, Syed Ali Khurram, Nasir Rajpoot", "title": "A digital score of tumour-associated stroma infiltrating lymphocytes\n  predicts survival in head and neck squamous cell carcinoma", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The infiltration of T-lymphocytes in the stroma and tumour is an indication\nof an effective immune response against the tumour, resulting in better\nsurvival. In this study, our aim is to explore the prognostic significance of\ntumour-associated stroma infiltrating lymphocytes (TASILs) in head and neck\nsquamous cell carcinoma (HNSCC) through an AI based automated method. A deep\nlearning based automated method was employed to segment tumour, stroma and\nlymphocytes in digitally scanned whole slide images of HNSCC tissue slides. The\nspatial patterns of lymphocytes and tumour-associated stroma were digitally\nquantified to compute the TASIL-score. Finally, prognostic significance of the\nTASIL-score for disease-specific and disease-free survival was investigated\nwith the Cox proportional hazard analysis. Three different cohorts of\nHaematoxylin & Eosin (H&E) stained tissue slides of HNSCC cases (n=537 in\ntotal) were studied, including publicly available TCGA head and neck cancer\ncases. The TASIL-score carries prognostic significance (p=0.002) for\ndisease-specific survival of HNSCC patients. The TASIL-score also shows a\nbetter separation between low- and high-risk patients as compared to the manual\nTIL scoring by pathologists for both disease-specific and disease-free\nsurvival. A positive correlation of TASIL-score with molecular estimates of\nCD8+ T cells was also found, which is in line with existing findings. To the\nbest of our knowledge, this is the first study to automate the quantification\nof TASIL from routine H&E slides of head and neck cancer. Our TASIL-score based\nfindings are aligned with the clinical knowledge with the added advantages of\nobjectivity, reproducibility and strong prognostic value. A comprehensive\nevaluation on large multicentric cohorts is required before the proposed\ndigital score can be adopted in clinical practice.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 19:45:00 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Shaban", "Muhammad", ""], ["Raza", "Shan E Ahmed", ""], ["Hassan", "Mariam", ""], ["Jamshed", "Arif", ""], ["Mushtaq", "Sajid", ""], ["Loya", "Asif", ""], ["Batis", "Nikolaos", ""], ["Brooks", "Jill", ""], ["Nankivell", "Paul", ""], ["Sharma", "Neil", ""], ["Robinson", "Max", ""], ["Mehanna", "Hisham", ""], ["Khurram", "Syed Ali", ""], ["Rajpoot", "Nasir", ""]]}, {"id": "2104.12863", "submitter": "Olivier Rukundo", "authors": "Olivier Rukundo, Hanqiang Cao", "title": "Advances on image interpolation based on ant colony algorithm", "comments": "17 pages, 14 figures, 3 tables", "journal-ref": "SpringerPlus, 5(1), 403, 2016", "doi": "10.1186/s40064-016-2040-9", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents an advance on image interpolation based on ant colony\nalgorithm (AACA) for high-resolution image scaling. The difference between the\nproposed algorithm and the previously proposed optimization of bilinear\ninterpolation based on ant colony algorithm (OBACA) is that AACA uses global\nweighting, whereas OBACA uses a local weighting scheme. The strength of the\nproposed global weighting of the AACA algorithm depends on employing solely the\npheromone matrix information present on any group of four adjacent pixels to\ndecide which case deserves a maximum global weight value or not. Experimental\nresults are further provided to show the higher performance of the proposed\nAACA algorithm with reference to the algorithms mentioned in this paper.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 04:27:59 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Rukundo", "Olivier", ""], ["Cao", "Hanqiang", ""]]}, {"id": "2104.12865", "submitter": "Zhao Wang", "authors": "Zhao Wang, Changyue Ma, Yan Ye", "title": "Multi-Density Attention Network for Loop Filtering in Video Compression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video compression is a basic requirement for consumer and professional video\napplications alike. Video coding standards such as H.264/AVC and H.265/HEVC are\nwidely deployed in the market to enable efficient use of bandwidth and storage\nfor many video applications. To reduce the coding artifacts and improve the\ncompression efficiency, neural network based loop filtering of the\nreconstructed video has been developed in the literature. However, loop\nfiltering is a challenging task due to the variation in video content and\nsampling densities. In this paper, we propose a on-line scaling based\nmulti-density attention network for loop filtering in video compression. The\ncore of our approach lies in several aspects: (a) parallel multi-resolution\nconvolution streams for extracting multi-density features, (b) single attention\nbranch to learn the sample correlations and generate mask maps, (c) a\nchannel-mutual attention procedure to fuse the data from multiple branches, (d)\non-line scaling technique to further optimize the output results of network\naccording to the actual signal. The proposed multi-density attention network\nlearns rich features from multiple sampling densities and performs robustly on\nvideo content of different resolutions. Moreover, the online scaling process\nenhances the signal adaptability of the off-line pre-trained model.\nExperimental results show that 10.18% bit-rate reduction at the same video\nquality can be achieved over the latest Versatile Video Coding (VVC) standard.\nThe objective performance of the proposed algorithm outperforms the\nstate-of-the-art methods and the subjective quality improvement is obvious in\nterms of detail preservation and artifact alleviation.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 05:46:38 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Wang", "Zhao", ""], ["Ma", "Changyue", ""], ["Ye", "Yan", ""]]}, {"id": "2104.12898", "submitter": "Kaidong Li", "authors": "Kaidong Li, Nina Y. Wang, Yiju Yang and Guanghui Wang", "title": "SGNet: A Super-class Guided Network for Image Classification and Object\n  Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most classification models treat different object classes in parallel and the\nmisclassifications between any two classes are treated equally. In contrast,\nhuman beings can exploit high-level information in making a prediction of an\nunknown object. Inspired by this observation, the paper proposes a super-class\nguided network (SGNet) to integrate the high-level semantic information into\nthe network so as to increase its performance in inference. SGNet takes\ntwo-level class annotations that contain both super-class and finer class\nlabels. The super-classes are higher-level semantic categories that consist of\na certain amount of finer classes. A super-class branch (SCB), trained on\nsuper-class labels, is introduced to guide finer class prediction. At the\ninference time, we adopt two different strategies: Two-step inference (TSI) and\ndirect inference (DI). TSI first predicts the super-class and then makes\npredictions of the corresponding finer class. On the other hand, DI directly\ngenerates predictions from the finer class branch (FCB). Extensive experiments\nhave been performed on CIFAR-100 and MS COCO datasets. The experimental results\nvalidate the proposed approach and demonstrate its superior performance on\nimage classification and object detection.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 22:26:12 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Li", "Kaidong", ""], ["Wang", "Nina Y.", ""], ["Yang", "Yiju", ""], ["Wang", "Guanghui", ""]]}, {"id": "2104.12927", "submitter": "Rodolfo Migon Favaretto", "authors": "Rodolfo Migon Favaretto, Paulo Knob, Soraia Raupp Musse, Felipe\n  Vilanova, \\^Angelo Brandelli Costa", "title": "Detecting Personality and Emotion Traits in Crowds from Video Sequences", "comments": null, "journal-ref": null, "doi": "10.1007/s00138-018-0979-y", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper presents a methodology to detect personality and basic emotion\ncharacteristics of crowds in video sequences. Firstly, individuals are detected\nand tracked, then groups are recognized and characterized. Such information is\nthen mapped to OCEAN dimensions, used to find out personality and emotion in\nvideos, based on OCC emotion models. Although it is a clear challenge to\nvalidate our results with real life experiments, we evaluate our method with\nthe available literature information regarding OCEAN values of different\nCountries and also emergent Personal distance among people. Hence, such\nanalysis refer to cultural differences of each country too. Our results\nindicate that this model generates coherent information when compared to data\nprovided in available literature, as shown in qualitative and quantitative\nresults.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 01:00:16 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Favaretto", "Rodolfo Migon", ""], ["Knob", "Paulo", ""], ["Musse", "Soraia Raupp", ""], ["Vilanova", "Felipe", ""], ["Costa", "\u00c2ngelo Brandelli", ""]]}, {"id": "2104.12928", "submitter": "Steffen Schneider", "authors": "Evgenia Rusak, Steffen Schneider, Peter Gehler, Oliver Bringmann,\n  Wieland Brendel and Matthias Bethge", "title": "Adapting ImageNet-scale models to complex distribution shifts with\n  self-learning", "comments": "Web: https://domainadaptation.org/selflearning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While self-learning methods are an important component in many recent domain\nadaptation techniques, they are not yet comprehensively evaluated on\nImageNet-scale datasets common in robustness research. In extensive experiments\non ResNet and EfficientNet models, we find that three components are crucial\nfor increasing performance with self-learning: (i) using short update times\nbetween the teacher and the student network, (ii) fine-tuning only few affine\nparameters distributed across the network, and (iii) leveraging methods from\nrobust classification to counteract the effect of label noise. We use these\ninsights to obtain drastically improved state-of-the-art results on ImageNet-C\n(22.0% mCE), ImageNet-R (17.4% error) and ImageNet-A (14.8% error). Our\ntechniques yield further improvements in combination with previously proposed\nrobustification methods. Self-learning is able to reduce the top-1 error to a\npoint where no substantial further progress can be expected. We therefore\nre-purpose the dataset from the Visual Domain Adaptation Challenge 2019 and use\na subset of it as a new robustness benchmark (ImageNet-D) which proves to be a\nmore challenging dataset for all current state-of-the-art models (58.2% error)\nto guide future research efforts at the intersection of robustness and domain\nadaptation on ImageNet scale.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 01:02:15 GMT"}, {"version": "v2", "created": "Wed, 28 Apr 2021 01:12:40 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Rusak", "Evgenia", ""], ["Schneider", "Steffen", ""], ["Gehler", "Peter", ""], ["Bringmann", "Oliver", ""], ["Brendel", "Wieland", ""], ["Bethge", "Matthias", ""]]}, {"id": "2104.12939", "submitter": "Qingchao Zhang", "authors": "Qingchao Zhang, Mehrdad Alvandipour, Wenjun Xia, Yi Zhang, Xiaojing Ye\n  and Yunmei Chen", "title": "Provably Convergent Learned Inexact Descent Algorithm for Low-Dose CT\n  Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a provably convergent method, called Efficient Learned Descent\nAlgorithm (ELDA), for low-dose CT (LDCT) reconstruction. ELDA is a highly\ninterpretable neural network architecture with learned parameters and meanwhile\nretains convergence guarantee as classical optimization algorithms. To improve\nreconstruction quality, the proposed ELDA also employs a new non-local feature\nmapping and an associated regularizer. We compare ELDA with several\nstate-of-the-art deep image methods, such as RED-CNN and Learned Primal-Dual,\non a set of LDCT reconstruction problems. Numerical experiments demonstrate\nimprovement of reconstruction quality using ELDA with merely 19 layers,\nsuggesting the promising performance of ELDA in solution accuracy and parameter\nefficiency.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 02:02:41 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Zhang", "Qingchao", ""], ["Alvandipour", "Mehrdad", ""], ["Xia", "Wenjun", ""], ["Zhang", "Yi", ""], ["Ye", "Xiaojing", ""], ["Chen", "Yunmei", ""]]}, {"id": "2104.12961", "submitter": "Zechen Bai", "authors": "Zechen Bai, Zhigang Wang, Jian Wang, Di Hu, Errui Ding", "title": "Unsupervised Multi-Source Domain Adaptation for Person Re-Identification", "comments": "CVPR 2021 Oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Unsupervised domain adaptation (UDA) methods for person re-identification\n(re-ID) aim at transferring re-ID knowledge from labeled source data to\nunlabeled target data. Although achieving great success, most of them only use\nlimited data from a single-source domain for model pre-training, making the\nrich labeled data insufficiently exploited. To make full use of the valuable\nlabeled data, we introduce the multi-source concept into UDA person re-ID\nfield, where multiple source datasets are used during training. However,\nbecause of domain gaps, simply combining different datasets only brings limited\nimprovement. In this paper, we try to address this problem from two\nperspectives, \\ie{} domain-specific view and domain-fusion view. Two\nconstructive modules are proposed, and they are compatible with each other.\nFirst, a rectification domain-specific batch normalization (RDSBN) module is\nexplored to simultaneously reduce domain-specific characteristics and increase\nthe distinctiveness of person features. Second, a graph convolutional network\n(GCN) based multi-domain information fusion (MDIF) module is developed, which\nminimizes domain distances by fusing features of different domains. The\nproposed method outperforms state-of-the-art UDA person re-ID methods by a\nlarge margin, and even achieves comparable performance to the supervised\napproaches without any post-processing techniques.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 03:33:35 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Bai", "Zechen", ""], ["Wang", "Zhigang", ""], ["Wang", "Jian", ""], ["Hu", "Di", ""], ["Ding", "Errui", ""]]}, {"id": "2104.13000", "submitter": "Jiyuan Liu", "authors": "Siqi Wang, Jiyuan Liu, Guang Yu, Xinwang Liu, Sihang Zhou, En Zhu,\n  Yuexiang Yang, Jianping Yin", "title": "Multi-view Deep One-class Classification: A Systematic Exploration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One-class classification (OCC), which models one single positive class and\ndistinguishes it from the negative class, has been a long-standing topic with\npivotal application to realms like anomaly detection. As modern society often\ndeals with massive high-dimensional complex data spawned by multiple sources,\nit is natural to consider OCC from the perspective of multi-view deep learning.\nHowever, it has not been discussed by the literature and remains an unexplored\ntopic. Motivated by this blank, this paper makes four-fold contributions:\nFirst, to our best knowledge, this is the first work that formally identifies\nand formulates the multi-view deep OCC problem. Second, we take recent advances\nin relevant areas into account and systematically devise eleven different\nbaseline solutions for multi-view deep OCC, which lays the foundation for\nresearch on multi-view deep OCC. Third, to remedy the problem that limited\nbenchmark datasets are available for multi-view deep OCC, we extensively\ncollect existing public data and process them into more than 30 new multi-view\nbenchmark datasets via multiple means, so as to provide a publicly available\nevaluation platform for multi-view deep OCC. Finally, by comprehensively\nevaluating the devised solutions on benchmark datasets, we conduct a thorough\nanalysis on the effectiveness of the designed baselines, and hopefully provide\nother researchers with beneficial guidance and insight to multi-view deep OCC.\nOur data and codes are opened at https://github.com/liujiyuan13/MvDOCC-datasets\nand https://github.com/liujiyuan13/MvDOCC-code respectively to facilitate\nfuture research.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 06:44:07 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Wang", "Siqi", ""], ["Liu", "Jiyuan", ""], ["Yu", "Guang", ""], ["Liu", "Xinwang", ""], ["Zhou", "Sihang", ""], ["Zhu", "En", ""], ["Yang", "Yuexiang", ""], ["Yin", "Jianping", ""]]}, {"id": "2104.13015", "submitter": "Chongyi Li", "authors": "Chongyi Li and Saeed Anwar and Junhui Hou and Runmin Cong and Chunle\n  Guo and Wenqi Ren", "title": "Underwater Image Enhancement via Medium Transmission-Guided Multi-Color\n  Space Embedding", "comments": "Accepted by IEEE Transactions on Image Processing", "journal-ref": null, "doi": "10.1109/TIP.2021.3076367", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Underwater images suffer from color casts and low contrast due to wavelength-\nand distance-dependent attenuation and scattering. To solve these two\ndegradation issues, we present an underwater image enhancement network via\nmedium transmission-guided multi-color space embedding, called Ucolor.\nConcretely, we first propose a multi-color space encoder network, which\nenriches the diversity of feature representations by incorporating the\ncharacteristics of different color spaces into a unified structure. Coupled\nwith an attention mechanism, the most discriminative features extracted from\nmultiple color spaces are adaptively integrated and highlighted. Inspired by\nunderwater imaging physical models, we design a medium transmission (indicating\nthe percentage of the scene radiance reaching the camera)-guided decoder\nnetwork to enhance the response of the network towards quality-degraded\nregions. As a result, our network can effectively improve the visual quality of\nunderwater images by exploiting multiple color spaces embedding and the\nadvantages of both physical model-based and learning-based methods. Extensive\nexperiments demonstrate that our Ucolor achieves superior performance against\nstate-of-the-art methods in terms of both visual quality and quantitative\nmetrics.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 07:35:30 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Li", "Chongyi", ""], ["Anwar", "Saeed", ""], ["Hou", "Junhui", ""], ["Cong", "Runmin", ""], ["Guo", "Chunle", ""], ["Ren", "Wenqi", ""]]}, {"id": "2104.13018", "submitter": "Hongxin Wang", "authors": "Hongxin Wang, Jiannan Zhao, Huatian Wang, Cheng Hu, Jigen Peng,\n  Shigang Yue", "title": "Attention and Prediction Guided Motion Detection for Low-Contrast Small\n  Moving Targets", "comments": "14 pages, 25 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Small target motion detection within complex natural environments is an\nextremely challenging task for autonomous robots. Surprisingly, the visual\nsystems of insects have evolved to be highly efficient in detecting mates and\ntracking prey, even though targets are as small as a few pixels in their visual\nfields. The excellent sensitivity to small target motion relies on a class of\nspecialized neurons called small target motion detectors (STMDs). However,\nexisting STMD-based models are heavily dependent on visual contrast and perform\npoorly in complex natural environments where small targets generally exhibit\nextremely low contrast against neighbouring backgrounds. In this paper, we\ndevelop an attention and prediction guided visual system to overcome this\nlimitation. The developed visual system comprises three main subsystems,\nnamely, an attention module, an STMD-based neural network, and a prediction\nmodule. The attention module searches for potential small targets in the\npredicted areas of the input image and enhances their contrast against complex\nbackground. The STMD-based neural network receives the contrast-enhanced image\nand discriminates small moving targets from background false positives. The\nprediction module foresees future positions of the detected targets and\ngenerates a prediction map for the attention module. The three subsystems are\nconnected in a recurrent architecture allowing information to be processed\nsequentially to activate specific areas for small target detection. Extensive\nexperiments on synthetic and real-world datasets demonstrate the effectiveness\nand superiority of the proposed visual system for detecting small, low-contrast\nmoving targets against complex natural environments.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 07:42:31 GMT"}, {"version": "v2", "created": "Wed, 28 Apr 2021 01:57:33 GMT"}, {"version": "v3", "created": "Sat, 1 May 2021 07:00:49 GMT"}, {"version": "v4", "created": "Sat, 8 May 2021 07:04:12 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Wang", "Hongxin", ""], ["Zhao", "Jiannan", ""], ["Wang", "Huatian", ""], ["Hu", "Cheng", ""], ["Peng", "Jigen", ""], ["Yue", "Shigang", ""]]}, {"id": "2104.13037", "submitter": "Martin Ki\\v{s}\\v{s}", "authors": "Martin Ki\\v{s}\\v{s} and Karel Bene\\v{s} and Michal Hradi\\v{s}", "title": "AT-ST: Self-Training Adaptation Strategy for OCR in Domains with Limited\n  Transcriptions", "comments": "15 pages, 6 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses text recognition for domains with limited manual\nannotations by a simple self-training strategy. Our approach should reduce\nhuman annotation effort when target domain data is plentiful, such as when\ntranscribing a collection of single person's correspondence or a large\nmanuscript. We propose to train a seed system on large scale data from related\ndomains mixed with available annotated data from the target domain. The seed\nsystem transcribes the unannotated data from the target domain which is then\nused to train a better system. We study several confidence measures and\neventually decide to use the posterior probability of a transcription for data\nselection. Additionally, we propose to augment the data using an aggressive\nmasking scheme. By self-training, we achieve up to 55 % reduction in character\nerror rate for handwritten data and up to 38 % on printed data. The masking\naugmentation itself reduces the error rate by about 10 % and its effect is\nbetter pronounced in case of difficult handwritten data.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 08:20:46 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Ki\u0161\u0161", "Martin", ""], ["Bene\u0161", "Karel", ""], ["Hradi\u0161", "Michal", ""]]}, {"id": "2104.13044", "submitter": "Xian-Feng Han", "authors": "Xian-Feng Han and Yi-Fei Jin and Hui-Xian Cheng and Guo-Qiang Xiao", "title": "Dual Transformer for Point Cloud Analysis", "comments": "8 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Following the tremendous success of transformer in natural language\nprocessing and image understanding tasks, in this paper, we present a novel\npoint cloud representation learning architecture, named Dual Transformer\nNetwork (DTNet), which mainly consists of Dual Point Cloud Transformer (DPCT)\nmodule. Specifically, by aggregating the well-designed point-wise and\nchannel-wise multi-head self-attention models simultaneously, DPCT module can\ncapture much richer contextual dependencies semantically from the perspective\nof position and channel. With the DPCT module as a fundamental component, we\nconstruct the DTNet for performing point cloud analysis in an end-to-end\nmanner. Extensive quantitative and qualitative experiments on publicly\navailable benchmarks demonstrate the effectiveness of our proposed transformer\nframework for the tasks of 3D point cloud classification and segmentation,\nachieving highly competitive performance in comparison with the\nstate-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 08:41:02 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Han", "Xian-Feng", ""], ["Jin", "Yi-Fei", ""], ["Cheng", "Hui-Xian", ""], ["Xiao", "Guo-Qiang", ""]]}, {"id": "2104.13051", "submitter": "Ivaxi Miteshkumar Sheth", "authors": "Ivaxi Sheth", "title": "Three-stream network for enriched Action Recognition", "comments": "CVPR 2021 workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Understanding accurate information on human behaviours is one of the most\nimportant tasks in machine intelligence. Human Activity Recognition that aims\nto understand human activities from a video is a challenging task due to\nvarious problems including background, camera motion and dataset variations.\nThis paper proposes two CNN based architectures with three streams which allow\nthe model to exploit the dataset under different settings. The three pathways\nare differentiated in frame rates. The single pathway, operates at a single\nframe rate captures spatial information, the slow pathway operates at low frame\nrates captures the spatial information and the fast pathway operates at high\nframe rates that capture fine temporal information. Post CNN encoders, we add\nbidirectional LSTM and attention heads respectively to capture the context and\ntemporal features. By experimenting with various algorithms on UCF-101,\nKinetics-600 and AVA dataset, we observe that the proposed models achieve\nstate-of-art performance for human action recognition task.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 08:56:11 GMT"}, {"version": "v2", "created": "Wed, 23 Jun 2021 23:11:00 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Sheth", "Ivaxi", ""]]}, {"id": "2104.13053", "submitter": "Xian-Feng Han", "authors": "Xian-Feng Han and Zhang-Yue He and Jia Chen and Guo-Qiang Xiao", "title": "Cross-Level Cross-Scale Cross-Attention Network for Point Cloud\n  Representation", "comments": "8 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Self-attention mechanism recently achieves impressive advancement in Natural\nLanguage Processing (NLP) and Image Processing domains. And its permutation\ninvariance property makes it ideally suitable for point cloud processing.\nInspired by this remarkable success, we propose an end-to-end architecture,\ndubbed Cross-Level Cross-Scale Cross-Attention Network (CLCSCANet), for point\ncloud representation learning. First, a point-wise feature pyramid module is\nintroduced to hierarchically extract features from different scales or\nresolutions. Then a cross-level cross-attention is designed to model long-range\ninter-level and intra-level dependencies. Finally, we develop a cross-scale\ncross-attention module to capture interactions between-and-within scales for\nrepresentation enhancement. Compared with state-of-the-art approaches, our\nnetwork can obtain competitive performance on challenging 3D object\nclassification, point cloud segmentation tasks via comprehensive experimental\nevaluation.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 09:01:14 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Han", "Xian-Feng", ""], ["He", "Zhang-Yue", ""], ["Chen", "Jia", ""], ["Xiao", "Guo-Qiang", ""]]}, {"id": "2104.13057", "submitter": "Hang Wang", "authors": "Minghao Xu, Hang Wang, Bingbing Ni", "title": "Graphical Modeling for Multi-Source Domain Adaptation", "comments": "Source code will be released upon acceptance", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-Source Domain Adaptation (MSDA) focuses on transferring the knowledge\nfrom multiple source domains to the target domain, which is a more practical\nand challenging problem compared to the conventional single-source domain\nadaptation. In this problem, it is essential to utilize the labeled source data\nand the unlabeled target data to approach the conditional distribution of\nsemantic label on target domain, which requires the joint modeling across\ndifferent domains and also an effective domain combination scheme. The\ngraphical structure among different domains is useful to tackle these\nchallenges, in which the interdependency among various instances/categories can\nbe effectively modeled. In this work, we propose two types of graphical\nmodels,i.e. Conditional Random Field for MSDA (CRF-MSDA) and Markov Random\nField for MSDA (MRF-MSDA), for cross-domain joint modeling and learnable domain\ncombination. In a nutshell, given an observation set composed of a query sample\nand the semantic prototypes i.e. representative category embeddings) on various\ndomains, the CRF-MSDA model seeks to learn the joint distribution of labels\nconditioned on the observations. We attain this goal by constructing a\nrelational graph over all observations and conducting local message passing on\nit. By comparison, MRF-MSDA aims to model the joint distribution of\nobservations over different Markov networks via an energy-based formulation,\nand it can naturally perform label prediction by summing the joint likelihoods\nover several specific networks. Compared to the CRF-MSDA counterpart, the\nMRF-MSDA model is more expressive and possesses lower computational cost. We\nevaluate these two models on four standard benchmark data sets of MSDA with\ndistinct domain shift and data complexity, and both models achieve superior\nperformance over existing methods on all benchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 09:04:22 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Xu", "Minghao", ""], ["Wang", "Hang", ""], ["Ni", "Bingbing", ""]]}, {"id": "2104.13082", "submitter": "Shuailin Li", "authors": "Qian He, Shuailin Li and Xuming He", "title": "Weakly Supervised Volumetric Segmentation via Self-taught Shape\n  Denoising Model", "comments": "To appear in MIDL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weakly supervised segmentation is an important problem in medical image\nanalysis due to the high cost of pixelwise annotation. Prior methods, while\noften focusing on weak labels of 2D images, exploit few structural cues of\nvolumetric medical images. To address this, we propose a novel\nweakly-supervised segmentation strategy capable of better capturing 3D shape\nprior in both model prediction and learning. Our main idea is to extract a\nself-taught shape representation by leveraging weak labels, and then integrate\nthis representation into segmentation prediction for shape refinement. To this\nend, we design a deep network consisting of a segmentation module and a shape\ndenoising module, which are trained by an iterative learning strategy.\nMoreover, we introduce a weak annotation scheme with a hybrid label design for\nvolumetric images, which improves model learning without increasing the overall\nannotation cost. The empirical experiments show that our approach outperforms\nexisting SOTA strategies on three organ segmentation benchmarks with\ndistinctive shape properties. Notably, we can achieve strong performance with\neven 10\\% labeled slices, which is significantly superior to other methods.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 10:03:45 GMT"}, {"version": "v2", "created": "Thu, 6 May 2021 14:14:28 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["He", "Qian", ""], ["Li", "Shuailin", ""], ["He", "Xuming", ""]]}, {"id": "2104.13119", "submitter": "Younggon Jo", "authors": "Young Gon Jo, Seok Hyeon Hong, Sung Soo Hwang, and Jeong Mok Ha", "title": "Fisheye Lens Camera based Autonomous Valet Parking System", "comments": "8 pages, 17 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes an efficient autonomous valet parking system utilizing\nonly cameras which are the most widely used sensor. To capture more information\ninstantaneously and respond rapidly to changes in the surrounding environment,\nfisheye cameras which have a wider angle of view compared to pinhole cameras\nare used. Accordingly, visual simultaneous localization and mapping is used to\nidentify the layout of the parking lot and track the location of the vehicle.\nIn addition, the input image frames are converted into around view monitor\nimages to resolve the distortion of fisheye lens because the algorithm to\ndetect edges are supposed to be applied to images taken with pinhole cameras.\nThe proposed system adopts a look up table for real time operation by\nminimizing the computational complexity encountered when processing AVM images.\nThe detection rate of each process and the success rate of autonomous parking\nwere measured to evaluate performance. The experimental results confirm that\nautonomous parking can be achieved using only visual sensors.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 11:36:03 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Jo", "Young Gon", ""], ["Hong", "Seok Hyeon", ""], ["Hwang", "Sung Soo", ""], ["Ha", "Jeong Mok", ""]]}, {"id": "2104.13135", "submitter": "Fotios Logothetis Dr", "authors": "Roberto Mecca, Fotios Logothetis, Ignas Budvytis, Roberto Cipolla", "title": "LUCES: A Dataset for Near-Field Point Light Source Photometric Stereo", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Three-dimensional reconstruction of objects from shading information is a\nchallenging task in computer vision. As most of the approaches facing the\nPhotometric Stereo problem use simplified far-field assumptions, real-world\nscenarios have essentially more complex physical effects that need to be\nhandled for accurately reconstructing the 3D shape. An increasing number of\nmethods have been proposed to address the problem when point light sources are\nassumed to be nearby the target object. The proximity of the light sources\ncomplicates the modeling of the image formation as the light behaviour requires\nnon-linear parameterisation to describe its propagation and attenuation.\n  To understand the capability of the approaches dealing with this near-field\nscenario, the literature till now has used synthetically rendered photometric\nimages or minimal and very customised real-world data. In order to fill the gap\nin evaluating near-field photometric stereo methods, we introduce LUCES the\nfirst real-world 'dataset for near-fieLd point light soUrCe photomEtric Stereo'\nof 14 objects of a varying of materials. A device counting 52 LEDs has been\ndesigned to lit each object positioned 10 to 30 centimeters away from the\ncamera. Together with the raw images, in order to evaluate the 3D\nreconstructions, the dataset includes both normal and depth maps for comparing\ndifferent features of the retrieved 3D geometry. Furthermore, we evaluate the\nperformance of the latest near-field Photometric Stereo algorithms on the\nproposed dataset to assess the SOTA method with respect to actual close range\neffects and object materials.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 12:30:42 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Mecca", "Roberto", ""], ["Logothetis", "Fotios", ""], ["Budvytis", "Ignas", ""], ["Cipolla", "Roberto", ""]]}, {"id": "2104.13188", "submitter": "Mingyuan Fan", "authors": "Mingyuan Fan, Shenqi Lai, Junshi Huang, Xiaoming Wei, Zhenhua Chai,\n  Junfeng Luo, Xiaolin Wei", "title": "Rethinking BiSeNet For Real-time Semantic Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  BiSeNet has been proved to be a popular two-stream network for real-time\nsegmentation. However, its principle of adding an extra path to encode spatial\ninformation is time-consuming, and the backbones borrowed from pretrained\ntasks, e.g., image classification, may be inefficient for image segmentation\ndue to the deficiency of task-specific design. To handle these problems, we\npropose a novel and efficient structure named Short-Term Dense Concatenate\nnetwork (STDC network) by removing structure redundancy. Specifically, we\ngradually reduce the dimension of feature maps and use the aggregation of them\nfor image representation, which forms the basic module of STDC network. In the\ndecoder, we propose a Detail Aggregation module by integrating the learning of\nspatial information into low-level layers in single-stream manner. Finally, the\nlow-level features and deep features are fused to predict the final\nsegmentation results. Extensive experiments on Cityscapes and CamVid dataset\ndemonstrate the effectiveness of our method by achieving promising trade-off\nbetween segmentation accuracy and inference speed. On Cityscapes, we achieve\n71.9% mIoU on the test set with a speed of 250.4 FPS on NVIDIA GTX 1080Ti,\nwhich is 45.2% faster than the latest methods, and achieve 76.8% mIoU with 97.0\nFPS while inferring on higher resolution images.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 13:49:47 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Fan", "Mingyuan", ""], ["Lai", "Shenqi", ""], ["Huang", "Junshi", ""], ["Wei", "Xiaoming", ""], ["Chai", "Zhenhua", ""], ["Luo", "Junfeng", ""], ["Wei", "Xiaolin", ""]]}, {"id": "2104.13202", "submitter": "Chenglong Li", "authors": "Chenglong Li, Wanlin Xue, Yaqing Jia, Zhichen Qu, Bin Luo, and Jin\n  Tang", "title": "LasHeR: A Large-scale High-diversity Benchmark for RGBT Tracking", "comments": "In submission to TIP", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  RGBT tracking receives a surge of interest in the computer vision community,\nbut this research field lacks a large-scale and high-diversity benchmark\ndataset, which is essential for both the training of deep RGBT trackers and the\ncomprehensive evaluation of RGBT tracking methods. To this end, we present a\nLarge-scale High-diversity benchmark for RGBT tracking (LasHeR) in this work.\nLasHeR consists of 1224 visible and thermal infrared video pairs with more than\n730K frame pairs in total. Each frame pair is spatially aligned and manually\nannotated with a bounding box, making the dataset well and densely annotated.\nLasHeR is highly diverse capturing from a broad range of object categories,\ncamera viewpoints, scene complexities and environmental factors across seasons,\nweathers, day and night. We conduct a comprehensive performance evaluation of\n12 RGBT tracking algorithms on the LasHeR dataset and present detailed analysis\nto clarify the research room in RGBT tracking. In addition, we release the\nunaligned version of LasHeR to attract the research interest for alignment-free\nRGBT tracking, which is a more practical task in real-world applications. The\ndatasets and evaluation protocols are available at:\nhttps://github.com/BUGPLEASEOUT/LasHeR.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 14:04:23 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Li", "Chenglong", ""], ["Xue", "Wanlin", ""], ["Jia", "Yaqing", ""], ["Qu", "Zhichen", ""], ["Luo", "Bin", ""], ["Tang", "Jin", ""]]}, {"id": "2104.13214", "submitter": "Guang Yang A", "authors": "Mengmeng Kuang, Yinzhe Wu, Diego Alonso-\\'Alvarez, David Firmin,\n  Jennifer Keegan, Peter Gatehouse, Guang Yang", "title": "Three-Dimensional Embedded Attentive RNN (3D-EAR) Segmentor for Left\n  Ventricle Delineation from Myocardial Velocity Mapping", "comments": "8 pages, 4 figures, Functional Imaging and Modeling of the Heart", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Myocardial Velocity Mapping Cardiac MR (MVM-CMR) can be used to measure\nglobal and regional myocardial velocities with proved reproducibility. Accurate\nleft ventricle delineation is a prerequisite for robust and reproducible\nmyocardial velocity estimation. Conventional manual segmentation on this\ndataset can be time-consuming and subjective, and an effective fully automated\ndelineation method is highly in demand. By leveraging recently proposed deep\nlearning-based semantic segmentation approaches, in this study, we propose a\nnovel fully automated framework incorporating a 3D-UNet backbone architecture\nwith Embedded multichannel Attention mechanism and LSTM based Recurrent neural\nnetworks (RNN) for the MVM-CMR datasets (dubbed 3D-EAR segmentor). The proposed\nmethod also utilises the amalgamation of magnitude and phase images as input to\nrealise an information fusion of this multichannel dataset and exploring the\ncorrelations of temporal frames via the embedded RNN. By comparing the baseline\nmodel of 3D-UNet and ablation studies with and without embedded attentive LSTM\nmodules and various loss functions, we can demonstrate that the proposed model\nhas outperformed the state-of-the-art baseline models with significant\nimprovement.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 11:04:43 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Kuang", "Mengmeng", ""], ["Wu", "Yinzhe", ""], ["Alonso-\u00c1lvarez", "Diego", ""], ["Firmin", "David", ""], ["Keegan", "Jennifer", ""], ["Gatehouse", "Peter", ""], ["Yang", "Guang", ""]]}, {"id": "2104.13243", "submitter": "Simon Rei{\\ss}", "authors": "Simon Rei{\\ss}, Constantin Seibold, Alexander Freytag, Erik Rodner,\n  Rainer Stiefelhagen", "title": "Every Annotation Counts: Multi-label Deep Supervision for Medical Image\n  Segmentation", "comments": "Accepted at CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pixel-wise segmentation is one of the most data and annotation hungry tasks\nin our field. Providing representative and accurate annotations is often\nmission-critical especially for challenging medical applications. In this\npaper, we propose a semi-weakly supervised segmentation algorithm to overcome\nthis barrier. Our approach is based on a new formulation of deep supervision\nand student-teacher model and allows for easy integration of different\nsupervision signals. In contrast to previous work, we show that care has to be\ntaken how deep supervision is integrated in lower layers and we present\nmulti-label deep supervision as the most important secret ingredient for\nsuccess. With our novel training regime for segmentation that flexibly makes\nuse of images that are either fully labeled, marked with bounding boxes, just\nglobal labels, or not at all, we are able to cut the requirement for expensive\nlabels by 94.22% - narrowing the gap to the best fully supervised baseline to\nonly 5% mean IoU. Our approach is validated by extensive experiments on retinal\nfluid segmentation and we provide an in-depth analysis of the anticipated\neffect each annotation type can have in boosting segmentation performance.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 14:51:19 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Rei\u00df", "Simon", ""], ["Seibold", "Constantin", ""], ["Freytag", "Alexander", ""], ["Rodner", "Erik", ""], ["Stiefelhagen", "Rainer", ""]]}, {"id": "2104.13255", "submitter": "Ting-Wu Chin", "authors": "Ting-Wu Chin, Diana Marculescu, Ari S. Morcos", "title": "Width Transfer: On the (In)variance of Width Optimization", "comments": "Full paper accepted at CVPR Workshops 2021; a 4-page abridged version\n  is accepted at ICLR 2021 NAS Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Optimizing the channel counts for different layers of a CNN has shown great\npromise in improving the efficiency of CNNs at test-time. However, these\nmethods often introduce large computational overhead (e.g., an additional 2x\nFLOPs of standard training). Minimizing this overhead could therefore\nsignificantly speed up training. In this work, we propose width transfer, a\ntechnique that harnesses the assumptions that the optimized widths (or channel\ncounts) are regular across sizes and depths. We show that width transfer works\nwell across various width optimization algorithms and networks. Specifically,\nwe can achieve up to 320x reduction in width optimization overhead without\ncompromising the top-1 accuracy on ImageNet, making the additional cost of\nwidth optimization negligible relative to initial training. Our findings not\nonly suggest an efficient way to conduct width optimization but also highlight\nthat the widths that lead to better accuracy are invariant to various aspects\nof network architectures and training data.\n", "versions": [{"version": "v1", "created": "Sat, 24 Apr 2021 19:51:53 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Chin", "Ting-Wu", ""], ["Marculescu", "Diana", ""], ["Morcos", "Ari S.", ""]]}, {"id": "2104.13268", "submitter": "Madeleine Kotzagiannidis", "authors": "Madeleine Kotzagiannidis, Carola-Bibiane Sch\\\"onlieb", "title": "Semi-supervised Superpixel-based Multi-Feature Graph Learning for\n  Hyperspectral Image Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphs naturally lend themselves to model the complexities of Hyperspectral\nImage (HSI) data as well as to serve as semi-supervised classifiers by\npropagating given labels among nearest neighbours. In this work, we present a\nnovel framework for the classification of HSI data in light of a very limited\namount of labelled data, inspired by multi-view graph learning and graph signal\nprocessing. Given an a priori superpixel-segmented hyperspectral image, we seek\na robust and efficient graph construction and label propagation method to\nconduct semi-supervised learning (SSL). Since the graph is paramount to the\nsuccess of the subsequent classification task, particularly in light of the\nintrinsic complexity of HSI data, we consider the problem of finding the\noptimal graph to model such data. Our contribution is two-fold: firstly, we\npropose a multi-stage edge-efficient semi-supervised graph learning framework\nfor HSI data which exploits given label information through pseudo-label\nfeatures embedded in the graph construction. Secondly, we examine and enhance\nthe contribution of multiple superpixel features embedded in the graph on the\nbasis of pseudo-labels in an extension of the previous framework, which is less\nreliant on excessive parameter tuning. Ultimately, we demonstrate the\nsuperiority of our approaches in comparison with state-of-the-art methods\nthrough extensive numerical experiments.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 15:36:26 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Kotzagiannidis", "Madeleine", ""], ["Sch\u00f6nlieb", "Carola-Bibiane", ""]]}, {"id": "2104.13293", "submitter": "Huang Ling", "authors": "Ling Huang, Su Ruan, Pierre Decazes, Thierry Denoeux", "title": "Evidential segmentation of 3D PET/CT images", "comments": "Belief2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  PET and CT are two modalities widely used in medical image analysis.\nAccurately detecting and segmenting lymphomas from these two imaging modalities\nare critical tasks for cancer staging and radiotherapy planning. However, this\ntask is still challenging due to the complexity of PET/CT images, and the\ncomputation cost to process 3D data. In this paper, a segmentation method based\non belief functions is proposed to segment lymphomas in 3D PET/CT images. The\narchitecture is composed of a feature extraction module and an evidential\nsegmentation (ES) module. The ES module outputs not only segmentation results\n(binary maps indicating the presence or absence of lymphoma in each voxel) but\nalso uncertainty maps quantifying the classification uncertainty. The whole\nmodel is optimized by minimizing Dice and uncertainty loss functions to\nincrease segmentation accuracy. The method was evaluated on a database of 173\npatients with diffuse large b-cell lymphoma. Quantitative and qualitative\nresults show that our method outperforms the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 16:06:27 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Huang", "Ling", ""], ["Ruan", "Su", ""], ["Decazes", "Pierre", ""], ["Denoeux", "Thierry", ""]]}, {"id": "2104.13298", "submitter": "Yixiao Ge", "authors": "Yixiao Ge, Ching Lam Choi, Xiao Zhang, Peipei Zhao, Feng Zhu, Rui\n  Zhao, Hongsheng Li", "title": "Self-distillation with Batch Knowledge Ensembling Improves ImageNet\n  Classification", "comments": "Project Page: https://geyixiao.com/projects/bake", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent studies of knowledge distillation have discovered that ensembling\nthe \"dark knowledge\" from multiple teachers or students contributes to creating\nbetter soft targets for training, but at the cost of significantly more\ncomputations and/or parameters. In this work, we present BAtch Knowledge\nEnsembling (BAKE) to produce refined soft targets for anchor images by\npropagating and ensembling the knowledge of the other samples in the same\nmini-batch. Specifically, for each sample of interest, the propagation of\nknowledge is weighted in accordance with the inter-sample affinities, which are\nestimated on-the-fly with the current network. The propagated knowledge can\nthen be ensembled to form a better soft target for distillation. In this way,\nour BAKE framework achieves online knowledge ensembling across multiple samples\nwith only a single network. It requires minimal computational and memory\noverhead compared to existing knowledge ensembling methods. Extensive\nexperiments demonstrate that the lightweight yet effective BAKE consistently\nboosts the classification performance of various architectures on multiple\ndatasets, e.g., a significant +1.2% gain of ResNet-50 on ImageNet with only\n+3.7% computational overhead and zero additional parameters. BAKE does not only\nimprove the vanilla baselines, but also surpasses the single-network\nstate-of-the-arts on all the benchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 16:11:45 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Ge", "Yixiao", ""], ["Choi", "Ching Lam", ""], ["Zhang", "Xiao", ""], ["Zhao", "Peipei", ""], ["Zhu", "Feng", ""], ["Zhao", "Rui", ""], ["Li", "Hongsheng", ""]]}, {"id": "2104.13325", "submitter": "Zhenpei Yang", "authors": "Zhenpei Yang, Zhile Ren, Qi Shan, Qixing Huang", "title": "MVS2D: Efficient Multi-view Stereo via Attention-Driven 2D Convolutions", "comments": "project page: https://zhenpeiyang.github.io/MVS2D", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has made significant impacts on multi-view stereo systems.\nState-of-the-art approaches typically involve building a cost volume, followed\nby multiple 3D convolution operations to recover the input image's pixel-wise\ndepth. While such end-to-end learning of plane-sweeping stereo advances public\nbenchmarks' accuracy, they are typically very slow to compute. We present\nMVS2D, a highly efficient multi-view stereo algorithm that seamlessly\nintegrates multi-view constraints into single-view networks via an attention\nmechanism. Since MVS2D only builds on 2D convolutions, it is at least 4x faster\nthan all the notable counterparts. Moreover, our algorithm produces precise\ndepth estimations, achieving state-of-the-art results on challenging benchmarks\nScanNet, SUN3D, and RGBD. Even under inexact camera poses, our algorithm still\nout-performs all other algorithms. Supplementary materials and code will be\navailable at the project page: https://zhenpeiyang.github.io/MVS2D\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 16:56:05 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Yang", "Zhenpei", ""], ["Ren", "Zhile", ""], ["Shan", "Qi", ""], ["Huang", "Qixing", ""]]}, {"id": "2104.13332", "submitter": "Rodrigo Mira", "authors": "Rodrigo Mira, Konstantinos Vougioukas, Pingchuan Ma, Stavros Petridis,\n  Bj\\\"orn W. Schuller, Maja Pantic", "title": "End-to-End Video-To-Speech Synthesis using Generative Adversarial\n  Networks", "comments": "This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video-to-speech is the process of reconstructing the audio speech from a\nvideo of a spoken utterance. Previous approaches to this task have relied on a\ntwo-step process where an intermediate representation is inferred from the\nvideo, and is then decoded into waveform audio using a vocoder or a waveform\nreconstruction algorithm. In this work, we propose a new end-to-end\nvideo-to-speech model based on Generative Adversarial Networks (GANs) which\ntranslates spoken video to waveform end-to-end without using any intermediate\nrepresentation or separate waveform synthesis algorithm. Our model consists of\nan encoder-decoder architecture that receives raw video as input and generates\nspeech, which is then fed to a waveform critic and a power critic. The use of\nan adversarial loss based on these two critics enables the direct synthesis of\nraw audio waveform and ensures its realism. In addition, the use of our three\ncomparative losses helps establish direct correspondence between the generated\naudio and the input video. We show that this model is able to reconstruct\nspeech with remarkable realism for constrained datasets such as GRID, and that\nit is the first end-to-end model to produce intelligible speech for LRW (Lip\nReading in the Wild), featuring hundreds of speakers recorded entirely `in the\nwild'. We evaluate the generated samples in two different scenarios -- seen and\nunseen speakers -- using four objective metrics which measure the quality and\nintelligibility of artificial speech. We demonstrate that the proposed approach\noutperforms all previous works in most metrics on GRID and LRW.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 17:12:30 GMT"}, {"version": "v2", "created": "Fri, 30 Apr 2021 17:04:57 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Mira", "Rodrigo", ""], ["Vougioukas", "Konstantinos", ""], ["Ma", "Pingchuan", ""], ["Petridis", "Stavros", ""], ["Schuller", "Bj\u00f6rn W.", ""], ["Pantic", "Maja", ""]]}, {"id": "2104.13343", "submitter": "Franco Pellegrini", "authors": "Franco Pellegrini, Giulio Biroli", "title": "Sifting out the features by pruning: Are convolutional networks the\n  winning lottery ticket of fully connected ones?", "comments": "25 pages, 18 figures; typos corrected, references added", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Pruning methods can considerably reduce the size of artificial neural\nnetworks without harming their performance. In some cases, they can even\nuncover sub-networks that, when trained in isolation, match or surpass the test\naccuracy of their dense counterparts. Here we study the inductive bias that\npruning imprints in such \"winning lottery tickets\". Focusing on visual tasks,\nwe analyze the architecture resulting from iterative magnitude pruning of a\nsimple fully connected network (FCN). We show that the surviving node\nconnectivity is local in input space, and organized in patterns reminiscent of\nthe ones found in convolutional networks (CNN). We investigate the role played\nby data and tasks in shaping the architecture of pruned sub-networks. Our\nresults show that the winning lottery tickets of FCNs display the key features\nof CNNs. The ability of such automatic network-simplifying procedure to recover\nthe key features \"hand-crafted\" in the design of CNNs suggests interesting\napplications to other datasets and tasks, in order to discover new and\nefficient architectural inductive biases.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 17:25:54 GMT"}, {"version": "v2", "created": "Fri, 14 May 2021 10:52:49 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Pellegrini", "Franco", ""], ["Biroli", "Giulio", ""]]}, {"id": "2104.13365", "submitter": "Majed El Helou", "authors": "Majed El Helou and Ruofan Zhou and Sabine Susstrunk and Radu Timofte", "title": "NTIRE 2021 Depth Guided Image Relighting Challenge", "comments": "Code and data available on https://github.com/majedelhelou/VIDIT", "journal-ref": "IEEE Conference on Computer Vision and Pattern Recognition\n  Workshops 2021", "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image relighting is attracting increasing interest due to its various\napplications. From a research perspective, image relighting can be exploited to\nconduct both image normalization for domain adaptation, and also for data\naugmentation. It also has multiple direct uses for photo montage and aesthetic\nenhancement. In this paper, we review the NTIRE 2021 depth guided image\nrelighting challenge.\n  We rely on the VIDIT dataset for each of our two challenge tracks, including\ndepth information. The first track is on one-to-one relighting where the goal\nis to transform the illumination setup of an input image (color temperature and\nlight source position) to the target illumination setup. In the second track,\nthe any-to-any relighting challenge, the objective is to transform the\nillumination settings of the input image to match those of another guide image,\nsimilar to style transfer. In both tracks, participants were given depth\ninformation about the captured scenes. We had nearly 250 registered\nparticipants, leading to 18 confirmed team submissions in the final competition\nstage. The competitions, methods, and final results are presented in this\npaper.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 17:53:32 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Helou", "Majed El", ""], ["Zhou", "Ruofan", ""], ["Susstrunk", "Sabine", ""], ["Timofte", "Radu", ""]]}, {"id": "2104.13366", "submitter": "Junzhe Zhang Mr", "authors": "Junzhe Zhang, Xinyi Chen, Zhongang Cai, Liang Pan, Haiyu Zhao, Shuai\n  Yi, Chai Kiat Yeo, Bo Dai, Chen Change Loy", "title": "Unsupervised 3D Shape Completion through GAN Inversion", "comments": "Accepted in CVPR 2021, project webpage:\n  https://junzhezhang.github.io/projects/ShapeInversion/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Most 3D shape completion approaches rely heavily on partial-complete shape\npairs and learn in a fully supervised manner. Despite their impressive\nperformances on in-domain data, when generalizing to partial shapes in other\nforms or real-world partial scans, they often obtain unsatisfactory results due\nto domain gaps. In contrast to previous fully supervised approaches, in this\npaper we present ShapeInversion, which introduces Generative Adversarial\nNetwork (GAN) inversion to shape completion for the first time. ShapeInversion\nuses a GAN pre-trained on complete shapes by searching for a latent code that\ngives a complete shape that best reconstructs the given partial input. In this\nway, ShapeInversion no longer needs paired training data, and is capable of\nincorporating the rich prior captured in a well-trained generative model. On\nthe ShapeNet benchmark, the proposed ShapeInversion outperforms the SOTA\nunsupervised method, and is comparable with supervised methods that are learned\nusing paired data. It also demonstrates remarkable generalization ability,\ngiving robust results for real-world scans and partial inputs of various forms\nand incompleteness levels. Importantly, ShapeInversion naturally enables a\nseries of additional abilities thanks to the involvement of a pre-trained GAN,\nsuch as producing multiple valid complete shapes for an ambiguous partial\ninput, as well as shape manipulation and interpolation.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 17:53:46 GMT"}, {"version": "v2", "created": "Thu, 29 Apr 2021 13:09:32 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Zhang", "Junzhe", ""], ["Chen", "Xinyi", ""], ["Cai", "Zhongang", ""], ["Pan", "Liang", ""], ["Zhao", "Haiyu", ""], ["Yi", "Shuai", ""], ["Yeo", "Chai Kiat", ""], ["Dai", "Bo", ""], ["Loy", "Chen Change", ""]]}, {"id": "2104.13369", "submitter": "Michal Yarom", "authors": "Oran Lang, Yossi Gandelsman, Michal Yarom, Yoav Wald, Gal Elidan,\n  Avinatan Hassidim, William T. Freeman, Phillip Isola, Amir Globerson, Michal\n  Irani, Inbar Mosseri", "title": "Explaining in Style: Training a GAN to explain a classifier in\n  StyleSpace", "comments": "First four authors contributed equally. Project page:\n  https://explaining-in-style.github.io/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image classification models can depend on multiple different semantic\nattributes of the image. An explanation of the decision of the classifier needs\nto both discover and visualize these properties. Here we present StylEx, a\nmethod for doing this, by training a generative model to specifically explain\nmultiple attributes that underlie classifier decisions. A natural source for\nsuch attributes is the StyleSpace of StyleGAN, which is known to generate\nsemantically meaningful dimensions in the image. However, because standard GAN\ntraining is not dependent on the classifier, it may not represent these\nattributes which are important for the classifier decision, and the dimensions\nof StyleSpace may represent irrelevant attributes. To overcome this, we propose\na training procedure for a StyleGAN, which incorporates the classifier model,\nin order to learn a classifier-specific StyleSpace. Explanatory attributes are\nthen selected from this space. These can be used to visualize the effect of\nchanging multiple attributes per image, thus providing image-specific\nexplanations. We apply StylEx to multiple domains, including animals, leaves,\nfaces and retinal images. For these, we show how an image can be modified in\ndifferent ways to change its classifier output. Our results show that the\nmethod finds attributes that align well with semantic ones, generate meaningful\nimage-specific explanations, and are human-interpretable as measured in\nuser-studies.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 17:57:19 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Lang", "Oran", ""], ["Gandelsman", "Yossi", ""], ["Yarom", "Michal", ""], ["Wald", "Yoav", ""], ["Elidan", "Gal", ""], ["Hassidim", "Avinatan", ""], ["Freeman", "William T.", ""], ["Isola", "Phillip", ""], ["Globerson", "Amir", ""], ["Irani", "Michal", ""], ["Mosseri", "Inbar", ""]]}, {"id": "2104.13371", "submitter": "Kelvin C.K. Chan", "authors": "Kelvin C.K. Chan, Shangchen Zhou, Xiangyu Xu, Chen Change Loy", "title": "BasicVSR++: Improving Video Super-Resolution with Enhanced Propagation\n  and Alignment", "comments": "3 champions and 1 runner-up in NTIRE 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A recurrent structure is a popular framework choice for the task of video\nsuper-resolution. The state-of-the-art method BasicVSR adopts bidirectional\npropagation with feature alignment to effectively exploit information from the\nentire input video. In this study, we redesign BasicVSR by proposing\nsecond-order grid propagation and flow-guided deformable alignment. We show\nthat by empowering the recurrent framework with the enhanced propagation and\nalignment, one can exploit spatiotemporal information across misaligned video\nframes more effectively. The new components lead to an improved performance\nunder a similar computational constraint. In particular, our model BasicVSR++\nsurpasses BasicVSR by 0.82 dB in PSNR with similar number of parameters. In\naddition to video super-resolution, BasicVSR++ generalizes well to other video\nrestoration tasks such as compressed video enhancement. In NTIRE 2021,\nBasicVSR++ obtains three champions and one runner-up in the Video\nSuper-Resolution and Compressed Video Enhancement Challenges. Codes and models\nwill be released to MMEditing.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 17:58:31 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Chan", "Kelvin C. K.", ""], ["Zhou", "Shangchen", ""], ["Xu", "Xiangyu", ""], ["Loy", "Chen Change", ""]]}, {"id": "2104.13395", "submitter": "Christos Sakaridis", "authors": "Christos Sakaridis, Dengxin Dai, Luc Van Gool", "title": "ACDC: The Adverse Conditions Dataset with Correspondences for Semantic\n  Driving Scene Understanding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Level 5 autonomy for self-driving cars requires a robust visual perception\nsystem that can parse input images under any visual condition. However,\nexisting semantic segmentation datasets are either dominated by images captured\nunder normal conditions or are small in scale. To address this, we introduce\nACDC, the Adverse Conditions Dataset with Correspondences for training and\ntesting semantic segmentation methods on adverse visual conditions. ACDC\nconsists of a large set of 4006 images which are equally distributed between\nfour common adverse conditions: fog, nighttime, rain, and snow. Each\nadverse-condition image comes with a high-quality fine pixel-level semantic\nannotation, a corresponding image of the same scene taken under normal\nconditions, and a binary mask that distinguishes between intra-image regions of\nclear and uncertain semantic content. Thus, ACDC supports both standard\nsemantic segmentation and the newly introduced uncertainty-aware semantic\nsegmentation. A detailed empirical study demonstrates the challenges that the\nadverse domains of ACDC pose to state-of-the-art supervised and unsupervised\napproaches and indicates the value of our dataset in steering future progress\nin the field. Our dataset and benchmark are publicly available.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 18:00:05 GMT"}, {"version": "v2", "created": "Thu, 29 Apr 2021 06:11:26 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Sakaridis", "Christos", ""], ["Dai", "Dengxin", ""], ["Van Gool", "Luc", ""]]}, {"id": "2104.13400", "submitter": "Babak Ehteshami Bejnordi", "authors": "Amir Ghodrati, Babak Ehteshami Bejnordi, Amirhossein Habibian", "title": "FrameExit: Conditional Early Exiting for Efficient Video Recognition", "comments": "CVPR 2021 | Oral paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we propose a conditional early exiting framework for efficient\nvideo recognition. While existing works focus on selecting a subset of salient\nframes to reduce the computation costs, we propose to use a simple sampling\nstrategy combined with conditional early exiting to enable efficient\nrecognition. Our model automatically learns to process fewer frames for simpler\nvideos and more frames for complex ones. To achieve this, we employ a cascade\nof gating modules to automatically determine the earliest point in processing\nwhere an inference is sufficiently reliable. We generate on-the-fly supervision\nsignals to the gates to provide a dynamic trade-off between accuracy and\ncomputational cost. Our proposed model outperforms competing methods on three\nlarge-scale video benchmarks. In particular, on ActivityNet1.3 and\nmini-kinetics, we outperform the state-of-the-art efficient video recognition\nmethods with 1.3$\\times$ and 2.1$\\times$ less GFLOPs, respectively.\nAdditionally, our method sets a new state of the art for efficient video\nunderstanding on the HVU benchmark.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 18:01:05 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Ghodrati", "Amir", ""], ["Bejnordi", "Babak Ehteshami", ""], ["Habibian", "Amirhossein", ""]]}, {"id": "2104.13415", "submitter": "I\\~nigo Alonso", "authors": "Inigo Alonso, Alberto Sabater, David Ferstl, Luis Montesano, Ana C.\n  Murillo", "title": "Semi-Supervised Semantic Segmentation with Pixel-Level Contrastive\n  Learning from a Class-wise Memory Bank", "comments": null, "journal-ref": "IEEE International Conference on Computer Vision 2021", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This work presents a novel approach for semi-supervised semantic\nsegmentation. The key element of this approach is our contrastive learning\nmodule that enforces the segmentation network to yield similar pixel-level\nfeature representations for same-class samples across the whole dataset. To\nachieve this, we maintain a memory bank continuously updated with relevant and\nhigh-quality feature vectors from labeled data. In an end-to-end training, the\nfeatures from both labeled and unlabeled data are optimized to be similar to\nsame-class samples from the memory bank. Our approach outperforms the current\nstate-of-the-art for semi-supervised semantic segmentation and semi-supervised\ndomain adaptation on well-known public benchmarks, with larger improvements on\nthe most challenging scenarios, i.e., less available labeled data.\nhttps://github.com/Shathe/SemiSeg-Contrastive\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 18:19:33 GMT"}, {"version": "v2", "created": "Mon, 26 Jul 2021 13:22:56 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Alonso", "Inigo", ""], ["Sabater", "Alberto", ""], ["Ferstl", "David", ""], ["Montesano", "Luis", ""], ["Murillo", "Ana C.", ""]]}, {"id": "2104.13417", "submitter": "Weituo Hao", "authors": "Weituo Hao, Mostafa El-Khamy, Jungwon Lee, Jianyi Zhang, Kevin J\n  Liang, Changyou Chen, Lawrence Carin", "title": "Towards Fair Federated Learning with Zero-Shot Data Augmentation", "comments": "Accepted by IEEE CVPR Workshop on Fair, Data Efficient And Trusted\n  Computer Vision", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning has emerged as an important distributed learning paradigm,\nwhere a server aggregates a global model from many client-trained models while\nhaving no access to the client data. Although it is recognized that statistical\nheterogeneity of the client local data yields slower global model convergence,\nit is less commonly recognized that it also yields a biased federated global\nmodel with a high variance of accuracy across clients. In this work, we aim to\nprovide federated learning schemes with improved fairness. To tackle this\nchallenge, we propose a novel federated learning system that employs zero-shot\ndata augmentation on under-represented data to mitigate statistical\nheterogeneity and encourage more uniform accuracy performance across clients in\nfederated networks. We study two variants of this scheme, Fed-ZDAC (federated\nlearning with zero-shot data augmentation at the clients) and Fed-ZDAS\n(federated learning with zero-shot data augmentation at the server). Empirical\nresults on a suite of datasets demonstrate the effectiveness of our methods on\nsimultaneously improving the test accuracy and fairness.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 18:23:54 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Hao", "Weituo", ""], ["El-Khamy", "Mostafa", ""], ["Lee", "Jungwon", ""], ["Zhang", "Jianyi", ""], ["Liang", "Kevin J", ""], ["Chen", "Changyou", ""], ["Carin", "Lawrence", ""]]}, {"id": "2104.13433", "submitter": "Siyuan Xiang", "authors": "Siyuan Xiang, Anbang Yang, Yanfei Xue, Yaoqing Yang, Chen Feng", "title": "Contrastive Spatial Reasoning on Multi-View Line Drawings", "comments": "The first two authors contributed equally. Chen Feng is the\n  corresponding author", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatial reasoning on multi-view line drawings by state-of-the-art supervised\ndeep networks is recently shown with puzzling low performances on the SPARE3D\ndataset. To study the reason behind the low performance and to further our\nunderstandings of these tasks, we design controlled experiments on both input\ndata and network designs. Guided by the hindsight from these experiment\nresults, we propose a simple contrastive learning approach along with other\nnetwork modifications to improve the baseline performance. Our approach uses a\nself-supervised binary classification network to compare the line drawing\ndifferences between various views of any two similar 3D objects. It enables\ndeep networks to effectively learn detail-sensitive yet view-invariant line\ndrawing representations of 3D objects. Experiments show that our method could\nsignificantly increase the baseline performance in SPARE3D, while some popular\nself-supervised learning methods cannot.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 19:05:27 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Xiang", "Siyuan", ""], ["Yang", "Anbang", ""], ["Xue", "Yanfei", ""], ["Yang", "Yaoqing", ""], ["Feng", "Chen", ""]]}, {"id": "2104.13437", "submitter": "Murat Tulga\\c{c}", "authors": "Murat Tulga\\c{c}, Enes Y\\\"unc\\\"u, Mohamad-Alhaddad and Ceylan\n  Yozgatl{\\i}gil", "title": "Incident Detection on Junctions Using Image Processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  In traffic management, it is a very important issue to shorten the response\ntime by detecting the incidents (accident, vehicle breakdown, an object falling\non the road, etc.) and informing the corresponding personnel. In this study, an\nanomaly detection framework for road junctions is proposed. The final judgment\nis based on the trajectories followed by the vehicles. Trajectory information\nis provided by vehicle detection and tracking algorithms on visual data\nstreamed from a fisheye camera. Deep learning algorithms are used for vehicle\ndetection, and Kalman Filter is used for tracking. To observe the trajectories\nmore accurately, the detected vehicle coordinates are transferred to the bird's\neye view coordinates using the lens distortion model prediction algorithm. The\nsystem determines whether there is an abnormality in trajectories by comparing\nhistorical trajectory data and instantaneous incoming data. The proposed system\nhas achieved 84.6% success in vehicle detection and 96.8% success in\nabnormality detection on synthetic data. The system also works with a 97.3%\nsuccess rate in detecting abnormalities on real data.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 19:18:05 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Tulga\u00e7", "Murat", ""], ["Y\u00fcnc\u00fc", "Enes", ""], ["Mohamad-Alhaddad", "", ""], ["Yozgatl\u0131gil", "Ceylan", ""]]}, {"id": "2104.13449", "submitter": "Elvis Nunez", "authors": "Elvis Nunez, Andrew Lizarraga, and Shantanu H. Joshi", "title": "SrvfNet: A Generative Network for Unsupervised Multiple Diffeomorphic\n  Shape Alignment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG math.DG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We present SrvfNet, a generative deep learning framework for the joint\nmultiple alignment of large collections of functional data comprising\nsquare-root velocity functions (SRVF) to their templates. Our proposed\nframework is fully unsupervised and is capable of aligning to a predefined\ntemplate as well as jointly predicting an optimal template from data while\nsimultaneously achieving alignment. Our network is constructed as a generative\nencoder-decoder architecture comprising fully-connected layers capable of\nproducing a distribution space of the warping functions. We demonstrate the\nstrength of our framework by validating it on synthetic data as well as\ndiffusion profiles from magnetic resonance imaging (MRI) data.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 19:49:46 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Nunez", "Elvis", ""], ["Lizarraga", "Andrew", ""], ["Joshi", "Shantanu H.", ""]]}, {"id": "2104.13450", "submitter": "Innfarn Yoo", "authors": "Innfarn Yoo and Huiwen Chang and Xiyang Luo and Ondrej Stava and Ce\n  Liu and Peyman Milanfar and Feng Yang", "title": "Deep 3D-to-2D Watermarking: Embedding Messages in 3D Meshes and\n  Extracting Them from 2D Renderings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Digital watermarking is widely used for copyright protection. Traditional 3D\nwatermarking approaches or commercial software are typically designed to embed\nmessages into 3D meshes, and later retrieve the messages directly from\ndistorted/undistorted watermarked 3D meshes. Retrieving messages from 2D\nrenderings of such meshes, however, is still challenging and underexplored. We\nintroduce a novel end-to-end learning framework to solve this problem through:\n1) an encoder to covertly embed messages in both mesh geometry and textures; 2)\na differentiable renderer to render watermarked 3D objects from different\ncamera angles and under varied lighting conditions; 3) a decoder to recover the\nmessages from 2D rendered images. From extensive experiments, we show that our\nmodels learn to embed information visually imperceptible to humans, and to\nreconstruct the embedded information from 2D renderings robust to 3D\ndistortions. In addition, we demonstrate that our method can be generalized to\nwork with different renderers, such as ray tracers and real-time renderers.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 19:51:39 GMT"}, {"version": "v2", "created": "Thu, 29 Apr 2021 05:37:55 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Yoo", "Innfarn", ""], ["Chang", "Huiwen", ""], ["Luo", "Xiyang", ""], ["Stava", "Ondrej", ""], ["Liu", "Ce", ""], ["Milanfar", "Peyman", ""], ["Yang", "Feng", ""]]}, {"id": "2104.13454", "submitter": "Jian Wang", "authors": "Jian Wang and Lingjie Liu and Weipeng Xu and Kripasindhu Sarkar and\n  Christian Theobalt", "title": "Estimating Egocentric 3D Human Pose in Global Space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Egocentric 3D human pose estimation using a single fisheye camera has become\npopular recently as it allows capturing a wide range of daily activities in\nunconstrained environments, which is difficult for traditional outside-in\nmotion capture with external cameras. However, existing methods have several\nlimitations. A prominent problem is that the estimated poses lie in the local\ncoordinate system of the fisheye camera, rather than in the world coordinate\nsystem, which is restrictive for many applications. Furthermore, these methods\nsuffer from limited accuracy and temporal instability due to ambiguities caused\nby the monocular setup and the severe occlusion in a strongly distorted\negocentric perspective. To tackle these limitations, we present a new method\nfor egocentric global 3D body pose estimation using a single head-mounted\nfisheye camera. To achieve accurate and temporally stable global poses, a\nspatio-temporal optimization is performed over a sequence of frames by\nminimizing heatmap reprojection errors and enforcing local and global body\nmotion priors learned from a mocap dataset. Experimental results show that our\napproach outperforms state-of-the-art methods both quantitatively and\nqualitatively.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 20:01:57 GMT"}, {"version": "v2", "created": "Fri, 30 Apr 2021 20:13:07 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Wang", "Jian", ""], ["Liu", "Lingjie", ""], ["Xu", "Weipeng", ""], ["Sarkar", "Kripasindhu", ""], ["Theobalt", "Christian", ""]]}, {"id": "2104.13464", "submitter": "Andrey Moskalenko", "authors": "Andrey Moskalenko, Mikhail Erofeev, Dmitriy Vatolin", "title": "Deep Two-Stage High-Resolution Image Inpainting", "comments": null, "journal-ref": null, "doi": "10.51130/graphicon-2020-2-4-18", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In recent years, the field of image inpainting has developed rapidly,\nlearning based approaches show impressive results in the task of filling\nmissing parts in an image. But most deep methods are strongly tied to the\nresolution of the images on which they were trained. A slight resolution\nincrease leads to serious artifacts and unsatisfactory filling quality. These\nmethods are therefore unsuitable for interactive image processing. In this\narticle, we propose a method that solves the problem of inpainting\narbitrary-size images. We also describe a way to better restore texture\nfragments in the filled area. For this, we propose to use information from\nneighboring pixels by shifting the original image in four directions. Moreover,\nthis approach can work with existing inpainting models, making them almost\nresolution independent without the need for retraining. We also created a GIMP\nplugin that implements our technique. The plugin, code, and model weights are\navailable at https://github.com/a-mos/High_Resolution_Image_Inpainting.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 20:32:21 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Moskalenko", "Andrey", ""], ["Erofeev", "Mikhail", ""], ["Vatolin", "Dmitriy", ""]]}, {"id": "2104.13473", "submitter": "Asad Anwar Butt", "authors": "George Awad, Asad A. Butt, Keith Curtis, Jonathan Fiscus, Afzal Godil,\n  Yooyoung Lee, Andrew Delgado, Jesse Zhang, Eliot Godard, Baptiste Chocot,\n  Lukas Diduch, Jeffrey Liu, Alan F. Smeaton, Yvette Graham, Gareth J. F.\n  Jones, Wessel Kraaij, Georges Quenot", "title": "TRECVID 2020: A comprehensive campaign for evaluating video retrieval\n  tasks across multiple application domains", "comments": "TRECVID 2020 Workshop Overview Paper. arXiv admin note: substantial\n  text overlap with arXiv:2009.09984", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The TREC Video Retrieval Evaluation (TRECVID) is a TREC-style video analysis\nand retrieval evaluation with the goal of promoting progress in research and\ndevelopment of content-based exploitation and retrieval of information from\ndigital video via open, metrics-based evaluation. Over the last twenty years\nthis effort has yielded a better understanding of how systems can effectively\naccomplish such processing and how one can reliably benchmark their\nperformance. TRECVID has been funded by NIST (National Institute of Standards\nand Technology) and other US government agencies. In addition, many\norganizations and individuals worldwide contribute significant time and effort.\nTRECVID 2020 represented a continuation of four tasks and the addition of two\nnew tasks. In total, 29 teams from various research organizations worldwide\ncompleted one or more of the following six tasks: 1. Ad-hoc Video Search (AVS),\n2. Instance Search (INS), 3. Disaster Scene Description and Indexing (DSDI), 4.\nVideo to Text Description (VTT), 5. Activities in Extended Video (ActEV), 6.\nVideo Summarization (VSUM). This paper is an introduction to the evaluation\nframework, tasks, data, and measures used in the evaluation campaign.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 20:59:27 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Awad", "George", ""], ["Butt", "Asad A.", ""], ["Curtis", "Keith", ""], ["Fiscus", "Jonathan", ""], ["Godil", "Afzal", ""], ["Lee", "Yooyoung", ""], ["Delgado", "Andrew", ""], ["Zhang", "Jesse", ""], ["Godard", "Eliot", ""], ["Chocot", "Baptiste", ""], ["Diduch", "Lukas", ""], ["Liu", "Jeffrey", ""], ["Smeaton", "Alan F.", ""], ["Graham", "Yvette", ""], ["Jones", "Gareth J. F.", ""], ["Kraaij", "Wessel", ""], ["Quenot", "Georges", ""]]}, {"id": "2104.13478", "submitter": "Petar Veli\\v{c}kovi\\'c", "authors": "Michael M. Bronstein, Joan Bruna, Taco Cohen, Petar Veli\\v{c}kovi\\'c", "title": "Geometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges", "comments": "156 pages. Work in progress -- comments welcome!", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The last decade has witnessed an experimental revolution in data science and\nmachine learning, epitomised by deep learning methods. Indeed, many\nhigh-dimensional learning tasks previously thought to be beyond reach -- such\nas computer vision, playing Go, or protein folding -- are in fact feasible with\nappropriate computational scale. Remarkably, the essence of deep learning is\nbuilt from two simple algorithmic principles: first, the notion of\nrepresentation or feature learning, whereby adapted, often hierarchical,\nfeatures capture the appropriate notion of regularity for each task, and\nsecond, learning by local gradient-descent type methods, typically implemented\nas backpropagation.\n  While learning generic functions in high dimensions is a cursed estimation\nproblem, most tasks of interest are not generic, and come with essential\npre-defined regularities arising from the underlying low-dimensionality and\nstructure of the physical world. This text is concerned with exposing these\nregularities through unified geometric principles that can be applied\nthroughout a wide spectrum of applications.\n  Such a 'geometric unification' endeavour, in the spirit of Felix Klein's\nErlangen Program, serves a dual purpose: on one hand, it provides a common\nmathematical framework to study the most successful neural network\narchitectures, such as CNNs, RNNs, GNNs, and Transformers. On the other hand,\nit gives a constructive procedure to incorporate prior physical knowledge into\nneural architectures and provide principled way to build future architectures\nyet to be invented.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 21:09:51 GMT"}, {"version": "v2", "created": "Sun, 2 May 2021 16:16:03 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Bronstein", "Michael M.", ""], ["Bruna", "Joan", ""], ["Cohen", "Taco", ""], ["Veli\u010dkovi\u0107", "Petar", ""]]}, {"id": "2104.13482", "submitter": "Andreas Mang", "authors": "Sorena Sarmadi, James J. Winkle, Razan N. Alnahhas, Matthew R.\n  Bennett, Kre\\v{s}imir Josi\\'c, Andreas Mang, and Robert Azencott", "title": "Stochastic Neural Networks for Automatic Cell Tracking in Microscopy\n  Image Sequences of Bacterial Colonies", "comments": "22 pages, 9 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe an automated analysis method to quantify the detailed growth\ndynamics of a population of bacilliform bacteria. We propose an innovative\napproach to frame-sequence tracking of deformable-cell motion by the automated\nminimization of a new, specific cost functional. This minimization is\nimplemented by dedicated Boltzmann machines (stochastic recurrent neural\nnetworks). Automated detection of cell divisions is handled similarly by\nsuccessive minimizations of two cost functions, alternating the identification\nof children pairs and parent identification. We validate this automatic cell\ntracking algorithm using recordings of simulated cell colonies that closely\nmimic the growth dynamics of \\emph{E. coli} in microfluidic traps. On a batch\nof 1100 image frames, cell registration accuracies per frame ranged from 94.5\\%\nto 100\\%, with a high average. Our initial tests using experimental image\nsequences of \\emph{E. coli} colonies also yield convincing results, with a\nregistration accuracy ranging from 90\\% to 100\\%.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 21:24:32 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Sarmadi", "Sorena", ""], ["Winkle", "James J.", ""], ["Alnahhas", "Razan N.", ""], ["Bennett", "Matthew R.", ""], ["Josi\u0107", "Kre\u0161imir", ""], ["Mang", "Andreas", ""], ["Azencott", "Robert", ""]]}, {"id": "2104.13486", "submitter": "Youshan Zhang", "authors": "Youshan Zhang and Brian D. Davison", "title": "Efficient Pre-trained Features and Recurrent Pseudo-Labeling in\n  Unsupervised Domain Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Domain adaptation (DA) mitigates the domain shift problem when transferring\nknowledge from one annotated domain to another similar but different unlabeled\ndomain. However, existing models often utilize one of the ImageNet models as\nthe backbone without exploring others, and fine-tuning or retraining the\nbackbone ImageNet model is also time-consuming. Moreover, pseudo-labeling has\nbeen used to improve the performance in the target domain, while how to\ngenerate confident pseudo labels and explicitly align domain distributions has\nnot been well addressed. In this paper, we show how to efficiently opt for the\nbest pre-trained features from seventeen well-known ImageNet models in\nunsupervised DA problems. In addition, we propose a recurrent pseudo-labeling\nmodel using the best pre-trained features (termed PRPL) to improve\nclassification performance. To show the effectiveness of PRPL, we evaluate it\non three benchmark datasets, Office+Caltech-10, Office-31, and Office-Home.\nExtensive experiments show that our model reduces computation time and boosts\nthe mean accuracy to 98.1%, 92.4%, and 81.2%, respectively, substantially\noutperforming the state of the art.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 21:35:28 GMT"}, {"version": "v2", "created": "Sat, 1 May 2021 15:46:31 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Zhang", "Youshan", ""], ["Davison", "Brian D.", ""]]}, {"id": "2104.13497", "submitter": "Haotian Yan", "authors": "Haotian Yan, Zhe Li, Weijian Li, Changhu Wang, Ming Wu, Chuang Zhang", "title": "ConTNet: Why not use convolution and transformer at the same time?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although convolutional networks (ConvNets) have enjoyed great success in\ncomputer vision (CV), it suffers from capturing global information crucial to\ndense prediction tasks such as object detection and segmentation. In this work,\nwe innovatively propose ConTNet (ConvolutionTransformer Network), combining\ntransformer with ConvNet architectures to provide large receptive fields.\nUnlike the recently-proposed transformer-based models (e.g., ViT, DeiT) that\nare sensitive to hyper-parameters and extremely dependent on a pile of data\naugmentations when trained from scratch on a midsize dataset (e.g.,\nImageNet1k), ConTNet can be optimized like normal ConvNets (e.g., ResNet) and\npreserve an outstanding robustness. It is also worth pointing that, given\nidentical strong data augmentations, the performance improvement of ConTNet is\nmore remarkable than that of ResNet. We present its superiority and\neffectiveness on image classification and downstream tasks. For example, our\nConTNet achieves 81.8% top-1 accuracy on ImageNet which is the same as DeiT-B\nwith less than 40% computational complexity. ConTNet-M also outperforms\nResNet50 as the backbone of both Faster-RCNN (by 2.6%) and Mask-RCNN (by 3.2%)\non COCO2017 dataset. We hope that ConTNet could serve as a useful backbone for\nCV tasks and bring new ideas for model design\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 22:29:55 GMT"}, {"version": "v2", "created": "Thu, 6 May 2021 20:37:49 GMT"}, {"version": "v3", "created": "Mon, 10 May 2021 18:39:48 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Yan", "Haotian", ""], ["Li", "Zhe", ""], ["Li", "Weijian", ""], ["Wang", "Changhu", ""], ["Wu", "Ming", ""], ["Zhang", "Chuang", ""]]}, {"id": "2104.13502", "submitter": "Umar Iqbal", "authors": "Umar Iqbal, Kevin Xie, Yunrong Guo, Jan Kautz, Pavlo Molchanov", "title": "KAMA: 3D Keypoint Aware Body Mesh Articulation", "comments": "\"Additional qualitative results: https://youtu.be/mPikZEIpUE0\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We present KAMA, a 3D Keypoint Aware Mesh Articulation approach that allows\nus to estimate a human body mesh from the positions of 3D body keypoints. To\nthis end, we learn to estimate 3D positions of 26 body keypoints and propose an\nanalytical solution to articulate a parametric body model, SMPL, via a set of\nstraightforward geometric transformations. Since keypoint estimation directly\nrelies on image clues, our approach offers significantly better alignment to\nimage content when compared to state-of-the-art approaches. Our proposed\napproach does not require any paired mesh annotations and is able to achieve\nstate-of-the-art mesh fittings through 3D keypoint regression only. Results on\nthe challenging 3DPW and Human3.6M demonstrate that our approach yields\nstate-of-the-art body mesh fittings.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 23:01:03 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Iqbal", "Umar", ""], ["Xie", "Kevin", ""], ["Guo", "Yunrong", ""], ["Kautz", "Jan", ""], ["Molchanov", "Pavlo", ""]]}, {"id": "2104.13526", "submitter": "Qiao Gu", "authors": "Brian Okorn, Qiao Gu, Martial Hebert, David Held", "title": "ZePHyR: Zero-shot Pose Hypothesis Rating", "comments": "8 pages, 4 figures. Accepted to ICRA 2021. Brian and Qiao have equal\n  contributions", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pose estimation is a basic module in many robot manipulation pipelines.\nEstimating the pose of objects in the environment can be useful for grasping,\nmotion planning, or manipulation. However, current state-of-the-art methods for\npose estimation either rely on large annotated training sets or simulated data.\nFurther, the long training times for these methods prohibit quick interaction\nwith novel objects. To address these issues, we introduce a novel method for\nzero-shot object pose estimation in clutter. Our approach uses a hypothesis\ngeneration and scoring framework, with a focus on learning a scoring function\nthat generalizes to objects not used for training. We achieve zero-shot\ngeneralization by rating hypotheses as a function of unordered point\ndifferences. We evaluate our method on challenging datasets with both textured\nand untextured objects in cluttered scenes and demonstrate that our method\nsignificantly outperforms previous methods on this task. We also demonstrate\nhow our system can be used by quickly scanning and building a model of a novel\nobject, which can immediately be used by our method for pose estimation. Our\nwork allows users to estimate the pose of novel objects without requiring any\nretraining. Additional information can be found on our website\nhttps://bokorn.github.io/zephyr/\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 01:48:39 GMT"}, {"version": "v2", "created": "Fri, 30 Apr 2021 04:11:27 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Okorn", "Brian", ""], ["Gu", "Qiao", ""], ["Hebert", "Martial", ""], ["Held", "David", ""]]}, {"id": "2104.13530", "submitter": "Ruojin Cai", "authors": "Ruojin Cai, Bharath Hariharan, Noah Snavely and Hadar Averbuch-Elor", "title": "Extreme Rotation Estimation using Dense Correlation Volumes", "comments": "Published in CVPR 2021; Project page:\n  https://ruojincai.github.io/ExtremeRotation/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a technique for estimating the relative 3D rotation of an RGB\nimage pair in an extreme setting, where the images have little or no overlap.\nWe observe that, even when images do not overlap, there may be rich hidden cues\nas to their geometric relationship, such as light source directions, vanishing\npoints, and symmetries present in the scene. We propose a network design that\ncan automatically learn such implicit cues by comparing all pairs of points\nbetween the two input images. Our method therefore constructs dense feature\ncorrelation volumes and processes these to predict relative 3D rotations. Our\npredictions are formed over a fine-grained discretization of rotations,\nbypassing difficulties associated with regressing 3D rotations. We demonstrate\nour approach on a large variety of extreme RGB image pairs, including indoor\nand outdoor images captured under different lighting conditions and geographic\nlocations. Our evaluation shows that our model can successfully estimate\nrelative rotations among non-overlapping images without compromising\nperformance over overlapping image pairs.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 02:00:04 GMT"}, {"version": "v2", "created": "Mon, 19 Jul 2021 05:00:22 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Cai", "Ruojin", ""], ["Hariharan", "Bharath", ""], ["Snavely", "Noah", ""], ["Averbuch-Elor", "Hadar", ""]]}, {"id": "2104.13534", "submitter": "Guanzhong Wang", "authors": "Ying Xin, Guanzhong Wang, Mingyuan Mao, Yuan Feng, Qingqing Dang,\n  Yanjun Ma, Errui Ding, Shumin Han", "title": "PAFNet: An Efficient Anchor-Free Object Detector Guidance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detection is a basic but challenging task in computer vision, which\nplays a key role in a variety of industrial applications. However, object\ndetectors based on deep learning usually require greater storage requirements\nand longer inference time, which hinders its practicality seriously. Therefore,\na trade-off between effectiveness and efficiency is necessary in practical\nscenarios. Considering that without constraint of pre-defined anchors,\nanchor-free detectors can achieve acceptable accuracy and inference speed\nsimultaneously. In this paper, we start from an anchor-free detector called\nTTFNet, modify the structure of TTFNet and introduce multiple existing tricks\nto realize effective server and mobile solutions respectively. Since all\nexperiments in this paper are conducted based on PaddlePaddle, we call the\nmodel as PAFNet(Paddle Anchor Free Network). For server side, PAFNet can\nachieve a better balance between effectiveness (42.2% mAP) and efficiency\n(67.15 FPS) on a single V100 GPU. For moblie side, PAFNet-lite can achieve a\nbetter accuracy of (23.9% mAP) and 26.00 ms on Kirin 990 ARM CPU, outperforming\nthe existing state-of-the-art anchor-free detectors by significant margins.\nSource code is at https://github.com/PaddlePaddle/PaddleDetection.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 02:32:54 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Xin", "Ying", ""], ["Wang", "Guanzhong", ""], ["Mao", "Mingyuan", ""], ["Feng", "Yuan", ""], ["Dang", "Qingqing", ""], ["Ma", "Yanjun", ""], ["Ding", "Errui", ""], ["Han", "Shumin", ""]]}, {"id": "2104.13537", "submitter": "Shixing Chen", "authors": "Shixing Chen, Xiaohan Nie, David Fan, Dongqing Zhang, Vimal Bhat,\n  Raffay Hamid", "title": "Shot Contrastive Self-Supervised Learning for Scene Boundary Detection", "comments": "Accepted to CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Scenes play a crucial role in breaking the storyline of movies and TV\nepisodes into semantically cohesive parts. However, given their complex\ntemporal structure, finding scene boundaries can be a challenging task\nrequiring large amounts of labeled training data. To address this challenge, we\npresent a self-supervised shot contrastive learning approach (ShotCoL) to learn\na shot representation that maximizes the similarity between nearby shots\ncompared to randomly selected shots. We show how to apply our learned shot\nrepresentation for the task of scene boundary detection to offer\nstate-of-the-art performance on the MovieNet dataset while requiring only ~25%\nof the training labels, using 9x fewer model parameters and offering 7x faster\nruntime. To assess the effectiveness of ShotCoL on novel applications of scene\nboundary detection, we take on the problem of finding timestamps in movies and\nTV episodes where video-ads can be inserted while offering a minimally\ndisruptive viewing experience. To this end, we collected a new dataset called\nAdCuepoints with 3,975 movies and TV episodes, 2.2 million shots and 19,119\nminimally disruptive ad cue-point labels. We present a thorough empirical\nanalysis on this dataset demonstrating the effectiveness of ShotCoL for ad\ncue-points detection.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 02:35:09 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Chen", "Shixing", ""], ["Nie", "Xiaohan", ""], ["Fan", "David", ""], ["Zhang", "Dongqing", ""], ["Bhat", "Vimal", ""], ["Hamid", "Raffay", ""]]}, {"id": "2104.13557", "submitter": "Shiba Kuanar", "authors": "Shiba Kuanar, Vassilis Athitsos, Dwarikanath Mahapatra, Anand Rajan", "title": "Multi-scale Deep Learning Architecture for Nucleus Detection in Renal\n  Cell Carcinoma Microscopy Image", "comments": "This article has been removed by arXiv administrators because the\n  submitter did not have the authority to grant the license applied at the time\n  of submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clear cell renal cell carcinoma (ccRCC) is one of the most common forms of\nintratumoral heterogeneity in the study of renal cancer. ccRCC originates from\nthe epithelial lining of proximal convoluted renal tubules. These cells undergo\nabnormal mutations in the presence of Ki67 protein and create a lump-like\nstructure through cell proliferation. Manual counting of tumor cells in the\ntissue-affected sections is one of the strongest prognostic markers for renal\ncancer. However, this procedure is time-consuming and also prone to\nsubjectivity. These assessments are based on the physical cell appearance and\nsuffer wide intra-observer variations. Therefore, better cell nucleus detection\nand counting techniques can be an important biomarker for the assessment of\ntumor cell proliferation in routine pathological investigations. In this paper,\nwe introduce a deep learning-based detection model for cell classification on\nIHC stained histology images. These images are classified into binary classes\nto find the presence of Ki67 protein in cancer-affected nucleus regions. Our\nmodel maps the multi-scale pyramid features and saliency information from local\nbounded regions and predicts the bounding box coordinates through regression.\nOur method validates the impact of Ki67 expression across a cohort of four\nhundred histology images treated with localized ccRCC and compares our results\nwith the existing state-of-the-art nucleus detection methods. The precision and\nrecall scores of the proposed method are computed and compared on the clinical\ndata sets. The experimental results demonstrate that our model improves the F1\nscore up to 86.3% and an average area under the Precision-Recall curve as\n85.73%.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 03:36:02 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Kuanar", "Shiba", ""], ["Athitsos", "Vassilis", ""], ["Mahapatra", "Dwarikanath", ""], ["Rajan", "Anand", ""]]}, {"id": "2104.13561", "submitter": "Byung Cheol Song", "authors": "Seunghyun Lee, Byung Cheol Song", "title": "Interpretable Embedding Procedure Knowledge Transfer via Stacked\n  Principal Component Analysis and Graph Neural Network", "comments": "accepted at AAAI2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge distillation (KD) is one of the most useful techniques for\nlight-weight neural networks. Although neural networks have a clear purpose of\nembedding datasets into the low-dimensional space, the existing knowledge was\nquite far from this purpose and provided only limited information. We argue\nthat good knowledge should be able to interpret the embedding procedure. This\npaper proposes a method of generating interpretable embedding procedure (IEP)\nknowledge based on principal component analysis, and distilling it based on a\nmessage passing neural network. Experimental results show that the student\nnetwork trained by the proposed KD method improves 2.28% in the CIFAR100\ndataset, which is higher performance than the state-of-the-art (SOTA) method.\nWe also demonstrate that the embedding procedure knowledge is interpretable via\nvisualization of the proposed KD process. The implemented code is available at\nhttps://github.com/sseung0703/IEPKT.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 03:40:37 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Lee", "Seunghyun", ""], ["Song", "Byung Cheol", ""]]}, {"id": "2104.13562", "submitter": "Julian Knodt", "authors": "Julian Knodt, Seung-Hwan Baek, Felix Heide", "title": "Neural Ray-Tracing: Learning Surfaces and Reflectance for Relighting and\n  View Synthesis", "comments": "10 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent neural rendering methods have demonstrated accurate view interpolation\nby predicting volumetric density and color with a neural network. Although such\nvolumetric representations can be supervised on static and dynamic scenes,\nexisting methods implicitly bake the complete scene light transport into a\nsingle neural network for a given scene, including surface modeling,\nbidirectional scattering distribution functions, and indirect lighting effects.\nIn contrast to traditional rendering pipelines, this prohibits changing surface\nreflectance, illumination, or composing other objects in the scene.\n  In this work, we explicitly model the light transport between scene surfaces\nand we rely on traditional integration schemes and the rendering equation to\nreconstruct a scene. The proposed method allows BSDF recovery with unknown\nlight conditions and classic light transports such as pathtracing. By learning\ndecomposed transport with surface representations established in conventional\nrendering methods, the method naturally facilitates editing shape, reflectance,\nlighting and scene composition. The method outperforms NeRV for relighting\nunder known lighting conditions, and produces realistic reconstructions for\nrelit and edited scenes. We validate the proposed approach for scene editing,\nrelighting and reflectance estimation learned from synthetic and captured views\non a subset of NeRV's datasets.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 03:47:48 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Knodt", "Julian", ""], ["Baek", "Seung-Hwan", ""], ["Heide", "Felix", ""]]}, {"id": "2104.13568", "submitter": "Youngtaek Kim", "authors": "Youngtaek Kim, Hyeon Jeon, Kiroong Choe, Hyunjoo Song, Bohyoung Kim,\n  Jinwook Seo", "title": "Interactive Visualization for Exploring Information Fragments in\n  Software Repositories", "comments": "14th IEEE Pacific Visualization Symposium (PacificVis '21) Poster", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Software developers explore and inspect software repository data to obtain\ndetailed information archived in the development history. However, developers\nwho are not acquainted with the development context suffer from delving into\nthe repositories with a handful of information; they have difficulty\ndiscovering and expanding information fragments considering the topological and\nsequential multi-dimensional structure of repositories. We introduce ExIF, an\ninteractive visualization for exploring information fragments in software\nrepositories. ExIF helps users discover new information fragments within\nclusters or topological neighbors and identify revisions incorporating\nuser-collected fragments.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 04:57:18 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Kim", "Youngtaek", ""], ["Jeon", "Hyeon", ""], ["Choe", "Kiroong", ""], ["Song", "Hyunjoo", ""], ["Kim", "Bohyoung", ""], ["Seo", "Jinwook", ""]]}, {"id": "2104.13581", "submitter": "Mohammad Mahfujur Rahman", "authors": "Mohammad Mahfujur Rahman, Clinton Fookes, Sridha Sridharan", "title": "Deep Domain Generalization with Feature-norm Network", "comments": "Submitted to Pattern Recognition", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In this paper, we tackle the problem of training with multiple source domains\nwith the aim to generalize to new domains at test time without an adaptation\nstep. This is known as domain generalization (DG). Previous works on DG assume\nidentical categories or label space across the source domains. In the case of\ncategory shift among the source domains, previous methods on DG are vulnerable\nto negative transfer due to the large mismatch among label spaces, decreasing\nthe target classification accuracy. To tackle the aforementioned problem, we\nintroduce an end-to-end feature-norm network (FNN) which is robust to negative\ntransfer as it does not need to match the feature distribution among the source\ndomains. We also introduce a collaborative feature-norm network (CFNN) to\nfurther improve the generalization capability of FNN. The CFNN matches the\npredictions of the next most likely categories for each training sample which\nincreases each network's posterior entropy. We apply the proposed FNN and CFNN\nnetworks to the problem of DG for image classification tasks and demonstrate\nsignificant improvement over the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 06:13:47 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Rahman", "Mohammad Mahfujur", ""], ["Fookes", "Clinton", ""], ["Sridharan", "Sridha", ""]]}, {"id": "2104.13582", "submitter": "Sunnie S. Y. Kim", "authors": "Sunnie S. Y. Kim, Sharon Zhang, Nicole Meister, Olga Russakovsky", "title": "[Re] Don't Judge an Object by Its Context: Learning to Overcome\n  Contextual Bias", "comments": "ML Reproducibility Challenge 2020. Accepted for publication in the\n  ReScience C journal", "journal-ref": null, "doi": "10.5281/zenodo.4834352", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Singh et al. (2020) point out the dangers of contextual bias in visual\nrecognition datasets. They propose two methods, CAM-based and feature-split,\nthat better recognize an object or attribute in the absence of its typical\ncontext while maintaining competitive within-context accuracy. To verify their\nperformance, we attempted to reproduce all 12 tables in the original paper,\nincluding those in the appendix. We also conducted additional experiments to\nbetter understand the proposed methods, including increasing the regularization\nin CAM-based and removing the weighted loss in feature-split. As the original\ncode was not made available, we implemented the entire pipeline from scratch in\nPyTorch 1.7.0. Our implementation is based on the paper and email exchanges\nwith the authors. We found that both proposed methods in the original paper\nhelp mitigate contextual bias, although for some methods, we could not\ncompletely replicate the quantitative results in the paper even after\ncompleting an extensive hyperparameter search. For example, on COCO-Stuff,\nDeepFashion, and UnRel, our feature-split model achieved an increase in\naccuracy on out-of-context images over the standard baseline, whereas on AwA,\nwe saw a drop in performance. For the proposed CAM-based method, we were able\nto reproduce the original paper's results to within 0.5$\\%$ mAP. Our\nimplementation can be found at\nhttps://github.com/princetonvisualai/ContextualBias.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 06:21:28 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Kim", "Sunnie S. Y.", ""], ["Zhang", "Sharon", ""], ["Meister", "Nicole", ""], ["Russakovsky", "Olga", ""]]}, {"id": "2104.13586", "submitter": "Haodong Duan", "authors": "Haodong Duan, Yue Zhao, Kai Chen, Dian Shao, Dahua Lin, Bo Dai", "title": "Revisiting Skeleton-based Action Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human skeleton, as a compact representation of human action, has received\nincreasing attention in recent years. Many skeleton-based action recognition\nmethods adopt graph convolutional networks (GCN) to extract features on top of\nhuman skeletons. Despite the positive results shown in previous works,\nGCN-based methods are subject to limitations in robustness, interoperability,\nand scalability. In this work, we propose PoseC3D, a new approach to\nskeleton-based action recognition, which relies on a 3D heatmap stack instead\nof a graph sequence as the base representation of human skeletons. Compared to\nGCN-based methods, PoseC3D is more effective in learning spatiotemporal\nfeatures, more robust against pose estimation noises, and generalizes better in\ncross-dataset settings. Also, PoseC3D can handle multiple-person scenarios\nwithout additional computation cost, and its features can be easily integrated\nwith other modalities at early fusion stages, which provides a great design\nspace to further boost the performance. On four challenging datasets, PoseC3D\nconsistently obtains superior performance, when used alone on skeletons and in\ncombination with the RGB modality.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 06:32:17 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Duan", "Haodong", ""], ["Zhao", "Yue", ""], ["Chen", "Kai", ""], ["Shao", "Dian", ""], ["Lin", "Dahua", ""], ["Dai", "Bo", ""]]}, {"id": "2104.13602", "submitter": "Yongjie Zhu", "authors": "Yongjie Zhu, Jiajun Tang, Si Li, and Boxin Shi", "title": "DeRenderNet: Intrinsic Image Decomposition of Urban Scenes with\n  Shape-(In)dependent Shading Rendering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose DeRenderNet, a deep neural network to decompose the albedo and\nlatent lighting, and render shape-(in)dependent shadings, given a single image\nof an outdoor urban scene, trained in a self-supervised manner. To achieve this\ngoal, we propose to use the albedo maps extracted from scenes in videogames as\ndirect supervision and pre-compute the normal and shadow prior maps based on\nthe depth maps provided as indirect supervision. Compared with state-of-the-art\nintrinsic image decomposition methods, DeRenderNet produces shadow-free albedo\nmaps with clean details and an accurate prediction of shadows in the\nshape-independent shading, which is shown to be effective in re-rendering and\nimproving the accuracy of high-level vision tasks for urban scenes.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 07:22:38 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Zhu", "Yongjie", ""], ["Tang", "Jiajun", ""], ["Li", "Si", ""], ["Shi", "Boxin", ""]]}, {"id": "2104.13613", "submitter": "Qin Wang", "authors": "Qin Wang, Dengxin Dai, Lukas Hoyer, Olga Fink, Luc Van Gool", "title": "Domain Adaptive Semantic Segmentation with Self-Supervised Depth\n  Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain adaptation for semantic segmentation aims to improve the model\nperformance in the presence of a distribution shift between source and target\ndomain. Leveraging the supervision from auxiliary tasks~(such as depth\nestimation) has the potential to heal this shift because many visual tasks are\nclosely related to each other. However, such a supervision is not always\navailable. In this work, we leverage the guidance from self-supervised depth\nestimation, which is available on both domains, to bridge the domain gap. On\nthe one hand, we propose to explicitly learn the task feature correlation to\nstrengthen the target semantic predictions with the help of target depth\nestimation. On the other hand, we use the depth prediction discrepancy from\nsource and target depth decoders to approximate the pixel-wise adaptation\ndifficulty. The adaptation difficulty, inferred from depth, is then used to\nrefine the target semantic segmentation pseudo-labels. The proposed method can\nbe easily implemented into existing segmentation frameworks. We demonstrate the\neffectiveness of our proposed approach on the benchmark tasks\nSYNTHIA-to-Cityscapes and GTA-to-Cityscapes, on which we achieve the new\nstate-of-the-art performance of $55.0\\%$ and $56.6\\%$, respectively. Our code\nis available at \\url{https://github.com/qinenergy/corda}.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 07:47:36 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Wang", "Qin", ""], ["Dai", "Dengxin", ""], ["Hoyer", "Lukas", ""], ["Fink", "Olga", ""], ["Van Gool", "Luc", ""]]}, {"id": "2104.13614", "submitter": "Changhong Zhong", "authors": "Zhuoyun Li, Changhong Zhong, Sijia Liu, Ruixuan Wang, and Wei-Shi\n  Zheng", "title": "Preserving Earlier Knowledge in Continual Learning with the Help of All\n  Previous Feature Extractors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continual learning of new knowledge over time is one desirable capability for\nintelligent systems to recognize more and more classes of objects. Without or\nwith very limited amount of old data stored, an intelligent system often\ncatastrophically forgets previously learned old knowledge when learning new\nknowledge. Recently, various approaches have been proposed to alleviate the\ncatastrophic forgetting issue. However, old knowledge learned earlier is\ncommonly less preserved than that learned more recently. In order to reduce the\nforgetting of particularly earlier learned old knowledge and improve the\noverall continual learning performance, we propose a simple yet effective\nfusion mechanism by including all the previously learned feature extractors\ninto the intelligent model. In addition, a new feature extractor is included to\nthe model when learning a new set of classes each time, and a feature extractor\npruning is also applied to prevent the whole model size from growing rapidly.\nExperiments on multiple classification tasks show that the proposed approach\ncan effectively reduce the forgetting of old knowledge, achieving\nstate-of-the-art continual learning performance.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 07:49:24 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Li", "Zhuoyun", ""], ["Zhong", "Changhong", ""], ["Liu", "Sijia", ""], ["Wang", "Ruixuan", ""], ["Zheng", "Wei-Shi", ""]]}, {"id": "2104.13633", "submitter": "Eunji Jun", "authors": "Eunji Jun, Seungwoo Jeong, Da-Woon Heo, Heung-Il Suk", "title": "Medical Transformer: Universal Brain Encoder for 3D MRI Analysis", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transfer learning has gained attention in medical image analysis due to\nlimited annotated 3D medical datasets for training data-driven deep learning\nmodels in the real world. Existing 3D-based methods have transferred the\npre-trained models to downstream tasks, which achieved promising results with\nonly a small number of training samples. However, they demand a massive amount\nof parameters to train the model for 3D medical imaging. In this work, we\npropose a novel transfer learning framework, called Medical Transformer, that\neffectively models 3D volumetric images in the form of a sequence of 2D image\nslices. To make a high-level representation in 3D-form empowering spatial\nrelations better, we take a multi-view approach that leverages plenty of\ninformation from the three planes of 3D volume, while providing\nparameter-efficient training. For building a source model generally applicable\nto various tasks, we pre-train the model in a self-supervised learning manner\nfor masked encoding vector prediction as a proxy task, using a large-scale\nnormal, healthy brain magnetic resonance imaging (MRI) dataset. Our pre-trained\nmodel is evaluated on three downstream tasks: (i) brain disease diagnosis, (ii)\nbrain age prediction, and (iii) brain tumor segmentation, which are actively\nstudied in brain MRI research. The experimental results show that our Medical\nTransformer outperforms the state-of-the-art transfer learning methods,\nefficiently reducing the number of parameters up to about 92% for\nclassification and\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 08:34:21 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Jun", "Eunji", ""], ["Jeong", "Seungwoo", ""], ["Heo", "Da-Woon", ""], ["Suk", "Heung-Il", ""]]}, {"id": "2104.13634", "submitter": "Hassan Noura", "authors": "Rapha\\\"el Couturier, Hassan N. Noura, Ola Salman, Abderrahmane Sider", "title": "A Deep Learning Object Detection Method for an Efficient Clusters\n  Initialization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering is an unsupervised machine learning method grouping data samples\ninto clusters of similar objects. In practice, clustering has been used in\nnumerous applications such as banking customers profiling, document retrieval,\nimage segmentation, and e-commerce recommendation engines. However, the\nexisting clustering techniques present significant limitations, from which is\nthe dependability of their stability on the initialization parameters (e.g.\nnumber of clusters, centroids). Different solutions were presented in the\nliterature to overcome this limitation (i.e. internal and external validation\nmetrics). However, these solutions require high computational complexity and\nmemory consumption, especially when dealing with big data. In this paper, we\napply the recent object detection Deep Learning (DL) model, named YOLO-v5, to\ndetect the initial clustering parameters such as the number of clusters with\ntheir sizes and centroids. Mainly, the proposed solution consists of adding a\nDL-based initialization phase making the clustering algorithms free of\ninitialization. Two model solutions are provided in this work, one for isolated\nclusters and the other one for overlapping clusters. The features of the\nincoming dataset determine which model to use. Moreover, The results show that\nthe proposed solution can provide near-optimal clusters initialization\nparameters with low computational and resources overhead compared to existing\nsolutions.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 08:34:25 GMT"}, {"version": "v2", "created": "Thu, 29 Apr 2021 07:03:27 GMT"}, {"version": "v3", "created": "Sun, 4 Jul 2021 18:43:43 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Couturier", "Rapha\u00ebl", ""], ["Noura", "Hassan N.", ""], ["Salman", "Ola", ""], ["Sider", "Abderrahmane", ""]]}, {"id": "2104.13636", "submitter": "Xian-Feng Han", "authors": "Xian-Feng Han, Yu-Jia Kuang, Guo-Qiang Xiao", "title": "Point Cloud Learning with Transformer", "comments": "10 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Remarkable performance from Transformer networks in Natural Language\nProcessing promote the development of these models in dealing with computer\nvision tasks such as image recognition and segmentation. In this paper, we\nintroduce a novel framework, called Multi-level Multi-scale Point Transformer\n(MLMSPT) that works directly on the irregular point clouds for representation\nlearning. Specifically, a point pyramid transformer is investigated to model\nfeatures with diverse resolutions or scales we defined, followed by a\nmulti-level transformer module to aggregate contextual information from\ndifferent levels of each scale and enhance their interactions. While a\nmulti-scale transformer module is designed to capture the dependencies among\nrepresentations across different scales. Extensive evaluation on public\nbenchmark datasets demonstrate the effectiveness and the competitive\nperformance of our methods on 3D shape classification, part segmentation and\nsemantic segmentation tasks.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 08:39:21 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Han", "Xian-Feng", ""], ["Kuang", "Yu-Jia", ""], ["Xiao", "Guo-Qiang", ""]]}, {"id": "2104.13643", "submitter": "Mikolaj Wieczorek", "authors": "Mikolaj Wieczorek, Barbara Rychalska, Jacek Dabrowski", "title": "On the Unreasonable Effectiveness of Centroids in Image Retrieval", "comments": "5 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image retrieval task consists of finding similar images to a query image from\na set of gallery (database) images. Such systems are used in various\napplications e.g. person re-identification (ReID) or visual product search.\nDespite active development of retrieval models it still remains a challenging\ntask mainly due to large intra-class variance caused by changes in view angle,\nlighting, background clutter or occlusion, while inter-class variance may be\nrelatively low. A large portion of current research focuses on creating more\nrobust features and modifying objective functions, usually based on Triplet\nLoss. Some works experiment with using centroid/proxy representation of a class\nto alleviate problems with computing speed and hard samples mining used with\nTriplet Loss. However, these approaches are used for training alone and\ndiscarded during the retrieval stage. In this paper we propose to use the mean\ncentroid representation both during training and retrieval. Such an aggregated\nrepresentation is more robust to outliers and assures more stable features. As\neach class is represented by a single embedding - the class centroid - both\nretrieval time and storage requirements are reduced significantly. Aggregating\nmultiple embeddings results in a significant reduction of the search space due\nto lowering the number of candidate target vectors, which makes the method\nespecially suitable for production deployments. Comprehensive experiments\nconducted on two ReID and Fashion Retrieval datasets demonstrate effectiveness\nof our method, which outperforms the current state-of-the-art. We propose\ncentroid training and retrieval as a viable method for both Fashion Retrieval\nand ReID applications.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 08:57:57 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Wieczorek", "Mikolaj", ""], ["Rychalska", "Barbara", ""], ["Dabrowski", "Jacek", ""]]}, {"id": "2104.13648", "submitter": "Fei Chen", "authors": "Fei Chen and Xiaodong Wang", "title": "Two stages for visual object tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Siamese-based trackers have achived promising performance on visual object\ntracking tasks. Most existing Siamese-based trackers contain two separate\nbranches for tracking, including classification branch and bounding box\nregression branch. In addition, image segmentation provides an alternative way\nto obetain the more accurate target region. In this paper, we propose a novel\ntracker with two-stages: detection and segmentation. The detection stage is\ncapable of locating the target by Siamese networks. Then more accurate tracking\nresults are obtained by segmentation module given the coarse state estimation\nin the first stage. We conduct experiments on four benchmarks. Our approach\nachieves state-of-the-art results, with the EAO of 52.6$\\%$ on VOT2016,\n51.3$\\%$ on VOT2018, and 39.0$\\%$ on VOT2019 datasets, respectively.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 09:11:33 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Chen", "Fei", ""], ["Wang", "Xiaodong", ""]]}, {"id": "2104.13665", "submitter": "Weinan Guan", "authors": "Weinan Guan, Wei Wang, Jing Dong, Bo Peng and Tieniu Tan", "title": "Robust Face-Swap Detection Based on 3D Facial Shape Information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Maliciously-manipulated images or videos - so-called deep fakes - especially\nface-swap images and videos have attracted more and more malicious attackers to\ndiscredit some key figures. Previous pixel-level artifacts based detection\ntechniques always focus on some unclear patterns but ignore some available\nsemantic clues. Therefore, these approaches show weak interpretability and\nrobustness. In this paper, we propose a biometric information based method to\nfully exploit the appearance and shape feature for face-swap detection of key\nfigures. The key aspect of our method is obtaining the inconsistency of 3D\nfacial shape and facial appearance, and the inconsistency based clue offers\nnatural interpretability for the proposed face-swap detection method.\nExperimental results show the superiority of our method in robustness on\nvarious laundering and cross-domain data, which validates the effectiveness of\nthe proposed method.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 09:35:48 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Guan", "Weinan", ""], ["Wang", "Wei", ""], ["Dong", "Jing", ""], ["Peng", "Bo", ""], ["Tan", "Tieniu", ""]]}, {"id": "2104.13666", "submitter": "Abbas Cheddad", "authors": "Mengqiao Zhao, Andre G. Hochuli, Abbas Cheddad", "title": "End-to-End Approach for Recognition of Historical Digit Strings", "comments": "Cite as: Mengqiao Zhao, Andre G. Hochuli and Abbas Cheddad,\n  End-to-End Approach for Recognition of Historical Digit Strings, to appear in\n  the 16th International Conference on Document Analysis and Recognition (ICDAR\n  2021), LNCS, Springer, Lausanne, Switzerland", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The plethora of digitalised historical document datasets released in recent\nyears has rekindled interest in advancing the field of handwriting pattern\nrecognition. In the same vein, a recently published data set, known as ARDIS,\npresents handwritten digits manually cropped from 15.000 scanned documents of\nSwedish church books and exhibiting various handwriting styles. To this end, we\npropose an end-to-end segmentation-free deep learning approach to handle this\nchallenging ancient handwriting style of dates present in the ARDIS dataset\n(4-digits long strings). We show that with slight modifications in the VGG-16\ndeep model, the framework can achieve a recognition rate of 93.2%, resulting in\na feasible solution free of heuristic methods, segmentation, and fusion\nmethods. Moreover, the proposed approach outperforms the well-known CRNN method\n(a model widely applied in handwriting recognition tasks).\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 09:39:29 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Zhao", "Mengqiao", ""], ["Hochuli", "Andre G.", ""], ["Cheddad", "Abbas", ""]]}, {"id": "2104.13673", "submitter": "Ruijun Gao", "authors": "Ruijun Gao, Qing Guo, Felix Juefei-Xu, Hongkai Yu, Wei Feng", "title": "AdvHaze: Adversarial Haze Attack", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, adversarial attacks have drawn more attention for their\nvalue on evaluating and improving the robustness of machine learning models,\nespecially, neural network models. However, previous attack methods have mainly\nfocused on applying some $l^p$ norm-bounded noise perturbations. In this paper,\nwe instead introduce a novel adversarial attack method based on haze, which is\na common phenomenon in real-world scenery. Our method can synthesize\npotentially adversarial haze into an image based on the atmospheric scattering\nmodel with high realisticity and mislead classifiers to predict an incorrect\nclass. We launch experiments on two popular datasets, i.e., ImageNet and\nNIPS~2017. We demonstrate that the proposed method achieves a high success\nrate, and holds better transferability across different classification models\nthan the baselines. We also visualize the correlation matrices, which inspire\nus to jointly apply different perturbations to improve the success rate of the\nattack. We hope this work can boost the development of non-noise-based\nadversarial attacks and help evaluate and improve the robustness of DNNs.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 09:52:25 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Gao", "Ruijun", ""], ["Guo", "Qing", ""], ["Juefei-Xu", "Felix", ""], ["Yu", "Hongkai", ""], ["Feng", "Wei", ""]]}, {"id": "2104.13682", "submitter": "Bumsoo Kim", "authors": "Bumsoo Kim, Junhyun Lee, Jaewoo Kang, Eun-Sol Kim, Hyunwoo J. Kim", "title": "HOTR: End-to-End Human-Object Interaction Detection with Transformers", "comments": "Accepted to CVPR 2021 (Oral Presentation)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Human-Object Interaction (HOI) detection is a task of identifying \"a set of\ninteractions\" in an image, which involves the i) localization of the subject\n(i.e., humans) and target (i.e., objects) of interaction, and ii) the\nclassification of the interaction labels. Most existing methods have indirectly\naddressed this task by detecting human and object instances and individually\ninferring every pair of the detected instances. In this paper, we present a\nnovel framework, referred to by HOTR, which directly predicts a set of <human,\nobject, interaction> triplets from an image based on a transformer\nencoder-decoder architecture. Through the set prediction, our method\neffectively exploits the inherent semantic relationships in an image and does\nnot require time-consuming post-processing which is the main bottleneck of\nexisting methods. Our proposed algorithm achieves the state-of-the-art\nperformance in two HOI detection benchmarks with an inference time under 1 ms\nafter object detection.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 10:10:29 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Kim", "Bumsoo", ""], ["Lee", "Junhyun", ""], ["Kang", "Jaewoo", ""], ["Kim", "Eun-Sol", ""], ["Kim", "Hyunwoo J.", ""]]}, {"id": "2104.13702", "submitter": "Jack Barker", "authors": "Jack W. Barker and Toby P. Breckon", "title": "PANDA : Perceptually Aware Neural Detection of Anomalies", "comments": "In-proceedings at 2021 International Joint Conference on Neural\n  Networks (IJCNN)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Semi-supervised methods of anomaly detection have seen substantial\nadvancement in recent years. Of particular interest are applications of such\nmethods to diverse, real-world anomaly detection problems where anomalous\nvariations can vary from the visually obvious to the very subtle. In this work,\nwe propose a novel fine-grained VAE-GAN architecture trained in a\nsemi-supervised manner in order to detect both visually distinct and subtle\nanomalies. With the use of a residually connected dual-feature extractor, a\nfine-grained discriminator and a perceptual loss function, we are able to\ndetect subtle, low inter-class (anomaly vs. normal) variant anomalies with\ngreater detection capability and smaller margins of deviation in AUC value\nduring inference compared to prior work whilst also remaining time-efficient\nduring inference. We achieve state of-the-art anomaly detection results when\ncompared extensively with prior semi-supervised approaches across a multitude\nof anomaly detection benchmark tasks including trivial leave-one out tasks\n(CIFAR-10 - AUPRCavg: 0.91; MNIST - AUPRCavg: 0.90) in addition to challenging\nreal-world anomaly detection tasks (plant leaf disease - AUC: 0.776; threat\nitem X-ray - AUC: 0.51), video frame-level anomaly detection (UCSDPed1 - AUC:\n0.95) and high frequency texture with object anomalous defect detection (MVTEC\n- AUCavg: 0.83).\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 11:03:50 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Barker", "Jack W.", ""], ["Breckon", "Toby P.", ""]]}, {"id": "2104.13710", "submitter": "Mohammed Daoudi", "authors": "Oussema Bouafif, Bogdan Khomutenko, Mohamed Daoudi", "title": "Hybrid Approach for 3D Head Reconstruction: Using Neural Networks and\n  Visual Geometry", "comments": "25th International Conference On Pattern Recognition 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recovering the 3D geometric structure of a face from a single input image is\na challenging active research area in computer vision. In this paper, we\npresent a novel method for reconstructing 3D heads from a single or multiple\nimage(s) using a hybrid approach based on deep learning and geometric\ntechniques. We propose an encoder-decoder network based on the U-net\narchitecture and trained on synthetic data only. It predicts both pixel-wise\nnormal vectors and landmarks maps from a single input photo. Landmarks are used\nfor the pose computation and the initialization of the optimization problem,\nwhich, in turn, reconstructs the 3D head geometry by using a parametric\nmorphable model and normal vector fields. State-of-the-art results are achieved\nthrough qualitative and quantitative evaluation tests on both single and\nmulti-view settings. Despite the fact that the model was trained only on\nsynthetic data, it successfully recovers 3D geometry and precise poses for\nreal-world images.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 11:31:35 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Bouafif", "Oussema", ""], ["Khomutenko", "Bogdan", ""], ["Daoudi", "Mohamed", ""]]}, {"id": "2104.13714", "submitter": "Radoslaw Martin Cichy", "authors": "R.M. Cichy, K. Dwivedi, B. Lahner, A. Lascelles, P. Iamshchinina, M.\n  Graumann, A. Andonian, N.A.R. Murty, K. Kay, G. Roig, A. Oliva", "title": "The Algonauts Project 2021 Challenge: How the Human Brain Makes Sense of\n  a World in Motion", "comments": "5 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The sciences of natural and artificial intelligence are fundamentally\nconnected. Brain-inspired human-engineered AI are now the standard for\npredicting human brain responses during vision, and conversely, the brain\ncontinues to inspire invention in AI. To promote even deeper connections\nbetween these fields, we here release the 2021 edition of the Algonauts Project\nChallenge: How the Human Brain Makes Sense of a World in Motion\n(http://algonauts.csail.mit.edu/). We provide whole-brain fMRI responses\nrecorded while 10 human participants viewed a rich set of over 1,000 short\nvideo clips depicting everyday events. The goal of the challenge is to\naccurately predict brain responses to these video clips. The format of our\nchallenge ensures rapid development, makes results directly comparable and\ntransparent, and is open to all. In this way it facilitates interdisciplinary\ncollaboration towards a common goal of understanding visual intelligence. The\n2021 Algonauts Project is conducted in collaboration with the Cognitive\nComputational Neuroscience (CCN) conference.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 11:38:31 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Cichy", "R. M.", ""], ["Dwivedi", "K.", ""], ["Lahner", "B.", ""], ["Lascelles", "A.", ""], ["Iamshchinina", "P.", ""], ["Graumann", "M.", ""], ["Andonian", "A.", ""], ["Murty", "N. A. R.", ""], ["Kay", "K.", ""], ["Roig", "G.", ""], ["Oliva", "A.", ""]]}, {"id": "2104.13725", "submitter": "Mohammad Mahfujur Rahman", "authors": "Mohammad Mahfujur Rahman, Clinton Fookes, Sridha Sridharan", "title": "Preserving Semantic Consistency in Unsupervised Domain Adaptation Using\n  Generative Adversarial Networks", "comments": "Submitted to Pattern Recognition Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Unsupervised domain adaptation seeks to mitigate the distribution discrepancy\nbetween source and target domains, given labeled samples of the source domain\nand unlabeled samples of the target domain. Generative adversarial networks\n(GANs) have demonstrated significant improvement in domain adaptation by\nproducing images which are domain specific for training. However, most of the\nexisting GAN based techniques for unsupervised domain adaptation do not\nconsider semantic information during domain matching, hence these methods\ndegrade the performance when the source and target domain data are semantically\ndifferent. In this paper, we propose an end-to-end novel semantic consistent\ngenerative adversarial network (SCGAN). This network can achieve source to\ntarget domain matching by capturing semantic information at the feature level\nand producing images for unsupervised domain adaptation from both the source\nand the target domains. We demonstrate the robustness of our proposed method\nwhich exceeds the state-of-the-art performance in unsupervised domain\nadaptation settings by performing experiments on digit and object\nclassification tasks.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 12:23:30 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Rahman", "Mohammad Mahfujur", ""], ["Fookes", "Clinton", ""], ["Sridharan", "Sridha", ""]]}, {"id": "2104.13742", "submitter": "Yaxing Wang", "authors": "Yaxing Wang, Abel Gonzalez-Garcia, Chenshen Wu, Luis Herranz, Fahad\n  Shahbaz Khan, Shangling Jui and Joost van de Weijer", "title": "MineGAN++: Mining Generative Models for Efficient Knowledge Transfer to\n  Limited Data Domains", "comments": "Technical report. arXiv admin note: substantial text overlap with\n  arXiv:1912.05270", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  GANs largely increases the potential impact of generative models. Therefore,\nwe propose a novel knowledge transfer method for generative models based on\nmining the knowledge that is most beneficial to a specific target domain,\neither from a single or multiple pretrained GANs. This is done using a miner\nnetwork that identifies which part of the generative distribution of each\npretrained GAN outputs samples closest to the target domain. Mining effectively\nsteers GAN sampling towards suitable regions of the latent space, which\nfacilitates the posterior finetuning and avoids pathologies of other methods,\nsuch as mode collapse and lack of flexibility. Furthermore, to prevent\noverfitting on small target domains, we introduce sparse subnetwork selection,\nthat restricts the set of trainable neurons to those that are relevant for the\ntarget dataset. We perform comprehensive experiments on several challenging\ndatasets using various GAN architectures (BigGAN, Progressive GAN, and\nStyleGAN) and show that the proposed method, called MineGAN, effectively\ntransfers knowledge to domains with few target images, outperforming existing\nmethods. In addition, MineGAN can successfully transfer knowledge from multiple\npretrained GANs.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 13:10:56 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Wang", "Yaxing", ""], ["Gonzalez-Garcia", "Abel", ""], ["Wu", "Chenshen", ""], ["Herranz", "Luis", ""], ["Khan", "Fahad Shahbaz", ""], ["Jui", "Shangling", ""], ["van de Weijer", "Joost", ""]]}, {"id": "2104.13743", "submitter": "Dongliang He", "authors": "Manyu Zhu, Dongliang He, Xin Li, Chao Li, Fu Li, Xiao Liu, Errui Ding\n  and Zhaoxiang Zhang", "title": "Image Inpainting by End-to-End Cascaded Refinement with Mask Awareness", "comments": "IEEE TIP, to appear", "journal-ref": null, "doi": "10.1109/TIP.2021.3076310", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inpainting arbitrary missing regions is challenging because learning valid\nfeatures for various masked regions is nontrivial. Though U-shaped\nencoder-decoder frameworks have been witnessed to be successful, most of them\nshare a common drawback of mask unawareness in feature extraction because all\nconvolution windows (or regions), including those with various shapes of\nmissing pixels, are treated equally and filtered with fixed learned kernels. To\nthis end, we propose our novel mask-aware inpainting solution. Firstly, a\nMask-Aware Dynamic Filtering (MADF) module is designed to effectively learn\nmulti-scale features for missing regions in the encoding phase. Specifically,\nfilters for each convolution window are generated from features of the\ncorresponding region of the mask. The second fold of mask awareness is achieved\nby adopting Point-wise Normalization (PN) in our decoding phase, considering\nthat statistical natures of features at masked points differentiate from those\nof unmasked points. The proposed PN can tackle this issue by dynamically\nassigning point-wise scaling factor and bias. Lastly, our model is designed to\nbe an end-to-end cascaded refinement one. Supervision information such as\nreconstruction loss, perceptual loss and total variation loss is incrementally\nleveraged to boost the inpainting results from coarse to fine. Effectiveness of\nthe proposed framework is validated both quantitatively and qualitatively via\nextensive experiments on three public datasets including Places2, CelebA and\nParis StreetView.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 13:17:47 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Zhu", "Manyu", ""], ["He", "Dongliang", ""], ["Li", "Xin", ""], ["Li", "Chao", ""], ["Li", "Fu", ""], ["Liu", "Xiao", ""], ["Ding", "Errui", ""], ["Zhang", "Zhaoxiang", ""]]}, {"id": "2104.13763", "submitter": "Xin Yi", "authors": "Xin Yi, Jiahao Wu, Bo Ma, Yangtong Ou, Longyao Liu", "title": "LGA-RCNN: Loss-Guided Attention for Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detection is widely studied in computer vision filed. In recent years,\ncertain representative deep learning based detection methods along with solid\nbenchmarks are proposed, which boosts the development of related researchs.\nHowever, existing detection methods still suffer from undesirable performance\nunder challenges such as camouflage, blur, inter-class similarity, intra-class\nvariance and complex environment. To address this issue, we propose LGA-RCNN\nwhich utilizes a loss-guided attention (LGA) module to highlight representative\nregion of objects. Then, those highlighted local information are fused with\nglobal information for precise classification and localization.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 13:52:58 GMT"}, {"version": "v2", "created": "Thu, 6 May 2021 07:47:04 GMT"}, {"version": "v3", "created": "Tue, 11 May 2021 03:15:27 GMT"}, {"version": "v4", "created": "Wed, 12 May 2021 10:45:24 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Yi", "Xin", ""], ["Wu", "Jiahao", ""], ["Ma", "Bo", ""], ["Ou", "Yangtong", ""], ["Liu", "Longyao", ""]]}, {"id": "2104.13764", "submitter": "Masato Tamura", "authors": "Masato Tamura, Tomoaki Yoshinaga", "title": "Segmentation-Based Bounding Box Generation for Omnidirectional\n  Pedestrian Detection", "comments": "Pre-print submitted to Journal of Computer Vision and Image\n  Understanding (CVIU)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a segmentation-based bounding box generation method for\nomnidirectional pedestrian detection, which enables detectors to tightly fit\nbounding boxes to pedestrians without omnidirectional images for training.\nBecause the appearance of pedestrians in omnidirectional images may be rotated\nto any angle, the performance of common pedestrian detectors is likely to be\nsubstantially degraded. Existing methods mitigate this issue by transforming\nimages during inference or training detectors with omnidirectional images.\nHowever, the first approach substantially degrades the inference speed, and the\nsecond approach requires laborious annotations. To overcome these drawbacks, we\nleverage an existing large-scale dataset, whose segmentation annotations can be\nutilized, to generate tightly fitted bounding box annotations. We also develop\na pseudo-fisheye distortion augmentation method, which further enhances the\nperformance. Extensive analysis shows that our detector successfully fits\nbounding boxes to pedestrians and demonstrates substantial performance\nimprovement.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 13:53:21 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Tamura", "Masato", ""], ["Yoshinaga", "Tomoaki", ""]]}, {"id": "2104.13766", "submitter": "Yingyi Chen", "authors": "Yingyi Chen, Xi Shen, Shell Xu Hu, Johan A.K. Suykens", "title": "Boosting Co-teaching with Compression Regularization for Label Noise", "comments": "Accepted by CVPR Workshop 2021. Project page:\n  https://github.com/yingyichen-cyy/Nested-Co-teaching", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the problem of learning image classification models\nin the presence of label noise. We revisit a simple compression regularization\nnamed Nested Dropout. We find that Nested Dropout, though originally proposed\nto perform fast information retrieval and adaptive data compression, can\nproperly regularize a neural network to combat label noise. Moreover, owing to\nits simplicity, it can be easily combined with Co-teaching to further boost the\nperformance.\n  Our final model remains simple yet effective: it achieves comparable or even\nbetter performance than the state-of-the-art approaches on two real-world\ndatasets with label noise which are Clothing1M and ANIMAL-10N. On Clothing1M,\nour approach obtains 74.9% accuracy which is slightly better than that of\nDivideMix. On ANIMAL-10N, we achieve 84.1% accuracy while the best public\nresult by PLC is 83.4%. We hope that our simple approach can be served as a\nstrong baseline for learning with label noise. Our implementation is available\nat https://github.com/yingyichen-cyy/Nested-Co-teaching.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 13:56:12 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Chen", "Yingyi", ""], ["Shen", "Xi", ""], ["Hu", "Shell Xu", ""], ["Suykens", "Johan A. K.", ""]]}, {"id": "2104.13773", "submitter": "Amena Khatun", "authors": "Amena Khatun, Simon Denman, Sridha Sridharan, Clinton Fookes", "title": "Pose-driven Attention-guided Image Generation for Person\n  Re-Identification", "comments": "Submitted to Pattern Recognition", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Person re-identification (re-ID) concerns the matching of subject images\nacross different camera views in a multi camera surveillance system. One of the\nmajor challenges in person re-ID is pose variations across the camera network,\nwhich significantly affects the appearance of a person. Existing development\ndata lack adequate pose variations to carry out effective training of person\nre-ID systems. To solve this issue, in this paper we propose an end-to-end\npose-driven attention-guided generative adversarial network, to generate\nmultiple poses of a person. We propose to attentively learn and transfer the\nsubject pose through an attention mechanism. A semantic-consistency loss is\nproposed to preserve the semantic information of the person during pose\ntransfer. To ensure fine image details are realistic after pose translation, an\nappearance discriminator is used while a pose discriminator is used to ensure\nthe pose of the transferred images will exactly be the same as the target pose.\nWe show that by incorporating the proposed approach in a person\nre-identification framework, realistic pose transferred images and\nstate-of-the-art re-identification results can be achieved.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 14:02:24 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Khatun", "Amena", ""], ["Denman", "Simon", ""], ["Sridharan", "Sridha", ""], ["Fookes", "Clinton", ""]]}, {"id": "2104.13780", "submitter": "Amena Khatun", "authors": "Amena Khatun, Simon Denman, Sridha Sridharan, Clinton Fookes", "title": "Semantic Consistency and Identity Mapping Multi-Component Generative\n  Adversarial Network for Person Re-Identification", "comments": "Accepted in WACV 2020", "journal-ref": "WACV, 2020", "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In a real world environment, person re-identification (Re-ID) is a\nchallenging task due to variations in lighting conditions, viewing angles, pose\nand occlusions. Despite recent performance gains, current person Re-ID\nalgorithms still suffer heavily when encountering these variations. To address\nthis problem, we propose a semantic consistency and identity mapping\nmulti-component generative adversarial network (SC-IMGAN) which provides style\nadaptation from one to many domains. To ensure that transformed images are as\nrealistic as possible, we propose novel identity mapping and semantic\nconsistency losses to maintain identity across the diverse domains. For the\nRe-ID task, we propose a joint verification-identification quartet network\nwhich is trained with generated and real images, followed by an effective\nquartet loss for verification. Our proposed method outperforms state-of-the-art\ntechniques on six challenging person Re-ID datasets: CUHK01, CUHK03, VIPeR,\nPRID2011, iLIDS and Market-1501.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 14:12:29 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Khatun", "Amena", ""], ["Denman", "Simon", ""], ["Sridharan", "Sridha", ""], ["Fookes", "Clinton", ""]]}, {"id": "2104.13786", "submitter": "Dejan Stepec", "authors": "Dejan Stepec and Danijel Skocaj", "title": "Unsupervised Detection of Cancerous Regions in Histology Imagery using\n  Image-to-Image Translation", "comments": "CVPR 2021 CVMI workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Detection of visual anomalies refers to the problem of finding patterns in\ndifferent imaging data that do not conform to the expected visual appearance\nand is a widely studied problem in different domains. Due to the nature of\nanomaly occurrences and underlying generating processes, it is hard to\ncharacterize them and obtain labeled data. Obtaining labeled data is especially\ndifficult in biomedical applications, where only trained domain experts can\nprovide labels, which often come in large diversity and complexity. Recently\npresented approaches for unsupervised detection of visual anomalies approaches\nomit the need for labeled data and demonstrate promising results in domains,\nwhere anomalous samples significantly deviate from the normal appearance.\nDespite promising results, the performance of such approaches still lags behind\nsupervised approaches and does not provide a one-fits-all solution. In this\nwork, we present an image-to-image translation-based framework that\nsignificantly surpasses the performance of existing unsupervised methods and\napproaches the performance of supervised methods in a challenging domain of\ncancerous region detection in histology imagery.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 14:19:00 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Stepec", "Dejan", ""], ["Skocaj", "Danijel", ""]]}, {"id": "2104.13797", "submitter": "Dejan Stepec", "authors": "Dejan Stepec and Danijel Skocaj", "title": "Image Synthesis as a Pretext for Unsupervised Histopathological\n  Diagnosis", "comments": "MICCAI 2020 SASHIMI workshop", "journal-ref": null, "doi": "10.1007/978-3-030-59520-3_18", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Anomaly detection in visual data refers to the problem of differentiating\nabnormal appearances from normal cases. Supervised approaches have been\nsuccessfully applied to different domains, but require an abundance of labeled\ndata. Due to the nature of how anomalies occur and their underlying generating\nprocesses, it is hard to characterize and label them. Recent advances in deep\ngenerative-based models have sparked interest in applying such methods for\nunsupervised anomaly detection and have shown promising results in medical and\nindustrial inspection domains. In this work we evaluate a crucial part of the\nunsupervised visual anomaly detection pipeline, that is needed for normal\nappearance modeling, as well as the ability to reconstruct closest looking\nnormal and tumor samples. We adapt and evaluate different high-resolution\nstate-of-the-art generative models from the face synthesis domain and\ndemonstrate their superiority over currently used approaches on a challenging\ndomain of digital pathology. Multifold improvement in image synthesis is\ndemonstrated in terms of the quality and resolution of the generated images,\nvalidated also against the supervised model.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 14:37:23 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Stepec", "Dejan", ""], ["Skocaj", "Danijel", ""]]}, {"id": "2104.13803", "submitter": "Kevin Bowyer", "authors": "Ying Qiu, V\\'itor Albiero, Michael C. King, Kevin W. Bowyer", "title": "Does Face Recognition Error Echo Gender Classification Error?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is the first to explore the question of whether images that are\nclassified incorrectly by a face analytics algorithm (e.g., gender\nclassification) are any more or less likely to participate in an image pair\nthat results in a face recognition error. We analyze results from three\ndifferent gender classification algorithms (one open-source and two\ncommercial), and two face recognition algorithms (one open-source and one\ncommercial), on image sets representing four demographic groups\n(African-American female and male, Caucasian female and male). For impostor\nimage pairs, our results show that pairs in which one image has a gender\nclassification error have a better impostor distribution than pairs in which\nboth images have correct gender classification, and so are less likely to\ngenerate a false match error. For genuine image pairs, our results show that\nindividuals whose images have a mix of correct and incorrect gender\nclassification have a worse genuine distribution (increased false non-match\nrate) compared to individuals whose images all have correct gender\nclassification. Thus, compared to images that generate correct gender\nclassification, images that generate gender classification errors do generate a\ndifferent pattern of recognition errors, both better (false match) and worse\n(false non-match).\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 14:43:31 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Qiu", "Ying", ""], ["Albiero", "V\u00edtor", ""], ["King", "Michael C.", ""], ["Bowyer", "Kevin W.", ""]]}, {"id": "2104.13807", "submitter": "Ruixu Geng", "authors": "Ruixu Geng, Yang Hu, Yan Chen", "title": "Recent Advances on Non-Line-of-Sight Imaging: Conventional Physical\n  Models, Deep Learning, and New Scenes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As an emerging technology that has attracted huge attention,\nnon-line-of-sight (NLOS) imaging can reconstruct hidden objects by analyzing\nthe diffuse reflection on a relay surface, with broad application prospects in\nthe fields of autonomous driving, medical imaging, and defense. Despite the\nchallenges of low signal-to-noise ratio (SNR) and high ill-posedness, NLOS\nimaging has been developed rapidly in recent years. Most current NLOS imaging\ntechnologies use conventional physical models, constructing imaging models\nthrough active or passive illumination and using reconstruction algorithms to\nrestore hidden scenes. Moreover, deep learning algorithms for NLOS imaging have\nalso received much attention recently. This paper presents a comprehensive\noverview of both conventional and deep learning-based NLOS imaging techniques.\nBesides, we also survey new proposed NLOS scenes, and discuss the challenges\nand prospects of existing technologies. Such a survey can help readers have an\noverview of different types of NLOS imaging, thus expediting the development of\nseeing around corners.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 14:56:04 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Geng", "Ruixu", ""], ["Hu", "Yang", ""], ["Chen", "Yan", ""]]}, {"id": "2104.13817", "submitter": "Katrin Renz", "authors": "Katrin Renz, Nicolaj C. Stache, Neil Fox, G\\\"ul Varol, Samuel Albanie", "title": "Sign Segmentation with Changepoint-Modulated Pseudo-Labelling", "comments": "Appears in: 2021 IEEE Conference on Computer Vision and Pattern\n  Recognition Workshops (CVPRW'21). 11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The objective of this work is to find temporal boundaries between signs in\ncontinuous sign language. Motivated by the paucity of annotation available for\nthis task, we propose a simple yet effective algorithm to improve segmentation\nperformance on unlabelled signing footage from a domain of interest. We make\nthe following contributions: (1) We motivate and introduce the task of\nsource-free domain adaptation for sign language segmentation, in which labelled\nsource data is available for an initial training phase, but is not available\nduring adaptation. (2) We propose the Changepoint-Modulated Pseudo-Labelling\n(CMPL) algorithm to leverage cues from abrupt changes in motion-sensitive\nfeature space to improve pseudo-labelling quality for adaptation. (3) We\nshowcase the effectiveness of our approach for category-agnostic sign\nsegmentation, transferring from the BSLCORPUS to the BSL-1K and\nRWTH-PHOENIX-Weather 2014 datasets, where we outperform the prior state of the\nart.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 15:05:19 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Renz", "Katrin", ""], ["Stache", "Nicolaj C.", ""], ["Fox", "Neil", ""], ["Varol", "G\u00fcl", ""], ["Albanie", "Samuel", ""]]}, {"id": "2104.13824", "submitter": "Michael Tarasiou", "authors": "Michail Tarasiou, Stefanos Zafeiriou", "title": "DeepSatData: Building large scale datasets of satellite images for\n  training machine learning models", "comments": "6 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  This report presents design considerations for automatically generating\nsatellite imagery datasets for training machine learning models with emphasis\nplaced on dense classification tasks, e.g. semantic segmentation. The\nimplementation presented makes use of freely available Sentinel-2 data which\nallows generation of large scale datasets required for training deep neural\nnetworks. We discuss issues faced from the point of view of deep neural network\ntraining and evaluation such as checking the quality of ground truth data and\ncomment on the scalability of the approach. Accompanying code is provided in\nhttps://github.com/michaeltrs/DeepSatData.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 15:13:12 GMT"}, {"version": "v2", "created": "Thu, 6 May 2021 17:02:06 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Tarasiou", "Michail", ""], ["Zafeiriou", "Stefanos", ""]]}, {"id": "2104.13826", "submitter": "David Dubois", "authors": "Philippe Raffy, Jean-Fran\\c{c}ois Pambrun, Ashish Kumar, David Dubois,\n  Jay Waldron Patti, Robyn Alexandra Cairns, Ryan Young", "title": "Deep Learning Body Region Classification of MRI and CT examinations", "comments": "21 pages, 2 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Standardized body region labelling of individual images provides data that\ncan improve human and computer use of medical images. A CNN-based classifier\nwas developed to identify body regions in CT and MRI. 17 CT (18 MRI) body\nregions covering the entire human body were defined for the classification\ntask. Three retrospective databases were built for the AI model training,\nvalidation, and testing, with a balanced distribution of studies per body\nregion. The test databases originated from a different healthcare network.\nAccuracy, recall and precision of the classifier was evaluated for patient age,\npatient gender, institution, scanner manufacturer, contrast, slice thickness,\nMRI sequence, and CT kernel. The data included a retrospective cohort of 2,934\nanonymized CT cases (training: 1,804 studies, validation: 602 studies, test:\n528 studies) and 3,185 anonymized MRI cases (training: 1,911 studies,\nvalidation: 636 studies, test: 638 studies). 27 institutions from primary care\nhospitals, community hospitals and imaging centers contributed to the test\ndatasets. The data included cases of all genders in equal proportions and\nsubjects aged from a few months old to +90 years old. An image-level prediction\naccuracy of 91.9% (90.2 - 92.1) for CT, and 94.2% (92.0 - 95.6) for MRI was\nachieved. The classification results were robust across all body regions and\nconfounding factors. Due to limited data, performance results for subjects\nunder 10 years-old could not be reliably evaluated. We show that deep learning\nmodels can classify CT and MRI images by body region including lower and upper\nextremities with high accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 15:20:21 GMT"}, {"version": "v2", "created": "Tue, 29 Jun 2021 15:17:03 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Raffy", "Philippe", ""], ["Pambrun", "Jean-Fran\u00e7ois", ""], ["Kumar", "Ashish", ""], ["Dubois", "David", ""], ["Patti", "Jay Waldron", ""], ["Cairns", "Robyn Alexandra", ""], ["Young", "Ryan", ""]]}, {"id": "2104.13840", "submitter": "Xiangxiang Chu", "authors": "Xiangxiang Chu and Zhi Tian and Yuqing Wang and Bo Zhang and Haibing\n  Ren and Xiaolin Wei and Huaxia Xia and Chunhua Shen", "title": "Twins: Revisiting the Design of Spatial Attention in Vision Transformers", "comments": "Two simple and effective designs of vision transformer, which is on\n  par with the Swin transformer", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Very recently, a variety of vision transformer architectures for dense\nprediction tasks have been proposed and they show that the design of spatial\nattention is critical to their success in these tasks. In this work, we revisit\nthe design of the spatial attention and demonstrate that a carefully-devised\nyet simple spatial attention mechanism performs favourably against the\nstate-of-the-art schemes. As a result, we propose two vision transformer\narchitectures, namely, Twins-PCPVT and Twins-SVT. Our proposed architectures\nare highly-efficient and easy to implement, only involving matrix\nmultiplications that are highly optimized in modern deep learning frameworks.\nMore importantly, the proposed architectures achieve excellent performance on a\nwide range of visual tasks including imagelevel classification as well as dense\ndetection and segmentation. The simplicity and strong performance suggest that\nour proposed architectures may serve as stronger backbones for many vision\ntasks. Our code will be released soon at\nhttps://github.com/Meituan-AutoML/Twins .\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 15:42:31 GMT"}, {"version": "v2", "created": "Tue, 11 May 2021 15:41:36 GMT"}, {"version": "v3", "created": "Fri, 11 Jun 2021 11:54:21 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Chu", "Xiangxiang", ""], ["Tian", "Zhi", ""], ["Wang", "Yuqing", ""], ["Zhang", "Bo", ""], ["Ren", "Haibing", ""], ["Wei", "Xiaolin", ""], ["Xia", "Huaxia", ""], ["Shen", "Chunhua", ""]]}, {"id": "2104.13854", "submitter": "Talha Bilal", "authors": "Minhaj Uddin Ansari, Talha Bilal, Naeem Akhter", "title": "D-OccNet: Detailed 3D Reconstruction Using Cross-Domain Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep learning based 3D reconstruction of single view 2D image is becoming\nincreasingly popular due to their wide range of real-world applications, but\nthis task is inherently challenging because of the partial observability of an\nobject from a single perspective. Recently, state of the art probability based\nOccupancy Networks reconstructed 3D surfaces from three different types of\ninput domains: single view 2D image, point cloud and voxel. In this study, we\nextend the work on Occupancy Networks by exploiting cross-domain learning of\nimage and point cloud domains. Specifically, we first convert the single view\n2D image into a simpler point cloud representation, and then reconstruct a 3D\nsurface from it. Our network, the Double Occupancy Network (D-OccNet)\noutperforms Occupancy Networks in terms of visual quality and details captured\nin the 3D reconstruction.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 16:00:54 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Ansari", "Minhaj Uddin", ""], ["Bilal", "Talha", ""], ["Akhter", "Naeem", ""]]}, {"id": "2104.13872", "submitter": "Ukyo Honda", "authors": "Ukyo Honda, Yoshitaka Ushiku, Atsushi Hashimoto, Taro Watanabe, Yuji\n  Matsumoto", "title": "Removing Word-Level Spurious Alignment between Images and\n  Pseudo-Captions in Unsupervised Image Captioning", "comments": "EACL 2021 (11 pages, 3 figures; added references)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Unsupervised image captioning is a challenging task that aims at generating\ncaptions without the supervision of image-sentence pairs, but only with images\nand sentences drawn from different sources and object labels detected from the\nimages. In previous work, pseudo-captions, i.e., sentences that contain the\ndetected object labels, were assigned to a given image. The focus of the\nprevious work was on the alignment of input images and pseudo-captions at the\nsentence level. However, pseudo-captions contain many words that are irrelevant\nto a given image. In this work, we investigate the effect of removing\nmismatched words from image-sentence alignment to determine how they make this\ntask difficult. We propose a simple gating mechanism that is trained to align\nimage features with only the most reliable words in pseudo-captions: the\ndetected object labels. The experimental results show that our proposed method\noutperforms the previous methods without introducing complex sentence-level\nlearning objectives. Combined with the sentence-level alignment method of\nprevious work, our method further improves its performance. These results\nconfirm the importance of careful alignment in word-level details.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 16:36:52 GMT"}, {"version": "v2", "created": "Tue, 1 Jun 2021 07:04:37 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Honda", "Ukyo", ""], ["Ushiku", "Yoshitaka", ""], ["Hashimoto", "Atsushi", ""], ["Watanabe", "Taro", ""], ["Matsumoto", "Yuji", ""]]}, {"id": "2104.13874", "submitter": "David Bruggemann", "authors": "David Bruggemann, Menelaos Kanakis, Anton Obukhov, Stamatios\n  Georgoulis, Luc Van Gool", "title": "Exploring Relational Context for Multi-Task Dense Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The timeline of computer vision research is marked with advances in learning\nand utilizing efficient contextual representations. Most of them, however, are\ntargeted at improving model performance on a single downstream task. We\nconsider a multi-task environment for dense prediction tasks, represented by a\ncommon backbone and independent task-specific heads. Our goal is to find the\nmost efficient way to refine each task prediction by capturing cross-task\ncontexts dependent on tasks' relations. We explore various attention-based\ncontexts, such as global and local, in the multi-task setting and analyze their\nbehavior when applied to refine each task independently. Empirical findings\nconfirm that different source-target task pairs benefit from different context\ntypes. To automate the selection process, we propose an Adaptive\nTask-Relational Context (ATRC) module, which samples the pool of all available\ncontexts for each task pair using neural architecture search and outputs the\noptimal configuration for deployment. Our method achieves state-of-the-art\nperformance on two important multi-task benchmarks, namely NYUD-v2 and\nPASCAL-Context. The proposed ATRC has a low computational toll and can be used\nas a drop-in refinement module for any supervised multi-task architecture.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 16:45:56 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Bruggemann", "David", ""], ["Kanakis", "Menelaos", ""], ["Obukhov", "Anton", ""], ["Georgoulis", "Stamatios", ""], ["Van Gool", "Luc", ""]]}, {"id": "2104.13876", "submitter": "Li Yang", "authors": "Li Yang, Yan Xu, Shaoru Wang, Chunfeng Yuan, Ziqi Zhang, Bing Li,\n  Weiming Hu", "title": "PDNet: Towards Better One-stage Object Detection with Prediction\n  Decoupling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent one-stage object detectors follow a per-pixel prediction approach that\npredicts both the object category scores and boundary positions from every\nsingle grid location. However, the most suitable positions for inferring\ndifferent targets, i.e., the object category and boundaries, are generally\ndifferent. Predicting all these targets from the same grid location thus may\nlead to sub-optimal results. In this paper, we analyze the suitable inference\npositions for object category and boundaries, and propose a\nprediction-target-decoupled detector named PDNet to establish a more flexible\ndetection paradigm. Our PDNet with the prediction decoupling mechanism encodes\ndifferent targets separately in different locations. A learnable prediction\ncollection module is devised with two sets of dynamic points, i.e., dynamic\nboundary points and semantic points, to collect and aggregate the predictions\nfrom the favorable regions for localization and classification. We adopt a\ntwo-step strategy to learn these dynamic point positions, where the prior\npositions are estimated for different targets first, and the network further\npredicts residual offsets to the positions with better perceptions of the\nobject properties. Extensive experiments on the MS COCO benchmark demonstrate\nthe effectiveness and efficiency of our method. With a single ResNeXt-64x4d-101\nas the backbone, our detector achieves 48.7 AP with single-scale testing, which\noutperforms the state-of-the-art methods by an appreciable margin under the\nsame experimental settings. Moreover, our detector is highly efficient as a\none-stage framework. Our code will be public.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 16:48:04 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Yang", "Li", ""], ["Xu", "Yan", ""], ["Wang", "Shaoru", ""], ["Yuan", "Chunfeng", ""], ["Zhang", "Ziqi", ""], ["Li", "Bing", ""], ["Hu", "Weiming", ""]]}, {"id": "2104.13896", "submitter": "Mustapha Saidallah", "authors": "Mustapha Saidallah, Fatimazahra Taki, Abdelbaki El Belrhiti El Alaoui\n  and Abdeslam El Fergougui", "title": "Classification and comparison of license plates localization algorithms", "comments": "11 pages", "journal-ref": "April 2021, Volume 12", "doi": "10.5121/sipij.2021.12201", "report-no": "02", "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Intelligent Transportation Systems (ITS) are the subject of a world\neconomic competition. They are the application of new information and\ncommunication technologies in the transport sector, to make the infrastructures\nmore efficient, more reliable and more ecological. License Plates Recognition\n(LPR) is the key module of these systems, in which the License Plate\nLocalization (LPL) is the most important stage, because it determines the speed\nand robustness of this module. Thus, during this step the algorithm must\nprocess the image and overcome several constraints as climatic and lighting\nconditions, sensors and angles variety, LPs no-standardization, and the real\ntime processing. This paper presents a classification and comparison of License\nPlates Localization (LPL) algorithms and describes the advantages,\ndisadvantages and improvements made by each of them\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 17:26:52 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Saidallah", "Mustapha", ""], ["Taki", "Fatimazahra", ""], ["Alaoui", "Abdelbaki El Belrhiti El", ""], ["Fergougui", "Abdeslam El", ""]]}, {"id": "2104.13897", "submitter": "Keng Chai", "authors": "Jonathan Pirnay, Keng Chai", "title": "Inpainting Transformer for Anomaly Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anomaly detection in computer vision is the task of identifying images which\ndeviate from a set of normal images. A common approach is to train deep\nconvolutional autoencoders to inpaint covered parts of an image and compare the\noutput with the original image. By training on anomaly-free samples only, the\nmodel is assumed to not being able to reconstruct anomalous regions properly.\nFor anomaly detection by inpainting we suggest it to be beneficial to\nincorporate information from potentially distant regions. In particular we pose\nanomaly detection as a patch-inpainting problem and propose to solve it with a\npurely self-attention based approach discarding convolutions. The proposed\nInpainting Transformer (InTra) is trained to inpaint covered patches in a large\nsequence of image patches, thereby integrating information across large regions\nof the input image. When learning from scratch, InTra achieves better than\nstate-of-the-art results on the MVTec AD [1] dataset for detection and\nlocalization.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 17:27:44 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Pirnay", "Jonathan", ""], ["Chai", "Keng", ""]]}, {"id": "2104.13915", "submitter": "Krzysztof Maziarz", "authors": "Krzysztof Maziarz, Anna Krason, Zbigniew Wojna", "title": "Deep Learning for Rheumatoid Arthritis: Joint Detection and Damage\n  Scoring in X-rays", "comments": "Presented at the Workshop on AI for Public Health at ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advancements in computer vision promise to automate medical image\nanalysis. Rheumatoid arthritis is an autoimmune disease that would profit from\ncomputer-based diagnosis, as there are no direct markers known, and doctors\nhave to rely on manual inspection of X-ray images. In this work, we present a\nmulti-task deep learning model that simultaneously learns to localize joints on\nX-ray images and diagnose two kinds of joint damage: narrowing and erosion.\nAdditionally, we propose a modification of label smoothing, which combines\nclassification and regression cues into a single loss and achieves 5% relative\nerror reduction compared to standard loss functions. Our final model obtained\n4th place in joint space narrowing and 5th place in joint erosion in the global\nRA2 DREAM challenge.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 17:53:19 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Maziarz", "Krzysztof", ""], ["Krason", "Anna", ""], ["Wojna", "Zbigniew", ""]]}, {"id": "2104.13916", "submitter": "Yi Zhang", "authors": "Yi Zhang, Geng Chen, Qian Chen, Yujia Sun, Olivier Deforges, Wassim\n  Hamidouche and Lu Zhang", "title": "Learning Synergistic Attention for Light Field Salient Object Detection", "comments": "14 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel Synergistic Attention Network (SA-Net) to address the\nlight field salient object detection by establishing a synergistic effect\nbetween multi-modal features with advanced attention mechanisms. Our SA-Net\nexploits the rich information of focal stacks via 3D convolutional neural\nnetworks, decodes the high-level features of multi-modal light field data with\ntwo cascaded synergistic attention modules, and predicts the saliency map using\nan effective feature fusion module in a progressive manner. Extensive\nexperiments on three widely-used benchmark datasets show that our SA-Net\noutperforms 28 state-of-the-art models, sufficiently demonstrating its\neffectiveness and superiority. Our code will be made publicly available.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 17:56:04 GMT"}, {"version": "v2", "created": "Fri, 30 Apr 2021 14:39:06 GMT"}, {"version": "v3", "created": "Sun, 16 May 2021 21:32:35 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Zhang", "Yi", ""], ["Chen", "Geng", ""], ["Chen", "Qian", ""], ["Sun", "Yujia", ""], ["Deforges", "Olivier", ""], ["Hamidouche", "Wassim", ""], ["Zhang", "Lu", ""]]}, {"id": "2104.13917", "submitter": "Yanglan  Ou", "authors": "Yanglan Ou, Ye Yuan, Xiaolei Huang, Kelvin Wong, John Volpi, James Z.\n  Wang, Stephen T.C. Wong", "title": "LambdaUNet: 2.5D Stroke Lesion Segmentation of Diffusion-weighted MR\n  Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diffusion-weighted (DW) magnetic resonance imaging is essential for the\ndiagnosis and treatment of ischemic stroke. DW images (DWIs) are usually\nacquired in multi-slice settings where lesion areas in two consecutive 2D\nslices are highly discontinuous due to large slice thickness and sometimes even\nslice gaps. Therefore, although DWIs contain rich 3D information, they cannot\nbe treated as regular 3D or 2D images. Instead, DWIs are somewhere in-between\n(or 2.5D) due to the volumetric nature but inter-slice discontinuities. Thus,\nit is not ideal to apply most existing segmentation methods as they are\ndesigned for either 2D or 3D images. To tackle this problem, we propose a new\nneural network architecture tailored for segmenting highly-discontinuous 2.5D\ndata such as DWIs. Our network, termed LambdaUNet, extends UNet by replacing\nconvolutional layers with our proposed Lambda+ layers. In particular, Lambda+\nlayers transform both intra-slice and inter-slice context around a pixel into\nlinear functions, called lambdas, which are then applied to the pixel to\nproduce informative 2.5D features. LambdaUNet is simple yet effective in\ncombining sparse inter-slice information from adjacent slices while also\ncapturing dense contextual features within a single slice. Experiments on a\nunique clinical dataset demonstrate that LambdaUNet outperforms existing 3D/2D\nimage segmentation methods including recent variants of UNet. Code for\nLambdaUNet will be released with the publication to facilitate future research.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 17:56:04 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Ou", "Yanglan", ""], ["Yuan", "Ye", ""], ["Huang", "Xiaolei", ""], ["Wong", "Kelvin", ""], ["Volpi", "John", ""], ["Wang", "James Z.", ""], ["Wong", "Stephen T. C.", ""]]}, {"id": "2104.13918", "submitter": "Haofei Xu", "authors": "Haofei Xu, Jiaolong Yang, Jianfei Cai, Juyong Zhang, Xin Tong", "title": "High-Resolution Optical Flow from 1D Attention and Correlation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Optical flow is inherently a 2D search problem, and thus the computational\ncomplexity grows quadratically with respect to the search window, making large\ndisplacements matching infeasible for high-resolution images. In this paper, we\npropose a new method for high-resolution optical flow estimation with\nsignificantly less computation, which is achieved by factorizing 2D optical\nflow with 1D attention and correlation. Specifically, we first perform a 1D\nattention operation in the vertical direction of the target image, and then a\nsimple 1D correlation in the horizontal direction of the attended image can\nachieve 2D correspondence modeling effect. The directions of attention and\ncorrelation can also be exchanged, resulting in two 3D cost volumes that are\nconcatenated for optical flow estimation. The novel 1D formulation empowers our\nmethod to scale to very high-resolution input images while maintaining\ncompetitive performance. Extensive experiments on Sintel, KITTI and real-world\n4K ($2160 \\times 3840$) resolution images demonstrated the effectiveness and\nsuperiority of our proposed method.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 17:56:34 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Xu", "Haofei", ""], ["Yang", "Jiaolong", ""], ["Cai", "Jianfei", ""], ["Zhang", "Juyong", ""], ["Tong", "Xin", ""]]}, {"id": "2104.13921", "submitter": "Xiuye Gu", "authors": "Xiuye Gu, Tsung-Yi Lin, Weicheng Kuo, Yin Cui", "title": "Zero-Shot Detection via Vision and Language Knowledge Distillation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zero-shot image classification has made promising progress by training the\naligned image and text encoders. The goal of this work is to advance zero-shot\nobject detection, which aims to detect novel objects without bounding box nor\nmask annotations. We propose ViLD, a training method via Vision and Language\nknowledge Distillation. We distill the knowledge from a pre-trained zero-shot\nimage classification model (e.g., CLIP) into a two-stage detector (e.g., Mask\nR-CNN). Our method aligns the region embeddings in the detector to the text and\nimage embeddings inferred by the pre-trained model. We use the text embeddings\nas the detection classifier, obtained by feeding category names into the\npre-trained text encoder. We then minimize the distance between the region\nembeddings and image embeddings, obtained by feeding region proposals into the\npre-trained image encoder. During inference, we include text embeddings of\nnovel categories into the detection classifier for zero-shot detection. We\nbenchmark the performance on LVIS dataset by holding out all rare categories as\nnovel categories. ViLD obtains 16.1 mask AP$_r$ with a Mask R-CNN (ResNet-50\nFPN) for zero-shot detection, outperforming the supervised counterpart by 3.8.\nThe model can directly transfer to other datasets, achieving 72.2 AP$_{50}$,\n36.6 AP and 11.8 AP on PASCAL VOC, COCO and Objects365, respectively.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 17:58:57 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Gu", "Xiuye", ""], ["Lin", "Tsung-Yi", ""], ["Kuo", "Weicheng", ""], ["Cui", "Yin", ""]]}, {"id": "2104.13923", "submitter": "Dejan Stepec", "authors": "Dejan Stepec and Tomaz Martincic and Danijel Skocaj", "title": "Automated System for Ship Detection from Medium Resolution Satellite\n  Optical Imagery", "comments": "OCEANS 2019 paper", "journal-ref": null, "doi": "10.23919/OCEANS40490.2019.8962707", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we present a ship detection pipeline for low-cost medium\nresolution satellite optical imagery obtained from ESA Sentinel-2 and Planet\nLabs Dove constellations. This optical satellite imagery is readily available\nfor any place on Earth and underutilized in the maritime domain, compared to\nexisting solutions based on synthetic-aperture radar (SAR) imagery. We\ndeveloped a ship detection method based on a state-of-the-art\ndeep-learning-based object detection method which was developed and evaluated\non a large-scale dataset that was collected and automatically annotated with\nthe help of Automatic Identification System (AIS) data.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 15:06:18 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Stepec", "Dejan", ""], ["Martincic", "Tomaz", ""], ["Skocaj", "Danijel", ""]]}, {"id": "2104.13946", "submitter": "Haoyue Bai", "authors": "Haoyue Bai, S.-H. Gary Chan", "title": "Motion-guided Non-local Spatial-Temporal Network for Video Crowd\n  Counting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study video crowd counting, which is to estimate the number of objects\n(people in this paper) in all the frames of a video sequence. Previous work on\ncrowd counting is mostly on still images. There has been little work on how to\nproperly extract and take advantage of the spatial-temporal correlation between\nneighboring frames in both short and long ranges to achieve high estimation\naccuracy for a video sequence. In this work, we propose Monet, a novel and\nhighly accurate motion-guided non-local spatial-temporal network for video\ncrowd counting. Monet first takes people flow (motion information) as guidance\nto coarsely segment the regions of pixels where a person may be. Given these\nregions, Monet then uses a non-local spatial-temporal network to extract\nspatial-temporally both short and long-range contextual information. The whole\nnetwork is finally trained end-to-end with a fused loss to generate a\nhigh-quality density map. Noting the scarcity and low quality (in terms of\nresolution and scene diversity) of the publicly available video crowd datasets,\nwe have collected and built a large-scale video crowd counting datasets,\nVidCrowd, to contribute to the community. VidCrowd contains 9,000 frames of\nhigh resolution (2560 x 1440), with 1,150,239 head annotations captured in\ndifferent scenes, crowd density and lighting in two cities. We have conducted\nextensive experiments on the challenging VideoCrowd and two public video crowd\ncounting datasets: UCSD and Mall. Our approach achieves substantially better\nperformance in terms of MAE and MSE as compared with other state-of-the-art\napproaches.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 18:05:13 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Bai", "Haoyue", ""], ["Chan", "S. -H. Gary", ""]]}, {"id": "2104.13950", "submitter": "Zafiirah Hosenie", "authors": "Zafiirah Hosenie, Steven Bloemen, Paul Groot, Robert Lyon, Bart\n  Scheers, Benjamin Stappers, Fiorenzo Stoppa, Paul Vreeswijk, Simon De Wet,\n  Marc Klein Wolt, Elmar K\\\"ording, Vanessa McBride, Rudolf Le Poole, Kerry\n  Paterson, Dani\\\"elle L. A. Pieterse and Patrick Woudt", "title": "MeerCRAB: MeerLICHT Classification of Real and Bogus Transients using\n  Deep Learning", "comments": "15 pages, 13 figures, Accepted for publication in Experimental\n  Astronomy and appeared in the 3rd Workshop on Machine Learning and the\n  Physical Sciences, NeurIPS 2020", "journal-ref": "Exp Astron (2021)", "doi": "10.1007/s10686-021-09757-1", "report-no": null, "categories": "astro-ph.IM astro-ph.GA cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Astronomers require efficient automated detection and classification\npipelines when conducting large-scale surveys of the (optical) sky for variable\nand transient sources. Such pipelines are fundamentally important, as they\npermit rapid follow-up and analysis of those detections most likely to be of\nscientific value. We therefore present a deep learning pipeline based on the\nconvolutional neural network architecture called $\\texttt{MeerCRAB}$. It is\ndesigned to filter out the so called 'bogus' detections from true astrophysical\nsources in the transient detection pipeline of the MeerLICHT telescope. Optical\ncandidates are described using a variety of 2D images and numerical features\nextracted from those images. The relationship between the input images and the\ntarget classes is unclear, since the ground truth is poorly defined and often\nthe subject of debate. This makes it difficult to determine which source of\ninformation should be used to train a classification algorithm. We therefore\nused two methods for labelling our data (i) thresholding and (ii) latent class\nmodel approaches. We deployed variants of $\\texttt{MeerCRAB}$ that employed\ndifferent network architectures trained using different combinations of input\nimages and training set choices, based on classification labels provided by\nvolunteers. The deepest network worked best with an accuracy of 99.5$\\%$ and\nMatthews correlation coefficient (MCC) value of 0.989. The best model was\nintegrated to the MeerLICHT transient vetting pipeline, enabling the accurate\nand efficient classification of detected transients that allows researchers to\nselect the most promising candidates for their research goals.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 18:12:51 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Hosenie", "Zafiirah", ""], ["Bloemen", "Steven", ""], ["Groot", "Paul", ""], ["Lyon", "Robert", ""], ["Scheers", "Bart", ""], ["Stappers", "Benjamin", ""], ["Stoppa", "Fiorenzo", ""], ["Vreeswijk", "Paul", ""], ["De Wet", "Simon", ""], ["Wolt", "Marc Klein", ""], ["K\u00f6rding", "Elmar", ""], ["McBride", "Vanessa", ""], ["Poole", "Rudolf Le", ""], ["Paterson", "Kerry", ""], ["Pieterse", "Dani\u00eblle L. A.", ""], ["Woudt", "Patrick", ""]]}, {"id": "2104.13963", "submitter": "Mahmoud Assran", "authors": "Mahmoud Assran, Mathilde Caron, Ishan Misra, Piotr Bojanowski, Armand\n  Joulin, Nicolas Ballas, Michael Rabbat", "title": "Semi-Supervised Learning of Visual Features by Non-Parametrically\n  Predicting View Assignments with Support Samples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel method of learning by predicting view assignments\nwith support samples (PAWS). The method trains a model to minimize a\nconsistency loss, which ensures that different views of the same unlabeled\ninstance are assigned similar pseudo-labels. The pseudo-labels are generated\nnon-parametrically, by comparing the representations of the image views to\nthose of a set of randomly sampled labeled images. The distance between the\nview representations and labeled representations is used to provide a weighting\nover class labels, which we interpret as a soft pseudo-label. By\nnon-parametrically incorporating labeled samples in this way, PAWS extends the\ndistance-metric loss used in self-supervised methods such as BYOL and SwAV to\nthe semi-supervised setting. Despite the simplicity of the approach, PAWS\noutperforms other semi-supervised methods across architectures, setting a new\nstate-of-the-art for a ResNet-50 on ImageNet trained with either 10% or 1% of\nthe labels, reaching 75.5% and 66.5% top-1 respectively. PAWS requires 4x to\n12x less training than the previous best methods.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 18:44:07 GMT"}, {"version": "v2", "created": "Thu, 27 May 2021 03:02:09 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Assran", "Mahmoud", ""], ["Caron", "Mathilde", ""], ["Misra", "Ishan", ""], ["Bojanowski", "Piotr", ""], ["Joulin", "Armand", ""], ["Ballas", "Nicolas", ""], ["Rabbat", "Michael", ""]]}, {"id": "2104.13969", "submitter": "Ryan Sander", "authors": "Jan Petrich, Ryan Sander, Eliza Bradley, Adam Dawood, Shawn Hough", "title": "On the Importance of 3D Surface Information for Remote Sensing\n  Classification Tasks", "comments": "Accepted to CODATA Data Science Journal", "journal-ref": null, "doi": "10.5334/dsj-2021-020", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  There has been a surge in remote sensing machine learning applications that\noperate on data from active or passive sensors as well as multi-sensor\ncombinations (Ma et al. (2019)). Despite this surge, however, there has been\nrelatively little study on the comparative value of 3D surface information for\nmachine learning classification tasks. Adding 3D surface information to RGB\nimagery can provide crucial geometric information for semantic classes such as\nbuildings, and can thus improve out-of-sample predictive performance. In this\npaper, we examine in-sample and out-of-sample classification performance of\nFully Convolutional Neural Networks (FCNNs) and Support Vector Machines (SVMs)\ntrained with and without 3D normalized digital surface model (nDSM)\ninformation. We assess classification performance using multispectral imagery\nfrom the International Society for Photogrammetry and Remote Sensing (ISPRS) 2D\nSemantic Labeling contest and the United States Special Operations Command\n(USSOCOM) Urban 3D Challenge. We find that providing RGB classifiers with\nadditional 3D nDSM information results in little increase in in-sample\nclassification performance, suggesting that spectral information alone may be\nsufficient for the given classification tasks. However, we observe that\nproviding these RGB classifiers with additional nDSM information leads to\nsignificant gains in out-of-sample predictive performance. Specifically, we\nobserve an average improvement in out-of-sample all-class accuracy of 14.4% on\nthe ISPRS dataset and an average improvement in out-of-sample F1 score of 8.6%\non the USSOCOM dataset. In addition, the experiments establish that nDSM\ninformation is critical in machine learning and classification settings that\nface training sample scarcity.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 19:55:51 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Petrich", "Jan", ""], ["Sander", "Ryan", ""], ["Bradley", "Eliza", ""], ["Dawood", "Adam", ""], ["Hough", "Shawn", ""]]}, {"id": "2104.13993", "submitter": "Ramon Izquierdo Cordova", "authors": "Ramon Izquierdo-Cordova and Walterio Mayol-Cuevas", "title": "Filter Distribution Templates in Convolutional Networks for Image\n  Classification Tasks", "comments": "arXiv admin note: text overlap with arXiv:2104.08446", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural network designers have reached progressive accuracy by increasing\nmodels depth, introducing new layer types and discovering new combinations of\nlayers. A common element in many architectures is the distribution of the\nnumber of filters in each layer. Neural network models keep a pattern design of\nincreasing filters in deeper layers such as those in LeNet, VGG, ResNet,\nMobileNet and even in automatic discovered architectures such as NASNet. It\nremains unknown if this pyramidal distribution of filters is the best for\ndifferent tasks and constrains. In this work we present a series of\nmodifications in the distribution of filters in four popular neural network\nmodels and their effects in accuracy and resource consumption. Results show\nthat by applying this approach, some models improve up to 8.9% in accuracy\nshowing reductions in parameters up to 54%.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 19:47:22 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Izquierdo-Cordova", "Ramon", ""], ["Mayol-Cuevas", "Walterio", ""]]}, {"id": "2104.14006", "submitter": "Christos Kyrkou", "authors": "Christos Kyrkou and Theocharis Theocharides", "title": "EmergencyNet: Efficient Aerial Image Classification for Drone-Based\n  Emergency Monitoring Using Atrous Convolutional Feature Fusion", "comments": "C.Kyrkou and T. Theocharides, \"EmergencyNet: Efficient Aerial Image\n  Classification for Drone-Based Emergency Monitoring Using Atrous\n  Convolutional Feature Fusion,\" in IEEE J Sel Top Appl Earth Obs Remote Sens.\n  (JSTARS), vol. 13, pp. 1687-1699, 2020. arXiv admin note: substantial text\n  overlap with arXiv:1906.08716", "journal-ref": "IEEE Journal of Selected Topics in Applied Earth Observations and\n  Remote Sensing ( Volume: 13), Page(s): 1687 - 1699, 2020", "doi": "10.1109/JSTARS.2020.2969809", "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Deep learning-based algorithms can provide state-of-the-art accuracy for\nremote sensing technologies such as unmanned aerial vehicles (UAVs)/drones,\npotentially enhancing their remote sensing capabilities for many emergency\nresponse and disaster management applications. In particular, UAVs equipped\nwith camera sensors can operating in remote and difficult to access\ndisaster-stricken areas, analyze the image and alert in the presence of various\ncalamities such as collapsed buildings, flood, or fire in order to faster\nmitigate their effects on the environment and on human population. However, the\nintegration of deep learning introduces heavy computational requirements,\npreventing the deployment of such deep neural networks in many scenarios that\nimpose low-latency constraints on inference, in order to make mission-critical\ndecisions in real time. To this end, this article focuses on the efficient\naerial image classification from on-board a UAV for emergency\nresponse/monitoring applications. Specifically, a dedicated Aerial Image\nDatabase for Emergency Response applications is introduced and a comparative\nanalysis of existing approaches is performed. Through this analysis a\nlightweight convolutional neural network architecture is proposed, referred to\nas EmergencyNet, based on atrous convolutions to process multiresolution\nfeatures and capable of running efficiently on low-power embedded platforms\nachieving upto 20x higher performance compared to existing models with minimal\nmemory requirements with less than 1% accuracy drop compared to\nstate-of-the-art models.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 20:24:10 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Kyrkou", "Christos", ""], ["Theocharides", "Theocharis", ""]]}, {"id": "2104.14007", "submitter": "Simone Felicioni", "authors": "Simone Felicioni, Mariella Dimiccoli", "title": "Interaction-GCN: A Graph Convolutional Network based framework for\n  social interaction recognition in egocentric videos", "comments": "Accepted to ICIP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a new framework to categorize social interactions in\negocentric videos, we named InteractionGCN. Our method extracts patterns of\nrelational and non-relational cues at the frame level and uses them to build a\nrelational graph from which the interactional context at the frame level is\nestimated via a Graph Convolutional Network based approach. Then it propagates\nthis context over time, together with first-person motion information, through\na Gated Recurrent Unit architecture. Ablation studies and experimental\nevaluation on two publicly available datasets validate the proposed approach\nand establish state of the art results.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 20:25:40 GMT"}, {"version": "v2", "created": "Tue, 8 Jun 2021 17:47:20 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Felicioni", "Simone", ""], ["Dimiccoli", "Mariella", ""]]}, {"id": "2104.14029", "submitter": "Krishanu Sarker", "authors": "Krishanu Sarker, Sharbani Pandit, Anupam Sarker, Saeid Belkasim and\n  Shihao Ji", "title": "Reducing Risk and Uncertainty of Deep Neural Networks on Diagnosing\n  COVID-19 Infection", "comments": "AAAI, TAIH workshop, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Effective and reliable screening of patients via Computer-Aided Diagnosis can\nplay a crucial part in the battle against COVID-19. Most of the existing works\nfocus on developing sophisticated methods yielding high detection performance,\nyet not addressing the issue of predictive uncertainty. In this work, we\nintroduce uncertainty estimation to detect confusing cases for expert referral\nto address the unreliability of state-of-the-art (SOTA) DNNs on COVID-19\ndetection. To the best of our knowledge, we are the first to address this issue\non the COVID-19 detection problem. In this work, we investigate a number of\nSOTA uncertainty estimation methods on publicly available COVID dataset and\npresent our experimental findings. In collaboration with medical professionals,\nwe further validate the results to ensure the viability of the best performing\nmethod in clinical practice.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 21:36:25 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Sarker", "Krishanu", ""], ["Pandit", "Sharbani", ""], ["Sarker", "Anupam", ""], ["Belkasim", "Saeid", ""], ["Ji", "Shihao", ""]]}, {"id": "2104.14032", "submitter": "Jordan Malof", "authors": "Can Yaras and Bohao Huang and Kyle Bradbury and Jordan M. Malof", "title": "Randomized Histogram Matching: A Simple Augmentation for Unsupervised\n  Domain Adaptation in Overhead Imagery", "comments": "Includes a main paper (10 pages) and supplementary material (4\n  additional pages). This paper is currently undergoing peer review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern deep neural networks (DNNs) achieve highly accurate results for many\nrecognition tasks on overhead (e.g., satellite) imagery. One challenge however\nis visual domain shifts (i.e., statistical changes), which can cause the\naccuracy of DNNs to degrade substantially and unpredictably when tested on new\nsets of imagery. In this work we model domain shifts caused by variations in\nimaging hardware, lighting, and other conditions as non-linear pixel-wise\ntransformations; and we show that modern DNNs can become largely invariant to\nthese types of transformations, if provided with appropriate training data\naugmentation. In general, however, we do not know the transformation between\ntwo sets of imagery. To overcome this problem, we propose a simple real-time\nunsupervised training augmentation technique, termed randomized histogram\nmatching (RHM). We conduct experiments with two large public benchmark datasets\nfor building segmentation and find that RHM consistently yields comparable\nperformance to recent state-of-the-art unsupervised domain adaptation\napproaches despite being simpler and faster. RHM also offers substantially\nbetter performance than other comparably simple approaches that are widely-used\nin overhead imagery.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 21:59:54 GMT"}, {"version": "v2", "created": "Fri, 30 Apr 2021 19:26:01 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Yaras", "Can", ""], ["Huang", "Bohao", ""], ["Bradbury", "Kyle", ""], ["Malof", "Jordan M.", ""]]}, {"id": "2104.14040", "submitter": "Kuo-Hao Zeng", "authors": "Kuo-Hao Zeng, Luca Weihs, Ali Farhadi, Roozbeh Mottaghi", "title": "Pushing it out of the Way: Interactive Visual Navigation", "comments": "14 pages, 13 figures, CVPR 2021,\n  https://prior.allenai.org/projects/interactive-visual-navigation,\n  https://youtu.be/GvTI5XCMvPw", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We have observed significant progress in visual navigation for embodied\nagents. A common assumption in studying visual navigation is that the\nenvironments are static; this is a limiting assumption. Intelligent navigation\nmay involve interacting with the environment beyond just moving\nforward/backward and turning left/right. Sometimes, the best way to navigate is\nto push something out of the way. In this paper, we study the problem of\ninteractive navigation where agents learn to change the environment to navigate\nmore efficiently to their goals. To this end, we introduce the Neural\nInteraction Engine (NIE) to explicitly predict the change in the environment\ncaused by the agent's actions. By modeling the changes while planning, we find\nthat agents exhibit significant improvements in their navigational\ncapabilities. More specifically, we consider two downstream tasks in the\nphysics-enabled, visually rich, AI2-THOR environment: (1) reaching a target\nwhile the path to the target is blocked (2) moving an object to a target\nlocation by pushing it. For both tasks, agents equipped with an NIE\nsignificantly outperform agents without the understanding of the effect of the\nactions indicating the benefits of our approach.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 22:46:41 GMT"}, {"version": "v2", "created": "Sun, 2 May 2021 00:41:22 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Zeng", "Kuo-Hao", ""], ["Weihs", "Luca", ""], ["Farhadi", "Ali", ""], ["Mottaghi", "Roozbeh", ""]]}, {"id": "2104.14042", "submitter": "Senthil Yogamani", "authors": "Mahesh M Dhananjaya, Varun Ravi Kumar and Senthil Yogamani", "title": "Weather and Light Level Classification for Autonomous Driving: Dataset,\n  Baseline and Active Learning", "comments": "Accepted for Oral Presentation at IEEE Intelligent Transportation\n  Systems Conference (ITSC) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous driving is rapidly advancing, and Level 2 functions are becoming a\nstandard feature. One of the foremost outstanding hurdles is to obtain robust\nvisual perception in harsh weather and low light conditions where accuracy\ndegradation is severe. It is critical to have a weather classification model to\ndecrease visual perception confidence during these scenarios. Thus, we have\nbuilt a new dataset for weather (fog, rain, and snow) classification and light\nlevel (bright, moderate, and low) classification. Furthermore, we provide\nstreet type (asphalt, grass, and cobblestone) classification, leading to 9\nlabels. Each image has three labels corresponding to weather, light level, and\nstreet type. We recorded the data utilizing an industrial front camera of RCCC\n(red/clear) format with a resolution of $1024\\times1084$. We collected 15k\nvideo sequences and sampled 60k images. We implement an active learning\nframework to reduce the dataset's redundancy and find the optimal set of frames\nfor training a model. We distilled the 60k images further to 1.1k images, which\nwill be shared publicly after privacy anonymization. There is no public dataset\nfor weather and light level classification focused on autonomous driving to the\nbest of our knowledge. The baseline ResNet18 network used for weather\nclassification achieves state-of-the-art results in two non-automotive weather\nclassification public datasets but significantly lower accuracy on our proposed\ndataset, demonstrating it is not saturated and needs further research.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 22:53:10 GMT"}, {"version": "v2", "created": "Sun, 11 Jul 2021 00:46:02 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Dhananjaya", "Mahesh M", ""], ["Kumar", "Varun Ravi", ""], ["Yogamani", "Senthil", ""]]}, {"id": "2104.14066", "submitter": "Yang Yang", "authors": "Yang Yang, Min Li, Bo Meng, Zihao Huang, Junxing Ren, Degang Sun", "title": "Objects as Extreme Points", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detection can be regarded as a pixel clustering task, and its boundary\nis determined by four extreme points (leftmost, top, rightmost, and bottom).\nHowever, most studies focus on the center or corner points of the object, which\nare actually conditional results of the extreme points. In this paper, we\npresent an Extreme-Point-Prediction- Based object detector (EPP-Net), which\ndirectly regresses the relative displacement vector between each pixel and the\nfour extreme points. We also propose a new metric to measure the similarity\nbetween two groups of extreme points, namely, Extreme Intersection over Union\n(EIoU), and incorporate this EIoU as a new regression loss. Moreover, we\npropose a novel branch to predict the EIoU between the ground-truth and the\nprediction results, and take it as the localization confidence to filter out\npoor detection results. On the MS-COCO dataset, our method achieves an average\nprecision (AP) of 44.0% with ResNet-50 and an AP of 50.3% with ResNeXt-101-DCN.\nThe proposed EPP-Net provides a new method to detect objects and outperforms\nstate-of-the-art anchor-free detectors.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 01:01:50 GMT"}, {"version": "v2", "created": "Thu, 20 May 2021 08:33:11 GMT"}, {"version": "v3", "created": "Sat, 22 May 2021 07:17:52 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Yang", "Yang", ""], ["Li", "Min", ""], ["Meng", "Bo", ""], ["Huang", "Zihao", ""], ["Ren", "Junxing", ""], ["Sun", "Degang", ""]]}, {"id": "2104.14079", "submitter": "Mohamed Hasan Dr", "authors": "Mohamed Hasan, Albert Solernou, Evangelos Paschalidis, He Wang, Gustav\n  Markkula and Richard Romano", "title": "Maneuver-Aware Pooling for Vehicle Trajectory Prediction", "comments": "Preprint (under review IROS'21). arXiv admin note: text overlap with\n  arXiv:2104.11180", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous vehicles should be able to predict the future states of its\nenvironment and respond appropriately. Specifically, predicting the behavior of\nsurrounding human drivers is vital for such platforms to share the same road\nwith humans. Behavior of each of the surrounding vehicles is governed by the\nmotion of its neighbor vehicles. This paper focuses on predicting the behavior\nof the surrounding vehicles of an autonomous vehicle on highways. We are\nmotivated by improving the prediction accuracy when a surrounding vehicle\nperforms lane change and highway merging maneuvers. We propose a novel pooling\nstrategy to capture the inter-dependencies between the neighbor vehicles.\nDepending solely on Euclidean trajectory representation, the existing pooling\nstrategies do not model the context information of the maneuvers intended by a\nsurrounding vehicle. In contrast, our pooling mechanism employs polar\ntrajectory representation, vehicles orientation and radial velocity. This\nresults in an implicitly maneuver-aware pooling operation. We incorporated the\nproposed pooling mechanism into a generative encoder-decoder model, and\nevaluated our method on the public NGSIM dataset. The results of maneuver-based\ntrajectory predictions demonstrate the effectiveness of the proposed method\ncompared with the state-of-the-art approaches. Our \"Pooling Toolbox\" code is\navailable at https://github.com/m-hasan-n/pooling.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 02:12:08 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Hasan", "Mohamed", ""], ["Solernou", "Albert", ""], ["Paschalidis", "Evangelos", ""], ["Wang", "He", ""], ["Markkula", "Gustav", ""], ["Romano", "Richard", ""]]}, {"id": "2104.14082", "submitter": "Jiachen Li", "authors": "Jiachen Li, Bowen Cheng, Rogerio Feris, Jinjun Xiong, Thomas S.Huang,\n  Wen-Mei Hwu and Humphrey Shi", "title": "Pseudo-IoU: Improving Label Assignment in Anchor-Free Object Detection", "comments": "CVPR 2021 Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current anchor-free object detectors are quite simple and effective yet lack\naccurate label assignment methods, which limits their potential in competing\nwith classic anchor-based models that are supported by well-designed assignment\nmethods based on the Intersection-over-Union~(IoU) metric. In this paper, we\npresent \\textbf{Pseudo-Intersection-over-Union~(Pseudo-IoU)}: a simple metric\nthat brings more standardized and accurate assignment rule into anchor-free\nobject detection frameworks without any additional computational cost or extra\nparameters for training and testing, making it possible to further improve\nanchor-free object detection by utilizing training samples of good quality\nunder effective assignment rules that have been previously applied in\nanchor-based methods. By incorporating Pseudo-IoU metric into an end-to-end\nsingle-stage anchor-free object detection framework, we observe consistent\nimprovements in their performance on general object detection benchmarks such\nas PASCAL VOC and MSCOCO. Our method (single-model and single-scale) also\nachieves comparable performance to other recent state-of-the-art anchor-free\nmethods without bells and whistles. Our code is based on mmdetection toolbox\nand will be made publicly available at\nhttps://github.com/SHI-Labs/Pseudo-IoU-for-Anchor-Free-Object-Detection.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 02:48:47 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Li", "Jiachen", ""], ["Cheng", "Bowen", ""], ["Feris", "Rogerio", ""], ["Xiong", "Jinjun", ""], ["Huang", "Thomas S.", ""], ["Hwu", "Wen-Mei", ""], ["Shi", "Humphrey", ""]]}, {"id": "2104.14085", "submitter": "Jungin Park", "authors": "Jungin Park, Jiyoung Lee, Kwanghoon Sohn", "title": "Bridge to Answer: Structure-aware Graph Interaction Network for Video\n  Question Answering", "comments": "CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents a novel method, termed Bridge to Answer, to infer correct\nanswers for questions about a given video by leveraging adequate graph\ninteractions of heterogeneous crossmodal graphs. To realize this, we learn\nquestion conditioned visual graphs by exploiting the relation between video and\nquestion to enable each visual node using question-to-visual interactions to\nencompass both visual and linguistic cues. In addition, we propose bridged\nvisual-to-visual interactions to incorporate two complementary visual\ninformation on appearance and motion by placing the question graph as an\nintermediate bridge. This bridged architecture allows reliable message passing\nthrough compositional semantics of the question to generate an appropriate\nanswer. As a result, our method can learn the question conditioned visual\nrepresentations attributed to appearance and motion that show powerful\ncapability for video question answering. Extensive experiments prove that the\nproposed method provides effective and superior performance than\nstate-of-the-art methods on several benchmarks.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 03:02:37 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Park", "Jungin", ""], ["Lee", "Jiyoung", ""], ["Sohn", "Kwanghoon", ""]]}, {"id": "2104.14102", "submitter": "Shravan Murlidaran", "authors": "Shravan Murlidaran, William Yang Wang, Miguel P. Eckstein", "title": "Comparing Visual Reasoning in Humans and AI", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent advances in natural language processing and computer vision have led\nto AI models that interpret simple scenes at human levels. Yet, we do not have\na complete understanding of how humans and AI models differ in their\ninterpretation of more complex scenes. We created a dataset of complex scenes\nthat contained human behaviors and social interactions. AI and humans had to\ndescribe the scenes with a sentence. We used a quantitative metric of\nsimilarity between scene descriptions of the AI/human and ground truth of five\nother human descriptions of each scene. Results show that the machine/human\nagreement scene descriptions are much lower than human/human agreement for our\ncomplex scenes. Using an experimental manipulation that occludes different\nspatial regions of the scenes, we assessed how machines and humans vary in\nutilizing regions of images to understand the scenes. Together, our results are\na first step toward understanding how machines fall short of human visual\nreasoning with complex scenes depicting human behaviors.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 04:44:13 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Murlidaran", "Shravan", ""], ["Wang", "William Yang", ""], ["Eckstein", "Miguel P.", ""]]}, {"id": "2104.14107", "submitter": "JIngkai Zhou", "authors": "Jingkai Zhou, Varun Jampani, Zhixiong Pi, Qiong Liu, Ming-Hsuan Yang", "title": "Decoupled Dynamic Filter Networks", "comments": "CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolution is one of the basic building blocks of CNN architectures. Despite\nits common use, standard convolution has two main shortcomings:\nContent-agnostic and Computation-heavy. Dynamic filters are content-adaptive,\nwhile further increasing the computational overhead. Depth-wise convolution is\na lightweight variant, but it usually leads to a drop in CNN performance or\nrequires a larger number of channels. In this work, we propose the Decoupled\nDynamic Filter (DDF) that can simultaneously tackle both of these shortcomings.\nInspired by recent advances in attention, DDF decouples a depth-wise dynamic\nfilter into spatial and channel dynamic filters. This decomposition\nconsiderably reduces the number of parameters and limits computational costs to\nthe same level as depth-wise convolution. Meanwhile, we observe a significant\nboost in performance when replacing standard convolution with DDF in\nclassification networks. ResNet50 / 101 get improved by 1.9% and 1.3% on the\ntop-1 accuracy, while their computational costs are reduced by nearly half.\nExperiments on the detection and joint upsampling networks also demonstrate the\nsuperior performance of the DDF upsampling variant (DDF-Up) in comparison with\nstandard convolution and specialized content-adaptive layers.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 04:55:33 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Zhou", "Jingkai", ""], ["Jampani", "Varun", ""], ["Pi", "Zhixiong", ""], ["Liu", "Qiong", ""], ["Yang", "Ming-Hsuan", ""]]}, {"id": "2104.14116", "submitter": "Abbas Raza Ali", "authors": "Abbas Raza Ali and Marcin Budka", "title": "An Automated Approach for Timely Diagnosis and Prognosis of Coronavirus\n  Disease", "comments": "to be published in IJCNN 2021", "journal-ref": "International Joint Conference on Neural Networks (IJCNN), 2021", "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Since the outbreak of Coronavirus Disease 2019 (COVID-19), most of the\nimpacted patients have been diagnosed with high fever, dry cough, and soar\nthroat leading to severe pneumonia. Hence, to date, the diagnosis of COVID-19\nfrom lung imaging is proved to be a major evidence for early diagnosis of the\ndisease. Although nucleic acid detection using real-time reverse-transcriptase\npolymerase chain reaction (rRT-PCR) remains a gold standard for the detection\nof COVID-19, the proposed approach focuses on the automated diagnosis and\nprognosis of the disease from a non-contrast chest computed tomography (CT)scan\nfor timely diagnosis and triage of the patient. The prognosis covers the\nquantification and assessment of the disease to help hospitals with the\nmanagement and planning of crucial resources, such as medical staff,\nventilators and intensive care units (ICUs) capacity. The approach utilises\ndeep learning techniques for automated quantification of the severity of\nCOVID-19 disease via measuring the area of multiple rounded ground-glass\nopacities (GGO) and consolidations in the periphery (CP) of the lungs and\naccumulating them to form a severity score. The severity of the disease can be\ncorrelated with the medicines prescribed during the triage to assess the\neffectiveness of the treatment. The proposed approach shows promising results\nwhere the classification model achieved 93% accuracy on hold-out data.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 05:26:30 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Ali", "Abbas Raza", ""], ["Budka", "Marcin", ""]]}, {"id": "2104.14118", "submitter": "Hanbo Zhang", "authors": "Hanbo Zhang, Deyu Yang, Han Wang, Binglei Zhao, Xuguang Lan, Jishiyu\n  Ding, Nanning Zheng", "title": "REGRAD: A Large-Scale Relational Grasp Dataset for Safe and\n  Object-Specific Robotic Grasping in Clutter", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the impressive progress achieved in robust grasp detection, robots\nare not skilled in sophisticated grasping tasks (e.g. search and grasp a\nspecific object in clutter). Such tasks involve not only grasping, but\ncomprehensive perception of the visual world (e.g. the relationship between\nobjects). Recently, the advanced deep learning techniques provide a promising\nway for understanding the high-level visual concepts. It encourages robotic\nresearchers to explore solutions for such hard and complicated fields. However,\ndeep learning usually means data-hungry. The lack of data severely limits the\nperformance of deep-learning-based algorithms. In this paper, we present a new\ndataset named \\regrad to sustain the modeling of relationships among objects\nand grasps. We collect the annotations of object poses, segmentations, grasps,\nand relationships in each image for comprehensive perception of grasping. Our\ndataset is collected in both forms of 2D images and 3D point clouds. Moreover,\nsince all the data are generated automatically, users are free to import their\nown object models for the generation of as many data as they want. We have\nreleased our dataset and codes. A video that demonstrates the process of data\ngeneration is also available.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 05:31:21 GMT"}, {"version": "v2", "created": "Mon, 17 May 2021 06:46:10 GMT"}, {"version": "v3", "created": "Mon, 31 May 2021 14:00:32 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Zhang", "Hanbo", ""], ["Yang", "Deyu", ""], ["Wang", "Han", ""], ["Zhao", "Binglei", ""], ["Lan", "Xuguang", ""], ["Ding", "Jishiyu", ""], ["Zheng", "Nanning", ""]]}, {"id": "2104.14124", "submitter": "Tse-Wei Chen", "authors": "Tse-Wei Chen, Motoki Yoshinaga, Hongxing Gao, Wei Tao, Dongchao Wen,\n  Junjie Liu, Kinya Osa, Masami Kato", "title": "Condensation-Net: Memory-Efficient Network Architecture with\n  Cross-Channel Pooling Layers and Virtual Feature Maps", "comments": "Camera-ready version for CVPR 2019 workshop (Embedded Vision\n  Workshop)", "journal-ref": "2019 IEEE/CVF Conference on Computer Vision and Pattern\n  Recognition Workshops (CVPRW)", "doi": "10.1109/CVPRW.2019.00024", "report-no": null, "categories": "cs.CV cs.AR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  \"Lightweight convolutional neural networks\" is an important research topic in\nthe field of embedded vision. To implement image recognition tasks on a\nresource-limited hardware platform, it is necessary to reduce the memory size\nand the computational cost. The contribution of this paper is stated as\nfollows. First, we propose an algorithm to process a specific network\narchitecture (Condensation-Net) without increasing the maximum memory storage\nfor feature maps. The architecture for virtual feature maps saves 26.5% of\nmemory bandwidth by calculating the results of cross-channel pooling before\nstoring the feature map into the memory. Second, we show that cross-channel\npooling can improve the accuracy of object detection tasks, such as face\ndetection, because it increases the number of filter weights. Compared with\nTiny-YOLOv2, the improvement of accuracy is 2.0% for quantized networks and\n1.5% for full-precision networks when the false-positive rate is 0.1. Last but\nnot the least, the analysis results show that the overhead to support the\ncross-channel pooling with the proposed hardware architecture is negligible\nsmall. The extra memory cost to support Condensation-Net is 0.2% of the total\nsize, and the extra gate count is only 1.0% of the total size.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 05:44:02 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Chen", "Tse-Wei", ""], ["Yoshinaga", "Motoki", ""], ["Gao", "Hongxing", ""], ["Tao", "Wei", ""], ["Wen", "Dongchao", ""], ["Liu", "Junjie", ""], ["Osa", "Kinya", ""], ["Kato", "Masami", ""]]}, {"id": "2104.14125", "submitter": "Tse-Wei Chen", "authors": "Tse-Wei Chen, Wei Tao, Deyu Wang, Dongchao Wen, Kinya Osa, Masami Kato", "title": "Hardware Architecture of Embedded Inference Accelerator and Analysis of\n  Algorithms for Depthwise and Large-Kernel Convolutions", "comments": "Camera-ready version for ECCV 2020 workshop (Embedded Vision\n  Workshop)", "journal-ref": "ECCV 2020 Workshops, LNCS 12539, pp. 3-17, 2020", "doi": "10.1007/978-3-030-68238-5_1", "report-no": null, "categories": "cs.CV cs.AR eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In order to handle modern convolutional neural networks (CNNs) efficiently, a\nhardware architecture of CNN inference accelerator is proposed to handle\ndepthwise convolutions and regular convolutions, which are both essential\nbuilding blocks for embedded-computer-vision algorithms. Different from related\nworks, the proposed architecture can support filter kernels with different\nsizes with high flexibility since it does not require extra costs for\nintra-kernel parallelism, and it can generate convolution results faster than\nthe architecture of the related works. The experimental results show the\nimportance of supporting depthwise convolutions and dilated convolutions with\nthe proposed hardware architecture. In addition to depthwise convolutions with\nlarge-kernels, a new structure called DDC layer, which includes the combination\nof depthwise convolutions and dilated convolutions, is also analyzed in this\npaper. For face detection, the computational costs decrease by 30%, and the\nmodel size decreases by 20% when the DDC layers are applied to the network. For\nimage classification, the accuracy is increased by 1% by simply replacing $3\n\\times 3$ filters with $5 \\times 5$ filters in depthwise convolutions.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 05:45:16 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Chen", "Tse-Wei", ""], ["Tao", "Wei", ""], ["Wang", "Deyu", ""], ["Wen", "Dongchao", ""], ["Osa", "Kinya", ""], ["Kato", "Masami", ""]]}, {"id": "2104.14126", "submitter": "Tse-Wei Chen", "authors": "Tse-Wei Chen, Deyu Wang, Wei Tao, Dongchao Wen, Lingxiao Yin, Tadayuki\n  Ito, Kinya Osa, Masami Kato", "title": "CASSOD-Net: Cascaded and Separable Structures of Dilated Convolution for\n  Embedded Vision Systems and Applications", "comments": "Camera-ready version for CVPR 2021 workshop (Embedded Vision\n  Workshop)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The field of view (FOV) of convolutional neural networks is highly related to\nthe accuracy of inference. Dilated convolutions are known as an effective\nsolution to the problems which require large FOVs. However, for general-purpose\nhardware or dedicated hardware, it usually takes extra time to handle dilated\nconvolutions compared with standard convolutions. In this paper, we propose a\nnetwork module, Cascaded and Separable Structure of Dilated (CASSOD)\nConvolution, and a special hardware system to handle the CASSOD networks\nefficiently. A CASSOD-Net includes multiple cascaded $2 \\times 2$ dilated\nfilters, which can be used to replace the traditional $3 \\times 3$ dilated\nfilters without decreasing the accuracy of inference. Two example applications,\nface detection and image segmentation, are tested with dilated convolutions and\nthe proposed CASSOD modules. The new network for face detection achieves higher\naccuracy than the previous work with only 47% of filter weights in the dilated\nconvolution layers of the context module. Moreover, the proposed hardware\nsystem can accelerate the computations of dilated convolutions, and it is 2.78\ntimes faster than traditional hardware systems when the filter size is $3\n\\times 3$.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 05:45:24 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Chen", "Tse-Wei", ""], ["Wang", "Deyu", ""], ["Tao", "Wei", ""], ["Wen", "Dongchao", ""], ["Yin", "Lingxiao", ""], ["Ito", "Tadayuki", ""], ["Osa", "Kinya", ""], ["Kato", "Masami", ""]]}, {"id": "2104.14129", "submitter": "Jianfei Chen", "authors": "Jianfei Chen, Lianmin Zheng, Zhewei Yao, Dequan Wang, Ion Stoica,\n  Michael W. Mahoney, Joseph E. Gonzalez", "title": "ActNN: Reducing Training Memory Footprint via 2-Bit Activation\n  Compressed Training", "comments": "to be published in ICML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing size of neural network models has been critical for\nimprovements in their accuracy, but device memory is not growing at the same\nrate. This creates fundamental challenges for training neural networks within\nlimited memory environments. In this work, we propose ActNN, a memory-efficient\ntraining framework that stores randomly quantized activations for back\npropagation. We prove the convergence of ActNN for general network\narchitectures, and we characterize the impact of quantization on the\nconvergence via an exact expression for the gradient variance. Using our\ntheory, we propose novel mixed-precision quantization strategies that exploit\nthe activation's heterogeneity across feature dimensions, samples, and layers.\nThese techniques can be readily applied to existing dynamic graph frameworks,\nsuch as PyTorch, simply by substituting the layers. We evaluate ActNN on\nmainstream computer vision models for classification, detection, and\nsegmentation tasks. On all these tasks, ActNN compresses the activation to 2\nbits on average, with negligible accuracy loss. ActNN reduces the memory\nfootprint of the activation by 12x, and it enables training with a 6.6x to 14x\nlarger batch size.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 05:50:54 GMT"}, {"version": "v2", "created": "Tue, 6 Jul 2021 05:22:49 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Chen", "Jianfei", ""], ["Zheng", "Lianmin", ""], ["Yao", "Zhewei", ""], ["Wang", "Dequan", ""], ["Stoica", "Ion", ""], ["Mahoney", "Michael W.", ""], ["Gonzalez", "Joseph E.", ""]]}, {"id": "2104.14130", "submitter": "Kun Jiang", "authors": "Kun Jiang, Zhaoli Liu, Zheng Liu and Qindong Sun", "title": "Locality Constrained Analysis Dictionary Learning via K-SVD Algorithm", "comments": "12 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years, analysis dictionary learning (ADL) and its applications for\nclassification have been well developed, due to its flexible projective ability\nand low classification complexity. With the learned analysis dictionary, test\nsamples can be transformed into a sparse subspace for classification\nefficiently. However, the underling locality of sample data has rarely been\nexplored in analysis dictionary to enhance the discriminative capability of the\nclassifier. In this paper, we propose a novel locality constrained analysis\ndictionary learning model with a synthesis K-SVD algorithm (SK-LADL). It\nconsiders the intrinsic geometric properties by imposing graph regularization\nto uncover the geometric structure for the image data. Through the learned\nanalysis dictionary, we transform the image to a new and compact space where\nthe manifold assumption can be further guaranteed. thus, the local geometrical\nstructure of images can be preserved in sparse representation coefficients.\nMoreover, the SK-LADL model is iteratively solved by the synthesis K-SVD and\ngradient technique. Experimental results on image classification validate the\nperformance superiority of our SK-LADL model.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 05:58:34 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Jiang", "Kun", ""], ["Liu", "Zhaoli", ""], ["Liu", "Zheng", ""], ["Sun", "Qindong", ""]]}, {"id": "2104.14131", "submitter": "Sathyanarayanan Aakur", "authors": "Sathyanarayanan N. Aakur, Sudeep Sarkar", "title": "Learning Actor-centered Representations for Action Localization in\n  Streaming Videos using Predictive Learning", "comments": "10 pages, 4 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Event perception tasks such as recognizing and localizing actions in\nstreaming videos are essential for tackling visual understanding tasks.\nProgress has primarily been driven by the use of large-scale, annotated\ntraining data in a supervised manner. In this work, we tackle the problem of\nlearning \\textit{actor-centered} representations through the notion of\ncontinual hierarchical predictive learning to localize actions in streaming\nvideos without any training annotations. Inspired by cognitive theories of\nevent perception, we propose a novel, self-supervised framework driven by the\nnotion of hierarchical predictive learning to construct actor-centered features\nby attention-based contextualization. Extensive experiments on three benchmark\ndatasets show that the approach can learn robust representations for localizing\nactions using only one epoch of training, i.e., we train the model continually\nin streaming fashion - one frame at a time, with a single pass through training\nvideos. We show that the proposed approach outperforms unsupervised and weakly\nsupervised baselines while offering competitive performance to fully supervised\napproaches. Finally, we show that the proposed model can generalize to\nout-of-domain data without significant loss in performance without any\nfinetuning for both the recognition and localization tasks.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 06:06:58 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Aakur", "Sathyanarayanan N.", ""], ["Sarkar", "Sudeep", ""]]}, {"id": "2104.14135", "submitter": "Wang Luo", "authors": "Wang Luo, Tianzhu Zhang, Wenfei Yang, Jingen Liu, Tao Mei, Feng Wu,\n  Yongdong Zhang", "title": "Action Unit Memory Network for Weakly Supervised Temporal Action\n  Localization", "comments": "Accepted by CVPR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weakly supervised temporal action localization aims to detect and localize\nactions in untrimmed videos with only video-level labels during training.\nHowever, without frame-level annotations, it is challenging to achieve\nlocalization completeness and relieve background interference. In this paper,\nwe present an Action Unit Memory Network (AUMN) for weakly supervised temporal\naction localization, which can mitigate the above two challenges by learning an\naction unit memory bank. In the proposed AUMN, two attention modules are\ndesigned to update the memory bank adaptively and learn action units specific\nclassifiers. Furthermore, three effective mechanisms (diversity, homogeneity\nand sparsity) are designed to guide the updating of the memory network. To the\nbest of our knowledge, this is the first work to explicitly model the action\nunits with a memory network. Extensive experimental results on two standard\nbenchmarks (THUMOS14 and ActivityNet) demonstrate that our AUMN performs\nfavorably against state-of-the-art methods. Specifically, the average mAP of\nIoU thresholds from 0.1 to 0.5 on the THUMOS14 dataset is significantly\nimproved from 47.0% to 52.1%.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 06:19:44 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Luo", "Wang", ""], ["Zhang", "Tianzhu", ""], ["Yang", "Wenfei", ""], ["Liu", "Jingen", ""], ["Mei", "Tao", ""], ["Wu", "Feng", ""], ["Zhang", "Yongdong", ""]]}, {"id": "2104.14156", "submitter": "Stefan Juergens", "authors": "Stefan J\\\"urgens, Niklas Koch and Marc-Michael Meinecke", "title": "Radar-based Automotive Localization using Landmarks in a Multimodal\n  Sensor Graph-based Approach", "comments": null, "journal-ref": "Proceedings of 21st International Radar Symposium (IRS 2020)", "doi": "10.23919/IRS48640.2020.9253921", "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Highly automated driving functions currently often rely on a-priori knowledge\nfrom maps for planning and prediction in complex scenarios like cities. This\nmakes map-relative localization an essential skill. In this paper, we address\nthe problem of localization with automotive-grade radars, using a real-time\ngraph-based SLAM approach. The system uses landmarks and odometry information\nas an abstraction layer. This way, besides radars, all kind of different sensor\nmodalities including cameras and lidars can contribute. A single, semantic\nlandmark map is used and maintained for all sensors. We implemented our\napproach using C++ and thoroughly tested it on data obtained with our test\nvehicles, comprising cars and trucks. Test scenarios include inner cities and\nindustrial areas like container terminals. The experiments presented in this\npaper suggest that the approach is able to provide a precise and stable pose in\nstructured environments, using radar data alone. The fusion of additional\nsensor information from cameras or lidars further boost performance, providing\nreliable semantic information needed for automated mapping.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 07:35:20 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["J\u00fcrgens", "Stefan", ""], ["Koch", "Niklas", ""], ["Meinecke", "Marc-Michael", ""]]}, {"id": "2104.14169", "submitter": "Luoyang Lin", "authors": "Luoyang Lin and Dihong Tian", "title": "Using Adaptive Gradient for Texture Learning in Single-View 3D\n  Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, learning-based approaches for 3D model reconstruction have\nattracted attention owing to its modern applications such as Extended\nReality(XR), robotics and self-driving cars. Several approaches presented good\nperformance on reconstructing 3D shapes by learning solely from images, i.e.,\nwithout using 3D models in training. Challenges, however, remain in texture\ngeneration due to the gap between 2D and 3D modals. In previous work, the grid\nsampling mechanism from Spatial Transformer Networks was adopted to sample\ncolor from an input image to formulate texture. Despite its success, the\nexisting framework has limitations on searching scope in sampling, resulting in\nflaws in generated texture and consequentially on rendered 3D models. In this\npaper, to solve that issue, we present a novel sampling algorithm by optimizing\nthe gradient of predicted coordinates based on the variance on the sampling\nimage. Taking into account the semantics of the image, we adopt Frechet\nInception Distance (FID) to form a loss function in learning, which helps\nbridging the gap between rendered images and input images. As a result, we\ngreatly improve generated texture. Furthermore, to optimize 3D shape\nreconstruction and to accelerate convergence at training, we adopt part\nsegmentation and template learning in our model. Without any 3D supervision in\nlearning, and with only a collection of single-view 2D images, the shape and\ntexture learned by our model outperform those from previous work. We\ndemonstrate the performance with experimental results on a publically available\ndataset.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 07:52:54 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Lin", "Luoyang", ""], ["Tian", "Dihong", ""]]}, {"id": "2104.14202", "submitter": "Javier Rodriguez-Puigvert", "authors": "Javier Rodr\\'iguez-Puigvert, Rub\\'en Mart\\'inez-Cant\\'in, Javier\n  Civera", "title": "Bayesian Deep Networks for Supervised Single-View Depth Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Uncertainty quantification is a key aspect in robotic perception, as\noverconfident or point estimators can lead to collisions and damages to the\nenvironment and the robot. In this paper, we evaluate scalable approaches to\nuncertainty quantification in single-view supervised depth learning,\nspecifically MC dropout and deep ensembles. For MC dropout, in particular, we\nexplore the effect of the dropout at different levels in the architecture. We\ndemonstrate that adding dropout in the encoder leads to better results than\nadding it in the decoder, the latest being the usual approach in the literature\nfor similar problems. We also propose the use of depth uncertainty in the\napplication of pseudo-RGBD ICP and demonstrate its potential for improving the\naccuracy in such a task.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 08:45:24 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Rodr\u00edguez-Puigvert", "Javier", ""], ["Mart\u00ednez-Cant\u00edn", "Rub\u00e9n", ""], ["Civera", "Javier", ""]]}, {"id": "2104.14203", "submitter": "Chen-Hao Chao", "authors": "Chen-Hao Chao, Bo-Wun Cheng, Chun-Yi Lee", "title": "Rethinking Ensemble-Distillation for Semantic Segmentation Based\n  Unsupervised Domain Adaptation", "comments": "Accepted to CVPRW (LLID) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent researches on unsupervised domain adaptation (UDA) have demonstrated\nthat end-to-end ensemble learning frameworks serve as a compelling option for\nUDA tasks. Nevertheless, these end-to-end ensemble learning methods often lack\nflexibility as any modification to the ensemble requires retraining of their\nframeworks. To address this problem, we propose a flexible\nensemble-distillation framework for performing semantic segmentation based UDA,\nallowing any arbitrary composition of the members in the ensemble while still\nmaintaining its superior performance. To achieve such flexibility, our\nframework is designed to be robust against the output inconsistency and the\nperformance variation of the members within the ensemble. To examine the\neffectiveness and the robustness of our method, we perform an extensive set of\nexperiments on both GTA5 to Cityscapes and SYNTHIA to Cityscapes benchmarks to\nquantitatively inspect the improvements achievable by our method. We further\nprovide detailed analyses to validate that our design choices are practical and\nbeneficial. The experimental evidence validates that the proposed method indeed\noffer superior performance, robustness and flexibility in semantic segmentation\nbased UDA tasks against contemporary baseline methods.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 08:47:24 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Chao", "Chen-Hao", ""], ["Cheng", "Bo-Wun", ""], ["Lee", "Chun-Yi", ""]]}, {"id": "2104.14205", "submitter": "Luo Yicheng", "authors": "Haotian Zhang, Yicheng Luo, Fangbo Qin, Yijia He, Xiao Liu", "title": "ELSD: Efficient Line Segment Detector and Descriptor", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the novel Efficient Line Segment Detector and Descriptor (ELSD) to\nsimultaneously detect line segments and extract their descriptors in an image.\nUnlike the traditional pipelines that conduct detection and description\nseparately, ELSD utilizes a shared feature extractor for both detection and\ndescription, to provide the essential line features to the higher-level tasks\nlike SLAM and image matching in real time. First, we design the one-stage\ncompact model, and propose to use the mid-point, angle and length as the\nminimal representation of line segment, which also guarantees the\ncenter-symmetry. The non-centerness suppression is proposed to filter out the\nfragmented line segments caused by lines' intersections. The fine offset\nprediction is designed to refine the mid-point localization. Second, the line\ndescriptor branch is integrated with the detector branch, and the two branches\nare jointly trained in an end-to-end manner. In the experiments, the proposed\nELSD achieves the state-of-the-art performance on the Wireframe dataset and\nYorkUrban dataset, in both accuracy and efficiency. The line description\nability of ELSD also outperforms the previous works on the line matching task.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 08:53:03 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Zhang", "Haotian", ""], ["Luo", "Yicheng", ""], ["Qin", "Fangbo", ""], ["He", "Yijia", ""], ["Liu", "Xiao", ""]]}, {"id": "2104.14207", "submitter": "Siddhesh Khandelwal", "authors": "Siddhesh Khandelwal, Mohammed Suhail, Leonid Sigal", "title": "Segmentation-grounded Scene Graph Generation", "comments": "11 pages, 3 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scene graph generation has emerged as an important problem in computer\nvision. While scene graphs provide a grounded representation of objects, their\nlocations and relations in an image, they do so only at the granularity of\nproposal bounding boxes. In this work, we propose the first, to our knowledge,\nframework for pixel-level segmentation-grounded scene graph generation. Our\nframework is agnostic to the underlying scene graph generation method and\naddress the lack of segmentation annotations in target scene graph datasets\n(e.g., Visual Genome) through transfer and multi-task learning from, and with,\nan auxiliary dataset (e.g., MS COCO). Specifically, each target object being\ndetected is endowed with a segmentation mask, which is expressed as a\nlingual-similarity weighted linear combination over categories that have\nannotations present in an auxiliary dataset. These inferred masks, along with a\nnovel Gaussian attention mechanism which grounds the relations at a pixel-level\nwithin the image, allow for improved relation prediction. The entire framework\nis end-to-end trainable and is learned in a multi-task manner with both target\nand auxiliary datasets.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 08:54:08 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Khandelwal", "Siddhesh", ""], ["Suhail", "Mohammed", ""], ["Sigal", "Leonid", ""]]}, {"id": "2104.14222", "submitter": "Sihan Ma", "authors": "Jizhizi Li, Sihan Ma, Jing Zhang, Dacheng Tao", "title": "Privacy-Preserving Portrait Matting", "comments": "Accepted to ACM Multimedia 2021, code and dataset available at\n  https://github.com/JizhiziLi/P3M", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, there has been an increasing concern about the privacy issue raised\nby using personally identifiable information in machine learning. However,\nprevious portrait matting methods were all based on identifiable portrait\nimages. To fill the gap, we present P3M-10k in this paper, which is the first\nlarge-scale anonymized benchmark for Privacy-Preserving Portrait Matting.\nP3M-10k consists of 10,000 high-resolution face-blurred portrait images along\nwith high-quality alpha mattes. We systematically evaluate both trimap-free and\ntrimap-based matting methods on P3M-10k and find that existing matting methods\nshow different generalization capabilities when following the\nPrivacy-Preserving Training (PPT) setting, i.e., training on face-blurred\nimages and testing on arbitrary images. To devise a better trimap-free portrait\nmatting model, we propose P3M-Net, which leverages the power of a unified\nframework for both semantic perception and detail matting, and specifically\nemphasizes the interaction between them and the encoder to facilitate the\nmatting process. Extensive experiments on P3M-10k demonstrate that P3M-Net\noutperforms the state-of-the-art methods in terms of both objective metrics and\nsubjective visual quality. Besides, it shows good generalization capacity under\nthe PPT setting, confirming the value of P3M-10k for facilitating future\nresearch and enabling potential real-world applications. The source code and\ndataset are available at https://github.com/JizhiziLi/P3M\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 09:20:19 GMT"}, {"version": "v2", "created": "Thu, 29 Jul 2021 13:07:00 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Li", "Jizhizi", ""], ["Ma", "Sihan", ""], ["Zhang", "Jing", ""], ["Tao", "Dacheng", ""]]}, {"id": "2104.14236", "submitter": "Yichao Yan", "authors": "Yichao Yan, Jie Qin, Bingbing Ni, Jiaxin Chen, Li Liu, Fan Zhu,\n  Wei-Shi Zheng, Xiaokang Yang, Ling Shao", "title": "Learning Multi-Attention Context Graph for Group-Based Re-Identification", "comments": null, "journal-ref": null, "doi": "10.1109/TPAMI.2020.3032542", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning to re-identify or retrieve a group of people across non-overlapped\ncamera systems has important applications in video surveillance. However, most\nexisting methods focus on (single) person re-identification (re-id), ignoring\nthe fact that people often walk in groups in real scenarios. In this work, we\ntake a step further and consider employing context information for identifying\ngroups of people, i.e., group re-id. We propose a novel unified framework based\non graph neural networks to simultaneously address the group-based re-id tasks,\ni.e., group re-id and group-aware person re-id. Specifically, we construct a\ncontext graph with group members as its nodes to exploit dependencies among\ndifferent people. A multi-level attention mechanism is developed to formulate\nboth intra-group and inter-group context, with an additional self-attention\nmodule for robust graph-level representations by attentively aggregating\nnode-level features. The proposed model can be directly generalized to tackle\ngroup-aware person re-id using node-level representations. Meanwhile, to\nfacilitate the deployment of deep learning models on these tasks, we build a\nnew group re-id dataset that contains more than 3.8K images with 1.5K annotated\ngroups, an order of magnitude larger than existing group re-id datasets.\nExtensive experiments on the novel dataset as well as three existing datasets\nclearly demonstrate the effectiveness of the proposed framework for both\ngroup-based re-id tasks. The code is available at\nhttps://github.com/daodaofr/group_reid.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 09:57:47 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Yan", "Yichao", ""], ["Qin", "Jie", ""], ["Ni", "Bingbing", ""], ["Chen", "Jiaxin", ""], ["Liu", "Li", ""], ["Zhu", "Fan", ""], ["Zheng", "Wei-Shi", ""], ["Yang", "Xiaokang", ""], ["Shao", "Ling", ""]]}, {"id": "2104.14237", "submitter": "Umar Khan", "authors": "Umar Khan, Sohaib Zahid, Muhammad Asad Ali, Adnan ul Hassan, Faisal\n  Shafait", "title": "TabAug: Data Driven Augmentation for Enhanced Table Structure\n  Recognition", "comments": "to be published in ICDAR2021 , 15 pages , \" packages and articles for\n  this work and its extensions at http://umarky.com \" , \" official repository\n  https://github.com/sohaib023/splerge-tab-aug?fbclid=IwAR37V79vDLMqLGcC5YCyqY_CsFYQRDZ1-wUMW7GJUYTzkf9oM1bZ25HPmgo\n  \"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Table Structure Recognition is an essential part of end-to-end tabular data\nextraction in document images. The recent success of deep learning model\narchitectures in computer vision remains to be non-reflective in table\nstructure recognition, largely because extensive datasets for this domain are\nstill unavailable while labeling new data is expensive and time-consuming.\nTraditionally, in computer vision, these challenges are addressed by standard\naugmentation techniques that are based on image transformations like color\njittering and random cropping. As demonstrated by our experiments, these\ntechniques are not effective for the task of table structure recognition. In\nthis paper, we propose TabAug, a re-imagined Data Augmentation technique that\nproduces structural changes in table images through replication and deletion of\nrows and columns. It also consists of a data-driven probabilistic model that\nallows control over the augmentation process. To demonstrate the efficacy of\nour approach, we perform experimentation on ICDAR 2013 dataset where our\napproach shows consistent improvements in all aspects of the evaluation\nmetrics, with cell-level correct detections improving from 92.16% to 96.11%\nover the baseline.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 09:59:46 GMT"}, {"version": "v2", "created": "Sat, 15 May 2021 14:31:19 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Khan", "Umar", ""], ["Zahid", "Sohaib", ""], ["Ali", "Muhammad Asad", ""], ["Hassan", "Adnan ul", ""], ["Shafait", "Faisal", ""]]}, {"id": "2104.14272", "submitter": "Khurram Azeem Hashmi", "authors": "Khurram Azeem Hashmi, Marcus Liwicki, Didier Stricker, Muhammad Adnan\n  Afzal, Muhammad Ahtsham Afzal and Muhammad Zeshan Afzal", "title": "Current Status and Performance Analysis of Table Recognition in Document\n  Images with Deep Neural Networks", "comments": "23 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The first phase of table recognition is to detect the tabular area in a\ndocument. Subsequently, the tabular structures are recognized in the second\nphase in order to extract information from the respective cells. Table\ndetection and structural recognition are pivotal problems in the domain of\ntable understanding. However, table analysis is a perplexing task due to the\ncolossal amount of diversity and asymmetry in tables. Therefore, it is an\nactive area of research in document image analysis. Recent advances in the\ncomputing capabilities of graphical processing units have enabled deep neural\nnetworks to outperform traditional state-of-the-art machine learning methods.\nTable understanding has substantially benefited from the recent breakthroughs\nin deep neural networks. However, there has not been a consolidated description\nof the deep learning methods for table detection and table structure\nrecognition. This review paper provides a thorough analysis of the modern\nmethodologies that utilize deep neural networks. This work provided a thorough\nunderstanding of the current state-of-the-art and related challenges of table\nunderstanding in document images. Furthermore, the leading datasets and their\nintricacies have been elaborated along with the quantitative results. Moreover,\na brief overview is given regarding the promising directions that can serve as\na guide to further improve table analysis in document images.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 11:43:48 GMT"}, {"version": "v2", "created": "Sat, 8 May 2021 20:58:12 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Hashmi", "Khurram Azeem", ""], ["Liwicki", "Marcus", ""], ["Stricker", "Didier", ""], ["Afzal", "Muhammad Adnan", ""], ["Afzal", "Muhammad Ahtsham", ""], ["Afzal", "Muhammad Zeshan", ""]]}, {"id": "2104.14273", "submitter": "Meng Li", "authors": "Meng Li, Changyan Lin, Lixia Shu, Xin Pu, Yu Chen, Heng Wu, Jiasong\n  Li, Hongshuai Cao", "title": "A Rigid Registration Method in TEVAR", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Since the mapping relationship between definitized intra-interventional 2D\nX-ray and undefined pre-interventional 3D Computed Tomography(CT) is uncertain,\nauxiliary positioning devices or body markers, such as medical implants, are\ncommonly used to determine this relationship. However, such approaches can not\nbe widely used in clinical due to the complex realities. To determine the\nmapping relationship, and achieve a initializtion post estimation of human body\nwithout auxiliary equipment or markers, proposed method applies image\nsegmentation and deep feature matching to directly match the 2D X-ray and 3D CT\nimages. As a result, the well-trained network can directly predict the spatial\ncorrespondence between arbitrary 2D X-ray and 3D CT. The experimental results\nshow that when combining our approach with the conventional approach, the\nachieved accuracy and speed can meet the basic clinical intervention needs, and\nit provides a new direction for intra-interventional registration.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 11:47:31 GMT"}, {"version": "v2", "created": "Thu, 13 May 2021 23:58:04 GMT"}, {"version": "v3", "created": "Wed, 19 May 2021 13:19:10 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Li", "Meng", ""], ["Lin", "Changyan", ""], ["Shu", "Lixia", ""], ["Pu", "Xin", ""], ["Chen", "Yu", ""], ["Wu", "Heng", ""], ["Li", "Jiasong", ""], ["Cao", "Hongshuai", ""]]}, {"id": "2104.14280", "submitter": "Negin Ghamsarian", "authors": "Negin Ghamsarian, Mario Taschwer, Doris Putzgruber-Adamitsch,\n  Stephanie Sarny, Klaus Schoeffmann", "title": "Relevance Detection in Cataract Surgery Videos by Spatio-Temporal Action\n  Localization", "comments": "8 pages, 4 figures, accepted at 5th International Conference on\n  Pattern Recognition (ICPR), Milan, Italy, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In cataract surgery, the operation is performed with the help of a\nmicroscope. Since the microscope enables watching real-time surgery by up to\ntwo people only, a major part of surgical training is conducted using the\nrecorded videos. To optimize the training procedure with the video content, the\nsurgeons require an automatic relevance detection approach. In addition to\nrelevance-based retrieval, these results can be further used for skill\nassessment and irregularity detection in cataract surgery videos. In this\npaper, a three-module framework is proposed to detect and classify the relevant\nphase segments in cataract videos. Taking advantage of an idle frame\nrecognition network, the video is divided into idle and action segments. To\nboost the performance in relevance detection, the cornea where the relevant\nsurgical actions are conducted is detected in all frames using Mask R-CNN. The\nspatiotemporally localized segments containing higher-resolution information\nabout the pupil texture and actions, and complementary temporal information\nfrom the same phase are fed into the relevance detection module. This module\nconsists of four parallel recurrent CNNs being responsible to detect four\nrelevant phases that have been defined with medical experts. The results will\nthen be integrated to classify the action phases as irrelevant or one of four\nrelevant phases. Experimental results reveal that the proposed approach\noutperforms static CNNs and different configurations of feature-based and\nend-to-end recurrent networks.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 12:01:08 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Ghamsarian", "Negin", ""], ["Taschwer", "Mario", ""], ["Putzgruber-Adamitsch", "Doris", ""], ["Sarny", "Stephanie", ""], ["Schoeffmann", "Klaus", ""]]}, {"id": "2104.14294", "submitter": "Mathilde Caron", "authors": "Mathilde Caron, Hugo Touvron, Ishan Misra, Herv\\'e J\\'egou, Julien\n  Mairal, Piotr Bojanowski, Armand Joulin", "title": "Emerging Properties in Self-Supervised Vision Transformers", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we question if self-supervised learning provides new\nproperties to Vision Transformer (ViT) that stand out compared to convolutional\nnetworks (convnets). Beyond the fact that adapting self-supervised methods to\nthis architecture works particularly well, we make the following observations:\nfirst, self-supervised ViT features contain explicit information about the\nsemantic segmentation of an image, which does not emerge as clearly with\nsupervised ViTs, nor with convnets. Second, these features are also excellent\nk-NN classifiers, reaching 78.3% top-1 on ImageNet with a small ViT. Our study\nalso underlines the importance of momentum encoder, multi-crop training, and\nthe use of small patches with ViTs. We implement our findings into a simple\nself-supervised method, called DINO, which we interpret as a form of\nself-distillation with no labels. We show the synergy between DINO and ViTs by\nachieving 80.1% top-1 on ImageNet in linear evaluation with ViT-Base.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 12:28:51 GMT"}, {"version": "v2", "created": "Mon, 24 May 2021 17:49:18 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Caron", "Mathilde", ""], ["Touvron", "Hugo", ""], ["Misra", "Ishan", ""], ["J\u00e9gou", "Herv\u00e9", ""], ["Mairal", "Julien", ""], ["Bojanowski", "Piotr", ""], ["Joulin", "Armand", ""]]}, {"id": "2104.14335", "submitter": "Oren Rippel", "authors": "Oren Rippel, Alexander G. Anderson, Kedar Tatwawadi, Sanjay Nair,\n  Craig Lytle, Lubomir Bourdev", "title": "ELF-VC: Efficient Learned Flexible-Rate Video Coding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While learned video codecs have demonstrated great promise, they have yet to\nachieve sufficient efficiency for practical deployment. In this work, we\npropose several novel ideas for learned video compression which allow for\nimproved performance for the low-latency mode (I- and P-frames only) along with\na considerable increase in computational efficiency. In this setting, for\nnatural videos our approach compares favorably across the entire R-D curve\nunder metrics PSNR, MS-SSIM and VMAF against all mainstream video standards\n(H.264, H.265, AV1) and all ML codecs. At the same time, our approach runs at\nleast 5x faster and has fewer parameters than all ML codecs which report these\nfigures.\n  Our contributions include a flexible-rate framework allowing a single model\nto cover a large and dense range of bitrates, at a negligible increase in\ncomputation and parameter count; an efficient backbone optimized for ML-based\ncodecs; and a novel in-loop flow prediction scheme which leverages prior\ninformation towards more efficient compression.\n  We benchmark our method, which we call ELF-VC (Efficient, Learned and\nFlexible Video Coding) on popular video test sets UVG and MCL-JCV under metrics\nPSNR, MS-SSIM and VMAF. For example, on UVG under PSNR, it reduces the BD-rate\nby 44% against H.264, 26% against H.265, 15% against AV1, and 35% against the\ncurrent best ML codec. At the same time, on an NVIDIA Titan V GPU our approach\nencodes/decodes VGA at 49/91 FPS, HD 720 at 19/35 FPS, and HD 1080 at 10/18\nFPS.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 17:50:35 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Rippel", "Oren", ""], ["Anderson", "Alexander G.", ""], ["Tatwawadi", "Kedar", ""], ["Nair", "Sanjay", ""], ["Lytle", "Craig", ""], ["Bourdev", "Lubomir", ""]]}, {"id": "2104.14349", "submitter": "Jing Qin", "authors": "Jing Qin and Joshua Ashley and Biyun Xie", "title": "Hand Gesture Recognition Based on a Nonconvex Regularization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG math.OC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recognition of hand gestures is one of the most fundamental tasks in\nhuman-robot interaction. Sparse representation based methods have been widely\nused due to their efficiency and low requirements on the training data.\nRecently, nonconvex regularization techniques including the $\\ell_{1-2}$\nregularization have been proposed in the image processing community to promote\nsparsity while achieving efficient performance. In this paper, we propose a\nvision-based hand gesture recognition model based on the $\\ell_{1-2}$\nregularization, which is solved by the alternating direction method of\nmultipliers (ADMM). Numerical experiments on binary and gray-scale data sets\nhave shown the effectiveness of this method in identifying hand gestures.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 13:58:55 GMT"}, {"version": "v2", "created": "Fri, 30 Apr 2021 04:42:36 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Qin", "Jing", ""], ["Ashley", "Joshua", ""], ["Xie", "Biyun", ""]]}, {"id": "2104.14353", "submitter": "Renato Krohling", "authors": "Breno Krohling, Pedro B. C. Castro, Andre G. C. Pacheco, and Renato A.\n  Krohling", "title": "A Smartphone based Application for Skin Cancer Classification Using Deep\n  Learning with Clinical Images and Lesion Information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Over the last decades, the incidence of skin cancer, melanoma and\nnon-melanoma, has increased at a continuous rate. In particular for melanoma,\nthe deadliest type of skin cancer, early detection is important to increase\npatient prognosis. Recently, deep neural networks (DNNs) have become viable to\ndeal with skin cancer detection. In this work, we present a smartphone-based\napplication to assist on skin cancer detection. This application is based on a\nConvolutional Neural Network(CNN) trained on clinical images and patients\ndemographics, both collected from smartphones. Also, as skin cancer datasets\nare imbalanced, we present an approach, based on the mutation operator of\nDifferential Evolution (DE) algorithm, to balance data. In this sense, beyond\nprovides a flexible tool to assist doctors on skin cancer screening phase, the\nmethod obtains promising results with a balanced accuracy of 85% and a recall\nof 96%.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 16:51:00 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Krohling", "Breno", ""], ["Castro", "Pedro B. C.", ""], ["Pacheco", "Andre G. C.", ""], ["Krohling", "Renato A.", ""]]}, {"id": "2104.14360", "submitter": "Yi Tang", "authors": "Yi Tang and Yuanman Li and Guoliang Xing", "title": "Video Salient Object Detection via Adaptive Local-Global Refinement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video salient object detection (VSOD) is an important task in many vision\napplications. Reliable VSOD requires to simultaneously exploit the information\nfrom both the spatial domain and the temporal domain. Most of the existing\nalgorithms merely utilize simple fusion strategies, such as addition and\nconcatenation, to merge the information from different domains. Despite their\nsimplicity, such fusion strategies may introduce feature redundancy, and also\nfail to fully exploit the relationship between multi-level features extracted\nfrom both spatial and temporal domains. In this paper, we suggest an adaptive\nlocal-global refinement framework for VSOD. Different from previous approaches,\nwe propose a local refinement architecture and a global one to refine the\nsimply fused features with different scopes, which can fully explore the local\ndependence and the global dependence of multi-level features. In addition, to\nemphasize the effective information and suppress the useless one, an adaptive\nweighting mechanism is designed based on graph convolutional neural network\n(GCN). We show that our weighting methodology can further exploit the feature\ncorrelations, thus driving the network to learn more discriminative feature\nrepresentation. Extensive experimental results on public video datasets\ndemonstrate the superiority of our method over the existing ones.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 14:14:11 GMT"}, {"version": "v2", "created": "Wed, 5 May 2021 01:47:17 GMT"}, {"version": "v3", "created": "Wed, 12 May 2021 07:07:37 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Tang", "Yi", ""], ["Li", "Yuanman", ""], ["Xing", "Guoliang", ""]]}, {"id": "2104.14374", "submitter": "Fuya Luo", "authors": "Fuya Luo, Yunhan Li, Guang Zeng, Peng Peng, Gang Wang, and Yongjie Li", "title": "Thermal Infrared Image Colorization for Nighttime Driving Scenes with\n  Top-Down Guided Attention", "comments": "A Manuscript Submitted to IEEE Transactions on Intelligent\n  Transpotation Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Benefitting from insensitivity to light and high penetration of foggy\nenvironments, infrared cameras are widely used for sensing in nighttime traffic\nscenes. However, the low contrast and lack of chromaticity of thermal infrared\n(TIR) images hinder the human interpretation and portability of high-level\ncomputer vision algorithms. Colorization to translate a nighttime TIR image\ninto a daytime color (NTIR2DC) image may be a promising way to facilitate\nnighttime scene perception. Despite recent impressive advances in image\ntranslation, semantic encoding entanglement and geometric distortion in the\nNTIR2DC task remain under-addressed. Hence, we propose a toP-down attEntion And\ngRadient aLignment based GAN, referred to as PearlGAN. A top-down guided\nattention module and an elaborate attentional loss are first designed to reduce\nthe semantic encoding ambiguity during translation. Then, a structured gradient\nalignment loss is introduced to encourage edge consistency between the\ntranslated and input images. In addition, pixel-level annotation is carried out\non a subset of FLIR and KAIST datasets to evaluate the semantic preservation\nperformance of multiple translation methods. Furthermore, a new metric is\ndevised to evaluate the geometric consistency in the translation process.\nExtensive experiments demonstrate the superiority of the proposed PearlGAN over\nother image translation methods for the NTIR2DC task. The source code and\nlabeled segmentation masks will be available at\n\\url{https://github.com/FuyaLuo/PearlGAN/}.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 14:35:25 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Luo", "Fuya", ""], ["Li", "Yunhan", ""], ["Zeng", "Guang", ""], ["Peng", "Peng", ""], ["Wang", "Gang", ""], ["Li", "Yongjie", ""]]}, {"id": "2104.14375", "submitter": "Kaili Wang", "authors": "Kaili Wang, Jose Oramas, Tinne Tuytelaars", "title": "MinMaxCAM: Improving object coverage for CAM-basedWeakly Supervised\n  Object Localization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the most common problems of weakly supervised object localization is\nthat of inaccurate object coverage. In the context of state-of-the-art methods\nbased on Class Activation Mapping, this is caused either by localization maps\nwhich focus, exclusively, on the most discriminative region of the objects of\ninterest or by activations occurring in background regions. To address these\ntwo problems, we propose two representation regularization mechanisms: Full\nRegion Regularizationwhich tries to maximize the coverage of the localization\nmap inside the object region, and Common Region Regularization which minimizes\nthe activations occurring in background regions. We evaluate the two\nregularizations on the ImageNet, CUB-200-2011 and OpenImages-segmentation\ndatasets, and show that the proposed regularizations tackle both problems,\noutperforming the state-of-the-art by a significant margin.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 14:39:53 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Wang", "Kaili", ""], ["Oramas", "Jose", ""], ["Tuytelaars", "Tinne", ""]]}, {"id": "2104.14385", "submitter": "Haoqing Wang", "authors": "Haoqing Wang, Zhi-Hong Deng", "title": "Cross-Domain Few-Shot Classification via Adversarial Task Augmentation", "comments": "Accepted by IJCAI-21 (the 30th International Joint Conference on\n  Artificial Intelligence) Main Track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Few-shot classification aims to recognize unseen classes with few labeled\nsamples from each class. Many meta-learning models for few-shot classification\nelaborately design various task-shared inductive bias (meta-knowledge) to solve\nsuch tasks, and achieve impressive performance. However, when there exists the\ndomain shift between the training tasks and the test tasks, the obtained\ninductive bias fails to generalize across domains, which degrades the\nperformance of the meta-learning models. In this work, we aim to improve the\nrobustness of the inductive bias through task augmentation. Concretely, we\nconsider the worst-case problem around the source task distribution, and\npropose the adversarial task augmentation method which can generate the\ninductive bias-adaptive 'challenging' tasks. Our method can be used as a simple\nplug-and-play module for various meta-learning models, and improve their\ncross-domain generalization capability. We conduct extensive experiments under\nthe cross-domain setting, using nine few-shot classification datasets:\nmini-ImageNet, CUB, Cars, Places, Plantae, CropDiseases, EuroSAT, ISIC and\nChestX. Experimental results show that our method can effectively improve the\nfew-shot classification performance of the meta-learning models under domain\nshift, and outperforms the existing works. Our code is available at\nhttps://github.com/Haoqing-Wang/CDFSL-ATA.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 14:51:53 GMT"}, {"version": "v2", "created": "Sun, 2 May 2021 10:40:33 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Wang", "Haoqing", ""], ["Deng", "Zhi-Hong", ""]]}, {"id": "2104.14403", "submitter": "Yilun Zhou", "authors": "Yilun Zhou, Serena Booth, Marco Tulio Ribeiro, Julie Shah", "title": "Do Feature Attribution Methods Correctly Attribute Features?", "comments": "21 pages, 20 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Feature attribution methods are exceedingly popular in interpretable machine\nlearning. They aim to compute the attribution of each input feature to\nrepresent its importance, but there is no consensus on the definition of\n\"attribution\", leading to many competing methods with little systematic\nevaluation. The lack of attribution ground truth further complicates\nevaluation, which has to rely on proxy metrics. To address this, we propose a\ndataset modification procedure such that models trained on the new dataset have\nground truth attribution available. We evaluate three methods: saliency maps,\nrationales, and attention. We identify their deficiencies and add a new\nperspective to the growing body of evidence questioning their correctness and\nreliability in the wild. Our evaluation approach is model-agnostic and can be\nused to assess future feature attribution method proposals as well. Code is\navailable at https://github.com/YilunZhou/feature-attribution-evaluation.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 20:35:30 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Zhou", "Yilun", ""], ["Booth", "Serena", ""], ["Ribeiro", "Marco Tulio", ""], ["Shah", "Julie", ""]]}, {"id": "2104.14420", "submitter": "Panyanat Aonpong", "authors": "Panyanat Aonpong, Yutaro Iwamoto, Xian-Hua Han, Lanfen Lin, Yen-Wei\n  Chen", "title": "Genotype-Guided Radiomics Signatures for Recurrence Prediction of\n  Non-Small-Cell Lung Cancer", "comments": "11 pages, 9 figures, 4 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Non-small cell lung cancer (NSCLC) is a serious disease and has a high\nrecurrence rate after the surgery. Recently, many machine learning methods have\nbeen proposed for recurrence prediction. The methods using gene data have high\nprediction accuracy but require high cost. Although the radiomics signatures\nusing only CT image are not expensive, its accuracy is relatively low. In this\npaper, we propose a genotype-guided radiomics method (GGR) for obtaining high\nprediction accuracy with low cost. We used a public radiogenomics dataset of\nNSCLC, which includes CT images and gene data. The proposed method is a\ntwo-step method, which consists of two models. The first model is a gene\nestimation model, which is used to estimate the gene expression from radiomics\nfeatures and deep features extracted from computer tomography (CT) image. The\nsecond model is used to predict the recurrence using the estimated gene\nexpression data. The proposed GGR method designed based on hybrid features\nwhich is combination of handcrafted-based and deep learning-based. The\nexperiments demonstrated that the prediction accuracy can be improved\nsignificantly from 78.61% (existing radiomics method) and 79.14% (deep learning\nmethod) to 83.28% by the proposed GGR.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 15:34:50 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Aonpong", "Panyanat", ""], ["Iwamoto", "Yutaro", ""], ["Han", "Xian-Hua", ""], ["Lin", "Lanfen", ""], ["Chen", "Yen-Wei", ""]]}, {"id": "2104.14430", "submitter": "Xin Guo", "authors": "Xin Guo, Zhongming Jin, Chong Chen, Helei Nie, Jianqiang Huang, Deng\n  Cai, Xiaofei He, Xiansheng Hua", "title": "Discriminative-Generative Dual Memory Video Anomaly Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, people tried to use a few anomalies for video anomaly detection\n(VAD) instead of only normal data during the training process. A side effect of\ndata imbalance occurs when a few abnormal data face a vast number of normal\ndata. The latest VAD works use triplet loss or data re-sampling strategy to\nlessen this problem. However, there is still no elaborately designed structure\nfor discriminative VAD with a few anomalies. In this paper, we propose a\nDiscRiminative-gEnerative duAl Memory (DREAM) anomaly detection model to take\nadvantage of a few anomalies and solve data imbalance. We use two shallow\ndiscriminators to tighten the normal feature distribution boundary along with a\ngenerator for the next frame prediction. Further, we propose a dual memory\nmodule to obtain a sparse feature representation in both normality and\nabnormality space. As a result, DREAM not only solves the data imbalance\nproblem but also learn a reasonable feature space. Further theoretical analysis\nshows that our DREAM also works for the unknown anomalies. Comparing with the\nprevious methods on UCSD Ped1, UCSD Ped2, CUHK Avenue, and ShanghaiTech, our\nmodel outperforms all the baselines with no extra parameters. The ablation\nstudy demonstrates the effectiveness of our dual memory module and\ndiscriminative-generative network.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 15:49:01 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Guo", "Xin", ""], ["Jin", "Zhongming", ""], ["Chen", "Chong", ""], ["Nie", "Helei", ""], ["Huang", "Jianqiang", ""], ["Cai", "Deng", ""], ["He", "Xiaofei", ""], ["Hua", "Xiansheng", ""]]}, {"id": "2104.14435", "submitter": "Changshun Wu", "authors": "Changshun Wu, Yli\\`es Falcone, Saddek Bensalem", "title": "Customizable Reference Runtime Monitoring of Neural Networks using\n  Resolution Boxes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classification neural networks fail to detect inputs that do not fall inside\nthe classes they have been trained for. Runtime monitoring techniques on the\nneuron activation pattern can be used to detect such inputs. We present an\napproach for monitoring classification systems via data abstraction. Data\nabstraction relies on the notion of box with a resolution. Box-based\nabstraction consists in representing a set of values by its minimal and maximal\nvalues in each dimension. We augment boxes with a notion of resolution and\ndefine their clustering coverage, which is intuitively a quantitative metric\nthat indicates the abstraction quality. This allows studying the effect of\ndifferent clustering parameters on the constructed boxes and estimating an\ninterval of sub-optimal parameters. Moreover, we automatically construct\nmonitors that leverage both the correct and incorrect behaviors of a system.\nThis allows checking the size of the monitor abstractions and analyzing the\nseparability of the network. Monitors are obtained by combining the\nsub-monitors of each class of the system placed at some selected layers. Our\nexperiments demonstrate the effectiveness of our clustering coverage estimation\nand show how to assess the effectiveness and precision of monitors according to\nthe selected clustering parameter and monitored layers.\n", "versions": [{"version": "v1", "created": "Sun, 25 Apr 2021 21:58:02 GMT"}, {"version": "v2", "created": "Mon, 12 Jul 2021 12:21:53 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Wu", "Changshun", ""], ["Falcone", "Yli\u00e8s", ""], ["Bensalem", "Saddek", ""]]}, {"id": "2104.14466", "submitter": "Linguo Li", "authors": "Linguo Li, Minsi Wang, Bingbing Ni, Hang Wang, Jiancheng Yang, Wenjun\n  Zhang", "title": "3D Human Action Representation Learning via Cross-View Consistency\n  Pursuit", "comments": "Accepted in CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a Cross-view Contrastive Learning framework for\nunsupervised 3D skeleton-based action Representation (CrosSCLR), by leveraging\nmulti-view complementary supervision signal. CrosSCLR consists of both\nsingle-view contrastive learning (SkeletonCLR) and cross-view consistent\nknowledge mining (CVC-KM) modules, integrated in a collaborative learning\nmanner. It is noted that CVC-KM works in such a way that high-confidence\npositive/negative samples and their distributions are exchanged among views\naccording to their embedding similarity, ensuring cross-view consistency in\nterms of contrastive context, i.e., similar distributions. Extensive\nexperiments show that CrosSCLR achieves remarkable action recognition results\non NTU-60 and NTU-120 datasets under unsupervised settings, with observed\nhigher-quality action representations. Our code is available at\nhttps://github.com/LinguoLi/CrosSCLR.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 16:29:41 GMT"}, {"version": "v2", "created": "Sat, 1 May 2021 15:30:07 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Li", "Linguo", ""], ["Wang", "Minsi", ""], ["Ni", "Bingbing", ""], ["Wang", "Hang", ""], ["Yang", "Jiancheng", ""], ["Zhang", "Wenjun", ""]]}, {"id": "2104.14467", "submitter": "Frigyes Viktor Arthur", "authors": "Frigyes Viktor Arthur and Tam\\'as G\\'abor Csap\\'o", "title": "Towards a practical lip-to-speech conversion system using deep neural\n  networks and mobile application frontend", "comments": "10 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Articulatory-to-acoustic (forward) mapping is a technique to predict speech\nusing various articulatory acquisition techniques as input (e.g. ultrasound\ntongue imaging, MRI, lip video). The advantage of lip video is that it is\neasily available and affordable: most modern smartphones have a front camera.\nThere are already a few solutions for lip-to-speech synthesis, but they mostly\nconcentrate on offline training and inference. In this paper, we propose a\nsystem built from a backend for deep neural network training and inference and\na fronted as a form of a mobile application. Our initial evaluation shows that\nthe scenario is feasible: a top-5 classification accuracy of 74% is combined\nwith feedback from the mobile application user, making sure that the speaking\nimpaired might be able to communicate with this solution.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 16:30:24 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Arthur", "Frigyes Viktor", ""], ["Csap\u00f3", "Tam\u00e1s G\u00e1bor", ""]]}, {"id": "2104.14506", "submitter": "Guang Yang A", "authors": "Qinghao Ye and Jun Xia and Guang Yang", "title": "Explainable AI For COVID-19 CT Classifiers: An Initial Comparison Study", "comments": "6 pages, 4 figures, IEEE CBMS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial Intelligence (AI) has made leapfrogs in development across all the\nindustrial sectors especially when deep learning has been introduced. Deep\nlearning helps to learn the behaviour of an entity through methods of\nrecognising and interpreting patterns. Despite its limitless potential, the\nmystery is how deep learning algorithms make a decision in the first place.\nExplainable AI (XAI) is the key to unlocking AI and the black-box for deep\nlearning. XAI is an AI model that is programmed to explain its goals, logic,\nand decision making so that the end users can understand. The end users can be\ndomain experts, regulatory agencies, managers and executive board members, data\nscientists, users that use AI, with or without awareness, or someone who is\naffected by the decisions of an AI model. Chest CT has emerged as a valuable\ntool for the clinical diagnostic and treatment management of the lung diseases\nassociated with COVID-19. AI can support rapid evaluation of CT scans to\ndifferentiate COVID-19 findings from other lung diseases. However, how these AI\ntools or deep learning algorithms reach such a decision and which are the most\ninfluential features derived from these neural networks with typically deep\nlayers are not clear. The aim of this study is to propose and develop XAI\nstrategies for COVID-19 classification models with an investigation of\ncomparison. The results demonstrate promising quantification and qualitative\nvisualisations that can further enhance the clinician's understanding and\ndecision making with more granular information from the results given by the\nlearned XAI models.\n", "versions": [{"version": "v1", "created": "Sun, 25 Apr 2021 23:39:14 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Ye", "Qinghao", ""], ["Xia", "Jun", ""], ["Yang", "Guang", ""]]}, {"id": "2104.14528", "submitter": "Haoyuan Chen", "authors": "Haoyuan Chen, Chen Li, Xiaoyan Li, Ge Wang, Weiming Hu, Yixin Li,\n  Wanli Liu, Changhao Sun, Yudong Yao, Yueyang Teng, Marcin Grzegorzek", "title": "GasHis-Transformer: A Multi-scale Visual Transformer Approach for\n  Gastric Histopathology Image Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing deep learning methods for diagnosis of gastric cancer commonly use\nconvolutional neural network. Recently, the Visual Transformer has attracted\ngreat attention because of its performance and efficiency, but its applications\nare mostly in the field of computer vision. In this paper, a multi-scale visual\ntransformer model, referred to as GasHis-Transformer, is proposed for Gastric\nHistopathological Image Classification (GHIC), which enables the automatic\nclassification of microscopic gastric images into abnormal and normal cases.\nThe GasHis-Transformer model consists of two key modules: A global information\nmodule and a local information module to extract histopathological features\neffectively. In our experiments, a public hematoxylin and eosin (H&E) stained\ngastric histopathological dataset with 280 abnormal and normal images are\ndivided into training, validation and test sets by a ratio of 1 : 1 : 2. The\nGasHis-Transformer model is applied to estimate precision, recall, F1-score and\naccuracy on the test set of gastric histopathological dataset as 98.0%, 100.0%,\n96.0% and 98.0%, respectively. Furthermore, a critical study is conducted to\nevaluate the robustness of GasHis-Transformer, where ten different noises\nincluding four adversarial attack and six conventional image noises are added.\nIn addition, a clinically meaningful study is executed to test the\ngastrointestinal cancer identification performance of GasHis-Transformer with\n620 abnormal images and achieves 96.8% accuracy. Finally, a comparative study\nis performed to test the generalizability with both H&E and immunohistochemical\nstained images on a lymphoma image dataset and a breast cancer dataset,\nproducing comparable F1-scores (85.6% and 82.8%) and accuracies (83.9% and\n89.4%), respectively. In conclusion, GasHisTransformer demonstrates high\nclassification performance and shows its significant potential in the GHIC\ntask.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 17:46:00 GMT"}, {"version": "v2", "created": "Fri, 30 Apr 2021 01:58:26 GMT"}, {"version": "v3", "created": "Tue, 25 May 2021 02:15:07 GMT"}, {"version": "v4", "created": "Tue, 8 Jun 2021 17:07:47 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Chen", "Haoyuan", ""], ["Li", "Chen", ""], ["Li", "Xiaoyan", ""], ["Wang", "Ge", ""], ["Hu", "Weiming", ""], ["Li", "Yixin", ""], ["Liu", "Wanli", ""], ["Sun", "Changhao", ""], ["Yao", "Yudong", ""], ["Teng", "Yueyang", ""], ["Grzegorzek", "Marcin", ""]]}, {"id": "2104.14535", "submitter": "Shelly Sheynin", "authors": "Shelly Sheynin, Sagie Benaim and Lior Wolf", "title": "A Hierarchical Transformation-Discriminating Generative Model for Few\n  Shot Anomaly Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anomaly detection, the task of identifying unusual samples in data, often\nrelies on a large set of training samples. In this work, we consider the\nsetting of few-shot anomaly detection in images, where only a few images are\ngiven at training. We devise a hierarchical generative model that captures the\nmulti-scale patch distribution of each training image. We further enhance the\nrepresentation of our model by using image transformations and optimize\nscale-specific patch-discriminators to distinguish between real and fake\npatches of the image, as well as between different transformations applied to\nthose patches. The anomaly score is obtained by aggregating the patch-based\nvotes of the correct transformation across scales and image regions. We\ndemonstrate the superiority of our method on both the one-shot and few-shot\nsettings, on the datasets of Paris, CIFAR10, MNIST and FashionMNIST as well as\nin the setting of defect detection on MVTec. In all cases, our method\noutperforms the recent baseline methods.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 17:49:48 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Sheynin", "Shelly", ""], ["Benaim", "Sagie", ""], ["Wolf", "Lior", ""]]}, {"id": "2104.14540", "submitter": "Jamie Watson", "authors": "Jamie Watson, Oisin Mac Aodha, Victor Prisacariu, Gabriel Brostow,\n  Michael Firman", "title": "The Temporal Opportunist: Self-Supervised Multi-Frame Monocular Depth", "comments": "CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-supervised monocular depth estimation networks are trained to predict\nscene depth using nearby frames as a supervision signal during training.\nHowever, for many applications, sequence information in the form of video\nframes is also available at test time. The vast majority of monocular networks\ndo not make use of this extra signal, thus ignoring valuable information that\ncould be used to improve the predicted depth. Those that do, either use\ncomputationally expensive test-time refinement techniques or off-the-shelf\nrecurrent networks, which only indirectly make use of the geometric information\nthat is inherently available.\n  We propose ManyDepth, an adaptive approach to dense depth estimation that can\nmake use of sequence information at test time, when it is available. Taking\ninspiration from multi-view stereo, we propose a deep end-to-end cost volume\nbased approach that is trained using self-supervision only. We present a novel\nconsistency loss that encourages the network to ignore the cost volume when it\nis deemed unreliable, e.g. in the case of moving objects, and an augmentation\nscheme to cope with static cameras. Our detailed experiments on both KITTI and\nCityscapes show that we outperform all published self-supervised baselines,\nincluding those that use single or multiple frames at test time.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 17:53:42 GMT"}, {"version": "v2", "created": "Wed, 14 Jul 2021 10:08:51 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Watson", "Jamie", ""], ["Mac Aodha", "Oisin", ""], ["Prisacariu", "Victor", ""], ["Brostow", "Gabriel", ""], ["Firman", "Michael", ""]]}, {"id": "2104.14544", "submitter": "Deqing Sun", "authors": "Deqing Sun, Daniel Vlasic, Charles Herrmann, Varun Jampani, Michael\n  Krainin, Huiwen Chang, Ramin Zabih, William T. Freeman, Ce Liu", "title": "AutoFlow: Learning a Better Training Set for Optical Flow", "comments": "CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Synthetic datasets play a critical role in pre-training CNN models for\noptical flow, but they are painstaking to generate and hard to adapt to new\napplications. To automate the process, we present AutoFlow, a simple and\neffective method to render training data for optical flow that optimizes the\nperformance of a model on a target dataset. AutoFlow takes a layered approach\nto render synthetic data, where the motion, shape, and appearance of each layer\nare controlled by learnable hyperparameters. Experimental results show that\nAutoFlow achieves state-of-the-art accuracy in pre-training both PWC-Net and\nRAFT. Our code and data are available at https://autoflow-google.github.io .\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 17:55:23 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Sun", "Deqing", ""], ["Vlasic", "Daniel", ""], ["Herrmann", "Charles", ""], ["Jampani", "Varun", ""], ["Krainin", "Michael", ""], ["Chang", "Huiwen", ""], ["Zabih", "Ramin", ""], ["Freeman", "William T.", ""], ["Liu", "Ce", ""]]}, {"id": "2104.14545", "submitter": "Houwen Peng", "authors": "Bin Yan, Houwen Peng, Kan Wu, Dong Wang, Jianlong Fu, Huchuan Lu", "title": "LightTrack: Finding Lightweight Neural Networks for Object Tracking via\n  One-Shot Architecture Search", "comments": "Accepted by CVPR 2021, Github:\n  https://github.com/researchmm/LightTrack", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object tracking has achieved significant progress over the past few years.\nHowever, state-of-the-art trackers become increasingly heavy and expensive,\nwhich limits their deployments in resource-constrained applications. In this\nwork, we present LightTrack, which uses neural architecture search (NAS) to\ndesign more lightweight and efficient object trackers. Comprehensive\nexperiments show that our LightTrack is effective. It can find trackers that\nachieve superior performance compared to handcrafted SOTA trackers, such as\nSiamRPN++ and Ocean, while using much fewer model Flops and parameters.\nMoreover, when deployed on resource-constrained mobile chipsets, the discovered\ntrackers run much faster. For example, on Snapdragon 845 Adreno GPU, LightTrack\nruns $12\\times$ faster than Ocean, while using $13\\times$ fewer parameters and\n$38\\times$ fewer Flops. Such improvements might narrow the gap between academic\nmodels and industrial deployments in object tracking task. LightTrack is\nreleased at https://github.com/researchmm/LightTrack.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 17:55:24 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Yan", "Bin", ""], ["Peng", "Houwen", ""], ["Wu", "Kan", ""], ["Wang", "Dong", ""], ["Fu", "Jianlong", ""], ["Lu", "Huchuan", ""]]}, {"id": "2104.14547", "submitter": "Anjana Deva Prasad", "authors": "Anjana Deva Prasad, Aditya Balu, Harshil Shah, Soumik Sarkar, Adarsh\n  Krishnamurthy", "title": "NURBS-Diff: A Differentiable NURBS Layer for Machine Learning CAD\n  Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Recent deep-learning-based techniques for the reconstruction of geometries\nfrom different input representations such as images and point clouds have been\ninstrumental in advancing research in geometric machine learning. Most of these\ntechniques rely on a triangular mesh representation for representing the\ngeometry, with very recent attempts in using B-splines. While Non-Uniform\nRational B-splines (NURBS) are the de facto standard in the CAD industry,\nminimal efforts have been made to bridge the gap between deep-learning\nframeworks and the NURBS representation for geometry. The backbone of modern\ndeep learning techniques is the use of a fully automatic differentiable\ndefinition for each mathematical operation to enable backpropagation of losses\nwhile training. In order to integrate the NURBS representation of CAD models\nwith deep learning methods, we propose a differentiable NURBS layer for\nevaluating the curve or surface given a set of NURBS parameters. We have\ndeveloped a NURBS layer defining the forward and backward pass required for\nautomatic differentiation. Our implementation is GPU accelerated and is\ndirectly integrated with PyTorch, a popular deep learning framework. We\ndemonstrate the efficacy of our NURBS layer by automatically incorporating it\nwith the stochastic gradient descent algorithm and performing CAD operations\nsuch as curve or surface fitting and surface offsetting. Further, we show its\nutility in deep learning applications such as point cloud reconstruction and\nstructural modeling and analysis of shell structures such as heart valves.\nThese examples show that our layer has better performance for certain deep\nlearning frameworks and can be directly integrated with any CAD deep-learning\nframework that require the use of NURBS.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 17:56:01 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Prasad", "Anjana Deva", ""], ["Balu", "Aditya", ""], ["Shah", "Harshil", ""], ["Sarkar", "Soumik", ""], ["Krishnamurthy", "Adarsh", ""]]}, {"id": "2104.14548", "submitter": "Debidatta Dwibedi", "authors": "Debidatta Dwibedi, Yusuf Aytar, Jonathan Tompson, Pierre Sermanet,\n  Andrew Zisserman", "title": "With a Little Help from My Friends: Nearest-Neighbor Contrastive\n  Learning of Visual Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-supervised learning algorithms based on instance discrimination train\nencoders to be invariant to pre-defined transformations of the same instance.\nWhile most methods treat different views of the same image as positives for a\ncontrastive loss, we are interested in using positives from other instances in\nthe dataset. Our method, Nearest-Neighbor Contrastive Learning of visual\nRepresentations (NNCLR), samples the nearest neighbors from the dataset in the\nlatent space, and treats them as positives. This provides more semantic\nvariations than pre-defined transformations.\n  We find that using the nearest-neighbor as positive in contrastive losses\nimproves performance significantly on ImageNet classification, from 71.7% to\n75.6%, outperforming previous state-of-the-art methods. On semi-supervised\nlearning benchmarks we improve performance significantly when only 1% ImageNet\nlabels are available, from 53.8% to 56.5%. On transfer learning benchmarks our\nmethod outperforms state-of-the-art methods (including supervised learning with\nImageNet) on 8 out of 12 downstream datasets. Furthermore, we demonstrate\nempirically that our method is less reliant on complex data augmentations. We\nsee a relative reduction of only 2.1% ImageNet Top-1 accuracy when we train\nusing only random crops.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 17:56:08 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Dwibedi", "Debidatta", ""], ["Aytar", "Yusuf", ""], ["Tompson", "Jonathan", ""], ["Sermanet", "Pierre", ""], ["Zisserman", "Andrew", ""]]}, {"id": "2104.14551", "submitter": "Lucy Chai", "authors": "Lucy Chai, Jun-Yan Zhu, Eli Shechtman, Phillip Isola, Richard Zhang", "title": "Ensembling with Deep Generative Views", "comments": "CVPR 2021 camera ready version; code available at\n  https://github.com/chail/gan-ensembling", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent generative models can synthesize \"views\" of artificial images that\nmimic real-world variations, such as changes in color or pose, simply by\nlearning from unlabeled image collections. Here, we investigate whether such\nviews can be applied to real images to benefit downstream analysis tasks such\nas image classification. Using a pretrained generator, we first find the latent\ncode corresponding to a given real input image. Applying perturbations to the\ncode creates natural variations of the image, which can then be ensembled\ntogether at test-time. We use StyleGAN2 as the source of generative\naugmentations and investigate this setup on classification tasks involving\nfacial attributes, cat faces, and cars. Critically, we find that several design\ndecisions are required towards making this process work; the perturbation\nprocedure, weighting between the augmentations and original image, and training\nthe classifier on synthesized images can all impact the result. Currently, we\nfind that while test-time ensembling with GAN-based augmentations can offer\nsome small improvements, the remaining bottlenecks are the efficiency and\naccuracy of the GAN reconstructions, coupled with classifier sensitivities to\nartifacts in GAN-generated images.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 17:58:35 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Chai", "Lucy", ""], ["Zhu", "Jun-Yan", ""], ["Shechtman", "Eli", ""], ["Isola", "Phillip", ""], ["Zhang", "Richard", ""]]}, {"id": "2104.14553", "submitter": "Dmitriy Smirnov", "authors": "Dmitriy Smirnov, Michael Gharbi, Matthew Fisher, Vitor Guizilini,\n  Alexei A. Efros, Justin Solomon", "title": "MarioNette: Self-Supervised Sprite Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Visual content often contains recurring elements. Text is made up of glyphs\nfrom the same font, animations, such as cartoons or video games, are composed\nof sprites moving around the screen, and natural videos frequently have\nrepeated views of objects. In this paper, we propose a deep learning approach\nfor obtaining a graphically disentangled representation of recurring elements\nin a completely self-supervised manner. By jointly learning a dictionary of\ntexture patches and training a network that places them onto a canvas, we\neffectively deconstruct sprite-based content into a sparse, consistent, and\ninterpretable representation that can be easily used in downstream tasks. Our\nframework offers a promising approach for discovering recurring patterns in\nimage collections without supervision.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 17:59:01 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Smirnov", "Dmitriy", ""], ["Gharbi", "Michael", ""], ["Fisher", "Matthew", ""], ["Guizilini", "Vitor", ""], ["Efros", "Alexei A.", ""], ["Solomon", "Justin", ""]]}, {"id": "2104.14554", "submitter": "L\\'eo Lebrat", "authors": "L\\'eo Lebrat, Rodrigo Santa Cruz, Clinton Fookes, Olivier Salvado", "title": "MongeNet: Efficient Sampler for Geometric Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in geometric deep-learning introduce complex computational\nchallenges for evaluating the distance between meshes. From a mesh model, point\nclouds are necessary along with a robust distance metric to assess surface\nquality or as part of the loss function for training models. Current methods\noften rely on a uniform random mesh discretization, which yields irregular\nsampling and noisy distance estimation. In this paper we introduce MongeNet, a\nfast and optimal transport based sampler that allows for an accurate\ndiscretization of a mesh with better approximation properties. We compare our\nmethod to the ubiquitous random uniform sampling and show that the\napproximation error is almost half with a very small computational overhead.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 17:59:01 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Lebrat", "L\u00e9o", ""], ["Cruz", "Rodrigo Santa", ""], ["Fookes", "Clinton", ""], ["Salvado", "Olivier", ""]]}, {"id": "2104.14556", "submitter": "Zhiheng Li", "authors": "Zhiheng Li, Chenliang Xu", "title": "Discover the Unknown Biased Attribute of an Image Classifier", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent works find that AI algorithms learn biases from data. Therefore, it is\nurgent and vital to identify biases in AI algorithms. However, the previous\nbias identification pipeline overly relies on human experts to conjecture\npotential biases (e.g., gender), which may neglect other underlying biases not\nrealized by humans. To help human experts better find the AI algorithms'\nbiases, we study a new problem in this work -- for a classifier that predicts a\ntarget attribute of the input image, discover its unknown biased attribute.\n  To solve this challenging problem, we use a hyperplane in the generative\nmodel's latent space to represent an image attribute; thus, the original\nproblem is transformed to optimizing the hyperplane's normal vector and offset.\nWe propose a novel total-variation loss within this framework as the objective\nfunction and a new orthogonalization penalty as a constraint. The latter\nprevents trivial solutions in which the discovered biased attribute is\nidentical with the target or one of the known-biased attributes. Extensive\nexperiments on both disentanglement datasets and real-world datasets show that\nour method can discover biased attributes and achieve better disentanglement\nw.r.t. target attributes. Furthermore, the qualitative results show that our\nmethod can discover unnoticeable biased attributes for various object and scene\nclassifiers, proving our method's generalizability for detecting biased\nattributes in diverse domains of images. The code is available at\nhttps://git.io/J3kMh.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 17:59:30 GMT"}, {"version": "v2", "created": "Tue, 8 Jun 2021 17:59:55 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Li", "Zhiheng", ""], ["Xu", "Chenliang", ""]]}, {"id": "2104.14557", "submitter": "Moustafa Meshry", "authors": "Moustafa Meshry, Saksham Suri, Larry S. Davis, Abhinav Shrivastava", "title": "Learned Spatial Representations for Few-shot Talking-Head Synthesis", "comments": "http://www.cs.umd.edu/~mmeshry/projects/lsr/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel approach for few-shot talking-head synthesis. While recent\nworks in neural talking heads have produced promising results, they can still\nproduce images that do not preserve the identity of the subject in source\nimages. We posit this is a result of the entangled representation of each\nsubject in a single latent code that models 3D shape information, identity\ncues, colors, lighting and even background details. In contrast, we propose to\nfactorize the representation of a subject into its spatial and style\ncomponents. Our method generates a target frame in two steps. First, it\npredicts a dense spatial layout for the target image. Second, an image\ngenerator utilizes the predicted layout for spatial denormalization and\nsynthesizes the target frame. We experimentally show that this disentangled\nrepresentation leads to a significant improvement over previous methods, both\nquantitatively and qualitatively.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 17:59:42 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Meshry", "Moustafa", ""], ["Suri", "Saksham", ""], ["Davis", "Larry S.", ""], ["Shrivastava", "Abhinav", ""]]}, {"id": "2104.14558", "submitter": "Christoph Feichtenhofer", "authors": "Christoph Feichtenhofer, Haoqi Fan, Bo Xiong, Ross Girshick, Kaiming\n  He", "title": "A Large-Scale Study on Unsupervised Spatiotemporal Representation\n  Learning", "comments": "CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a large-scale study on unsupervised spatiotemporal representation\nlearning from videos. With a unified perspective on four recent image-based\nframeworks, we study a simple objective that can easily generalize all these\nmethods to space-time. Our objective encourages temporally-persistent features\nin the same video, and in spite of its simplicity, it works surprisingly well\nacross: (i) different unsupervised frameworks, (ii) pre-training datasets,\n(iii) downstream datasets, and (iv) backbone architectures. We draw a series of\nintriguing observations from this study, e.g., we discover that encouraging\nlong-spanned persistency can be effective even if the timespan is 60 seconds.\nIn addition to state-of-the-art results in multiple benchmarks, we report a few\npromising cases in which unsupervised pre-training can outperform its\nsupervised counterpart. Code is made available at\nhttps://github.com/facebookresearch/SlowFast\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 17:59:53 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Feichtenhofer", "Christoph", ""], ["Fan", "Haoqi", ""], ["Xiong", "Bo", ""], ["Girshick", "Ross", ""], ["He", "Kaiming", ""]]}, {"id": "2104.14559", "submitter": "Fangzhou Han", "authors": "Fangzhou Han, Shuquan Ye, Mingming He, Menglei Chai and Jing Liao", "title": "Exemplar-Based 3D Portrait Stylization", "comments": "Project page: https://halfjoe.github.io/projs/3DPS/index.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exemplar-based portrait stylization is widely attractive and highly desired.\nDespite recent successes, it remains challenging, especially when considering\nboth texture and geometric styles. In this paper, we present the first\nframework for one-shot 3D portrait style transfer, which can generate 3D face\nmodels with both the geometry exaggerated and the texture stylized while\npreserving the identity from the original content. It requires only one\narbitrary style image instead of a large set of training examples for a\nparticular style, provides geometry and texture outputs that are fully\nparameterized and disentangled, and enables further graphics applications with\nthe 3D representations. The framework consists of two stages. In the first\ngeometric style transfer stage, we use facial landmark translation to capture\nthe coarse geometry style and guide the deformation of the dense 3D face\ngeometry. In the second texture style transfer stage, we focus on performing\nstyle transfer on the canonical texture by adopting a differentiable renderer\nto optimize the texture in a multi-view framework. Experiments show that our\nmethod achieves robustly good results on different artistic styles and\noutperforms existing methods. We also demonstrate the advantages of our method\nvia various 2D and 3D graphics applications. Project page is\nhttps://halfjoe.github.io/projs/3DPS/index.html.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 17:59:54 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Han", "Fangzhou", ""], ["Ye", "Shuquan", ""], ["He", "Mingming", ""], ["Chai", "Menglei", ""], ["Liao", "Jing", ""]]}, {"id": "2104.14575", "submitter": "Tom Monnier", "authors": "Tom Monnier, Elliot Vincent, Jean Ponce, Mathieu Aubry", "title": "Unsupervised Layered Image Decomposition into Object Prototypes", "comments": "Project webpage: https://imagine.enpc.fr/~monniert/DTI-Sprites", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an unsupervised learning framework for decomposing images into\nlayers of automatically discovered object models. Contrary to recent approaches\nthat model image layers with autoencoder networks, we represent them as\nexplicit transformations of a small set of prototypical images. Our model has\nthree main components: (i) a set of object prototypes in the form of learnable\nimages with a transparency channel, which we refer to as sprites; (ii)\ndifferentiable parametric functions predicting occlusions and transformation\nparameters necessary to instantiate the sprites in a given image; (iii) a\nlayered image formation model with occlusion for compositing these instances\ninto complete images including background. By jointly learning the sprites and\nocclusion/transformation predictors to reconstruct images, our approach not\nonly yields accurate layered image decompositions, but also identifies object\ncategories and instance parameters. We first validate our approach by providing\nresults on par with the state of the art on standard multi-object synthetic\nbenchmarks (Tetrominoes, Multi-dSprites, CLEVR6). We then demonstrate the\napplicability of our model to real images in tasks that include clustering\n(SVHN, GTSRB), cosegmentation (Weizmann Horse) and object discovery from\nunfiltered social network images. To the best of our knowledge, our approach is\nthe first layered image decomposition algorithm that learns an explicit and\nshared concept of object type, and is robust enough to be applied to real\nimages.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 18:02:01 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Monnier", "Tom", ""], ["Vincent", "Elliot", ""], ["Ponce", "Jean", ""], ["Aubry", "Mathieu", ""]]}, {"id": "2104.14586", "submitter": "Fangzheng Lin Mr.", "authors": "Fangzheng Lin, Jiesheng Yang, Jiangpeng Shu, Raimar J. Scherer", "title": "Crack Semantic Segmentation using the U-Net with Full Attention Strategy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structures suffer from the emergence of cracks, therefore, crack detection is\nalways an issue with much concern in structural health monitoring. Along with\nthe rapid progress of deep learning technology, image semantic segmentation, an\nactive research field, offers another solution, which is more effective and\nintelligent, to crack detection Through numerous artificial neural networks\nhave been developed to address the preceding issue, corresponding explorations\nare never stopped improving the quality of crack detection. This paper presents\na novel artificial neural network architecture named Full Attention U-net for\nimage semantic segmentation. The proposed architecture leverages the U-net as\nthe backbone and adopts the Full Attention Strategy, which is a synthesis of\nthe attention mechanism and the outputs from each encoding layer in skip\nconnection. Subject to the hardware in training, the experiments are composed\nof verification and validation. In verification, 4 networks including U-net,\nAttention U-net, Advanced Attention U-net, and Full Attention U-net are tested\nthrough cell images for a competitive study. With respect to mean\nintersection-over-unions and clarity of edge identification, the Full Attention\nU-net performs best in verification, and is hence applied for crack semantic\nsegmentation in validation to demonstrate its effectiveness.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 18:16:40 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Lin", "Fangzheng", ""], ["Yang", "Jiesheng", ""], ["Shu", "Jiangpeng", ""], ["Scherer", "Raimar J.", ""]]}, {"id": "2104.14623", "submitter": "Alexander Wong", "authors": "Xiaoyu Wen, Mahmoud Famouri, Andrew Hryniowski, and Alexander Wong", "title": "AttendSeg: A Tiny Attention Condenser Neural Network for Semantic\n  Segmentation on the Edge", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, we introduce \\textbf{AttendSeg}, a low-precision, highly\ncompact deep neural network tailored for on-device semantic segmentation.\nAttendSeg possesses a self-attention network architecture comprising of\nlight-weight attention condensers for improved spatial-channel selective\nattention at a very low complexity. The unique macro-architecture and\nmicro-architecture design properties of AttendSeg strike a strong balance\nbetween representational power and efficiency, achieved via a machine-driven\ndesign exploration strategy tailored specifically for the task at hand.\nExperimental results demonstrated that the proposed AttendSeg can achieve\nsegmentation accuracy comparable to much larger deep neural networks with\ngreater complexity while possessing a significantly lower architecture and\ncomputational complexity (requiring as much as >27x fewer MACs, >72x fewer\nparameters, and >288x lower weight memory requirements), making it well-suited\nfor TinyML applications on the edge.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 19:19:04 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Wen", "Xiaoyu", ""], ["Famouri", "Mahmoud", ""], ["Hryniowski", "Andrew", ""], ["Wong", "Alexander", ""]]}, {"id": "2104.14628", "submitter": "Debora Caldarola", "authors": "Debora Caldarola, Massimiliano Mancini, Fabio Galasso, Marco Ciccone,\n  Emanuele Rodol\\`a, Barbara Caputo", "title": "Cluster-driven Graph Federated Learning over Multiple Domains", "comments": "Accepted to CVPR21 Workshop Learning from Limited or Imperfect Data\n  (L^2ID)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated Learning (FL) deals with learning a central model (i.e. the server)\nin privacy-constrained scenarios, where data are stored on multiple devices\n(i.e. the clients). The central model has no direct access to the data, but\nonly to the updates of the parameters computed locally by each client. This\nraises a problem, known as statistical heterogeneity, because the clients may\nhave different data distributions (i.e. domains). This is only partly\nalleviated by clustering the clients. Clustering may reduce heterogeneity by\nidentifying the domains, but it deprives each cluster model of the data and\nsupervision of others. Here we propose a novel Cluster-driven Graph Federated\nLearning (FedCG). In FedCG, clustering serves to address statistical\nheterogeneity, while Graph Convolutional Networks (GCNs) enable sharing\nknowledge across them. FedCG: i) identifies the domains via an FL-compliant\nclustering and instantiates domain-specific modules (residual branches) for\neach domain; ii) connects the domain-specific modules through a GCN at training\nto learn the interactions among domains and share knowledge; and iii) learns to\ncluster unsupervised via teacher-student classifier-training iterations and to\naddress novel unseen test domains via their domain soft-assignment scores.\nThanks to the unique interplay of GCN over clusters, FedCG achieves the\nstate-of-the-art on multiple FL benchmarks.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 19:31:19 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Caldarola", "Debora", ""], ["Mancini", "Massimiliano", ""], ["Galasso", "Fabio", ""], ["Ciccone", "Marco", ""], ["Rodol\u00e0", "Emanuele", ""], ["Caputo", "Barbara", ""]]}, {"id": "2104.14629", "submitter": "Xiao-Yun Zhou", "authors": "Xiao-Yun Zhou, Bolin Lai, Weijian Li, Yirui Wang, Kang Zheng, Fakai\n  Wang, Chihung Lin, Le Lu, Lingyun Huang, Mei Han, Guotong Xie, Jing Xiao, Kuo\n  Chang-Fu, Adam Harrison, Shun Miao", "title": "Scalable Semi-supervised Landmark Localization for X-ray Images using\n  Few-shot Deep Adaptive Graph", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Landmark localization plays an important role in medical image analysis.\nLearning based methods, including CNN and GCN, have demonstrated the\nstate-of-the-art performance. However, most of these methods are\nfully-supervised and heavily rely on manual labeling of a large training\ndataset. In this paper, based on a fully-supervised graph-based method, DAG, we\nproposed a semi-supervised extension of it, termed few-shot DAG, \\ie five-shot\nDAG. It first trains a DAG model on the labeled data and then fine-tunes the\npre-trained model on the unlabeled data with a teacher-student SSL mechanism.\nIn addition to the semi-supervised loss, we propose another loss using JS\ndivergence to regulate the consistency of the intermediate feature maps. We\nextensively evaluated our method on pelvis, hand and chest landmark detection\ntasks. Our experiment results demonstrate consistent and significant\nimprovements over previous methods.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 19:46:18 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Zhou", "Xiao-Yun", ""], ["Lai", "Bolin", ""], ["Li", "Weijian", ""], ["Wang", "Yirui", ""], ["Zheng", "Kang", ""], ["Wang", "Fakai", ""], ["Lin", "Chihung", ""], ["Lu", "Le", ""], ["Huang", "Lingyun", ""], ["Han", "Mei", ""], ["Xie", "Guotong", ""], ["Xiao", "Jing", ""], ["Chang-Fu", "Kuo", ""], ["Harrison", "Adam", ""], ["Miao", "Shun", ""]]}, {"id": "2104.14631", "submitter": "Sibo Zhang", "authors": "Sibo Zhang, Jiahong Yuan, Miao Liao, Liangjun Zhang", "title": "Text2Video: Text-driven Talking-head Video Synthesis with Phonetic\n  Dictionary", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advance of deep learning technology, automatic video generation from\naudio or text has become an emerging and promising research topic. In this\npaper, we present a novel approach to synthesize video from the text. The\nmethod builds a phoneme-pose dictionary and trains a generative adversarial\nnetwork (GAN) to generate video from interpolated phoneme poses. Compared to\naudio-driven video generation algorithms, our approach has a number of\nadvantages: 1) It only needs a fraction of the training data used by an\naudio-driven approach; 2) It is more flexible and not subject to vulnerability\ndue to speaker variation; 3) It significantly reduces the preprocessing,\ntraining and inference time. We perform extensive experiments to compare the\nproposed method with state-of-the-art talking face generation methods on a\nbenchmark dataset and datasets of our own. The results demonstrate the\neffectiveness and superiority of our approach.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 19:54:41 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Zhang", "Sibo", ""], ["Yuan", "Jiahong", ""], ["Liao", "Miao", ""], ["Zhang", "Liangjun", ""]]}, {"id": "2104.14639", "submitter": "Shreyas Hampali", "authors": "Shreyas Hampali, Sayan Deb Sarkar, Mahdi Rad, Vincent Lepetit", "title": "HandsFormer: Keypoint Transformer for Monocular 3D Pose Estimation\n  ofHands and Object in Interaction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a robust and accurate method for estimating the 3D poses of two\nhands in close interaction from a single color image. This is a very\nchallenging problem, as large occlusions and many confusions between the joints\nmay happen. Our method starts by extracting a set of potential 2D locations for\nthe joints of both hands as extrema of a heatmap. We do not require that all\nlocations correctly correspond to a joint, not that all the joints are\ndetected. We use appearance and spatial encodings of these locations as input\nto a transformer, and leverage the attention mechanisms to sort out the correct\nconfiguration of the joints and output the 3D poses of both hands. Our approach\nthus allies the recognition power of a Transformer to the accuracy of\nheatmap-based methods. We also show it can be extended to estimate the 3D pose\nof an object manipulated by one or two hands. We evaluate our approach on the\nrecent and challenging InterHand2.6M and HO-3D datasets. We obtain 17%\nimprovement over the baseline. Moreover, we introduce the first dataset made of\naction sequences of two hands manipulating an object fully annotated in 3D and\nwill make it publicly available.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 20:19:20 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Hampali", "Shreyas", ""], ["Sarkar", "Sayan Deb", ""], ["Rad", "Mahdi", ""], ["Lepetit", "Vincent", ""]]}, {"id": "2104.14643", "submitter": "Chun-Hao Paul Huang", "authors": "Priyanka Patel, Chun-Hao P. Huang, Joachim Tesch, David T. Hoffmann,\n  Shashank Tripathi, Michael J. Black", "title": "AGORA: Avatars in Geography Optimized for Regression Analysis", "comments": null, "journal-ref": "CVPR 2021", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While the accuracy of 3D human pose estimation from images has steadily\nimproved on benchmark datasets, the best methods still fail in many real-world\nscenarios. This suggests that there is a domain gap between current datasets\nand common scenes containing people. To obtain ground-truth 3D pose, current\ndatasets limit the complexity of clothing, environmental conditions, number of\nsubjects, and occlusion. Moreover, current datasets evaluate sparse 3D joint\nlocations corresponding to the major joints of the body, ignoring the hand pose\nand the face shape. To evaluate the current state-of-the-art methods on more\nchallenging images, and to drive the field to address new problems, we\nintroduce AGORA, a synthetic dataset with high realism and highly accurate\nground truth. Here we use 4240 commercially-available, high-quality, textured\nhuman scans in diverse poses and natural clothing; this includes 257 scans of\nchildren. We create reference 3D poses and body shapes by fitting the SMPL-X\nbody model (with face and hands) to the 3D scans, taking into account clothing.\nWe create around 14K training and 3K test images by rendering between 5 and 15\npeople per image using either image-based lighting or rendered 3D environments,\ntaking care to make the images physically plausible and photoreal. In total,\nAGORA consists of 173K individual person crops. We evaluate existing\nstate-of-the-art methods for 3D human pose estimation on this dataset and find\nthat most methods perform poorly on images of children. Hence, we extend the\nSMPL-X model to better capture the shape of children. Additionally, we\nfine-tune methods on AGORA and show improved performance on both AGORA and\n3DPW, confirming the realism of the dataset. We provide all the registered 3D\nreference training data, rendered images, and a web-based evaluation site at\nhttps://agora.is.tue.mpg.de/.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 20:33:25 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Patel", "Priyanka", ""], ["Huang", "Chun-Hao P.", ""], ["Tesch", "Joachim", ""], ["Hoffmann", "David T.", ""], ["Tripathi", "Shashank", ""], ["Black", "Michael J.", ""]]}, {"id": "2104.14655", "submitter": "Junhua Chen", "authors": "Junhua Chen, Haiyan Zeng, Chong Zhang, Zhenwei Shi, Andre Dekker,\n  Leonard Wee, Inigo Bermejo", "title": "Lung Cancer Diagnosis Using Deep Attention Based on Multiple Instance\n  Learning and Radiomics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Early diagnosis of lung cancer is a key intervention for the treatment of\nlung cancer computer aided diagnosis (CAD) can play a crucial role. However,\nmost published CAD methods treat lung cancer diagnosis as a lung nodule\nclassification problem, which does not reflect clinical practice, where\nclinicians diagnose a patient based on a set of images of nodules, instead of\none specific nodule. Besides, the low interpretability of the output provided\nby these methods presents an important barrier for their adoption. In this\narticle, we treat lung cancer diagnosis as a multiple instance learning (MIL)\nproblem in order to better reflect the diagnosis process in the clinical\nsetting and for the higher interpretability of the output. We chose radiomics\nas the source of input features and deep attention-based MIL as the\nclassification algorithm.The attention mechanism provides higher\ninterpretability by estimating the importance of each instance in the set for\nthe final diagnosis.In order to improve the model's performance in a small\nimbalanced dataset, we introduce a new bag simulation method for MIL.The\nresults show that our method can achieve a mean accuracy of 0.807 with a\nstandard error of the mean (SEM) of 0.069, a recall of 0.870 (SEM 0.061), a\npositive predictive value of 0.928 (SEM 0.078), a negative predictive value of\n0.591 (SEM 0.155) and an area under the curve (AUC) of 0.842 (SEM 0.074),\noutperforming other MIL methods.Additional experiments show that the proposed\noversampling strategy significantly improves the model's performance. In\naddition, our experiments show that our method provides an indication of the\nimportance of each nodule in determining the diagnosis, which combined with the\nwell-defined radiomic features, make the results more interpretable and\nacceptable for doctors and patients.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 21:04:02 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Chen", "Junhua", ""], ["Zeng", "Haiyan", ""], ["Zhang", "Chong", ""], ["Shi", "Zhenwei", ""], ["Dekker", "Andre", ""], ["Wee", "Leonard", ""], ["Bermejo", "Inigo", ""]]}, {"id": "2104.14659", "submitter": "Bjorn Burkle", "authors": "Michael Andrews, Bjorn Burkle, Yi-fan Chen, Davide DiCroce, Sergei\n  Gleyzer, Ulrich Heintz, Meenakshi Narain, Manfred Paulini, Nikolas Pervan,\n  Yusef Shafi, Wei Sun, Emanuele Usai, Kun Yang", "title": "End-to-End Jet Classification of Boosted Top Quarks with the CMS Open\n  Data", "comments": "14 pages, 5 figures, 7 tables; v1: unpublished", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.data-an cs.CV cs.LG hep-ex", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We describe a novel application of the end-to-end deep learning technique to\nthe task of discriminating top quark-initiated jets from those originating from\nthe hadronization of a light quark or a gluon. The end-to-end deep learning\ntechnique combines deep learning algorithms and low-level detector\nrepresentation of the high-energy collision event. In this study, we use\nlow-level detector information from the simulated CMS Open Data samples to\nconstruct the top jet classifiers. To optimize classifier performance we\nprogressively add low-level information from the CMS tracking detector,\nincluding pixel detector reconstructed hits and impact parameters, and\ndemonstrate the value of additional tracking information even when no new\nspatial structures are added. Relying only on calorimeter energy deposits and\nreconstructed pixel detector hits, the end-to-end classifier achieves an AUC\nscore of 0.975$\\pm$0.002 for the task of classifying boosted top quark jets.\nAfter adding derived track quantities, the classifier AUC score increases to\n0.9824$\\pm$0.0013, serving as the first performance benchmark for these CMS\nOpen Data samples. We additionally provide a timing performance comparison of\ndifferent processor unit architectures for training the network.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 19:36:43 GMT"}, {"version": "v2", "created": "Mon, 26 Jul 2021 14:43:45 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Andrews", "Michael", ""], ["Burkle", "Bjorn", ""], ["Chen", "Yi-fan", ""], ["DiCroce", "Davide", ""], ["Gleyzer", "Sergei", ""], ["Heintz", "Ulrich", ""], ["Narain", "Meenakshi", ""], ["Paulini", "Manfred", ""], ["Pervan", "Nikolas", ""], ["Shafi", "Yusef", ""], ["Sun", "Wei", ""], ["Usai", "Emanuele", ""], ["Yang", "Kun", ""]]}, {"id": "2104.14679", "submitter": "Harshayu Girase", "authors": "Harshayu Girase, Jerrick Hoang, Sai Yalamanchi, and Micol\n  Marchetti-Bowick", "title": "Physically Feasible Vehicle Trajectory Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Predicting the future motion of actors in a traffic scene is a crucial part\nof any autonomous driving system. Recent research in this area has focused on\ntrajectory prediction approaches that optimize standard trajectory error\nmetrics. In this work, we describe three important properties -- physical\nrealism guarantees, system maintainability, and sample efficiency -- which we\nbelieve are equally important for developing a self-driving system that can\noperate safely and practically in the real world. Furthermore, we introduce\nPTNet (PathTrackingNet), a novel approach for vehicle trajectory prediction\nthat is a hybrid of the classical pure pursuit path tracking algorithm and\nmodern graph-based neural networks. By combining a structured robotics\ntechnique with a flexible learning approach, we are able to produce a system\nthat not only achieves the same level of performance as other state-of-the-art\nmethods on traditional trajectory error metrics, but also provides strong\nguarantees about the physical realism of the predicted trajectories while\nrequiring half the amount of data. We believe focusing on this new class of\nhybrid approaches is an useful direction for developing and maintaining a\nsafety-critical autonomous driving system.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 22:13:41 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Girase", "Harshayu", ""], ["Hoang", "Jerrick", ""], ["Yalamanchi", "Sai", ""], ["Marchetti-Bowick", "Micol", ""]]}, {"id": "2104.14682", "submitter": "Aleksandr Kim", "authors": "Aleksandr Kim, Aljo\\v{s}a O\\v{s}ep, Laura Leal-Taix\\'e", "title": "EagerMOT: 3D Multi-Object Tracking via Sensor Fusion", "comments": "To be published at ICRA 2021. Source code available at\n  https://github.com/aleksandrkim61/EagerMOT", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-object tracking (MOT) enables mobile robots to perform well-informed\nmotion planning and navigation by localizing surrounding objects in 3D space\nand time. Existing methods rely on depth sensors (e.g., LiDAR) to detect and\ntrack targets in 3D space, but only up to a limited sensing range due to the\nsparsity of the signal. On the other hand, cameras provide a dense and rich\nvisual signal that helps to localize even distant objects, but only in the\nimage domain. In this paper, we propose EagerMOT, a simple tracking formulation\nthat eagerly integrates all available object observations from both sensor\nmodalities to obtain a well-informed interpretation of the scene dynamics.\nUsing images, we can identify distant incoming objects, while depth estimates\nallow for precise trajectory localization as soon as objects are within the\ndepth-sensing range. With EagerMOT, we achieve state-of-the-art results across\nseveral MOT tasks on the KITTI and NuScenes datasets. Our code is available at\nhttps://github.com/aleksandrkim61/EagerMOT.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 22:30:29 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Kim", "Aleksandr", ""], ["O\u0161ep", "Aljo\u0161a", ""], ["Leal-Taix\u00e9", "Laura", ""]]}, {"id": "2104.14685", "submitter": "Krishnapriya K. S", "authors": "KS Krishnapriya, Michael C. King, Kevin W. Bowyer", "title": "Analysis of Manual and Automated Skin Tone Assignments for Face\n  Recognition Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  News reports have suggested that darker skin tone causes an increase in face\nrecognition errors. The Fitzpatrick scale is widely used in dermatology to\nclassify sensitivity to sun exposure and skin tone. In this paper, we analyze a\nset of manual Fitzpatrick skin type assignments and also employ the individual\ntypology angle to automatically estimate the skin tone from face images. The\nset of manual skin tone rating experiments shows that there are inconsistencies\nbetween human raters that are difficult to eliminate. Efforts to automate skin\ntone rating suggest that it is particularly challenging on images collected\nwithout a calibration object in the scene. However, after the color-correction,\nthe level of agreement between automated and manual approaches is found to be\n96% or better for the MORPH images. To our knowledge, this is the first work\nto: (a) examine the consistency of manual skin tone ratings across observers,\n(b) document that there is substantial variation in the rating of the same\nimage by different observers even when exemplar images are given for guidance\nand all images are color-corrected, and (c) compare manual versus automated\nskin tone ratings.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 22:35:47 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Krishnapriya", "KS", ""], ["King", "Michael C.", ""], ["Bowyer", "Kevin W.", ""]]}, {"id": "2104.14696", "submitter": "Zhi Yuan Wu", "authors": "Zhiyuan Wu, Yu Jiang, Minghao Zhao, Chupeng Cui, Zongmin Yang, Xinhui\n  Xue, Hong Qi", "title": "Spirit Distillation: A Model Compression Method with Multi-domain\n  Knowledge Transfer", "comments": "13 pages, 4 figures, 4 tables. arXiv admin note: text overlap with\n  arXiv:2103.13733", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent applications pose requirements of both cross-domain knowledge transfer\nand model compression to machine learning models due to insufficient training\ndata and limited computational resources. In this paper, we propose a new\nknowledge distillation model, named Spirit Distillation (SD), which is a model\ncompression method with multi-domain knowledge transfer. The compact student\nnetwork mimics out a representation equivalent to the front part of the teacher\nnetwork, through which the general knowledge can be transferred from the source\ndomain (teacher) to the target domain (student). To further improve the\nrobustness of the student, we extend SD to Enhanced Spirit Distillation (ESD)\nin exploiting a more comprehensive knowledge by introducing the proximity\ndomain which is similar to the target domain for feature extraction. Results\ndemonstrate that our method can boost mIOU and high-precision accuracy by 1.4%\nand 8.2% respectively with 78.2% segmentation variance, and can gain a precise\ncompact network with only 41.8% FLOPs.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 23:19:51 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Wu", "Zhiyuan", ""], ["Jiang", "Yu", ""], ["Zhao", "Minghao", ""], ["Cui", "Chupeng", ""], ["Yang", "Zongmin", ""], ["Xue", "Xinhui", ""], ["Qi", "Hong", ""]]}, {"id": "2104.14702", "submitter": "Zhuangzhuang Zhang", "authors": "Zhuangzhuang Zhang, Baozhou Sun, Weixiong Zhang", "title": "Pyramid Medical Transformer for Medical Image Segmentation", "comments": "13 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have been a prevailing technique in the field of medical\nimage processing. However, the most popular convolutional neural networks\n(CNNs) based methods for medical image segmentation are imperfect because they\ncannot adequately model long-range pixel relations. Transformers and the\nself-attention mechanism are recently proposed to effectively learn long-range\ndependencies by modeling all pairs of word-to-word attention regardless of\ntheir positions. The idea has also been extended to the computer vision field\nby creating and treating image patches as embeddings. Considering the\ncomputation complexity for whole image self-attention, current\ntransformer-based models settle for a rigid partitioning scheme that would\npotentially lose informative relations. Besides, current medical transformers\nmodel global context on full resolution images, leading to unnecessary\ncomputation costs. To address these issues, we developed a novel method to\nintegrate multi-scale attention and CNN feature extraction using a pyramidal\nnetwork architecture, namely Pyramid Medical Transformer (PMTrans). The PMTrans\ncaptured multi-range relations by working on multi-resolution images. An\nadaptive partitioning scheme was implemented to retain informative relations\nand to access different receptive fields efficiently. Experimental results on\ntwo medical image datasets, gland segmentation and MoNuSeg datasets, showed\nthat PMTrans outperformed the latest CNN-based and transformer-based models for\nmedical image segmentation.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 23:57:20 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Zhang", "Zhuangzhuang", ""], ["Sun", "Baozhou", ""], ["Zhang", "Weixiong", ""]]}, {"id": "2104.14721", "submitter": "Shubham Gupta", "authors": "Carola Sundaramoorthy, Lin Ziwen Kelvin, Mahak Sarin, Shubham Gupta", "title": "End-to-End Attention-based Image Captioning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we address the problem of image captioning specifically for\nmolecular translation where the result would be a predicted chemical notation\nin InChI format for a given molecular structure. Current approaches mainly\nfollow rule-based or CNN+RNN based methodology. However, they seem to\nunderperform on noisy images and images with small number of distinguishable\nfeatures. To overcome this, we propose an end-to-end transformer model. When\ncompared to attention-based techniques, our proposed model outperforms on\nmolecular datasets.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 01:54:38 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Sundaramoorthy", "Carola", ""], ["Kelvin", "Lin Ziwen", ""], ["Sarin", "Mahak", ""], ["Gupta", "Shubham", ""]]}, {"id": "2104.14726", "submitter": "Ziqian Lin", "authors": "Ziqian Lin, Sreya Dutta Roy, Yixuan Li", "title": "MOOD: Multi-level Out-of-distribution Detection", "comments": "12 pages, 8 figures, CVPR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Out-of-distribution (OOD) detection is essential to prevent anomalous inputs\nfrom causing a model to fail during deployment. While improved OOD detection\nmethods have emerged, they often rely on the final layer outputs and require a\nfull feedforward pass for any given input. In this paper, we propose a novel\nframework, multi-level out-of-distribution detection MOOD, which exploits\nintermediate classifier outputs for dynamic and efficient OOD inference. We\nexplore and establish a direct relationship between the OOD data complexity and\noptimal exit level, and show that easy OOD examples can be effectively detected\nearly without propagating to deeper layers. At each exit, the OOD examples can\nbe distinguished through our proposed adjusted energy score, which is both\nempirically and theoretically suitable for networks with multiple classifiers.\nWe extensively evaluate MOOD across 10 OOD datasets spanning a wide range of\ncomplexities. Experiments demonstrate that MOOD achieves up to 71.05%\ncomputational reduction in inference, while maintaining competitive OOD\ndetection performance.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 02:18:31 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Lin", "Ziqian", ""], ["Roy", "Sreya Dutta", ""], ["Li", "Yixuan", ""]]}, {"id": "2104.14729", "submitter": "Lv Tang", "authors": "Lv Tang", "title": "CoSformer: Detecting Co-Salient Object with Transformers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Co-Salient Object Detection (CoSOD) aims at simulating the human visual\nsystem to discover the common and salient objects from a group of relevant\nimages. Recent methods typically develop sophisticated deep learning based\nmodels have greatly improved the performance of CoSOD task. But there are still\ntwo major drawbacks that need to be further addressed, 1) sub-optimal\ninter-image relationship modeling; 2) lacking consideration of inter-image\nseparability. In this paper, we propose the Co-Salient Object Detection\nTransformer (CoSformer) network to capture both salient and common visual\npatterns from multiple images. By leveraging Transformer architecture, the\nproposed method address the influence of the input orders and greatly improve\nthe stability of the CoSOD task. We also introduce a novel concept of\ninter-image separability. We construct a contrast learning scheme to modeling\nthe inter-image separability and learn more discriminative embedding space to\ndistinguish true common objects from noisy objects. Extensive experiments on\nthree challenging benchmarks, i.e., CoCA, CoSOD3k, and Cosal2015, demonstrate\nthat our CoSformer outperforms cutting-edge models and achieves the new\nstate-of-the-art. We hope that CoSformer can motivate future research for more\nvisual co-analysis tasks.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 02:39:12 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Tang", "Lv", ""]]}, {"id": "2104.14730", "submitter": "Manri Cheon", "authors": "Manri Cheon, Sung-Jun Yoon, Byungyeon Kang, Junwoo Lee", "title": "Perceptual Image Quality Assessment with Transformers", "comments": "Accepted to NTIRE workshop at CVPR 2021. 1st Place in NTIRE 2021\n  perceptual IQA challenge. https://github.com/manricheon/IQT", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we propose an image quality transformer (IQT) that\nsuccessfully applies a transformer architecture to a perceptual full-reference\nimage quality assessment (IQA) task. Perceptual representation becomes more\nimportant in image quality assessment. In this context, we extract the\nperceptual feature representations from each of input images using a\nconvolutional neural network (CNN) backbone. The extracted feature maps are fed\ninto the transformer encoder and decoder in order to compare a reference and\ndistorted images. Following an approach of the transformer-based vision models,\nwe use extra learnable quality embedding and position embedding. The output of\nthe transformer is passed to a prediction head in order to predict a final\nquality score. The experimental results show that our proposed model has an\noutstanding performance for the standard IQA datasets. For a large-scale IQA\ndataset containing output images of generative model, our model also shows the\npromising results. The proposed IQT was ranked first among 13 participants in\nthe NTIRE 2021 perceptual image quality assessment challenge. Our work will be\nan opportunity to further expand the approach for the perceptual IQA task.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 02:45:29 GMT"}, {"version": "v2", "created": "Wed, 5 May 2021 03:18:47 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Cheon", "Manri", ""], ["Yoon", "Sung-Jun", ""], ["Kang", "Byungyeon", ""], ["Lee", "Junwoo", ""]]}, {"id": "2104.14735", "submitter": "Canqun Xiang", "authors": "Canqun Xiang and Zhennan Wang and Wenbin Zou and Chen Xu", "title": "DPR-CAE: Capsule Autoencoder with Dynamic Part Representation for Image\n  Parsing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parsing an image into a hierarchy of objects, parts, and relations is\nimportant and also challenging in many computer vision tasks. This paper\nproposes a simple and effective capsule autoencoder to address this issue,\ncalled DPR-CAE. In our approach, the encoder parses the input into a set of\npart capsules, including pose, intensity, and dynamic vector. The decoder\nintroduces a novel dynamic part representation (DPR) by combining the dynamic\nvector and a shared template bank. These part representations are then\nregulated by corresponding capsules to composite the final output in an\ninterpretable way. Besides, an extra translation-invariant module is proposed\nto avoid directly learning the uncertain scene-part relationship in our\nDPR-CAE, which makes the resulting method achieves a promising performance gain\non $rm$-MNIST and $rm$-Fashion-MNIST. % to model the scene-object relationship\nDPR-CAE can be easily combined with the existing stacked capsule autoencoder\nand experimental results show it significantly improves performance in terms of\nunsupervised object classification. Our code is available in the Appendix.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 03:14:17 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Xiang", "Canqun", ""], ["Wang", "Zhennan", ""], ["Zou", "Wenbin", ""], ["Xu", "Chen", ""]]}, {"id": "2104.14741", "submitter": "Peng Wang", "authors": "Chenyu Gao and Qi Zhu and Peng Wang and Qi Wu", "title": "Chop Chop BERT: Visual Question Answering by Chopping VisualBERT's Heads", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vision-and-Language (VL) pre-training has shown great potential on many\nrelated downstream tasks, such as Visual Question Answering (VQA), one of the\nmost popular problems in the VL field. All of these pre-trained models (such as\nVisualBERT, ViLBERT, LXMERT and UNITER) are built with Transformer, which\nextends the classical attention mechanism to multiple layers and heads. To\ninvestigate why and how these models work on VQA so well, in this paper we\nexplore the roles of individual heads and layers in Transformer models when\nhandling $12$ different types of questions. Specifically, we manually remove\n(chop) heads (or layers) from a pre-trained VisualBERT model at a time, and\ntest it on different levels of questions to record its performance. As shown in\nthe interesting echelon shape of the result matrices, experiments reveal\ndifferent heads and layers are responsible for different question types, with\nhigher-level layers activated by higher-level visual reasoning questions. Based\non this observation, we design a dynamic chopping module that can automatically\nremove heads and layers of the VisualBERT at an instance level when dealing\nwith different questions. Our dynamic chopping module can effectively reduce\nthe parameters of the original model by 50%, while only damaging the accuracy\nby less than 1% on the VQA task.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 03:32:02 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Gao", "Chenyu", ""], ["Zhu", "Qi", ""], ["Wang", "Peng", ""], ["Wu", "Qi", ""]]}, {"id": "2104.14746", "submitter": "Lu Yang", "authors": "Lu Yang, Yunlong Wang, Lingqiao Liu, Peng Wang, Lu Chi, Zehuan Yuan,\n  Changhu Wang and Yanning Zhang", "title": "Center Prediction Loss for Re-identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The training loss function that enforces certain training sample distribution\npatterns plays a critical role in building a re-identification (ReID) system.\nBesides the basic requirement of discrimination, i.e., the features\ncorresponding to different identities should not be mixed, additional\nintra-class distribution constraints, such as features from the same identities\nshould be close to their centers, have been adopted to construct losses.\nDespite the advances of various new loss functions, it is still challenging to\nstrike the balance between the need of reducing the intra-class variation and\nallowing certain distribution freedom. In this paper, we propose a new loss\nbased on center predictivity, that is, a sample must be positioned in a\nlocation of the feature space such that from it we can roughly predict the\nlocation of the center of same-class samples. The prediction error is then\nregarded as a loss called Center Prediction Loss (CPL). We show that, without\nintroducing additional hyper-parameters, this new loss leads to a more flexible\nintra-class distribution constraint while ensuring the between-class samples\nare well-separated. Extensive experiments on various real-world ReID datasets\nshow that the proposed loss can achieve superior performance and can also be\ncomplementary to existing losses.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 03:57:31 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Yang", "Lu", ""], ["Wang", "Yunlong", ""], ["Liu", "Lingqiao", ""], ["Wang", "Peng", ""], ["Chi", "Lu", ""], ["Yuan", "Zehuan", ""], ["Wang", "Changhu", ""], ["Zhang", "Yanning", ""]]}, {"id": "2104.14749", "submitter": "Arnesh Kumar Issar", "authors": "Arnesh Kumar Issar, Kirtan Mali, Aryan Mehta, Karan Uppal, Saurabh\n  Mishra, Debashish Chakravarty", "title": "Reproducibility of \"FDA: Fourier Domain Adaptation forSemantic\n  Segmentation", "comments": "11 pages, 7 figures, 7 Tables, ML Reproducibility Challenge 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The following paper is a reproducibility report for \"FDA: Fourier Domain\nAdaptation for Semantic Segmentation\" published in the CVPR 2020 as part of the\nML Reproducibility Challenge 2020. The original code was made available by the\nauthor. The well-commented version of the code containing all ablation studies\nperformed derived from the original code along with WANDB integration is\navailable at <github.com/thefatbandit/FDA> with proper instructions to execute\nexperiments in README.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 04:20:35 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Issar", "Arnesh Kumar", ""], ["Mali", "Kirtan", ""], ["Mehta", "Aryan", ""], ["Uppal", "Karan", ""], ["Mishra", "Saurabh", ""], ["Chakravarty", "Debashish", ""]]}, {"id": "2104.14750", "submitter": "Yi-Shuai Niu", "authors": "Yu You, Yi-Shuai Niu", "title": "A Refined Inertial DCA for DC Programming", "comments": "27 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the difference-of-convex (DC) programming problems whose\nobjective function is level-bounded. The classical DC algorithm (DCA) is\nwell-known for solving this kind of problems, which returns a critical point.\nRecently, de Oliveira and Tcheo incorporated the inertial-force procedure into\nDCA (InDCA) for potential acceleration and preventing the algorithm from\nconverging to a critical point which is not d(directional)-stationary. In this\npaper, based on InDCA, we propose two refined inertial DCA (RInDCA) with\nenlarged inertial step-sizes for better acceleration. We demonstrate the\nsubsequential convergence of our refined versions to a critical point. In\naddition, by assuming the Kurdyka-Lojasiewicz (KL) property of the objective\nfunction, we establish the sequential convergence of RInDCA. Numerical\nsimulations on image restoration problem show the benefit of enlarged\nstep-size.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 04:21:57 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["You", "Yu", ""], ["Niu", "Yi-Shuai", ""]]}, {"id": "2104.14753", "submitter": "Rajiv Movva", "authors": "Rajiv Movva, Jonathan Frankle, Michael Carbin", "title": "Studying the Consistency and Composability of Lottery Ticket Pruning\n  Masks", "comments": "Workshop on Science and Engineering of Deep Learning (ICLR 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Magnitude pruning is a common, effective technique to identify sparse\nsubnetworks at little cost to accuracy. In this work, we ask whether a\nparticular architecture's accuracy-sparsity tradeoff can be improved by\ncombining pruning information across multiple runs of training. From a shared\nResNet-20 initialization, we train several network copies (\\emph{siblings}) to\ncompletion using different SGD data orders on CIFAR-10. While the siblings'\npruning masks are naively not much more similar than chance, starting sibling\ntraining after a few epochs of shared pretraining significantly increases\npruning overlap. We then choose a subnetwork by either (1) taking all weights\nthat survive pruning in any sibling (mask union), or (2) taking only the\nweights that survive pruning across all siblings (mask intersection). The\nresulting subnetwork is retrained. Strikingly, we find that union and\nintersection masks perform very similarly. Both methods match the\naccuracy-sparsity tradeoffs of the one-shot magnitude pruning baseline, even\nwhen we combine masks from up to $k = 10$ siblings.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 04:38:06 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Movva", "Rajiv", ""], ["Frankle", "Jonathan", ""], ["Carbin", "Michael", ""]]}, {"id": "2104.14754", "submitter": "Hyunsu Kim", "authors": "Hyunsu Kim, Yunjey Choi, Junho Kim, Sungjoo Yoo, Youngjung Uh", "title": "Exploiting Spatial Dimensions of Latent in GAN for Real-time Image\n  Editing", "comments": "Accepted to CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Generative adversarial networks (GANs) synthesize realistic images from\nrandom latent vectors. Although manipulating the latent vectors controls the\nsynthesized outputs, editing real images with GANs suffers from i)\ntime-consuming optimization for projecting real images to the latent vectors,\nii) or inaccurate embedding through an encoder. We propose StyleMapGAN: the\nintermediate latent space has spatial dimensions, and a spatially variant\nmodulation replaces AdaIN. It makes the embedding through an encoder more\naccurate than existing optimization-based methods while maintaining the\nproperties of GANs. Experimental results demonstrate that our method\nsignificantly outperforms state-of-the-art models in various image manipulation\ntasks such as local editing and image interpolation. Last but not least,\nconventional editing methods on GANs are still valid on our StyleMapGAN. Source\ncode is available at https://github.com/naver-ai/StyleMapGAN.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 04:43:24 GMT"}, {"version": "v2", "created": "Wed, 23 Jun 2021 02:05:12 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Kim", "Hyunsu", ""], ["Choi", "Yunjey", ""], ["Kim", "Junho", ""], ["Yoo", "Sungjoo", ""], ["Uh", "Youngjung", ""]]}, {"id": "2104.14762", "submitter": "Yanan Wu", "authors": "Yanan Wu, He Liu, Songhe Feng, Yi Jin, Gengyu Lyu, Zizhang Wu", "title": "GM-MLIC: Graph Matching based Multi-Label Image Classification", "comments": "Accepted by International Joint Conferences on Artificial\n  Intelligence (IJCAI-2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multi-Label Image Classification (MLIC) aims to predict a set of labels that\npresent in an image. The key to deal with such problem is to mine the\nassociations between image contents and labels, and further obtain the correct\nassignments between images and their labels. In this paper, we treat each image\nas a bag of instances, and reformulate the task of MLIC as an instance-label\nmatching selection problem. To model such problem, we propose a novel deep\nlearning framework named Graph Matching based Multi-Label Image Classification\n(GM-MLIC), where Graph Matching (GM) scheme is introduced owing to its\nexcellent capability of excavating the instance and label relationship.\nSpecifically, we first construct an instance spatial graph and a label semantic\ngraph respectively, and then incorporate them into a constructed assignment\ngraph by connecting each instance to all labels. Subsequently, the graph\nnetwork block is adopted to aggregate and update all nodes and edges state on\nthe assignment graph to form structured representations for each instance and\nlabel. Our network finally derives a prediction score for each instance-label\ncorrespondence and optimizes such correspondence with a weighted cross-entropy\nloss. Extensive experiments conducted on various image datasets demonstrate the\nsuperiority of our proposed method.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 05:36:25 GMT"}, {"version": "v2", "created": "Fri, 7 May 2021 09:20:47 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Wu", "Yanan", ""], ["Liu", "He", ""], ["Feng", "Songhe", ""], ["Jin", "Yi", ""], ["Lyu", "Gengyu", ""], ["Wu", "Zizhang", ""]]}, {"id": "2104.14763", "submitter": "Lei Sun", "authors": "Lei Sun", "title": "ICOS: Efficient and Highly Robust Rotation Search and Point Cloud\n  Registration with Correspondences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Rotation search and point cloud registration are two fundamental problems in\nrobotics and computer vision, which aim to estimate the rotation and the\ntransformation between the 3D vector sets and point clouds, respectively. Due\nto the presence of outliers, probably in very large numbers, among the putative\nvector or point correspondences in real-world applications, robust estimation\nis of great importance. In this paper, we present ICOS (Inlier searching using\nCOmpatible Structures), a novel, efficient and highly robust solver for both\nthe correspondence-based rotation search and point cloud registration problems.\nSpecifically, we (i) propose and construct a series of compatible structures\nfor the two problems where various invariants can be established, and (ii)\ndesign three time-efficient frameworks, the first for rotation search, the\nsecond for known-scale registration and the third for unknown-scale\nregistration, to filter out outliers and seek inliers from the\ninvariant-constrained random sampling based on the compatible structures\nproposed. In this manner, even with extreme outlier ratios, inliers can be\nsifted out and collected for solving the optimal rotation and transformation\neffectively, leading to our robust solver ICOS. Through plentiful experiments\nover standard datasets, we demonstrate that: (i) our solver ICOS is fast,\naccurate, robust against over 95% outliers with nearly 100% recall ratio of\ninliers for rotation search and both known-scale and unknown-scale\nregistration, outperforming other state-of-the-art methods, and (ii) ICOS is\npractical for use in multiple real-world applications.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 05:41:53 GMT"}, {"version": "v2", "created": "Mon, 10 May 2021 06:23:39 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Sun", "Lei", ""]]}, {"id": "2104.14764", "submitter": "Nishant Rai", "authors": "Nishant Rai, Ehsan Adeli, Kuan-Hui Lee, Adrien Gaidon, Juan Carlos\n  Niebles", "title": "CoCon: Cooperative-Contrastive Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Labeling videos at scale is impractical. Consequently, self-supervised visual\nrepresentation learning is key for efficient video analysis. Recent success in\nlearning image representations suggests contrastive learning is a promising\nframework to tackle this challenge. However, when applied to real-world videos,\ncontrastive learning may unknowingly lead to the separation of instances that\ncontain semantically similar events. In our work, we introduce a cooperative\nvariant of contrastive learning to utilize complementary information across\nviews and address this issue. We use data-driven sampling to leverage implicit\nrelationships between multiple input video views, whether observed (e.g. RGB)\nor inferred (e.g. flow, segmentation masks, poses). We are one of the firsts to\nexplore exploiting inter-instance relationships to drive learning. We\nexperimentally evaluate our representations on the downstream task of action\nrecognition. Our method achieves competitive performance on standard benchmarks\n(UCF101, HMDB51, Kinetics400). Furthermore, qualitative experiments illustrate\nthat our models can capture higher-order class relationships.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 05:46:02 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Rai", "Nishant", ""], ["Adeli", "Ehsan", ""], ["Lee", "Kuan-Hui", ""], ["Gaidon", "Adrien", ""], ["Niebles", "Juan Carlos", ""]]}, {"id": "2104.14767", "submitter": "Junghyuk Lee", "authors": "Junghyuk Lee and Jong-Seok Lee", "title": "TREND: Truncated Generalized Normal Density Estimation of Inception\n  Embeddings for Accurate GAN Evaluation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evaluating image generation models such as generative adversarial networks\n(GANs) is a challenging problem. A common approach is to compare the\ndistributions of the set of ground truth images and the set of generated test\nimages. The Frech\\'et Inception distance is one of the most widely used metrics\nfor evaluation of GANs, which assumes that the features from a trained\nInception model for a set of images follow a normal distribution. In this\npaper, we argue that this is an over-simplified assumption, which may lead to\nunreliable evaluation results, and more accurate density estimation can be\nachieved using a truncated generalized normal distribution. Based on this, we\npropose a novel metric for accurate evaluation of GANs, named TREND (TRuncated\ngEneralized Normal Density estimation of inception embeddings). We demonstrate\nthat our approach significantly reduces errors of density estimation, which\nconsequently eliminates the risk of faulty evaluation results. Furthermore, we\nshow that the proposed metric significantly improves robustness of evaluation\nresults against variation of the number of image samples.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 05:51:07 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Lee", "Junghyuk", ""], ["Lee", "Jong-Seok", ""]]}, {"id": "2104.14769", "submitter": "Xu Yan", "authors": "Weibing Zhao, Xu Yan, Jiantao Gao, Ruimao Zhang, Jiayan Zhang, Zhen\n  Li, Song Wu, Shuguang Cui", "title": "PointLIE: Locally Invertible Embedding for Point Cloud Sampling and\n  Recovery", "comments": "To appear in IJCAI 2021", "journal-ref": "IJCAI 2021", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point Cloud Sampling and Recovery (PCSR) is critical for massive real-time\npoint cloud collection and processing since raw data usually requires large\nstorage and computation. In this paper, we address a fundamental problem in\nPCSR: How to downsample the dense point cloud with arbitrary scales while\npreserving the local topology of discarding points in a case-agnostic manner\n(i.e. without additional storage for point relationship)? We propose a novel\nLocally Invertible Embedding for point cloud adaptive sampling and recovery\n(PointLIE). Instead of learning to predict the underlying geometry details in a\nseemingly plausible manner, PointLIE unifies point cloud sampling and\nupsampling to one single framework through bi-directional learning.\nSpecifically, PointLIE recursively samples and adjusts neighboring points on\neach scale. Then it encodes the neighboring offsets of sampled points to a\nlatent space and thus decouples the sampled points and the corresponding local\ngeometric relationship. Once the latent space is determined and that the deep\nmodel is optimized, the recovery process could be conducted by passing the\nrecover-pleasing sampled points and a randomly-drawn embedding to the same\nnetwork through an invertible operation. Such a scheme could guarantee the\nfidelity of dense point recovery from sampled points. Extensive experiments\ndemonstrate that the proposed PointLIE outperforms state-of-the-arts both\nquantitatively and qualitatively. Our code is released through\nhttps://github.com/zwb0/PointLIE.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 05:55:59 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Zhao", "Weibing", ""], ["Yan", "Xu", ""], ["Gao", "Jiantao", ""], ["Zhang", "Ruimao", ""], ["Zhang", "Jiayan", ""], ["Li", "Zhen", ""], ["Wu", "Song", ""], ["Cui", "Shuguang", ""]]}, {"id": "2104.14770", "submitter": "Muhammad Zaigham Zaheer", "authors": "Muhammad Zaigham Zaheer, Jin-ha Lee, Marcella Astrid, Arif Mahmood,\n  Seung-Ik Lee", "title": "Cleaning Label Noise with Clusters for Minimally Supervised Anomaly\n  Detection", "comments": "Presented in the CVPR20 Workshop Learning from Unlabeled Videos. An\n  archival version of this research work, published in SPL, can be accessed at:\n  https://ieeexplore.ieee.org/document/9204830. arXiv admin note: substantial\n  text overlap with arXiv:2008.11887", "journal-ref": "Computer Vision and Pattern Recognition Workshops (2020)", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Learning to detect real-world anomalous events using video-level annotations\nis a difficult task mainly because of the noise present in labels. An anomalous\nlabelled video may actually contain anomaly only in a short duration while the\nrest of the video can be normal. In the current work, we formulate a weakly\nsupervised anomaly detection method that is trained using only video-level\nlabels. To this end, we propose to utilize binary clustering which helps in\nmitigating the noise present in the labels of anomalous videos. Our formulation\nencourages both the main network and the clustering to complement each other in\nachieving the goal of weakly supervised training. The proposed method yields\n78.27% and 84.16% frame-level AUC on UCF-crime and ShanghaiTech datasets\nrespectively, demonstrating its superiority over existing state-of-the-art\nalgorithms.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 06:03:24 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Zaheer", "Muhammad Zaigham", ""], ["Lee", "Jin-ha", ""], ["Astrid", "Marcella", ""], ["Mahmood", "Arif", ""], ["Lee", "Seung-Ik", ""]]}, {"id": "2104.14783", "submitter": "Ruibing Hou", "authors": "Ruibing Hou, Hong Chang, Bingpeng Ma, Rui Huang and Shiguang Shan", "title": "BiCnet-TKS: Learning Efficient Spatial-Temporal Representation for Video\n  Person Re-Identification", "comments": "Accepted by IEEE Conference on Computer Vision and Pattern\n  Recognition (CVPR 2021) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we present an efficient spatial-temporal representation for\nvideo person re-identification (reID). Firstly, we propose a Bilateral\nComplementary Network (BiCnet) for spatial complementarity modeling.\nSpecifically, BiCnet contains two branches. Detail Branch processes frames at\noriginal resolution to preserve the detailed visual clues, and Context Branch\nwith a down-sampling strategy is employed to capture long-range contexts. On\neach branch, BiCnet appends multiple parallel and diverse attention modules to\ndiscover divergent body parts for consecutive frames, so as to obtain an\nintegral characteristic of target identity. Furthermore, a Temporal Kernel\nSelection (TKS) block is designed to capture short-term as well as long-term\ntemporal relations by an adaptive mode. TKS can be inserted into BiCnet at any\ndepth to construct BiCnetTKS for spatial-temporal modeling. Experimental\nresults on multiple benchmarks show that BiCnet-TKS outperforms\nstate-of-the-arts with about 50% less computations. The source code is\navailable at https://github.com/ blue-blue272/BiCnet-TKS.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 06:44:34 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Hou", "Ruibing", ""], ["Chang", "Hong", ""], ["Ma", "Bingpeng", ""], ["Huang", "Rui", ""], ["Shan", "Shiguang", ""]]}, {"id": "2104.14786", "submitter": "Jiakai Zhang", "authors": "Jiakai Zhang, Xinhang Liu, Xinyi Ye, Fuqiang Zhao, Yanshun Zhang,\n  Minye Wu, Yingliang Zhang, Lan Xu, Jingyi Yu", "title": "Editable Free-viewpoint Video Using a Layered Neural Representation", "comments": null, "journal-ref": null, "doi": "10.1145/3450626.3459756", "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Generating free-viewpoint videos is critical for immersive VR/AR experience\nbut recent neural advances still lack the editing ability to manipulate the\nvisual perception for large dynamic scenes. To fill this gap, in this paper we\npropose the first approach for editable photo-realistic free-viewpoint video\ngeneration for large-scale dynamic scenes using only sparse 16 cameras. The\ncore of our approach is a new layered neural representation, where each dynamic\nentity including the environment itself is formulated into a space-time\ncoherent neural layered radiance representation called ST-NeRF. Such layered\nrepresentation supports fully perception and realistic manipulation of the\ndynamic scene whilst still supporting a free viewing experience in a wide\nrange. In our ST-NeRF, the dynamic entity/layer is represented as continuous\nfunctions, which achieves the disentanglement of location, deformation as well\nas the appearance of the dynamic entity in a continuous and self-supervised\nmanner. We propose a scene parsing 4D label map tracking to disentangle the\nspatial information explicitly, and a continuous deform module to disentangle\nthe temporal motion implicitly. An object-aware volume rendering scheme is\nfurther introduced for the re-assembling of all the neural layers. We adopt a\nnovel layered loss and motion-aware ray sampling strategy to enable efficient\ntraining for a large dynamic scene with multiple performers, Our framework\nfurther enables a variety of editing functions, i.e., manipulating the scale\nand location, duplicating or retiming individual neural layers to create\nnumerous visual effects while preserving high realism. Extensive experiments\ndemonstrate the effectiveness of our approach to achieve high-quality,\nphoto-realistic, and editable free-viewpoint video generation for dynamic\nscenes.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 06:50:45 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Zhang", "Jiakai", ""], ["Liu", "Xinhang", ""], ["Ye", "Xinyi", ""], ["Zhao", "Fuqiang", ""], ["Zhang", "Yanshun", ""], ["Wu", "Minye", ""], ["Zhang", "Yingliang", ""], ["Xu", "Lan", ""], ["Yu", "Jingyi", ""]]}, {"id": "2104.14805", "submitter": "Qi Fan", "authors": "Qi Fan, Chi-Keung Tang, Yu-Wing Tai", "title": "Few-Shot Video Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We introduce Few-Shot Video Object Detection (FSVOD) with three important\ncontributions: 1) a large-scale video dataset FSVOD-500 comprising of 500\nclasses with class-balanced videos in each category for few-shot learning; 2) a\nnovel Tube Proposal Network (TPN) to generate high-quality video tube proposals\nto aggregate feature representation for the target video object; 3) a\nstrategically improved Temporal Matching Network (TMN+) to match representative\nquery tube features and supports with better discriminative ability. Our TPN\nand TMN+ are jointly and end-to-end trained. Extensive experiments demonstrate\nthat our method produces significantly better detection results on two few-shot\nvideo object detection datasets compared to image-based methods and other naive\nvideo-based extensions. Codes and datasets will be released at\nhttps://github.com/fanq15/FewX.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 07:38:04 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Fan", "Qi", ""], ["Tang", "Chi-Keung", ""], ["Tai", "Yu-Wing", ""]]}, {"id": "2104.14806", "submitter": "Chenfei Wu", "authors": "Chenfei Wu, Lun Huang, Qianxi Zhang, Binyang Li, Lei Ji, Fan Yang,\n  Guillermo Sapiro, Nan Duan", "title": "GODIVA: Generating Open-DomaIn Videos from nAtural Descriptions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating videos from text is a challenging task due to its high\ncomputational requirements for training and infinite possible answers for\nevaluation. Existing works typically experiment on simple or small datasets,\nwhere the generalization ability is quite limited. In this work, we propose\nGODIVA, an open-domain text-to-video pretrained model that can generate videos\nfrom text in an auto-regressive manner using a three-dimensional sparse\nattention mechanism. We pretrain our model on Howto100M, a large-scale\ntext-video dataset that contains more than 136 million text-video pairs.\nExperiments show that GODIVA not only can be fine-tuned on downstream video\ngeneration tasks, but also has a good zero-shot capability on unseen texts. We\nalso propose a new metric called Relative Matching (RM) to automatically\nevaluate the video generation quality. Several challenges are listed and\ndiscussed as future work.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 07:40:35 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Wu", "Chenfei", ""], ["Huang", "Lun", ""], ["Zhang", "Qianxi", ""], ["Li", "Binyang", ""], ["Ji", "Lei", ""], ["Yang", "Fan", ""], ["Sapiro", "Guillermo", ""], ["Duan", "Nan", ""]]}, {"id": "2104.14812", "submitter": "Robin Chan", "authors": "Robin Chan, Krzysztof Lis, Svenja Uhlemeyer, Hermann Blum, Sina\n  Honari, Roland Siegwart, Mathieu Salzmann, Pascal Fua and Matthias Rottmann", "title": "SegmentMeIfYouCan: A Benchmark for Anomaly Segmentation", "comments": "10 pages, 13 figures, website http://www.segmentmeifyoucan.com/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art semantic or instance segmentation deep neural networks\n(DNNs) are usually trained on a closed set of semantic classes. As such, they\nare ill-equipped to handle previously-unseen objects. However, detecting and\nlocalizing such objects is crucial for safety-critical applications such as\nperception for automated driving, especially if they appear on the road ahead.\nWhile some methods have tackled the tasks of anomalous or out-of-distribution\nobject segmentation, progress remains slow, in large part due to the lack of\nsolid benchmarks; existing datasets either consist of synthetic data, or suffer\nfrom label inconsistencies. In this paper, we bridge this gap by introducing\nthe \"SegmentMeIfYouCan\" benchmark. Our benchmark addresses two tasks: Anomalous\nobject segmentation, which considers any previously-unseen object category; and\nroad obstacle segmentation, which focuses on any object on the road, may it be\nknown or unknown. We provide two corresponding datasets together with a test\nsuite performing an in-depth method analysis, considering both established\npixel-wise performance metrics and recent component-wise ones, which are\ninsensitive to object sizes. We empirically evaluate multiple state-of-the-art\nbaseline methods, including several specifically designed for anomaly /\nobstacle segmentation, on our datasets as well as on public ones, using our\nbenchmark suite. The anomaly and obstacle segmentation results show that our\ndatasets contribute to the diversity and challengingness of both dataset\nlandscapes.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 07:58:19 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Chan", "Robin", ""], ["Lis", "Krzysztof", ""], ["Uhlemeyer", "Svenja", ""], ["Blum", "Hermann", ""], ["Honari", "Sina", ""], ["Siegwart", "Roland", ""], ["Salzmann", "Mathieu", ""], ["Fua", "Pascal", ""], ["Rottmann", "Matthias", ""]]}, {"id": "2104.14834", "submitter": "Wei Zhou", "authors": "Wei Zhou, Xin Cao, Xiaodan Zhang, Xingxing Hao, Dekui Wang, Ying He", "title": "Multi Voxel-Point Neurons Convolution (MVPConv) for Fast and Accurate 3D\n  Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We present a new convolutional neural network, called Multi Voxel-Point\nNeurons Convolution (MVPConv), for fast and accurate 3D deep learning. The\nprevious works adopt either individual point-based features or\nlocal-neighboring voxel-based features to process 3D model, which limits the\nperformance of models due to the inefficient computation. Moreover, most of the\nexisting 3D deep learning frameworks aim at solving one specific task, and only\na few of them can handle a variety of tasks. Integrating both the advantages of\nthe voxel and point-based methods, the proposed MVPConv can effectively\nincrease the neighboring collection between point-based features and also\npromote the independence among voxel-based features. Simply replacing the\ncorresponding convolution module with MVPConv, we show that MVPConv can fit in\ndifferent backbones to solve a wide range of 3D tasks. Extensive experiments on\nbenchmark datasets such as ShapeNet Part, S3DIS and KITTI for various tasks\nshow that MVPConv improves the accuracy of the backbone (PointNet) by up to\n36%, and achieves higher accuracy than the voxel-based model with up to 34\ntimes speedup. In addition, MVPConv also outperforms the state-of-the-art\npoint-based models with up to 8 times speedup. Notably, our MVPConv achieves\nbetter accuracy than the newest point-voxel-based model PVCNN (a model more\nefficient than PointNet) with lower latency.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 08:36:03 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Zhou", "Wei", ""], ["Cao", "Xin", ""], ["Zhang", "Xiaodan", ""], ["Hao", "Xingxing", ""], ["Wang", "Dekui", ""], ["He", "Ying", ""]]}, {"id": "2104.14837", "submitter": "Zhuo Su", "authors": "Zhuo Su, Lan Xu, Dawei Zhong, Zhong Li, Fan Deng, Shuxue Quan and Lu\n  Fang", "title": "RobustFusion: Robust Volumetric Performance Reconstruction under\n  Human-object Interactions from Monocular RGBD Stream", "comments": "16 pages, 18 figures. Under review by IEEE TPAMI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-quality 4D reconstruction of human performance with complex interactions\nto various objects is essential in real-world scenarios, which enables numerous\nimmersive VR/AR applications. However, recent advances still fail to provide\nreliable performance reconstruction, suffering from challenging interaction\npatterns and severe occlusions, especially for the monocular setting. To fill\nthis gap, in this paper, we propose RobustFusion, a robust volumetric\nperformance reconstruction system for human-object interaction scenarios using\nonly a single RGBD sensor, which combines various data-driven visual and\ninteraction cues to handle the complex interaction patterns and severe\nocclusions. We propose a semantic-aware scene decoupling scheme to model the\nocclusions explicitly, with a segmentation refinement and robust object\ntracking to prevent disentanglement uncertainty and maintain temporal\nconsistency. We further introduce a robust performance capture scheme with the\naid of various data-driven cues, which not only enables re-initialization\nability, but also models the complex human-object interaction patterns in a\ndata-driven manner. To this end, we introduce a spatial relation prior to\nprevent implausible intersections, as well as data-driven interaction cues to\nmaintain natural motions, especially for those regions under severe\nhuman-object occlusions. We also adopt an adaptive fusion scheme for temporally\ncoherent human-object reconstruction with occlusion analysis and human parsing\ncue. Extensive experiments demonstrate the effectiveness of our approach to\nachieve high-quality 4D human performance reconstruction under complex\nhuman-object interactions whilst still maintaining the lightweight monocular\nsetting.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 08:41:45 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Su", "Zhuo", ""], ["Xu", "Lan", ""], ["Zhong", "Dawei", ""], ["Li", "Zhong", ""], ["Deng", "Fan", ""], ["Quan", "Shuxue", ""], ["Fang", "Lu", ""]]}, {"id": "2104.14852", "submitter": "Seungjun Nah", "authors": "Sanghyun Son, Suyoung Lee, Seungjun Nah, Radu Timofte, and Kyoung Mu\n  Lee", "title": "NTIRE 2021 Challenge on Video Super-Resolution", "comments": "An official report for NTIRE 2021 Video Super-Resolution Challenge,\n  in conjunction with CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Super-Resolution (SR) is a fundamental computer vision task that aims to\nobtain a high-resolution clean image from the given low-resolution counterpart.\nThis paper reviews the NTIRE 2021 Challenge on Video Super-Resolution. We\npresent evaluation results from two competition tracks as well as the proposed\nsolutions. Track 1 aims to develop conventional video SR methods focusing on\nthe restoration quality. Track 2 assumes a more challenging environment with\nlower frame rates, casting spatio-temporal SR problem. In each competition, 247\nand 223 participants have registered, respectively. During the final testing\nphase, 14 teams competed in each track to achieve state-of-the-art performance\non video SR tasks.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 09:12:19 GMT"}, {"version": "v2", "created": "Mon, 10 May 2021 06:31:59 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Son", "Sanghyun", ""], ["Lee", "Suyoung", ""], ["Nah", "Seungjun", ""], ["Timofte", "Radu", ""], ["Lee", "Kyoung Mu", ""]]}, {"id": "2104.14854", "submitter": "Seungjun Nah", "authors": "Seungjun Nah, Sanghyun Son, Suyoung Lee, Radu Timofte, Kyoung Mu Lee", "title": "NTIRE 2021 Challenge on Image Deblurring", "comments": "To be published in CVPR 2021 Workshop - NTIRE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Motion blur is a common photography artifact in dynamic environments that\ntypically comes jointly with the other types of degradation. This paper reviews\nthe NTIRE 2021 Challenge on Image Deblurring. In this challenge report, we\ndescribe the challenge specifics and the evaluation results from the 2\ncompetition tracks with the proposed solutions. While both the tracks aim to\nrecover a high-quality clean image from a blurry image, different artifacts are\njointly involved. In track 1, the blurry images are in a low resolution while\ntrack 2 images are compressed in JPEG format. In each competition, there were\n338 and 238 registered participants and in the final testing phase, 18 and 17\nteams competed. The winning methods demonstrate the state-of-the-art\nperformance on the image deblurring task with the jointly combined artifacts.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 09:12:53 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Nah", "Seungjun", ""], ["Son", "Sanghyun", ""], ["Lee", "Suyoung", ""], ["Timofte", "Radu", ""], ["Lee", "Kyoung Mu", ""]]}, {"id": "2104.14870", "submitter": "Zahra Gharaee", "authors": "Zahra Gharaee", "title": "Action in Mind: A Neural Network Approach to Action Recognition and\n  Segmentation", "comments": "Lund University Cognitive Science 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.HC cs.LG cs.RO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Recognizing and categorizing human actions is an important task with\napplications in various fields such as human-robot interaction, video analysis,\nsurveillance, video retrieval, health care system and entertainment industry.\nThis thesis presents a novel computational approach for human action\nrecognition through different implementations of multi-layer architectures\nbased on artificial neural networks. Each system level development is designed\nto solve different aspects of the action recognition problem including online\nreal-time processing, action segmentation and the involvement of objects. The\nanalysis of the experimental results are illustrated and described in six\narticles. The proposed action recognition architecture of this thesis is\ncomposed of several processing layers including a preprocessing layer, an\nordered vector representation layer and three layers of neural networks. It\nutilizes self-organizing neural networks such as Kohonen feature maps and\ngrowing grids as the main neural network layers. Thus the architecture presents\na biological plausible approach with certain features such as topographic\norganization of the neurons, lateral interactions, semi-supervised learning and\nthe ability to represent high dimensional input space in lower dimensional\nmaps. For each level of development the system is trained with the input data\nconsisting of consecutive 3D body postures and tested with generalized input\ndata that the system has never met before. The experimental results of\ndifferent system level developments show that the system performs well with\nquite high accuracy for recognizing human actions.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 09:53:28 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Gharaee", "Zahra", ""]]}, {"id": "2104.14873", "submitter": "Adri\\`a Casamitjana D\\'iaz", "authors": "Adri\\`a Casamitjana, Marco Lorenzi, Sebastiano Ferraris, Loc Peter,\n  Marc Modat, Allison Stevens, Bruce Fischl, Tom Vercauteren, Juan Eugenio\n  Iglesias", "title": "Robust joint registration of multiple stains and MRI for multimodal 3D\n  histology reconstruction: Application to the Allen human brain atlas", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Joint registration of a stack of 2D histological sections to recover 3D\nstructure (3D histology reconstruction) finds application in areas such as\natlas building and validation of in vivo imaging. Straighforward pairwise\nregistration of neighbouring sections yields smooth reconstructions but has\nwell-known problems such as banana effect (straightening of curved structures)\nand z-shift (drift). While these problems can be alleviated with an external,\nlinearly aligned reference (e.g., Magnetic Resonance images), registration is\noften inaccurate due to contrast differences and the strong nonlinear\ndistortion of the tissue, including artefacts such as folds and tears. In this\npaper, we present a probabilistic model of spatial deformation that yields\nreconstructions for multiple histological stains that that are jointly smooth,\nrobust to outliers, and follow the reference shape. The model relies on a\nspanning tree of latent transforms connecting all the sections and slices, and\nassumes that the registration between any pair of images can be see as a noisy\nversion of the composition of (possibly inverted) latent transforms connecting\nthe two images. Bayesian inference is used to compute the most likely latent\ntransforms given a set of pairwise registrations between image pairs within and\nacross modalities. Results on synthetic deformations on multiple MR modalities,\nshow that our method can accurately and robustly register multiple contrasts\neven in the presence of outliers. The 3D histology reconstruction of two stains\n(Nissl and parvalbumin) from the Allen human brain atlas, show its benefits on\nreal data with severe distortions. We also provide the correspondence to MNI\nspace, bridging the gap between two of the most used atlases in histology and\nMRI. Data is available at https://openneuro.org/datasets/ds003590 and code at\nhttps://github.com/acasamitjana/3dhirest.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 09:57:33 GMT"}, {"version": "v2", "created": "Tue, 4 May 2021 13:39:40 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Casamitjana", "Adri\u00e0", ""], ["Lorenzi", "Marco", ""], ["Ferraris", "Sebastiano", ""], ["Peter", "Loc", ""], ["Modat", "Marc", ""], ["Stevens", "Allison", ""], ["Fischl", "Bruce", ""], ["Vercauteren", "Tom", ""], ["Iglesias", "Juan Eugenio", ""]]}, {"id": "2104.14882", "submitter": "Nick Chen", "authors": "Junru Chen, Shiqing Geng, Yongluan Yan, Danyang Huang, Hao Liu, Yadong\n  Li", "title": "Vehicle Re-identification Method Based on Vehicle Attribute and Mutual\n  Exclusion Between Cameras", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vehicle Re-identification aims to identify a specific vehicle across time and\ncamera view. With the rapid growth of intelligent transportation systems and\nsmart cities, vehicle Re-identification technology gets more and more\nattention. However, due to the difference of shooting angle and the high\nsimilarity of vehicles belonging to the same brand, vehicle re-identification\nbecomes a great challenge for existing method. In this paper, we propose a\nvehicle attribute-guided method to re-rank vehicle Re-ID result. The attributes\nused include vehicle orientation and vehicle brand . We also focus on the\ncamera information and introduce camera mutual exclusion theory to further\nfine-tune the search results. In terms of feature extraction, we combine the\ndata augmentations of multi-resolutions with the large model ensemble to get a\nmore robust vehicle features. Our method achieves mAP of 63.73% and rank-1\naccuracy 76.61% in the CVPR 2021 AI City Challenge.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 10:11:46 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Chen", "Junru", ""], ["Geng", "Shiqing", ""], ["Yan", "Yongluan", ""], ["Huang", "Danyang", ""], ["Liu", "Hao", ""], ["Li", "Yadong", ""]]}, {"id": "2104.14907", "submitter": "Dingming Yang D.Y.", "authors": "Dingming Yang, Yanrong Cui, Zeyu Yu and Hongqiang Yuan", "title": "Deep Learning Based Steel Pipe Weld Defect Detection", "comments": "17 pages,8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Steel pipes are widely used in high-risk and high-pressure scenarios such as\noil, chemical, natural gas, shale gas, etc. If there is some defect in steel\npipes, it will lead to serious adverse consequences. Applying object detection\nin the field of deep learning to pipe weld defect detection and identification\ncan effectively improve inspection efficiency and promote the development of\nindustrial automation. Most predecessors used traditional computer vision\nmethods applied to detect defects of steel pipe weld seams. However,\ntraditional computer vision methods rely on prior knowledge and can only detect\ndefects with a single feature, so it is difficult to complete the task of\nmulti-defect classification, while deep learning is end-to-end. In this paper,\nthe state-of-the-art single-stage object detection algorithm YOLOv5 is proposed\nto be applied to the field of steel pipe weld defect detection, and compared\nwith the two-stage representative object detection algorithm Faster R-CNN. The\nexperimental results show that applying YOLOv5 to steel pipe weld defect\ndetection can greatly improve the accuracy, complete the multi-classification\ntask, and meet the criteria of real-time detection.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 11:15:13 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Yang", "Dingming", ""], ["Cui", "Yanrong", ""], ["Yu", "Zeyu", ""], ["Yuan", "Hongqiang", ""]]}, {"id": "2104.14913", "submitter": "Yichao Yan", "authors": "Yichao Yan, Jie Qin1, Jiaxin Chen, Li Liu, Fan Zhu, Ying Tai, Ling\n  Shao", "title": "Learning Multi-Granular Hypergraphs for Video-Based Person\n  Re-Identification", "comments": "CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video-based person re-identification (re-ID) is an important research topic\nin computer vision. The key to tackling the challenging task is to exploit both\nspatial and temporal clues in video sequences. In this work, we propose a novel\ngraph-based framework, namely Multi-Granular Hypergraph (MGH), to pursue better\nrepresentational capabilities by modeling spatiotemporal dependencies in terms\nof multiple granularities. Specifically, hypergraphs with different spatial\ngranularities are constructed using various levels of part-based features\nacross the video sequence. In each hypergraph, different temporal granularities\nare captured by hyperedges that connect a set of graph nodes (i.e., part-based\nfeatures) across different temporal ranges. Two critical issues (misalignment\nand occlusion) are explicitly addressed by the proposed hypergraph propagation\nand feature aggregation schemes. Finally, we further enhance the overall video\nrepresentation by learning more diversified graph-level representations of\nmultiple granularities based on mutual information minimization. Extensive\nexperiments on three widely adopted benchmarks clearly demonstrate the\neffectiveness of the proposed framework. Notably, 90.0% top-1 accuracy on MARS\nis achieved using MGH, outperforming the state-of-the-arts. Code is available\nat https://github.com/daodaofr/hypergraph_reid.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 11:20:02 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Yan", "Yichao", ""], ["Qin1", "Jie", ""], ["Chen", "Jiaxin", ""], ["Liu", "Li", ""], ["Zhu", "Fan", ""], ["Tai", "Ying", ""], ["Shao", "Ling", ""]]}, {"id": "2104.14928", "submitter": "Joris Gu\\'erin", "authors": "Joris Guerin, Kevin Delmas and J\\'er\\'emie Guiochet", "title": "Certifying Emergency Landing for Safe Urban UAV", "comments": "8 pages, 4 figure, 4 tables To appear in the proceedings of the 7th\n  international workshop on Safety and Security of Intelligent Vehicles (SSIV\n  2021) at DSN 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unmanned Aerial Vehicles (UAVs) have the potential to be used for many\napplications in urban environments. However, allowing UAVs to fly above densely\npopulated areas raises concerns regarding safety. One of the main safety issues\nis the possibility for a failure to cause the loss of navigation capabilities,\nwhich can result in the UAV falling/landing in hazardous areas such as busy\nroads, where it can cause fatal accidents. Current standards, such as the SORA\npublished in 2019, do not consider applicable mitigation techniques to handle\nthis kind of hazardous situations. Consequently, certifying UAV urban\noperations implies to demonstrate very high levels of integrity, which results\nin prohibitive development costs. To address this issue, this paper explores\nthe concept of Emergency Landing (EL). A safety analysis is conducted on an\nurban UAV case study, and requirements are proposed to enable the integration\nof EL as an acceptable mitigation mean in the SORA. Based on these\nrequirements, an EL implementation was developed, together with a runtime\nmonitoring architecture to enhance confidence in the system. Preliminary\nqualitative results are presented and the monitor seem to be able to detect\nerrors of the EL system effectively.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 11:47:46 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Guerin", "Joris", ""], ["Delmas", "Kevin", ""], ["Guiochet", "J\u00e9r\u00e9mie", ""]]}, {"id": "2104.14939", "submitter": "Tarun Krishna", "authors": "Tarun Krishna, Kevin McGuinness and Noel O'Connor", "title": "Evaluating Contrastive Models for Instance-based Image Retrieval", "comments": "Accepted In Proceedings of the 2021 International Conference on\n  Multimedia Retrieval (ICMR 21)", "journal-ref": null, "doi": "10.1145/3460426.3463585", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work, we evaluate contrastive models for the task of image retrieval.\nWe hypothesise that models that are learned to encode semantic similarity among\ninstances via discriminative learning should perform well on the task of image\nretrieval, where relevancy is defined in terms of instances of the same object.\nThrough our extensive evaluation, we find that representations from models\ntrained using contrastive methods perform on-par with (and outperforms) a\npre-trained supervised baseline trained on the ImageNet labels in retrieval\ntasks under various configurations. This is remarkable given that the\ncontrastive models require no explicit supervision. Thus, we conclude that\nthese models can be used to bootstrap base models to build more robust image\nretrieval engines.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 12:05:23 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Krishna", "Tarun", ""], ["McGuinness", "Kevin", ""], ["O'Connor", "Noel", ""]]}, {"id": "2104.14945", "submitter": "Yuandu Lai", "authors": "Yuandu Lai, Yahong Han", "title": "Anomaly Detection with Prototype-Guided Discriminative Latent Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent efforts towards video anomaly detection try to learn a deep\nautoencoder to describe normal event patterns with small reconstruction errors.\nThe video inputs with large reconstruction errors are regarded as anomalies at\nthe test time. However, these methods sometimes reconstruct abnormal inputs\nwell because of the powerful generalization ability of deep autoencoder. To\naddress this problem, we present a novel approach for anomaly detection, which\nutilizes discriminative prototypes of normal data to reconstruct video frames.\nIn this way, the model will favor the reconstruction of normal events and\ndistort the reconstruction of abnormal events. Specifically, we use a\nprototype-guided memory module to perform discriminative latent embedding. We\nintroduce a new discriminative criterion for the memory module, as well as a\nloss function correspondingly, which can encourage memory items to record the\nrepresentative embeddings of normal data, i.e. prototypes. Besides, we design a\nnovel two-branch autoencoder, which is composed of a future frame prediction\nnetwork and an RGB difference generation network that share the same encoder.\nThe stacked RGB difference contains motion information just like optical flow,\nso our model can learn temporal regularity. We evaluate the effectiveness of\nour method on three benchmark datasets and experimental results demonstrate the\nproposed method outperforms the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 12:16:52 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Lai", "Yuandu", ""], ["Han", "Yahong", ""]]}, {"id": "2104.14951", "submitter": "Haoying Li", "authors": "Haoying Li, Yifan Yang, Meng Chang, Huajun Feng, Zhihai Xu, Qi Li,\n  Yueting Chen", "title": "SRDiff: Single Image Super-Resolution with Diffusion Probabilistic\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single image super-resolution (SISR) aims to reconstruct high-resolution (HR)\nimages from the given low-resolution (LR) ones, which is an ill-posed problem\nbecause one LR image corresponds to multiple HR images. Recently,\nlearning-based SISR methods have greatly outperformed traditional ones, while\nsuffering from over-smoothing, mode collapse or large model footprint issues\nfor PSNR-oriented, GAN-driven and flow-based methods respectively. To solve\nthese problems, we propose a novel single image super-resolution diffusion\nprobabilistic model (SRDiff), which is the first diffusion-based model for\nSISR. SRDiff is optimized with a variant of the variational bound on the data\nlikelihood and can provide diverse and realistic SR predictions by gradually\ntransforming the Gaussian noise into a super-resolution (SR) image conditioned\non an LR input through a Markov chain. In addition, we introduce residual\nprediction to the whole framework to speed up convergence. Our extensive\nexperiments on facial and general benchmarks (CelebA and DIV2K datasets) show\nthat 1) SRDiff can generate diverse SR results in rich details with\nstate-of-the-art performance, given only one LR input; 2) SRDiff is easy to\ntrain with a small footprint; and 3) SRDiff can perform flexible image\nmanipulation including latent space interpolation and content fusion.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 12:31:25 GMT"}, {"version": "v2", "created": "Tue, 18 May 2021 14:41:12 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Li", "Haoying", ""], ["Yang", "Yifan", ""], ["Chang", "Meng", ""], ["Feng", "Huajun", ""], ["Xu", "Zhihai", ""], ["Li", "Qi", ""], ["Chen", "Yueting", ""]]}, {"id": "2104.14963", "submitter": "Georg W\\\"olflein", "authors": "Georg W\\\"olflein and Ognjen Arandjelovi\\'c", "title": "Determining Chess Game State From an Image", "comments": "https://github.com/georgw777/chesscog", "journal-ref": "J. Imaging 2021, 7(6), 94", "doi": "10.3390/jimaging7060094", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Identifying the configuration of chess pieces from an image of a chessboard\nis a problem in computer vision that has not yet been solved accurately.\nHowever, it is important for helping amateur chess players improve their games\nby facilitating automatic computer analysis without the overhead of manually\nentering the pieces. Current approaches are limited by the lack of large\ndatasets and are not designed to adapt to unseen chess sets. This paper puts\nforth a new dataset synthesised from a 3D model that is an order of magnitude\nlarger than existing ones. Trained on this dataset, a novel end-to-end chess\nrecognition system is presented that combines traditional computer vision\ntechniques with deep learning. It localises the chessboard using a RANSAC-based\nalgorithm that computes a projective transformation of the board onto a regular\ngrid. Using two convolutional neural networks, it then predicts an occupancy\nmask for the squares in the warped image and finally classifies the pieces. The\ndescribed system achieves an error rate of 0.23% per square on the test set, 28\ntimes better than the current state of the art. Further, a few-shot transfer\nlearning approach is developed that is able to adapt the inference system to a\npreviously unseen chess set using just two photos of the starting position,\nobtaining a per-square accuracy of 99.83% on images of that new chess set. The\ncode, dataset, and trained models are made available online.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 13:02:13 GMT"}, {"version": "v2", "created": "Wed, 2 Jun 2021 14:27:50 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["W\u00f6lflein", "Georg", ""], ["Arandjelovi\u0107", "Ognjen", ""]]}, {"id": "2104.14964", "submitter": "Albert Clap\\'es", "authors": "Penny Tarling, Mauricio Cantor, Albert Clap\\'es and Sergio Escalera", "title": "Deep learning with self-supervision and uncertainty regularization to\n  count fish in underwater images", "comments": "22 pages, 6 figures, submitted to indexed journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Effective conservation actions require effective population monitoring.\nHowever, accurately counting animals in the wild to inform conservation\ndecision-making is difficult. Monitoring populations through image sampling has\nmade data collection cheaper, wide-reaching and less intrusive but created a\nneed to process and analyse this data efficiently. Counting animals from such\ndata is challenging, particularly when densely packed in noisy images.\nAttempting this manually is slow and expensive, while traditional computer\nvision methods are limited in their generalisability. Deep learning is the\nstate-of-the-art method for many computer vision tasks, but it has yet to be\nproperly explored to count animals. To this end, we employ deep learning, with\na density-based regression approach, to count fish in low-resolution sonar\nimages. We introduce a large dataset of sonar videos, deployed to record wild\nmullet schools (Mugil liza), with a subset of 500 labelled images. We utilise\nabundant unlabelled data in a self-supervised task to improve the supervised\ncounting task. For the first time in this context, by introducing uncertainty\nquantification, we improve model training and provide an accompanying measure\nof prediction uncertainty for more informed biological decision-making.\nFinally, we demonstrate the generalisability of our proposed counting framework\nthrough testing it on a recent benchmark dataset of high-resolution annotated\nunderwater images from varying habitats (DeepFish). From experiments on both\ncontrasting datasets, we demonstrate our network outperforms the few other deep\nlearning models implemented for solving this task. By providing an open-source\nframework along with training data, our study puts forth an efficient deep\nlearning template for crowd counting aquatic animals thereby contributing\neffective methods to assess natural populations from the ever-increasing visual\ndata.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 13:02:19 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Tarling", "Penny", ""], ["Cantor", "Mauricio", ""], ["Clap\u00e9s", "Albert", ""], ["Escalera", "Sergio", ""]]}, {"id": "2104.14965", "submitter": "Yichen Zhang", "authors": "Yichen Zhang, Zeyang Song, Wenbo Li", "title": "Unsupervised data augmentation for object detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Data augmentation has always been an effective way to overcome overfitting\nissue when the dataset is small. There are already lots of augmentation\noperations such as horizontal flip, random crop or even Mixup. However, unlike\nimage classification task, we cannot simply perform these operations for object\ndetection task because of the lack of labeled bounding boxes information for\ncorresponding generated images. To address this challenge, we propose a\nframework making use of Generative Adversarial Networks(GAN) to perform\nunsupervised data augmentation. To be specific, based on the recently supreme\nperformance of YOLOv4, we propose a two-step pipeline that enables us to\ngenerate an image where the object lies in a certain position. In this way, we\ncan accomplish the goal that generating an image with bounding box label.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 13:02:42 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Zhang", "Yichen", ""], ["Song", "Zeyang", ""], ["Li", "Wenbo", ""]]}, {"id": "2104.14970", "submitter": "Luis Sa-Couto", "authors": "Luis Sa-Couto and Andreas Wichert", "title": "Using brain inspired principles to unsupervisedly learn good\n  representations for visual pattern recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although deep learning has solved difficult problems in visual pattern\nrecognition, it is mostly successful in tasks where there are lots of labeled\ntraining data available. Furthermore, the global back-propagation based\ntraining rule and the amount of employed layers represents a departure from\nbiological inspiration. The brain is able to perform most of these tasks in a\nvery general way from limited to no labeled data. For these reasons it is still\na key research question to look into computational principles in the brain that\ncan help guide models to unsupervisedly learn good representations which can\nthen be used to perform tasks like classification. In this work we explore some\nof these principles to generate such representations for the MNIST data set. We\ncompare the obtained results with similar recent works and verify extremely\ncompetitive results.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 13:08:14 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Sa-Couto", "Luis", ""], ["Wichert", "Andreas", ""]]}, {"id": "2104.14984", "submitter": "Peng Wang", "authors": "Weidong Lin, Yuyan Deng, Yang Gao, Ning Wang, Jinghao Zhou, Lingqiao\n  Liu, Lei Zhang, Peng Wang", "title": "CAT: Cross-Attention Transformer for One-Shot Object Detection", "comments": "7 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a query patch from a novel class, one-shot object detection aims to\ndetect all instances of that class in a target image through the semantic\nsimilarity comparison. However, due to the extremely limited guidance in the\nnovel class as well as the unseen appearance difference between query and\ntarget instances, it is difficult to appropriately exploit their semantic\nsimilarity and generalize well. To mitigate this problem, we present a\nuniversal Cross-Attention Transformer (CAT) module for accurate and efficient\nsemantic similarity comparison in one-shot object detection. The proposed CAT\nutilizes transformer mechanism to comprehensively capture bi-directional\ncorrespondence between any paired pixels from the query and the target image,\nwhich empowers us to sufficiently exploit their semantic characteristics for\naccurate similarity comparison. In addition, the proposed CAT enables feature\ndimensionality compression for inference speedup without performance loss.\nExtensive experiments on COCO, VOC, and FSOD under one-shot settings\ndemonstrate the effectiveness and efficiency of our method, e.g., it surpasses\nCoAE, a major baseline in this task by 1.0% in AP on COCO and runs nearly 2.5\ntimes faster. Code will be available in the future.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 13:18:53 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Lin", "Weidong", ""], ["Deng", "Yuyan", ""], ["Gao", "Yang", ""], ["Wang", "Ning", ""], ["Zhou", "Jinghao", ""], ["Liu", "Lingqiao", ""], ["Zhang", "Lei", ""], ["Wang", "Peng", ""]]}, {"id": "2104.14995", "submitter": "Jonas Theiner", "authors": "Jonas Theiner, Eric M\\\"uller-Budack, Ralph Ewerth", "title": "Interpretable Semantic Photo Geolocalization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Planet-scale photo geolocalization is the complex task of estimating the\nlocation depicted in an image solely based on its visual content. Due to the\nsuccess of convolutional neural networks (CNNs), current approaches achieve\nsuper-human performance. However, previous work has exclusively focused on\noptimizing geolocalization accuracy. Moreover, due to the black-box property of\ndeep learning systems, their predictions are difficult to validate for humans.\nState-of-the-art methods treat the task as a classification problem, where the\nchoice of the classes, that is the partitioning of the world map, is the key\nfor success. In this paper, we present two contributions in order to improve\nthe interpretability of a geolocalization model: (1) We propose a novel,\nsemantic partitioning method which intuitively leads to an improved\nunderstanding of the predictions, while at the same time state-of-the-art\nresults are achieved for geolocational accuracy on benchmark test sets; (2) We\nintroduce a novel metric to assess the importance of semantic visual concepts\nfor a certain prediction to provide additional interpretable information, which\nallows for a large-scale analysis of already trained models.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 13:28:18 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Theiner", "Jonas", ""], ["M\u00fcller-Budack", "Eric", ""], ["Ewerth", "Ralph", ""]]}, {"id": "2104.15007", "submitter": "Roohallah Alizadehsani", "authors": "Nooshin Ayoobi, Danial Sharifrazi, Roohallah Alizadehsani, Afshin\n  Shoeibi, Juan M. Gorriz, Hossein Moosaei, Abbas Khosravi, Saeid Nahavandi,\n  Abdoulmohammad Gholamzadeh Chofreh, Feybi Ariani Goni, Jiri Jaromir Klemes,\n  Amir Mosavi", "title": "Time Series Forecasting of New Cases and New Deaths Rate for COVID-19\n  using Deep Learning Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Covid-19 has been started in the year 2019 and imposed restrictions in many\ncountries and costs organisations and governments. Predicting the number of new\ncases and deaths during this period can be a useful step in predicting the\ncosts and facilities required in the future. The purpose of this study is to\npredict new cases and death rate for seven days ahead. Deep learning methods\nand statistical analysis model these predictions for 100 days. Six different\ndeep learning methods are examined for the data adopted from the WHO website.\nThree methods are known as LSTM, Convolutional LSTM, and GRU. The\nbi-directional mode is then considered for each method to forecast the rate of\nnew cases and new deaths for Australia and Iran countries. This study is novel\nas it attempts to implement the mentioned three deep learning methods, along\nwith their Bi-directional models, to predict COVID-19 new cases and new death\nrate time series. All methods are compared, and results are presented. The\nresults are examined in the form of graphs and statistical analyses. The\nresults show that the Bi-directional models have lower error than other models.\nSeveral error evaluation metrics are presented to compare all models, and\nfinally, the superiority of Bi-directional methods are determined. The\nexperimental results and statistical test show on datasets to compare the\nproposed method with other baseline methods. This research could be useful for\norganisations working against COVID-19 and determining their long-term plans.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 05:44:02 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Ayoobi", "Nooshin", ""], ["Sharifrazi", "Danial", ""], ["Alizadehsani", "Roohallah", ""], ["Shoeibi", "Afshin", ""], ["Gorriz", "Juan M.", ""], ["Moosaei", "Hossein", ""], ["Khosravi", "Abbas", ""], ["Nahavandi", "Saeid", ""], ["Chofreh", "Abdoulmohammad Gholamzadeh", ""], ["Goni", "Feybi Ariani", ""], ["Klemes", "Jiri Jaromir", ""], ["Mosavi", "Amir", ""]]}, {"id": "2104.15015", "submitter": "Yang Dongming", "authors": "Dongming Yang, Yuexian Zou, Can Zhang, Meng Cao, Jie Chen", "title": "RR-Net: Injecting Interactive Semantics in Human-Object Interaction\n  Detection", "comments": "7 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human-Object Interaction (HOI) detection devotes to learn how humans interact\nwith surrounding objects. Latest end-to-end HOI detectors are short of relation\nreasoning, which leads to inability to learn HOI-specific interactive semantics\nfor predictions. In this paper, we therefore propose novel relation reasoning\nfor HOI detection. We first present a progressive Relation-aware Frame, which\nbrings a new structure and parameter sharing pattern for interaction inference.\nUpon the frame, an Interaction Intensifier Module and a Correlation Parsing\nModule are carefully designed, where: a) interactive semantics from humans can\nbe exploited and passed to objects to intensify interactions, b) interactive\ncorrelations among humans, objects and interactions are integrated to promote\npredictions. Based on modules above, we construct an end-to-end trainable\nframework named Relation Reasoning Network (abbr. RR-Net). Extensive\nexperiments show that our proposed RR-Net sets a new state-of-the-art on both\nV-COCO and HICO-DET benchmarks and improves the baseline about 5.5% and 9.8%\nrelatively, validating that this first effort in exploring relation reasoning\nand integrating interactive semantics has brought obvious improvement for\nend-to-end HOI detection.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 14:03:10 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Yang", "Dongming", ""], ["Zou", "Yuexian", ""], ["Zhang", "Can", ""], ["Cao", "Meng", ""], ["Chen", "Jie", ""]]}, {"id": "2104.15022", "submitter": "Jun-Ho Choi", "authors": "Jun-Ho Choi, Huan Zhang, Jun-Hyuk Kim, Cho-Jui Hsieh, Jong-Seok Lee", "title": "Deep Image Destruction: A Comprehensive Study on Vulnerability of Deep\n  Image-to-Image Models against Adversarial Attacks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the vulnerability of deep image classification models to\nadversarial attacks has been investigated. However, such an issue has not been\nthoroughly studied for image-to-image models that can have different\ncharacteristics in quantitative evaluation, consequences of attacks, and\ndefense strategy. To tackle this, we present comprehensive investigations into\nthe vulnerability of deep image-to-image models to adversarial attacks. For\nfive popular image-to-image tasks, 16 deep models are analyzed from various\nstandpoints such as output quality degradation due to attacks, transferability\nof adversarial examples across different tasks, and characteristics of\nperturbations. We show that unlike in image classification tasks, the\nperformance degradation on image-to-image tasks can largely differ depending on\nvarious factors, e.g., attack methods and task objectives. In addition, we\nanalyze the effectiveness of conventional defense methods used for\nclassification models in improving the robustness of the image-to-image models.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 14:20:33 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Choi", "Jun-Ho", ""], ["Zhang", "Huan", ""], ["Kim", "Jun-Hyuk", ""], ["Hsieh", "Cho-Jui", ""], ["Lee", "Jong-Seok", ""]]}, {"id": "2104.15023", "submitter": "Ivan Lazarevich", "authors": "Ivan Lazarevich and Alexander Kozlov and Nikita Malinin", "title": "Post-training deep neural network pruning via layer-wise calibration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a post-training weight pruning method for deep neural networks\nthat achieves accuracy levels tolerable for the production setting and that is\nsufficiently fast to be run on commodity hardware such as desktop CPUs or edge\ndevices. We propose a data-free extension of the approach for computer vision\nmodels based on automatically-generated synthetic fractal images. We obtain\nstate-of-the-art results for data-free neural network pruning, with ~1.5% top@1\naccuracy drop for a ResNet50 on ImageNet at 50% sparsity rate. When using real\ndata, we are able to get a ResNet50 model on ImageNet with 65% sparsity rate in\n8-bit precision in a post-training setting with a ~1% top@1 accuracy drop. We\nrelease the code as a part of the OpenVINO(TM) Post-Training Optimization tool.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 14:20:51 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Lazarevich", "Ivan", ""], ["Kozlov", "Alexander", ""], ["Malinin", "Nikita", ""]]}, {"id": "2104.15049", "submitter": "Xinglong Sun", "authors": "Xinglong Sun, Guangliang Han, Lihong Guo, Tingfa Xu, Jianan Li, Peixun\n  Liu", "title": "Updatable Siamese Tracker with Two-stage One-shot Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Offline Siamese networks have achieved very promising tracking performance,\nespecially in accuracy and efficiency. However, they often fail to track an\nobject in complex scenes due to the incapacity in online update. Traditional\nupdaters are difficult to process the irregular variations and sampling noises\nof objects, so it is quite risky to adopt them to update Siamese networks. In\nthis paper, we first present a two-stage one-shot learner, which can predict\nthe local parameters of primary classifier with object samples from diverse\nstages. Then, an updatable Siamese network is proposed based on the learner\n(SiamTOL), which is able to complement online update by itself. Concretely, we\nintroduce an extra inputting branch to sequentially capture the latest object\nfeatures, and design a residual module to update the initial exemplar using\nthese features. Besides, an effective multi-aspect training loss is designed\nfor our network to avoid overfit. Extensive experimental results on several\npopular benchmarks including OTB100, VOT2018, VOT2019, LaSOT, UAV123 and GOT10k\nmanifest that the proposed tracker achieves the leading performance and\noutperforms other state-of-the-art methods\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 15:18:41 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Sun", "Xinglong", ""], ["Han", "Guangliang", ""], ["Guo", "Lihong", ""], ["Xu", "Tingfa", ""], ["Li", "Jianan", ""], ["Liu", "Peixun", ""]]}, {"id": "2104.15060", "submitter": "Seung Wook Kim", "authors": "Seung Wook Kim, Jonah Philion, Antonio Torralba, Sanja Fidler", "title": "DriveGAN: Towards a Controllable High-Quality Neural Simulation", "comments": "CVPR 2021 Oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Realistic simulators are critical for training and verifying robotics\nsystems. While most of the contemporary simulators are hand-crafted, a\nscaleable way to build simulators is to use machine learning to learn how the\nenvironment behaves in response to an action, directly from data. In this work,\nwe aim to learn to simulate a dynamic environment directly in pixel-space, by\nwatching unannotated sequences of frames and their associated action pairs. We\nintroduce a novel high-quality neural simulator referred to as DriveGAN that\nachieves controllability by disentangling different components without\nsupervision. In addition to steering controls, it also includes controls for\nsampling features of a scene, such as the weather as well as the location of\nnon-player objects. Since DriveGAN is a fully differentiable simulator, it\nfurther allows for re-simulation of a given video sequence, offering an agent\nto drive through a recorded scene again, possibly taking different actions. We\ntrain DriveGAN on multiple datasets, including 160 hours of real-world driving\ndata. We showcase that our approach greatly surpasses the performance of\nprevious data-driven simulators, and allows for new features not explored\nbefore.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 15:30:05 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Kim", "Seung Wook", ""], ["Philion", "Jonah", ""], ["Torralba", "Antonio", ""], ["Fidler", "Sanja", ""]]}, {"id": "2104.15064", "submitter": "Giovanni Iacca Prof.", "authors": "Hao Qiu, Leonardo Lucio Custode, Giovanni Iacca", "title": "Black-box adversarial attacks using Evolution Strategies", "comments": "To be published in the proceedings of ACM Genetic and Evolutionary\n  Computation Conference (GECCO) Companion 2021", "journal-ref": null, "doi": "10.1145/3449726.3463137", "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the last decade, deep neural networks have proven to be very powerful in\ncomputer vision tasks, starting a revolution in the computer vision and machine\nlearning fields. However, deep neural networks, usually, are not robust to\nperturbations of the input data. In fact, several studies showed that slightly\nchanging the content of the images can cause a dramatic decrease in the\naccuracy of the attacked neural network. Several methods able to generate\nadversarial samples make use of gradients, which usually are not available to\nan attacker in real-world scenarios. As opposed to this class of attacks,\nanother class of adversarial attacks, called black-box adversarial attacks,\nemerged, which does not make use of information on the gradients, being more\nsuitable for real-world attack scenarios. In this work, we compare three\nwell-known evolution strategies on the generation of black-box adversarial\nattacks for image classification tasks. While our results show that the\nattacked neural networks can be, in most cases, easily fooled by all the\nalgorithms under comparison, they also show that some black-box optimization\nalgorithms may be better in \"harder\" setups, both in terms of attack success\nrate and efficiency (i.e., number of queries).\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 15:33:07 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Qiu", "Hao", ""], ["Custode", "Leonardo Lucio", ""], ["Iacca", "Giovanni", ""]]}, {"id": "2104.15069", "submitter": "Yu Tian", "authors": "Yu Tian, Jian Ren, Menglei Chai, Kyle Olszewski, Xi Peng, Dimitris N.\n  Metaxas, Sergey Tulyakov", "title": "A Good Image Generator Is What You Need for High-Resolution Video\n  Synthesis", "comments": "Accepted to ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image and video synthesis are closely related areas aiming at generating\ncontent from noise. While rapid progress has been demonstrated in improving\nimage-based models to handle large resolutions, high-quality renderings, and\nwide variations in image content, achieving comparable video generation results\nremains problematic. We present a framework that leverages contemporary image\ngenerators to render high-resolution videos. We frame the video synthesis\nproblem as discovering a trajectory in the latent space of a pre-trained and\nfixed image generator. Not only does such a framework render high-resolution\nvideos, but it also is an order of magnitude more computationally efficient. We\nintroduce a motion generator that discovers the desired trajectory, in which\ncontent and motion are disentangled. With such a representation, our framework\nallows for a broad range of applications, including content and motion\nmanipulation. Furthermore, we introduce a new task, which we call cross-domain\nvideo synthesis, in which the image and motion generators are trained on\ndisjoint datasets belonging to different domains. This allows for generating\nmoving objects for which the desired video data is not available. Extensive\nexperiments on various datasets demonstrate the advantages of our methods over\nexisting video generation techniques. Code will be released at\nhttps://github.com/snap-research/MoCoGAN-HD.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 15:38:41 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Tian", "Yu", ""], ["Ren", "Jian", ""], ["Chai", "Menglei", ""], ["Olszewski", "Kyle", ""], ["Peng", "Xi", ""], ["Metaxas", "Dimitris N.", ""], ["Tulyakov", "Sergey", ""]]}, {"id": "2104.15082", "submitter": "Ruowei Jiang", "authors": "Zeqi Li, Ruowei Jiang and Parham Aarabi", "title": "Semantic Relation Preserving Knowledge Distillation for Image-to-Image\n  Translation", "comments": "Accepted to ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative adversarial networks (GANs) have shown significant potential in\nmodeling high dimensional distributions of image data, especially on\nimage-to-image translation tasks. However, due to the complexity of these\ntasks, state-of-the-art models often contain a tremendous amount of parameters,\nwhich results in large model size and long inference time. In this work, we\npropose a novel method to address this problem by applying knowledge\ndistillation together with distillation of a semantic relation preserving\nmatrix. This matrix, derived from the teacher's feature encoding, helps the\nstudent model learn better semantic relations. In contrast to existing\ncompression methods designed for classification tasks, our proposed method\nadapts well to the image-to-image translation task on GANs. Experiments\nconducted on 5 different datasets and 3 different pairs of teacher and student\nmodels provide strong evidence that our methods achieve impressive results both\nqualitatively and quantitatively.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 16:04:19 GMT"}, {"version": "v2", "created": "Wed, 19 May 2021 01:44:41 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Li", "Zeqi", ""], ["Jiang", "Ruowei", ""], ["Aarabi", "Parham", ""]]}, {"id": "2104.15092", "submitter": "Youjiang Xu", "authors": "Youjiang Xu, Linchao Zhu, Lu Jiang, Yi Yang", "title": "Faster Meta Update Strategy for Noise-Robust Deep Learning", "comments": "Accepted to CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has been shown that deep neural networks are prone to overfitting on\nbiased training data. Towards addressing this issue, meta-learning employs a\nmeta model for correcting the training bias. Despite the promising\nperformances, super slow training is currently the bottleneck in the meta\nlearning approaches. In this paper, we introduce a novel Faster Meta Update\nStrategy (FaMUS) to replace the most expensive step in the meta gradient\ncomputation with a faster layer-wise approximation. We empirically find that\nFaMUS yields not only a reasonably accurate but also a low-variance\napproximation of the meta gradient. We conduct extensive experiments to verify\nthe proposed method on two tasks. We show our method is able to save two-thirds\nof the training time while still maintaining the comparable or achieving even\nbetter generalization performance. In particular, our method achieves the\nstate-of-the-art performance on both synthetic and realistic noisy labels, and\nobtains promising performance on long-tailed recognition on standard\nbenchmarks.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 16:19:07 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Xu", "Youjiang", ""], ["Zhu", "Linchao", ""], ["Jiang", "Lu", ""], ["Yang", "Yi", ""]]}, {"id": "2104.15119", "submitter": "Francois Darmon", "authors": "Fran\\c{c}ois Darmon and B\\'en\\'edicte Bascle and Jean-Cl\\'ement Devaux\n  and Pascal Monasse and Mathieu Aubry", "title": "Deep Multi-View Stereo gone wild", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Deep multi-view stereo (deep MVS) methods have been developed and extensively\ncompared on simple datasets, where they now outperform classical approaches. In\nthis paper, we ask whether the conclusions reached in controlled scenarios are\nstill valid when working with Internet photo collections. We propose a\nmethodology for evaluation and explore the influence of three aspects of deep\nMVS methods: network architecture, training data, and supervision. We make\nseveral key observations, which we extensively validate quantitatively and\nqualitatively, both for depth prediction and complete 3D reconstructions.\nFirst, we outline the promises of unsupervised techniques by introducing a\nsimple approach which provides more complete reconstructions than supervised\noptions when using a simple network architecture. Second, we emphasize that not\nall multiscale architectures generalize to the unconstrained scenario,\nespecially without supervision. Finally, we show the efficiency of noisy\nsupervision from large-scale 3D reconstructions which can even lead to networks\nthat outperform classical methods in scenarios where very few images are\navailable.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 17:07:17 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Darmon", "Fran\u00e7ois", ""], ["Bascle", "B\u00e9n\u00e9dicte", ""], ["Devaux", "Jean-Cl\u00e9ment", ""], ["Monasse", "Pascal", ""], ["Aubry", "Mathieu", ""]]}, {"id": "2104.15139", "submitter": "Vladislav Golyanik", "authors": "Jalees Nehvi and Vladislav Golyanik and Franziska Mueller and\n  Hans-Peter Seidel and Mohamed Elgharib and Christian Theobalt", "title": "Differentiable Event Stream Simulator for Non-Rigid 3D Tracking", "comments": "In CVPR 2021 Workshop on Event-based Vision. Project page:\n  http://gvv.mpi-inf.mpg.de/projects/Event-based_Non-rigid_3D_Tracking", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces the first differentiable simulator of event streams,\ni.e., streams of asynchronous brightness change signals recorded by event\ncameras. Our differentiable simulator enables non-rigid 3D tracking of\ndeformable objects (such as human hands, isometric surfaces and general\nwatertight meshes) from event streams by leveraging an analysis-by-synthesis\nprinciple. So far, event-based tracking and reconstruction of non-rigid objects\nin 3D, like hands and body, has been either tackled using explicit event\ntrajectories or large-scale datasets. In contrast, our method does not require\nany such processing or data, and can be readily applied to incoming event\nstreams. We show the effectiveness of our approach for various types of\nnon-rigid objects and compare to existing methods for non-rigid 3D tracking. In\nour experiments, the proposed energy-based formulations outperform competing\nRGB-based methods in terms of 3D errors. The source code and the new data are\npublicly available.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 17:58:07 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Nehvi", "Jalees", ""], ["Golyanik", "Vladislav", ""], ["Mueller", "Franziska", ""], ["Seidel", "Hans-Peter", ""], ["Elgharib", "Mohamed", ""], ["Theobalt", "Christian", ""]]}]